{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Incremental_learning_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riu9BVr_-nl2"
      },
      "source": [
        "# Covid Dataset\n",
        "\n",
        "**Required Dataset features and target**\n",
        "\n",
        "The dataset has 53 columns; 1 to represent the country, 1 to represent the day (it will be an integer), 50 floats to represent the positive cases of the 50 previous days, and 1 column to represent the output that is the average of a full week of cases.\n",
        "\n",
        "![required_features.jpg](https://drive.google.com/uc?id=1smUwSHRwMT8h-M8kjG3ymmxdhQbe1HvY)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "ZhjnZH15MMQA",
        "outputId": "2b3e59c7-9d0c-4796-8def-5f7739892c2c"
      },
      "source": [
        "# Installing Incremental learner: Scikit-Multiflow\n",
        "!pip install scikit-multiflow\n",
        "\n",
        "\n",
        "# Overdiding some files from scikit multiflow library\n",
        "!gdown https://drive.google.com/uc?id=1f5GgBqjAsTUFnubHODmfqn6qiNyDBqIw\n",
        "!unzip /content/src.zip -d /content/src\n",
        "!cp -r /content/src/src /content/\n",
        "!rm -r /content/src/src\n",
        "\n",
        "# Creating a seperate directory to store all csv's\n",
        "! mkdir -p /content/csv_files\n",
        "! mkdir -p /content/csv_files/processed_null\n",
        "! mkdir -p /content/csv_files/processed\n",
        "! mkdir -p /content/Result/exp1\n",
        "! mkdir -p /content/Result/exp2\n",
        "! mkdir -p /content/Result/exp1/runtime\n",
        "! mkdir -p /content/Result/exp2/runtime\n",
        "! mkdir -p /content/Result/exp1/summary\n",
        "! mkdir -p /content/Result/exp2/summary\n",
        "! mkdir -p /content/Plots\n",
        "! mkdir -p /content/Plots/barplot\n",
        "! mkdir -p /content/Plots/boxplots\n",
        "! mkdir -p /content/Result/exp1/united_dataframe\n",
        "! mkdir -p /content/Result/exp1/united_dataframe/incremental\n",
        "! mkdir -p /content/Result/exp1/united_dataframe/static\n",
        "! mkdir -p /content/Result/exp2/united_dataframe\n",
        "! mkdir -p /content/Result/exp2/united_dataframe/incremental\n",
        "! mkdir -p /content/Result/exp2/united_dataframe/static\n",
        "\n",
        "# Download the zip file\n",
        "\"\"\"\n",
        "!zip -r /content/file.zip /content/csv_files\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-multiflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/ac/5f4675aa1e9f4c2a1d139241b50288a17e4048f5bad3484b18efc6acc4b8/scikit_multiflow-0.5.3-cp36-cp36m-manylinux2010_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (1.1.5)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (0.22.2.post1)\n",
            "Requirement already satisfied: sortedcontainers>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.3->scikit-multiflow) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.3->scikit-multiflow) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->scikit-multiflow) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.3->scikit-multiflow) (1.15.0)\n",
            "Installing collected packages: scikit-multiflow\n",
            "Successfully installed scikit-multiflow-0.5.3\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1f5GgBqjAsTUFnubHODmfqn6qiNyDBqIw\n",
            "To: /content/src.zip\n",
            "100% 26.1k/26.1k [00:00<00:00, 9.22MB/s]\n",
            "Archive:  /content/src.zip\n",
            "  inflating: /content/src/src/_classification_performance_evaluator.py  \n",
            "  inflating: /content/src/src/base_evaluator.py  \n",
            "  inflating: /content/src/src/constants.py  \n",
            "  inflating: /content/src/src/evaluate_prequential.py  \n",
            "  inflating: /content/src/src/evaluation_data_buffer.py  \n",
            "  inflating: /content/src/src/measure_collection.py  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!zip -r /content/file.zip /content/csv_files\\nfrom google.colab import files\\nfiles.download(\"/content/file.zip\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz3gWpvxYPLo",
        "outputId": "7bf80aa4-8956-4ffc-b9c5-65d4da30ffab"
      },
      "source": [
        "# For Box plot: Run this only if manually uploaded the results\n",
        "!unzip /content/Result.zip -d /content/Result\n",
        "!cp -r /content/Result/content/Result /content/\n",
        "!rm -r /content/Result/content/Result\n",
        "!rm -r /content/Result/content\n",
        "\n",
        "csv_processed_path = '/content/csv_files/processed'\n",
        "csv_processed_with_null_path = '/content/csv_files/processed_null'\n",
        "exp1_path = '/content/Result/exp1'\n",
        "exp2_path = '/content/Result/exp2'\n",
        "exp1_runtime_path = '/content/Result/exp1/runtime'\n",
        "exp2_runtime_path = '/content/Result/exp2/runtime'\n",
        "exp1_summary_path = '/content/Result/exp1/summary'\n",
        "exp2_summary_path = '/content/Result/exp2/summary'\n",
        "bar_plot_path = r'/content/Plots/barplot'\n",
        "box_plot_path = r'/content/Plots/boxplots'\n",
        "exp1_static_united_df_path = '/content/Result/exp1/united_dataframe/static'\n",
        "exp1_inc_united_df_path = '/content/Result/exp1/united_dataframe/incremental'\n",
        "exp2_static_united_df_path = '/content/Result/exp2/united_dataframe/static'\n",
        "exp2_inc_united_df_path = '/content/Result/exp2/united_dataframe/incremental'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open /content/Result.zip, /content/Result.zip.zip or /content/Result.zip.ZIP.\n",
            "cp: cannot stat '/content/Result/content/Result': No such file or directory\n",
            "rm: cannot remove '/content/Result/content/Result': No such file or directory\n",
            "rm: cannot remove '/content/Result/content': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqTR_N3fpMON",
        "outputId": "02efd985-50eb-40d3-8a91-1f343744e9c7"
      },
      "source": [
        "#!pip uninstall keras\n",
        "#!pip uninstall tensorflow\n",
        "\n",
        "!pip install keras==2.3.1\n",
        "!pip install tensorflow==2.1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.19.5)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.15.0)\n",
            "Installing collected packages: keras-applications, keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.3.1 keras-applications-1.0.8\n",
            "Collecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 37kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.10.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.36.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.3.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.19.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.32.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 44.3MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 28.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.12.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.25.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (53.0.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.2.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.12.5)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=be1edcedbdc43fdf77126884eda78dbe6fb218acbb61009d27f2d8a9f65b91d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvDISgy5CUOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0db1973-e4d8-4206-96b6-e8f049ed4f47"
      },
      "source": [
        "# General Imports \n",
        "import pandas as pd\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "import warnings\n",
        "from pandas.core.common import SettingWithCopyWarning\n",
        "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme(style='darkgrid')\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "# Imports for incremental learner\n",
        "from skmultiflow.data import DataStream\n",
        "from skmultiflow.trees import HoeffdingTreeRegressor\n",
        "from src.evaluate_prequential import EvaluatePrequential\n",
        "from skmultiflow.meta import AdaptiveRandomForestRegressor\n",
        "from skmultiflow.trees import HoeffdingAdaptiveTreeRegressor\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.linear_model import PassiveAggressiveRegressor\n",
        "\n",
        "# Imports for static Learner\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from time import perf_counter as pc_timer\n",
        "from functools import wraps\n",
        "\n",
        "import keras\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "#from tensorflow.keras import Sequential\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# For significance tests\n",
        "from scipy.stats import normaltest\n",
        "from scipy import stats \n",
        "# pd.set_option('display.max_colwidth', 500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "wHwu0MJOCm68",
        "outputId": "8eb5cbb6-7765-4019-c440-54a231ab5ecd"
      },
      "source": [
        "#url = 'https://drive.google.com/file/d/1e7NsptfEFLG2gGLykYlrzjNbDJLiRbGm/view?usp=sharing'\n",
        "url = 'https://drive.google.com/file/d/1VH-nkePskK3gT6U5qkoOP-0hFT4beszC/view?usp=sharing'\n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "df = pd.read_csv(path)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dateRep</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "      <th>cases</th>\n",
              "      <th>deaths</th>\n",
              "      <th>countriesAndTerritories</th>\n",
              "      <th>geoId</th>\n",
              "      <th>countryterritoryCode</th>\n",
              "      <th>popData2019</th>\n",
              "      <th>continentExp</th>\n",
              "      <th>Cumulative_number_for_14_days_of_COVID-19_cases_per_100000</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>02/11/2020</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>2020</td>\n",
              "      <td>132</td>\n",
              "      <td>5</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>3.767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01/11/2020</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>2020</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>3.575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31/10/2020</td>\n",
              "      <td>31</td>\n",
              "      <td>10</td>\n",
              "      <td>2020</td>\n",
              "      <td>157</td>\n",
              "      <td>4</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>3.554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30/10/2020</td>\n",
              "      <td>30</td>\n",
              "      <td>10</td>\n",
              "      <td>2020</td>\n",
              "      <td>123</td>\n",
              "      <td>3</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>3.265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>29/10/2020</td>\n",
              "      <td>29</td>\n",
              "      <td>10</td>\n",
              "      <td>2020</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>2.942</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      dateRep  ...  Cumulative_number_for_14_days_of_COVID-19_cases_per_100000\n",
              "0  02/11/2020  ...                                              3.767         \n",
              "1  01/11/2020  ...                                              3.575         \n",
              "2  31/10/2020  ...                                              3.554         \n",
              "3  30/10/2020  ...                                              3.265         \n",
              "4  29/10/2020  ...                                              2.942         \n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6NQxu_LuoTV"
      },
      "source": [
        "# Grouping countries together for analysis\n",
        "total_countries = df['countriesAndTerritories'].unique()\n",
        "df_grouped = df.groupby('countriesAndTerritories')\n",
        "pretrain_days = [30,60,90,120,150,180]  # List of pretrain days\n",
        "valid_countries = []\n",
        "decimal = 3  # Specify the scale of decimal places \n",
        "error_metrics = ['MAE','MAPE', 'RMSE']\n",
        "\n",
        "# Setting path variables for both experiments\n",
        "csv_processed_path = '/content/csv_files/processed'\n",
        "csv_processed_with_null_path = '/content/csv_files/processed_null'\n",
        "exp1_path = '/content/Result/exp1'\n",
        "exp2_path = '/content/Result/exp2'\n",
        "exp1_runtime_path = '/content/Result/exp1/runtime'\n",
        "exp2_runtime_path = '/content/Result/exp2/runtime'\n",
        "exp1_summary_path = '/content/Result/exp1/summary'\n",
        "exp2_summary_path = '/content/Result/exp2/summary'\n",
        "bar_plot_path = r'/content/Plots/barplot'\n",
        "box_plot_path = r'/content/Plots/boxplots'\n",
        "\n",
        "\n",
        "# Top countries to select for experiment 1\n",
        "Number_of_countries = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oubf_WqnATmM"
      },
      "source": [
        "## Feature Set with Individual Countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0vj8668gNrP"
      },
      "source": [
        "# Create lags\n",
        "def create_features_with_lags(df):\n",
        "  for i in range(89, 0, -1):  # Loop in reverse order for creating ordered lags eg: cases_t-10, cases_t-9... cases_t-1. t=current cases\n",
        "    df[f'cases_t-{i}'] = df['cases'].shift(i, axis=0)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G23w6M6HdyEb",
        "outputId": "aa53e908-acb9-4a47-c3ae-c51cb3aa2376"
      },
      "source": [
        "# Pre-Processing dataset and saving them into csv's.\n",
        "for country in total_countries:\n",
        "  df = df_grouped.get_group(country)\n",
        "\n",
        "  # Selecting required features\n",
        "  df= df[['dateRep','cases','countriesAndTerritories']]\n",
        "\n",
        "  # Rename features\n",
        "  df.rename(columns={'countriesAndTerritories':'country', 'dateRep':'date'}, inplace=True)\n",
        "\n",
        "  # Convert to date, sort and set index\n",
        "  df['date'] = pd.to_datetime(df['date'],format='%d/%m/%Y')\n",
        "  df.sort_values('date', inplace=True)\n",
        "  df.set_index('date', inplace=True)\n",
        "\n",
        "  # Adding feature\n",
        "  df['day_no']= pd.Series([i for i in range(1,len(df)+1)], index=df.index)\n",
        "\n",
        "  # Reordering features\n",
        "  df = df[['day_no', 'country','cases']]\n",
        "\n",
        "  # Adding features through lags\n",
        "  df = create_features_with_lags(df)\n",
        "\n",
        "  # Creating target with last 10 days cases\n",
        "  df['target'] = df.iloc[:,[2]+[i*-1 for i in range(1,10)]].mean(axis=1)\n",
        "\n",
        "  # Dropping mid columns\n",
        "  drop_columns = list(df.loc[:,'cases_t-39':'cases_t-1'].columns)  #list(df.loc[:,'cases_t-38':'cases_t-1'].columns)\n",
        "  df.drop(drop_columns, axis=1, inplace=True)\n",
        "\n",
        "  # Country name\n",
        "  filename = df['country'].unique()[0]\n",
        "\n",
        "  # Saving file\n",
        "  df.to_csv(f'{csv_processed_with_null_path}/{filename}.csv')\n",
        "\n",
        "  # Dropping null records\n",
        "  df.dropna(how='any', axis=0, inplace=True)\n",
        "\n",
        "  # Valid countries that have records more than max of pretrain\n",
        "  if len(df)>max(pretrain_days):\n",
        "    valid_countries.append(country)  \n",
        "    df.to_csv(f'{csv_processed_path}/{filename}.csv')\n",
        "  \n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMZCoXFwPsHA"
      },
      "source": [
        "## Total cases of top selected countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTQjGxITEeMD"
      },
      "source": [
        "# Added just for plots. Remove later\n",
        "Number_of_countries = 25\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDWlSV8OJUQP"
      },
      "source": [
        "# Replaces underscore from country names\n",
        "def format_names(list_countries):\n",
        "  updated_country_list = []\n",
        "  for country_name in list_countries:\n",
        "    updated_country_list.append(country_name.replace(\"_\",\" \"))\n",
        "  return updated_country_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQfWz6LlAyHf"
      },
      "source": [
        "# A dictionary of all countries\n",
        "dict_countries = Counter(valid_countries)\n",
        "\n",
        "for country in valid_countries:\n",
        "  dict_countries[country] = df_grouped.get_group(country)['cases'].sum()\n",
        "\n",
        "# Select top_countries and order(Ascending/Decending) \n",
        "top_countries = sorted(dict_countries.items(), key=lambda dict_countries: dict_countries[1], reverse=True) [0:Number_of_countries]\n",
        "\n",
        "# Creating dataframe of top selected countries\n",
        "df_top_countries = pd.DataFrame.from_dict(dict(top_countries), orient='index', columns=['Total Cases'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_U3MIv16lLfX",
        "outputId": "e4ba7aa7-659c-40f8-95a2-c7681a27d4e0"
      },
      "source": [
        "# Plotting graph\n",
        "sns.set_theme(style='white')\n",
        "plt.figure(figsize=(12,16))\n",
        "top_countries_list = format_names(df_top_countries.index)\n",
        "plt.barh(top_countries_list[::-1], df_top_countries['Total Cases'].values[::-1]) # Reversing the order to have heighest values at the top of bar chart\n",
        "#plt.title(f'Top {len(top_countries)} Countries with Most Cases')\n",
        "plt.xscale('log')\n",
        "ax = plt.axes() # for updating axes values to plain text\n",
        "ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "plt.margins(y=0)\n",
        "ax.tick_params(axis='both', which='major', labelsize=18)\n",
        "plt.xlabel('Number of Cases', fontsize=20)\n",
        "plt.ylabel('Countries',fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{bar_plot_path}/top_selected_country_cases.pdf')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x1152 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "qbgnoDsBEsoQ",
        "outputId": "75d83488-e2d8-47d0-eedc-5ed1a0307f18"
      },
      "source": [
        "\"\"\"\n",
        "# Plotting graph\n",
        "sns.set_theme(style='white')\n",
        "plt.figure(figsize=(12,16), dpi=90)\n",
        "\n",
        "#plt.bar(df_top_countries.index, df_top_countries['Total Cases'].values)\n",
        "top_countries_list = format_names(df_top_countries.index)\n",
        "plt.barh(top_countries_list, df_top_countries['Total Cases'].values)\n",
        "\n",
        "#plt.axvline(Number_of_countries-0.5, 0,1, ls='--', c='black')\n",
        "plt.axhline(y=(Number_of_countries-0.5), ls='--', c='black')\n",
        "\n",
        "plt.title(f'Top {len(top_countries)} Countries with Most Cases')\n",
        "\n",
        "plt.xscale('log')\n",
        "ax = plt.axes() # for updating \n",
        "ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "\n",
        "#plt.gcf().axes[0].yaxis.get_major_formatter().set_scientific(False)  # To get labels in plain text \n",
        "#plt.xticks(rotation=90)\n",
        "plt.margins(y=0)\n",
        "plt.xlabel('Number of Cases')\n",
        "plt.ylabel('Countries')\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Plotting graph\\nsns.set_theme(style='white')\\nplt.figure(figsize=(12,16), dpi=90)\\n\\n#plt.bar(df_top_countries.index, df_top_countries['Total Cases'].values)\\ntop_countries_list = format_names(df_top_countries.index)\\nplt.barh(top_countries_list, df_top_countries['Total Cases'].values)\\n\\n#plt.axvline(Number_of_countries-0.5, 0,1, ls='--', c='black')\\nplt.axhline(y=(Number_of_countries-0.5), ls='--', c='black')\\n\\nplt.title(f'Top {len(top_countries)} Countries with Most Cases')\\n\\nplt.xscale('log')\\nax = plt.axes() # for updating \\nax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\\n\\n#plt.gcf().axes[0].yaxis.get_major_formatter().set_scientific(False)  # To get labels in plain text \\n#plt.xticks(rotation=90)\\nplt.margins(y=0)\\nplt.xlabel('Number of Cases')\\nplt.ylabel('Countries')\\nplt.show()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0TQucg2Pwu1"
      },
      "source": [
        "## Average cases of top selected countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "6kOjiJOSRCtG",
        "outputId": "2a7c6af5-f806-46b6-ec64-ea25186708a3"
      },
      "source": [
        "dict_countries_avg = Counter(total_countries)\n",
        "\n",
        "for country in dict_countries.keys():\n",
        "  dict_countries_avg[country] = df_grouped.get_group(country)['cases'].mean()\n",
        "\n",
        "# Average cases for all countries\n",
        "df_avg_cases_countries = pd.DataFrame.from_dict(dict_countries_avg, orient='index', columns=['Average'])\n",
        "\n",
        "# List of top selected countries\n",
        "top_countries = list(df_top_countries.index)\n",
        "\n",
        "# Average of selected top countries\n",
        "avg_df = df_avg_cases_countries[df_avg_cases_countries.index.isin(top_countries)]\n",
        "avg_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Belgium</th>\n",
              "      <td>1431.568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Brazil</th>\n",
              "      <td>18005.536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Canada</th>\n",
              "      <td>768.964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Czechia</th>\n",
              "      <td>1109.234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ecuador</th>\n",
              "      <td>558.396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>France</th>\n",
              "      <td>4590.633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Germany</th>\n",
              "      <td>1769.568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>India</th>\n",
              "      <td>26805.580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Indonesia</th>\n",
              "      <td>1371.375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iran</th>\n",
              "      <td>2014.581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iraq</th>\n",
              "      <td>1553.229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Israel</th>\n",
              "      <td>1033.216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Italy</th>\n",
              "      <td>2303.036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mexico</th>\n",
              "      <td>3097.973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nepal</th>\n",
              "      <td>588.363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Netherlands</th>\n",
              "      <td>1167.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pakistan</th>\n",
              "      <td>1105.917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Philippines</th>\n",
              "      <td>1260.240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Romania</th>\n",
              "      <td>806.088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Russia</th>\n",
              "      <td>5314.224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Spain</th>\n",
              "      <td>3862.143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Switzerland</th>\n",
              "      <td>499.117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>United_Arab_Emirates</th>\n",
              "      <td>443.401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>United_Kingdom</th>\n",
              "      <td>3360.110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>United_States_of_America</th>\n",
              "      <td>29894.032</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           Average\n",
              "Belgium                   1431.568\n",
              "Brazil                   18005.536\n",
              "Canada                     768.964\n",
              "Czechia                   1109.234\n",
              "Ecuador                    558.396\n",
              "France                    4590.633\n",
              "Germany                   1769.568\n",
              "India                    26805.580\n",
              "Indonesia                 1371.375\n",
              "Iran                      2014.581\n",
              "Iraq                      1553.229\n",
              "Israel                    1033.216\n",
              "Italy                     2303.036\n",
              "Mexico                    3097.973\n",
              "Nepal                      588.363\n",
              "Netherlands               1167.003\n",
              "Pakistan                  1105.917\n",
              "Philippines               1260.240\n",
              "Romania                    806.088\n",
              "Russia                    5314.224\n",
              "Spain                     3862.143\n",
              "Switzerland                499.117\n",
              "United_Arab_Emirates       443.401\n",
              "United_Kingdom            3360.110\n",
              "United_States_of_America 29894.032"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkabfg44B2iE"
      },
      "source": [
        "# Common Methods for All Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jkIXUcMCpwy"
      },
      "source": [
        "## Common Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONXUYHUmNJka"
      },
      "source": [
        "#### Updated Number of countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiLed1lngDEB"
      },
      "source": [
        "Number_of_countries = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2wFG6bg0-0W"
      },
      "source": [
        "# Global variables for countries\n",
        "countries = top_countries[0:Number_of_countries]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHYQO1XBALXe"
      },
      "source": [
        "# Return a combined dataframe for a each error statistics(MAE,RMSE,MAPE etc) along with the newly added mean row.\n",
        "def get_metric_with_mean(result: pd.DataFrame, error_metric: str)->pd.DataFrame:\n",
        "  df_grouped = result.groupby('EvaluationMeasurement')\n",
        "  df = df_grouped.get_group(error_metric).reset_index(drop=True)\n",
        "  df = df.append(df.describe().loc['mean'])\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AVGVaKLwhHu"
      },
      "source": [
        "def calc_mean_to_max_error(df, max_of_pretrain_days, max_of_df):\n",
        "  i=-1\n",
        "  for row_num in range(len(df)-1):  # Go before mean row\n",
        "    i += 1\n",
        "    for col_num in df.columns[2:]:\n",
        "      df.loc[row_num,col_num] = df.loc[row_num,col_num]/max_of_pretrain_days[i] \n",
        "  \n",
        "  for col in df.columns[2:]:\n",
        "      df.loc['mean',col] = df.loc['mean',col]/max_of_df\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgvjK5nJkUaG"
      },
      "source": [
        "# Note: Do not change the filenames, since they are later being used for visualizations \n",
        "def save_runtime(df,path,country=None,static_learner=True,alternate_batch=False, transpose=False):\n",
        "  df = df.apply(pd.to_numeric,errors='ignore') # Converting the dataframe to numeric\n",
        "  df = df.round(decimal) # Setting the precision\n",
        "  \n",
        "  # if transpose flag is set to true\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "\n",
        "  if country==None:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/combined25country_runtime_static.tex')\n",
        "      df.to_csv(f'{path}/combined25country_runtime_static.csv')\n",
        "    else:\n",
        "      if alternate_batch:\n",
        "         df.to_latex(f'{path}/combined25country_runtime_incremental_alternate_batch.tex')\n",
        "         df.to_csv(f'{path}/combined25country_runtime_incremental_alternate_batch.csv')\n",
        "      else:\n",
        "        df.to_latex(f'{path}/combined25country_runtime_incremental.tex')\n",
        "        df.to_csv(f'{path}/combined25country_runtime_incremental.csv')\n",
        "  else:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/{country}_runtime_static.tex')\n",
        "      df.to_csv(f'{path}/{country}_runtime_static.csv')\n",
        "    else:\n",
        "      df.to_latex(f'{path}/{country}_runtime_incremental.tex')\n",
        "      df.to_csv(f'{path}/{country}_runtime_incremental.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJCVpF7oT7r0"
      },
      "source": [
        "# Note: Do not change the filenames, since they are later being used for visualizations \n",
        "def save_summary_table(df,path,country=False,static_learner=True,alternate_batch=False, transpose=False):\n",
        "\n",
        "  df = df.apply(pd.to_numeric,errors='ignore') # Converting the dataframe to numeric\n",
        "  df = df.round(decimal) # Setting the precision\n",
        "  \n",
        "  # if transpose flag is set to true\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "\n",
        "  if country:\n",
        "    metric = df.loc['EvaluationMeasurement'].unique()[0]\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/top_countries_{metric}_summary_table_static.tex')\n",
        "      df.to_csv(f'{path}/top_countries_{metric}_summary_table_static.csv')\n",
        "    else:\n",
        "      df.to_latex(f'{path}/top_countries_{metric}_summary_table_incremental.tex')\n",
        "      df.to_csv(f'{path}/top_countries_{metric}_summary_table_incremental.csv')\n",
        "    \n",
        "  else:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/combined25country_summary_table_static.tex')\n",
        "      df.to_csv(f'{path}/combined25country_summary_table_static.csv')\n",
        "    else:\n",
        "      if alternate_batch:\n",
        "         df.to_latex(f'{path}/combined25country_summary_table_incremental_alternate_batch.tex')\n",
        "         df.to_csv(f'{path}/combined25country_summary_table_incremental_alternate_batch.csv')\n",
        "      else:\n",
        "        df.to_latex(f'{path}/combined25country_summary_table_incremental.tex')\n",
        "        df.to_csv(f'{path}/combined25country_summary_table_incremental.csv')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnS1UsFdKRbW"
      },
      "source": [
        "# Note: Do not change the filenames since they are later being used for visualizations\n",
        "def save_metrics(df, path, country=None, static_learner=True, alternate_batch=False, transpose=False): \n",
        "  df = df.apply(pd.to_numeric,errors='ignore') # Converting the dataframe to numeric\n",
        "  df = df.round(decimal) # Setting the precision\n",
        "  \n",
        "  # if transpose flag is set to true\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "\n",
        "  metric_type = df.loc['EvaluationMeasurement'].unique()[0]\n",
        "  if country==None:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/combined25country_{metric_type}_static.tex')\n",
        "      df.to_csv(f'{path}/combined25country_{metric_type}_static.csv')\n",
        "    else:\n",
        "      if alternate_batch:\n",
        "         df.to_latex(f'{path}/combined25country_{metric_type}_incremental_alternate_batch.tex')\n",
        "         df.to_csv(f'{path}/combined25country_{metric_type}_incremental_alternate_batch.csv')\n",
        "      else:\n",
        "        df.to_latex(f'{path}/combined25country_{metric_type}_incremental.tex')\n",
        "        df.to_csv(f'{path}/combined25country_{metric_type}_incremental.csv')\n",
        "  else:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/{country}_{metric_type}_static.tex')\n",
        "      df.to_csv(f'{path}/{country}_{metric_type}_static.csv')\n",
        "    else:\n",
        "      df.to_latex(f'{path}/{country}_{metric_type}_incremental.tex')\n",
        "      df.to_csv(f'{path}/{country}_{metric_type}_incremental.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQFR3NKzuSK-"
      },
      "source": [
        "def save_combined_summary_table(df, path, static_learner=False,transpose=False):\n",
        "  df = df.apply(pd.to_numeric,errors='ignore')\n",
        "  df = df.round(decimal)\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "  \n",
        "  if static_learner:\n",
        "    save_path = f'{path}/summary_table_combined_mean_static'\n",
        "  else:\n",
        "    save_path = f'{path}/summary_table_combined_mean_incremental'\n",
        "\n",
        "  df.to_csv(f'{save_path}.csv')\n",
        "  df.to_latex(f'{save_path}.tex')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umh96O9TSp2Z"
      },
      "source": [
        "def save_united_df(df, path, country=None):\r\n",
        "    if country:\r\n",
        "        df.to_csv(f'{path}/{country}.csv')\r\n",
        "    else:\r\n",
        "        df.to_csv(f'{path}/united_df.csv')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUrUegDEPqof"
      },
      "source": [
        "def display_runtime_per_country(results_runtime,countries):\n",
        "  for i in range(len(countries)):\n",
        "    print(f'_____________Running Time for {countries[i]}________________')\n",
        "    print(results_runtime[i].to_string())\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI4IiENZtLFG"
      },
      "source": [
        "def calc_save_err_metric_countrywise(countries, error_metrics, results, max_of_pretrain_per_country, max_cases_per_country, path, static_learner, transpose):\n",
        "  countrywise_error_scores={}\n",
        "  for i in range(len(countries)):\n",
        "    country_error_score = []\n",
        "    for error_metric in error_metrics:\n",
        "      \n",
        "      df_error_metric = get_metric_with_mean(results[i], error_metric=error_metric)\n",
        "\n",
        "      #if error_metric != 'MAPE':\n",
        "      #  df_error_metric = calc_mean_to_max_error(df_error_metric, max_of_pretrain_per_country[i], max_cases_per_country[i])\n",
        "\n",
        "      country_error_score.append(df_error_metric)\n",
        "      display_countrywise_scores(countries[i],df_error_metric)\n",
        "\n",
        "      # Transposing the metrics while saving\n",
        "      save_metrics(df_error_metric, path=path, country=countries[i], static_learner=static_learner, transpose=transpose)\n",
        "      \n",
        "    countrywise_error_scores[countries[i]] = pd.concat(country_error_score,ignore_index=True)\n",
        "    \n",
        "  return countrywise_error_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e4uCM0f65tm"
      },
      "source": [
        "def calc_save_err_metric_combined(error_metrics, results, max_of_pretrain_days, max_selected_countries, path, static_learner, alternate_batch, transpose):\n",
        "  combined_err_metric = []\n",
        "  for error_metric in error_metrics:\n",
        "    df_error_metric = get_metric_with_mean(results, error_metric=error_metric)\n",
        "\n",
        "    #if error_metric != 'MAPE':\n",
        "    #  df_error_metric = calc_mean_to_max_error(df_error_metric, max_of_pretrain_days, max_selected_countries)\n",
        "\n",
        "    # Transposing the metrics while saving\n",
        "    save_metrics(df_error_metric, path=path, static_learner=static_learner, alternate_batch=alternate_batch, transpose=transpose)\n",
        "    \n",
        "    combined_err_metric.append(df_error_metric)\n",
        "  return (pd.concat(combined_err_metric, ignore_index=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7nnCdGu8QVD"
      },
      "source": [
        "def get_summary_table(df_result, df_runtime_result, error_metrics, static_learner=True):\n",
        "  sum_metric=[]\n",
        "  measure_col_name = 'Metric'\n",
        "  \n",
        "  # Setting start row and column for static and incremental learner\n",
        "  for metric in error_metrics:\n",
        "    start_row = 'mean'\n",
        "    if static_learner:\n",
        "      start_col='RandomForest'\n",
        "    else:\n",
        "      start_col='HT_Reg'\n",
        "\n",
        "    df_metric = get_metric_with_mean(df_result, metric)\n",
        "    df_row = pd.DataFrame([df_metric.loc[start_row][start_col:]])\n",
        "    \n",
        "    df_row[measure_col_name] = str(metric)    \n",
        "    sum_metric.append(df_row)\n",
        "\n",
        "  # Adding run time\n",
        "  df_runtime_row = pd.DataFrame([df_runtime_result.describe().loc[start_row][start_col:]])\n",
        "  df_runtime_row[measure_col_name]='Time(sec)'\n",
        "  sum_metric.append(df_runtime_row)\n",
        "\n",
        "  df_summary = pd.concat(sum_metric, ignore_index=True)\n",
        "  df_summary.set_index(measure_col_name, inplace=True)\n",
        "\n",
        "  return df_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLrQqgWXAPgi"
      },
      "source": [
        "def get_summary_table_countrywise(df_result_dict, error_metrics, static_learner=True):  #df_runtime_result,\n",
        "  summary_metric=[]\n",
        "  measure_col_name = f'Country({str(error_metrics[0])})'\n",
        "  eval_measure_col = 'EvaluationMeasurement'\n",
        "  start_row = 'mean'\n",
        "  if static_learner:\n",
        "    start_col='RandomForest'\n",
        "  else:\n",
        "    start_col='HT_Reg'\n",
        "\n",
        "  for country in df_result_dict.keys():\n",
        "    df_result = df_result_dict[country]\n",
        "\n",
        "    # Setting start row and column for static and incremental learner\n",
        "    for metric in error_metrics:      \n",
        "      df_metric = get_metric_with_mean(df_result, metric)\n",
        "      df_row = pd.DataFrame([df_metric.loc[start_row][start_col:]])\n",
        "      df_row[eval_measure_col] = metric\n",
        "      df_row[measure_col_name] = country\n",
        "      summary_metric.append(df_row)\n",
        "\n",
        "  df_summary = pd.concat(summary_metric, ignore_index=True)\n",
        "  df_summary.set_index(measure_col_name, inplace=True)\n",
        "\n",
        "  return df_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSX0nYK3SW2Q"
      },
      "source": [
        "def get_sum_table_combined_mean(countrywise_error_score_incremental,results_runtime, static_learner=False):\n",
        "  sum_table_combined_mean=[]\n",
        "  measure_col_name = 'Metric'\n",
        "  start_row = 'mean'\n",
        "  if static_learner:\n",
        "    start_col = 'RandomForest'\n",
        "  else:\n",
        "    start_col= 'HT_Reg'\n",
        "\n",
        "  for metric in error_metrics:\n",
        "    df_sum_cur_metric = get_summary_table_countrywise(countrywise_error_score_incremental, [metric], static_learner=static_learner)\n",
        "    df_row = pd.DataFrame([df_sum_cur_metric.describe().loc[start_row]])\n",
        "\n",
        "    df_row[measure_col_name] = metric\n",
        "    sum_table_combined_mean.append(df_row)\n",
        "\n",
        "  # Adding run time\n",
        "  df_runtime = pd.concat(results_runtime, ignore_index=True).describe().loc[start_row][start_col:]\n",
        "  df_runtime_row = pd.DataFrame([df_runtime])\n",
        "  df_runtime_row[measure_col_name]='Time(sec)'\n",
        "  sum_table_combined_mean.append(df_runtime_row)\n",
        "\n",
        "  # Concating results to one dataframe\n",
        "  sum_table_combined_mean = pd.concat(sum_table_combined_mean, ignore_index=True)\n",
        "  sum_table_combined_mean.set_index(measure_col_name, inplace=True)\n",
        "  return sum_table_combined_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AKhvSFwAzUj"
      },
      "source": [
        "def check_significance(target_pop, competitor_pop, significance_at: float):\n",
        "    \"\"\"\n",
        "    Comparing algorithms per batch or per country pairs (exp 2 or 1 respectively), \n",
        "      so for each pair, we compare the significance of the best algo to all of the the other algos.\n",
        "    Ttest performed if the distribution is normal, otherwise we perform a non-parametric test.\n",
        "    \"\"\"\n",
        "    model_pop, population = target_pop, competitor_pop  \n",
        "    \n",
        "    # Normality tests\n",
        "    if len(model_pop) >= 8:  # skew test not valid for smaller populations\n",
        "      value_mdl, p_mdl = normaltest(model_pop.values)\n",
        "      value_pop, p_pop = normaltest(population.values)\n",
        "      if (p_mdl >= 0.05) & (p_pop >= 0.05):\n",
        "          # print('It is likely that both populations are normal. Thus, running T-Test...')\n",
        "          tset, pval = stats.ttest_ind(model_pop, population)\n",
        "          if pval < significance_at:    # alpha value is 0.05 or 5%\n",
        "              significant = 'Significant (Ttest)'\n",
        "          else:\n",
        "              significant = 'Not Significant (Ttest)'\n",
        "      else:\n",
        "          # print('It is unlikely that the result is normal. Thus, running Wilcoxon test...')\n",
        "          if np.sum(np.subtract(list(model_pop), list(competitor_pop))) != 0.0:  # if values are identical the test will crash, but we now it's not significant\n",
        "              tset, pval = stats.wilcoxon(model_pop, population)\n",
        "              if pval < significance_at:    # alpha value is 0.05 or 5%\n",
        "                  significant = 'Significant (Wilcox Test)'\n",
        "              else:\n",
        "                  significant = 'Not Significant (Wilcox Test)'\n",
        "          else:\n",
        "              # print('Warning: results are identical')\n",
        "              tset, pval = stats.ttest_ind(model_pop, population)\n",
        "              significant = 'Not Significant (Wilcox Test)'\n",
        "    else:\n",
        "      print('Population too small.')\n",
        "      if np.sum(np.subtract(list(model_pop), list(competitor_pop))) != 0.0:  # if values are identical the test will crash, but we now it's not significant\n",
        "          tset, pval = stats.wilcoxon(model_pop, population)\n",
        "          if pval < significance_at:    # alpha value is 0.05 or 5%\n",
        "              significant = 'Significant (Wilcox Test)'\n",
        "          else:\n",
        "              significant = 'Not Significant (Wilcox Test)'\n",
        "    return pval, significant "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuW-RWNMWGE5"
      },
      "source": [
        "def unit_incremental_df(country_name, evaluator, date, milestone):  # Added Now\r\n",
        "    frame = {}\r\n",
        "    frame['date'] = date\r\n",
        "    frame['Country'] = [country_name] * len(date)\r\n",
        "    frame['Milestone'] = [milestone] * len(date)\r\n",
        "    frame['y_true'] = evaluator.mean_eval_measurements[0].y_true_vector\r\n",
        "    for i in range(len(evaluator.model_names)):\r\n",
        "        frame[f'pred_{evaluator.model_names[i]}'] = evaluator.mean_eval_measurements[i].y_pred_vector\r\n",
        "    return pd.DataFrame(frame)\r\n",
        "\r\n",
        "\r\n",
        "def unit_static_df(country_name, date, y_true,  milestone, model_predictions):\r\n",
        "    frame = {}\r\n",
        "    frame['date'] = date\r\n",
        "    frame['Country'] = [country_name] * len(date)\r\n",
        "    frame['Milestone'] = [milestone] * len(date)\r\n",
        "    frame['y_true'] = y_true\r\n",
        "    for algo, y_pred in model_predictions.items():\r\n",
        "        frame[f'pred_{algo}'] = y_pred\r\n",
        "\r\n",
        "    return pd.DataFrame(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZYx2PYrRBMV"
      },
      "source": [
        "## Combining Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g11ZDONMQvHT"
      },
      "source": [
        "def sortby_date_and_set_index(df):\n",
        "  df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y')\n",
        "  df.sort_values('date', inplace=True)\n",
        "  df.set_index('date', inplace=True)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZr-9CCDwIVo"
      },
      "source": [
        "def get_dataset_with_target(countries, df_grouped):\n",
        "  \n",
        "  # Empty list to store Dataframes of each country\n",
        "  frames = []\n",
        "  \n",
        "  for country in countries:\n",
        "    \n",
        "    df = df_grouped.get_group(country)\n",
        "\n",
        "    # Creating feature 'day_no'\n",
        "    df['day_no']= pd.Series([i for i in range(1,len(df)+1)], index=df.index)\n",
        "\n",
        "    # Reordering features\n",
        "    df = df[['day_no', 'country','cases']]\n",
        "\n",
        "    # Adding features through lags\n",
        "    df = create_features_with_lags(df)\n",
        "\n",
        "    # Creating target with last 10 days cases\n",
        "    df['target'] = df.iloc[:,[2]+[i*-1 for i in range(1,10)]].mean(axis=1)\n",
        "\n",
        "    # Dropping null columns\n",
        "    df.dropna(how='any', axis=0, inplace=True)\n",
        "\n",
        "    # Dropping mid columns\n",
        "    drop_columns = list(df.loc[:,'cases_t-38':'cases_t-1'].columns)\n",
        "    df.drop(drop_columns, axis=1, inplace=True)\n",
        "\n",
        "    frames.append(df)\n",
        "\n",
        "  return (pd.concat(frames, ignore_index=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sit5iHT_0UWl"
      },
      "source": [
        "def reshape_dataframe(*data: np.ndarray):\n",
        "    # This function adds an extra dimension which is necessary in the LSTM\n",
        "    arr = []\n",
        "    for d in data:\n",
        "        arr.append(np.reshape(np.array(d), (d.shape[0], 1, d.shape[1])))\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1buCiuYoliN"
      },
      "source": [
        "def get_countries_sortedby_cases(valid_countries, df_grouped):\n",
        "  # A dictionary of all countries\n",
        "  dict_countries = Counter(valid_countries)\n",
        "\n",
        "  for country in dict_countries.keys():\n",
        "    dict_countries[country] = df_grouped.get_group(country)['cases'].sum()\n",
        "\n",
        "  # Sorting countries based on number of cases\n",
        "  countries_sortedby_cases = sorted(dict_countries.items(), key=lambda dict_countries: dict_countries[1], reverse=True)\n",
        "\n",
        "  # Creating dataframe \n",
        "  df_countries_sortedbycases = pd.DataFrame.from_dict(dict(countries_sortedby_cases), orient='index', columns=['Total Cases'])\n",
        "  \n",
        "  return df_countries_sortedbycases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvuRHEVArNJ0"
      },
      "source": [
        "# Getting a list of valid countries\n",
        "def get_countries_with_valid_size(df):\n",
        "  total_countries = list(df_grouped.groups.keys())\n",
        "\n",
        "  # A list for countries with required datasize\n",
        "  valid_countries = []\n",
        "\n",
        "  # List of countries with more than 230 records. Because, max training size = 150, lags removed = 50, prediction = 30.\n",
        "  for country in total_countries:\n",
        "    if len(df_grouped.get_group(country)) >= 230:\n",
        "      valid_countries.append(country)\n",
        "\n",
        "  return valid_countries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tqHxnYhwsbk"
      },
      "source": [
        "def preprocess_dataset(df):  \n",
        "  # Selecting required features\n",
        "  df= df[['dateRep','cases','countriesAndTerritories']]\n",
        "\n",
        "  # Rename features\n",
        "  df.rename(columns={'countriesAndTerritories':'country', 'dateRep':'date'}, inplace=True)\n",
        "\n",
        "  # Convert to date, sort and set index\n",
        "  df = sortby_date_and_set_index(df)\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6nW5zkHuFLn"
      },
      "source": [
        "# Calculating maximum of dataframe for every pretrain size\n",
        "def calc_max_of_pretrain_days(pretrain_days,df)->list:\n",
        "  max_of_pretrain_days = []\n",
        "  \n",
        "  for day in pretrain_days:\n",
        "    df_subset = create_subset(df,day)\n",
        "    max_of_pretrain_days.append(df_subset['cases'].max())\n",
        "  \n",
        "  return max_of_pretrain_days"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze3Ju2mOFltA"
      },
      "source": [
        "def display_scores(results):\n",
        "  #print(f'_________________________________{country}____________________________________________')\n",
        "  df_MAE = get_metric_with_mean(results,'MAE' )\n",
        "  df_RMSE = get_metric_with_mean(results,'RMSE')\n",
        "  df_MAPE = get_metric_with_mean(results,'MAPE')\n",
        "  print('MAE Score')\n",
        "  print(df_MAE.to_string())\n",
        "  print('-----------------------------------------------------------------------------------')\n",
        "  print('RMSE Score')\n",
        "  print(df_RMSE.to_string())\n",
        "  print('-----------------------------------------------------------------------------------')\n",
        "  print('MAPE Score')\n",
        "  print(df_MAPE.to_string())\n",
        "  print('\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZbb27ZOpQmT"
      },
      "source": [
        "## Alternate Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SKzw1NdWenN"
      },
      "source": [
        "def get_alternate_batch_records_idx(batch_size,total_records): \n",
        "  total_batches = total_records//batch_size\n",
        "  current_batch=1\n",
        "  start_idx = 0\n",
        "  end_idx = batch_size\n",
        "  idx_list = []\n",
        "  \n",
        "  while current_batch <= total_batches:\n",
        "    if current_batch%2!=0:\n",
        "      idx_list.extend([x for x in range(start_idx,end_idx)])\n",
        "      start_idx = idx_list[-1]+(batch_size+1)\n",
        "      end_idx = start_idx + batch_size\n",
        "    current_batch += 1\n",
        "\n",
        "  return idx_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1Sdy4o7oufO"
      },
      "source": [
        "def create_alternate_batch_subset(df,days,batch_size):\n",
        "  df_grouped = df.groupby('country')\n",
        "  countries = df['country'].unique()\n",
        "  frame1,frame2 = [],[]\n",
        "\n",
        "  for country in countries:\n",
        "    df_cur_country = df_grouped.get_group(country)\n",
        "\n",
        "    df1 = df_cur_country.iloc[0:days//2]\n",
        "    df2 = df_cur_country.iloc[days:days+30]  # Adding 30 for a testing batch that is one month ahead\n",
        "    \n",
        "    # Selecting alternate batches\n",
        "    idx = get_alternate_batch_records_idx(batch_size,total_records=len(df2))\n",
        "    df2 = df2.iloc[idx]\n",
        "\n",
        "    # Appending dataframes\n",
        "    frame1.append(df1)\n",
        "    frame2.append(df2)\n",
        "\n",
        "  r1 = pd.concat(frame1, ignore_index=True)\n",
        "  r2 = pd.concat(frame2, ignore_index=True)\n",
        "  r = r1.append(r2, ignore_index=True)\n",
        "  \n",
        "  return (r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPViJY9uDZ0q"
      },
      "source": [
        "## Incremental Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9DZn8PjUter"
      },
      "source": [
        "def instantiate_regressors():\n",
        "  ht_reg = HoeffdingTreeRegressor()\n",
        "  hat_reg = HoeffdingAdaptiveTreeRegressor()\n",
        "  arf_reg = AdaptiveRandomForestRegressor()\n",
        "  pa_reg = PassiveAggressiveRegressor(max_iter=1, random_state=0, tol=1e-3)\n",
        "\n",
        "  model = [ht_reg, hat_reg, arf_reg, pa_reg]\n",
        "  model_names = ['HT_Reg', 'HAT_Reg', 'ARF_Reg', 'PA_Reg']\n",
        "\n",
        "  return model, model_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NwhtdOBXBUy"
      },
      "source": [
        "def get_error_scores_per_model(evaluator, mdl_evaluation_scores)-> pd.DataFrame:\n",
        "  \n",
        "  for i in range(len(evaluator.model_names)):\n",
        "    # Desired error metrics\n",
        "    mse = evaluator.mean_eval_measurements[i].get_mean_square_error()\n",
        "    mae = evaluator.mean_eval_measurements[i].get_average_error()\n",
        "    mape = evaluator.mean_eval_measurements[i].get_mean_absolute_percentage_error()\n",
        "    rmse = sqrt(mse)\n",
        "\n",
        "    # Dictionary of errors per model\n",
        "    mdl_evaluation_scores[str(evaluator.model_names[i])] = [rmse, mae, mape]\n",
        "\n",
        "  return(pd.DataFrame(mdl_evaluation_scores))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSq_iuypN2BE"
      },
      "source": [
        "def get_running_time_per_model_incremental_learner(evaluator,day):\n",
        "    cols = ['PretrainDays']  # Adding pretrain as first column\n",
        "    cols += evaluator.model_names  # Adding remaining columns of different algorithm\n",
        "    running_time = []\n",
        "    running_time.append(day)\n",
        "    for i in range(len(evaluator.model_names)):\n",
        "        running_time.append(evaluator.running_time_measurements[i]._total_time)\n",
        "\n",
        "    return (pd.DataFrame([running_time],columns=cols))  # Passing running_time as a list of list to insert it as a row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VbOOY4WfUzT"
      },
      "source": [
        "def display_countrywise_scores(country,df_error_metric):\n",
        "  print(f'_________________________________{country}____________________________________________')\n",
        "  print(df_error_metric.to_string())\n",
        "  print('\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC8vD-Dl3uNs"
      },
      "source": [
        "# Create a dataframe of all countries with pre-train size = pretrain days and test&train size = pretrain days\n",
        "def create_subset(result,days):\n",
        "  result_grouped = result.groupby('country')\n",
        "  countries = result['country'].unique()\n",
        "  frame1,frame2 = [],[]\n",
        "  for country in countries:\n",
        "    df = result_grouped.get_group(country) \n",
        "    df1 = df.iloc[0:days]\n",
        "    df2 = df.iloc[days:days+30]\n",
        "    frame1.append(df1)\n",
        "    frame2.append(df2)\n",
        "\n",
        "  r1 = pd.concat(frame1, ignore_index=True)\n",
        "  r2 = pd.concat(frame2, ignore_index=True)\n",
        "  r = r1.append(r2, ignore_index=True)\n",
        "  \n",
        "  return (r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGzbZRfVDc-c"
      },
      "source": [
        "## Static Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKVRq7O9onX7"
      },
      "source": [
        "def mean_absolute_percentage_error(actual, predicted):\n",
        "    \"\"\"\n",
        "    Mean absolute percentage error (MAPE).\n",
        "    :return error\n",
        "    \"\"\"\n",
        "    actual =  np.array(actual) \n",
        "    predicted = np.array(predicted) \n",
        "\n",
        "    mask = actual != 0\n",
        "    return (np.fabs(actual - predicted) / np.fabs(actual))[mask].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olpCsifni4p-"
      },
      "source": [
        "def get_scores(y_true, model_predictions, days):\n",
        "    mdl_evaluation_scores = {}\n",
        "    mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE', 'MAE', 'MAPE']\n",
        "    mdl_evaluation_scores['PretrainDays'] = [days] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "\n",
        "    for model in model_predictions:\n",
        "        y_pred = model_predictions[model]\n",
        "        if model == 'LSTM':\n",
        "            rmse = mean_squared_error(y_true[:, np.newaxis], y_pred, squared=False)\n",
        "            mae = mean_absolute_error(y_true[:, np.newaxis], y_pred)\n",
        "            mape = mean_absolute_percentage_error(y_true[:, np.newaxis], y_pred)\n",
        "        else:\n",
        "            rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "            mae = mean_absolute_error(y_true, y_pred)\n",
        "            mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "\n",
        "        mdl_evaluation_scores[model] = [rmse, mae, mape]\n",
        "    return pd.DataFrame(mdl_evaluation_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBKHhy5I0sWy"
      },
      "source": [
        "def get_running_time_per_model_static_learner(model_predictions,total_execution_time):\n",
        "    cols = ['PretrainDays']\n",
        "    cols += model_predictions.keys()\n",
        "    return pd.DataFrame(total_execution_time, columns=cols)\n",
        "\n",
        "\n",
        "def measure(wrapped_func):\n",
        "    @wraps(wrapped_func)\n",
        "    def _time_it(*args, **kwargs):\n",
        "        start = pc_timer()\n",
        "        try:\n",
        "            model_predictions = wrapped_func(*args, **kwargs)\n",
        "        finally:\n",
        "            end_ = pc_timer() - start\n",
        "            return model_predictions, end_\n",
        "    return _time_it\n",
        "\n",
        "\n",
        "@measure\n",
        "def train_test_model(regressor, X_train, y_train, X_test):\n",
        "    regressor.fit(X_train, y_train)\n",
        "    return regressor.predict(X_test)\n",
        "\n",
        "\n",
        "@measure\n",
        "def train_test_lstm(regressor, X_train_lstm, y_train, X_val_lstm, y_val, X_test_lstm, patience, epochs, batch_size_lstm):\n",
        "    regressor.compile(loss='mae', optimizer='adagrad', metrics=['mse', 'mae'])\n",
        "\n",
        "    history = regressor.fit(\n",
        "        X_train_lstm,\n",
        "        y_train,\n",
        "        validation_data=(X_val_lstm, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size_lstm,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss',\n",
        "                                 mode='min',\n",
        "                                 patience=patience)])\n",
        "\n",
        "    return regressor.predict(X_test_lstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB-GhumBoRRH"
      },
      "source": [
        "def define_lstm_model(x_train_lstm, layers, activations, patience):\n",
        "    # Start defining the model\n",
        "    input_shape = x_train_lstm.shape\n",
        "\n",
        "    # Definining model first with LSTM n layers\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(layers[0], input_shape=input_shape[1:], activation=activations[0], return_sequences=True))\n",
        "\n",
        "    # Adding middle layers\n",
        "    for l in range(1, len(layers)-1):\n",
        "      model.add(LSTM(layers[l], activation=activations[l], return_sequences=True))\n",
        "      model.add(Dropout(0.2))\n",
        "\n",
        "    # Add last Dense and LSTMs layers\n",
        "    if len(layers) > 1:\n",
        "      model.add(Dense(layers[-1], activation=activations[-1]))\n",
        "      model.add(Dropout(0.2))\n",
        "      model.add(LSTM(layers[-1], activation=activations[-1]))\n",
        "\n",
        "    model.add(Dense(1))  # output layer. Since we have only 1 output value\n",
        "    # End defining model\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjnk6w9_cqG6"
      },
      "source": [
        "def normalize_dataset(*dataframes):\n",
        "    arr = []\n",
        "    for df in dataframes:\n",
        "        arr.append(StandardScaler().fit_transform(df))\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GQGxLLNw1z5"
      },
      "source": [
        "def get_validation_set(df_train, batch_size=10):\n",
        "    lst_idx = -1\n",
        "    total_batches = len(df_train) // batch_size\n",
        "    train_set, val_set = [], []\n",
        "\n",
        "    for cur_batch in range(total_batches):\n",
        "        start = lst_idx + 1\n",
        "        end = start + batch_size\n",
        "        if cur_batch % 2 == 0:\n",
        "            train_set.append(df_train.iloc[start:end])\n",
        "        else:\n",
        "            val_set.append(df_train.iloc[start:end])\n",
        "\n",
        "        lst_idx = end - 1  # adjusting last index because we add 1 in starting\n",
        "\n",
        "    return pd.concat(train_set, ignore_index=True), pd.concat(val_set, ignore_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMyZ5jcy_m6j"
      },
      "source": [
        "# Experiment 1\n",
        "Training and testing with five countries individually. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDnpy-TycyY9"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "* cases(t): Number of cases on current day(Column='cases') \n",
        "\n",
        "* cases(t-n): Number of cases 'n' days before current day 't'\n",
        "\n",
        "* 30 day gap: Training from day number t-89 to t-39(50 days). Then a gap of 30 days and then creating target by averaging t to t-9(10 Days).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "SeJJB0sdNnIq",
        "outputId": "608f4cdc-44eb-4db3-d763-aee4d6f985af"
      },
      "source": [
        "# Sample set for understanding dataset\n",
        "sample_df = pd.read_csv(f'{csv_processed_path}/United_States_of_America.csv')\n",
        "sample_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>day_no</th>\n",
              "      <th>country</th>\n",
              "      <th>cases</th>\n",
              "      <th>cases_t-89</th>\n",
              "      <th>cases_t-88</th>\n",
              "      <th>cases_t-87</th>\n",
              "      <th>cases_t-86</th>\n",
              "      <th>cases_t-85</th>\n",
              "      <th>cases_t-84</th>\n",
              "      <th>cases_t-83</th>\n",
              "      <th>cases_t-82</th>\n",
              "      <th>cases_t-81</th>\n",
              "      <th>cases_t-80</th>\n",
              "      <th>cases_t-79</th>\n",
              "      <th>cases_t-78</th>\n",
              "      <th>cases_t-77</th>\n",
              "      <th>cases_t-76</th>\n",
              "      <th>cases_t-75</th>\n",
              "      <th>cases_t-74</th>\n",
              "      <th>cases_t-73</th>\n",
              "      <th>cases_t-72</th>\n",
              "      <th>cases_t-71</th>\n",
              "      <th>cases_t-70</th>\n",
              "      <th>cases_t-69</th>\n",
              "      <th>cases_t-68</th>\n",
              "      <th>cases_t-67</th>\n",
              "      <th>cases_t-66</th>\n",
              "      <th>cases_t-65</th>\n",
              "      <th>cases_t-64</th>\n",
              "      <th>cases_t-63</th>\n",
              "      <th>cases_t-62</th>\n",
              "      <th>cases_t-61</th>\n",
              "      <th>cases_t-60</th>\n",
              "      <th>cases_t-59</th>\n",
              "      <th>cases_t-58</th>\n",
              "      <th>cases_t-57</th>\n",
              "      <th>cases_t-56</th>\n",
              "      <th>cases_t-55</th>\n",
              "      <th>cases_t-54</th>\n",
              "      <th>cases_t-53</th>\n",
              "      <th>cases_t-52</th>\n",
              "      <th>cases_t-51</th>\n",
              "      <th>cases_t-50</th>\n",
              "      <th>cases_t-49</th>\n",
              "      <th>cases_t-48</th>\n",
              "      <th>cases_t-47</th>\n",
              "      <th>cases_t-46</th>\n",
              "      <th>cases_t-45</th>\n",
              "      <th>cases_t-44</th>\n",
              "      <th>cases_t-43</th>\n",
              "      <th>cases_t-42</th>\n",
              "      <th>cases_t-41</th>\n",
              "      <th>cases_t-40</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>90</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>19979</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>11525.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-30</td>\n",
              "      <td>91</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>18360</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>12877.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-03-31</td>\n",
              "      <td>92</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>21595</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>14499.600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-04-01</td>\n",
              "      <td>93</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>24998</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>16287.100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-04-02</td>\n",
              "      <td>94</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>27103</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>19.000</td>\n",
              "      <td>18151.500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         date  day_no  ... cases_t-40    target\n",
              "0  2020-03-29      90  ...      0.000 11525.000\n",
              "1  2020-03-30      91  ...      0.000 12877.500\n",
              "2  2020-03-31      92  ...      0.000 14499.600\n",
              "3  2020-04-01      93  ...      1.000 16287.100\n",
              "4  2020-04-02      94  ...     19.000 18151.500\n",
              "\n",
              "[5 rows x 55 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJiGVxO_hsNv"
      },
      "source": [
        "## Incremental Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZrg_v5CLReb"
      },
      "source": [
        "def reset_evaluator(evaluator):  # Added Now\r\n",
        "  for j in range(evaluator.n_models):\r\n",
        "      evaluator.mean_eval_measurements[j].reset()\r\n",
        "      evaluator.current_eval_measurements[j].reset()\r\n",
        "  return evaluator\r\n",
        "\r\n",
        "\r\n",
        "def update_incremental_metrics(evaluator, y, prediction):  # Added Now\r\n",
        "  for j in range(evaluator.n_models):\r\n",
        "    for i in range(len(prediction[0])):\r\n",
        "      evaluator.mean_eval_measurements[j].add_result(y[i], prediction[j][i])\r\n",
        "      evaluator.current_eval_measurements[j].add_result(y[i], prediction[j][i])\r\n",
        "\r\n",
        "    # Adding result manually causes y_true_vector to have a objects inserted like array([123.45]) in a list.\r\n",
        "    # For calculating metrics we have to convert them into flat list.\r\n",
        "    evaluator.mean_eval_measurements[j].y_true_vector = np.array(evaluator.mean_eval_measurements[j].y_true_vector).flatten().tolist()\r\n",
        "    evaluator.current_eval_measurements[j].y_true_vector = np.array(evaluator.current_eval_measurements[j].y_true_vector).flatten().tolist()\r\n",
        "  return evaluator\r\n",
        "\r\n",
        "\r\n",
        "def get_error_scores_per_model(evaluator, mdl_evaluation_scores) -> pd.DataFrame:\r\n",
        "  for i in range(len(evaluator.model_names)):\r\n",
        "    # Desired error metrics\r\n",
        "    mse = evaluator.mean_eval_measurements[i].get_mean_square_error()\r\n",
        "    mae = evaluator.mean_eval_measurements[i].get_average_error()\r\n",
        "    mae = mae[0]  # get_average_error() is returning a List instead of single value.  # Updated Now\r\n",
        "    mape = evaluator.mean_eval_measurements[i].get_mean_absolute_percentage_error()\r\n",
        "    rmse = sqrt(mse)\r\n",
        "\r\n",
        "    # Dictionary of errors per model\r\n",
        "    mdl_evaluation_scores[str(evaluator.model_names[i])] = [rmse, mae, mape]\r\n",
        "  return (pd.DataFrame(mdl_evaluation_scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "pw3R1IKXR05k",
        "outputId": "1507f74e-f4d2-44e0-be25-f37891fb61aa"
      },
      "source": [
        "\"\"\"\r\n",
        "# Old\r\n",
        "\r\n",
        "def scikit_multiflow(df, pretrain_days):\r\n",
        "\r\n",
        "  # Creating a stream from dataframe\r\n",
        "  stream = DataStream(np.array(df.iloc[:,4:-1]), y=np.array(df.iloc[:,-1])) # Selecting features x=[t-89:t-39] and y=[target]. TODO: Drop columns with name \r\n",
        "\r\n",
        "  model, model_names = instantiate_regressors()\r\n",
        "\r\n",
        "  frames, running_time_frames = [], []\r\n",
        "\r\n",
        "  # Setup the evaluator\r\n",
        "  for day in pretrain_days:\r\n",
        "\r\n",
        "      pretrain_days = day\r\n",
        "      max_samples = pretrain_days + 30  #Testing on set one month ahead only\r\n",
        "\r\n",
        "      '''evaluator = EvaluatePrequential(show_plot=False,\r\n",
        "                                    pretrain_size=day,\r\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'], \r\n",
        "                                    max_samples=max_samples)'''\r\n",
        "      \r\n",
        "      evaluator = EvaluatePrequential(show_plot=False,\r\n",
        "                                    pretrain_size=day-1,\r\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'], \r\n",
        "                                    max_samples=day)\r\n",
        "\r\n",
        "\r\n",
        "      # Run evaluation\r\n",
        "      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\r\n",
        "\r\n",
        "      # Dictionary to store each iteration error scores\r\n",
        "      mdl_evaluation_scores = {}\r\n",
        "\r\n",
        "      # Adding Evaluation Measurements and pretraining days\r\n",
        "      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE']\r\n",
        "      mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\r\n",
        "      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\r\n",
        "\r\n",
        "      # Errors of each model on a specific pre-train days\r\n",
        "      frames.append(mdl_evaluation_df)\r\n",
        "\r\n",
        "      # Run time for each algorithm\r\n",
        "      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\r\n",
        "\r\n",
        "  # Final Run Time DataFrame\r\n",
        "  running_time_df = pd.concat(running_time_frames,ignore_index=True)\r\n",
        "\r\n",
        "  # Final Evaluation Score Dataframe\r\n",
        "  evaluation_scores_df = pd.concat(frames, ignore_index=True)\r\n",
        "  return evaluation_scores_df, running_time_df\r\n",
        "\r\n",
        "for country in countries:\r\n",
        "  # Read each country  \r\n",
        "  df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\r\n",
        "\r\n",
        "  # Get evaluation scores and running time for country\r\n",
        "  evaluation_scores_df, running_time_df = scikit_multiflow(df_country,pretrain_days)\r\n",
        "\r\n",
        "  # Appending evaluation scores and runtime for each country\r\n",
        "  results_incremental.append(evaluation_scores_df)\r\n",
        "  results_runtime_incremental.append(running_time_df)\r\n",
        "\r\n",
        "  # Get max of each pretrain subset and for each country dataset\r\n",
        "  max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days,df_country))\r\n",
        "  max_cases_per_country.append(df_country['cases'].max())\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Old\\n\\ndef scikit_multiflow(df, pretrain_days):\\n\\n  # Creating a stream from dataframe\\n  stream = DataStream(np.array(df.iloc[:,4:-1]), y=np.array(df.iloc[:,-1])) # Selecting features x=[t-89:t-39] and y=[target]. TODO: Drop columns with name \\n\\n  model, model_names = instantiate_regressors()\\n\\n  frames, running_time_frames = [], []\\n\\n  # Setup the evaluator\\n  for day in pretrain_days:\\n\\n      pretrain_days = day\\n      max_samples = pretrain_days + 30  #Testing on set one month ahead only\\n\\n      '''evaluator = EvaluatePrequential(show_plot=False,\\n                                    pretrain_size=day,\\n                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'], \\n                                    max_samples=max_samples)'''\\n      \\n      evaluator = EvaluatePrequential(show_plot=False,\\n                                    pretrain_size=day-1,\\n                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'], \\n                                    max_samples=day)\\n\\n\\n      # Run evaluation\\n      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\\n\\n      # Dictionary to store each iteration error scores\\n      mdl_evaluation_scores = {}\\n\\n      # Adding Evaluation Measurements and pretraining days\\n      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE']\\n      mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\\n      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\\n\\n      # Errors of each model on a specific pre-train days\\n      frames.append(mdl_evaluation_df)\\n\\n      # Run time for each algorithm\\n      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\\n\\n  # Final Run Time DataFrame\\n  running_time_df = pd.concat(running_time_frames,ignore_index=True)\\n\\n  # Final Evaluation Score Dataframe\\n  evaluation_scores_df = pd.concat(frames, ignore_index=True)\\n  return evaluation_scores_df, running_time_df\\n\\nfor country in countries:\\n  # Read each country  \\n  df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\\n\\n  # Get evaluation scores and running time for country\\n  evaluation_scores_df, running_time_df = scikit_multiflow(df_country,pretrain_days)\\n\\n  # Appending evaluation scores and runtime for each country\\n  results_incremental.append(evaluation_scores_df)\\n  results_runtime_incremental.append(running_time_df)\\n\\n  # Get max of each pretrain subset and for each country dataset\\n  max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days,df_country))\\n  max_cases_per_country.append(df_country['cases'].max())\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ3htjYaL-1o"
      },
      "source": [
        "def scikit_multiflow(df, pretrain_days, country):  # Added Country in parameter\r\n",
        "    # Creating a stream from dataframe\r\n",
        "    stream = DataStream(np.array(df.iloc[:, 4:-1]), y=np.array(\r\n",
        "        df.iloc[:, -1]))  # Selecting features x=[t-89:t-39] and y=[target].\r\n",
        "\r\n",
        "    model, model_names = instantiate_regressors()\r\n",
        "\r\n",
        "    frames, running_time_frames = [], []\r\n",
        "\r\n",
        "    united_dataframe = []  # Added Now\r\n",
        "\r\n",
        "    # Setup the evaluator\r\n",
        "    for day in pretrain_days:\r\n",
        "        pretrain_days = day\r\n",
        "        # max_samples = pretrain_days + 30  # Training and then testing on set one month ahead only\r\n",
        "        max_samples = pretrain_days + 1\r\n",
        "        testing_samples_size = 30\r\n",
        "\r\n",
        "        evaluator = EvaluatePrequential(show_plot=False,\r\n",
        "                                        pretrain_size=pretrain_days,\r\n",
        "                                        metrics=['mean_square_error', 'mean_absolute_error',\r\n",
        "                                                 'mean_absolute_percentage_error'],\r\n",
        "                                        max_samples=max_samples)\r\n",
        "\r\n",
        "        # Run evaluation\r\n",
        "        evaluator.evaluate(stream=stream, model=model, model_names=model_names)\r\n",
        "\r\n",
        "        X = stream.X[pretrain_days: pretrain_days + testing_samples_size]\r\n",
        "        y = stream.y[pretrain_days: pretrain_days + testing_samples_size]\r\n",
        "        target_dates = df.iloc[pretrain_days: pretrain_days +testing_samples_size, 0]  # Added Now\r\n",
        "\r\n",
        "        prediction = evaluator.predict(X)\r\n",
        "\r\n",
        "        # Since we add one extra sample, reset the evaluator\r\n",
        "        evaluator = reset_evaluator(evaluator)\r\n",
        "\r\n",
        "        evaluator = update_incremental_metrics(evaluator, y, prediction)\r\n",
        "\r\n",
        "        united_dataframe.append(unit_incremental_df(country, evaluator, target_dates, pretrain_days))  # Added now\r\n",
        "\r\n",
        "        # Dictionary to store each iteration error scores\r\n",
        "        mdl_evaluation_scores = {}\r\n",
        "\r\n",
        "        # Adding Evaluation Measurements and pretraining days\r\n",
        "        mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE', 'MAE', 'MAPE']\r\n",
        "        mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\r\n",
        "        mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\r\n",
        "\r\n",
        "        # Errors of each model on a specific pre-train days\r\n",
        "        frames.append(mdl_evaluation_df)\r\n",
        "\r\n",
        "        # Run time for each algorithm\r\n",
        "        running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\r\n",
        "\r\n",
        "    # Final Run Time DataFrame\r\n",
        "    running_time_df = pd.concat(running_time_frames, ignore_index=True)\r\n",
        "\r\n",
        "    # Final Evaluation Score Dataframe\r\n",
        "    evaluation_scores_df = pd.concat(frames, ignore_index=True)\r\n",
        "\r\n",
        "    united_dataframe = pd.concat(united_dataframe, ignore_index=True)  # Added Now\r\n",
        "    return evaluation_scores_df, running_time_df, united_dataframe  # Added united_dataframe in return statement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUw_30g9T4fS",
        "outputId": "7d3f1206-e048-4f2c-d533-943a2772dbf7"
      },
      "source": [
        "# Training all countries\n",
        "results_incremental = []\n",
        "results_runtime_incremental = []\n",
        "max_of_pretrain_per_country = []\n",
        "max_cases_per_country = []\n",
        "\n",
        "for country in countries:\n",
        "    # Read each country\n",
        "    df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\n",
        "\n",
        "    # Get evaluation scores and running time for country\n",
        "    evaluation_scores_df, running_time_df, united_dataframe = scikit_multiflow(df_country,pretrain_days, country)\n",
        "\n",
        "    save_united_df(united_dataframe, exp1_inc_united_df_path, country=country)\n",
        "\n",
        "    # Appending evaluation scores and runtime for each country\n",
        "    results_incremental.append(evaluation_scores_df)\n",
        "\n",
        "    results_runtime_incremental.append(running_time_df)\n",
        "\n",
        "    # Get max of each pretrain subset and for each country dataset\n",
        "    max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days,df_country))\n",
        "    max_cases_per_country.append(df_country['cases'].max())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.66s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 26846153.3900\n",
            "HT_Reg - MAPE          : 0.1810\n",
            "HT_Reg - MAE          : 5181.327377\n",
            "HAT_Reg - MSE          : 5909054.9340\n",
            "HAT_Reg - MAPE          : 0.0849\n",
            "HAT_Reg - MAE          : 2430.854774\n",
            "ARF_Reg - MSE          : 49955487.8689\n",
            "ARF_Reg - MAPE          : 0.2469\n",
            "ARF_Reg - MAE          : 7067.919628\n",
            "PA_Reg - MSE          : 116920245843.2590\n",
            "PA_Reg - MAPE          : 11.9438\n",
            "PA_Reg - MAE          : 341936.025951\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.16s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1235710712.3051\n",
            "HT_Reg - MAPE          : 1.6490\n",
            "HT_Reg - MAE          : 35152.677171\n",
            "HAT_Reg - MSE          : 1206184359.9385\n",
            "HAT_Reg - MAPE          : 1.6292\n",
            "HAT_Reg - MAE          : 34730.164986\n",
            "ARF_Reg - MSE          : 14785427.1793\n",
            "ARF_Reg - MAPE          : 0.1804\n",
            "ARF_Reg - MAE          : 3845.182334\n",
            "PA_Reg - MSE          : 10016903936937.6016\n",
            "PA_Reg - MAPE          : 148.4665\n",
            "PA_Reg - MAE          : 3164949.278731\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.88s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 4467646.4306\n",
            "HT_Reg - MAPE          : 0.0640\n",
            "HT_Reg - MAE          : 2113.680778\n",
            "HAT_Reg - MSE          : 4467646.4716\n",
            "HAT_Reg - MAPE          : 0.0640\n",
            "HAT_Reg - MAE          : 2113.680788\n",
            "ARF_Reg - MSE          : 131747627.8088\n",
            "ARF_Reg - MAPE          : 0.3477\n",
            "ARF_Reg - MAE          : 11478.136949\n",
            "PA_Reg - MSE          : 5823844781569.0518\n",
            "PA_Reg - MAPE          : 73.1057\n",
            "PA_Reg - MAE          : 2413264.341420\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.13s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 466570325.4190\n",
            "HT_Reg - MAPE          : 0.3284\n",
            "HT_Reg - MAE          : 21600.239013\n",
            "HAT_Reg - MSE          : 466570325.4210\n",
            "HAT_Reg - MAPE          : 0.3284\n",
            "HAT_Reg - MAE          : 21600.239013\n",
            "ARF_Reg - MSE          : 447571301.0660\n",
            "ARF_Reg - MAPE          : 0.3216\n",
            "ARF_Reg - MAE          : 21155.881004\n",
            "PA_Reg - MSE          : 23272453574946.4727\n",
            "PA_Reg - MAPE          : 73.3378\n",
            "PA_Reg - MAE          : 4824153.145884\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.56s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 110984742.2613\n",
            "HT_Reg - MAPE          : 0.2521\n",
            "HT_Reg - MAE          : 10534.929628\n",
            "HAT_Reg - MSE          : 110984742.2613\n",
            "HAT_Reg - MAPE          : 0.2521\n",
            "HAT_Reg - MAE          : 10534.929628\n",
            "ARF_Reg - MSE          : 49624486.7229\n",
            "ARF_Reg - MAPE          : 0.1686\n",
            "ARF_Reg - MAE          : 7044.464971\n",
            "PA_Reg - MSE          : 138922480273135.6875\n",
            "PA_Reg - MAPE          : 282.0670\n",
            "PA_Reg - MAE          : 11786538.095350\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.24s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 7973.3554\n",
            "HT_Reg - MAPE          : 0.0021\n",
            "HT_Reg - MAE          : 89.293647\n",
            "HAT_Reg - MSE          : 7973.3554\n",
            "HAT_Reg - MAPE          : 0.0021\n",
            "HAT_Reg - MAE          : 89.293647\n",
            "ARF_Reg - MSE          : 1988342974.6039\n",
            "ARF_Reg - MAPE          : 1.0527\n",
            "ARF_Reg - MAE          : 44590.839582\n",
            "PA_Reg - MSE          : 407133534203.6523\n",
            "PA_Reg - MAPE          : 15.0631\n",
            "PA_Reg - MAE          : 638070.164013\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.57s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 372549.3508\n",
            "HT_Reg - MAPE          : 0.3908\n",
            "HT_Reg - MAE          : 610.368209\n",
            "HAT_Reg - MSE          : 78644.4369\n",
            "HAT_Reg - MAPE          : 0.1795\n",
            "HAT_Reg - MAE          : 280.436155\n",
            "ARF_Reg - MSE          : 60233.7302\n",
            "ARF_Reg - MAPE          : 0.1571\n",
            "ARF_Reg - MAE          : 245.425610\n",
            "PA_Reg - MSE          : 1551618.2210\n",
            "PA_Reg - MAPE          : 0.7975\n",
            "PA_Reg - MAE          : 1245.639683\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.16s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2629114.7467\n",
            "HT_Reg - MAPE          : 0.2508\n",
            "HT_Reg - MAE          : 1621.454516\n",
            "HAT_Reg - MSE          : 4150082.3279\n",
            "HAT_Reg - MAPE          : 0.3151\n",
            "HAT_Reg - MAE          : 2037.175085\n",
            "ARF_Reg - MSE          : 12160355.7311\n",
            "ARF_Reg - MAPE          : 0.5393\n",
            "ARF_Reg - MAE          : 3487.170161\n",
            "PA_Reg - MSE          : 1090244779.9753\n",
            "PA_Reg - MAPE          : 5.1065\n",
            "PA_Reg - MAE          : 33018.854916\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.80s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3470512.7020\n",
            "HT_Reg - MAPE          : 0.1151\n",
            "HT_Reg - MAE          : 1862.931212\n",
            "HAT_Reg - MSE          : 19489.4401\n",
            "HAT_Reg - MAPE          : 0.0086\n",
            "HAT_Reg - MAE          : 139.604585\n",
            "ARF_Reg - MSE          : 1085560679.0228\n",
            "ARF_Reg - MAPE          : 2.0349\n",
            "ARF_Reg - MAE          : 32947.847866\n",
            "PA_Reg - MSE          : 14335884411.2420\n",
            "PA_Reg - MAPE          : 7.3949\n",
            "PA_Reg - MAE          : 119732.553682\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.83s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 770744.6508\n",
            "HT_Reg - MAPE          : 0.0198\n",
            "HT_Reg - MAE          : 877.920640\n",
            "HAT_Reg - MSE          : 900025.0394\n",
            "HAT_Reg - MAPE          : 0.0213\n",
            "HAT_Reg - MAE          : 948.696495\n",
            "ARF_Reg - MSE          : 2051373.0382\n",
            "ARF_Reg - MAPE          : 0.0322\n",
            "ARF_Reg - MAE          : 1432.261512\n",
            "PA_Reg - MSE          : 10315609496.2229\n",
            "PA_Reg - MAPE          : 2.2853\n",
            "PA_Reg - MAE          : 101565.789005\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.09s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 14692301.1329\n",
            "HT_Reg - MAPE          : 0.0579\n",
            "HT_Reg - MAE          : 3833.053761\n",
            "HAT_Reg - MSE          : 2076428.6569\n",
            "HAT_Reg - MAPE          : 0.0217\n",
            "HAT_Reg - MAE          : 1440.981838\n",
            "ARF_Reg - MSE          : 15708010100.2162\n",
            "ARF_Reg - MAPE          : 1.8916\n",
            "ARF_Reg - MAE          : 125331.600565\n",
            "PA_Reg - MSE          : 5746350386.9501\n",
            "PA_Reg - MAPE          : 1.1441\n",
            "PA_Reg - MAE          : 75804.685785\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.11s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 42986096778.7577\n",
            "HT_Reg - MAPE          : 2.3465\n",
            "HT_Reg - MAE          : 207330.887180\n",
            "HAT_Reg - MSE          : 3753935413.1739\n",
            "HAT_Reg - MAPE          : 0.6934\n",
            "HAT_Reg - MAE          : 61269.367658\n",
            "ARF_Reg - MSE          : 1436140731.8516\n",
            "ARF_Reg - MAPE          : 0.4289\n",
            "ARF_Reg - MAE          : 37896.447483\n",
            "PA_Reg - MSE          : 4947979233318.7197\n",
            "PA_Reg - MAPE          : 25.1751\n",
            "PA_Reg - MAE          : 2224405.366231\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.50s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 275136.0258\n",
            "HT_Reg - MAPE          : 0.1598\n",
            "HT_Reg - MAE          : 524.534104\n",
            "HAT_Reg - MSE          : 77228.3097\n",
            "HAT_Reg - MAPE          : 0.0847\n",
            "HAT_Reg - MAE          : 277.899819\n",
            "ARF_Reg - MSE          : 71990.7470\n",
            "ARF_Reg - MAPE          : 0.0818\n",
            "ARF_Reg - MAE          : 268.310915\n",
            "PA_Reg - MSE          : 6131073.4676\n",
            "PA_Reg - MAPE          : 0.7545\n",
            "PA_Reg - MAE          : 2476.100456\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.12s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 12261174.7374\n",
            "HT_Reg - MAPE          : 0.2051\n",
            "HT_Reg - MAE          : 3501.596027\n",
            "HAT_Reg - MSE          : 332175.5375\n",
            "HAT_Reg - MAPE          : 0.0338\n",
            "HAT_Reg - MAE          : 576.346716\n",
            "ARF_Reg - MSE          : 133612914.4735\n",
            "ARF_Reg - MAPE          : 0.6770\n",
            "ARF_Reg - MAE          : 11559.105263\n",
            "PA_Reg - MSE          : 908456228.3308\n",
            "PA_Reg - MAPE          : 1.7653\n",
            "PA_Reg - MAE          : 30140.607630\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.64s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 395848358.2825\n",
            "HT_Reg - MAPE          : 0.5656\n",
            "HT_Reg - MAE          : 19895.938236\n",
            "HAT_Reg - MSE          : 395684305.8724\n",
            "HAT_Reg - MAPE          : 0.5655\n",
            "HAT_Reg - MAE          : 19891.815047\n",
            "ARF_Reg - MSE          : 5162635816.4025\n",
            "ARF_Reg - MAPE          : 2.0425\n",
            "ARF_Reg - MAE          : 71851.484441\n",
            "PA_Reg - MSE          : 25154012961.7168\n",
            "PA_Reg - MAPE          : 4.5084\n",
            "PA_Reg - MAE          : 158600.166966\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.09s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1492004756.5248\n",
            "HT_Reg - MAPE          : 0.9492\n",
            "HT_Reg - MAE          : 38626.477402\n",
            "HAT_Reg - MSE          : 1492007813.1822\n",
            "HAT_Reg - MAPE          : 0.9492\n",
            "HAT_Reg - MAE          : 38626.516969\n",
            "ARF_Reg - MSE          : 872217440.8345\n",
            "ARF_Reg - MAPE          : 0.7257\n",
            "ARF_Reg - MAE          : 29533.327629\n",
            "PA_Reg - MSE          : 119026286088.2047\n",
            "PA_Reg - MAPE          : 8.4780\n",
            "PA_Reg - MAE          : 345001.863891\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.18s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1252358363.2115\n",
            "HT_Reg - MAPE          : 1.0028\n",
            "HT_Reg - MAE          : 35388.675635\n",
            "HAT_Reg - MSE          : 1252371801.7113\n",
            "HAT_Reg - MAPE          : 1.0028\n",
            "HAT_Reg - MAE          : 35388.865505\n",
            "ARF_Reg - MSE          : 377742825.2622\n",
            "ARF_Reg - MAPE          : 0.5507\n",
            "ARF_Reg - MAE          : 19435.607149\n",
            "PA_Reg - MSE          : 2392688382669.3599\n",
            "PA_Reg - MAPE          : 43.8321\n",
            "PA_Reg - MAE          : 1546831.724096\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.51s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 5471110.5257\n",
            "HT_Reg - MAPE          : 0.0749\n",
            "HT_Reg - MAE          : 2339.040514\n",
            "HAT_Reg - MSE          : 5471109.5184\n",
            "HAT_Reg - MAPE          : 0.0749\n",
            "HAT_Reg - MAE          : 2339.040299\n",
            "ARF_Reg - MSE          : 130184581.7964\n",
            "ARF_Reg - MAPE          : 0.3656\n",
            "ARF_Reg - MAE          : 11409.845827\n",
            "PA_Reg - MSE          : 5881873219.8205\n",
            "PA_Reg - MAPE          : 2.4574\n",
            "PA_Reg - MAE          : 76693.371420\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.41s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 4216.4993\n",
            "HT_Reg - MAPE          : 0.0118\n",
            "HT_Reg - MAE          : 64.934577\n",
            "HAT_Reg - MSE          : 122883.0871\n",
            "HAT_Reg - MAPE          : 0.0636\n",
            "HAT_Reg - MAE          : 350.546840\n",
            "ARF_Reg - MSE          : 260630.9747\n",
            "ARF_Reg - MAPE          : 0.0926\n",
            "ARF_Reg - MAE          : 510.520298\n",
            "PA_Reg - MSE          : 4742279.9691\n",
            "PA_Reg - MAPE          : 0.3949\n",
            "PA_Reg - MAE          : 2177.677655\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.26s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 64388.5053\n",
            "HT_Reg - MAPE          : 0.0285\n",
            "HT_Reg - MAE          : 253.748902\n",
            "HAT_Reg - MSE          : 4441697.2762\n",
            "HAT_Reg - MAPE          : 0.2370\n",
            "HAT_Reg - MAE          : 2107.533458\n",
            "ARF_Reg - MSE          : 614509013.4654\n",
            "ARF_Reg - MAPE          : 2.7876\n",
            "ARF_Reg - MAE          : 24789.292315\n",
            "PA_Reg - MSE          : 224168514.8517\n",
            "PA_Reg - MAPE          : 1.6836\n",
            "PA_Reg - MAE          : 14972.258175\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.86s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 482978515.5333\n",
            "HT_Reg - MAPE          : 2.9172\n",
            "HT_Reg - MAE          : 21976.772182\n",
            "HAT_Reg - MSE          : 482978515.5408\n",
            "HAT_Reg - MAPE          : 2.9172\n",
            "HAT_Reg - MAE          : 21976.772182\n",
            "ARF_Reg - MSE          : 486578287.3608\n",
            "ARF_Reg - MAPE          : 2.9280\n",
            "ARF_Reg - MAE          : 22058.519609\n",
            "PA_Reg - MSE          : 1401103626561.1465\n",
            "PA_Reg - MAPE          : 157.1204\n",
            "PA_Reg - MAE          : 1183682.232088\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.48s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 17558351.3849\n",
            "HT_Reg - MAPE          : 0.7020\n",
            "HT_Reg - MAE          : 4190.268653\n",
            "HAT_Reg - MSE          : 17558351.3849\n",
            "HAT_Reg - MAPE          : 0.7020\n",
            "HAT_Reg - MAE          : 4190.268653\n",
            "ARF_Reg - MSE          : 1588.4849\n",
            "ARF_Reg - MAPE          : 0.0067\n",
            "ARF_Reg - MAE          : 39.855802\n",
            "PA_Reg - MSE          : 53194449517.3266\n",
            "PA_Reg - MAPE          : 38.6408\n",
            "PA_Reg - MAE          : 230639.219382\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.39s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 21074.1018\n",
            "HT_Reg - MAPE          : 0.0301\n",
            "HT_Reg - MAE          : 145.169218\n",
            "HAT_Reg - MSE          : 21074.1018\n",
            "HAT_Reg - MAPE          : 0.0301\n",
            "HAT_Reg - MAE          : 145.169218\n",
            "ARF_Reg - MSE          : 1990.0229\n",
            "ARF_Reg - MAPE          : 0.0092\n",
            "ARF_Reg - MAE          : 44.609672\n",
            "PA_Reg - MSE          : 77889069315.2173\n",
            "PA_Reg - MAPE          : 57.7758\n",
            "PA_Reg - MAE          : 279086.132431\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.17s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 97763.9415\n",
            "HT_Reg - MAPE          : 0.0517\n",
            "HT_Reg - MAE          : 312.672259\n",
            "HAT_Reg - MSE          : 97763.9415\n",
            "HAT_Reg - MAPE          : 0.0517\n",
            "HAT_Reg - MAE          : 312.672259\n",
            "ARF_Reg - MSE          : 13398.8377\n",
            "ARF_Reg - MAPE          : 0.0191\n",
            "ARF_Reg - MAE          : 115.753348\n",
            "PA_Reg - MSE          : 268813038958.8328\n",
            "PA_Reg - MAPE          : 85.6752\n",
            "PA_Reg - MAE          : 518471.830439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HQdWByW7GTn"
      },
      "source": [
        "# Save the running time for each country\n",
        "for i in range(len(countries)):\n",
        "  save_runtime(results_runtime_incremental[i], path=exp1_runtime_path, country = countries[i], static_learner=False)\n",
        "\n",
        "# Display countrywise running time complexity\n",
        "display_runtime_per_country(results_runtime_incremental, countries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMH2CG_nuloA"
      },
      "source": [
        "countrywise_error_score_incremental = calc_save_err_metric_countrywise(countries, error_metrics, results_incremental, max_of_pretrain_per_country, max_cases_per_country, path=exp1_path, static_learner=False, transpose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6hi5rPZF4Uf"
      },
      "source": [
        "# Get summary table for each country for specified metric\n",
        "summary_table_countrywise_incremental = get_summary_table_countrywise(countrywise_error_score_incremental, ['MAPE'], static_learner=False)\n",
        "\n",
        "# Saving the summary table\n",
        "save_summary_table(summary_table_countrywise_incremental, exp1_summary_path,country=True, static_learner=False,alternate_batch=False,transpose=True)\n",
        "\n",
        "summary_table_countrywise_incremental"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jEmM1GgdAqq"
      },
      "source": [
        "sum_inc_countrywise_mean = get_sum_table_combined_mean(countrywise_error_score_incremental,results_runtime_incremental)\n",
        "save_combined_summary_table(sum_inc_countrywise_mean, exp1_summary_path, static_learner=False, transpose=True) \n",
        "sum_inc_countrywise_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEBzePUFh8FO"
      },
      "source": [
        "## Static Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtk8rnQGTPgy"
      },
      "source": [
        "def scikit_learn(df, training_days, country):  #Added country now\r\n",
        "    frames = []\r\n",
        "    model_predictions = {\r\n",
        "        'RandomForest': [],\r\n",
        "        'GradientBoosting': [],\r\n",
        "        'LinearSVR': [],\r\n",
        "        'DecisionTree': [],\r\n",
        "        'BayesianRidge': []\r\n",
        "      #  'LSTM': []  #TODO: Uncomment later\r\n",
        "      # 'MLPRegressor': [],\r\n",
        "      # 'LinearRegression': []\r\n",
        "    }\r\n",
        "    total_execution_time = []\r\n",
        "\r\n",
        "    # params (others like epoch and batch size are also hardcoded in train_test_lstm())\r\n",
        "    layers = [50, 30, 20, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\r\n",
        "    activations = ['tanh', 'tanh', 'relu']\r\n",
        "    epochs = 500\r\n",
        "    patience = 20\r\n",
        "    batch_size_lstm = 10\r\n",
        "    united_dataframe = []  # Added Now\r\n",
        "\r\n",
        "    for day in training_days:\r\n",
        "        testing_samples_size = 30  # Added Now\r\n",
        "        cur_exec_time = [day]  # Keeping running time for each pre-train set\r\n",
        "        target_dates = df.iloc[day: day + testing_samples_size, 0]  # Added Now\r\n",
        "        train = df.iloc[:day, :]\r\n",
        "        test = df.iloc[day:day + testing_samples_size, :]  # Testing on set one month ahead only, hence day+30.\r\n",
        "\r\n",
        "        # training and test sets for all models except LSTM\r\n",
        "        X_train, y_train = train.iloc[:, 4:-1], train.iloc[:, -1]\r\n",
        "        X_test, y_test = test.iloc[:, 4:-1], test.iloc[:, -1]\r\n",
        "\r\n",
        "        # Seperating validation set from train set\r\n",
        "        train_df, val_df = get_validation_set(train, batch_size=10)\r\n",
        "\r\n",
        "        # Splitting test and validation into dependent and independent sets\r\n",
        "        X_train_batch, y_train_batch = train_df.iloc[:, 4:-1], train_df.iloc[:, -1]  # Consist only odd batches\r\n",
        "        X_val_batch, y_val_batch = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\r\n",
        "\r\n",
        "        # Normalizing dataset\r\n",
        "        X_train_lstm_norm, X_test_lstm_norm, X_val_lstm_norm = normalize_dataset(X_train_batch, X_test, X_val_batch)\r\n",
        "\r\n",
        "        # Reshaping the dataframes\r\n",
        "        X_train_lstm, X_val_lstm, X_test_lstm = reshape_dataframe(X_train_lstm_norm, X_val_lstm_norm, X_test_lstm_norm)\r\n",
        "\r\n",
        "        rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\r\n",
        "        model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train, y_train, X_test)\r\n",
        "        cur_exec_time.append(exec_time)\r\n",
        "\r\n",
        "        gb_reg = GradientBoostingRegressor(random_state=0)\r\n",
        "        model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\r\n",
        "        cur_exec_time.append(exec_time)\r\n",
        "\r\n",
        "        lsv_reg = LinearSVR(random_state=0, tol=1e-5)\r\n",
        "        model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\r\n",
        "        cur_exec_time.append(exec_time)\r\n",
        "\r\n",
        "        dt_reg = DecisionTreeRegressor(random_state=0)\r\n",
        "        model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\r\n",
        "        cur_exec_time.append(exec_time)\r\n",
        "\r\n",
        "        br_reg = BayesianRidge()\r\n",
        "        model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\r\n",
        "        cur_exec_time.append(exec_time)\r\n",
        "\r\n",
        "        #lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\r\n",
        "        #model_predictions['LSTM'], exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train_batch, X_val_lstm, y_val_batch, X_test_lstm, patience, epochs, batch_size_lstm)\r\n",
        "        #cur_exec_time.append(exec_time)\r\n",
        "\r\n",
        "        #TODO: Check what y_true to send for LSTM\r\n",
        "        united_dataframe.append(unit_static_df(country, target_dates, y_test, day, model_predictions))  # Added now\r\n",
        "\r\n",
        "        mdl_evaluation_df = get_scores(y_test, model_predictions, day)\r\n",
        "        total_execution_time.append(cur_exec_time)\r\n",
        "        frames.append(mdl_evaluation_df)\r\n",
        "\r\n",
        "    evaluation_score_df = pd.concat(frames, ignore_index=True)\r\n",
        "    united_dataframe = pd.concat(united_dataframe, ignore_index=True)  # Added Now\r\n",
        "    running_time_df = get_running_time_per_model_static_learner(model_predictions, total_execution_time)\r\n",
        "    return evaluation_score_df, running_time_df, united_dataframe\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCrlBMu3TZMm",
        "outputId": "68d2ba71-21ba-4a25-82b1-42d0e0b9799a"
      },
      "source": [
        "results_static = []\r\n",
        "results_runtime_static = []\r\n",
        "max_of_pretrain_per_country = []\r\n",
        "max_cases_per_country = []\r\n",
        "\r\n",
        "for country in countries:\r\n",
        "    # Read country wise csv file\r\n",
        "    df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\r\n",
        "\r\n",
        "    # Evaluation scores and running time of each algorithm over different pre-training days\r\n",
        "    evaluation_scores_df, running_time_df, united_dataframe = scikit_learn(df_country, pretrain_days, country)  # Returning united_dataframe also\r\n",
        "\r\n",
        "    save_united_df(united_dataframe, exp1_static_united_df_path, country=country)\r\n",
        "\r\n",
        "    # Append result of each pretrain size in results\r\n",
        "    results_static.append(evaluation_scores_df)\r\n",
        "\r\n",
        "    # Appending every country runtime\r\n",
        "    results_runtime_static.append(running_time_df)\r\n",
        "\r\n",
        "    # Calculating max cases per country based on pre-train size\r\n",
        "    max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days, df_country))\r\n",
        "\r\n",
        "    # Maximum case of each country\r\n",
        "    max_cases_per_country.append(df_country['cases'].max())\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdlwyL_f04aH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "185c0a68-bf2a-46ec-f30d-7fe25854b0e6"
      },
      "source": [
        "\"\"\"\n",
        "# Old Script\n",
        "\n",
        "def scikit_learn(df, training_days):\n",
        "    frames = []\n",
        "    model_predictions = {\n",
        "        'RandomForest': [],\n",
        "        'GradientBoosting': [],\n",
        "        'LinearSVR': [],\n",
        "        'DecisionTree': [],\n",
        "        'BayesianRidge': [],\n",
        "        'LSTM': []\n",
        "        #'MLPRegressor': [],\n",
        "        #'LinearRegression': []\n",
        "    }\n",
        "    total_execution_time = []\n",
        "            \n",
        "    # LSTM (TODO: feel free to put the whole LSTM definition in a funtion)\n",
        "    # params (others like epoch and batch size are also hardcoded in train_test_lstm())\n",
        "    layers = [50, 30, 20, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\n",
        "    activations = ['tanh', 'tanh', 'relu']\n",
        "    epochs = 500\n",
        "    patience = 20\n",
        "    batch_size_lstm = 10\n",
        "\n",
        "    for day in training_days:\n",
        "        \n",
        "        cur_exec_time = [day]  # Keeping runing time for each pre-train set\n",
        "\n",
        "        train = df.iloc[:day, :]\n",
        "        test = df.iloc[day:day + 30, :]  # Testing on set one month ahead only, hence day+30.\n",
        "        \n",
        "        # training and test sets for all models except LSTM\n",
        "        X_train, y_train = train.iloc[:, 4:-1], train.iloc[:, -1]\n",
        "        X_test, y_test = test.iloc[:, 4:-1], test.iloc[:, -1]\n",
        "\n",
        "        # Seperating validation set from train set\n",
        "        train_df, val_df = get_validation_set(train, batch_size=10)\n",
        "\n",
        "        # Splitting test and validation into dependent and independent sets\n",
        "        X_train_batch, y_train_batch = train_df.iloc[:, 4:-1], train_df.iloc[:, -1]  # Consist only odd batches\n",
        "        X_val_batch, y_val_batch = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\n",
        "\n",
        "        # Normalizing dataset\n",
        "        X_train_lstm_norm, X_test_lstm_norm, X_val_lstm_norm = normalize_dataset(X_train_batch, X_test, X_val_batch)\n",
        "\n",
        "        # Reshaping the dataframes\n",
        "        X_train_lstm, X_val_lstm, X_test_lstm = reshape_dataframe(X_train_lstm_norm, X_val_lstm_norm, X_test_lstm_norm)\n",
        "       \n",
        "\n",
        "        rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "        model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train,y_train,X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "        gb_reg = GradientBoostingRegressor(random_state=0)\n",
        "        model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "        lsv_reg = LinearSVR(random_state=0, tol=1e-5)\n",
        "        model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "        dt_reg = DecisionTreeRegressor(random_state=0)\n",
        "        model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "        br_reg = BayesianRidge()\n",
        "        model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        \n",
        "        lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\n",
        "        model_predictions['LSTM'],exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train_batch, X_val_lstm, y_val_batch, X_test_lstm, patience, epochs, batch_size_lstm)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        \n",
        "        mdl_evaluation_df = get_scores(y_test, model_predictions, day)\n",
        "        total_execution_time.append(cur_exec_time)\n",
        "        frames.append(mdl_evaluation_df)\n",
        "\n",
        "    evaluation_score_df = pd.concat(frames, ignore_index=True)\n",
        "    running_time_df = get_running_time_per_model_static_learner(model_predictions,total_execution_time)\n",
        "    return evaluation_score_df, running_time_df\n",
        "\n",
        "# Training all countries\n",
        "results_static = []\n",
        "results_runtime_static = []\n",
        "max_of_pretrain_per_country = []\n",
        "max_cases_per_country = []\n",
        "\n",
        "results_static = []\n",
        "results_runtime_static = []\n",
        "max_of_pretrain_per_country = []\n",
        "max_cases_per_country = []\n",
        "\n",
        "for country in countries:\n",
        "  # Read country wise csv file\n",
        "  df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\n",
        "\n",
        "  # Evaluation scores and running time of each algorithm over different pre-training days\n",
        "  evaluation_scores_df, running_time_df = scikit_learn(df_country, pretrain_days)\n",
        "\n",
        "  # Append result of each pretrain size in results\n",
        "  results_static.append(evaluation_scores_df)\n",
        "\n",
        "  # Appending every country runtime \n",
        "  results_runtime_static.append(running_time_df)\n",
        "\n",
        "  # Calculating max cases per country based on pre-train size\n",
        "  max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days,df_country))\n",
        "\n",
        "  # Maximum case of each country\n",
        "  max_cases_per_country.append(df_country['cases'].max())\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Old Script\\n\\ndef scikit_learn(df, training_days):\\n    frames = []\\n    model_predictions = {\\n        'RandomForest': [],\\n        'GradientBoosting': [],\\n        'LinearSVR': [],\\n        'DecisionTree': [],\\n        'BayesianRidge': [],\\n        'LSTM': []\\n        #'MLPRegressor': [],\\n        #'LinearRegression': []\\n    }\\n    total_execution_time = []\\n            \\n    # LSTM (TODO: feel free to put the whole LSTM definition in a funtion)\\n    # params (others like epoch and batch size are also hardcoded in train_test_lstm())\\n    layers = [50, 30, 20, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\\n    activations = ['tanh', 'tanh', 'relu']\\n    epochs = 500\\n    patience = 20\\n    batch_size_lstm = 10\\n\\n    for day in training_days:\\n        \\n        cur_exec_time = [day]  # Keeping runing time for each pre-train set\\n\\n        train = df.iloc[:day, :]\\n        test = df.iloc[day:day + 30, :]  # Testing on set one month ahead only, hence day+30.\\n        \\n        # training and test sets for all models except LSTM\\n        X_train, y_train = train.iloc[:, 4:-1], train.iloc[:, -1]\\n        X_test, y_test = test.iloc[:, 4:-1], test.iloc[:, -1]\\n\\n        # Seperating validation set from train set\\n        train_df, val_df = get_validation_set(train, batch_size=10)\\n\\n        # Splitting test and validation into dependent and independent sets\\n        X_train_batch, y_train_batch = train_df.iloc[:, 4:-1], train_df.iloc[:, -1]  # Consist only odd batches\\n        X_val_batch, y_val_batch = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\\n\\n        # Normalizing dataset\\n        X_train_lstm_norm, X_test_lstm_norm, X_val_lstm_norm = normalize_dataset(X_train_batch, X_test, X_val_batch)\\n\\n        # Reshaping the dataframes\\n        X_train_lstm, X_val_lstm, X_test_lstm = reshape_dataframe(X_train_lstm_norm, X_val_lstm_norm, X_test_lstm_norm)\\n       \\n\\n        rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\\n        model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train,y_train,X_test)\\n        cur_exec_time.append(exec_time)\\n\\n\\n        gb_reg = GradientBoostingRegressor(random_state=0)\\n        model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\\n        cur_exec_time.append(exec_time)\\n\\n\\n        lsv_reg = LinearSVR(random_state=0, tol=1e-5)\\n        model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\\n        cur_exec_time.append(exec_time)\\n\\n\\n        dt_reg = DecisionTreeRegressor(random_state=0)\\n        model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\\n        cur_exec_time.append(exec_time)\\n\\n\\n        br_reg = BayesianRidge()\\n        model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\\n        cur_exec_time.append(exec_time)\\n\\n        \\n        lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\\n        model_predictions['LSTM'],exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train_batch, X_val_lstm, y_val_batch, X_test_lstm, patience, epochs, batch_size_lstm)\\n        cur_exec_time.append(exec_time)\\n\\n        \\n        mdl_evaluation_df = get_scores(y_test, model_predictions, day)\\n        total_execution_time.append(cur_exec_time)\\n        frames.append(mdl_evaluation_df)\\n\\n    evaluation_score_df = pd.concat(frames, ignore_index=True)\\n    running_time_df = get_running_time_per_model_static_learner(model_predictions,total_execution_time)\\n    return evaluation_score_df, running_time_df\\n\\n# Training all countries\\nresults_static = []\\nresults_runtime_static = []\\nmax_of_pretrain_per_country = []\\nmax_cases_per_country = []\\n\\nresults_static = []\\nresults_runtime_static = []\\nmax_of_pretrain_per_country = []\\nmax_cases_per_country = []\\n\\nfor country in countries:\\n  # Read country wise csv file\\n  df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\\n\\n  # Evaluation scores and running time of each algorithm over different pre-training days\\n  evaluation_scores_df, running_time_df = scikit_learn(df_country, pretrain_days)\\n\\n  # Append result of each pretrain size in results\\n  results_static.append(evaluation_scores_df)\\n\\n  # Appending every country runtime \\n  results_runtime_static.append(running_time_df)\\n\\n  # Calculating max cases per country based on pre-train size\\n  max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days,df_country))\\n\\n  # Maximum case of each country\\n  max_cases_per_country.append(df_country['cases'].max())\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im2-1jnV8T_W"
      },
      "source": [
        "# Save the running time for each country\n",
        "for i in range(len(countries)):\n",
        "  save_runtime(results_runtime_static[i], path=exp1_runtime_path, country = countries[i], static_learner=True)\n",
        "\n",
        "display_runtime_per_country(results_runtime_static, countries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NlVmdHK37Bk"
      },
      "source": [
        "countrywise_error_scores_static = calc_save_err_metric_countrywise(countries, error_metrics, results_static, max_of_pretrain_per_country, max_cases_per_country, path=exp1_path, static_learner=True, transpose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU9yFzgj_IHX"
      },
      "source": [
        "summary_table_countrywise_static = get_summary_table_countrywise(countrywise_error_scores_static, ['MAPE'], static_learner=True)\n",
        "\n",
        "# Saving the transposed matrix\n",
        "save_summary_table(summary_table_countrywise_static, exp1_summary_path, country=True, static_learner=True, alternate_batch=False, transpose=True)\n",
        "\n",
        "summary_table_countrywise_static"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTMQFGlMfCkT"
      },
      "source": [
        "sum_static_countrywise_mean = get_sum_table_combined_mean(countrywise_error_scores_static,results_runtime_static,static_learner=True)\n",
        "save_combined_summary_table(sum_static_countrywise_mean, exp1_summary_path, static_learner=True, transpose=True) \n",
        "sum_static_countrywise_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIMr3U1QAc63"
      },
      "source": [
        "## Significance tests for Experiment 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlZxtco1AbsI"
      },
      "source": [
        "## EXP1\n",
        "# Significance results for Experiment 1\n",
        "err_metric_for_significance = 'MAPE'\n",
        "significance_thresh = 0.01\n",
        "\n",
        "# Concatenating a population of all results (as in boxplot) for experiment 1\n",
        "concated_df = pd.concat([summary_table_countrywise_incremental.transpose(), summary_table_countrywise_static.transpose()]).transpose().drop(columns=['EvaluationMeasurement'], axis=1)\n",
        "concated_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyz23f1PMrpY"
      },
      "source": [
        "# Selecting the best algorithm for statistical comparisons\n",
        "# We want to know if the best is statistically significantly better compared to the rest.\n",
        "best_algo = concated_df.mean().sort_values(ascending=True).index[0]\n",
        "best_algo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFR6AjtxMv-M"
      },
      "source": [
        "print('AVG results across countries')\n",
        "concated_df.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLYuRNC8M0yF"
      },
      "source": [
        "print('STDEV across countries')\n",
        "concated_df.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpiiYDApAbeX"
      },
      "source": [
        "# Iterate through all the other algorithms to see if the difference in results is significant\n",
        "competitors = list(concated_df.columns)\n",
        "competitors.remove(best_algo)\n",
        "\n",
        "for significance_thresh in [0.01,0.05]:\n",
        "  print(f'Running significane at: {significance_thresh}')\n",
        "  for competitor in competitors:\n",
        "    # print(competitor)\n",
        "    pval, significant = check_significance(concated_df[best_algo], concated_df[competitor], significance_at=significance_thresh)\n",
        "    print(f'Comparison of {best_algo} to {competitor} pvalue: {pval}   /  significant?: {significant}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0kt9cV_XWS5"
      },
      "source": [
        "# Iterate through all the other algorithms to see if the difference in results is significant\n",
        "best_algo2 = concated_df.mean().sort_values(ascending=True).index[1]\n",
        "competitors = list(concated_df.columns)\n",
        "competitors.remove(best_algo2)\n",
        "for significance_thresh in [0.01,0.05]:\n",
        "  print(f'Running significane at: {significance_thresh}')\n",
        "  for competitor in competitors:\n",
        "    # print(competitor)\n",
        "    pval, significant = check_significance(concated_df[best_algo2], concated_df[competitor], significance_at=significance_thresh)\n",
        "    print(f'Comparison of {best_algo2} to {competitor} pvalue: {pval}   /  significant?: {significant}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3abKxWJwqNU6"
      },
      "source": [
        "# Experiment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM8GB6LTRPxY"
      },
      "source": [
        "### Creating Combined dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7vb_yHqvwPX"
      },
      "source": [
        "# Reading file \n",
        "#url = 'https://drive.google.com/file/d/1e7NsptfEFLG2gGLykYlrzjNbDJLiRbGm/view?usp=sharing'\n",
        "url = 'https://drive.google.com/file/d/1VH-nkePskK3gT6U5qkoOP-0hFT4beszC/view?usp=sharing'\n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "df = pd.read_csv(path)\n",
        "num_selected_countries = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxDuX7-ansDh"
      },
      "source": [
        "\n",
        "# Pre-processing dataset\n",
        "df = preprocess_dataset(df)\n",
        "\n",
        "# Grouping records by country\n",
        "df_grouped = df.groupby('country')\n",
        "\n",
        "# Taking only those countries which have sufficient data records\n",
        "#valid_countries = get_countries_with_valid_size(df_grouped)\n",
        "\n",
        "# Sorting countries by number of cases\n",
        "df_countries_sortedbycases = get_countries_sortedby_cases(valid_countries, df_grouped)\n",
        "\n",
        "# Taking only top selected countries\n",
        "top_selected_countries = df_countries_sortedbycases.iloc[0:num_selected_countries].index\n",
        "\n",
        "# Calculating targets and lags for the above countries\n",
        "result = get_dataset_with_target(top_selected_countries,df_grouped)\n",
        "\n",
        "# Getting max of each subset in pretrain size\n",
        "max_of_pretrain_days = calc_max_of_pretrain_days(pretrain_days,result)\n",
        "\n",
        "# Mean of top selected countries\n",
        "max_selected_countries = result['cases'].max()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtxpRv3FvxdD"
      },
      "source": [
        "### Incremental Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcRN9CoVFigI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "78779a44-eab5-448c-cf75-cbb89ae94994"
      },
      "source": [
        "# Old Script\n",
        "\"\"\"\n",
        "def scikit_multiflow(df, pretrain_days):\n",
        "\n",
        "  model, model_names = instantiate_regressors()\n",
        "\n",
        "  len_countries = len(df['country'].unique())\n",
        "\n",
        "  frames , running_time_frames = [], []\n",
        "\n",
        "  # Setup the evaluator\n",
        "  for day in pretrain_days:\n",
        "\n",
        "      df_subset = create_subset(df,day)\n",
        "      \n",
        "      # Creating a stream from dataframe\n",
        "      stream = DataStream(np.array(df_subset.iloc[:,4:-1]), y=np.array(df_subset.iloc[:,-1])) #TODO: Drop columns with name \n",
        "\n",
        "      pretrain_size = day * len_countries\n",
        "      max_samples = (day + 30) * len_countries #Testing on set one month ahead only\n",
        "\n",
        "      evaluator = EvaluatePrequential(show_plot=False,\n",
        "                                    pretrain_size=pretrain_size,\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\n",
        "                                    max_samples=max_samples)\n",
        "      # Run evaluation\n",
        "      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
        "\n",
        "      # Dictionary to store each iteration error scores\n",
        "      mdl_evaluation_scores = {}\n",
        "\n",
        "      # Adding Evaluation Measurements and pretraining days\n",
        "      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\n",
        "      mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "\n",
        "      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\n",
        "\n",
        "      # Errors of each model on a specific pre-train days\n",
        "      frames.append(mdl_evaluation_df)\n",
        "\n",
        "      # Run time for each algorithm\n",
        "      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
        "\n",
        "   # Final Run Time DataFrame\n",
        "  running_time_df = pd.concat(running_time_frames,ignore_index=True)\n",
        "\n",
        "  # Final Evaluation Score Dataframe\n",
        "  evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
        "  return evaluation_scores_df, running_time_df\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef scikit_multiflow(df, pretrain_days):\\n\\n  model, model_names = instantiate_regressors()\\n\\n  len_countries = len(df['country'].unique())\\n\\n  frames , running_time_frames = [], []\\n\\n  # Setup the evaluator\\n  for day in pretrain_days:\\n\\n      df_subset = create_subset(df,day)\\n      \\n      # Creating a stream from dataframe\\n      stream = DataStream(np.array(df_subset.iloc[:,4:-1]), y=np.array(df_subset.iloc[:,-1])) #TODO: Drop columns with name \\n\\n      pretrain_size = day * len_countries\\n      max_samples = (day + 30) * len_countries #Testing on set one month ahead only\\n\\n      evaluator = EvaluatePrequential(show_plot=False,\\n                                    pretrain_size=pretrain_size,\\n                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\\n                                    max_samples=max_samples)\\n      # Run evaluation\\n      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\\n\\n      # Dictionary to store each iteration error scores\\n      mdl_evaluation_scores = {}\\n\\n      # Adding Evaluation Measurements and pretraining days\\n      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\\n      mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\\n\\n      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\\n\\n      # Errors of each model on a specific pre-train days\\n      frames.append(mdl_evaluation_df)\\n\\n      # Run time for each algorithm\\n      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\\n\\n   # Final Run Time DataFrame\\n  running_time_df = pd.concat(running_time_frames,ignore_index=True)\\n\\n  # Final Evaluation Score Dataframe\\n  evaluation_scores_df = pd.concat(frames, ignore_index=True)\\n  return evaluation_scores_df, running_time_df\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6RpyYrC-HSB"
      },
      "source": [
        "def scikit_multiflow(df, pretrain_days):\r\n",
        "    model, model_names = instantiate_regressors()\r\n",
        "\r\n",
        "    len_countries = len(df['country'].unique())\r\n",
        "\r\n",
        "    frames, running_time_frames = [], []\r\n",
        "\r\n",
        "    # Setup the evaluator\r\n",
        "    for day in pretrain_days:\r\n",
        "        df_subset = create_subset(df, day)\r\n",
        "\r\n",
        "        # Creating a stream from dataframe\r\n",
        "        stream = DataStream(np.array(df_subset.iloc[:, 4:-1]),\r\n",
        "                            y=np.array(df_subset.iloc[:, -1]))  # TODO: Drop columns with name\r\n",
        "\r\n",
        "        pretrain_size = day * len_countries\r\n",
        "        max_samples = pretrain_size + 1  # One Extra Sample\r\n",
        "        testing_samples_size = (day + 30) * len_countries\r\n",
        "\r\n",
        "        evaluator = EvaluatePrequential(show_plot=False,\r\n",
        "                                        pretrain_size=pretrain_size,\r\n",
        "                                        metrics=['mean_square_error', 'mean_absolute_error',\r\n",
        "                                                 'mean_absolute_percentage_error'],\r\n",
        "                                        max_samples=max_samples)\r\n",
        "        # Run evaluation\r\n",
        "        evaluator.evaluate(stream=stream, model=model, model_names=model_names)\r\n",
        "\r\n",
        "        # Added Now\r\n",
        "        X = stream.X[pretrain_size:  testing_samples_size]\r\n",
        "        y = stream.y[pretrain_size:  testing_samples_size]\r\n",
        "\r\n",
        "        prediction = evaluator.predict(X)\r\n",
        "\r\n",
        "        # Since we add one extra sample, reset the evaluator\r\n",
        "        evaluator = reset_evaluator(evaluator)\r\n",
        "\r\n",
        "        evaluator = update_incremental_metrics(evaluator, y, prediction)\r\n",
        "\r\n",
        "        # Dictionary to store each iteration error scores\r\n",
        "        mdl_evaluation_scores = {}\r\n",
        "\r\n",
        "        # Adding Evaluation Measurements and pretraining days\r\n",
        "        mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE', 'MAE', 'MAPE']  # ,'MSE']\r\n",
        "        mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\r\n",
        "\r\n",
        "        mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\r\n",
        "\r\n",
        "        # Errors of each model on a specific pre-train days\r\n",
        "        frames.append(mdl_evaluation_df)\r\n",
        "\r\n",
        "        # Run time for each algorithm\r\n",
        "        running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\r\n",
        "\r\n",
        "    # Final Run Time DataFrame\r\n",
        "    running_time_df = pd.concat(running_time_frames, ignore_index=True)\r\n",
        "\r\n",
        "    # Final Evaluation Score Dataframe\r\n",
        "    evaluation_scores_df = pd.concat(frames, ignore_index=True)\r\n",
        "    return evaluation_scores_df, running_time_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32jMOvemFnHG",
        "outputId": "6bce4863-3d3a-41c2-82b0-4897d7e0ea02"
      },
      "source": [
        "result_skmlflow, running_time_combined_incremental = scikit_multiflow(result, pretrain_days)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 750 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [12.56s]\n",
            "Processed samples: 751\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 659277103.5398\n",
            "HT_Reg - MAPE          : 0.8969\n",
            "HT_Reg - MAE          : 25676.391949\n",
            "HAT_Reg - MSE          : 665215060.7189\n",
            "HAT_Reg - MAPE          : 0.9009\n",
            "HAT_Reg - MAE          : 25791.763428\n",
            "ARF_Reg - MSE          : 617432218.5067\n",
            "ARF_Reg - MAPE          : 0.8679\n",
            "ARF_Reg - MAE          : 24848.183405\n",
            "PA_Reg - MSE          : 3944202760.3951\n",
            "PA_Reg - MAPE          : 2.1937\n",
            "PA_Reg - MAE          : 62802.888153\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 1500 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [32.25s]\n",
            "Processed samples: 1501\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 126089267.9679\n",
            "HT_Reg - MAPE          : 0.5267\n",
            "HT_Reg - MAE          : 11228.947768\n",
            "HAT_Reg - MSE          : 126089267.9679\n",
            "HAT_Reg - MAPE          : 0.5267\n",
            "HAT_Reg - MAE          : 11228.947768\n",
            "ARF_Reg - MSE          : 485466223.3304\n",
            "ARF_Reg - MAPE          : 1.0336\n",
            "ARF_Reg - MAE          : 22033.298058\n",
            "PA_Reg - MSE          : 187409907640.1493\n",
            "PA_Reg - MAPE          : 20.3076\n",
            "PA_Reg - MAE          : 432908.659696\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 2250 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [53.23s]\n",
            "Processed samples: 2251\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1329199402.8041\n",
            "HT_Reg - MAPE          : 1.1044\n",
            "HT_Reg - MAE          : 36458.187048\n",
            "HAT_Reg - MSE          : 1329199402.8041\n",
            "HAT_Reg - MAPE          : 1.1044\n",
            "HAT_Reg - MAE          : 36458.187048\n",
            "ARF_Reg - MSE          : 1293771026.6563\n",
            "ARF_Reg - MAPE          : 1.0896\n",
            "ARF_Reg - MAE          : 35969.028714\n",
            "PA_Reg - MSE          : 2691246400.5024\n",
            "PA_Reg - MAPE          : 1.5715\n",
            "PA_Reg - MAE          : 51877.224295\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 3000 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [90.43s]\n",
            "Processed samples: 3001\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2806816619.4232\n",
            "HT_Reg - MAPE          : 0.8054\n",
            "HT_Reg - MAE          : 52979.398066\n",
            "HAT_Reg - MSE          : 2806816619.4232\n",
            "HAT_Reg - MAPE          : 0.8054\n",
            "HAT_Reg - MAE          : 52979.398066\n",
            "ARF_Reg - MSE          : 3216334294.2183\n",
            "ARF_Reg - MAPE          : 0.8622\n",
            "ARF_Reg - MAE          : 56712.734850\n",
            "PA_Reg - MSE          : 26235553038.8082\n",
            "PA_Reg - MAPE          : 2.4624\n",
            "PA_Reg - MAE          : 161973.927034\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 3750 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [97.32s]\n",
            "Processed samples: 3751\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1529232457.1778\n",
            "HT_Reg - MAPE          : 0.9358\n",
            "HT_Reg - MAE          : 39105.401893\n",
            "HAT_Reg - MSE          : 1529232457.1778\n",
            "HAT_Reg - MAPE          : 0.9358\n",
            "HAT_Reg - MAE          : 39105.401893\n",
            "ARF_Reg - MSE          : 308649118.8651\n",
            "ARF_Reg - MAPE          : 0.4204\n",
            "ARF_Reg - MAE          : 17568.412531\n",
            "PA_Reg - MSE          : 2331702005.2665\n",
            "PA_Reg - MAPE          : 1.1556\n",
            "PA_Reg - MAE          : 48287.700352\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 4500 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [123.64s]\n",
            "Processed samples: 4501\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 45334377.7687\n",
            "HT_Reg - MAPE          : 0.1590\n",
            "HT_Reg - MAE          : 6733.080853\n",
            "HAT_Reg - MSE          : 914188297.1162\n",
            "HAT_Reg - MAPE          : 0.7138\n",
            "HAT_Reg - MAE          : 30235.546913\n",
            "ARF_Reg - MSE          : 1304723530.5725\n",
            "ARF_Reg - MAPE          : 0.8527\n",
            "ARF_Reg - MAE          : 36120.956944\n",
            "PA_Reg - MSE          : 211647096531.5542\n",
            "PA_Reg - MAPE          : 10.8606\n",
            "PA_Reg - MAE          : 460051.189034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xOniEJ11niO"
      },
      "source": [
        "df_skmlflow = calc_save_err_metric_combined(error_metrics, result_skmlflow, max_of_pretrain_days, max_selected_countries, path=exp2_path, static_learner=False, alternate_batch=False, transpose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "Cq1ffodM5C4f",
        "outputId": "7d1dfed3-28bd-4682-ece2-4580345dadce"
      },
      "source": [
        "save_runtime(running_time_combined_incremental, path=exp2_runtime_path, static_learner=False)\n",
        "running_time_combined_incremental"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PretrainDays</th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30</td>\n",
              "      <td>0.307</td>\n",
              "      <td>0.475</td>\n",
              "      <td>11.808</td>\n",
              "      <td>0.002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60</td>\n",
              "      <td>1.179</td>\n",
              "      <td>1.571</td>\n",
              "      <td>29.536</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>90</td>\n",
              "      <td>2.886</td>\n",
              "      <td>4.303</td>\n",
              "      <td>46.069</td>\n",
              "      <td>0.004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>120</td>\n",
              "      <td>3.940</td>\n",
              "      <td>16.657</td>\n",
              "      <td>69.849</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>150</td>\n",
              "      <td>5.302</td>\n",
              "      <td>13.564</td>\n",
              "      <td>78.481</td>\n",
              "      <td>0.004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>180</td>\n",
              "      <td>6.758</td>\n",
              "      <td>22.722</td>\n",
              "      <td>94.178</td>\n",
              "      <td>0.004</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
              "0            30   0.307    0.475   11.808   0.002\n",
              "1            60   1.179    1.571   29.536   0.003\n",
              "2            90   2.886    4.303   46.069   0.004\n",
              "3           120   3.940   16.657   69.849   0.003\n",
              "4           150   5.302   13.564   78.481   0.004\n",
              "5           180   6.758   22.722   94.178   0.004"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "yrGTNx3kTNDU",
        "outputId": "c8a83128-d7b4-4d40-bd86-b5d416fbbd2c"
      },
      "source": [
        "summary_table_incremental = get_summary_table(df_skmlflow, running_time_combined_incremental, error_metrics, static_learner=False)\n",
        "save_summary_table(summary_table_incremental, exp2_summary_path,static_learner=False,alternate_batch=False, transpose=True)\n",
        "summary_table_incremental"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Metric</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MAE</th>\n",
              "      <td>97008.805</td>\n",
              "      <td>97124.232</td>\n",
              "      <td>46052.247</td>\n",
              "      <td>139782.778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MAPE</th>\n",
              "      <td>256.524</td>\n",
              "      <td>256.098</td>\n",
              "      <td>113.012</td>\n",
              "      <td>273.883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RMSE</th>\n",
              "      <td>288704.649</td>\n",
              "      <td>288954.911</td>\n",
              "      <td>147885.278</td>\n",
              "      <td>365712.391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time(sec)</th>\n",
              "      <td>3.395</td>\n",
              "      <td>9.882</td>\n",
              "      <td>54.987</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              HT_Reg    HAT_Reg    ARF_Reg     PA_Reg\n",
              "Metric                                               \n",
              "MAE        97008.805  97124.232  46052.247 139782.778\n",
              "MAPE         256.524    256.098    113.012    273.883\n",
              "RMSE      288704.649 288954.911 147885.278 365712.391\n",
              "Time(sec)      3.395      9.882     54.987      0.003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m4-HvnEwA-l"
      },
      "source": [
        "### Static Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "uYtaCPtjjpv5",
        "outputId": "d6ddbf12-b988-4e56-b221-84addc63b0dd"
      },
      "source": [
        "\"\"\"\n",
        "def scikit_learn(df, training_days):\n",
        "  len_countries = len(df['country'].unique())\n",
        "  frames = []\n",
        "  model_predictions = {\n",
        "      'RandomForest':[],\n",
        "      'GradientBoosting':[],\n",
        "      'LinearSVR':[],\n",
        "      'DecisionTree':[],\n",
        "      'BayesianRidge':[],\n",
        "      'LSTM': []\n",
        "      #'MLPRegressor': [],\n",
        "      #'LinearRegression': []\n",
        "    }\n",
        "\n",
        "  # LSTM (TODO: feel free to put the whole LSTM definition in a funtion)\n",
        "  # params (others like epoch and batch size are also hardcoded in train_test_lstm())\n",
        "  layers = [20, 10, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\n",
        "  activations = ['tanh', 'tanh', 'relu']\n",
        "  patience = 20\n",
        "  total_execution_time = []\n",
        "\n",
        "  for day in training_days:\n",
        "\n",
        "    df_subset = create_subset(df,day)\n",
        "    \n",
        "    train_end_day = day * len_countries\n",
        "    test_end_day = (day+30) * len_countries\n",
        "\n",
        "    train = df_subset.iloc[:train_end_day, :] \n",
        "    test = df_subset.iloc[train_end_day:test_end_day, :]  # Testing on set one month ahead only, hence day+30.\n",
        "    test_df, val_df = get_validation_set(test, batch_size=10)  # Getting validation set from test set\n",
        "\n",
        "    # training and test sets for all models except LSTM\n",
        "    X_train, y_train = train.iloc[:,4:-1], train.iloc[:,-1]\n",
        "    X_test, y_test = test.iloc[:,4:-1], test.iloc[:,-1]\n",
        "\n",
        "    # Validation and test set for LSTM\n",
        "    X_test_lstm, y_test_lstm = test_df.iloc[:, 4:-1], test_df.iloc[:, -1]\n",
        "    X_val, y_val = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\n",
        "\n",
        "    X_train_lstm, X_val, X_test_lstm = reshape_dataframe(X_train, X_val, X_test_lstm)  # reshaping the vector for lstm\n",
        "    cur_exec_time = [day]\n",
        "\n",
        "    rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "    model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train,y_train,X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    gb_reg = GradientBoostingRegressor(random_state=0)\n",
        "    model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    lsv_reg = LinearSVR(random_state=0, tol=1e-5)\n",
        "    model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    dt_reg = DecisionTreeRegressor(random_state=0)\n",
        "    model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    br_reg = BayesianRidge()\n",
        "    model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    '''\n",
        "    mlp_reg = MLPRegressor(random_state=1, max_iter=500)\n",
        "    model_predictions['MLPRegressor'], exec_time = train_test_model(mlp_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    lin_reg = LinearRegression()\n",
        "    model_predictions['LinearRegression'], exec_time = train_test_model(lin_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "    '''\n",
        "\n",
        "    lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\n",
        "    model_predictions['LSTM'], exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train, X_val, y_val, X_test_lstm, patience)         \n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "    mdl_evaluation_df = get_scores(y_test,y_test_lstm, model_predictions, day)\n",
        "    total_execution_time.append(cur_exec_time)\n",
        "    frames.append(mdl_evaluation_df)\n",
        "\n",
        "  evaluation_score_df = pd.concat(frames, ignore_index=True)\n",
        "  running_time_df = get_running_time_per_model_static_learner(model_predictions,total_execution_time)\n",
        "  return evaluation_score_df, running_time_df\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef scikit_learn(df, training_days):\\n  len_countries = len(df['country'].unique())\\n  frames = []\\n  model_predictions = {\\n      'RandomForest':[],\\n      'GradientBoosting':[],\\n      'LinearSVR':[],\\n      'DecisionTree':[],\\n      'BayesianRidge':[],\\n      'LSTM': []\\n      #'MLPRegressor': [],\\n      #'LinearRegression': []\\n    }\\n\\n  # LSTM (TODO: feel free to put the whole LSTM definition in a funtion)\\n  # params (others like epoch and batch size are also hardcoded in train_test_lstm())\\n  layers = [20, 10, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\\n  activations = ['tanh', 'tanh', 'relu']\\n  patience = 20\\n  total_execution_time = []\\n\\n  for day in training_days:\\n\\n    df_subset = create_subset(df,day)\\n    \\n    train_end_day = day * len_countries\\n    test_end_day = (day+30) * len_countries\\n\\n    train = df_subset.iloc[:train_end_day, :] \\n    test = df_subset.iloc[train_end_day:test_end_day, :]  # Testing on set one month ahead only, hence day+30.\\n    test_df, val_df = get_validation_set(test, batch_size=10)  # Getting validation set from test set\\n\\n    # training and test sets for all models except LSTM\\n    X_train, y_train = train.iloc[:,4:-1], train.iloc[:,-1]\\n    X_test, y_test = test.iloc[:,4:-1], test.iloc[:,-1]\\n\\n    # Validation and test set for LSTM\\n    X_test_lstm, y_test_lstm = test_df.iloc[:, 4:-1], test_df.iloc[:, -1]\\n    X_val, y_val = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\\n\\n    X_train_lstm, X_val, X_test_lstm = reshape_dataframe(X_train, X_val, X_test_lstm)  # reshaping the vector for lstm\\n    cur_exec_time = [day]\\n\\n    rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\\n    model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train,y_train,X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    gb_reg = GradientBoostingRegressor(random_state=0)\\n    model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    lsv_reg = LinearSVR(random_state=0, tol=1e-5)\\n    model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    dt_reg = DecisionTreeRegressor(random_state=0)\\n    model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    br_reg = BayesianRidge()\\n    model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    '''\\n    mlp_reg = MLPRegressor(random_state=1, max_iter=500)\\n    model_predictions['MLPRegressor'], exec_time = train_test_model(mlp_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    lin_reg = LinearRegression()\\n    model_predictions['LinearRegression'], exec_time = train_test_model(lin_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n    '''\\n\\n    lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\\n    model_predictions['LSTM'], exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train, X_val, y_val, X_test_lstm, patience)         \\n    cur_exec_time.append(exec_time)\\n\\n    mdl_evaluation_df = get_scores(y_test,y_test_lstm, model_predictions, day)\\n    total_execution_time.append(cur_exec_time)\\n    frames.append(mdl_evaluation_df)\\n\\n  evaluation_score_df = pd.concat(frames, ignore_index=True)\\n  running_time_df = get_running_time_per_model_static_learner(model_predictions,total_execution_time)\\n  return evaluation_score_df, running_time_df\\n  \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2oijsU0UJkD"
      },
      "source": [
        "def scikit_learn(df, training_days):\n",
        "  len_countries = len(df['country'].unique())\n",
        "  frames = []\n",
        "  model_predictions = {\n",
        "      'RandomForest':[],\n",
        "      'GradientBoosting':[],\n",
        "      'LinearSVR':[],\n",
        "      'DecisionTree':[],\n",
        "      'BayesianRidge':[],\n",
        "      'LSTM': []\n",
        "      #'MLPRegressor': [],\n",
        "      #'LinearRegression': []\n",
        "    }\n",
        "  total_execution_time = []\n",
        "\n",
        "  # LSTM (TODO: feel free to put the whole LSTM definition in a funtion)\n",
        "  # params (others like epoch and batch size are also hardcoded in train_test_lstm())\n",
        "  layers = [50, 30, 20, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\n",
        "  activations = ['tanh', 'tanh', 'relu']\n",
        "  epochs = 500\n",
        "  patience = 20 * num_selected_countries\n",
        "  batch_size_lstm = 10 * num_selected_countries\n",
        "  \n",
        "\n",
        "  for day in training_days:\n",
        "\n",
        "    df_subset = create_subset(df,day)\n",
        "    \n",
        "    train_end_day = day * len_countries\n",
        "    test_end_day = (day+30) * len_countries\n",
        "\n",
        "    train = df_subset.iloc[:train_end_day, :] \n",
        "    test = df_subset.iloc[train_end_day:test_end_day, :]  # Testing on set one month ahead only, hence day+30.\n",
        "    cur_exec_time = [day]\n",
        "\n",
        "    # training and test sets for all models except LSTM\n",
        "    X_train, y_train = train.iloc[:,4:-1], train.iloc[:,-1]\n",
        "    X_test, y_test = test.iloc[:,4:-1], test.iloc[:,-1]\n",
        "\n",
        "    # Seperating validation set from train set\n",
        "    train_df, val_df = get_validation_set(train, batch_size=10)\n",
        "\n",
        "    # Splitting test and validation into dependent and independent sets\n",
        "    X_train_batch, y_train_batch = train_df.iloc[:, 4:-1], train_df.iloc[:, -1]  # Consist only odd batches\n",
        "    X_val_batch, y_val_batch = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\n",
        "\n",
        "    # Normalizing dataset\n",
        "    X_train_lstm_norm, X_test_lstm_norm, X_val_lstm_norm = normalize_dataset(X_train_batch, X_test, X_val_batch)\n",
        "\n",
        "    # Reshaping the dataframes\n",
        "    X_train_lstm, X_val_lstm, X_test_lstm = reshape_dataframe(X_train_lstm_norm, X_val_lstm_norm, X_test_lstm_norm)\n",
        "   \n",
        "\n",
        "    rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "    model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train,y_train,X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    gb_reg = GradientBoostingRegressor(random_state=0)\n",
        "    model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    lsv_reg = LinearSVR(random_state=0, tol=1e-5)\n",
        "    model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    dt_reg = DecisionTreeRegressor(random_state=0)\n",
        "    model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    br_reg = BayesianRidge()\n",
        "    model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\n",
        "    model_predictions['LSTM'],exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train_batch, X_val_lstm, y_val_batch, X_test_lstm, patience, epochs, batch_size_lstm)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    mdl_evaluation_df = get_scores(y_test, model_predictions, day)\n",
        "    total_execution_time.append(cur_exec_time)\n",
        "    frames.append(mdl_evaluation_df)\n",
        "\n",
        "  evaluation_score_df = pd.concat(frames, ignore_index=True)\n",
        "  running_time_df = get_running_time_per_model_static_learner(model_predictions,total_execution_time)\n",
        "  return evaluation_score_df, running_time_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inEIGOoYn0ng",
        "outputId": "6db48ce0-6b13-46a5-f780-f44bf1de6afe"
      },
      "source": [
        "result_sklearn, running_time_static = scikit_learn(result, pretrain_days)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 380 samples, validate on 370 samples\n",
            "Epoch 1/500\n",
            "380/380 [==============================] - 2s 5ms/step - loss: 2700.3827 - mse: 37416908.0000 - mae: 2700.3828 - val_loss: 2149.9685 - val_mse: 29119166.0000 - val_mae: 2149.9685\n",
            "Epoch 2/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2700.3526 - mse: 37416764.0000 - mae: 2700.3525 - val_loss: 2149.9390 - val_mse: 29119056.0000 - val_mae: 2149.9390\n",
            "Epoch 3/500\n",
            "380/380 [==============================] - 0s 70us/step - loss: 2700.3208 - mse: 37416608.0000 - mae: 2700.3208 - val_loss: 2149.8889 - val_mse: 29118920.0000 - val_mae: 2149.8889\n",
            "Epoch 4/500\n",
            "380/380 [==============================] - 0s 72us/step - loss: 2700.2522 - mse: 37416396.0000 - mae: 2700.2522 - val_loss: 2149.7351 - val_mse: 29118594.0000 - val_mae: 2149.7351\n",
            "Epoch 5/500\n",
            "380/380 [==============================] - 0s 74us/step - loss: 2700.0078 - mse: 37415712.0000 - mae: 2700.0078 - val_loss: 2149.1125 - val_mse: 29117388.0000 - val_mae: 2149.1125\n",
            "Epoch 6/500\n",
            "380/380 [==============================] - 0s 68us/step - loss: 2699.0423 - mse: 37413192.0000 - mae: 2699.0425 - val_loss: 2147.3426 - val_mse: 29113900.0000 - val_mae: 2147.3425\n",
            "Epoch 7/500\n",
            "380/380 [==============================] - 0s 73us/step - loss: 2696.7399 - mse: 37406756.0000 - mae: 2696.7400 - val_loss: 2144.9561 - val_mse: 29108900.0000 - val_mae: 2144.9561\n",
            "Epoch 8/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2694.0849 - mse: 37399692.0000 - mae: 2694.0850 - val_loss: 2142.3119 - val_mse: 29103200.0000 - val_mae: 2142.3120\n",
            "Epoch 9/500\n",
            "380/380 [==============================] - 0s 73us/step - loss: 2690.8413 - mse: 37387764.0000 - mae: 2690.8416 - val_loss: 2139.5010 - val_mse: 29097026.0000 - val_mae: 2139.5010\n",
            "Epoch 10/500\n",
            "380/380 [==============================] - 0s 73us/step - loss: 2687.5560 - mse: 37374932.0000 - mae: 2687.5559 - val_loss: 2136.5073 - val_mse: 29090290.0000 - val_mae: 2136.5073\n",
            "Epoch 11/500\n",
            "380/380 [==============================] - 0s 81us/step - loss: 2683.7457 - mse: 37363240.0000 - mae: 2683.7456 - val_loss: 2133.1999 - val_mse: 29082756.0000 - val_mae: 2133.2000\n",
            "Epoch 12/500\n",
            "380/380 [==============================] - 0s 75us/step - loss: 2679.2291 - mse: 37347576.0000 - mae: 2679.2290 - val_loss: 2129.8055 - val_mse: 29074586.0000 - val_mae: 2129.8054\n",
            "Epoch 13/500\n",
            "380/380 [==============================] - 0s 78us/step - loss: 2676.8231 - mse: 37333920.0000 - mae: 2676.8230 - val_loss: 2126.4049 - val_mse: 29066090.0000 - val_mae: 2126.4048\n",
            "Epoch 14/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2671.3079 - mse: 37310480.0000 - mae: 2671.3079 - val_loss: 2122.8025 - val_mse: 29056556.0000 - val_mae: 2122.8025\n",
            "Epoch 15/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2667.6693 - mse: 37299768.0000 - mae: 2667.6694 - val_loss: 2119.1935 - val_mse: 29046770.0000 - val_mae: 2119.1934\n",
            "Epoch 16/500\n",
            "380/380 [==============================] - 0s 87us/step - loss: 2662.0168 - mse: 37272160.0000 - mae: 2662.0168 - val_loss: 2115.4171 - val_mse: 29035586.0000 - val_mae: 2115.4172\n",
            "Epoch 17/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2656.1313 - mse: 37239200.0000 - mae: 2656.1313 - val_loss: 2111.3250 - val_mse: 29023410.0000 - val_mae: 2111.3250\n",
            "Epoch 18/500\n",
            "380/380 [==============================] - 0s 72us/step - loss: 2651.2179 - mse: 37221712.0000 - mae: 2651.2180 - val_loss: 2107.1288 - val_mse: 29010576.0000 - val_mae: 2107.1287\n",
            "Epoch 19/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2646.4045 - mse: 37197912.0000 - mae: 2646.4045 - val_loss: 2102.7471 - val_mse: 28996226.0000 - val_mae: 2102.7471\n",
            "Epoch 20/500\n",
            "380/380 [==============================] - 0s 101us/step - loss: 2641.3461 - mse: 37179684.0000 - mae: 2641.3459 - val_loss: 2098.2682 - val_mse: 28981852.0000 - val_mae: 2098.2683\n",
            "Epoch 21/500\n",
            "380/380 [==============================] - 0s 89us/step - loss: 2636.1727 - mse: 37156044.0000 - mae: 2636.1726 - val_loss: 2093.4214 - val_mse: 28965342.0000 - val_mae: 2093.4214\n",
            "Epoch 22/500\n",
            "380/380 [==============================] - 0s 87us/step - loss: 2632.1163 - mse: 37114992.0000 - mae: 2632.1162 - val_loss: 2088.4636 - val_mse: 28948432.0000 - val_mae: 2088.4636\n",
            "Epoch 23/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2622.0325 - mse: 37089028.0000 - mae: 2622.0325 - val_loss: 2083.1176 - val_mse: 28929668.0000 - val_mae: 2083.1177\n",
            "Epoch 24/500\n",
            "380/380 [==============================] - 0s 76us/step - loss: 2620.2341 - mse: 37079004.0000 - mae: 2620.2341 - val_loss: 2077.9924 - val_mse: 28911102.0000 - val_mae: 2077.9924\n",
            "Epoch 25/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2612.6688 - mse: 37028628.0000 - mae: 2612.6689 - val_loss: 2072.5468 - val_mse: 28890438.0000 - val_mae: 2072.5469\n",
            "Epoch 26/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2604.7264 - mse: 37025184.0000 - mae: 2604.7263 - val_loss: 2067.0973 - val_mse: 28867942.0000 - val_mae: 2067.0974\n",
            "Epoch 27/500\n",
            "380/380 [==============================] - 0s 92us/step - loss: 2598.0885 - mse: 36967208.0000 - mae: 2598.0884 - val_loss: 2061.4203 - val_mse: 28843606.0000 - val_mae: 2061.4202\n",
            "Epoch 28/500\n",
            "380/380 [==============================] - 0s 88us/step - loss: 2590.1259 - mse: 36942072.0000 - mae: 2590.1257 - val_loss: 2055.7795 - val_mse: 28817644.0000 - val_mae: 2055.7793\n",
            "Epoch 29/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2585.5745 - mse: 36914504.0000 - mae: 2585.5745 - val_loss: 2050.4853 - val_mse: 28790692.0000 - val_mae: 2050.4854\n",
            "Epoch 30/500\n",
            "380/380 [==============================] - 0s 78us/step - loss: 2578.5906 - mse: 36885224.0000 - mae: 2578.5906 - val_loss: 2044.8625 - val_mse: 28761116.0000 - val_mae: 2044.8625\n",
            "Epoch 31/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2568.5827 - mse: 36840220.0000 - mae: 2568.5830 - val_loss: 2038.6984 - val_mse: 28727370.0000 - val_mae: 2038.6984\n",
            "Epoch 32/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2564.7493 - mse: 36824276.0000 - mae: 2564.7493 - val_loss: 2033.1631 - val_mse: 28692920.0000 - val_mae: 2033.1632\n",
            "Epoch 33/500\n",
            "380/380 [==============================] - 0s 74us/step - loss: 2555.9404 - mse: 36783812.0000 - mae: 2555.9404 - val_loss: 2027.2862 - val_mse: 28655618.0000 - val_mae: 2027.2861\n",
            "Epoch 34/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2547.4486 - mse: 36747696.0000 - mae: 2547.4487 - val_loss: 2021.8221 - val_mse: 28616234.0000 - val_mae: 2021.8221\n",
            "Epoch 35/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2544.8839 - mse: 36720536.0000 - mae: 2544.8838 - val_loss: 2016.7489 - val_mse: 28579688.0000 - val_mae: 2016.7488\n",
            "Epoch 36/500\n",
            "380/380 [==============================] - 0s 91us/step - loss: 2537.4432 - mse: 36639740.0000 - mae: 2537.4431 - val_loss: 2011.9347 - val_mse: 28541308.0000 - val_mae: 2011.9346\n",
            "Epoch 37/500\n",
            "380/380 [==============================] - 0s 84us/step - loss: 2531.2814 - mse: 36637528.0000 - mae: 2531.2812 - val_loss: 2007.2235 - val_mse: 28501354.0000 - val_mae: 2007.2235\n",
            "Epoch 38/500\n",
            "380/380 [==============================] - 0s 97us/step - loss: 2528.2887 - mse: 36593552.0000 - mae: 2528.2888 - val_loss: 2002.6473 - val_mse: 28461820.0000 - val_mae: 2002.6473\n",
            "Epoch 39/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2520.9983 - mse: 36542964.0000 - mae: 2520.9983 - val_loss: 1997.9503 - val_mse: 28419664.0000 - val_mae: 1997.9503\n",
            "Epoch 40/500\n",
            "380/380 [==============================] - 0s 84us/step - loss: 2518.1105 - mse: 36563288.0000 - mae: 2518.1106 - val_loss: 1993.8424 - val_mse: 28381040.0000 - val_mae: 1993.8424\n",
            "Epoch 41/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2513.5075 - mse: 36492836.0000 - mae: 2513.5073 - val_loss: 1989.5453 - val_mse: 28339604.0000 - val_mae: 1989.5453\n",
            "Epoch 42/500\n",
            "380/380 [==============================] - 0s 77us/step - loss: 2506.4793 - mse: 36478104.0000 - mae: 2506.4792 - val_loss: 1985.8122 - val_mse: 28302090.0000 - val_mae: 1985.8124\n",
            "Epoch 43/500\n",
            "380/380 [==============================] - 0s 93us/step - loss: 2504.6046 - mse: 36392340.0000 - mae: 2504.6045 - val_loss: 1982.2250 - val_mse: 28263912.0000 - val_mae: 1982.2250\n",
            "Epoch 44/500\n",
            "380/380 [==============================] - 0s 84us/step - loss: 2511.7348 - mse: 36409380.0000 - mae: 2511.7349 - val_loss: 1979.0922 - val_mse: 28227540.0000 - val_mae: 1979.0923\n",
            "Epoch 45/500\n",
            "380/380 [==============================] - 0s 74us/step - loss: 2496.9597 - mse: 36345156.0000 - mae: 2496.9597 - val_loss: 1976.0440 - val_mse: 28189806.0000 - val_mae: 1976.0441\n",
            "Epoch 46/500\n",
            "380/380 [==============================] - 0s 73us/step - loss: 2494.1494 - mse: 36264520.0000 - mae: 2494.1494 - val_loss: 1973.1240 - val_mse: 28148248.0000 - val_mae: 1973.1240\n",
            "Epoch 47/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2488.3193 - mse: 36326224.0000 - mae: 2488.3193 - val_loss: 1970.7955 - val_mse: 28110798.0000 - val_mae: 1970.7957\n",
            "Epoch 48/500\n",
            "380/380 [==============================] - 0s 89us/step - loss: 2484.1041 - mse: 36220512.0000 - mae: 2484.1040 - val_loss: 1968.7977 - val_mse: 28076486.0000 - val_mae: 1968.7979\n",
            "Epoch 49/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2475.5404 - mse: 36213380.0000 - mae: 2475.5405 - val_loss: 1966.8572 - val_mse: 28042040.0000 - val_mae: 1966.8571\n",
            "Epoch 50/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2483.9441 - mse: 36208428.0000 - mae: 2483.9441 - val_loss: 1964.9536 - val_mse: 28007400.0000 - val_mae: 1964.9535\n",
            "Epoch 51/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2477.9881 - mse: 36083188.0000 - mae: 2477.9880 - val_loss: 1963.0570 - val_mse: 27971528.0000 - val_mae: 1963.0571\n",
            "Epoch 52/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2473.5876 - mse: 36115940.0000 - mae: 2473.5874 - val_loss: 1961.2004 - val_mse: 27933604.0000 - val_mae: 1961.2003\n",
            "Epoch 53/500\n",
            "380/380 [==============================] - 0s 76us/step - loss: 2465.4742 - mse: 36047020.0000 - mae: 2465.4744 - val_loss: 1959.2146 - val_mse: 27893618.0000 - val_mae: 1959.2145\n",
            "Epoch 54/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2471.8622 - mse: 36033136.0000 - mae: 2471.8621 - val_loss: 1957.5580 - val_mse: 27858704.0000 - val_mae: 1957.5580\n",
            "Epoch 55/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2461.8728 - mse: 35979940.0000 - mae: 2461.8728 - val_loss: 1956.1380 - val_mse: 27825332.0000 - val_mae: 1956.1381\n",
            "Epoch 56/500\n",
            "380/380 [==============================] - 0s 88us/step - loss: 2451.9147 - mse: 35965072.0000 - mae: 2451.9148 - val_loss: 1954.8564 - val_mse: 27785390.0000 - val_mae: 1954.8564\n",
            "Epoch 57/500\n",
            "380/380 [==============================] - 0s 80us/step - loss: 2464.0377 - mse: 35934892.0000 - mae: 2464.0378 - val_loss: 1953.8424 - val_mse: 27756334.0000 - val_mae: 1953.8424\n",
            "Epoch 58/500\n",
            "380/380 [==============================] - 0s 93us/step - loss: 2457.2007 - mse: 35907256.0000 - mae: 2457.2007 - val_loss: 1952.8413 - val_mse: 27723804.0000 - val_mae: 1952.8412\n",
            "Epoch 59/500\n",
            "380/380 [==============================] - 0s 78us/step - loss: 2460.5976 - mse: 35891388.0000 - mae: 2460.5977 - val_loss: 1951.8980 - val_mse: 27695906.0000 - val_mae: 1951.8979\n",
            "Epoch 60/500\n",
            "380/380 [==============================] - 0s 91us/step - loss: 2449.3119 - mse: 35850716.0000 - mae: 2449.3118 - val_loss: 1951.0770 - val_mse: 27662054.0000 - val_mae: 1951.0770\n",
            "Epoch 61/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2444.3074 - mse: 35779728.0000 - mae: 2444.3076 - val_loss: 1950.3692 - val_mse: 27627078.0000 - val_mae: 1950.3693\n",
            "Epoch 62/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2448.4422 - mse: 35844168.0000 - mae: 2448.4424 - val_loss: 1949.7960 - val_mse: 27601598.0000 - val_mae: 1949.7959\n",
            "Epoch 63/500\n",
            "380/380 [==============================] - 0s 78us/step - loss: 2450.4202 - mse: 35670640.0000 - mae: 2450.4202 - val_loss: 1949.2833 - val_mse: 27578020.0000 - val_mae: 1949.2833\n",
            "Epoch 64/500\n",
            "380/380 [==============================] - 0s 69us/step - loss: 2439.3276 - mse: 35695300.0000 - mae: 2439.3276 - val_loss: 1948.8800 - val_mse: 27548306.0000 - val_mae: 1948.8799\n",
            "Epoch 65/500\n",
            "380/380 [==============================] - 0s 92us/step - loss: 2434.4434 - mse: 35658788.0000 - mae: 2434.4434 - val_loss: 1948.4399 - val_mse: 27523774.0000 - val_mae: 1948.4398\n",
            "Epoch 66/500\n",
            "380/380 [==============================] - 0s 86us/step - loss: 2436.6167 - mse: 35647828.0000 - mae: 2436.6167 - val_loss: 1947.9489 - val_mse: 27497784.0000 - val_mae: 1947.9490\n",
            "Epoch 67/500\n",
            "380/380 [==============================] - 0s 77us/step - loss: 2432.1736 - mse: 35578956.0000 - mae: 2432.1736 - val_loss: 1947.5477 - val_mse: 27467270.0000 - val_mae: 1947.5476\n",
            "Epoch 68/500\n",
            "380/380 [==============================] - 0s 80us/step - loss: 2424.6749 - mse: 35525452.0000 - mae: 2424.6748 - val_loss: 1947.2634 - val_mse: 27431964.0000 - val_mae: 1947.2633\n",
            "Epoch 69/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2435.4722 - mse: 35535084.0000 - mae: 2435.4724 - val_loss: 1946.8725 - val_mse: 27401632.0000 - val_mae: 1946.8727\n",
            "Epoch 70/500\n",
            "380/380 [==============================] - 0s 77us/step - loss: 2433.4429 - mse: 35419632.0000 - mae: 2433.4429 - val_loss: 1946.4602 - val_mse: 27375600.0000 - val_mae: 1946.4601\n",
            "Epoch 71/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2424.6119 - mse: 35415072.0000 - mae: 2424.6118 - val_loss: 1946.1837 - val_mse: 27348128.0000 - val_mae: 1946.1836\n",
            "Epoch 72/500\n",
            "380/380 [==============================] - 0s 89us/step - loss: 2425.8453 - mse: 35436700.0000 - mae: 2425.8455 - val_loss: 1946.0072 - val_mse: 27318054.0000 - val_mae: 1946.0073\n",
            "Epoch 73/500\n",
            "380/380 [==============================] - 0s 88us/step - loss: 2430.1619 - mse: 35317796.0000 - mae: 2430.1621 - val_loss: 1945.7576 - val_mse: 27293378.0000 - val_mae: 1945.7576\n",
            "Epoch 74/500\n",
            "380/380 [==============================] - 0s 86us/step - loss: 2422.0916 - mse: 35305932.0000 - mae: 2422.0916 - val_loss: 1945.5781 - val_mse: 27270166.0000 - val_mae: 1945.5782\n",
            "Epoch 75/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2432.4771 - mse: 35304848.0000 - mae: 2432.4771 - val_loss: 1945.3862 - val_mse: 27247038.0000 - val_mae: 1945.3861\n",
            "Epoch 76/500\n",
            "380/380 [==============================] - 0s 81us/step - loss: 2424.6722 - mse: 35312004.0000 - mae: 2424.6721 - val_loss: 1945.1288 - val_mse: 27225472.0000 - val_mae: 1945.1287\n",
            "Epoch 77/500\n",
            "380/380 [==============================] - 0s 89us/step - loss: 2419.6653 - mse: 35198616.0000 - mae: 2419.6653 - val_loss: 1944.9248 - val_mse: 27199956.0000 - val_mae: 1944.9248\n",
            "Epoch 78/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2413.6404 - mse: 35312380.0000 - mae: 2413.6404 - val_loss: 1944.8298 - val_mse: 27168270.0000 - val_mae: 1944.8300\n",
            "Epoch 79/500\n",
            "380/380 [==============================] - 0s 75us/step - loss: 2405.2273 - mse: 35176156.0000 - mae: 2405.2273 - val_loss: 1944.7373 - val_mse: 27140780.0000 - val_mae: 1944.7373\n",
            "Epoch 80/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2421.2144 - mse: 35222264.0000 - mae: 2421.2144 - val_loss: 1944.6294 - val_mse: 27119700.0000 - val_mae: 1944.6294\n",
            "Epoch 81/500\n",
            "380/380 [==============================] - 0s 76us/step - loss: 2414.9612 - mse: 35178948.0000 - mae: 2414.9612 - val_loss: 1944.5878 - val_mse: 27096660.0000 - val_mae: 1944.5879\n",
            "Epoch 82/500\n",
            "380/380 [==============================] - 0s 94us/step - loss: 2401.3257 - mse: 34979264.0000 - mae: 2401.3257 - val_loss: 1944.5613 - val_mse: 27073098.0000 - val_mae: 1944.5613\n",
            "Epoch 83/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2410.1322 - mse: 35019872.0000 - mae: 2410.1323 - val_loss: 1944.4120 - val_mse: 27056504.0000 - val_mae: 1944.4120\n",
            "Epoch 84/500\n",
            "380/380 [==============================] - 0s 96us/step - loss: 2405.2278 - mse: 35117956.0000 - mae: 2405.2275 - val_loss: 1944.3563 - val_mse: 27034558.0000 - val_mae: 1944.3562\n",
            "Epoch 85/500\n",
            "380/380 [==============================] - 0s 93us/step - loss: 2408.6026 - mse: 34999672.0000 - mae: 2408.6025 - val_loss: 1944.2763 - val_mse: 27016518.0000 - val_mae: 1944.2764\n",
            "Epoch 86/500\n",
            "380/380 [==============================] - 0s 79us/step - loss: 2397.3235 - mse: 35036824.0000 - mae: 2397.3235 - val_loss: 1944.4010 - val_mse: 26990802.0000 - val_mae: 1944.4010\n",
            "Epoch 87/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2410.9439 - mse: 34839712.0000 - mae: 2410.9438 - val_loss: 1944.2590 - val_mse: 26976782.0000 - val_mae: 1944.2589\n",
            "Epoch 88/500\n",
            "380/380 [==============================] - 0s 101us/step - loss: 2401.9754 - mse: 34977344.0000 - mae: 2401.9753 - val_loss: 1944.1347 - val_mse: 26960932.0000 - val_mae: 1944.1348\n",
            "Epoch 89/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2405.3807 - mse: 34871312.0000 - mae: 2405.3809 - val_loss: 1944.0644 - val_mse: 26944092.0000 - val_mae: 1944.0646\n",
            "Epoch 90/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2401.8294 - mse: 34949540.0000 - mae: 2401.8293 - val_loss: 1943.9144 - val_mse: 26931664.0000 - val_mae: 1943.9146\n",
            "Epoch 91/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2396.3967 - mse: 34817972.0000 - mae: 2396.3967 - val_loss: 1943.8112 - val_mse: 26919466.0000 - val_mae: 1943.8112\n",
            "Epoch 92/500\n",
            "380/380 [==============================] - 0s 92us/step - loss: 2387.4074 - mse: 34954772.0000 - mae: 2387.4072 - val_loss: 1943.7726 - val_mse: 26902288.0000 - val_mae: 1943.7726\n",
            "Epoch 93/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2390.1651 - mse: 34771116.0000 - mae: 2390.1650 - val_loss: 1943.8036 - val_mse: 26883870.0000 - val_mae: 1943.8037\n",
            "Epoch 94/500\n",
            "380/380 [==============================] - 0s 75us/step - loss: 2393.2271 - mse: 34818136.0000 - mae: 2393.2271 - val_loss: 1943.6021 - val_mse: 26873164.0000 - val_mae: 1943.6021\n",
            "Epoch 95/500\n",
            "380/380 [==============================] - 0s 88us/step - loss: 2409.5717 - mse: 34788368.0000 - mae: 2409.5718 - val_loss: 1943.5142 - val_mse: 26858100.0000 - val_mae: 1943.5142\n",
            "Epoch 96/500\n",
            "380/380 [==============================] - 0s 87us/step - loss: 2407.6468 - mse: 34813972.0000 - mae: 2407.6467 - val_loss: 1943.2985 - val_mse: 26848344.0000 - val_mae: 1943.2985\n",
            "Epoch 97/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2389.9127 - mse: 34852940.0000 - mae: 2389.9128 - val_loss: 1943.3440 - val_mse: 26829132.0000 - val_mae: 1943.3439\n",
            "Epoch 98/500\n",
            "380/380 [==============================] - 0s 84us/step - loss: 2383.8723 - mse: 34722532.0000 - mae: 2383.8723 - val_loss: 1943.2488 - val_mse: 26813864.0000 - val_mae: 1943.2488\n",
            "Epoch 99/500\n",
            "380/380 [==============================] - 0s 94us/step - loss: 2395.8394 - mse: 34713836.0000 - mae: 2395.8394 - val_loss: 1943.1689 - val_mse: 26796618.0000 - val_mae: 1943.1689\n",
            "Epoch 100/500\n",
            "380/380 [==============================] - 0s 94us/step - loss: 2393.3387 - mse: 34674624.0000 - mae: 2393.3389 - val_loss: 1943.0314 - val_mse: 26784760.0000 - val_mae: 1943.0312\n",
            "Epoch 101/500\n",
            "380/380 [==============================] - 0s 93us/step - loss: 2383.4080 - mse: 34733232.0000 - mae: 2383.4080 - val_loss: 1943.0166 - val_mse: 26766878.0000 - val_mae: 1943.0167\n",
            "Epoch 102/500\n",
            "380/380 [==============================] - 0s 92us/step - loss: 2402.9171 - mse: 34761200.0000 - mae: 2402.9170 - val_loss: 1942.7241 - val_mse: 26758762.0000 - val_mae: 1942.7241\n",
            "Epoch 103/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2382.6819 - mse: 34608372.0000 - mae: 2382.6819 - val_loss: 1942.5332 - val_mse: 26747062.0000 - val_mae: 1942.5333\n",
            "Epoch 104/500\n",
            "380/380 [==============================] - 0s 92us/step - loss: 2389.8829 - mse: 34548888.0000 - mae: 2389.8828 - val_loss: 1942.3366 - val_mse: 26734770.0000 - val_mae: 1942.3367\n",
            "Epoch 105/500\n",
            "380/380 [==============================] - 0s 89us/step - loss: 2380.6001 - mse: 34584796.0000 - mae: 2380.6001 - val_loss: 1942.1542 - val_mse: 26719382.0000 - val_mae: 1942.1542\n",
            "Epoch 106/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2376.2717 - mse: 34371732.0000 - mae: 2376.2717 - val_loss: 1941.9176 - val_mse: 26706568.0000 - val_mae: 1941.9177\n",
            "Epoch 107/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2397.3797 - mse: 34523788.0000 - mae: 2397.3799 - val_loss: 1941.7086 - val_mse: 26693900.0000 - val_mae: 1941.7086\n",
            "Epoch 108/500\n",
            "380/380 [==============================] - 0s 86us/step - loss: 2381.0802 - mse: 34396544.0000 - mae: 2381.0803 - val_loss: 1941.4681 - val_mse: 26685490.0000 - val_mae: 1941.4680\n",
            "Epoch 109/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2382.4461 - mse: 34435460.0000 - mae: 2382.4460 - val_loss: 1941.3141 - val_mse: 26668472.0000 - val_mae: 1941.3142\n",
            "Epoch 110/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2388.1343 - mse: 34499968.0000 - mae: 2388.1343 - val_loss: 1941.0050 - val_mse: 26659258.0000 - val_mae: 1941.0051\n",
            "Epoch 111/500\n",
            "380/380 [==============================] - 0s 87us/step - loss: 2376.7762 - mse: 34458148.0000 - mae: 2376.7764 - val_loss: 1940.7934 - val_mse: 26644430.0000 - val_mae: 1940.7935\n",
            "Epoch 112/500\n",
            "380/380 [==============================] - 0s 75us/step - loss: 2373.4775 - mse: 34361620.0000 - mae: 2373.4775 - val_loss: 1940.5810 - val_mse: 26627990.0000 - val_mae: 1940.5809\n",
            "Epoch 113/500\n",
            "380/380 [==============================] - 0s 91us/step - loss: 2387.4214 - mse: 34480348.0000 - mae: 2387.4214 - val_loss: 1940.3938 - val_mse: 26612626.0000 - val_mae: 1940.3939\n",
            "Epoch 114/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2392.4650 - mse: 34460024.0000 - mae: 2392.4651 - val_loss: 1939.8544 - val_mse: 26606728.0000 - val_mae: 1939.8544\n",
            "Epoch 115/500\n",
            "380/380 [==============================] - 0s 116us/step - loss: 2388.0777 - mse: 34520644.0000 - mae: 2388.0776 - val_loss: 1939.4642 - val_mse: 26600578.0000 - val_mae: 1939.4642\n",
            "Epoch 116/500\n",
            "380/380 [==============================] - 0s 101us/step - loss: 2373.6596 - mse: 34297100.0000 - mae: 2373.6594 - val_loss: 1938.9658 - val_mse: 26597556.0000 - val_mae: 1938.9658\n",
            "Epoch 117/500\n",
            "380/380 [==============================] - 0s 86us/step - loss: 2388.2498 - mse: 34426992.0000 - mae: 2388.2498 - val_loss: 1938.4962 - val_mse: 26598826.0000 - val_mae: 1938.4961\n",
            "Epoch 118/500\n",
            "380/380 [==============================] - 0s 78us/step - loss: 2374.2237 - mse: 34401188.0000 - mae: 2374.2236 - val_loss: 1938.1023 - val_mse: 26595900.0000 - val_mae: 1938.1022\n",
            "Epoch 119/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2384.3206 - mse: 34303344.0000 - mae: 2384.3206 - val_loss: 1937.8055 - val_mse: 26586314.0000 - val_mae: 1937.8055\n",
            "Epoch 120/500\n",
            "380/380 [==============================] - 0s 89us/step - loss: 2376.2524 - mse: 34287636.0000 - mae: 2376.2524 - val_loss: 1937.5553 - val_mse: 26578346.0000 - val_mae: 1937.5552\n",
            "Epoch 121/500\n",
            "380/380 [==============================] - 0s 101us/step - loss: 2371.6392 - mse: 34301236.0000 - mae: 2371.6394 - val_loss: 1937.4785 - val_mse: 26560116.0000 - val_mae: 1937.4784\n",
            "Epoch 122/500\n",
            "380/380 [==============================] - 0s 101us/step - loss: 2371.6602 - mse: 34285768.0000 - mae: 2371.6602 - val_loss: 1937.3676 - val_mse: 26548288.0000 - val_mae: 1937.3676\n",
            "Epoch 123/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2383.7481 - mse: 34253152.0000 - mae: 2383.7480 - val_loss: 1936.9129 - val_mse: 26547770.0000 - val_mae: 1936.9130\n",
            "Epoch 124/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2372.9128 - mse: 34251212.0000 - mae: 2372.9128 - val_loss: 1936.6414 - val_mse: 26539320.0000 - val_mae: 1936.6416\n",
            "Epoch 125/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2367.3785 - mse: 34193780.0000 - mae: 2367.3787 - val_loss: 1936.6100 - val_mse: 26523772.0000 - val_mae: 1936.6100\n",
            "Epoch 126/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2369.0292 - mse: 34217864.0000 - mae: 2369.0293 - val_loss: 1936.5777 - val_mse: 26512868.0000 - val_mae: 1936.5778\n",
            "Epoch 127/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2379.6443 - mse: 34225432.0000 - mae: 2379.6443 - val_loss: 1936.5390 - val_mse: 26501584.0000 - val_mae: 1936.5391\n",
            "Epoch 128/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2368.6970 - mse: 34267508.0000 - mae: 2368.6970 - val_loss: 1936.5568 - val_mse: 26488740.0000 - val_mae: 1936.5569\n",
            "Epoch 129/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2379.5157 - mse: 34264480.0000 - mae: 2379.5159 - val_loss: 1936.3665 - val_mse: 26486184.0000 - val_mae: 1936.3666\n",
            "Epoch 130/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2362.0463 - mse: 34204060.0000 - mae: 2362.0464 - val_loss: 1936.3755 - val_mse: 26476372.0000 - val_mae: 1936.3755\n",
            "Epoch 131/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2367.4886 - mse: 34028044.0000 - mae: 2367.4888 - val_loss: 1936.4997 - val_mse: 26459714.0000 - val_mae: 1936.4996\n",
            "Epoch 132/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2355.9629 - mse: 34055400.0000 - mae: 2355.9629 - val_loss: 1936.4247 - val_mse: 26453740.0000 - val_mae: 1936.4247\n",
            "Epoch 133/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2364.5453 - mse: 34064304.0000 - mae: 2364.5454 - val_loss: 1936.0985 - val_mse: 26452950.0000 - val_mae: 1936.0985\n",
            "Epoch 134/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2376.1954 - mse: 34294448.0000 - mae: 2376.1953 - val_loss: 1936.0113 - val_mse: 26444376.0000 - val_mae: 1936.0114\n",
            "Epoch 135/500\n",
            "380/380 [==============================] - 0s 97us/step - loss: 2353.2959 - mse: 33867600.0000 - mae: 2353.2957 - val_loss: 1936.0752 - val_mse: 26429374.0000 - val_mae: 1936.0752\n",
            "Epoch 136/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2372.0758 - mse: 34190380.0000 - mae: 2372.0757 - val_loss: 1935.7224 - val_mse: 26427618.0000 - val_mae: 1935.7224\n",
            "Epoch 137/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2372.0155 - mse: 34143928.0000 - mae: 2372.0156 - val_loss: 1935.7604 - val_mse: 26415462.0000 - val_mae: 1935.7605\n",
            "Epoch 138/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2363.5845 - mse: 33979444.0000 - mae: 2363.5845 - val_loss: 1935.4186 - val_mse: 26415142.0000 - val_mae: 1935.4186\n",
            "Epoch 139/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2349.9684 - mse: 34047812.0000 - mae: 2349.9685 - val_loss: 1935.2953 - val_mse: 26408298.0000 - val_mae: 1935.2953\n",
            "Epoch 140/500\n",
            "380/380 [==============================] - 0s 122us/step - loss: 2356.9356 - mse: 33950236.0000 - mae: 2356.9355 - val_loss: 1935.2214 - val_mse: 26398120.0000 - val_mae: 1935.2214\n",
            "Epoch 141/500\n",
            "380/380 [==============================] - 0s 94us/step - loss: 2369.4837 - mse: 33930936.0000 - mae: 2369.4839 - val_loss: 1935.2173 - val_mse: 26386014.0000 - val_mae: 1935.2174\n",
            "Epoch 142/500\n",
            "380/380 [==============================] - 0s 94us/step - loss: 2370.5099 - mse: 33969612.0000 - mae: 2370.5098 - val_loss: 1935.0951 - val_mse: 26379720.0000 - val_mae: 1935.0951\n",
            "Epoch 143/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2362.1863 - mse: 34084412.0000 - mae: 2362.1863 - val_loss: 1934.7367 - val_mse: 26381124.0000 - val_mae: 1934.7367\n",
            "Epoch 144/500\n",
            "380/380 [==============================] - 0s 102us/step - loss: 2359.0746 - mse: 33995992.0000 - mae: 2359.0747 - val_loss: 1934.6866 - val_mse: 26372066.0000 - val_mae: 1934.6866\n",
            "Epoch 145/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2371.9700 - mse: 34131856.0000 - mae: 2371.9700 - val_loss: 1934.7985 - val_mse: 26359114.0000 - val_mae: 1934.7987\n",
            "Epoch 146/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2362.3873 - mse: 33947320.0000 - mae: 2362.3875 - val_loss: 1934.6433 - val_mse: 26358568.0000 - val_mae: 1934.6432\n",
            "Epoch 147/500\n",
            "380/380 [==============================] - 0s 78us/step - loss: 2348.8260 - mse: 33937896.0000 - mae: 2348.8259 - val_loss: 1934.4574 - val_mse: 26356976.0000 - val_mae: 1934.4574\n",
            "Epoch 148/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2354.7649 - mse: 33807488.0000 - mae: 2354.7649 - val_loss: 1934.4803 - val_mse: 26346568.0000 - val_mae: 1934.4803\n",
            "Epoch 149/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2351.5929 - mse: 33905172.0000 - mae: 2351.5928 - val_loss: 1934.6268 - val_mse: 26330456.0000 - val_mae: 1934.6268\n",
            "Epoch 150/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2356.8786 - mse: 33898728.0000 - mae: 2356.8787 - val_loss: 1934.5396 - val_mse: 26325106.0000 - val_mae: 1934.5396\n",
            "Epoch 151/500\n",
            "380/380 [==============================] - 0s 94us/step - loss: 2360.5274 - mse: 33895328.0000 - mae: 2360.5273 - val_loss: 1934.6419 - val_mse: 26312864.0000 - val_mae: 1934.6418\n",
            "Epoch 152/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2377.3874 - mse: 33927320.0000 - mae: 2377.3875 - val_loss: 1934.2355 - val_mse: 26320018.0000 - val_mae: 1934.2356\n",
            "Epoch 153/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2356.0893 - mse: 33775264.0000 - mae: 2356.0894 - val_loss: 1934.4093 - val_mse: 26303466.0000 - val_mae: 1934.4094\n",
            "Epoch 154/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2358.7962 - mse: 33843388.0000 - mae: 2358.7961 - val_loss: 1934.2026 - val_mse: 26303328.0000 - val_mae: 1934.2028\n",
            "Epoch 155/500\n",
            "380/380 [==============================] - 0s 80us/step - loss: 2370.1334 - mse: 34008708.0000 - mae: 2370.1335 - val_loss: 1933.9828 - val_mse: 26304788.0000 - val_mae: 1933.9828\n",
            "Epoch 156/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2356.7202 - mse: 33734884.0000 - mae: 2356.7205 - val_loss: 1933.8032 - val_mse: 26304388.0000 - val_mae: 1933.8032\n",
            "Epoch 157/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2367.1807 - mse: 33753984.0000 - mae: 2367.1807 - val_loss: 1933.7715 - val_mse: 26296614.0000 - val_mae: 1933.7715\n",
            "Epoch 158/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2365.2669 - mse: 33995804.0000 - mae: 2365.2668 - val_loss: 1933.6585 - val_mse: 26294618.0000 - val_mae: 1933.6584\n",
            "Epoch 159/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2359.8454 - mse: 34018492.0000 - mae: 2359.8455 - val_loss: 1933.6364 - val_mse: 26288078.0000 - val_mae: 1933.6365\n",
            "Epoch 160/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2357.1076 - mse: 33688036.0000 - mae: 2357.1077 - val_loss: 1933.5604 - val_mse: 26285078.0000 - val_mae: 1933.5604\n",
            "Epoch 161/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2357.6018 - mse: 33938468.0000 - mae: 2357.6021 - val_loss: 1933.3539 - val_mse: 26286976.0000 - val_mae: 1933.3539\n",
            "Epoch 162/500\n",
            "380/380 [==============================] - 0s 96us/step - loss: 2357.2047 - mse: 33915096.0000 - mae: 2357.2048 - val_loss: 1933.2187 - val_mse: 26286540.0000 - val_mae: 1933.2188\n",
            "Epoch 163/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2347.6041 - mse: 33634324.0000 - mae: 2347.6040 - val_loss: 1933.2807 - val_mse: 26277036.0000 - val_mae: 1933.2808\n",
            "Epoch 164/500\n",
            "380/380 [==============================] - 0s 136us/step - loss: 2351.7413 - mse: 33610052.0000 - mae: 2351.7412 - val_loss: 1933.2731 - val_mse: 26270460.0000 - val_mae: 1933.2729\n",
            "Epoch 165/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2366.0378 - mse: 33931740.0000 - mae: 2366.0378 - val_loss: 1933.2814 - val_mse: 26264908.0000 - val_mae: 1933.2814\n",
            "Epoch 166/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2355.7415 - mse: 33783704.0000 - mae: 2355.7415 - val_loss: 1933.1869 - val_mse: 26263696.0000 - val_mae: 1933.1868\n",
            "Epoch 167/500\n",
            "380/380 [==============================] - 0s 89us/step - loss: 2365.8052 - mse: 33794976.0000 - mae: 2365.8052 - val_loss: 1933.1712 - val_mse: 26259018.0000 - val_mae: 1933.1713\n",
            "Epoch 168/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2356.2910 - mse: 33636536.0000 - mae: 2356.2910 - val_loss: 1932.9975 - val_mse: 26259514.0000 - val_mae: 1932.9977\n",
            "Epoch 169/500\n",
            "380/380 [==============================] - 0s 93us/step - loss: 2378.9988 - mse: 33953144.0000 - mae: 2378.9988 - val_loss: 1932.7653 - val_mse: 26262782.0000 - val_mae: 1932.7654\n",
            "Epoch 170/500\n",
            "380/380 [==============================] - 0s 86us/step - loss: 2364.8375 - mse: 33858100.0000 - mae: 2364.8374 - val_loss: 1932.5794 - val_mse: 26263164.0000 - val_mae: 1932.5796\n",
            "Epoch 171/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2364.0317 - mse: 33663504.0000 - mae: 2364.0317 - val_loss: 1932.4596 - val_mse: 26261858.0000 - val_mae: 1932.4596\n",
            "Epoch 172/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2355.2068 - mse: 33617240.0000 - mae: 2355.2068 - val_loss: 1932.4042 - val_mse: 26257134.0000 - val_mae: 1932.4042\n",
            "Epoch 173/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2366.9319 - mse: 33704112.0000 - mae: 2366.9319 - val_loss: 1932.2643 - val_mse: 26258684.0000 - val_mae: 1932.2644\n",
            "Epoch 174/500\n",
            "380/380 [==============================] - 0s 78us/step - loss: 2336.2773 - mse: 33799948.0000 - mae: 2336.2773 - val_loss: 1932.2372 - val_mse: 26254364.0000 - val_mae: 1932.2372\n",
            "Epoch 175/500\n",
            "380/380 [==============================] - 0s 96us/step - loss: 2366.7501 - mse: 33810088.0000 - mae: 2366.7500 - val_loss: 1932.2806 - val_mse: 26249066.0000 - val_mae: 1932.2804\n",
            "Epoch 176/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2358.7822 - mse: 33699496.0000 - mae: 2358.7822 - val_loss: 1932.3222 - val_mse: 26243454.0000 - val_mae: 1932.3223\n",
            "Epoch 177/500\n",
            "380/380 [==============================] - 0s 84us/step - loss: 2356.0095 - mse: 33618100.0000 - mae: 2356.0095 - val_loss: 1932.3768 - val_mse: 26237132.0000 - val_mae: 1932.3768\n",
            "Epoch 178/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2369.8799 - mse: 33687160.0000 - mae: 2369.8799 - val_loss: 1932.2079 - val_mse: 26237514.0000 - val_mae: 1932.2079\n",
            "Epoch 179/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2363.1565 - mse: 33647892.0000 - mae: 2363.1565 - val_loss: 1932.0588 - val_mse: 26240164.0000 - val_mae: 1932.0588\n",
            "Epoch 180/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2359.8947 - mse: 33825928.0000 - mae: 2359.8948 - val_loss: 1931.7863 - val_mse: 26248728.0000 - val_mae: 1931.7864\n",
            "Epoch 181/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2368.6502 - mse: 33610564.0000 - mae: 2368.6504 - val_loss: 1931.6933 - val_mse: 26247790.0000 - val_mae: 1931.6934\n",
            "Epoch 182/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2358.4097 - mse: 33670548.0000 - mae: 2358.4097 - val_loss: 1931.6170 - val_mse: 26248824.0000 - val_mae: 1931.6171\n",
            "Epoch 183/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2365.6364 - mse: 33770952.0000 - mae: 2365.6365 - val_loss: 1931.5171 - val_mse: 26250080.0000 - val_mae: 1931.5171\n",
            "Epoch 184/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2356.3591 - mse: 33736676.0000 - mae: 2356.3591 - val_loss: 1931.4650 - val_mse: 26248086.0000 - val_mae: 1931.4651\n",
            "Epoch 185/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2349.0433 - mse: 33605100.0000 - mae: 2349.0435 - val_loss: 1931.5455 - val_mse: 26242774.0000 - val_mae: 1931.5454\n",
            "Epoch 186/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2365.9772 - mse: 33919936.0000 - mae: 2365.9773 - val_loss: 1931.4555 - val_mse: 26243352.0000 - val_mae: 1931.4554\n",
            "Epoch 187/500\n",
            "380/380 [==============================] - 0s 102us/step - loss: 2358.7053 - mse: 33775536.0000 - mae: 2358.7053 - val_loss: 1931.5104 - val_mse: 26237788.0000 - val_mae: 1931.5105\n",
            "Epoch 188/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2364.0782 - mse: 33859736.0000 - mae: 2364.0781 - val_loss: 1931.4193 - val_mse: 26239096.0000 - val_mae: 1931.4193\n",
            "Epoch 189/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2354.7974 - mse: 33447120.0000 - mae: 2354.7974 - val_loss: 1931.2314 - val_mse: 26240716.0000 - val_mae: 1931.2314\n",
            "Epoch 190/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2364.0286 - mse: 33684232.0000 - mae: 2364.0286 - val_loss: 1930.9056 - val_mse: 26248118.0000 - val_mae: 1930.9055\n",
            "Epoch 191/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2345.0732 - mse: 33826688.0000 - mae: 2345.0732 - val_loss: 1930.6600 - val_mse: 26251978.0000 - val_mae: 1930.6599\n",
            "Epoch 192/500\n",
            "380/380 [==============================] - 0s 91us/step - loss: 2361.1913 - mse: 33709708.0000 - mae: 2361.1912 - val_loss: 1930.4536 - val_mse: 26256134.0000 - val_mae: 1930.4537\n",
            "Epoch 193/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2358.1970 - mse: 33757512.0000 - mae: 2358.1968 - val_loss: 1930.2610 - val_mse: 26258102.0000 - val_mae: 1930.2610\n",
            "Epoch 194/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2346.7396 - mse: 33498586.0000 - mae: 2346.7397 - val_loss: 1930.3376 - val_mse: 26252280.0000 - val_mae: 1930.3376\n",
            "Epoch 195/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2351.1640 - mse: 33669876.0000 - mae: 2351.1641 - val_loss: 1930.1852 - val_mse: 26254078.0000 - val_mae: 1930.1852\n",
            "Epoch 196/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2364.7497 - mse: 33725560.0000 - mae: 2364.7498 - val_loss: 1930.0626 - val_mse: 26252540.0000 - val_mae: 1930.0626\n",
            "Epoch 197/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2360.1391 - mse: 33741580.0000 - mae: 2360.1392 - val_loss: 1929.8731 - val_mse: 26256046.0000 - val_mae: 1929.8729\n",
            "Epoch 198/500\n",
            "380/380 [==============================] - 0s 136us/step - loss: 2357.2972 - mse: 33608904.0000 - mae: 2357.2974 - val_loss: 1930.0886 - val_mse: 26243402.0000 - val_mae: 1930.0885\n",
            "Epoch 199/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2357.5285 - mse: 33847468.0000 - mae: 2357.5286 - val_loss: 1930.1768 - val_mse: 26235026.0000 - val_mae: 1930.1769\n",
            "Epoch 200/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2346.6025 - mse: 33651156.0000 - mae: 2346.6025 - val_loss: 1930.2258 - val_mse: 26228454.0000 - val_mae: 1930.2258\n",
            "Epoch 201/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2339.3584 - mse: 33415484.0000 - mae: 2339.3582 - val_loss: 1930.3441 - val_mse: 26218762.0000 - val_mae: 1930.3441\n",
            "Epoch 202/500\n",
            "380/380 [==============================] - 0s 102us/step - loss: 2340.4027 - mse: 33409846.0000 - mae: 2340.4026 - val_loss: 1930.2312 - val_mse: 26214654.0000 - val_mae: 1930.2312\n",
            "Epoch 203/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2347.6598 - mse: 33401164.0000 - mae: 2347.6599 - val_loss: 1930.3737 - val_mse: 26205566.0000 - val_mae: 1930.3737\n",
            "Epoch 204/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2336.3950 - mse: 33402482.0000 - mae: 2336.3950 - val_loss: 1930.2759 - val_mse: 26200574.0000 - val_mae: 1930.2760\n",
            "Epoch 205/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2354.6813 - mse: 33498856.0000 - mae: 2354.6814 - val_loss: 1929.9577 - val_mse: 26208140.0000 - val_mae: 1929.9578\n",
            "Epoch 206/500\n",
            "380/380 [==============================] - 0s 102us/step - loss: 2347.7488 - mse: 33426548.0000 - mae: 2347.7488 - val_loss: 1929.7329 - val_mse: 26208254.0000 - val_mae: 1929.7328\n",
            "Epoch 207/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2359.9872 - mse: 33523420.0000 - mae: 2359.9871 - val_loss: 1929.5134 - val_mse: 26212112.0000 - val_mae: 1929.5135\n",
            "Epoch 208/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2355.2833 - mse: 33778592.0000 - mae: 2355.2832 - val_loss: 1929.5641 - val_mse: 26205630.0000 - val_mae: 1929.5642\n",
            "Epoch 209/500\n",
            "380/380 [==============================] - 0s 102us/step - loss: 2363.7736 - mse: 33718264.0000 - mae: 2363.7737 - val_loss: 1929.2764 - val_mse: 26208818.0000 - val_mae: 1929.2765\n",
            "Epoch 210/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2358.8650 - mse: 33702620.0000 - mae: 2358.8652 - val_loss: 1929.0241 - val_mse: 26215750.0000 - val_mae: 1929.0242\n",
            "Epoch 211/500\n",
            "380/380 [==============================] - 0s 122us/step - loss: 2347.5157 - mse: 33660592.0000 - mae: 2347.5159 - val_loss: 1928.8205 - val_mse: 26216562.0000 - val_mae: 1928.8204\n",
            "Epoch 212/500\n",
            "380/380 [==============================] - 0s 135us/step - loss: 2347.2611 - mse: 33619220.0000 - mae: 2347.2612 - val_loss: 1928.7574 - val_mse: 26211074.0000 - val_mae: 1928.7574\n",
            "Epoch 213/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2368.2280 - mse: 33619160.0000 - mae: 2368.2280 - val_loss: 1928.6825 - val_mse: 26206744.0000 - val_mae: 1928.6824\n",
            "Epoch 214/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2352.8921 - mse: 33732328.0000 - mae: 2352.8921 - val_loss: 1928.6556 - val_mse: 26201858.0000 - val_mae: 1928.6555\n",
            "Epoch 215/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2354.2559 - mse: 33722760.0000 - mae: 2354.2559 - val_loss: 1928.5533 - val_mse: 26199706.0000 - val_mae: 1928.5533\n",
            "Epoch 216/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2353.4707 - mse: 33654312.0000 - mae: 2353.4707 - val_loss: 1928.3828 - val_mse: 26198116.0000 - val_mae: 1928.3828\n",
            "Epoch 217/500\n",
            "380/380 [==============================] - 0s 101us/step - loss: 2379.8494 - mse: 33762832.0000 - mae: 2379.8494 - val_loss: 1927.9936 - val_mse: 26210792.0000 - val_mae: 1927.9935\n",
            "Epoch 218/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2346.1167 - mse: 33582556.0000 - mae: 2346.1167 - val_loss: 1927.7652 - val_mse: 26208372.0000 - val_mae: 1927.7653\n",
            "Epoch 219/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2345.7971 - mse: 33765628.0000 - mae: 2345.7971 - val_loss: 1927.6167 - val_mse: 26205506.0000 - val_mae: 1927.6167\n",
            "Epoch 220/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2342.6771 - mse: 33496284.0000 - mae: 2342.6770 - val_loss: 1927.5772 - val_mse: 26201392.0000 - val_mae: 1927.5774\n",
            "Epoch 221/500\n",
            "380/380 [==============================] - 0s 97us/step - loss: 2342.6810 - mse: 33457868.0000 - mae: 2342.6812 - val_loss: 1927.4731 - val_mse: 26205744.0000 - val_mae: 1927.4731\n",
            "Epoch 222/500\n",
            "380/380 [==============================] - 0s 93us/step - loss: 2335.5164 - mse: 33571640.0000 - mae: 2335.5164 - val_loss: 1927.3816 - val_mse: 26196998.0000 - val_mae: 1927.3816\n",
            "Epoch 223/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2360.0314 - mse: 33600092.0000 - mae: 2360.0315 - val_loss: 1927.3523 - val_mse: 26189226.0000 - val_mae: 1927.3524\n",
            "Epoch 224/500\n",
            "380/380 [==============================] - 0s 97us/step - loss: 2345.3190 - mse: 33519536.0000 - mae: 2345.3191 - val_loss: 1927.1792 - val_mse: 26186400.0000 - val_mae: 1927.1791\n",
            "Epoch 225/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2334.9175 - mse: 33508702.0000 - mae: 2334.9175 - val_loss: 1927.1286 - val_mse: 26183948.0000 - val_mae: 1927.1285\n",
            "Epoch 226/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2355.0978 - mse: 33679544.0000 - mae: 2355.0979 - val_loss: 1927.2318 - val_mse: 26185730.0000 - val_mae: 1927.2318\n",
            "Epoch 227/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2358.0152 - mse: 33726080.0000 - mae: 2358.0151 - val_loss: 1927.1775 - val_mse: 26188202.0000 - val_mae: 1927.1775\n",
            "Epoch 228/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2371.4935 - mse: 33641144.0000 - mae: 2371.4934 - val_loss: 1926.9024 - val_mse: 26187966.0000 - val_mae: 1926.9023\n",
            "Epoch 229/500\n",
            "380/380 [==============================] - 0s 120us/step - loss: 2348.3129 - mse: 33568012.0000 - mae: 2348.3127 - val_loss: 1926.8111 - val_mse: 26188910.0000 - val_mae: 1926.8112\n",
            "Epoch 230/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2347.2941 - mse: 33595356.0000 - mae: 2347.2942 - val_loss: 1926.7599 - val_mse: 26188546.0000 - val_mae: 1926.7598\n",
            "Epoch 231/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2343.6637 - mse: 33666192.0000 - mae: 2343.6636 - val_loss: 1926.5940 - val_mse: 26186170.0000 - val_mae: 1926.5939\n",
            "Epoch 232/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2350.9079 - mse: 33590716.0000 - mae: 2350.9077 - val_loss: 1926.4340 - val_mse: 26187350.0000 - val_mae: 1926.4341\n",
            "Epoch 233/500\n",
            "380/380 [==============================] - 0s 139us/step - loss: 2337.5863 - mse: 33483934.0000 - mae: 2337.5862 - val_loss: 1926.3584 - val_mse: 26185562.0000 - val_mae: 1926.3584\n",
            "Epoch 234/500\n",
            "380/380 [==============================] - 0s 141us/step - loss: 2348.9465 - mse: 33689548.0000 - mae: 2348.9465 - val_loss: 1926.3840 - val_mse: 26180614.0000 - val_mae: 1926.3839\n",
            "Epoch 235/500\n",
            "380/380 [==============================] - 0s 129us/step - loss: 2359.6711 - mse: 33594880.0000 - mae: 2359.6711 - val_loss: 1926.5321 - val_mse: 26177046.0000 - val_mae: 1926.5321\n",
            "Epoch 236/500\n",
            "380/380 [==============================] - 0s 149us/step - loss: 2354.4402 - mse: 33489378.0000 - mae: 2354.4402 - val_loss: 1926.5475 - val_mse: 26174960.0000 - val_mae: 1926.5475\n",
            "Epoch 237/500\n",
            "380/380 [==============================] - 0s 123us/step - loss: 2345.0970 - mse: 33717840.0000 - mae: 2345.0969 - val_loss: 1926.4937 - val_mse: 26176914.0000 - val_mae: 1926.4938\n",
            "Epoch 238/500\n",
            "380/380 [==============================] - 0s 120us/step - loss: 2353.8750 - mse: 33586732.0000 - mae: 2353.8750 - val_loss: 1926.2968 - val_mse: 26180666.0000 - val_mae: 1926.2970\n",
            "Epoch 239/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2362.7629 - mse: 33673976.0000 - mae: 2362.7629 - val_loss: 1926.1166 - val_mse: 26185114.0000 - val_mae: 1926.1166\n",
            "Epoch 240/500\n",
            "380/380 [==============================] - 0s 116us/step - loss: 2364.8484 - mse: 33814156.0000 - mae: 2364.8484 - val_loss: 1925.7912 - val_mse: 26191614.0000 - val_mae: 1925.7913\n",
            "Epoch 241/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2345.1580 - mse: 33576056.0000 - mae: 2345.1582 - val_loss: 1925.9212 - val_mse: 26181076.0000 - val_mae: 1925.9213\n",
            "Epoch 242/500\n",
            "380/380 [==============================] - 0s 91us/step - loss: 2345.9119 - mse: 33512976.0000 - mae: 2345.9119 - val_loss: 1925.8007 - val_mse: 26179360.0000 - val_mae: 1925.8007\n",
            "Epoch 243/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2358.3112 - mse: 33616148.0000 - mae: 2358.3113 - val_loss: 1925.6915 - val_mse: 26173534.0000 - val_mae: 1925.6915\n",
            "Epoch 244/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2351.1615 - mse: 33512422.0000 - mae: 2351.1616 - val_loss: 1925.7409 - val_mse: 26165450.0000 - val_mae: 1925.7408\n",
            "Epoch 245/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2358.9062 - mse: 33595024.0000 - mae: 2358.9062 - val_loss: 1925.3650 - val_mse: 26172176.0000 - val_mae: 1925.3650\n",
            "Epoch 246/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2361.0424 - mse: 33470190.0000 - mae: 2361.0425 - val_loss: 1925.2712 - val_mse: 26169264.0000 - val_mae: 1925.2712\n",
            "Epoch 247/500\n",
            "380/380 [==============================] - 0s 118us/step - loss: 2347.5929 - mse: 33357170.0000 - mae: 2347.5930 - val_loss: 1925.0007 - val_mse: 26177292.0000 - val_mae: 1925.0007\n",
            "Epoch 248/500\n",
            "380/380 [==============================] - 0s 102us/step - loss: 2339.5012 - mse: 33644712.0000 - mae: 2339.5012 - val_loss: 1925.0125 - val_mse: 26170592.0000 - val_mae: 1925.0123\n",
            "Epoch 249/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2342.4142 - mse: 33529128.0000 - mae: 2342.4141 - val_loss: 1924.6967 - val_mse: 26169238.0000 - val_mae: 1924.6967\n",
            "Epoch 250/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2360.8778 - mse: 33572308.0000 - mae: 2360.8779 - val_loss: 1924.3998 - val_mse: 26175148.0000 - val_mae: 1924.3998\n",
            "Epoch 251/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2353.4287 - mse: 33577592.0000 - mae: 2353.4287 - val_loss: 1924.0073 - val_mse: 26177664.0000 - val_mae: 1924.0074\n",
            "Epoch 252/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2344.9974 - mse: 33548310.0000 - mae: 2344.9973 - val_loss: 1923.9032 - val_mse: 26179988.0000 - val_mae: 1923.9032\n",
            "Epoch 253/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2349.3604 - mse: 33423246.0000 - mae: 2349.3604 - val_loss: 1923.5627 - val_mse: 26185432.0000 - val_mae: 1923.5626\n",
            "Epoch 254/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2340.8673 - mse: 33546714.0000 - mae: 2340.8672 - val_loss: 1923.4622 - val_mse: 26187270.0000 - val_mae: 1923.4622\n",
            "Epoch 255/500\n",
            "380/380 [==============================] - 0s 123us/step - loss: 2335.6425 - mse: 33409316.0000 - mae: 2335.6426 - val_loss: 1923.6000 - val_mse: 26177804.0000 - val_mae: 1923.6000\n",
            "Epoch 256/500\n",
            "380/380 [==============================] - 0s 149us/step - loss: 2348.4758 - mse: 33539206.0000 - mae: 2348.4758 - val_loss: 1923.6825 - val_mse: 26173900.0000 - val_mae: 1923.6824\n",
            "Epoch 257/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2353.3820 - mse: 33424728.0000 - mae: 2353.3818 - val_loss: 1923.4763 - val_mse: 26172280.0000 - val_mae: 1923.4763\n",
            "Epoch 258/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2338.3881 - mse: 33604316.0000 - mae: 2338.3882 - val_loss: 1923.6614 - val_mse: 26169504.0000 - val_mae: 1923.6614\n",
            "Epoch 259/500\n",
            "380/380 [==============================] - 0s 96us/step - loss: 2338.9962 - mse: 33786976.0000 - mae: 2338.9963 - val_loss: 1923.6921 - val_mse: 26170882.0000 - val_mae: 1923.6923\n",
            "Epoch 260/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2355.5471 - mse: 33520678.0000 - mae: 2355.5471 - val_loss: 1923.6902 - val_mse: 26166972.0000 - val_mae: 1923.6902\n",
            "Epoch 261/500\n",
            "380/380 [==============================] - 0s 96us/step - loss: 2357.5345 - mse: 33678520.0000 - mae: 2357.5344 - val_loss: 1923.7337 - val_mse: 26163296.0000 - val_mae: 1923.7336\n",
            "Epoch 262/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2339.0725 - mse: 33289572.0000 - mae: 2339.0723 - val_loss: 1923.5883 - val_mse: 26160870.0000 - val_mae: 1923.5884\n",
            "Epoch 263/500\n",
            "380/380 [==============================] - 0s 132us/step - loss: 2363.4367 - mse: 33552406.0000 - mae: 2363.4368 - val_loss: 1923.4941 - val_mse: 26161082.0000 - val_mae: 1923.4941\n",
            "Epoch 264/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2342.9493 - mse: 33537458.0000 - mae: 2342.9495 - val_loss: 1923.6923 - val_mse: 26154404.0000 - val_mae: 1923.6924\n",
            "Epoch 265/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2344.3473 - mse: 33407442.0000 - mae: 2344.3472 - val_loss: 1923.8470 - val_mse: 26148070.0000 - val_mae: 1923.8469\n",
            "Epoch 266/500\n",
            "380/380 [==============================] - 0s 97us/step - loss: 2344.0423 - mse: 33432126.0000 - mae: 2344.0425 - val_loss: 1923.7439 - val_mse: 26144754.0000 - val_mae: 1923.7439\n",
            "Epoch 267/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2364.6704 - mse: 33599524.0000 - mae: 2364.6704 - val_loss: 1923.7061 - val_mse: 26142044.0000 - val_mae: 1923.7061\n",
            "Epoch 268/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2347.5062 - mse: 33362282.0000 - mae: 2347.5063 - val_loss: 1923.6962 - val_mse: 26139360.0000 - val_mae: 1923.6963\n",
            "Epoch 269/500\n",
            "380/380 [==============================] - 0s 92us/step - loss: 2355.9124 - mse: 33607840.0000 - mae: 2355.9126 - val_loss: 1923.7773 - val_mse: 26134544.0000 - val_mae: 1923.7773\n",
            "Epoch 270/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2334.5058 - mse: 33385814.0000 - mae: 2334.5056 - val_loss: 1923.8261 - val_mse: 26134656.0000 - val_mae: 1923.8260\n",
            "Epoch 271/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2359.5651 - mse: 33606908.0000 - mae: 2359.5652 - val_loss: 1923.9264 - val_mse: 26135148.0000 - val_mae: 1923.9264\n",
            "Epoch 272/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2340.7210 - mse: 33486434.0000 - mae: 2340.7209 - val_loss: 1923.8665 - val_mse: 26130016.0000 - val_mae: 1923.8666\n",
            "Epoch 273/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2361.1155 - mse: 33647104.0000 - mae: 2361.1155 - val_loss: 1923.5230 - val_mse: 26141956.0000 - val_mae: 1923.5232\n",
            "Epoch 274/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2345.4517 - mse: 33266108.0000 - mae: 2345.4517 - val_loss: 1923.8721 - val_mse: 26139186.0000 - val_mae: 1923.8721\n",
            "Epoch 275/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2335.2534 - mse: 33347364.0000 - mae: 2335.2534 - val_loss: 1924.0132 - val_mse: 26130208.0000 - val_mae: 1924.0132\n",
            "Epoch 276/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2350.0401 - mse: 33463142.0000 - mae: 2350.0400 - val_loss: 1923.9746 - val_mse: 26131910.0000 - val_mae: 1923.9746\n",
            "Epoch 277/500\n",
            "380/380 [==============================] - 0s 116us/step - loss: 2343.6939 - mse: 33429584.0000 - mae: 2343.6938 - val_loss: 1924.0331 - val_mse: 26128538.0000 - val_mae: 1924.0331\n",
            "Epoch 278/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2359.1917 - mse: 33666620.0000 - mae: 2359.1917 - val_loss: 1923.9691 - val_mse: 26128484.0000 - val_mae: 1923.9691\n",
            "Epoch 279/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2348.4028 - mse: 33347532.0000 - mae: 2348.4028 - val_loss: 1924.0138 - val_mse: 26128802.0000 - val_mae: 1924.0138\n",
            "Epoch 280/500\n",
            "380/380 [==============================] - 0s 137us/step - loss: 2343.4087 - mse: 33409654.0000 - mae: 2343.4087 - val_loss: 1924.1300 - val_mse: 26125160.0000 - val_mae: 1924.1301\n",
            "Epoch 281/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2357.5921 - mse: 33315198.0000 - mae: 2357.5920 - val_loss: 1924.0377 - val_mse: 26127490.0000 - val_mae: 1924.0377\n",
            "Epoch 282/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2352.2823 - mse: 33289356.0000 - mae: 2352.2822 - val_loss: 1924.0312 - val_mse: 26126170.0000 - val_mae: 1924.0311\n",
            "Epoch 283/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2345.3993 - mse: 33256680.0000 - mae: 2345.3994 - val_loss: 1923.8177 - val_mse: 26128210.0000 - val_mae: 1923.8177\n",
            "Epoch 284/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2335.2023 - mse: 33323106.0000 - mae: 2335.2024 - val_loss: 1923.6883 - val_mse: 26127538.0000 - val_mae: 1923.6884\n",
            "Epoch 285/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2345.0116 - mse: 33530374.0000 - mae: 2345.0115 - val_loss: 1923.5797 - val_mse: 26127628.0000 - val_mae: 1923.5797\n",
            "Epoch 286/500\n",
            "380/380 [==============================] - 0s 140us/step - loss: 2359.6420 - mse: 33601320.0000 - mae: 2359.6418 - val_loss: 1923.4301 - val_mse: 26138878.0000 - val_mae: 1923.4301\n",
            "Epoch 287/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2346.6527 - mse: 33421452.0000 - mae: 2346.6526 - val_loss: 1923.4308 - val_mse: 26137040.0000 - val_mae: 1923.4309\n",
            "Epoch 288/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2349.3144 - mse: 33455430.0000 - mae: 2349.3145 - val_loss: 1923.3098 - val_mse: 26143736.0000 - val_mae: 1923.3098\n",
            "Epoch 289/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2335.4962 - mse: 33380284.0000 - mae: 2335.4963 - val_loss: 1923.3040 - val_mse: 26149144.0000 - val_mae: 1923.3041\n",
            "Epoch 290/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2329.0608 - mse: 33454826.0000 - mae: 2329.0608 - val_loss: 1923.3948 - val_mse: 26144120.0000 - val_mae: 1923.3948\n",
            "Epoch 291/500\n",
            "380/380 [==============================] - 0s 125us/step - loss: 2346.4291 - mse: 33489236.0000 - mae: 2346.4292 - val_loss: 1923.4507 - val_mse: 26145230.0000 - val_mae: 1923.4507\n",
            "Epoch 292/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2362.0707 - mse: 33780136.0000 - mae: 2362.0708 - val_loss: 1923.3055 - val_mse: 26141848.0000 - val_mae: 1923.3054\n",
            "Epoch 293/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2340.4798 - mse: 33531464.0000 - mae: 2340.4800 - val_loss: 1923.6825 - val_mse: 26131014.0000 - val_mae: 1923.6824\n",
            "Epoch 294/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2334.1738 - mse: 33385896.0000 - mae: 2334.1736 - val_loss: 1923.8076 - val_mse: 26125412.0000 - val_mae: 1923.8076\n",
            "Epoch 295/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2336.8398 - mse: 33295214.0000 - mae: 2336.8398 - val_loss: 1923.8980 - val_mse: 26123810.0000 - val_mae: 1923.8979\n",
            "Epoch 296/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2352.3050 - mse: 33601696.0000 - mae: 2352.3049 - val_loss: 1923.8416 - val_mse: 26126400.0000 - val_mae: 1923.8417\n",
            "Epoch 297/500\n",
            "380/380 [==============================] - 0s 126us/step - loss: 2359.2641 - mse: 33557204.0000 - mae: 2359.2639 - val_loss: 1924.0918 - val_mse: 26112434.0000 - val_mae: 1924.0919\n",
            "Epoch 298/500\n",
            "380/380 [==============================] - 0s 123us/step - loss: 2344.3166 - mse: 33373998.0000 - mae: 2344.3169 - val_loss: 1924.4265 - val_mse: 26106714.0000 - val_mae: 1924.4264\n",
            "Epoch 299/500\n",
            "380/380 [==============================] - 0s 127us/step - loss: 2334.8751 - mse: 33373632.0000 - mae: 2334.8750 - val_loss: 1924.2515 - val_mse: 26110182.0000 - val_mae: 1924.2515\n",
            "Epoch 300/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2326.8773 - mse: 33492420.0000 - mae: 2326.8772 - val_loss: 1924.0597 - val_mse: 26116082.0000 - val_mae: 1924.0598\n",
            "Epoch 301/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2347.9607 - mse: 33454416.0000 - mae: 2347.9607 - val_loss: 1923.9482 - val_mse: 26113888.0000 - val_mae: 1923.9481\n",
            "Epoch 302/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2341.1248 - mse: 33356398.0000 - mae: 2341.1248 - val_loss: 1924.1091 - val_mse: 26108774.0000 - val_mae: 1924.1091\n",
            "Epoch 303/500\n",
            "380/380 [==============================] - 0s 125us/step - loss: 2345.2024 - mse: 33258696.0000 - mae: 2345.2024 - val_loss: 1923.8707 - val_mse: 26110708.0000 - val_mae: 1923.8706\n",
            "Epoch 304/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2340.6344 - mse: 33443986.0000 - mae: 2340.6345 - val_loss: 1923.8941 - val_mse: 26107984.0000 - val_mae: 1923.8940\n",
            "Epoch 305/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2341.9321 - mse: 33272912.0000 - mae: 2341.9321 - val_loss: 1923.9668 - val_mse: 26113494.0000 - val_mae: 1923.9669\n",
            "Epoch 306/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2332.8673 - mse: 33384210.0000 - mae: 2332.8674 - val_loss: 1924.0294 - val_mse: 26114238.0000 - val_mae: 1924.0295\n",
            "Epoch 307/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2355.6927 - mse: 33533946.0000 - mae: 2355.6929 - val_loss: 1923.7258 - val_mse: 26123168.0000 - val_mae: 1923.7258\n",
            "Epoch 308/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2364.1971 - mse: 33536716.0000 - mae: 2364.1970 - val_loss: 1923.4288 - val_mse: 26132206.0000 - val_mae: 1923.4288\n",
            "Epoch 309/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2340.9531 - mse: 33569268.0000 - mae: 2340.9529 - val_loss: 1923.6482 - val_mse: 26127186.0000 - val_mae: 1923.6482\n",
            "Epoch 310/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2350.5631 - mse: 33303948.0000 - mae: 2350.5632 - val_loss: 1923.4337 - val_mse: 26129914.0000 - val_mae: 1923.4338\n",
            "Epoch 311/500\n",
            "380/380 [==============================] - 0s 96us/step - loss: 2340.6538 - mse: 33392362.0000 - mae: 2340.6538 - val_loss: 1923.3221 - val_mse: 26131700.0000 - val_mae: 1923.3219\n",
            "Epoch 312/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2341.9685 - mse: 33408288.0000 - mae: 2341.9685 - val_loss: 1923.5614 - val_mse: 26127800.0000 - val_mae: 1923.5615\n",
            "Epoch 313/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2363.0716 - mse: 33726004.0000 - mae: 2363.0718 - val_loss: 1923.2646 - val_mse: 26133238.0000 - val_mae: 1923.2645\n",
            "Epoch 314/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2350.0519 - mse: 33560144.0000 - mae: 2350.0520 - val_loss: 1923.4738 - val_mse: 26129204.0000 - val_mae: 1923.4739\n",
            "Epoch 315/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2359.4749 - mse: 33602820.0000 - mae: 2359.4751 - val_loss: 1923.3460 - val_mse: 26131774.0000 - val_mae: 1923.3459\n",
            "Epoch 316/500\n",
            "380/380 [==============================] - 0s 97us/step - loss: 2345.2235 - mse: 33483030.0000 - mae: 2345.2236 - val_loss: 1923.5213 - val_mse: 26124750.0000 - val_mae: 1923.5212\n",
            "Epoch 317/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2352.9060 - mse: 33464508.0000 - mae: 2352.9060 - val_loss: 1923.7384 - val_mse: 26120198.0000 - val_mae: 1923.7385\n",
            "Epoch 318/500\n",
            "380/380 [==============================] - 0s 93us/step - loss: 2355.5970 - mse: 33540524.0000 - mae: 2355.5969 - val_loss: 1923.5533 - val_mse: 26123176.0000 - val_mae: 1923.5533\n",
            "Epoch 319/500\n",
            "380/380 [==============================] - 0s 97us/step - loss: 2325.2598 - mse: 33539320.0000 - mae: 2325.2598 - val_loss: 1923.6766 - val_mse: 26119910.0000 - val_mae: 1923.6766\n",
            "Epoch 320/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2355.0498 - mse: 33336730.0000 - mae: 2355.0496 - val_loss: 1923.6579 - val_mse: 26113036.0000 - val_mae: 1923.6580\n",
            "Epoch 321/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2328.8635 - mse: 33220070.0000 - mae: 2328.8635 - val_loss: 1923.8676 - val_mse: 26104134.0000 - val_mae: 1923.8677\n",
            "Epoch 322/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2329.4864 - mse: 33378040.0000 - mae: 2329.4863 - val_loss: 1923.8667 - val_mse: 26105698.0000 - val_mae: 1923.8667\n",
            "Epoch 323/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2345.6336 - mse: 33424174.0000 - mae: 2345.6335 - val_loss: 1923.8264 - val_mse: 26109540.0000 - val_mae: 1923.8263\n",
            "Epoch 324/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2344.3609 - mse: 33441142.0000 - mae: 2344.3608 - val_loss: 1923.8968 - val_mse: 26104856.0000 - val_mae: 1923.8969\n",
            "Epoch 325/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2368.0971 - mse: 33528452.0000 - mae: 2368.0969 - val_loss: 1923.8002 - val_mse: 26111194.0000 - val_mae: 1923.8002\n",
            "Epoch 326/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2345.4130 - mse: 33444082.0000 - mae: 2345.4131 - val_loss: 1923.7291 - val_mse: 26109838.0000 - val_mae: 1923.7292\n",
            "Epoch 327/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2356.1676 - mse: 33376316.0000 - mae: 2356.1675 - val_loss: 1923.6844 - val_mse: 26116768.0000 - val_mae: 1923.6844\n",
            "Epoch 328/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2367.0036 - mse: 33612004.0000 - mae: 2367.0037 - val_loss: 1923.4930 - val_mse: 26121404.0000 - val_mae: 1923.4930\n",
            "Epoch 329/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2346.5392 - mse: 33371940.0000 - mae: 2346.5391 - val_loss: 1923.3644 - val_mse: 26121586.0000 - val_mae: 1923.3644\n",
            "Epoch 330/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2328.8806 - mse: 33403928.0000 - mae: 2328.8806 - val_loss: 1923.5615 - val_mse: 26115296.0000 - val_mae: 1923.5615\n",
            "Epoch 331/500\n",
            "380/380 [==============================] - 0s 116us/step - loss: 2331.4187 - mse: 33370090.0000 - mae: 2331.4187 - val_loss: 1923.4901 - val_mse: 26115900.0000 - val_mae: 1923.4900\n",
            "Epoch 332/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2347.9322 - mse: 33387908.0000 - mae: 2347.9321 - val_loss: 1923.3047 - val_mse: 26121692.0000 - val_mae: 1923.3047\n",
            "Epoch 333/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2340.1485 - mse: 33405732.0000 - mae: 2340.1484 - val_loss: 1923.1569 - val_mse: 26130078.0000 - val_mae: 1923.1570\n",
            "Epoch 334/500\n",
            "380/380 [==============================] - 0s 123us/step - loss: 2338.3283 - mse: 33431274.0000 - mae: 2338.3284 - val_loss: 1923.3328 - val_mse: 26127972.0000 - val_mae: 1923.3328\n",
            "Epoch 335/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2351.1613 - mse: 33496108.0000 - mae: 2351.1611 - val_loss: 1923.3349 - val_mse: 26123884.0000 - val_mae: 1923.3348\n",
            "Epoch 336/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2335.7070 - mse: 33323196.0000 - mae: 2335.7068 - val_loss: 1923.3837 - val_mse: 26120344.0000 - val_mae: 1923.3838\n",
            "Epoch 337/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2347.7704 - mse: 33561540.0000 - mae: 2347.7705 - val_loss: 1923.3357 - val_mse: 26123634.0000 - val_mae: 1923.3357\n",
            "Epoch 338/500\n",
            "380/380 [==============================] - 0s 118us/step - loss: 2339.1221 - mse: 33501520.0000 - mae: 2339.1221 - val_loss: 1923.2424 - val_mse: 26126662.0000 - val_mae: 1923.2424\n",
            "Epoch 339/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2328.4896 - mse: 33476336.0000 - mae: 2328.4897 - val_loss: 1923.3196 - val_mse: 26123486.0000 - val_mae: 1923.3196\n",
            "Epoch 340/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2348.6058 - mse: 33468648.0000 - mae: 2348.6057 - val_loss: 1923.3729 - val_mse: 26124080.0000 - val_mae: 1923.3729\n",
            "Epoch 341/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2356.6276 - mse: 33560028.0000 - mae: 2356.6277 - val_loss: 1923.1812 - val_mse: 26127770.0000 - val_mae: 1923.1813\n",
            "Epoch 342/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2332.1741 - mse: 33439770.0000 - mae: 2332.1743 - val_loss: 1923.1586 - val_mse: 26124980.0000 - val_mae: 1923.1586\n",
            "Epoch 343/500\n",
            "380/380 [==============================] - 0s 141us/step - loss: 2341.7920 - mse: 33338316.0000 - mae: 2341.7920 - val_loss: 1923.1071 - val_mse: 26124982.0000 - val_mae: 1923.1071\n",
            "Epoch 344/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2345.3004 - mse: 33413190.0000 - mae: 2345.3003 - val_loss: 1922.8491 - val_mse: 26126308.0000 - val_mae: 1922.8490\n",
            "Epoch 345/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2338.6501 - mse: 33325592.0000 - mae: 2338.6501 - val_loss: 1922.8369 - val_mse: 26125636.0000 - val_mae: 1922.8368\n",
            "Epoch 346/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2341.4491 - mse: 33473316.0000 - mae: 2341.4490 - val_loss: 1922.7203 - val_mse: 26126710.0000 - val_mae: 1922.7205\n",
            "Epoch 347/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2350.8228 - mse: 33488120.0000 - mae: 2350.8228 - val_loss: 1922.6719 - val_mse: 26126770.0000 - val_mae: 1922.6720\n",
            "Epoch 348/500\n",
            "380/380 [==============================] - 0s 127us/step - loss: 2350.3907 - mse: 33323456.0000 - mae: 2350.3909 - val_loss: 1922.7297 - val_mse: 26128988.0000 - val_mae: 1922.7296\n",
            "Epoch 349/500\n",
            "380/380 [==============================] - 0s 120us/step - loss: 2346.0915 - mse: 33662304.0000 - mae: 2346.0916 - val_loss: 1922.7072 - val_mse: 26131246.0000 - val_mae: 1922.7073\n",
            "Epoch 350/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2348.1477 - mse: 33509810.0000 - mae: 2348.1477 - val_loss: 1922.7065 - val_mse: 26131432.0000 - val_mae: 1922.7064\n",
            "Epoch 351/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2337.7938 - mse: 33457006.0000 - mae: 2337.7937 - val_loss: 1922.7060 - val_mse: 26134520.0000 - val_mae: 1922.7059\n",
            "Epoch 352/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2354.6751 - mse: 33628228.0000 - mae: 2354.6750 - val_loss: 1922.7034 - val_mse: 26133516.0000 - val_mae: 1922.7035\n",
            "Epoch 353/500\n",
            "380/380 [==============================] - 0s 129us/step - loss: 2341.3946 - mse: 33278206.0000 - mae: 2341.3945 - val_loss: 1922.7745 - val_mse: 26131990.0000 - val_mae: 1922.7745\n",
            "Epoch 354/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2359.6328 - mse: 33590552.0000 - mae: 2359.6328 - val_loss: 1922.8238 - val_mse: 26133352.0000 - val_mae: 1922.8239\n",
            "Epoch 355/500\n",
            "380/380 [==============================] - 0s 118us/step - loss: 2358.4727 - mse: 33557828.0000 - mae: 2358.4727 - val_loss: 1922.7164 - val_mse: 26131838.0000 - val_mae: 1922.7164\n",
            "Epoch 356/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2347.5721 - mse: 33692460.0000 - mae: 2347.5720 - val_loss: 1922.7246 - val_mse: 26130988.0000 - val_mae: 1922.7245\n",
            "Epoch 357/500\n",
            "380/380 [==============================] - 0s 127us/step - loss: 2341.4785 - mse: 33436186.0000 - mae: 2341.4785 - val_loss: 1922.7614 - val_mse: 26132510.0000 - val_mae: 1922.7614\n",
            "Epoch 358/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2337.9273 - mse: 33118914.0000 - mae: 2337.9272 - val_loss: 1922.8291 - val_mse: 26130658.0000 - val_mae: 1922.8292\n",
            "Epoch 359/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2339.3100 - mse: 33355464.0000 - mae: 2339.3101 - val_loss: 1922.9464 - val_mse: 26124202.0000 - val_mae: 1922.9464\n",
            "Epoch 360/500\n",
            "380/380 [==============================] - 0s 116us/step - loss: 2332.2969 - mse: 33509796.0000 - mae: 2332.2971 - val_loss: 1922.9175 - val_mse: 26121914.0000 - val_mae: 1922.9176\n",
            "Epoch 361/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2338.5550 - mse: 33485008.0000 - mae: 2338.5549 - val_loss: 1922.8782 - val_mse: 26118396.0000 - val_mae: 1922.8782\n",
            "Epoch 362/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2362.0173 - mse: 33338836.0000 - mae: 2362.0173 - val_loss: 1922.7854 - val_mse: 26123006.0000 - val_mae: 1922.7853\n",
            "Epoch 363/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2339.4724 - mse: 33355806.0000 - mae: 2339.4724 - val_loss: 1922.6717 - val_mse: 26120286.0000 - val_mae: 1922.6718\n",
            "Epoch 364/500\n",
            "380/380 [==============================] - 0s 141us/step - loss: 2330.9305 - mse: 33529516.0000 - mae: 2330.9307 - val_loss: 1922.8752 - val_mse: 26114400.0000 - val_mae: 1922.8754\n",
            "Epoch 365/500\n",
            "380/380 [==============================] - 0s 122us/step - loss: 2328.9857 - mse: 33221836.0000 - mae: 2328.9856 - val_loss: 1922.9704 - val_mse: 26109844.0000 - val_mae: 1922.9705\n",
            "Epoch 366/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2341.8897 - mse: 33532076.0000 - mae: 2341.8896 - val_loss: 1922.6497 - val_mse: 26113298.0000 - val_mae: 1922.6497\n",
            "Epoch 367/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2346.6839 - mse: 33300518.0000 - mae: 2346.6838 - val_loss: 1922.8355 - val_mse: 26109940.0000 - val_mae: 1922.8354\n",
            "Epoch 368/500\n",
            "380/380 [==============================] - 0s 129us/step - loss: 2349.9409 - mse: 33419870.0000 - mae: 2349.9409 - val_loss: 1922.7656 - val_mse: 26109440.0000 - val_mae: 1922.7657\n",
            "Epoch 369/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2339.6506 - mse: 33393976.0000 - mae: 2339.6506 - val_loss: 1922.7990 - val_mse: 26106910.0000 - val_mae: 1922.7990\n",
            "Epoch 370/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2347.5343 - mse: 33320598.0000 - mae: 2347.5342 - val_loss: 1922.7249 - val_mse: 26106614.0000 - val_mae: 1922.7250\n",
            "Epoch 371/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2344.2751 - mse: 33586596.0000 - mae: 2344.2751 - val_loss: 1922.6718 - val_mse: 26111104.0000 - val_mae: 1922.6720\n",
            "Epoch 372/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2352.3747 - mse: 33468728.0000 - mae: 2352.3748 - val_loss: 1922.4873 - val_mse: 26110610.0000 - val_mae: 1922.4873\n",
            "Epoch 373/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2341.7541 - mse: 33217622.0000 - mae: 2341.7542 - val_loss: 1922.4510 - val_mse: 26108104.0000 - val_mae: 1922.4510\n",
            "Epoch 374/500\n",
            "380/380 [==============================] - 0s 125us/step - loss: 2351.8740 - mse: 33384152.0000 - mae: 2351.8740 - val_loss: 1922.5419 - val_mse: 26111284.0000 - val_mae: 1922.5419\n",
            "Epoch 375/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2341.1331 - mse: 33342704.0000 - mae: 2341.1331 - val_loss: 1922.4263 - val_mse: 26112880.0000 - val_mae: 1922.4264\n",
            "Epoch 376/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2333.0459 - mse: 33165410.0000 - mae: 2333.0461 - val_loss: 1922.5751 - val_mse: 26111356.0000 - val_mae: 1922.5750\n",
            "Epoch 377/500\n",
            "380/380 [==============================] - 0s 130us/step - loss: 2349.3012 - mse: 33405642.0000 - mae: 2349.3013 - val_loss: 1922.5509 - val_mse: 26108958.0000 - val_mae: 1922.5509\n",
            "Epoch 378/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2337.1818 - mse: 33306894.0000 - mae: 2337.1816 - val_loss: 1922.2393 - val_mse: 26113862.0000 - val_mae: 1922.2394\n",
            "Epoch 379/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2344.6325 - mse: 33321774.0000 - mae: 2344.6326 - val_loss: 1922.4504 - val_mse: 26109100.0000 - val_mae: 1922.4506\n",
            "Epoch 380/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2343.4679 - mse: 33279068.0000 - mae: 2343.4680 - val_loss: 1922.3359 - val_mse: 26113040.0000 - val_mae: 1922.3358\n",
            "Epoch 381/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2334.3192 - mse: 33368050.0000 - mae: 2334.3191 - val_loss: 1922.1703 - val_mse: 26111740.0000 - val_mae: 1922.1703\n",
            "Epoch 382/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2338.4976 - mse: 33498058.0000 - mae: 2338.4978 - val_loss: 1922.1962 - val_mse: 26108178.0000 - val_mae: 1922.1962\n",
            "Epoch 383/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2358.4140 - mse: 33516962.0000 - mae: 2358.4138 - val_loss: 1922.0014 - val_mse: 26114956.0000 - val_mae: 1922.0013\n",
            "Epoch 384/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2333.3769 - mse: 33402564.0000 - mae: 2333.3767 - val_loss: 1921.8697 - val_mse: 26113146.0000 - val_mae: 1921.8698\n",
            "Epoch 385/500\n",
            "380/380 [==============================] - 0s 144us/step - loss: 2334.1592 - mse: 33377142.0000 - mae: 2334.1589 - val_loss: 1922.0413 - val_mse: 26112124.0000 - val_mae: 1922.0413\n",
            "Epoch 386/500\n",
            "380/380 [==============================] - 0s 136us/step - loss: 2341.6081 - mse: 33430084.0000 - mae: 2341.6082 - val_loss: 1922.0752 - val_mse: 26110386.0000 - val_mae: 1922.0752\n",
            "Epoch 387/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2349.6558 - mse: 33367008.0000 - mae: 2349.6558 - val_loss: 1921.9797 - val_mse: 26109406.0000 - val_mae: 1921.9797\n",
            "Epoch 388/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2331.4817 - mse: 33554088.0000 - mae: 2331.4817 - val_loss: 1921.6209 - val_mse: 26115986.0000 - val_mae: 1921.6210\n",
            "Epoch 389/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2321.5330 - mse: 33344044.0000 - mae: 2321.5330 - val_loss: 1921.5941 - val_mse: 26109290.0000 - val_mae: 1921.5941\n",
            "Epoch 390/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2324.7002 - mse: 33300364.0000 - mae: 2324.7002 - val_loss: 1921.5420 - val_mse: 26110296.0000 - val_mae: 1921.5421\n",
            "Epoch 391/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2343.4916 - mse: 33395690.0000 - mae: 2343.4915 - val_loss: 1921.4384 - val_mse: 26112712.0000 - val_mae: 1921.4385\n",
            "Epoch 392/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2345.8458 - mse: 33560232.0000 - mae: 2345.8459 - val_loss: 1921.2252 - val_mse: 26111264.0000 - val_mae: 1921.2252\n",
            "Epoch 393/500\n",
            "380/380 [==============================] - 0s 130us/step - loss: 2355.1615 - mse: 33494534.0000 - mae: 2355.1616 - val_loss: 1921.2771 - val_mse: 26106904.0000 - val_mae: 1921.2772\n",
            "Epoch 394/500\n",
            "380/380 [==============================] - 0s 123us/step - loss: 2344.8340 - mse: 33605064.0000 - mae: 2344.8340 - val_loss: 1921.4938 - val_mse: 26100376.0000 - val_mae: 1921.4938\n",
            "Epoch 395/500\n",
            "380/380 [==============================] - 0s 132us/step - loss: 2358.7549 - mse: 33612728.0000 - mae: 2358.7549 - val_loss: 1921.3073 - val_mse: 26103438.0000 - val_mae: 1921.3073\n",
            "Epoch 396/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2350.3967 - mse: 33436938.0000 - mae: 2350.3967 - val_loss: 1921.2409 - val_mse: 26103950.0000 - val_mae: 1921.2411\n",
            "Epoch 397/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2321.5233 - mse: 33429002.0000 - mae: 2321.5232 - val_loss: 1921.1486 - val_mse: 26104384.0000 - val_mae: 1921.1487\n",
            "Epoch 398/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2340.7833 - mse: 33352890.0000 - mae: 2340.7832 - val_loss: 1921.3064 - val_mse: 26095628.0000 - val_mae: 1921.3064\n",
            "Epoch 399/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2336.2686 - mse: 33615088.0000 - mae: 2336.2683 - val_loss: 1921.1996 - val_mse: 26096150.0000 - val_mae: 1921.1997\n",
            "Epoch 400/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2341.6631 - mse: 33264560.0000 - mae: 2341.6631 - val_loss: 1921.3351 - val_mse: 26093452.0000 - val_mae: 1921.3350\n",
            "Epoch 401/500\n",
            "380/380 [==============================] - 0s 130us/step - loss: 2336.8875 - mse: 33439084.0000 - mae: 2336.8875 - val_loss: 1921.1965 - val_mse: 26097742.0000 - val_mae: 1921.1964\n",
            "Epoch 402/500\n",
            "380/380 [==============================] - 0s 122us/step - loss: 2332.8161 - mse: 33435394.0000 - mae: 2332.8162 - val_loss: 1921.3196 - val_mse: 26092806.0000 - val_mae: 1921.3196\n",
            "Epoch 403/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2332.8275 - mse: 33418122.0000 - mae: 2332.8274 - val_loss: 1921.3637 - val_mse: 26094548.0000 - val_mae: 1921.3636\n",
            "Epoch 404/500\n",
            "380/380 [==============================] - 0s 127us/step - loss: 2340.2003 - mse: 33381408.0000 - mae: 2340.2004 - val_loss: 1921.3417 - val_mse: 26095882.0000 - val_mae: 1921.3416\n",
            "Epoch 405/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2363.3267 - mse: 33370780.0000 - mae: 2363.3267 - val_loss: 1921.2246 - val_mse: 26096166.0000 - val_mae: 1921.2246\n",
            "Epoch 406/500\n",
            "380/380 [==============================] - 0s 129us/step - loss: 2337.3812 - mse: 33271728.0000 - mae: 2337.3813 - val_loss: 1920.9704 - val_mse: 26090354.0000 - val_mae: 1920.9705\n",
            "Epoch 407/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2338.5306 - mse: 33433978.0000 - mae: 2338.5305 - val_loss: 1921.1630 - val_mse: 26087040.0000 - val_mae: 1921.1632\n",
            "Epoch 408/500\n",
            "380/380 [==============================] - 0s 116us/step - loss: 2344.0603 - mse: 33350096.0000 - mae: 2344.0603 - val_loss: 1921.6738 - val_mse: 26081324.0000 - val_mae: 1921.6738\n",
            "Epoch 409/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2347.2148 - mse: 33348808.0000 - mae: 2347.2148 - val_loss: 1921.7051 - val_mse: 26079434.0000 - val_mae: 1921.7051\n",
            "Epoch 410/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2354.2613 - mse: 33632620.0000 - mae: 2354.2612 - val_loss: 1921.2974 - val_mse: 26084994.0000 - val_mae: 1921.2975\n",
            "Epoch 411/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2331.8326 - mse: 33320292.0000 - mae: 2331.8325 - val_loss: 1920.8919 - val_mse: 26090332.0000 - val_mae: 1920.8918\n",
            "Epoch 412/500\n",
            "380/380 [==============================] - 0s 123us/step - loss: 2346.4194 - mse: 33361122.0000 - mae: 2346.4194 - val_loss: 1921.0267 - val_mse: 26091246.0000 - val_mae: 1921.0267\n",
            "Epoch 413/500\n",
            "380/380 [==============================] - 0s 143us/step - loss: 2329.5124 - mse: 33589080.0000 - mae: 2329.5125 - val_loss: 1921.0441 - val_mse: 26088254.0000 - val_mae: 1921.0441\n",
            "Epoch 414/500\n",
            "380/380 [==============================] - 0s 130us/step - loss: 2349.8870 - mse: 33479682.0000 - mae: 2349.8870 - val_loss: 1920.6902 - val_mse: 26091072.0000 - val_mae: 1920.6902\n",
            "Epoch 415/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2329.6316 - mse: 33302926.0000 - mae: 2329.6316 - val_loss: 1920.6311 - val_mse: 26093674.0000 - val_mae: 1920.6311\n",
            "Epoch 416/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2347.8289 - mse: 33470296.0000 - mae: 2347.8289 - val_loss: 1920.5362 - val_mse: 26095820.0000 - val_mae: 1920.5361\n",
            "Epoch 417/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2338.5893 - mse: 33295246.0000 - mae: 2338.5894 - val_loss: 1919.9758 - val_mse: 26100612.0000 - val_mae: 1919.9758\n",
            "Epoch 418/500\n",
            "380/380 [==============================] - 0s 136us/step - loss: 2354.4933 - mse: 33527172.0000 - mae: 2354.4934 - val_loss: 1919.6277 - val_mse: 26101312.0000 - val_mae: 1919.6277\n",
            "Epoch 419/500\n",
            "380/380 [==============================] - 0s 123us/step - loss: 2330.6747 - mse: 33193628.0000 - mae: 2330.6746 - val_loss: 1919.4070 - val_mse: 26101586.0000 - val_mae: 1919.4071\n",
            "Epoch 420/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2349.1440 - mse: 33455616.0000 - mae: 2349.1440 - val_loss: 1919.2372 - val_mse: 26095290.0000 - val_mae: 1919.2372\n",
            "Epoch 421/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2351.3594 - mse: 33285382.0000 - mae: 2351.3591 - val_loss: 1919.2725 - val_mse: 26095394.0000 - val_mae: 1919.2726\n",
            "Epoch 422/500\n",
            "380/380 [==============================] - 0s 126us/step - loss: 2341.3160 - mse: 33139482.0000 - mae: 2341.3162 - val_loss: 1919.1680 - val_mse: 26092090.0000 - val_mae: 1919.1681\n",
            "Epoch 423/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2332.7414 - mse: 33190300.0000 - mae: 2332.7415 - val_loss: 1919.2567 - val_mse: 26090128.0000 - val_mae: 1919.2567\n",
            "Epoch 424/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2352.5158 - mse: 33681632.0000 - mae: 2352.5159 - val_loss: 1918.8607 - val_mse: 26096200.0000 - val_mae: 1918.8606\n",
            "Epoch 425/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2346.3708 - mse: 33449416.0000 - mae: 2346.3708 - val_loss: 1918.0634 - val_mse: 26100174.0000 - val_mae: 1918.0634\n",
            "Epoch 426/500\n",
            "380/380 [==============================] - 0s 136us/step - loss: 2375.0037 - mse: 33725668.0000 - mae: 2375.0037 - val_loss: 1917.5291 - val_mse: 26107594.0000 - val_mae: 1917.5292\n",
            "Epoch 427/500\n",
            "380/380 [==============================] - 0s 122us/step - loss: 2335.5423 - mse: 33331598.0000 - mae: 2335.5420 - val_loss: 1917.5563 - val_mse: 26107010.0000 - val_mae: 1917.5563\n",
            "Epoch 428/500\n",
            "380/380 [==============================] - 0s 125us/step - loss: 2329.6132 - mse: 33211816.0000 - mae: 2329.6130 - val_loss: 1916.6771 - val_mse: 26109310.0000 - val_mae: 1916.6770\n",
            "Epoch 429/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2331.0748 - mse: 33429450.0000 - mae: 2331.0747 - val_loss: 1916.5816 - val_mse: 26104524.0000 - val_mae: 1916.5815\n",
            "Epoch 430/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2337.2942 - mse: 33550342.0000 - mae: 2337.2942 - val_loss: 1916.3838 - val_mse: 26105260.0000 - val_mae: 1916.3838\n",
            "Epoch 431/500\n",
            "380/380 [==============================] - 0s 132us/step - loss: 2338.1497 - mse: 33543556.0000 - mae: 2338.1497 - val_loss: 1915.7933 - val_mse: 26105040.0000 - val_mae: 1915.7935\n",
            "Epoch 432/500\n",
            "380/380 [==============================] - 0s 134us/step - loss: 2341.9772 - mse: 33368426.0000 - mae: 2341.9771 - val_loss: 1914.8028 - val_mse: 26103460.0000 - val_mae: 1914.8027\n",
            "Epoch 433/500\n",
            "380/380 [==============================] - 0s 125us/step - loss: 2327.4130 - mse: 33647104.0000 - mae: 2327.4131 - val_loss: 1914.0929 - val_mse: 26100100.0000 - val_mae: 1914.0929\n",
            "Epoch 434/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2347.0514 - mse: 33519934.0000 - mae: 2347.0515 - val_loss: 1913.0315 - val_mse: 26095088.0000 - val_mae: 1913.0316\n",
            "Epoch 435/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2346.2147 - mse: 33468514.0000 - mae: 2346.2146 - val_loss: 1911.3946 - val_mse: 26094980.0000 - val_mae: 1911.3947\n",
            "Epoch 436/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2318.5838 - mse: 33208012.0000 - mae: 2318.5837 - val_loss: 1909.9088 - val_mse: 26091074.0000 - val_mae: 1909.9089\n",
            "Epoch 437/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2326.2559 - mse: 33319526.0000 - mae: 2326.2559 - val_loss: 1907.8694 - val_mse: 26092564.0000 - val_mae: 1907.8694\n",
            "Epoch 438/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2340.0328 - mse: 33489936.0000 - mae: 2340.0330 - val_loss: 1905.1737 - val_mse: 26089486.0000 - val_mae: 1905.1737\n",
            "Epoch 439/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2340.1289 - mse: 33647136.0000 - mae: 2340.1289 - val_loss: 1901.6097 - val_mse: 26089632.0000 - val_mae: 1901.6096\n",
            "Epoch 440/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2329.3289 - mse: 33462118.0000 - mae: 2329.3289 - val_loss: 1897.1574 - val_mse: 26090854.0000 - val_mae: 1897.1575\n",
            "Epoch 441/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2334.6808 - mse: 33434934.0000 - mae: 2334.6807 - val_loss: 1890.1566 - val_mse: 26091760.0000 - val_mae: 1890.1567\n",
            "Epoch 442/500\n",
            "380/380 [==============================] - 0s 116us/step - loss: 2312.7239 - mse: 33260048.0000 - mae: 2312.7241 - val_loss: 1881.5041 - val_mse: 26092318.0000 - val_mae: 1881.5040\n",
            "Epoch 443/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2322.3476 - mse: 33554748.0000 - mae: 2322.3477 - val_loss: 1867.4073 - val_mse: 26107904.0000 - val_mae: 1867.4075\n",
            "Epoch 444/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2295.3485 - mse: 33241662.0000 - mae: 2295.3484 - val_loss: 1852.0290 - val_mse: 26107466.0000 - val_mae: 1852.0291\n",
            "Epoch 445/500\n",
            "380/380 [==============================] - 0s 140us/step - loss: 2283.9194 - mse: 33333638.0000 - mae: 2283.9194 - val_loss: 1842.4218 - val_mse: 26098674.0000 - val_mae: 1842.4218\n",
            "Epoch 446/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2300.2739 - mse: 33258340.0000 - mae: 2300.2739 - val_loss: 1831.8715 - val_mse: 26095004.0000 - val_mae: 1831.8715\n",
            "Epoch 447/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2288.5717 - mse: 33337672.0000 - mae: 2288.5718 - val_loss: 1817.5273 - val_mse: 26108122.0000 - val_mae: 1817.5273\n",
            "Epoch 448/500\n",
            "380/380 [==============================] - 0s 134us/step - loss: 2275.7149 - mse: 33197676.0000 - mae: 2275.7148 - val_loss: 1811.1952 - val_mse: 26107962.0000 - val_mae: 1811.1953\n",
            "Epoch 449/500\n",
            "380/380 [==============================] - 0s 134us/step - loss: 2259.1679 - mse: 33397490.0000 - mae: 2259.1682 - val_loss: 1806.5767 - val_mse: 26107018.0000 - val_mae: 1806.5769\n",
            "Epoch 450/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2257.2560 - mse: 33291484.0000 - mae: 2257.2559 - val_loss: 1803.8581 - val_mse: 26108594.0000 - val_mae: 1803.8582\n",
            "Epoch 451/500\n",
            "380/380 [==============================] - 0s 130us/step - loss: 2259.4517 - mse: 33336062.0000 - mae: 2259.4517 - val_loss: 1802.1232 - val_mse: 26070608.0000 - val_mae: 1802.1232\n",
            "Epoch 452/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2255.9829 - mse: 33315484.0000 - mae: 2255.9829 - val_loss: 1801.8560 - val_mse: 26014972.0000 - val_mae: 1801.8561\n",
            "Epoch 453/500\n",
            "380/380 [==============================] - 0s 122us/step - loss: 2258.9522 - mse: 33135018.0000 - mae: 2258.9524 - val_loss: 1798.7935 - val_mse: 25997066.0000 - val_mae: 1798.7935\n",
            "Epoch 454/500\n",
            "380/380 [==============================] - 0s 133us/step - loss: 2252.2983 - mse: 33182940.0000 - mae: 2252.2983 - val_loss: 1797.1706 - val_mse: 25971040.0000 - val_mae: 1797.1707\n",
            "Epoch 455/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2251.6971 - mse: 33257374.0000 - mae: 2251.6970 - val_loss: 1796.0242 - val_mse: 25967134.0000 - val_mae: 1796.0242\n",
            "Epoch 456/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2242.0656 - mse: 33216474.0000 - mae: 2242.0657 - val_loss: 1794.2652 - val_mse: 25920120.0000 - val_mae: 1794.2653\n",
            "Epoch 457/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2256.6674 - mse: 33172710.0000 - mae: 2256.6675 - val_loss: 1793.9780 - val_mse: 25918758.0000 - val_mae: 1793.9780\n",
            "Epoch 458/500\n",
            "380/380 [==============================] - 0s 120us/step - loss: 2229.6770 - mse: 32821140.0000 - mae: 2229.6770 - val_loss: 1791.0697 - val_mse: 25872008.0000 - val_mae: 1791.0698\n",
            "Epoch 459/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2247.0771 - mse: 33192166.0000 - mae: 2247.0771 - val_loss: 1789.6565 - val_mse: 25843804.0000 - val_mae: 1789.6564\n",
            "Epoch 460/500\n",
            "380/380 [==============================] - 0s 127us/step - loss: 2239.7608 - mse: 32842762.0000 - mae: 2239.7607 - val_loss: 1788.9216 - val_mse: 25798524.0000 - val_mae: 1788.9215\n",
            "Epoch 461/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2245.6432 - mse: 32983806.0000 - mae: 2245.6433 - val_loss: 1788.3032 - val_mse: 25758462.0000 - val_mae: 1788.3032\n",
            "Epoch 462/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2216.8180 - mse: 32737992.0000 - mae: 2216.8181 - val_loss: 1786.3589 - val_mse: 25769846.0000 - val_mae: 1786.3590\n",
            "Epoch 463/500\n",
            "380/380 [==============================] - 0s 153us/step - loss: 2223.8090 - mse: 32915446.0000 - mae: 2223.8088 - val_loss: 1786.3509 - val_mse: 25755386.0000 - val_mae: 1786.3511\n",
            "Epoch 464/500\n",
            "380/380 [==============================] - 0s 132us/step - loss: 2214.8312 - mse: 32770914.0000 - mae: 2214.8313 - val_loss: 1782.4268 - val_mse: 25705984.0000 - val_mae: 1782.4266\n",
            "Epoch 465/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2213.6431 - mse: 32616256.0000 - mae: 2213.6431 - val_loss: 1780.8991 - val_mse: 25675392.0000 - val_mae: 1780.8989\n",
            "Epoch 466/500\n",
            "380/380 [==============================] - 0s 137us/step - loss: 2214.8101 - mse: 32900882.0000 - mae: 2214.8103 - val_loss: 1779.3776 - val_mse: 25646222.0000 - val_mae: 1779.3776\n",
            "Epoch 467/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2230.8439 - mse: 33118834.0000 - mae: 2230.8440 - val_loss: 1781.0993 - val_mse: 25648748.0000 - val_mae: 1781.0994\n",
            "Epoch 468/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2217.7422 - mse: 32784482.0000 - mae: 2217.7422 - val_loss: 1779.3694 - val_mse: 25622158.0000 - val_mae: 1779.3693\n",
            "Epoch 469/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2216.0359 - mse: 32684822.0000 - mae: 2216.0359 - val_loss: 1778.6585 - val_mse: 25603086.0000 - val_mae: 1778.6584\n",
            "Epoch 470/500\n",
            "380/380 [==============================] - 0s 137us/step - loss: 2221.0450 - mse: 32804298.0000 - mae: 2221.0452 - val_loss: 1782.6529 - val_mse: 25604570.0000 - val_mae: 1782.6528\n",
            "Epoch 471/500\n",
            "380/380 [==============================] - 0s 118us/step - loss: 2202.6740 - mse: 32645588.0000 - mae: 2202.6741 - val_loss: 1774.3276 - val_mse: 25541950.0000 - val_mae: 1774.3278\n",
            "Epoch 472/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2188.0030 - mse: 32134446.0000 - mae: 2188.0029 - val_loss: 1777.8469 - val_mse: 25541776.0000 - val_mae: 1777.8469\n",
            "Epoch 473/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2207.5245 - mse: 32534070.0000 - mae: 2207.5247 - val_loss: 1784.3038 - val_mse: 25545598.0000 - val_mae: 1784.3038\n",
            "Epoch 474/500\n",
            "380/380 [==============================] - 0s 129us/step - loss: 2196.6981 - mse: 32421402.0000 - mae: 2196.6982 - val_loss: 1776.7680 - val_mse: 25496362.0000 - val_mae: 1776.7679\n",
            "Epoch 475/500\n",
            "380/380 [==============================] - 0s 136us/step - loss: 2206.1596 - mse: 32541664.0000 - mae: 2206.1597 - val_loss: 1773.7692 - val_mse: 25465026.0000 - val_mae: 1773.7693\n",
            "Epoch 476/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2184.9332 - mse: 32266882.0000 - mae: 2184.9331 - val_loss: 1768.1014 - val_mse: 25405800.0000 - val_mae: 1768.1013\n",
            "Epoch 477/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2195.6467 - mse: 32316810.0000 - mae: 2195.6467 - val_loss: 1767.7556 - val_mse: 25386682.0000 - val_mae: 1767.7556\n",
            "Epoch 478/500\n",
            "380/380 [==============================] - 0s 133us/step - loss: 2199.8003 - mse: 32588676.0000 - mae: 2199.8003 - val_loss: 1769.3103 - val_mse: 25374670.0000 - val_mae: 1769.3103\n",
            "Epoch 479/500\n",
            "380/380 [==============================] - 0s 118us/step - loss: 2178.3730 - mse: 32048214.0000 - mae: 2178.3730 - val_loss: 1770.6429 - val_mse: 25363920.0000 - val_mae: 1770.6429\n",
            "Epoch 480/500\n",
            "380/380 [==============================] - 0s 135us/step - loss: 2180.1803 - mse: 32125802.0000 - mae: 2180.1802 - val_loss: 1765.3974 - val_mse: 25305962.0000 - val_mae: 1765.3973\n",
            "Epoch 481/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2187.5830 - mse: 32421220.0000 - mae: 2187.5830 - val_loss: 1768.2877 - val_mse: 25305854.0000 - val_mae: 1768.2877\n",
            "Epoch 482/500\n",
            "380/380 [==============================] - 0s 129us/step - loss: 2178.2017 - mse: 32061432.0000 - mae: 2178.2017 - val_loss: 1768.0874 - val_mse: 25285222.0000 - val_mae: 1768.0875\n",
            "Epoch 483/500\n",
            "380/380 [==============================] - 0s 142us/step - loss: 2192.2623 - mse: 31934846.0000 - mae: 2192.2625 - val_loss: 1772.4453 - val_mse: 25283460.0000 - val_mae: 1772.4453\n",
            "Epoch 484/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2179.1956 - mse: 32004602.0000 - mae: 2179.1956 - val_loss: 1768.9540 - val_mse: 25242954.0000 - val_mae: 1768.9539\n",
            "Epoch 485/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2152.1489 - mse: 31747012.0000 - mae: 2152.1489 - val_loss: 1767.0397 - val_mse: 25208974.0000 - val_mae: 1767.0397\n",
            "Epoch 486/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2171.2688 - mse: 31927140.0000 - mae: 2171.2688 - val_loss: 1771.5316 - val_mse: 25210144.0000 - val_mae: 1771.5317\n",
            "Epoch 487/500\n",
            "380/380 [==============================] - 0s 146us/step - loss: 2163.7424 - mse: 32081316.0000 - mae: 2163.7424 - val_loss: 1767.7592 - val_mse: 25173780.0000 - val_mae: 1767.7592\n",
            "Epoch 488/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2171.6214 - mse: 31955810.0000 - mae: 2171.6213 - val_loss: 1766.0467 - val_mse: 25144416.0000 - val_mae: 1766.0466\n",
            "Epoch 489/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2176.8817 - mse: 32004126.0000 - mae: 2176.8818 - val_loss: 1766.0107 - val_mse: 25117514.0000 - val_mae: 1766.0106\n",
            "Epoch 490/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2158.2004 - mse: 31696530.0000 - mae: 2158.2004 - val_loss: 1774.7146 - val_mse: 25133144.0000 - val_mae: 1774.7145\n",
            "Epoch 491/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2173.4677 - mse: 31690366.0000 - mae: 2173.4678 - val_loss: 1766.9567 - val_mse: 25086716.0000 - val_mae: 1766.9568\n",
            "Epoch 492/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2132.7767 - mse: 31645132.0000 - mae: 2132.7766 - val_loss: 1763.2207 - val_mse: 25041324.0000 - val_mae: 1763.2208\n",
            "Epoch 493/500\n",
            "380/380 [==============================] - 0s 102us/step - loss: 2162.2057 - mse: 31743124.0000 - mae: 2162.2056 - val_loss: 1766.1014 - val_mse: 25037300.0000 - val_mae: 1766.1016\n",
            "Epoch 494/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2145.1160 - mse: 31632524.0000 - mae: 2145.1162 - val_loss: 1776.1406 - val_mse: 25058270.0000 - val_mae: 1776.1405\n",
            "Epoch 495/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2168.3629 - mse: 31677066.0000 - mae: 2168.3628 - val_loss: 1767.4213 - val_mse: 24997902.0000 - val_mae: 1767.4213\n",
            "Epoch 496/500\n",
            "380/380 [==============================] - 0s 132us/step - loss: 2147.3823 - mse: 31468534.0000 - mae: 2147.3823 - val_loss: 1780.6792 - val_mse: 25022098.0000 - val_mae: 1780.6792\n",
            "Epoch 497/500\n",
            "380/380 [==============================] - 0s 120us/step - loss: 2153.1028 - mse: 31350554.0000 - mae: 2153.1025 - val_loss: 1774.1942 - val_mse: 24976126.0000 - val_mae: 1774.1942\n",
            "Epoch 498/500\n",
            "380/380 [==============================] - 0s 135us/step - loss: 2153.4251 - mse: 31478360.0000 - mae: 2153.4250 - val_loss: 1768.6183 - val_mse: 24931946.0000 - val_mae: 1768.6183\n",
            "Epoch 499/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2142.0890 - mse: 31696440.0000 - mae: 2142.0889 - val_loss: 1769.7887 - val_mse: 24916810.0000 - val_mae: 1769.7887\n",
            "Epoch 500/500\n",
            "380/380 [==============================] - 0s 120us/step - loss: 2127.2591 - mse: 31268622.0000 - mae: 2127.2593 - val_loss: 1770.6304 - val_mse: 24898164.0000 - val_mae: 1770.6304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 750 samples, validate on 750 samples\n",
            "Epoch 1/500\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 2424.7415 - mse: 31068046.0000 - mae: 2424.7415 - val_loss: 2618.7375 - val_mse: 37074252.0000 - val_mae: 2618.7375\n",
            "Epoch 2/500\n",
            "750/750 [==============================] - 0s 67us/step - loss: 2424.7091 - mse: 31067884.0000 - mae: 2424.7092 - val_loss: 2618.7100 - val_mse: 37074104.0000 - val_mae: 2618.7100\n",
            "Epoch 3/500\n",
            "750/750 [==============================] - 0s 63us/step - loss: 2424.6800 - mse: 31067744.0000 - mae: 2424.6802 - val_loss: 2618.6679 - val_mse: 37073896.0000 - val_mae: 2618.6677\n",
            "Epoch 4/500\n",
            "750/750 [==============================] - 0s 59us/step - loss: 2424.6188 - mse: 31067456.0000 - mae: 2424.6189 - val_loss: 2618.5103 - val_mse: 37073184.0000 - val_mae: 2618.5103\n",
            "Epoch 5/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2424.3164 - mse: 31066014.0000 - mae: 2424.3164 - val_loss: 2617.5102 - val_mse: 37068848.0000 - val_mae: 2617.5103\n",
            "Epoch 6/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2422.7000 - mse: 31058946.0000 - mae: 2422.7000 - val_loss: 2613.8130 - val_mse: 37052188.0000 - val_mae: 2613.8130\n",
            "Epoch 7/500\n",
            "750/750 [==============================] - 0s 67us/step - loss: 2418.2027 - mse: 31038352.0000 - mae: 2418.2026 - val_loss: 2607.6422 - val_mse: 37023280.0000 - val_mae: 2607.6421\n",
            "Epoch 8/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 2412.5474 - mse: 31012746.0000 - mae: 2412.5474 - val_loss: 2601.1156 - val_mse: 36991472.0000 - val_mae: 2601.1157\n",
            "Epoch 9/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2405.8530 - mse: 30978796.0000 - mae: 2405.8530 - val_loss: 2594.1843 - val_mse: 36956556.0000 - val_mae: 2594.1843\n",
            "Epoch 10/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2397.6316 - mse: 30942418.0000 - mae: 2397.6316 - val_loss: 2586.6116 - val_mse: 36917872.0000 - val_mae: 2586.6116\n",
            "Epoch 11/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2390.8132 - mse: 30902854.0000 - mae: 2390.8130 - val_loss: 2578.8986 - val_mse: 36877384.0000 - val_mae: 2578.8984\n",
            "Epoch 12/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2383.2820 - mse: 30862032.0000 - mae: 2383.2820 - val_loss: 2570.7995 - val_mse: 36832764.0000 - val_mae: 2570.7996\n",
            "Epoch 13/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2376.0800 - mse: 30825906.0000 - mae: 2376.0801 - val_loss: 2562.7834 - val_mse: 36785160.0000 - val_mae: 2562.7834\n",
            "Epoch 14/500\n",
            "750/750 [==============================] - 0s 59us/step - loss: 2364.9820 - mse: 30766224.0000 - mae: 2364.9819 - val_loss: 2554.1198 - val_mse: 36732600.0000 - val_mae: 2554.1199\n",
            "Epoch 15/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2356.7030 - mse: 30717296.0000 - mae: 2356.7031 - val_loss: 2545.3688 - val_mse: 36677852.0000 - val_mae: 2545.3687\n",
            "Epoch 16/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2348.5086 - mse: 30668712.0000 - mae: 2348.5085 - val_loss: 2536.2834 - val_mse: 36619604.0000 - val_mae: 2536.2834\n",
            "Epoch 17/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2339.5995 - mse: 30617646.0000 - mae: 2339.5996 - val_loss: 2527.1936 - val_mse: 36560332.0000 - val_mae: 2527.1936\n",
            "Epoch 18/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 2331.7472 - mse: 30566640.0000 - mae: 2331.7471 - val_loss: 2517.8668 - val_mse: 36498608.0000 - val_mae: 2517.8669\n",
            "Epoch 19/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2321.3102 - mse: 30494932.0000 - mae: 2321.3101 - val_loss: 2508.2673 - val_mse: 36434560.0000 - val_mae: 2508.2673\n",
            "Epoch 20/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 2315.0736 - mse: 30445760.0000 - mae: 2315.0737 - val_loss: 2498.7987 - val_mse: 36369832.0000 - val_mae: 2498.7988\n",
            "Epoch 21/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2304.1532 - mse: 30373344.0000 - mae: 2304.1531 - val_loss: 2489.5923 - val_mse: 36304628.0000 - val_mae: 2489.5923\n",
            "Epoch 22/500\n",
            "750/750 [==============================] - 0s 58us/step - loss: 2290.4399 - mse: 30303880.0000 - mae: 2290.4399 - val_loss: 2480.2395 - val_mse: 36235972.0000 - val_mae: 2480.2395\n",
            "Epoch 23/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2286.9706 - mse: 30278150.0000 - mae: 2286.9707 - val_loss: 2471.7520 - val_mse: 36170612.0000 - val_mae: 2471.7520\n",
            "Epoch 24/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 2278.1212 - mse: 30200244.0000 - mae: 2278.1211 - val_loss: 2463.8184 - val_mse: 36104860.0000 - val_mae: 2463.8186\n",
            "Epoch 25/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2270.4736 - mse: 30112150.0000 - mae: 2270.4736 - val_loss: 2456.0261 - val_mse: 36036676.0000 - val_mae: 2456.0264\n",
            "Epoch 26/500\n",
            "750/750 [==============================] - 0s 61us/step - loss: 2264.9180 - mse: 30088888.0000 - mae: 2264.9180 - val_loss: 2448.8159 - val_mse: 35971592.0000 - val_mae: 2448.8159\n",
            "Epoch 27/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2258.1474 - mse: 30001474.0000 - mae: 2258.1472 - val_loss: 2441.6672 - val_mse: 35906952.0000 - val_mae: 2441.6672\n",
            "Epoch 28/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 2255.2153 - mse: 29987690.0000 - mae: 2255.2153 - val_loss: 2435.0192 - val_mse: 35846128.0000 - val_mae: 2435.0193\n",
            "Epoch 29/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2248.0793 - mse: 29935752.0000 - mae: 2248.0796 - val_loss: 2428.3293 - val_mse: 35783988.0000 - val_mae: 2428.3296\n",
            "Epoch 30/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2243.0234 - mse: 29839202.0000 - mae: 2243.0234 - val_loss: 2422.3315 - val_mse: 35724844.0000 - val_mae: 2422.3315\n",
            "Epoch 31/500\n",
            "750/750 [==============================] - 0s 63us/step - loss: 2243.0010 - mse: 29856486.0000 - mae: 2243.0010 - val_loss: 2417.2252 - val_mse: 35670028.0000 - val_mae: 2417.2251\n",
            "Epoch 32/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2238.6069 - mse: 29764308.0000 - mae: 2238.6069 - val_loss: 2412.8612 - val_mse: 35620968.0000 - val_mae: 2412.8611\n",
            "Epoch 33/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2234.5262 - mse: 29742798.0000 - mae: 2234.5261 - val_loss: 2408.8686 - val_mse: 35574376.0000 - val_mae: 2408.8687\n",
            "Epoch 34/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2223.8929 - mse: 29636772.0000 - mae: 2223.8928 - val_loss: 2404.2282 - val_mse: 35517540.0000 - val_mae: 2404.2283\n",
            "Epoch 35/500\n",
            "750/750 [==============================] - 0s 60us/step - loss: 2226.3045 - mse: 29640414.0000 - mae: 2226.3047 - val_loss: 2400.3998 - val_mse: 35468852.0000 - val_mae: 2400.3997\n",
            "Epoch 36/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2222.2995 - mse: 29647092.0000 - mae: 2222.2996 - val_loss: 2397.1194 - val_mse: 35425024.0000 - val_mae: 2397.1194\n",
            "Epoch 37/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2225.2987 - mse: 29561422.0000 - mae: 2225.2986 - val_loss: 2393.7857 - val_mse: 35378836.0000 - val_mae: 2393.7856\n",
            "Epoch 38/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2218.2253 - mse: 29488300.0000 - mae: 2218.2253 - val_loss: 2390.5295 - val_mse: 35332748.0000 - val_mae: 2390.5298\n",
            "Epoch 39/500\n",
            "750/750 [==============================] - 0s 67us/step - loss: 2215.6401 - mse: 29484710.0000 - mae: 2215.6399 - val_loss: 2387.4681 - val_mse: 35288220.0000 - val_mae: 2387.4683\n",
            "Epoch 40/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2211.9540 - mse: 29435336.0000 - mae: 2211.9541 - val_loss: 2384.6501 - val_mse: 35246480.0000 - val_mae: 2384.6501\n",
            "Epoch 41/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2216.6820 - mse: 29434100.0000 - mae: 2216.6819 - val_loss: 2381.8482 - val_mse: 35203932.0000 - val_mae: 2381.8481\n",
            "Epoch 42/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2212.9866 - mse: 29404272.0000 - mae: 2212.9866 - val_loss: 2379.3575 - val_mse: 35165348.0000 - val_mae: 2379.3574\n",
            "Epoch 43/500\n",
            "750/750 [==============================] - 0s 61us/step - loss: 2208.8182 - mse: 29289758.0000 - mae: 2208.8181 - val_loss: 2376.9130 - val_mse: 35126240.0000 - val_mae: 2376.9131\n",
            "Epoch 44/500\n",
            "750/750 [==============================] - 0s 59us/step - loss: 2202.4941 - mse: 29304330.0000 - mae: 2202.4939 - val_loss: 2374.4722 - val_mse: 35085708.0000 - val_mae: 2374.4722\n",
            "Epoch 45/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2203.2447 - mse: 29271676.0000 - mae: 2203.2446 - val_loss: 2372.0699 - val_mse: 35044684.0000 - val_mae: 2372.0698\n",
            "Epoch 46/500\n",
            "750/750 [==============================] - 0s 75us/step - loss: 2205.9620 - mse: 29244058.0000 - mae: 2205.9619 - val_loss: 2369.7129 - val_mse: 35001340.0000 - val_mae: 2369.7129\n",
            "Epoch 47/500\n",
            "750/750 [==============================] - 0s 67us/step - loss: 2202.2464 - mse: 29135832.0000 - mae: 2202.2463 - val_loss: 2367.3641 - val_mse: 34957480.0000 - val_mae: 2367.3640\n",
            "Epoch 48/500\n",
            "750/750 [==============================] - 0s 58us/step - loss: 2194.8490 - mse: 29110652.0000 - mae: 2194.8491 - val_loss: 2364.6948 - val_mse: 34907452.0000 - val_mae: 2364.6948\n",
            "Epoch 49/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2194.0022 - mse: 29078100.0000 - mae: 2194.0022 - val_loss: 2362.2479 - val_mse: 34860956.0000 - val_mae: 2362.2478\n",
            "Epoch 50/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2199.6083 - mse: 28981272.0000 - mae: 2199.6084 - val_loss: 2359.8922 - val_mse: 34816204.0000 - val_mae: 2359.8923\n",
            "Epoch 51/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 2184.5007 - mse: 28857262.0000 - mae: 2184.5007 - val_loss: 2357.3323 - val_mse: 34766092.0000 - val_mae: 2357.3325\n",
            "Epoch 52/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2189.4723 - mse: 28907510.0000 - mae: 2189.4724 - val_loss: 2355.0168 - val_mse: 34719680.0000 - val_mae: 2355.0166\n",
            "Epoch 53/500\n",
            "750/750 [==============================] - 0s 61us/step - loss: 2188.6219 - mse: 28920632.0000 - mae: 2188.6218 - val_loss: 2352.8668 - val_mse: 34675572.0000 - val_mae: 2352.8667\n",
            "Epoch 54/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2187.0348 - mse: 28941392.0000 - mae: 2187.0349 - val_loss: 2350.4905 - val_mse: 34625680.0000 - val_mae: 2350.4905\n",
            "Epoch 55/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 2185.3532 - mse: 28705396.0000 - mae: 2185.3533 - val_loss: 2348.4199 - val_mse: 34584904.0000 - val_mae: 2348.4199\n",
            "Epoch 56/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2180.2143 - mse: 28719246.0000 - mae: 2180.2144 - val_loss: 2345.7339 - val_mse: 34531192.0000 - val_mae: 2345.7339\n",
            "Epoch 57/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 2175.9530 - mse: 28649224.0000 - mae: 2175.9529 - val_loss: 2343.1363 - val_mse: 34480728.0000 - val_mae: 2343.1362\n",
            "Epoch 58/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2175.3322 - mse: 28666828.0000 - mae: 2175.3323 - val_loss: 2340.8788 - val_mse: 34439228.0000 - val_mae: 2340.8787\n",
            "Epoch 59/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 2170.0310 - mse: 28575066.0000 - mae: 2170.0310 - val_loss: 2338.5425 - val_mse: 34396896.0000 - val_mae: 2338.5427\n",
            "Epoch 60/500\n",
            "750/750 [==============================] - 0s 59us/step - loss: 2167.4863 - mse: 28433824.0000 - mae: 2167.4863 - val_loss: 2336.3500 - val_mse: 34358628.0000 - val_mae: 2336.3501\n",
            "Epoch 61/500\n",
            "750/750 [==============================] - 0s 63us/step - loss: 2177.6638 - mse: 28478204.0000 - mae: 2177.6641 - val_loss: 2334.4391 - val_mse: 34325092.0000 - val_mae: 2334.4390\n",
            "Epoch 62/500\n",
            "750/750 [==============================] - 0s 59us/step - loss: 2171.2740 - mse: 28467542.0000 - mae: 2171.2739 - val_loss: 2332.6473 - val_mse: 34291932.0000 - val_mae: 2332.6472\n",
            "Epoch 63/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2173.6043 - mse: 28607774.0000 - mae: 2173.6042 - val_loss: 2331.2010 - val_mse: 34268024.0000 - val_mae: 2331.2009\n",
            "Epoch 64/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2177.0427 - mse: 28450638.0000 - mae: 2177.0427 - val_loss: 2329.4026 - val_mse: 34232516.0000 - val_mae: 2329.4026\n",
            "Epoch 65/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2174.2793 - mse: 28419978.0000 - mae: 2174.2793 - val_loss: 2327.9843 - val_mse: 34207088.0000 - val_mae: 2327.9844\n",
            "Epoch 66/500\n",
            "750/750 [==============================] - 0s 60us/step - loss: 2165.8726 - mse: 28354380.0000 - mae: 2165.8726 - val_loss: 2326.6520 - val_mse: 34177528.0000 - val_mae: 2326.6521\n",
            "Epoch 67/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2163.0245 - mse: 28315436.0000 - mae: 2163.0244 - val_loss: 2325.0333 - val_mse: 34145464.0000 - val_mae: 2325.0332\n",
            "Epoch 68/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2170.9993 - mse: 28262368.0000 - mae: 2170.9993 - val_loss: 2323.8138 - val_mse: 34121084.0000 - val_mae: 2323.8137\n",
            "Epoch 69/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2166.1132 - mse: 28330634.0000 - mae: 2166.1133 - val_loss: 2322.5155 - val_mse: 34095948.0000 - val_mae: 2322.5154\n",
            "Epoch 70/500\n",
            "750/750 [==============================] - 0s 59us/step - loss: 2159.5086 - mse: 28355668.0000 - mae: 2159.5085 - val_loss: 2321.2881 - val_mse: 34072672.0000 - val_mae: 2321.2881\n",
            "Epoch 71/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2155.6557 - mse: 28234400.0000 - mae: 2155.6558 - val_loss: 2320.0290 - val_mse: 34038724.0000 - val_mae: 2320.0291\n",
            "Epoch 72/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 2164.3944 - mse: 28304294.0000 - mae: 2164.3943 - val_loss: 2319.0018 - val_mse: 34015332.0000 - val_mae: 2319.0020\n",
            "Epoch 73/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2154.8250 - mse: 28180808.0000 - mae: 2154.8252 - val_loss: 2317.9032 - val_mse: 33988552.0000 - val_mae: 2317.9031\n",
            "Epoch 74/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2168.1644 - mse: 28300264.0000 - mae: 2168.1643 - val_loss: 2316.9208 - val_mse: 33968724.0000 - val_mae: 2316.9209\n",
            "Epoch 75/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 2169.4382 - mse: 28216214.0000 - mae: 2169.4382 - val_loss: 2316.0820 - val_mse: 33952136.0000 - val_mae: 2316.0820\n",
            "Epoch 76/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 2155.3788 - mse: 28176786.0000 - mae: 2155.3787 - val_loss: 2314.9683 - val_mse: 33930840.0000 - val_mae: 2314.9683\n",
            "Epoch 77/500\n",
            "750/750 [==============================] - 0s 63us/step - loss: 2136.6230 - mse: 28180896.0000 - mae: 2136.6230 - val_loss: 2313.6995 - val_mse: 33901416.0000 - val_mae: 2313.6995\n",
            "Epoch 78/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2160.7316 - mse: 28254066.0000 - mae: 2160.7317 - val_loss: 2312.4991 - val_mse: 33895504.0000 - val_mae: 2312.4993\n",
            "Epoch 79/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2157.9004 - mse: 28151884.0000 - mae: 2157.9004 - val_loss: 2311.1820 - val_mse: 33881304.0000 - val_mae: 2311.1819\n",
            "Epoch 80/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2150.2583 - mse: 28059762.0000 - mae: 2150.2583 - val_loss: 2308.9612 - val_mse: 33881076.0000 - val_mae: 2308.9614\n",
            "Epoch 81/500\n",
            "750/750 [==============================] - 0s 67us/step - loss: 2150.2176 - mse: 28231478.0000 - mae: 2150.2178 - val_loss: 2306.8478 - val_mse: 33875572.0000 - val_mae: 2306.8477\n",
            "Epoch 82/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2140.0839 - mse: 28173358.0000 - mae: 2140.0840 - val_loss: 2305.0069 - val_mse: 33869616.0000 - val_mae: 2305.0068\n",
            "Epoch 83/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2150.4269 - mse: 28084134.0000 - mae: 2150.4270 - val_loss: 2303.1344 - val_mse: 33861180.0000 - val_mae: 2303.1343\n",
            "Epoch 84/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2139.7801 - mse: 28204576.0000 - mae: 2139.7803 - val_loss: 2300.9686 - val_mse: 33846816.0000 - val_mae: 2300.9688\n",
            "Epoch 85/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2149.2210 - mse: 28249968.0000 - mae: 2149.2209 - val_loss: 2298.3141 - val_mse: 33856032.0000 - val_mae: 2298.3140\n",
            "Epoch 86/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2137.3833 - mse: 28145528.0000 - mae: 2137.3833 - val_loss: 2294.9507 - val_mse: 33883552.0000 - val_mae: 2294.9509\n",
            "Epoch 87/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 2129.2915 - mse: 28246494.0000 - mae: 2129.2915 - val_loss: 2291.7541 - val_mse: 33878464.0000 - val_mae: 2291.7539\n",
            "Epoch 88/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2138.2375 - mse: 28262402.0000 - mae: 2138.2375 - val_loss: 2289.0369 - val_mse: 33915428.0000 - val_mae: 2289.0369\n",
            "Epoch 89/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2128.4236 - mse: 28313554.0000 - mae: 2128.4236 - val_loss: 2286.9155 - val_mse: 33995784.0000 - val_mae: 2286.9158\n",
            "Epoch 90/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2113.4859 - mse: 28290512.0000 - mae: 2113.4861 - val_loss: 2283.7474 - val_mse: 33945756.0000 - val_mae: 2283.7476\n",
            "Epoch 91/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2122.7607 - mse: 28281564.0000 - mae: 2122.7607 - val_loss: 2283.1278 - val_mse: 34005064.0000 - val_mae: 2283.1279\n",
            "Epoch 92/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2111.1465 - mse: 28311566.0000 - mae: 2111.1465 - val_loss: 2277.2980 - val_mse: 33771096.0000 - val_mae: 2277.2981\n",
            "Epoch 93/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2098.5919 - mse: 28141186.0000 - mae: 2098.5920 - val_loss: 2274.2695 - val_mse: 33760896.0000 - val_mae: 2274.2693\n",
            "Epoch 94/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2113.7312 - mse: 28145838.0000 - mae: 2113.7314 - val_loss: 2272.5247 - val_mse: 33818352.0000 - val_mae: 2272.5247\n",
            "Epoch 95/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 2105.8810 - mse: 28092758.0000 - mae: 2105.8811 - val_loss: 2268.8182 - val_mse: 33701812.0000 - val_mae: 2268.8181\n",
            "Epoch 96/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2104.1017 - mse: 28197766.0000 - mae: 2104.1016 - val_loss: 2266.3412 - val_mse: 33529716.0000 - val_mae: 2266.3413\n",
            "Epoch 97/500\n",
            "750/750 [==============================] - 0s 59us/step - loss: 2100.7027 - mse: 27895090.0000 - mae: 2100.7026 - val_loss: 2263.0363 - val_mse: 33601120.0000 - val_mae: 2263.0364\n",
            "Epoch 98/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2093.6593 - mse: 28012396.0000 - mae: 2093.6594 - val_loss: 2261.2173 - val_mse: 33605884.0000 - val_mae: 2261.2173\n",
            "Epoch 99/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2085.5419 - mse: 28037332.0000 - mae: 2085.5420 - val_loss: 2258.5547 - val_mse: 33450420.0000 - val_mae: 2258.5549\n",
            "Epoch 100/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2098.1242 - mse: 27745850.0000 - mae: 2098.1243 - val_loss: 2257.0952 - val_mse: 33582260.0000 - val_mae: 2257.0952\n",
            "Epoch 101/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 2082.3062 - mse: 27931088.0000 - mae: 2082.3064 - val_loss: 2254.0058 - val_mse: 33451576.0000 - val_mae: 2254.0056\n",
            "Epoch 102/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2085.5372 - mse: 27896860.0000 - mae: 2085.5374 - val_loss: 2251.8579 - val_mse: 33476446.0000 - val_mae: 2251.8579\n",
            "Epoch 103/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2076.3055 - mse: 27916096.0000 - mae: 2076.3054 - val_loss: 2249.3255 - val_mse: 33390606.0000 - val_mae: 2249.3254\n",
            "Epoch 104/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2076.3456 - mse: 27744240.0000 - mae: 2076.3457 - val_loss: 2249.9373 - val_mse: 33510458.0000 - val_mae: 2249.9373\n",
            "Epoch 105/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2078.0780 - mse: 27772492.0000 - mae: 2078.0779 - val_loss: 2245.0329 - val_mse: 33280734.0000 - val_mae: 2245.0330\n",
            "Epoch 106/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 2072.7919 - mse: 27654614.0000 - mae: 2072.7917 - val_loss: 2243.0557 - val_mse: 33267068.0000 - val_mae: 2243.0557\n",
            "Epoch 107/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2063.5724 - mse: 27623536.0000 - mae: 2063.5723 - val_loss: 2241.5058 - val_mse: 33315394.0000 - val_mae: 2241.5059\n",
            "Epoch 108/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2079.6047 - mse: 27755586.0000 - mae: 2079.6047 - val_loss: 2241.1257 - val_mse: 33351924.0000 - val_mae: 2241.1257\n",
            "Epoch 109/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2058.9189 - mse: 27563344.0000 - mae: 2058.9189 - val_loss: 2237.2602 - val_mse: 33211518.0000 - val_mae: 2237.2603\n",
            "Epoch 110/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2072.5297 - mse: 27472338.0000 - mae: 2072.5298 - val_loss: 2242.0786 - val_mse: 33377580.0000 - val_mae: 2242.0786\n",
            "Epoch 111/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2071.2032 - mse: 27634352.0000 - mae: 2071.2031 - val_loss: 2237.1097 - val_mse: 33267134.0000 - val_mae: 2237.1096\n",
            "Epoch 112/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2067.8674 - mse: 27743394.0000 - mae: 2067.8674 - val_loss: 2232.8904 - val_mse: 33155974.0000 - val_mae: 2232.8904\n",
            "Epoch 113/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 2062.6108 - mse: 27514422.0000 - mae: 2062.6108 - val_loss: 2230.4177 - val_mse: 33043914.0000 - val_mae: 2230.4177\n",
            "Epoch 114/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2036.1860 - mse: 27358536.0000 - mae: 2036.1860 - val_loss: 2228.6590 - val_mse: 32919116.0000 - val_mae: 2228.6589\n",
            "Epoch 115/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2051.1392 - mse: 27421238.0000 - mae: 2051.1392 - val_loss: 2228.0993 - val_mse: 32834074.0000 - val_mae: 2228.0994\n",
            "Epoch 116/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2052.3367 - mse: 27448310.0000 - mae: 2052.3367 - val_loss: 2226.0516 - val_mse: 32987928.0000 - val_mae: 2226.0515\n",
            "Epoch 117/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 2062.6064 - mse: 27565086.0000 - mae: 2062.6064 - val_loss: 2226.0504 - val_mse: 32726082.0000 - val_mae: 2226.0503\n",
            "Epoch 118/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 2051.9808 - mse: 27269536.0000 - mae: 2051.9810 - val_loss: 2222.9738 - val_mse: 32803578.0000 - val_mae: 2222.9739\n",
            "Epoch 119/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2051.0872 - mse: 27397496.0000 - mae: 2051.0874 - val_loss: 2223.1830 - val_mse: 32689072.0000 - val_mae: 2223.1831\n",
            "Epoch 120/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2045.8297 - mse: 27303728.0000 - mae: 2045.8297 - val_loss: 2222.7411 - val_mse: 32905120.0000 - val_mae: 2222.7410\n",
            "Epoch 121/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2036.6934 - mse: 27187670.0000 - mae: 2036.6934 - val_loss: 2219.4455 - val_mse: 32785724.0000 - val_mae: 2219.4456\n",
            "Epoch 122/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2039.5649 - mse: 27024532.0000 - mae: 2039.5648 - val_loss: 2219.6068 - val_mse: 32576066.0000 - val_mae: 2219.6069\n",
            "Epoch 123/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 2038.7233 - mse: 27324648.0000 - mae: 2038.7234 - val_loss: 2217.7504 - val_mse: 32765474.0000 - val_mae: 2217.7502\n",
            "Epoch 124/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2033.6336 - mse: 26993826.0000 - mae: 2033.6337 - val_loss: 2217.0894 - val_mse: 32706498.0000 - val_mae: 2217.0896\n",
            "Epoch 125/500\n",
            "750/750 [==============================] - 0s 67us/step - loss: 2024.7017 - mse: 27105068.0000 - mae: 2024.7017 - val_loss: 2215.9691 - val_mse: 32625686.0000 - val_mae: 2215.9692\n",
            "Epoch 126/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 2042.5922 - mse: 27025656.0000 - mae: 2042.5922 - val_loss: 2215.7310 - val_mse: 32683332.0000 - val_mae: 2215.7310\n",
            "Epoch 127/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2021.3962 - mse: 27037308.0000 - mae: 2021.3961 - val_loss: 2215.4862 - val_mse: 32465014.0000 - val_mae: 2215.4861\n",
            "Epoch 128/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2022.6748 - mse: 26832004.0000 - mae: 2022.6748 - val_loss: 2216.1146 - val_mse: 32685626.0000 - val_mae: 2216.1145\n",
            "Epoch 129/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2021.4074 - mse: 27185338.0000 - mae: 2021.4073 - val_loss: 2212.6098 - val_mse: 32492334.0000 - val_mae: 2212.6099\n",
            "Epoch 130/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 2035.7449 - mse: 27114920.0000 - mae: 2035.7450 - val_loss: 2212.3931 - val_mse: 32582180.0000 - val_mae: 2212.3931\n",
            "Epoch 131/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2017.7196 - mse: 27079234.0000 - mae: 2017.7197 - val_loss: 2214.9523 - val_mse: 32259296.0000 - val_mae: 2214.9524\n",
            "Epoch 132/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2028.3540 - mse: 26871066.0000 - mae: 2028.3540 - val_loss: 2211.4972 - val_mse: 32447758.0000 - val_mae: 2211.4971\n",
            "Epoch 133/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2023.8309 - mse: 26900788.0000 - mae: 2023.8311 - val_loss: 2211.7264 - val_mse: 32336940.0000 - val_mae: 2211.7263\n",
            "Epoch 134/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2030.0618 - mse: 27010466.0000 - mae: 2030.0619 - val_loss: 2211.0792 - val_mse: 32501102.0000 - val_mae: 2211.0791\n",
            "Epoch 135/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2032.6401 - mse: 26924718.0000 - mae: 2032.6400 - val_loss: 2210.7180 - val_mse: 32321932.0000 - val_mae: 2210.7180\n",
            "Epoch 136/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2031.2773 - mse: 26715138.0000 - mae: 2031.2773 - val_loss: 2210.1108 - val_mse: 32395516.0000 - val_mae: 2210.1108\n",
            "Epoch 137/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2024.8576 - mse: 26757628.0000 - mae: 2024.8577 - val_loss: 2220.1453 - val_mse: 32663890.0000 - val_mae: 2220.1453\n",
            "Epoch 138/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2031.0609 - mse: 26959556.0000 - mae: 2031.0610 - val_loss: 2208.8752 - val_mse: 32399518.0000 - val_mae: 2208.8752\n",
            "Epoch 139/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 2014.6034 - mse: 26567274.0000 - mae: 2014.6034 - val_loss: 2209.8644 - val_mse: 32461898.0000 - val_mae: 2209.8643\n",
            "Epoch 140/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2017.5780 - mse: 26834152.0000 - mae: 2017.5780 - val_loss: 2210.3403 - val_mse: 32461882.0000 - val_mae: 2210.3403\n",
            "Epoch 141/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2027.4804 - mse: 26963984.0000 - mae: 2027.4803 - val_loss: 2209.0086 - val_mse: 32422904.0000 - val_mae: 2209.0085\n",
            "Epoch 142/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 2013.2449 - mse: 26852014.0000 - mae: 2013.2450 - val_loss: 2219.4860 - val_mse: 32588746.0000 - val_mae: 2219.4861\n",
            "Epoch 143/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 2014.5920 - mse: 26789954.0000 - mae: 2014.5920 - val_loss: 2207.0740 - val_mse: 32301788.0000 - val_mae: 2207.0740\n",
            "Epoch 144/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2013.4893 - mse: 26661828.0000 - mae: 2013.4894 - val_loss: 2223.9497 - val_mse: 32610806.0000 - val_mae: 2223.9497\n",
            "Epoch 145/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 2019.0015 - mse: 26618826.0000 - mae: 2019.0017 - val_loss: 2226.4745 - val_mse: 32624070.0000 - val_mae: 2226.4744\n",
            "Epoch 146/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2022.7751 - mse: 26920304.0000 - mae: 2022.7750 - val_loss: 2216.9449 - val_mse: 32513892.0000 - val_mae: 2216.9448\n",
            "Epoch 147/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2011.6051 - mse: 26671256.0000 - mae: 2011.6050 - val_loss: 2206.7754 - val_mse: 32320804.0000 - val_mae: 2206.7754\n",
            "Epoch 148/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 2018.6658 - mse: 26636326.0000 - mae: 2018.6659 - val_loss: 2206.5971 - val_mse: 32274732.0000 - val_mae: 2206.5972\n",
            "Epoch 149/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 2020.2199 - mse: 26570062.0000 - mae: 2020.2200 - val_loss: 2208.3258 - val_mse: 32352816.0000 - val_mae: 2208.3259\n",
            "Epoch 150/500\n",
            "750/750 [==============================] - 0s 75us/step - loss: 2010.4115 - mse: 26734422.0000 - mae: 2010.4115 - val_loss: 2214.4669 - val_mse: 32453510.0000 - val_mae: 2214.4668\n",
            "Epoch 151/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 2005.2985 - mse: 26636622.0000 - mae: 2005.2987 - val_loss: 2206.0895 - val_mse: 32182412.0000 - val_mae: 2206.0896\n",
            "Epoch 152/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 2006.8222 - mse: 26441106.0000 - mae: 2006.8221 - val_loss: 2211.3662 - val_mse: 32366728.0000 - val_mae: 2211.3662\n",
            "Epoch 153/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 2017.6948 - mse: 26616568.0000 - mae: 2017.6948 - val_loss: 2209.2189 - val_mse: 32308582.0000 - val_mae: 2209.2188\n",
            "Epoch 154/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 2013.5194 - mse: 26694408.0000 - mae: 2013.5193 - val_loss: 2208.7890 - val_mse: 32314458.0000 - val_mae: 2208.7891\n",
            "Epoch 155/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2017.1944 - mse: 26745562.0000 - mae: 2017.1945 - val_loss: 2206.0523 - val_mse: 32260826.0000 - val_mae: 2206.0522\n",
            "Epoch 156/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2011.2639 - mse: 26556006.0000 - mae: 2011.2638 - val_loss: 2205.8746 - val_mse: 32184352.0000 - val_mae: 2205.8745\n",
            "Epoch 157/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 2010.8442 - mse: 26672798.0000 - mae: 2010.8441 - val_loss: 2207.3926 - val_mse: 32247806.0000 - val_mae: 2207.3926\n",
            "Epoch 158/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 2010.2313 - mse: 26334468.0000 - mae: 2010.2313 - val_loss: 2209.7874 - val_mse: 32280188.0000 - val_mae: 2209.7874\n",
            "Epoch 159/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2011.7478 - mse: 26500020.0000 - mae: 2011.7478 - val_loss: 2206.3515 - val_mse: 32101862.0000 - val_mae: 2206.3516\n",
            "Epoch 160/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 2021.4459 - mse: 26537798.0000 - mae: 2021.4460 - val_loss: 2207.6755 - val_mse: 32184774.0000 - val_mae: 2207.6755\n",
            "Epoch 161/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2005.8773 - mse: 26405440.0000 - mae: 2005.8773 - val_loss: 2207.6804 - val_mse: 32198022.0000 - val_mae: 2207.6804\n",
            "Epoch 162/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 2005.3367 - mse: 26504902.0000 - mae: 2005.3367 - val_loss: 2210.3959 - val_mse: 32265722.0000 - val_mae: 2210.3960\n",
            "Epoch 163/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2015.9082 - mse: 26432810.0000 - mae: 2015.9083 - val_loss: 2214.3580 - val_mse: 32325634.0000 - val_mae: 2214.3579\n",
            "Epoch 164/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2009.0258 - mse: 26350884.0000 - mae: 2009.0259 - val_loss: 2221.7317 - val_mse: 32395122.0000 - val_mae: 2221.7319\n",
            "Epoch 165/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 2012.0195 - mse: 26330200.0000 - mae: 2012.0195 - val_loss: 2229.7929 - val_mse: 32445634.0000 - val_mae: 2229.7927\n",
            "Epoch 166/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2003.4469 - mse: 26330480.0000 - mae: 2003.4470 - val_loss: 2211.4707 - val_mse: 32211124.0000 - val_mae: 2211.4707\n",
            "Epoch 167/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2003.4384 - mse: 26272296.0000 - mae: 2003.4384 - val_loss: 2208.7228 - val_mse: 32134568.0000 - val_mae: 2208.7229\n",
            "Epoch 168/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 2002.1176 - mse: 26256212.0000 - mae: 2002.1177 - val_loss: 2213.0976 - val_mse: 32212022.0000 - val_mae: 2213.0977\n",
            "Epoch 169/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2001.9441 - mse: 26620740.0000 - mae: 2001.9440 - val_loss: 2211.5628 - val_mse: 32184222.0000 - val_mae: 2211.5627\n",
            "Epoch 170/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2000.3335 - mse: 26473370.0000 - mae: 2000.3335 - val_loss: 2215.9494 - val_mse: 32236994.0000 - val_mae: 2215.9492\n",
            "Epoch 171/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 2004.8833 - mse: 26308916.0000 - mae: 2004.8833 - val_loss: 2210.8899 - val_mse: 32137926.0000 - val_mae: 2210.8899\n",
            "Epoch 172/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 2000.1853 - mse: 26297974.0000 - mae: 2000.1853 - val_loss: 2213.1704 - val_mse: 32160932.0000 - val_mae: 2213.1704\n",
            "Epoch 173/500\n",
            "750/750 [==============================] - 0s 75us/step - loss: 1998.5944 - mse: 26365820.0000 - mae: 1998.5944 - val_loss: 2211.8380 - val_mse: 32154802.0000 - val_mae: 2211.8381\n",
            "Epoch 174/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 2018.0839 - mse: 26510918.0000 - mae: 2018.0840 - val_loss: 2217.4451 - val_mse: 32198224.0000 - val_mae: 2217.4451\n",
            "Epoch 175/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 1997.7650 - mse: 26573046.0000 - mae: 1997.7650 - val_loss: 2208.7983 - val_mse: 32048784.0000 - val_mae: 2208.7983\n",
            "Epoch 176/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2000.7109 - mse: 26357910.0000 - mae: 2000.7108 - val_loss: 2214.0389 - val_mse: 32173010.0000 - val_mae: 2214.0388\n",
            "Epoch 177/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2001.2823 - mse: 26145052.0000 - mae: 2001.2823 - val_loss: 2215.4069 - val_mse: 32163736.0000 - val_mae: 2215.4070\n",
            "Epoch 178/500\n",
            "750/750 [==============================] - 0s 67us/step - loss: 1990.1594 - mse: 26264638.0000 - mae: 1990.1593 - val_loss: 2215.8295 - val_mse: 32174740.0000 - val_mae: 2215.8296\n",
            "Epoch 179/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 1998.8508 - mse: 26420134.0000 - mae: 1998.8508 - val_loss: 2213.0396 - val_mse: 32143086.0000 - val_mae: 2213.0396\n",
            "Epoch 180/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 1996.4336 - mse: 26363128.0000 - mae: 1996.4337 - val_loss: 2215.5215 - val_mse: 32147222.0000 - val_mae: 2215.5212\n",
            "Epoch 181/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2007.3561 - mse: 26163208.0000 - mae: 2007.3560 - val_loss: 2214.9377 - val_mse: 32121580.0000 - val_mae: 2214.9377\n",
            "Epoch 182/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1997.7810 - mse: 26233848.0000 - mae: 1997.7810 - val_loss: 2208.5582 - val_mse: 32004522.0000 - val_mae: 2208.5581\n",
            "Epoch 183/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1992.6981 - mse: 26080576.0000 - mae: 1992.6980 - val_loss: 2218.5907 - val_mse: 32146348.0000 - val_mae: 2218.5906\n",
            "Epoch 184/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 2000.3997 - mse: 26065458.0000 - mae: 2000.3997 - val_loss: 2214.4143 - val_mse: 32098040.0000 - val_mae: 2214.4143\n",
            "Epoch 185/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 1997.1808 - mse: 26412094.0000 - mae: 1997.1808 - val_loss: 2230.4087 - val_mse: 32279906.0000 - val_mae: 2230.4084\n",
            "Epoch 186/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1996.4925 - mse: 26071242.0000 - mae: 1996.4926 - val_loss: 2217.0223 - val_mse: 32148754.0000 - val_mae: 2217.0222\n",
            "Epoch 187/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1995.9793 - mse: 26118782.0000 - mae: 1995.9794 - val_loss: 2220.0884 - val_mse: 32154692.0000 - val_mae: 2220.0884\n",
            "Epoch 188/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1997.8176 - mse: 25995964.0000 - mae: 1997.8176 - val_loss: 2223.3190 - val_mse: 32159926.0000 - val_mae: 2223.3188\n",
            "Epoch 189/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 1986.0516 - mse: 26044804.0000 - mae: 1986.0515 - val_loss: 2223.6870 - val_mse: 32125950.0000 - val_mae: 2223.6870\n",
            "Epoch 190/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1996.0636 - mse: 26024318.0000 - mae: 1996.0637 - val_loss: 2208.9301 - val_mse: 31909460.0000 - val_mae: 2208.9302\n",
            "Epoch 191/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 1982.9341 - mse: 26179560.0000 - mae: 1982.9340 - val_loss: 2214.3460 - val_mse: 31972398.0000 - val_mae: 2214.3459\n",
            "Epoch 192/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1986.4256 - mse: 26180652.0000 - mae: 1986.4257 - val_loss: 2212.9617 - val_mse: 31943006.0000 - val_mae: 2212.9617\n",
            "Epoch 193/500\n",
            "750/750 [==============================] - 0s 99us/step - loss: 1988.5256 - mse: 26024564.0000 - mae: 1988.5256 - val_loss: 2212.0893 - val_mse: 31957134.0000 - val_mae: 2212.0894\n",
            "Epoch 194/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1989.6569 - mse: 26042018.0000 - mae: 1989.6570 - val_loss: 2218.0905 - val_mse: 32042980.0000 - val_mae: 2218.0906\n",
            "Epoch 195/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1980.7074 - mse: 25867084.0000 - mae: 1980.7073 - val_loss: 2240.1357 - val_mse: 32257800.0000 - val_mae: 2240.1357\n",
            "Epoch 196/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 1987.6257 - mse: 25966046.0000 - mae: 1987.6256 - val_loss: 2222.8317 - val_mse: 32056776.0000 - val_mae: 2222.8318\n",
            "Epoch 197/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1981.0366 - mse: 25773084.0000 - mae: 1981.0366 - val_loss: 2216.2095 - val_mse: 31965844.0000 - val_mae: 2216.2097\n",
            "Epoch 198/500\n",
            "750/750 [==============================] - 0s 111us/step - loss: 1981.5874 - mse: 25996826.0000 - mae: 1981.5873 - val_loss: 2216.4773 - val_mse: 31969226.0000 - val_mae: 2216.4771\n",
            "Epoch 199/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1977.9654 - mse: 25779606.0000 - mae: 1977.9653 - val_loss: 2224.6447 - val_mse: 32076112.0000 - val_mae: 2224.6448\n",
            "Epoch 200/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1978.5400 - mse: 26037108.0000 - mae: 1978.5400 - val_loss: 2222.0692 - val_mse: 32020346.0000 - val_mae: 2222.0693\n",
            "Epoch 201/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 1980.1426 - mse: 25986860.0000 - mae: 1980.1427 - val_loss: 2222.8695 - val_mse: 32031664.0000 - val_mae: 2222.8694\n",
            "Epoch 202/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 1991.7840 - mse: 26085778.0000 - mae: 1991.7841 - val_loss: 2233.0664 - val_mse: 32094610.0000 - val_mae: 2233.0664\n",
            "Epoch 203/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1984.1325 - mse: 25596882.0000 - mae: 1984.1327 - val_loss: 2227.0171 - val_mse: 32060484.0000 - val_mae: 2227.0171\n",
            "Epoch 204/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1985.9161 - mse: 26099550.0000 - mae: 1985.9160 - val_loss: 2237.2018 - val_mse: 32147396.0000 - val_mae: 2237.2017\n",
            "Epoch 205/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1996.6338 - mse: 26108658.0000 - mae: 1996.6338 - val_loss: 2240.4807 - val_mse: 32168668.0000 - val_mae: 2240.4807\n",
            "Epoch 206/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1995.0043 - mse: 26166128.0000 - mae: 1995.0043 - val_loss: 2225.9491 - val_mse: 32038050.0000 - val_mae: 2225.9490\n",
            "Epoch 207/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1993.4965 - mse: 25979388.0000 - mae: 1993.4963 - val_loss: 2224.8797 - val_mse: 32017012.0000 - val_mae: 2224.8799\n",
            "Epoch 208/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 2001.0179 - mse: 25843598.0000 - mae: 2001.0178 - val_loss: 2217.4842 - val_mse: 31925234.0000 - val_mae: 2217.4841\n",
            "Epoch 209/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1982.2142 - mse: 25818950.0000 - mae: 1982.2141 - val_loss: 2220.6664 - val_mse: 31947140.0000 - val_mae: 2220.6663\n",
            "Epoch 210/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1987.4453 - mse: 25851588.0000 - mae: 1987.4453 - val_loss: 2225.1312 - val_mse: 31999706.0000 - val_mae: 2225.1313\n",
            "Epoch 211/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1973.1889 - mse: 25786734.0000 - mae: 1973.1888 - val_loss: 2226.0073 - val_mse: 32034250.0000 - val_mae: 2226.0073\n",
            "Epoch 212/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1981.4565 - mse: 25983330.0000 - mae: 1981.4565 - val_loss: 2215.8866 - val_mse: 31891612.0000 - val_mae: 2215.8865\n",
            "Epoch 213/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1986.7547 - mse: 25705212.0000 - mae: 1986.7546 - val_loss: 2228.8099 - val_mse: 32028214.0000 - val_mae: 2228.8101\n",
            "Epoch 214/500\n",
            "750/750 [==============================] - 0s 92us/step - loss: 1973.1100 - mse: 26005832.0000 - mae: 1973.1100 - val_loss: 2225.8549 - val_mse: 32006346.0000 - val_mae: 2225.8547\n",
            "Epoch 215/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1984.9797 - mse: 25957046.0000 - mae: 1984.9799 - val_loss: 2225.3664 - val_mse: 31985162.0000 - val_mae: 2225.3665\n",
            "Epoch 216/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1987.9220 - mse: 26016298.0000 - mae: 1987.9220 - val_loss: 2255.2913 - val_mse: 32264926.0000 - val_mae: 2255.2913\n",
            "Epoch 217/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 1993.7325 - mse: 26201778.0000 - mae: 1993.7325 - val_loss: 2221.5534 - val_mse: 31915824.0000 - val_mae: 2221.5532\n",
            "Epoch 218/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1971.9306 - mse: 25658266.0000 - mae: 1971.9307 - val_loss: 2221.0700 - val_mse: 31921352.0000 - val_mae: 2221.0701\n",
            "Epoch 219/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1983.5223 - mse: 25607536.0000 - mae: 1983.5223 - val_loss: 2251.6007 - val_mse: 32161060.0000 - val_mae: 2251.6006\n",
            "Epoch 220/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1984.6355 - mse: 25966996.0000 - mae: 1984.6354 - val_loss: 2225.1522 - val_mse: 31917782.0000 - val_mae: 2225.1521\n",
            "Epoch 221/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 2000.0434 - mse: 25964796.0000 - mae: 2000.0433 - val_loss: 2215.2599 - val_mse: 31800694.0000 - val_mae: 2215.2598\n",
            "Epoch 222/500\n",
            "750/750 [==============================] - 0s 75us/step - loss: 1973.9426 - mse: 25642068.0000 - mae: 1973.9426 - val_loss: 2221.1658 - val_mse: 31871198.0000 - val_mae: 2221.1658\n",
            "Epoch 223/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1981.0113 - mse: 25805318.0000 - mae: 1981.0114 - val_loss: 2240.2635 - val_mse: 32043698.0000 - val_mae: 2240.2634\n",
            "Epoch 224/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1981.8087 - mse: 25807198.0000 - mae: 1981.8087 - val_loss: 2233.7785 - val_mse: 31982502.0000 - val_mae: 2233.7786\n",
            "Epoch 225/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1991.7833 - mse: 26055456.0000 - mae: 1991.7833 - val_loss: 2225.5348 - val_mse: 31913358.0000 - val_mae: 2225.5349\n",
            "Epoch 226/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 1989.6572 - mse: 25859638.0000 - mae: 1989.6573 - val_loss: 2244.9711 - val_mse: 32085748.0000 - val_mae: 2244.9709\n",
            "Epoch 227/500\n",
            "750/750 [==============================] - 0s 97us/step - loss: 1966.8735 - mse: 25861604.0000 - mae: 1966.8735 - val_loss: 2225.0000 - val_mse: 31893550.0000 - val_mae: 2225.0000\n",
            "Epoch 228/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1984.3671 - mse: 25909650.0000 - mae: 1984.3669 - val_loss: 2228.5448 - val_mse: 31967552.0000 - val_mae: 2228.5449\n",
            "Epoch 229/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1977.6503 - mse: 25999928.0000 - mae: 1977.6504 - val_loss: 2239.8058 - val_mse: 32019876.0000 - val_mae: 2239.8059\n",
            "Epoch 230/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1957.2962 - mse: 25810848.0000 - mae: 1957.2961 - val_loss: 2230.3639 - val_mse: 31936900.0000 - val_mae: 2230.3640\n",
            "Epoch 231/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1970.1265 - mse: 25550750.0000 - mae: 1970.1265 - val_loss: 2240.2589 - val_mse: 32023356.0000 - val_mae: 2240.2590\n",
            "Epoch 232/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1968.0766 - mse: 25713498.0000 - mae: 1968.0765 - val_loss: 2236.4096 - val_mse: 31996746.0000 - val_mae: 2236.4094\n",
            "Epoch 233/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1976.9194 - mse: 25913246.0000 - mae: 1976.9193 - val_loss: 2246.7692 - val_mse: 32099232.0000 - val_mae: 2246.7693\n",
            "Epoch 234/500\n",
            "750/750 [==============================] - 0s 75us/step - loss: 1978.8416 - mse: 25787002.0000 - mae: 1978.8416 - val_loss: 2236.8317 - val_mse: 31993356.0000 - val_mae: 2236.8318\n",
            "Epoch 235/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1970.2148 - mse: 25669268.0000 - mae: 1970.2147 - val_loss: 2251.9596 - val_mse: 32109510.0000 - val_mae: 2251.9597\n",
            "Epoch 236/500\n",
            "750/750 [==============================] - 0s 75us/step - loss: 1983.7795 - mse: 25893424.0000 - mae: 1983.7795 - val_loss: 2240.6092 - val_mse: 31984964.0000 - val_mae: 2240.6091\n",
            "Epoch 237/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1975.5674 - mse: 26041838.0000 - mae: 1975.5675 - val_loss: 2242.9169 - val_mse: 32006278.0000 - val_mae: 2242.9167\n",
            "Epoch 238/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1976.1778 - mse: 25616850.0000 - mae: 1976.1779 - val_loss: 2254.2724 - val_mse: 32092512.0000 - val_mae: 2254.2722\n",
            "Epoch 239/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1976.2598 - mse: 25871262.0000 - mae: 1976.2599 - val_loss: 2247.9393 - val_mse: 32052480.0000 - val_mae: 2247.9395\n",
            "Epoch 240/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1958.5549 - mse: 25792294.0000 - mae: 1958.5551 - val_loss: 2224.8750 - val_mse: 31860230.0000 - val_mae: 2224.8750\n",
            "Epoch 241/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 1967.4552 - mse: 25956958.0000 - mae: 1967.4553 - val_loss: 2254.3788 - val_mse: 32091276.0000 - val_mae: 2254.3789\n",
            "Epoch 242/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1973.4528 - mse: 25824390.0000 - mae: 1973.4529 - val_loss: 2245.5405 - val_mse: 32013566.0000 - val_mae: 2245.5405\n",
            "Epoch 243/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1972.3600 - mse: 25554234.0000 - mae: 1972.3600 - val_loss: 2252.9247 - val_mse: 32037978.0000 - val_mae: 2252.9246\n",
            "Epoch 244/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1970.1912 - mse: 25826646.0000 - mae: 1970.1912 - val_loss: 2240.1152 - val_mse: 31948382.0000 - val_mae: 2240.1152\n",
            "Epoch 245/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1974.3116 - mse: 26070440.0000 - mae: 1974.3116 - val_loss: 2240.1500 - val_mse: 31950192.0000 - val_mae: 2240.1499\n",
            "Epoch 246/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1964.8719 - mse: 25788700.0000 - mae: 1964.8719 - val_loss: 2235.1636 - val_mse: 31918910.0000 - val_mae: 2235.1636\n",
            "Epoch 247/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1958.0500 - mse: 25661670.0000 - mae: 1958.0500 - val_loss: 2239.6051 - val_mse: 31925272.0000 - val_mae: 2239.6050\n",
            "Epoch 248/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1950.9663 - mse: 25427028.0000 - mae: 1950.9663 - val_loss: 2236.5405 - val_mse: 31931888.0000 - val_mae: 2236.5405\n",
            "Epoch 249/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1969.7505 - mse: 25783754.0000 - mae: 1969.7505 - val_loss: 2257.3245 - val_mse: 32068262.0000 - val_mae: 2257.3245\n",
            "Epoch 250/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1971.7190 - mse: 25985366.0000 - mae: 1971.7190 - val_loss: 2239.8370 - val_mse: 31922930.0000 - val_mae: 2239.8372\n",
            "Epoch 251/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1979.8377 - mse: 25589202.0000 - mae: 1979.8376 - val_loss: 2257.1986 - val_mse: 32042304.0000 - val_mae: 2257.1985\n",
            "Epoch 252/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1948.0129 - mse: 25875672.0000 - mae: 1948.0128 - val_loss: 2232.6172 - val_mse: 31839112.0000 - val_mae: 2232.6172\n",
            "Epoch 253/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1971.4997 - mse: 25616034.0000 - mae: 1971.4996 - val_loss: 2242.8456 - val_mse: 31898162.0000 - val_mae: 2242.8457\n",
            "Epoch 254/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1983.1375 - mse: 25955994.0000 - mae: 1983.1375 - val_loss: 2226.6253 - val_mse: 31750638.0000 - val_mae: 2226.6252\n",
            "Epoch 255/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 1980.3352 - mse: 25586352.0000 - mae: 1980.3353 - val_loss: 2240.4148 - val_mse: 31877028.0000 - val_mae: 2240.4148\n",
            "Epoch 256/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1970.3396 - mse: 25358162.0000 - mae: 1970.3395 - val_loss: 2254.9585 - val_mse: 31947920.0000 - val_mae: 2254.9587\n",
            "Epoch 257/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 1962.5698 - mse: 25426684.0000 - mae: 1962.5698 - val_loss: 2252.5362 - val_mse: 31972770.0000 - val_mae: 2252.5364\n",
            "Epoch 258/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1980.2342 - mse: 26054478.0000 - mae: 1980.2341 - val_loss: 2266.5987 - val_mse: 32076620.0000 - val_mae: 2266.5986\n",
            "Epoch 259/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1959.5102 - mse: 25497136.0000 - mae: 1959.5101 - val_loss: 2244.0064 - val_mse: 31874708.0000 - val_mae: 2244.0066\n",
            "Epoch 260/500\n",
            "750/750 [==============================] - 0s 94us/step - loss: 1954.7781 - mse: 25378284.0000 - mae: 1954.7780 - val_loss: 2243.9831 - val_mse: 31891268.0000 - val_mae: 2243.9829\n",
            "Epoch 261/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1961.8933 - mse: 25676150.0000 - mae: 1961.8933 - val_loss: 2240.2031 - val_mse: 31875982.0000 - val_mae: 2240.2031\n",
            "Epoch 262/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1964.0700 - mse: 25561078.0000 - mae: 1964.0699 - val_loss: 2252.9571 - val_mse: 31953160.0000 - val_mae: 2252.9573\n",
            "Epoch 263/500\n",
            "750/750 [==============================] - 0s 96us/step - loss: 1981.7625 - mse: 25729836.0000 - mae: 1981.7625 - val_loss: 2258.9395 - val_mse: 32014988.0000 - val_mae: 2258.9395\n",
            "Epoch 264/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1972.3267 - mse: 25716646.0000 - mae: 1972.3267 - val_loss: 2242.3871 - val_mse: 31875708.0000 - val_mae: 2242.3872\n",
            "Epoch 265/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1965.0080 - mse: 25702126.0000 - mae: 1965.0081 - val_loss: 2243.1897 - val_mse: 31900554.0000 - val_mae: 2243.1897\n",
            "Epoch 266/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 1951.1691 - mse: 25644772.0000 - mae: 1951.1689 - val_loss: 2260.6008 - val_mse: 32041034.0000 - val_mae: 2260.6011\n",
            "Epoch 267/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1963.9510 - mse: 25743814.0000 - mae: 1963.9510 - val_loss: 2269.6811 - val_mse: 32082864.0000 - val_mae: 2269.6812\n",
            "Epoch 268/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1959.1704 - mse: 25415036.0000 - mae: 1959.1705 - val_loss: 2238.9051 - val_mse: 31812012.0000 - val_mae: 2238.9050\n",
            "Epoch 269/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1955.6158 - mse: 25529388.0000 - mae: 1955.6158 - val_loss: 2242.0948 - val_mse: 31833446.0000 - val_mae: 2242.0947\n",
            "Epoch 270/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1965.8505 - mse: 25591250.0000 - mae: 1965.8505 - val_loss: 2239.6317 - val_mse: 31801734.0000 - val_mae: 2239.6316\n",
            "Epoch 271/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1961.4334 - mse: 25664968.0000 - mae: 1961.4333 - val_loss: 2252.2985 - val_mse: 31925988.0000 - val_mae: 2252.2986\n",
            "Epoch 272/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1969.9738 - mse: 25760426.0000 - mae: 1969.9739 - val_loss: 2263.4991 - val_mse: 32001188.0000 - val_mae: 2263.4993\n",
            "Epoch 273/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1966.9755 - mse: 25668554.0000 - mae: 1966.9755 - val_loss: 2257.8298 - val_mse: 31971590.0000 - val_mae: 2257.8296\n",
            "Epoch 274/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1979.0249 - mse: 25772660.0000 - mae: 1979.0248 - val_loss: 2257.8867 - val_mse: 31958736.0000 - val_mae: 2257.8867\n",
            "Epoch 275/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1968.3112 - mse: 25623148.0000 - mae: 1968.3113 - val_loss: 2259.9117 - val_mse: 31957074.0000 - val_mae: 2259.9116\n",
            "Epoch 276/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1963.8234 - mse: 25359630.0000 - mae: 1963.8235 - val_loss: 2250.3395 - val_mse: 31885232.0000 - val_mae: 2250.3394\n",
            "Epoch 277/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 1956.2282 - mse: 25774624.0000 - mae: 1956.2281 - val_loss: 2253.1801 - val_mse: 31891328.0000 - val_mae: 2253.1799\n",
            "Epoch 278/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1955.2635 - mse: 25577376.0000 - mae: 1955.2637 - val_loss: 2248.1887 - val_mse: 31840684.0000 - val_mae: 2248.1887\n",
            "Epoch 279/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 1973.6123 - mse: 25388630.0000 - mae: 1973.6123 - val_loss: 2245.3193 - val_mse: 31833430.0000 - val_mae: 2245.3193\n",
            "Epoch 280/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1957.7952 - mse: 25597398.0000 - mae: 1957.7953 - val_loss: 2255.9066 - val_mse: 31909644.0000 - val_mae: 2255.9065\n",
            "Epoch 281/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1952.3648 - mse: 25457852.0000 - mae: 1952.3649 - val_loss: 2259.3067 - val_mse: 31908174.0000 - val_mae: 2259.3064\n",
            "Epoch 282/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1963.4179 - mse: 25274952.0000 - mae: 1963.4180 - val_loss: 2272.3657 - val_mse: 32014644.0000 - val_mae: 2272.3657\n",
            "Epoch 283/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1965.2574 - mse: 25517536.0000 - mae: 1965.2573 - val_loss: 2262.1652 - val_mse: 31939126.0000 - val_mae: 2262.1653\n",
            "Epoch 284/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1964.1781 - mse: 25294454.0000 - mae: 1964.1782 - val_loss: 2257.2180 - val_mse: 31930374.0000 - val_mae: 2257.2180\n",
            "Epoch 285/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1954.0053 - mse: 25407846.0000 - mae: 1954.0054 - val_loss: 2244.2597 - val_mse: 31823142.0000 - val_mae: 2244.2598\n",
            "Epoch 286/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1954.0715 - mse: 25407226.0000 - mae: 1954.0715 - val_loss: 2247.3666 - val_mse: 31831064.0000 - val_mae: 2247.3667\n",
            "Epoch 287/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1969.6177 - mse: 25664686.0000 - mae: 1969.6177 - val_loss: 2240.2552 - val_mse: 31804668.0000 - val_mae: 2240.2549\n",
            "Epoch 288/500\n",
            "750/750 [==============================] - 0s 92us/step - loss: 1955.1149 - mse: 25466858.0000 - mae: 1955.1150 - val_loss: 2261.8301 - val_mse: 31962346.0000 - val_mae: 2261.8301\n",
            "Epoch 289/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1954.0589 - mse: 25837814.0000 - mae: 1954.0590 - val_loss: 2258.1024 - val_mse: 31925360.0000 - val_mae: 2258.1023\n",
            "Epoch 290/500\n",
            "750/750 [==============================] - 0s 102us/step - loss: 1950.2459 - mse: 25398318.0000 - mae: 1950.2458 - val_loss: 2256.5374 - val_mse: 31891852.0000 - val_mae: 2256.5374\n",
            "Epoch 291/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1943.5480 - mse: 25409684.0000 - mae: 1943.5480 - val_loss: 2255.8418 - val_mse: 31884122.0000 - val_mae: 2255.8418\n",
            "Epoch 292/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1949.9147 - mse: 25447142.0000 - mae: 1949.9148 - val_loss: 2268.4636 - val_mse: 31999738.0000 - val_mae: 2268.4634\n",
            "Epoch 293/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1975.8193 - mse: 25700390.0000 - mae: 1975.8193 - val_loss: 2253.2564 - val_mse: 31894168.0000 - val_mae: 2253.2566\n",
            "Epoch 294/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1948.7020 - mse: 25133634.0000 - mae: 1948.7020 - val_loss: 2268.9900 - val_mse: 31993662.0000 - val_mae: 2268.9902\n",
            "Epoch 295/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1978.9786 - mse: 25637198.0000 - mae: 1978.9786 - val_loss: 2269.4695 - val_mse: 32000626.0000 - val_mae: 2269.4692\n",
            "Epoch 296/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1957.8261 - mse: 25336480.0000 - mae: 1957.8262 - val_loss: 2255.9049 - val_mse: 31900462.0000 - val_mae: 2255.9048\n",
            "Epoch 297/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1961.6237 - mse: 25728418.0000 - mae: 1961.6237 - val_loss: 2249.5116 - val_mse: 31828590.0000 - val_mae: 2249.5117\n",
            "Epoch 298/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1946.8801 - mse: 25289280.0000 - mae: 1946.8801 - val_loss: 2258.4389 - val_mse: 31901758.0000 - val_mae: 2258.4390\n",
            "Epoch 299/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1958.5459 - mse: 25419490.0000 - mae: 1958.5460 - val_loss: 2264.1249 - val_mse: 31951768.0000 - val_mae: 2264.1250\n",
            "Epoch 300/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1948.5160 - mse: 25399874.0000 - mae: 1948.5160 - val_loss: 2253.5579 - val_mse: 31835694.0000 - val_mae: 2253.5581\n",
            "Epoch 301/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1945.8535 - mse: 25174966.0000 - mae: 1945.8535 - val_loss: 2243.4777 - val_mse: 31754152.0000 - val_mae: 2243.4778\n",
            "Epoch 302/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 1946.8804 - mse: 25074778.0000 - mae: 1946.8804 - val_loss: 2244.0493 - val_mse: 31787126.0000 - val_mae: 2244.0493\n",
            "Epoch 303/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1945.9172 - mse: 24896992.0000 - mae: 1945.9174 - val_loss: 2262.6626 - val_mse: 31928886.0000 - val_mae: 2262.6628\n",
            "Epoch 304/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1959.4401 - mse: 25311478.0000 - mae: 1959.4399 - val_loss: 2261.8363 - val_mse: 31915694.0000 - val_mae: 2261.8362\n",
            "Epoch 305/500\n",
            "750/750 [==============================] - 0s 103us/step - loss: 1932.3741 - mse: 25211514.0000 - mae: 1932.3740 - val_loss: 2259.2248 - val_mse: 31893760.0000 - val_mae: 2259.2249\n",
            "Epoch 306/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1957.3321 - mse: 25417726.0000 - mae: 1957.3320 - val_loss: 2262.5271 - val_mse: 31897722.0000 - val_mae: 2262.5271\n",
            "Epoch 307/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1967.1285 - mse: 25470050.0000 - mae: 1967.1285 - val_loss: 2267.3303 - val_mse: 31922908.0000 - val_mae: 2267.3303\n",
            "Epoch 308/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1950.4369 - mse: 25465436.0000 - mae: 1950.4370 - val_loss: 2262.6749 - val_mse: 31897360.0000 - val_mae: 2262.6748\n",
            "Epoch 309/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 1950.8437 - mse: 24982376.0000 - mae: 1950.8436 - val_loss: 2254.4562 - val_mse: 31836578.0000 - val_mae: 2254.4561\n",
            "Epoch 310/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1946.5409 - mse: 25175192.0000 - mae: 1946.5409 - val_loss: 2253.5183 - val_mse: 31793180.0000 - val_mae: 2253.5181\n",
            "Epoch 311/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1953.9248 - mse: 25487572.0000 - mae: 1953.9248 - val_loss: 2270.3009 - val_mse: 31934590.0000 - val_mae: 2270.3010\n",
            "Epoch 312/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1950.3038 - mse: 25366946.0000 - mae: 1950.3038 - val_loss: 2273.4149 - val_mse: 31973682.0000 - val_mae: 2273.4150\n",
            "Epoch 313/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 1952.9133 - mse: 25138548.0000 - mae: 1952.9132 - val_loss: 2277.7227 - val_mse: 31999128.0000 - val_mae: 2277.7227\n",
            "Epoch 314/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1944.8768 - mse: 25393114.0000 - mae: 1944.8768 - val_loss: 2270.3305 - val_mse: 31944706.0000 - val_mae: 2270.3306\n",
            "Epoch 315/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1917.8676 - mse: 25003474.0000 - mae: 1917.8676 - val_loss: 2274.7555 - val_mse: 31951140.0000 - val_mae: 2274.7556\n",
            "Epoch 316/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1956.7621 - mse: 25589960.0000 - mae: 1956.7620 - val_loss: 2264.5483 - val_mse: 31868246.0000 - val_mae: 2264.5481\n",
            "Epoch 317/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1952.0203 - mse: 25471446.0000 - mae: 1952.0204 - val_loss: 2267.4831 - val_mse: 31885124.0000 - val_mae: 2267.4832\n",
            "Epoch 318/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1953.1124 - mse: 25458232.0000 - mae: 1953.1123 - val_loss: 2277.7404 - val_mse: 31996862.0000 - val_mae: 2277.7405\n",
            "Epoch 319/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1952.6868 - mse: 25440272.0000 - mae: 1952.6869 - val_loss: 2266.9496 - val_mse: 31920836.0000 - val_mae: 2266.9497\n",
            "Epoch 320/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1954.6681 - mse: 25300850.0000 - mae: 1954.6680 - val_loss: 2258.2186 - val_mse: 31852334.0000 - val_mae: 2258.2188\n",
            "Epoch 321/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1929.7019 - mse: 25025878.0000 - mae: 1929.7018 - val_loss: 2265.4111 - val_mse: 31886052.0000 - val_mae: 2265.4109\n",
            "Epoch 322/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1951.8577 - mse: 25497900.0000 - mae: 1951.8577 - val_loss: 2263.6934 - val_mse: 31854034.0000 - val_mae: 2263.6934\n",
            "Epoch 323/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1933.4984 - mse: 25160654.0000 - mae: 1933.4983 - val_loss: 2274.1900 - val_mse: 31921070.0000 - val_mae: 2274.1899\n",
            "Epoch 324/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1946.6860 - mse: 25400796.0000 - mae: 1946.6860 - val_loss: 2276.6713 - val_mse: 31925158.0000 - val_mae: 2276.6711\n",
            "Epoch 325/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 1939.8405 - mse: 25097620.0000 - mae: 1939.8405 - val_loss: 2272.1658 - val_mse: 31878906.0000 - val_mae: 2272.1658\n",
            "Epoch 326/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1941.9145 - mse: 25834314.0000 - mae: 1941.9146 - val_loss: 2272.3336 - val_mse: 31861394.0000 - val_mae: 2272.3335\n",
            "Epoch 327/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1945.1361 - mse: 25579174.0000 - mae: 1945.1360 - val_loss: 2277.0254 - val_mse: 31921002.0000 - val_mae: 2277.0254\n",
            "Epoch 328/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1929.6440 - mse: 25276276.0000 - mae: 1929.6440 - val_loss: 2276.1084 - val_mse: 31907408.0000 - val_mae: 2276.1084\n",
            "Epoch 329/500\n",
            "750/750 [==============================] - 0s 98us/step - loss: 1952.0574 - mse: 25505016.0000 - mae: 1952.0574 - val_loss: 2270.1163 - val_mse: 31860646.0000 - val_mae: 2270.1162\n",
            "Epoch 330/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1937.0088 - mse: 25185922.0000 - mae: 1937.0088 - val_loss: 2278.9244 - val_mse: 31958982.0000 - val_mae: 2278.9246\n",
            "Epoch 331/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1942.7504 - mse: 25436090.0000 - mae: 1942.7504 - val_loss: 2273.7073 - val_mse: 31922812.0000 - val_mae: 2273.7073\n",
            "Epoch 332/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1930.0531 - mse: 25429894.0000 - mae: 1930.0530 - val_loss: 2256.3890 - val_mse: 31763984.0000 - val_mae: 2256.3892\n",
            "Epoch 333/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 1945.2469 - mse: 25388030.0000 - mae: 1945.2469 - val_loss: 2270.0157 - val_mse: 31852412.0000 - val_mae: 2270.0156\n",
            "Epoch 334/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1928.7520 - mse: 25274800.0000 - mae: 1928.7520 - val_loss: 2259.7234 - val_mse: 31756054.0000 - val_mae: 2259.7234\n",
            "Epoch 335/500\n",
            "750/750 [==============================] - 0s 97us/step - loss: 1948.6101 - mse: 25071968.0000 - mae: 1948.6100 - val_loss: 2275.5318 - val_mse: 31881468.0000 - val_mae: 2275.5317\n",
            "Epoch 336/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1934.9514 - mse: 25174608.0000 - mae: 1934.9513 - val_loss: 2284.9842 - val_mse: 31922648.0000 - val_mae: 2284.9841\n",
            "Epoch 337/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1954.0027 - mse: 25361526.0000 - mae: 1954.0027 - val_loss: 2269.2811 - val_mse: 31790192.0000 - val_mae: 2269.2812\n",
            "Epoch 338/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1930.9621 - mse: 25100968.0000 - mae: 1930.9622 - val_loss: 2267.7692 - val_mse: 31782066.0000 - val_mae: 2267.7693\n",
            "Epoch 339/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1946.1875 - mse: 25315268.0000 - mae: 1946.1875 - val_loss: 2270.4358 - val_mse: 31810588.0000 - val_mae: 2270.4360\n",
            "Epoch 340/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1950.7847 - mse: 25249222.0000 - mae: 1950.7847 - val_loss: 2281.4595 - val_mse: 31909366.0000 - val_mae: 2281.4595\n",
            "Epoch 341/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1934.3815 - mse: 25090916.0000 - mae: 1934.3815 - val_loss: 2284.3628 - val_mse: 31907078.0000 - val_mae: 2284.3625\n",
            "Epoch 342/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 1928.5130 - mse: 25154192.0000 - mae: 1928.5129 - val_loss: 2272.6641 - val_mse: 31814664.0000 - val_mae: 2272.6641\n",
            "Epoch 343/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1938.3691 - mse: 25386844.0000 - mae: 1938.3690 - val_loss: 2263.2772 - val_mse: 31753652.0000 - val_mae: 2263.2773\n",
            "Epoch 344/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1937.5085 - mse: 25306912.0000 - mae: 1937.5083 - val_loss: 2279.0090 - val_mse: 31903930.0000 - val_mae: 2279.0090\n",
            "Epoch 345/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1944.7520 - mse: 25452504.0000 - mae: 1944.7520 - val_loss: 2275.5695 - val_mse: 31873434.0000 - val_mae: 2275.5696\n",
            "Epoch 346/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1936.1217 - mse: 25531312.0000 - mae: 1936.1217 - val_loss: 2280.7223 - val_mse: 31869280.0000 - val_mae: 2280.7224\n",
            "Epoch 347/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1939.4211 - mse: 25212792.0000 - mae: 1939.4210 - val_loss: 2274.5257 - val_mse: 31787880.0000 - val_mae: 2274.5256\n",
            "Epoch 348/500\n",
            "750/750 [==============================] - 0s 100us/step - loss: 1920.5286 - mse: 25033846.0000 - mae: 1920.5287 - val_loss: 2268.0226 - val_mse: 31732626.0000 - val_mae: 2268.0227\n",
            "Epoch 349/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1926.9556 - mse: 25295356.0000 - mae: 1926.9554 - val_loss: 2272.6682 - val_mse: 31743688.0000 - val_mae: 2272.6682\n",
            "Epoch 350/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1939.4320 - mse: 25475908.0000 - mae: 1939.4320 - val_loss: 2281.3411 - val_mse: 31801732.0000 - val_mae: 2281.3411\n",
            "Epoch 351/500\n",
            "750/750 [==============================] - 0s 113us/step - loss: 1930.1402 - mse: 25273036.0000 - mae: 1930.1401 - val_loss: 2271.1997 - val_mse: 31735846.0000 - val_mae: 2271.1997\n",
            "Epoch 352/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1933.3706 - mse: 25168934.0000 - mae: 1933.3707 - val_loss: 2279.7712 - val_mse: 31800544.0000 - val_mae: 2279.7712\n",
            "Epoch 353/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1953.2950 - mse: 25480598.0000 - mae: 1953.2950 - val_loss: 2289.8250 - val_mse: 31844068.0000 - val_mae: 2289.8250\n",
            "Epoch 354/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1923.0634 - mse: 25088466.0000 - mae: 1923.0634 - val_loss: 2278.5990 - val_mse: 31765038.0000 - val_mae: 2278.5991\n",
            "Epoch 355/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1935.8704 - mse: 25547008.0000 - mae: 1935.8704 - val_loss: 2286.1529 - val_mse: 31826048.0000 - val_mae: 2286.1531\n",
            "Epoch 356/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1938.5704 - mse: 25565544.0000 - mae: 1938.5703 - val_loss: 2274.8008 - val_mse: 31753924.0000 - val_mae: 2274.8008\n",
            "Epoch 357/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1939.6908 - mse: 25242714.0000 - mae: 1939.6907 - val_loss: 2271.7673 - val_mse: 31744984.0000 - val_mae: 2271.7673\n",
            "Epoch 358/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1931.8075 - mse: 24905790.0000 - mae: 1931.8075 - val_loss: 2277.8235 - val_mse: 31767004.0000 - val_mae: 2277.8235\n",
            "Epoch 359/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1925.6955 - mse: 25136934.0000 - mae: 1925.6956 - val_loss: 2284.1133 - val_mse: 31800442.0000 - val_mae: 2284.1133\n",
            "Epoch 360/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1944.5231 - mse: 25086354.0000 - mae: 1944.5232 - val_loss: 2292.3458 - val_mse: 31878228.0000 - val_mae: 2292.3457\n",
            "Epoch 361/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 1932.3256 - mse: 25368600.0000 - mae: 1932.3257 - val_loss: 2276.2920 - val_mse: 31738820.0000 - val_mae: 2276.2920\n",
            "Epoch 362/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1928.3008 - mse: 24992352.0000 - mae: 1928.3008 - val_loss: 2273.1997 - val_mse: 31690138.0000 - val_mae: 2273.2000\n",
            "Epoch 363/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1931.3788 - mse: 25254858.0000 - mae: 1931.3788 - val_loss: 2287.4701 - val_mse: 31813930.0000 - val_mae: 2287.4702\n",
            "Epoch 364/500\n",
            "750/750 [==============================] - 0s 96us/step - loss: 1917.7983 - mse: 25159330.0000 - mae: 1917.7983 - val_loss: 2295.8222 - val_mse: 31868390.0000 - val_mae: 2295.8223\n",
            "Epoch 365/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1938.5331 - mse: 25130430.0000 - mae: 1938.5330 - val_loss: 2271.0133 - val_mse: 31681762.0000 - val_mae: 2271.0134\n",
            "Epoch 366/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1929.3119 - mse: 24973170.0000 - mae: 1929.3119 - val_loss: 2284.5083 - val_mse: 31768758.0000 - val_mae: 2284.5083\n",
            "Epoch 367/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1928.1406 - mse: 24942068.0000 - mae: 1928.1406 - val_loss: 2282.2009 - val_mse: 31763664.0000 - val_mae: 2282.2009\n",
            "Epoch 368/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1929.9488 - mse: 25178286.0000 - mae: 1929.9489 - val_loss: 2290.8112 - val_mse: 31828860.0000 - val_mae: 2290.8113\n",
            "Epoch 369/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1932.9365 - mse: 25189082.0000 - mae: 1932.9365 - val_loss: 2290.0112 - val_mse: 31804424.0000 - val_mae: 2290.0110\n",
            "Epoch 370/500\n",
            "750/750 [==============================] - 0s 92us/step - loss: 1934.0730 - mse: 24801278.0000 - mae: 1934.0730 - val_loss: 2283.3313 - val_mse: 31731644.0000 - val_mae: 2283.3313\n",
            "Epoch 371/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1919.2657 - mse: 24844094.0000 - mae: 1919.2656 - val_loss: 2291.5574 - val_mse: 31816572.0000 - val_mae: 2291.5574\n",
            "Epoch 372/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1932.1166 - mse: 25151390.0000 - mae: 1932.1167 - val_loss: 2283.8848 - val_mse: 31757546.0000 - val_mae: 2283.8848\n",
            "Epoch 373/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1937.1799 - mse: 25084548.0000 - mae: 1937.1798 - val_loss: 2282.6462 - val_mse: 31734590.0000 - val_mae: 2282.6462\n",
            "Epoch 374/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1940.8364 - mse: 24982796.0000 - mae: 1940.8365 - val_loss: 2291.3084 - val_mse: 31789618.0000 - val_mae: 2291.3083\n",
            "Epoch 375/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1925.2199 - mse: 25083356.0000 - mae: 1925.2198 - val_loss: 2291.9710 - val_mse: 31777122.0000 - val_mae: 2291.9709\n",
            "Epoch 376/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1909.8170 - mse: 24870518.0000 - mae: 1909.8170 - val_loss: 2278.3144 - val_mse: 31618154.0000 - val_mae: 2278.3145\n",
            "Epoch 377/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1923.0919 - mse: 24720884.0000 - mae: 1923.0920 - val_loss: 2293.4472 - val_mse: 31772084.0000 - val_mae: 2293.4473\n",
            "Epoch 378/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 1924.3720 - mse: 24877630.0000 - mae: 1924.3719 - val_loss: 2296.6709 - val_mse: 31805876.0000 - val_mae: 2296.6709\n",
            "Epoch 379/500\n",
            "750/750 [==============================] - 0s 94us/step - loss: 1945.5501 - mse: 25374862.0000 - mae: 1945.5502 - val_loss: 2289.9588 - val_mse: 31763912.0000 - val_mae: 2289.9587\n",
            "Epoch 380/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1917.7350 - mse: 25111142.0000 - mae: 1917.7350 - val_loss: 2291.4423 - val_mse: 31771126.0000 - val_mae: 2291.4424\n",
            "Epoch 381/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1922.8499 - mse: 25039268.0000 - mae: 1922.8500 - val_loss: 2285.1742 - val_mse: 31726456.0000 - val_mae: 2285.1741\n",
            "Epoch 382/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1926.8929 - mse: 25310724.0000 - mae: 1926.8929 - val_loss: 2279.1069 - val_mse: 31650120.0000 - val_mae: 2279.1069\n",
            "Epoch 383/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1910.6421 - mse: 24959068.0000 - mae: 1910.6422 - val_loss: 2286.6676 - val_mse: 31694842.0000 - val_mae: 2286.6675\n",
            "Epoch 384/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1932.6065 - mse: 25152220.0000 - mae: 1932.6064 - val_loss: 2281.7955 - val_mse: 31661114.0000 - val_mae: 2281.7954\n",
            "Epoch 385/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 1911.0350 - mse: 24788194.0000 - mae: 1911.0350 - val_loss: 2285.3173 - val_mse: 31693886.0000 - val_mae: 2285.3174\n",
            "Epoch 386/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1931.0118 - mse: 25189510.0000 - mae: 1931.0118 - val_loss: 2290.2226 - val_mse: 31743828.0000 - val_mae: 2290.2224\n",
            "Epoch 387/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1936.8189 - mse: 25403788.0000 - mae: 1936.8190 - val_loss: 2287.6987 - val_mse: 31735078.0000 - val_mae: 2287.6987\n",
            "Epoch 388/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1919.3826 - mse: 24744798.0000 - mae: 1919.3824 - val_loss: 2295.7094 - val_mse: 31770736.0000 - val_mae: 2295.7092\n",
            "Epoch 389/500\n",
            "750/750 [==============================] - 0s 94us/step - loss: 1919.8662 - mse: 25101910.0000 - mae: 1919.8660 - val_loss: 2296.4660 - val_mse: 31798658.0000 - val_mae: 2296.4661\n",
            "Epoch 390/500\n",
            "750/750 [==============================] - 0s 100us/step - loss: 1917.3102 - mse: 25141112.0000 - mae: 1917.3102 - val_loss: 2294.4012 - val_mse: 31789660.0000 - val_mae: 2294.4014\n",
            "Epoch 391/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1914.6709 - mse: 24800860.0000 - mae: 1914.6708 - val_loss: 2293.7849 - val_mse: 31746226.0000 - val_mae: 2293.7849\n",
            "Epoch 392/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1930.1319 - mse: 25042752.0000 - mae: 1930.1320 - val_loss: 2293.3967 - val_mse: 31716006.0000 - val_mae: 2293.3967\n",
            "Epoch 393/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1929.3750 - mse: 25054610.0000 - mae: 1929.3750 - val_loss: 2298.1805 - val_mse: 31769676.0000 - val_mae: 2298.1804\n",
            "Epoch 394/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1934.6926 - mse: 25188882.0000 - mae: 1934.6926 - val_loss: 2286.6483 - val_mse: 31688150.0000 - val_mae: 2286.6482\n",
            "Epoch 395/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1924.8121 - mse: 25086302.0000 - mae: 1924.8120 - val_loss: 2293.6536 - val_mse: 31729478.0000 - val_mae: 2293.6536\n",
            "Epoch 396/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1928.8279 - mse: 25325502.0000 - mae: 1928.8280 - val_loss: 2288.6000 - val_mse: 31711136.0000 - val_mae: 2288.6001\n",
            "Epoch 397/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1922.2529 - mse: 25114406.0000 - mae: 1922.2531 - val_loss: 2282.4854 - val_mse: 31623594.0000 - val_mae: 2282.4854\n",
            "Epoch 398/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1928.1875 - mse: 24811212.0000 - mae: 1928.1875 - val_loss: 2296.4699 - val_mse: 31700826.0000 - val_mae: 2296.4697\n",
            "Epoch 399/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 1925.4347 - mse: 24916858.0000 - mae: 1925.4347 - val_loss: 2284.5469 - val_mse: 31624638.0000 - val_mae: 2284.5471\n",
            "Epoch 400/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1925.4782 - mse: 25082252.0000 - mae: 1925.4784 - val_loss: 2285.9216 - val_mse: 31639388.0000 - val_mae: 2285.9216\n",
            "Epoch 401/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1926.4146 - mse: 24949870.0000 - mae: 1926.4147 - val_loss: 2289.0713 - val_mse: 31662182.0000 - val_mae: 2289.0713\n",
            "Epoch 402/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1937.1106 - mse: 25373332.0000 - mae: 1937.1107 - val_loss: 2284.2689 - val_mse: 31619438.0000 - val_mae: 2284.2688\n",
            "Epoch 403/500\n",
            "750/750 [==============================] - 0s 96us/step - loss: 1942.2298 - mse: 25337222.0000 - mae: 1942.2299 - val_loss: 2292.0990 - val_mse: 31666564.0000 - val_mae: 2292.0991\n",
            "Epoch 404/500\n",
            "750/750 [==============================] - 0s 98us/step - loss: 1913.6874 - mse: 25004584.0000 - mae: 1913.6874 - val_loss: 2289.8846 - val_mse: 31666244.0000 - val_mae: 2289.8848\n",
            "Epoch 405/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 1933.7950 - mse: 24861230.0000 - mae: 1933.7950 - val_loss: 2294.8676 - val_mse: 31695992.0000 - val_mae: 2294.8677\n",
            "Epoch 406/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1923.3938 - mse: 24712356.0000 - mae: 1923.3938 - val_loss: 2301.6880 - val_mse: 31737978.0000 - val_mae: 2301.6882\n",
            "Epoch 407/500\n",
            "750/750 [==============================] - 0s 94us/step - loss: 1901.1416 - mse: 24904568.0000 - mae: 1901.1417 - val_loss: 2290.9227 - val_mse: 31638602.0000 - val_mae: 2290.9229\n",
            "Epoch 408/500\n",
            "750/750 [==============================] - 0s 114us/step - loss: 1911.9958 - mse: 24619130.0000 - mae: 1911.9958 - val_loss: 2297.5431 - val_mse: 31661910.0000 - val_mae: 2297.5432\n",
            "Epoch 409/500\n",
            "750/750 [==============================] - 0s 96us/step - loss: 1904.6994 - mse: 24772598.0000 - mae: 1904.6993 - val_loss: 2294.3489 - val_mse: 31625896.0000 - val_mae: 2294.3489\n",
            "Epoch 410/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 1923.1025 - mse: 25024928.0000 - mae: 1923.1023 - val_loss: 2291.6161 - val_mse: 31622890.0000 - val_mae: 2291.6162\n",
            "Epoch 411/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1939.9010 - mse: 25231158.0000 - mae: 1939.9010 - val_loss: 2292.3317 - val_mse: 31605700.0000 - val_mae: 2292.3318\n",
            "Epoch 412/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1907.4004 - mse: 24834838.0000 - mae: 1907.4004 - val_loss: 2285.7847 - val_mse: 31545418.0000 - val_mae: 2285.7847\n",
            "Epoch 413/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1930.5124 - mse: 25044484.0000 - mae: 1930.5123 - val_loss: 2291.0461 - val_mse: 31570024.0000 - val_mae: 2291.0461\n",
            "Epoch 414/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1915.6642 - mse: 24884016.0000 - mae: 1915.6643 - val_loss: 2290.5759 - val_mse: 31567740.0000 - val_mae: 2290.5759\n",
            "Epoch 415/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1927.1355 - mse: 25015398.0000 - mae: 1927.1355 - val_loss: 2296.4898 - val_mse: 31605710.0000 - val_mae: 2296.4897\n",
            "Epoch 416/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1917.2777 - mse: 24919954.0000 - mae: 1917.2777 - val_loss: 2312.3771 - val_mse: 31752340.0000 - val_mae: 2312.3772\n",
            "Epoch 417/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1930.6568 - mse: 25344256.0000 - mae: 1930.6569 - val_loss: 2297.5982 - val_mse: 31624082.0000 - val_mae: 2297.5981\n",
            "Epoch 418/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1902.8902 - mse: 24666300.0000 - mae: 1902.8901 - val_loss: 2288.8045 - val_mse: 31556042.0000 - val_mae: 2288.8047\n",
            "Epoch 419/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1914.9957 - mse: 24546702.0000 - mae: 1914.9958 - val_loss: 2299.3590 - val_mse: 31643440.0000 - val_mae: 2299.3589\n",
            "Epoch 420/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1922.9028 - mse: 25033600.0000 - mae: 1922.9028 - val_loss: 2288.8393 - val_mse: 31612270.0000 - val_mae: 2288.8396\n",
            "Epoch 421/500\n",
            "750/750 [==============================] - 0s 97us/step - loss: 1915.9122 - mse: 24720382.0000 - mae: 1915.9121 - val_loss: 2284.8611 - val_mse: 31590048.0000 - val_mae: 2284.8611\n",
            "Epoch 422/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1899.6913 - mse: 24586464.0000 - mae: 1899.6913 - val_loss: 2285.8222 - val_mse: 31570426.0000 - val_mae: 2285.8223\n",
            "Epoch 423/500\n",
            "750/750 [==============================] - 0s 97us/step - loss: 1927.8277 - mse: 24565908.0000 - mae: 1927.8276 - val_loss: 2298.2856 - val_mse: 31683044.0000 - val_mae: 2298.2856\n",
            "Epoch 424/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1916.3730 - mse: 24932740.0000 - mae: 1916.3730 - val_loss: 2294.2662 - val_mse: 31645914.0000 - val_mae: 2294.2661\n",
            "Epoch 425/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1915.1320 - mse: 24839280.0000 - mae: 1915.1320 - val_loss: 2293.1365 - val_mse: 31626546.0000 - val_mae: 2293.1365\n",
            "Epoch 426/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1902.7511 - mse: 24714910.0000 - mae: 1902.7512 - val_loss: 2296.3183 - val_mse: 31643022.0000 - val_mae: 2296.3184\n",
            "Epoch 427/500\n",
            "750/750 [==============================] - 0s 104us/step - loss: 1896.0438 - mse: 24634590.0000 - mae: 1896.0438 - val_loss: 2282.3929 - val_mse: 31544112.0000 - val_mae: 2282.3928\n",
            "Epoch 428/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1914.2350 - mse: 24806488.0000 - mae: 1914.2350 - val_loss: 2287.8136 - val_mse: 31567422.0000 - val_mae: 2287.8137\n",
            "Epoch 429/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1912.2392 - mse: 24995938.0000 - mae: 1912.2391 - val_loss: 2287.8590 - val_mse: 31596146.0000 - val_mae: 2287.8589\n",
            "Epoch 430/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1905.1390 - mse: 24528430.0000 - mae: 1905.1390 - val_loss: 2294.6440 - val_mse: 31637782.0000 - val_mae: 2294.6440\n",
            "Epoch 431/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1924.2970 - mse: 25080884.0000 - mae: 1924.2970 - val_loss: 2294.2044 - val_mse: 31633100.0000 - val_mae: 2294.2043\n",
            "Epoch 432/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1914.8794 - mse: 24761922.0000 - mae: 1914.8793 - val_loss: 2298.1547 - val_mse: 31601320.0000 - val_mae: 2298.1548\n",
            "Epoch 433/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1910.2045 - mse: 25049696.0000 - mae: 1910.2045 - val_loss: 2292.5339 - val_mse: 31586476.0000 - val_mae: 2292.5339\n",
            "Epoch 434/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1911.5900 - mse: 24723128.0000 - mae: 1911.5900 - val_loss: 2289.5777 - val_mse: 31577694.0000 - val_mae: 2289.5776\n",
            "Epoch 435/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 1916.4956 - mse: 24918374.0000 - mae: 1916.4957 - val_loss: 2298.0297 - val_mse: 31640870.0000 - val_mae: 2298.0298\n",
            "Epoch 436/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1898.2955 - mse: 24707724.0000 - mae: 1898.2953 - val_loss: 2300.7394 - val_mse: 31670212.0000 - val_mae: 2300.7395\n",
            "Epoch 437/500\n",
            "750/750 [==============================] - 0s 108us/step - loss: 1940.7740 - mse: 25408674.0000 - mae: 1940.7740 - val_loss: 2296.8170 - val_mse: 31637954.0000 - val_mae: 2296.8171\n",
            "Epoch 438/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1903.5837 - mse: 24908720.0000 - mae: 1903.5836 - val_loss: 2297.6396 - val_mse: 31623798.0000 - val_mae: 2297.6396\n",
            "Epoch 439/500\n",
            "750/750 [==============================] - 0s 98us/step - loss: 1922.6776 - mse: 24885226.0000 - mae: 1922.6776 - val_loss: 2300.0068 - val_mse: 31634280.0000 - val_mae: 2300.0068\n",
            "Epoch 440/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1910.7947 - mse: 24897628.0000 - mae: 1910.7947 - val_loss: 2289.2326 - val_mse: 31535480.0000 - val_mae: 2289.2327\n",
            "Epoch 441/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1926.5766 - mse: 24930910.0000 - mae: 1926.5765 - val_loss: 2293.0768 - val_mse: 31559802.0000 - val_mae: 2293.0769\n",
            "Epoch 442/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 1911.6959 - mse: 25129844.0000 - mae: 1911.6960 - val_loss: 2292.9551 - val_mse: 31577788.0000 - val_mae: 2292.9551\n",
            "Epoch 443/500\n",
            "750/750 [==============================] - 0s 95us/step - loss: 1917.5794 - mse: 24912284.0000 - mae: 1917.5793 - val_loss: 2302.3068 - val_mse: 31640100.0000 - val_mae: 2302.3069\n",
            "Epoch 444/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1922.5940 - mse: 24855798.0000 - mae: 1922.5940 - val_loss: 2290.3993 - val_mse: 31585886.0000 - val_mae: 2290.3994\n",
            "Epoch 445/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1910.2167 - mse: 24822508.0000 - mae: 1910.2167 - val_loss: 2295.3881 - val_mse: 31596930.0000 - val_mae: 2295.3882\n",
            "Epoch 446/500\n",
            "750/750 [==============================] - 0s 100us/step - loss: 1914.2565 - mse: 24625236.0000 - mae: 1914.2567 - val_loss: 2294.0455 - val_mse: 31588778.0000 - val_mae: 2294.0454\n",
            "Epoch 447/500\n",
            "750/750 [==============================] - 0s 92us/step - loss: 1921.3052 - mse: 25083140.0000 - mae: 1921.3052 - val_loss: 2294.8211 - val_mse: 31571302.0000 - val_mae: 2294.8210\n",
            "Epoch 448/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1912.3317 - mse: 25137242.0000 - mae: 1912.3317 - val_loss: 2291.4371 - val_mse: 31546680.0000 - val_mae: 2291.4373\n",
            "Epoch 449/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1914.1453 - mse: 24986480.0000 - mae: 1914.1454 - val_loss: 2300.5489 - val_mse: 31631908.0000 - val_mae: 2300.5491\n",
            "Epoch 450/500\n",
            "750/750 [==============================] - 0s 95us/step - loss: 1909.8600 - mse: 24778602.0000 - mae: 1909.8600 - val_loss: 2296.1969 - val_mse: 31587442.0000 - val_mae: 2296.1970\n",
            "Epoch 451/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1936.3190 - mse: 25136534.0000 - mae: 1936.3190 - val_loss: 2300.0898 - val_mse: 31597286.0000 - val_mae: 2300.0898\n",
            "Epoch 452/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1904.9620 - mse: 24909272.0000 - mae: 1904.9620 - val_loss: 2306.4356 - val_mse: 31624648.0000 - val_mae: 2306.4355\n",
            "Epoch 453/500\n",
            "750/750 [==============================] - 0s 97us/step - loss: 1921.3444 - mse: 25232412.0000 - mae: 1921.3445 - val_loss: 2304.8721 - val_mse: 31643200.0000 - val_mae: 2304.8721\n",
            "Epoch 454/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 1913.1326 - mse: 24762800.0000 - mae: 1913.1327 - val_loss: 2302.5306 - val_mse: 31620804.0000 - val_mae: 2302.5308\n",
            "Epoch 455/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1926.4561 - mse: 24884574.0000 - mae: 1926.4561 - val_loss: 2305.7803 - val_mse: 31620820.0000 - val_mae: 2305.7803\n",
            "Epoch 456/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1915.2838 - mse: 25236146.0000 - mae: 1915.2837 - val_loss: 2296.2577 - val_mse: 31573612.0000 - val_mae: 2296.2578\n",
            "Epoch 457/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1900.9775 - mse: 24476768.0000 - mae: 1900.9775 - val_loss: 2294.5613 - val_mse: 31554312.0000 - val_mae: 2294.5613\n",
            "Epoch 458/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1922.6990 - mse: 25061750.0000 - mae: 1922.6990 - val_loss: 2299.4829 - val_mse: 31592838.0000 - val_mae: 2299.4829\n",
            "Epoch 459/500\n",
            "750/750 [==============================] - 0s 97us/step - loss: 1910.6244 - mse: 24854994.0000 - mae: 1910.6244 - val_loss: 2304.6500 - val_mse: 31636344.0000 - val_mae: 2304.6499\n",
            "Epoch 460/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1902.5445 - mse: 25026304.0000 - mae: 1902.5443 - val_loss: 2300.4556 - val_mse: 31605826.0000 - val_mae: 2300.4556\n",
            "Epoch 461/500\n",
            "750/750 [==============================] - 0s 95us/step - loss: 1916.6330 - mse: 24877260.0000 - mae: 1916.6331 - val_loss: 2294.4254 - val_mse: 31538256.0000 - val_mae: 2294.4253\n",
            "Epoch 462/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1919.6847 - mse: 24798908.0000 - mae: 1919.6847 - val_loss: 2304.8093 - val_mse: 31585782.0000 - val_mae: 2304.8093\n",
            "Epoch 463/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1901.8016 - mse: 24694034.0000 - mae: 1901.8016 - val_loss: 2295.9619 - val_mse: 31524594.0000 - val_mae: 2295.9619\n",
            "Epoch 464/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1907.4535 - mse: 24578554.0000 - mae: 1907.4535 - val_loss: 2302.0320 - val_mse: 31548072.0000 - val_mae: 2302.0322\n",
            "Epoch 465/500\n",
            "750/750 [==============================] - 0s 99us/step - loss: 1895.7017 - mse: 24519536.0000 - mae: 1895.7018 - val_loss: 2299.5739 - val_mse: 31495794.0000 - val_mae: 2299.5737\n",
            "Epoch 466/500\n",
            "750/750 [==============================] - 0s 97us/step - loss: 1911.1325 - mse: 24618640.0000 - mae: 1911.1324 - val_loss: 2303.4985 - val_mse: 31541324.0000 - val_mae: 2303.4985\n",
            "Epoch 467/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1899.6913 - mse: 24656670.0000 - mae: 1899.6913 - val_loss: 2300.7533 - val_mse: 31520580.0000 - val_mae: 2300.7534\n",
            "Epoch 468/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1914.4944 - mse: 24516718.0000 - mae: 1914.4944 - val_loss: 2302.3594 - val_mse: 31536380.0000 - val_mae: 2302.3594\n",
            "Epoch 469/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1896.9718 - mse: 24428008.0000 - mae: 1896.9718 - val_loss: 2295.5227 - val_mse: 31450544.0000 - val_mae: 2295.5229\n",
            "Epoch 470/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1899.5593 - mse: 24582624.0000 - mae: 1899.5593 - val_loss: 2300.7422 - val_mse: 31487972.0000 - val_mae: 2300.7422\n",
            "Epoch 471/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1907.8474 - mse: 24850666.0000 - mae: 1907.8473 - val_loss: 2300.4895 - val_mse: 31487016.0000 - val_mae: 2300.4893\n",
            "Epoch 472/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1907.7489 - mse: 24727218.0000 - mae: 1907.7490 - val_loss: 2299.0771 - val_mse: 31467652.0000 - val_mae: 2299.0771\n",
            "Epoch 473/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1917.5810 - mse: 24976588.0000 - mae: 1917.5811 - val_loss: 2298.4596 - val_mse: 31444378.0000 - val_mae: 2298.4597\n",
            "Epoch 474/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1913.8070 - mse: 24768848.0000 - mae: 1913.8070 - val_loss: 2305.1710 - val_mse: 31523020.0000 - val_mae: 2305.1709\n",
            "Epoch 475/500\n",
            "750/750 [==============================] - 0s 96us/step - loss: 1884.1726 - mse: 24866462.0000 - mae: 1884.1726 - val_loss: 2301.6481 - val_mse: 31481578.0000 - val_mae: 2301.6482\n",
            "Epoch 476/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 1905.1740 - mse: 24813738.0000 - mae: 1905.1740 - val_loss: 2306.4026 - val_mse: 31509818.0000 - val_mae: 2306.4026\n",
            "Epoch 477/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1907.9950 - mse: 24857636.0000 - mae: 1907.9950 - val_loss: 2312.7462 - val_mse: 31577718.0000 - val_mae: 2312.7461\n",
            "Epoch 478/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1898.0142 - mse: 24950544.0000 - mae: 1898.0143 - val_loss: 2298.8478 - val_mse: 31425216.0000 - val_mae: 2298.8479\n",
            "Epoch 479/500\n",
            "750/750 [==============================] - 0s 92us/step - loss: 1894.2192 - mse: 24639130.0000 - mae: 1894.2191 - val_loss: 2298.6502 - val_mse: 31418408.0000 - val_mae: 2298.6501\n",
            "Epoch 480/500\n",
            "750/750 [==============================] - 0s 95us/step - loss: 1904.0959 - mse: 24500462.0000 - mae: 1904.0958 - val_loss: 2309.5443 - val_mse: 31506344.0000 - val_mae: 2309.5442\n",
            "Epoch 481/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1879.4742 - mse: 24490550.0000 - mae: 1879.4744 - val_loss: 2307.9071 - val_mse: 31510446.0000 - val_mae: 2307.9072\n",
            "Epoch 482/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1908.1206 - mse: 24730290.0000 - mae: 1908.1207 - val_loss: 2307.2484 - val_mse: 31464454.0000 - val_mae: 2307.2483\n",
            "Epoch 483/500\n",
            "750/750 [==============================] - 0s 92us/step - loss: 1905.6899 - mse: 24807142.0000 - mae: 1905.6898 - val_loss: 2304.7352 - val_mse: 31430792.0000 - val_mae: 2304.7351\n",
            "Epoch 484/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1898.6721 - mse: 24760604.0000 - mae: 1898.6720 - val_loss: 2301.9955 - val_mse: 31418862.0000 - val_mae: 2301.9956\n",
            "Epoch 485/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1887.7439 - mse: 24358678.0000 - mae: 1887.7440 - val_loss: 2297.1142 - val_mse: 31377418.0000 - val_mae: 2297.1143\n",
            "Epoch 486/500\n",
            "750/750 [==============================] - 0s 95us/step - loss: 1899.1149 - mse: 24239358.0000 - mae: 1899.1150 - val_loss: 2310.6146 - val_mse: 31453822.0000 - val_mae: 2310.6145\n",
            "Epoch 487/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1900.3820 - mse: 24922570.0000 - mae: 1900.3820 - val_loss: 2294.0991 - val_mse: 31313362.0000 - val_mae: 2294.0991\n",
            "Epoch 488/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1896.3091 - mse: 24598102.0000 - mae: 1896.3092 - val_loss: 2300.4139 - val_mse: 31375158.0000 - val_mae: 2300.4141\n",
            "Epoch 489/500\n",
            "750/750 [==============================] - 0s 100us/step - loss: 1907.3782 - mse: 24373748.0000 - mae: 1907.3783 - val_loss: 2297.2665 - val_mse: 31343476.0000 - val_mae: 2297.2664\n",
            "Epoch 490/500\n",
            "750/750 [==============================] - 0s 94us/step - loss: 1920.1912 - mse: 24682460.0000 - mae: 1920.1912 - val_loss: 2300.8556 - val_mse: 31409996.0000 - val_mae: 2300.8555\n",
            "Epoch 491/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1907.7276 - mse: 24649324.0000 - mae: 1907.7277 - val_loss: 2301.6801 - val_mse: 31410780.0000 - val_mae: 2301.6799\n",
            "Epoch 492/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1886.2599 - mse: 24278986.0000 - mae: 1886.2599 - val_loss: 2305.0095 - val_mse: 31440282.0000 - val_mae: 2305.0093\n",
            "Epoch 493/500\n",
            "750/750 [==============================] - 0s 108us/step - loss: 1906.6218 - mse: 24693566.0000 - mae: 1906.6218 - val_loss: 2313.0752 - val_mse: 31492924.0000 - val_mae: 2313.0750\n",
            "Epoch 494/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1901.4957 - mse: 24666020.0000 - mae: 1901.4957 - val_loss: 2308.4597 - val_mse: 31414202.0000 - val_mae: 2308.4597\n",
            "Epoch 495/500\n",
            "750/750 [==============================] - 0s 105us/step - loss: 1889.2739 - mse: 24171908.0000 - mae: 1889.2740 - val_loss: 2313.9529 - val_mse: 31444298.0000 - val_mae: 2313.9529\n",
            "Epoch 496/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1902.6336 - mse: 24809474.0000 - mae: 1902.6337 - val_loss: 2299.1341 - val_mse: 31355598.0000 - val_mae: 2299.1340\n",
            "Epoch 497/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1884.6264 - mse: 24512444.0000 - mae: 1884.6263 - val_loss: 2295.5488 - val_mse: 31350412.0000 - val_mae: 2295.5486\n",
            "Epoch 498/500\n",
            "750/750 [==============================] - 0s 111us/step - loss: 1893.3578 - mse: 24497322.0000 - mae: 1893.3578 - val_loss: 2306.0730 - val_mse: 31463030.0000 - val_mae: 2306.0730\n",
            "Epoch 499/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1897.4460 - mse: 24705158.0000 - mae: 1897.4460 - val_loss: 2305.1502 - val_mse: 31442516.0000 - val_mae: 2305.1501\n",
            "Epoch 500/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1910.3576 - mse: 24846620.0000 - mae: 1910.3577 - val_loss: 2295.7720 - val_mse: 31343812.0000 - val_mae: 2295.7717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1130 samples, validate on 1120 samples\n",
            "Epoch 1/500\n",
            "1130/1130 [==============================] - 2s 2ms/step - loss: 2909.3071 - mse: 44133796.0000 - mae: 2909.3071 - val_loss: 2775.3115 - val_mse: 39073272.0000 - val_mae: 2775.3115\n",
            "Epoch 2/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2909.2685 - mse: 44133560.0000 - mae: 2909.2686 - val_loss: 2775.2707 - val_mse: 39073052.0000 - val_mae: 2775.2710\n",
            "Epoch 3/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2909.1677 - mse: 44133072.0000 - mae: 2909.1675 - val_loss: 2774.8077 - val_mse: 39071140.0000 - val_mae: 2774.8081\n",
            "Epoch 4/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2907.9254 - mse: 44127908.0000 - mae: 2907.9253 - val_loss: 2772.0912 - val_mse: 39059424.0000 - val_mae: 2772.0911\n",
            "Epoch 5/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2904.4046 - mse: 44110644.0000 - mae: 2904.4045 - val_loss: 2767.0448 - val_mse: 39035596.0000 - val_mae: 2767.0449\n",
            "Epoch 6/500\n",
            "1130/1130 [==============================] - 0s 59us/step - loss: 2898.6392 - mse: 44081848.0000 - mae: 2898.6392 - val_loss: 2760.0133 - val_mse: 38999160.0000 - val_mae: 2760.0134\n",
            "Epoch 7/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2890.2096 - mse: 44034768.0000 - mae: 2890.2097 - val_loss: 2750.8371 - val_mse: 38947280.0000 - val_mae: 2750.8372\n",
            "Epoch 8/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2880.4779 - mse: 43980372.0000 - mae: 2880.4778 - val_loss: 2739.8253 - val_mse: 38882100.0000 - val_mae: 2739.8254\n",
            "Epoch 9/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2869.3734 - mse: 43912684.0000 - mae: 2869.3738 - val_loss: 2727.9321 - val_mse: 38807956.0000 - val_mae: 2727.9319\n",
            "Epoch 10/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2855.7034 - mse: 43819760.0000 - mae: 2855.7036 - val_loss: 2714.3942 - val_mse: 38717300.0000 - val_mae: 2714.3943\n",
            "Epoch 11/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2843.2973 - mse: 43726840.0000 - mae: 2843.2974 - val_loss: 2699.9548 - val_mse: 38617392.0000 - val_mae: 2699.9548\n",
            "Epoch 12/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2827.0089 - mse: 43619072.0000 - mae: 2827.0088 - val_loss: 2683.9291 - val_mse: 38504096.0000 - val_mae: 2683.9290\n",
            "Epoch 13/500\n",
            "1130/1130 [==============================] - 0s 59us/step - loss: 2810.4657 - mse: 43492708.0000 - mae: 2810.4658 - val_loss: 2666.8645 - val_mse: 38382844.0000 - val_mae: 2666.8645\n",
            "Epoch 14/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2790.6889 - mse: 43373380.0000 - mae: 2790.6890 - val_loss: 2648.4042 - val_mse: 38248956.0000 - val_mae: 2648.4043\n",
            "Epoch 15/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2772.9473 - mse: 43206636.0000 - mae: 2772.9473 - val_loss: 2630.4233 - val_mse: 38111408.0000 - val_mae: 2630.4231\n",
            "Epoch 16/500\n",
            "1130/1130 [==============================] - 0s 57us/step - loss: 2756.6283 - mse: 43072944.0000 - mae: 2756.6282 - val_loss: 2613.0660 - val_mse: 37972500.0000 - val_mae: 2613.0662\n",
            "Epoch 17/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2741.0756 - mse: 42924128.0000 - mae: 2741.0757 - val_loss: 2596.5667 - val_mse: 37830316.0000 - val_mae: 2596.5664\n",
            "Epoch 18/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2724.8831 - mse: 42766608.0000 - mae: 2724.8833 - val_loss: 2581.8603 - val_mse: 37692948.0000 - val_mae: 2581.8604\n",
            "Epoch 19/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2713.1029 - mse: 42632152.0000 - mae: 2713.1028 - val_loss: 2568.5296 - val_mse: 37558416.0000 - val_mae: 2568.5298\n",
            "Epoch 20/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2697.8725 - mse: 42461320.0000 - mae: 2697.8726 - val_loss: 2556.8003 - val_mse: 37426412.0000 - val_mae: 2556.8003\n",
            "Epoch 21/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2686.3256 - mse: 42334344.0000 - mae: 2686.3257 - val_loss: 2546.3512 - val_mse: 37294792.0000 - val_mae: 2546.3511\n",
            "Epoch 22/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2673.9456 - mse: 42251624.0000 - mae: 2673.9453 - val_loss: 2537.2803 - val_mse: 37168104.0000 - val_mae: 2537.2800\n",
            "Epoch 23/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2669.1698 - mse: 42137612.0000 - mae: 2669.1699 - val_loss: 2529.6000 - val_mse: 37051284.0000 - val_mae: 2529.6001\n",
            "Epoch 24/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2652.8783 - mse: 41971880.0000 - mae: 2652.8782 - val_loss: 2522.9467 - val_mse: 36932320.0000 - val_mae: 2522.9468\n",
            "Epoch 25/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2647.7160 - mse: 41839984.0000 - mae: 2647.7158 - val_loss: 2517.3672 - val_mse: 36824556.0000 - val_mae: 2517.3674\n",
            "Epoch 26/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2646.6589 - mse: 41785596.0000 - mae: 2646.6589 - val_loss: 2512.4380 - val_mse: 36716436.0000 - val_mae: 2512.4382\n",
            "Epoch 27/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2639.5938 - mse: 41610260.0000 - mae: 2639.5938 - val_loss: 2508.4590 - val_mse: 36620468.0000 - val_mae: 2508.4590\n",
            "Epoch 28/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2637.0962 - mse: 41480084.0000 - mae: 2637.0959 - val_loss: 2505.2756 - val_mse: 36538320.0000 - val_mae: 2505.2756\n",
            "Epoch 29/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2626.3127 - mse: 41448008.0000 - mae: 2626.3127 - val_loss: 2501.0737 - val_mse: 36428008.0000 - val_mae: 2501.0737\n",
            "Epoch 30/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2627.2906 - mse: 41348420.0000 - mae: 2627.2908 - val_loss: 2498.4798 - val_mse: 36352596.0000 - val_mae: 2498.4800\n",
            "Epoch 31/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2623.3563 - mse: 41267840.0000 - mae: 2623.3564 - val_loss: 2496.9521 - val_mse: 36291168.0000 - val_mae: 2496.9521\n",
            "Epoch 32/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2623.1412 - mse: 41225876.0000 - mae: 2623.1411 - val_loss: 2494.9590 - val_mse: 36223544.0000 - val_mae: 2494.9587\n",
            "Epoch 33/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2612.2333 - mse: 41061416.0000 - mae: 2612.2334 - val_loss: 2493.9471 - val_mse: 36144232.0000 - val_mae: 2493.9473\n",
            "Epoch 34/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2618.0881 - mse: 41052240.0000 - mae: 2618.0881 - val_loss: 2492.1213 - val_mse: 36093044.0000 - val_mae: 2492.1211\n",
            "Epoch 35/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2602.0082 - mse: 40838428.0000 - mae: 2602.0081 - val_loss: 2490.8006 - val_mse: 36031292.0000 - val_mae: 2490.8008\n",
            "Epoch 36/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2616.5900 - mse: 40862792.0000 - mae: 2616.5898 - val_loss: 2489.6198 - val_mse: 35990748.0000 - val_mae: 2489.6194\n",
            "Epoch 37/500\n",
            "1130/1130 [==============================] - 0s 59us/step - loss: 2615.1111 - mse: 40903660.0000 - mae: 2615.1113 - val_loss: 2488.4067 - val_mse: 35940740.0000 - val_mae: 2488.4067\n",
            "Epoch 38/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2614.2737 - mse: 40880764.0000 - mae: 2614.2734 - val_loss: 2487.2050 - val_mse: 35888132.0000 - val_mae: 2487.2048\n",
            "Epoch 39/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2615.4128 - mse: 40894628.0000 - mae: 2615.4131 - val_loss: 2484.8538 - val_mse: 35855632.0000 - val_mae: 2484.8538\n",
            "Epoch 40/500\n",
            "1130/1130 [==============================] - 0s 59us/step - loss: 2613.6430 - mse: 40886756.0000 - mae: 2613.6428 - val_loss: 2482.1640 - val_mse: 35837604.0000 - val_mae: 2482.1641\n",
            "Epoch 41/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2606.5314 - mse: 40769760.0000 - mae: 2606.5315 - val_loss: 2478.8706 - val_mse: 35839580.0000 - val_mae: 2478.8706\n",
            "Epoch 42/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2610.1990 - mse: 40919600.0000 - mae: 2610.1990 - val_loss: 2477.4559 - val_mse: 35791996.0000 - val_mae: 2477.4561\n",
            "Epoch 43/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2601.3708 - mse: 40755720.0000 - mae: 2601.3708 - val_loss: 2474.7327 - val_mse: 35781448.0000 - val_mae: 2474.7329\n",
            "Epoch 44/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2592.9972 - mse: 40779212.0000 - mae: 2592.9973 - val_loss: 2474.3495 - val_mse: 35692580.0000 - val_mae: 2474.3496\n",
            "Epoch 45/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2599.5896 - mse: 40671152.0000 - mae: 2599.5896 - val_loss: 2474.0158 - val_mse: 35619480.0000 - val_mae: 2474.0159\n",
            "Epoch 46/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2598.3004 - mse: 40535284.0000 - mae: 2598.3005 - val_loss: 2471.7432 - val_mse: 35586284.0000 - val_mae: 2471.7434\n",
            "Epoch 47/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2592.0008 - mse: 40521700.0000 - mae: 2592.0010 - val_loss: 2470.2910 - val_mse: 35530640.0000 - val_mae: 2470.2910\n",
            "Epoch 48/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2593.3024 - mse: 40569800.0000 - mae: 2593.3025 - val_loss: 2467.1182 - val_mse: 35521192.0000 - val_mae: 2467.1184\n",
            "Epoch 49/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2585.2829 - mse: 40501360.0000 - mae: 2585.2830 - val_loss: 2464.5845 - val_mse: 35494472.0000 - val_mae: 2464.5847\n",
            "Epoch 50/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2599.9821 - mse: 40638672.0000 - mae: 2599.9822 - val_loss: 2462.6384 - val_mse: 35470080.0000 - val_mae: 2462.6384\n",
            "Epoch 51/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2586.1114 - mse: 40369168.0000 - mae: 2586.1116 - val_loss: 2462.1896 - val_mse: 35407864.0000 - val_mae: 2462.1895\n",
            "Epoch 52/500\n",
            "1130/1130 [==============================] - 0s 58us/step - loss: 2589.6300 - mse: 40466660.0000 - mae: 2589.6296 - val_loss: 2461.1242 - val_mse: 35351376.0000 - val_mae: 2461.1243\n",
            "Epoch 53/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2583.2592 - mse: 40232336.0000 - mae: 2583.2593 - val_loss: 2458.8902 - val_mse: 35334580.0000 - val_mae: 2458.8901\n",
            "Epoch 54/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2586.3035 - mse: 40335428.0000 - mae: 2586.3032 - val_loss: 2452.6483 - val_mse: 35404560.0000 - val_mae: 2452.6482\n",
            "Epoch 55/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2579.2507 - mse: 40235288.0000 - mae: 2579.2507 - val_loss: 2454.0351 - val_mse: 35297952.0000 - val_mae: 2454.0349\n",
            "Epoch 56/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2579.7076 - mse: 40232440.0000 - mae: 2579.7078 - val_loss: 2453.5849 - val_mse: 35247480.0000 - val_mae: 2453.5850\n",
            "Epoch 57/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2580.2119 - mse: 40244816.0000 - mae: 2580.2117 - val_loss: 2452.1331 - val_mse: 35202804.0000 - val_mae: 2452.1335\n",
            "Epoch 58/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2580.9274 - mse: 40162788.0000 - mae: 2580.9275 - val_loss: 2448.9603 - val_mse: 35211692.0000 - val_mae: 2448.9600\n",
            "Epoch 59/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2573.8294 - mse: 40156392.0000 - mae: 2573.8293 - val_loss: 2451.3087 - val_mse: 35099104.0000 - val_mae: 2451.3086\n",
            "Epoch 60/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2569.3347 - mse: 40035288.0000 - mae: 2569.3347 - val_loss: 2452.8407 - val_mse: 35009776.0000 - val_mae: 2452.8408\n",
            "Epoch 61/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2573.6142 - mse: 40025640.0000 - mae: 2573.6145 - val_loss: 2446.5786 - val_mse: 35051740.0000 - val_mae: 2446.5784\n",
            "Epoch 62/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2572.8946 - mse: 39964456.0000 - mae: 2572.8945 - val_loss: 2448.0067 - val_mse: 34963988.0000 - val_mae: 2448.0066\n",
            "Epoch 63/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2566.2136 - mse: 39996440.0000 - mae: 2566.2134 - val_loss: 2446.5426 - val_mse: 34934740.0000 - val_mae: 2446.5425\n",
            "Epoch 64/500\n",
            "1130/1130 [==============================] - 0s 56us/step - loss: 2565.7963 - mse: 39901124.0000 - mae: 2565.7959 - val_loss: 2442.8830 - val_mse: 34944212.0000 - val_mae: 2442.8831\n",
            "Epoch 65/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2559.4858 - mse: 39757424.0000 - mae: 2559.4858 - val_loss: 2445.6674 - val_mse: 34838724.0000 - val_mae: 2445.6675\n",
            "Epoch 66/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2574.6596 - mse: 39798008.0000 - mae: 2574.6594 - val_loss: 2434.6851 - val_mse: 34976980.0000 - val_mae: 2434.6853\n",
            "Epoch 67/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2570.7981 - mse: 39909176.0000 - mae: 2570.7981 - val_loss: 2441.0786 - val_mse: 34816616.0000 - val_mae: 2441.0789\n",
            "Epoch 68/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2563.8382 - mse: 39785860.0000 - mae: 2563.8384 - val_loss: 2436.1352 - val_mse: 34854848.0000 - val_mae: 2436.1353\n",
            "Epoch 69/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2562.9430 - mse: 39984964.0000 - mae: 2562.9429 - val_loss: 2443.7787 - val_mse: 34686800.0000 - val_mae: 2443.7786\n",
            "Epoch 70/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2567.4688 - mse: 39606532.0000 - mae: 2567.4688 - val_loss: 2441.3968 - val_mse: 34676800.0000 - val_mae: 2441.3970\n",
            "Epoch 71/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2570.7008 - mse: 39840088.0000 - mae: 2570.7009 - val_loss: 2434.1928 - val_mse: 34746788.0000 - val_mae: 2434.1929\n",
            "Epoch 72/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2563.2046 - mse: 39736516.0000 - mae: 2563.2043 - val_loss: 2431.7545 - val_mse: 34753552.0000 - val_mae: 2431.7544\n",
            "Epoch 73/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2561.5808 - mse: 39781456.0000 - mae: 2561.5811 - val_loss: 2435.4748 - val_mse: 34654864.0000 - val_mae: 2435.4746\n",
            "Epoch 74/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2561.9812 - mse: 39624328.0000 - mae: 2561.9812 - val_loss: 2431.2550 - val_mse: 34665516.0000 - val_mae: 2431.2549\n",
            "Epoch 75/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2558.1055 - mse: 39724632.0000 - mae: 2558.1055 - val_loss: 2437.7344 - val_mse: 34545052.0000 - val_mae: 2437.7344\n",
            "Epoch 76/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2560.9190 - mse: 39517508.0000 - mae: 2560.9189 - val_loss: 2432.5638 - val_mse: 34564648.0000 - val_mae: 2432.5637\n",
            "Epoch 77/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2554.1802 - mse: 39573944.0000 - mae: 2554.1802 - val_loss: 2428.5970 - val_mse: 34578952.0000 - val_mae: 2428.5969\n",
            "Epoch 78/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2542.3551 - mse: 39428792.0000 - mae: 2542.3550 - val_loss: 2426.9586 - val_mse: 34563656.0000 - val_mae: 2426.9583\n",
            "Epoch 79/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2550.7935 - mse: 39476304.0000 - mae: 2550.7937 - val_loss: 2429.3530 - val_mse: 34475760.0000 - val_mae: 2429.3530\n",
            "Epoch 80/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2544.0156 - mse: 39416444.0000 - mae: 2544.0154 - val_loss: 2429.1620 - val_mse: 34426164.0000 - val_mae: 2429.1621\n",
            "Epoch 81/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2540.6097 - mse: 39290860.0000 - mae: 2540.6096 - val_loss: 2420.2595 - val_mse: 34514656.0000 - val_mae: 2420.2593\n",
            "Epoch 82/500\n",
            "1130/1130 [==============================] - 0s 59us/step - loss: 2539.8262 - mse: 39279008.0000 - mae: 2539.8264 - val_loss: 2429.2216 - val_mse: 34361192.0000 - val_mae: 2429.2214\n",
            "Epoch 83/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2549.7297 - mse: 39474716.0000 - mae: 2549.7297 - val_loss: 2411.0801 - val_mse: 34626760.0000 - val_mae: 2411.0798\n",
            "Epoch 84/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2541.9681 - mse: 39450572.0000 - mae: 2541.9683 - val_loss: 2422.8271 - val_mse: 34338284.0000 - val_mae: 2422.8269\n",
            "Epoch 85/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2545.6786 - mse: 39241840.0000 - mae: 2545.6787 - val_loss: 2410.8420 - val_mse: 34476564.0000 - val_mae: 2410.8420\n",
            "Epoch 86/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2545.8002 - mse: 39252280.0000 - mae: 2545.8005 - val_loss: 2412.2164 - val_mse: 34406788.0000 - val_mae: 2412.2166\n",
            "Epoch 87/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2544.0445 - mse: 39444300.0000 - mae: 2544.0444 - val_loss: 2408.4280 - val_mse: 34417368.0000 - val_mae: 2408.4282\n",
            "Epoch 88/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2540.6447 - mse: 39404760.0000 - mae: 2540.6448 - val_loss: 2406.1864 - val_mse: 34415704.0000 - val_mae: 2406.1863\n",
            "Epoch 89/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2543.7006 - mse: 39323616.0000 - mae: 2543.7004 - val_loss: 2419.9801 - val_mse: 34174784.0000 - val_mae: 2419.9800\n",
            "Epoch 90/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2541.7486 - mse: 39323928.0000 - mae: 2541.7485 - val_loss: 2411.8869 - val_mse: 34240140.0000 - val_mae: 2411.8867\n",
            "Epoch 91/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2536.5505 - mse: 39280040.0000 - mae: 2536.5508 - val_loss: 2413.3117 - val_mse: 34179152.0000 - val_mae: 2413.3118\n",
            "Epoch 92/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2535.2653 - mse: 39152564.0000 - mae: 2535.2654 - val_loss: 2412.2833 - val_mse: 34157164.0000 - val_mae: 2412.2834\n",
            "Epoch 93/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2536.6126 - mse: 39070836.0000 - mae: 2536.6123 - val_loss: 2410.3144 - val_mse: 34124280.0000 - val_mae: 2410.3142\n",
            "Epoch 94/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2536.3467 - mse: 39062072.0000 - mae: 2536.3469 - val_loss: 2406.9677 - val_mse: 34130880.0000 - val_mae: 2406.9675\n",
            "Epoch 95/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2526.7250 - mse: 39019316.0000 - mae: 2526.7251 - val_loss: 2418.7649 - val_mse: 33955120.0000 - val_mae: 2418.7646\n",
            "Epoch 96/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2522.8709 - mse: 38933140.0000 - mae: 2522.8711 - val_loss: 2408.5417 - val_mse: 34023444.0000 - val_mae: 2408.5415\n",
            "Epoch 97/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2526.7792 - mse: 38858876.0000 - mae: 2526.7793 - val_loss: 2407.3204 - val_mse: 33994856.0000 - val_mae: 2407.3206\n",
            "Epoch 98/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2523.8505 - mse: 38923436.0000 - mae: 2523.8503 - val_loss: 2403.8831 - val_mse: 33995356.0000 - val_mae: 2403.8828\n",
            "Epoch 99/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2531.6071 - mse: 38985552.0000 - mae: 2531.6072 - val_loss: 2404.9245 - val_mse: 33936300.0000 - val_mae: 2404.9246\n",
            "Epoch 100/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2531.1335 - mse: 38724576.0000 - mae: 2531.1335 - val_loss: 2408.1635 - val_mse: 33871936.0000 - val_mae: 2408.1633\n",
            "Epoch 101/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2532.0774 - mse: 38882564.0000 - mae: 2532.0774 - val_loss: 2393.1481 - val_mse: 34041644.0000 - val_mae: 2393.1477\n",
            "Epoch 102/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2530.7728 - mse: 38890192.0000 - mae: 2530.7725 - val_loss: 2398.7304 - val_mse: 33914640.0000 - val_mae: 2398.7305\n",
            "Epoch 103/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2522.5971 - mse: 38782388.0000 - mae: 2522.5972 - val_loss: 2405.0073 - val_mse: 33799864.0000 - val_mae: 2405.0073\n",
            "Epoch 104/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2528.6669 - mse: 38862704.0000 - mae: 2528.6667 - val_loss: 2391.4487 - val_mse: 33941484.0000 - val_mae: 2391.4490\n",
            "Epoch 105/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2523.3319 - mse: 38758540.0000 - mae: 2523.3318 - val_loss: 2387.7005 - val_mse: 33978876.0000 - val_mae: 2387.7007\n",
            "Epoch 106/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2524.4379 - mse: 38818088.0000 - mae: 2524.4377 - val_loss: 2407.2282 - val_mse: 33671888.0000 - val_mae: 2407.2283\n",
            "Epoch 107/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2529.2902 - mse: 38903824.0000 - mae: 2529.2903 - val_loss: 2392.2497 - val_mse: 33813648.0000 - val_mae: 2392.2498\n",
            "Epoch 108/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2516.0083 - mse: 38724632.0000 - mae: 2516.0083 - val_loss: 2394.9430 - val_mse: 33737724.0000 - val_mae: 2394.9431\n",
            "Epoch 109/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2529.7222 - mse: 38664208.0000 - mae: 2529.7222 - val_loss: 2392.2863 - val_mse: 33747692.0000 - val_mae: 2392.2864\n",
            "Epoch 110/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2516.7449 - mse: 38616812.0000 - mae: 2516.7446 - val_loss: 2385.9045 - val_mse: 33815744.0000 - val_mae: 2385.9048\n",
            "Epoch 111/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2513.7630 - mse: 38482520.0000 - mae: 2513.7629 - val_loss: 2388.7762 - val_mse: 33713900.0000 - val_mae: 2388.7764\n",
            "Epoch 112/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2516.4848 - mse: 38546516.0000 - mae: 2516.4849 - val_loss: 2391.7736 - val_mse: 33627144.0000 - val_mae: 2391.7734\n",
            "Epoch 113/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2510.5925 - mse: 38615524.0000 - mae: 2510.5925 - val_loss: 2381.3056 - val_mse: 33732992.0000 - val_mae: 2381.3059\n",
            "Epoch 114/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2517.9881 - mse: 38454952.0000 - mae: 2517.9880 - val_loss: 2384.6509 - val_mse: 33630916.0000 - val_mae: 2384.6509\n",
            "Epoch 115/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2518.0902 - mse: 38634016.0000 - mae: 2518.0903 - val_loss: 2392.1036 - val_mse: 33493076.0000 - val_mae: 2392.1035\n",
            "Epoch 116/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2516.6110 - mse: 38506904.0000 - mae: 2516.6108 - val_loss: 2389.6056 - val_mse: 33480562.0000 - val_mae: 2389.6057\n",
            "Epoch 117/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2513.7839 - mse: 38415152.0000 - mae: 2513.7839 - val_loss: 2392.3372 - val_mse: 33425716.0000 - val_mae: 2392.3372\n",
            "Epoch 118/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2521.8154 - mse: 38533320.0000 - mae: 2521.8154 - val_loss: 2391.5375 - val_mse: 33406804.0000 - val_mae: 2391.5376\n",
            "Epoch 119/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2513.9033 - mse: 38333184.0000 - mae: 2513.9033 - val_loss: 2387.0724 - val_mse: 33428802.0000 - val_mae: 2387.0723\n",
            "Epoch 120/500\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 2504.6914 - mse: 38343428.0000 - mae: 2504.6914 - val_loss: 2386.9241 - val_mse: 33409880.0000 - val_mae: 2386.9241\n",
            "Epoch 121/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2509.4682 - mse: 38456132.0000 - mae: 2509.4683 - val_loss: 2382.7425 - val_mse: 33424776.0000 - val_mae: 2382.7424\n",
            "Epoch 122/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2509.3147 - mse: 38395924.0000 - mae: 2509.3149 - val_loss: 2383.0931 - val_mse: 33390600.0000 - val_mae: 2383.0933\n",
            "Epoch 123/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2514.9005 - mse: 38357824.0000 - mae: 2514.9004 - val_loss: 2376.7320 - val_mse: 33462334.0000 - val_mae: 2376.7319\n",
            "Epoch 124/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2510.9982 - mse: 38557184.0000 - mae: 2510.9983 - val_loss: 2396.0134 - val_mse: 33193128.0000 - val_mae: 2396.0134\n",
            "Epoch 125/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2510.3486 - mse: 38104084.0000 - mae: 2510.3486 - val_loss: 2371.4971 - val_mse: 33479478.0000 - val_mae: 2371.4971\n",
            "Epoch 126/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2500.7696 - mse: 38215788.0000 - mae: 2500.7695 - val_loss: 2372.3370 - val_mse: 33393906.0000 - val_mae: 2372.3372\n",
            "Epoch 127/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2505.0398 - mse: 38155824.0000 - mae: 2505.0398 - val_loss: 2368.5699 - val_mse: 33456186.0000 - val_mae: 2368.5698\n",
            "Epoch 128/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2505.0866 - mse: 38270928.0000 - mae: 2505.0867 - val_loss: 2379.1330 - val_mse: 33210514.0000 - val_mae: 2379.1328\n",
            "Epoch 129/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2506.3267 - mse: 37981880.0000 - mae: 2506.3267 - val_loss: 2377.6852 - val_mse: 33191300.0000 - val_mae: 2377.6851\n",
            "Epoch 130/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2514.0818 - mse: 38188496.0000 - mae: 2514.0818 - val_loss: 2373.5256 - val_mse: 33230534.0000 - val_mae: 2373.5254\n",
            "Epoch 131/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2501.1318 - mse: 38068364.0000 - mae: 2501.1316 - val_loss: 2372.3352 - val_mse: 33213638.0000 - val_mae: 2372.3352\n",
            "Epoch 132/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2500.4849 - mse: 38059680.0000 - mae: 2500.4849 - val_loss: 2365.5748 - val_mse: 33315192.0000 - val_mae: 2365.5747\n",
            "Epoch 133/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2508.5434 - mse: 38062368.0000 - mae: 2508.5437 - val_loss: 2371.4316 - val_mse: 33170546.0000 - val_mae: 2371.4316\n",
            "Epoch 134/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2501.2806 - mse: 38110836.0000 - mae: 2501.2808 - val_loss: 2370.8225 - val_mse: 33144938.0000 - val_mae: 2370.8223\n",
            "Epoch 135/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2492.8882 - mse: 37816052.0000 - mae: 2492.8879 - val_loss: 2377.2988 - val_mse: 33010512.0000 - val_mae: 2377.2983\n",
            "Epoch 136/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2510.6855 - mse: 38069260.0000 - mae: 2510.6853 - val_loss: 2366.8718 - val_mse: 33153718.0000 - val_mae: 2366.8718\n",
            "Epoch 137/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2503.1488 - mse: 37890820.0000 - mae: 2503.1487 - val_loss: 2370.4488 - val_mse: 33070790.0000 - val_mae: 2370.4490\n",
            "Epoch 138/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2493.5596 - mse: 37794500.0000 - mae: 2493.5596 - val_loss: 2364.8049 - val_mse: 33138506.0000 - val_mae: 2364.8047\n",
            "Epoch 139/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2509.5540 - mse: 38092084.0000 - mae: 2509.5537 - val_loss: 2371.7572 - val_mse: 33003740.0000 - val_mae: 2371.7571\n",
            "Epoch 140/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2494.1178 - mse: 37606440.0000 - mae: 2494.1177 - val_loss: 2363.8711 - val_mse: 33085638.0000 - val_mae: 2363.8711\n",
            "Epoch 141/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2493.2453 - mse: 37778172.0000 - mae: 2493.2451 - val_loss: 2363.9989 - val_mse: 33067070.0000 - val_mae: 2363.9990\n",
            "Epoch 142/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2489.0609 - mse: 37737804.0000 - mae: 2489.0608 - val_loss: 2361.6519 - val_mse: 33073616.0000 - val_mae: 2361.6521\n",
            "Epoch 143/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2503.7421 - mse: 37973612.0000 - mae: 2503.7419 - val_loss: 2363.5605 - val_mse: 32998426.0000 - val_mae: 2363.5608\n",
            "Epoch 144/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2493.9813 - mse: 37871648.0000 - mae: 2493.9812 - val_loss: 2368.5910 - val_mse: 32900074.0000 - val_mae: 2368.5911\n",
            "Epoch 145/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2498.4501 - mse: 37822716.0000 - mae: 2498.4502 - val_loss: 2356.7215 - val_mse: 33078436.0000 - val_mae: 2356.7214\n",
            "Epoch 146/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2492.8630 - mse: 37802776.0000 - mae: 2492.8630 - val_loss: 2359.8028 - val_mse: 32960530.0000 - val_mae: 2359.8027\n",
            "Epoch 147/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2482.6434 - mse: 37447416.0000 - mae: 2482.6433 - val_loss: 2359.9812 - val_mse: 32929686.0000 - val_mae: 2359.9810\n",
            "Epoch 148/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2495.7300 - mse: 37611280.0000 - mae: 2495.7300 - val_loss: 2369.0205 - val_mse: 32776886.0000 - val_mae: 2369.0205\n",
            "Epoch 149/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2492.1114 - mse: 37414424.0000 - mae: 2492.1116 - val_loss: 2358.0065 - val_mse: 32889932.0000 - val_mae: 2358.0066\n",
            "Epoch 150/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2495.7668 - mse: 37534380.0000 - mae: 2495.7668 - val_loss: 2353.5612 - val_mse: 32967050.0000 - val_mae: 2353.5613\n",
            "Epoch 151/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2474.6948 - mse: 37454296.0000 - mae: 2474.6948 - val_loss: 2360.1511 - val_mse: 32782172.0000 - val_mae: 2360.1514\n",
            "Epoch 152/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2490.9793 - mse: 37601392.0000 - mae: 2490.9792 - val_loss: 2351.2707 - val_mse: 32951022.0000 - val_mae: 2351.2705\n",
            "Epoch 153/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2492.6504 - mse: 37544328.0000 - mae: 2492.6501 - val_loss: 2350.9029 - val_mse: 32934568.0000 - val_mae: 2350.9028\n",
            "Epoch 154/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2485.2916 - mse: 37392392.0000 - mae: 2485.2915 - val_loss: 2349.6360 - val_mse: 32983648.0000 - val_mae: 2349.6362\n",
            "Epoch 155/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2494.4606 - mse: 37574716.0000 - mae: 2494.4607 - val_loss: 2357.7071 - val_mse: 32720092.0000 - val_mae: 2357.7073\n",
            "Epoch 156/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2479.2207 - mse: 37278176.0000 - mae: 2479.2207 - val_loss: 2351.6950 - val_mse: 32810152.0000 - val_mae: 2351.6951\n",
            "Epoch 157/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2484.3604 - mse: 37507572.0000 - mae: 2484.3604 - val_loss: 2353.8197 - val_mse: 32704604.0000 - val_mae: 2353.8196\n",
            "Epoch 158/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2488.2266 - mse: 37360172.0000 - mae: 2488.2266 - val_loss: 2350.8333 - val_mse: 32735002.0000 - val_mae: 2350.8333\n",
            "Epoch 159/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2487.1997 - mse: 37505828.0000 - mae: 2487.1997 - val_loss: 2349.0256 - val_mse: 32737254.0000 - val_mae: 2349.0256\n",
            "Epoch 160/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2481.0502 - mse: 37421736.0000 - mae: 2481.0500 - val_loss: 2346.4101 - val_mse: 32747172.0000 - val_mae: 2346.4099\n",
            "Epoch 161/500\n",
            "1130/1130 [==============================] - 0s 59us/step - loss: 2488.9289 - mse: 37486776.0000 - mae: 2488.9287 - val_loss: 2349.4892 - val_mse: 32641356.0000 - val_mae: 2349.4893\n",
            "Epoch 162/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2484.2350 - mse: 37459212.0000 - mae: 2484.2349 - val_loss: 2343.7654 - val_mse: 32748862.0000 - val_mae: 2343.7656\n",
            "Epoch 163/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2479.8559 - mse: 37505176.0000 - mae: 2479.8560 - val_loss: 2354.3259 - val_mse: 32509532.0000 - val_mae: 2354.3259\n",
            "Epoch 164/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2476.7700 - mse: 37111808.0000 - mae: 2476.7700 - val_loss: 2341.0454 - val_mse: 32724228.0000 - val_mae: 2341.0457\n",
            "Epoch 165/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2471.8601 - mse: 37133432.0000 - mae: 2471.8601 - val_loss: 2339.6870 - val_mse: 32752216.0000 - val_mae: 2339.6870\n",
            "Epoch 166/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2468.9853 - mse: 37240092.0000 - mae: 2468.9854 - val_loss: 2339.3489 - val_mse: 32623242.0000 - val_mae: 2339.3491\n",
            "Epoch 167/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2482.2255 - mse: 37362244.0000 - mae: 2482.2256 - val_loss: 2343.9952 - val_mse: 32449996.0000 - val_mae: 2343.9954\n",
            "Epoch 168/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2480.3301 - mse: 37087688.0000 - mae: 2480.3301 - val_loss: 2338.1518 - val_mse: 32530868.0000 - val_mae: 2338.1519\n",
            "Epoch 169/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2491.4511 - mse: 37392616.0000 - mae: 2491.4514 - val_loss: 2339.8545 - val_mse: 32509846.0000 - val_mae: 2339.8545\n",
            "Epoch 170/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2477.2950 - mse: 37276220.0000 - mae: 2477.2952 - val_loss: 2335.7431 - val_mse: 32584064.0000 - val_mae: 2335.7432\n",
            "Epoch 171/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2481.3912 - mse: 37193992.0000 - mae: 2481.3911 - val_loss: 2335.2803 - val_mse: 32528274.0000 - val_mae: 2335.2803\n",
            "Epoch 172/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2476.1717 - mse: 36921556.0000 - mae: 2476.1716 - val_loss: 2334.1171 - val_mse: 32677530.0000 - val_mae: 2334.1172\n",
            "Epoch 173/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2466.8355 - mse: 37133180.0000 - mae: 2466.8357 - val_loss: 2333.4454 - val_mse: 32484088.0000 - val_mae: 2333.4456\n",
            "Epoch 174/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2476.3977 - mse: 37154472.0000 - mae: 2476.3975 - val_loss: 2331.0807 - val_mse: 32511796.0000 - val_mae: 2331.0806\n",
            "Epoch 175/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2474.1147 - mse: 37082012.0000 - mae: 2474.1145 - val_loss: 2335.2518 - val_mse: 32346928.0000 - val_mae: 2335.2520\n",
            "Epoch 176/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2458.2217 - mse: 36724480.0000 - mae: 2458.2217 - val_loss: 2329.8314 - val_mse: 32530640.0000 - val_mae: 2329.8315\n",
            "Epoch 177/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2469.1707 - mse: 37063472.0000 - mae: 2469.1709 - val_loss: 2328.1833 - val_mse: 32454554.0000 - val_mae: 2328.1833\n",
            "Epoch 178/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2466.8295 - mse: 36744812.0000 - mae: 2466.8293 - val_loss: 2326.9324 - val_mse: 32418892.0000 - val_mae: 2326.9324\n",
            "Epoch 179/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2458.5184 - mse: 36750136.0000 - mae: 2458.5186 - val_loss: 2329.6694 - val_mse: 32254094.0000 - val_mae: 2329.6692\n",
            "Epoch 180/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2467.0909 - mse: 37070296.0000 - mae: 2467.0908 - val_loss: 2328.8003 - val_mse: 32231066.0000 - val_mae: 2328.8003\n",
            "Epoch 181/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2463.9821 - mse: 36747532.0000 - mae: 2463.9822 - val_loss: 2326.0432 - val_mse: 32455778.0000 - val_mae: 2326.0435\n",
            "Epoch 182/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2472.9039 - mse: 36793332.0000 - mae: 2472.9041 - val_loss: 2326.1002 - val_mse: 32216078.0000 - val_mae: 2326.1003\n",
            "Epoch 183/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2468.3009 - mse: 36648708.0000 - mae: 2468.3008 - val_loss: 2322.6570 - val_mse: 32268318.0000 - val_mae: 2322.6572\n",
            "Epoch 184/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2456.3916 - mse: 36801816.0000 - mae: 2456.3916 - val_loss: 2321.4809 - val_mse: 32269038.0000 - val_mae: 2321.4807\n",
            "Epoch 185/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2461.8116 - mse: 36593780.0000 - mae: 2461.8115 - val_loss: 2320.1647 - val_mse: 32265136.0000 - val_mae: 2320.1646\n",
            "Epoch 186/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2463.6698 - mse: 36958136.0000 - mae: 2463.6697 - val_loss: 2322.2154 - val_mse: 32179804.0000 - val_mae: 2322.2153\n",
            "Epoch 187/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2458.5426 - mse: 36191744.0000 - mae: 2458.5425 - val_loss: 2319.5816 - val_mse: 32269834.0000 - val_mae: 2319.5818\n",
            "Epoch 188/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2461.4115 - mse: 36678688.0000 - mae: 2461.4116 - val_loss: 2318.8332 - val_mse: 32260600.0000 - val_mae: 2318.8333\n",
            "Epoch 189/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2476.2817 - mse: 36835444.0000 - mae: 2476.2817 - val_loss: 2319.5709 - val_mse: 32281994.0000 - val_mae: 2319.5710\n",
            "Epoch 190/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2459.0146 - mse: 36615728.0000 - mae: 2459.0146 - val_loss: 2322.8260 - val_mse: 32339190.0000 - val_mae: 2322.8259\n",
            "Epoch 191/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2469.4003 - mse: 36500384.0000 - mae: 2469.4001 - val_loss: 2316.4635 - val_mse: 32185376.0000 - val_mae: 2316.4634\n",
            "Epoch 192/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2456.2758 - mse: 36520512.0000 - mae: 2456.2756 - val_loss: 2321.5530 - val_mse: 32321576.0000 - val_mae: 2321.5530\n",
            "Epoch 193/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2461.3468 - mse: 36671036.0000 - mae: 2461.3467 - val_loss: 2313.8504 - val_mse: 32100326.0000 - val_mae: 2313.8503\n",
            "Epoch 194/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2460.7625 - mse: 36594832.0000 - mae: 2460.7625 - val_loss: 2313.9302 - val_mse: 32126544.0000 - val_mae: 2313.9304\n",
            "Epoch 195/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2455.0538 - mse: 36395680.0000 - mae: 2455.0537 - val_loss: 2313.2641 - val_mse: 32120316.0000 - val_mae: 2313.2642\n",
            "Epoch 196/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2452.9469 - mse: 36462684.0000 - mae: 2452.9470 - val_loss: 2327.4301 - val_mse: 32342674.0000 - val_mae: 2327.4302\n",
            "Epoch 197/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2465.9821 - mse: 36664224.0000 - mae: 2465.9819 - val_loss: 2310.3932 - val_mse: 31989600.0000 - val_mae: 2310.3933\n",
            "Epoch 198/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2452.8985 - mse: 36491812.0000 - mae: 2452.8984 - val_loss: 2314.8704 - val_mse: 31781116.0000 - val_mae: 2314.8704\n",
            "Epoch 199/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2456.5413 - mse: 36183360.0000 - mae: 2456.5413 - val_loss: 2309.7014 - val_mse: 32009300.0000 - val_mae: 2309.7017\n",
            "Epoch 200/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2446.8257 - mse: 36290336.0000 - mae: 2446.8257 - val_loss: 2312.7401 - val_mse: 32082026.0000 - val_mae: 2312.7400\n",
            "Epoch 201/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2444.9956 - mse: 36347884.0000 - mae: 2444.9956 - val_loss: 2307.4030 - val_mse: 31937700.0000 - val_mae: 2307.4028\n",
            "Epoch 202/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2443.6437 - mse: 36234936.0000 - mae: 2443.6438 - val_loss: 2307.3724 - val_mse: 31919956.0000 - val_mae: 2307.3726\n",
            "Epoch 203/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2452.9888 - mse: 36285268.0000 - mae: 2452.9890 - val_loss: 2317.4862 - val_mse: 32089856.0000 - val_mae: 2317.4861\n",
            "Epoch 204/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2444.8221 - mse: 36342940.0000 - mae: 2444.8220 - val_loss: 2311.0598 - val_mse: 31993352.0000 - val_mae: 2311.0598\n",
            "Epoch 205/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2462.9040 - mse: 36469096.0000 - mae: 2462.9041 - val_loss: 2309.2882 - val_mse: 31962386.0000 - val_mae: 2309.2881\n",
            "Epoch 206/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2437.0551 - mse: 36290472.0000 - mae: 2437.0554 - val_loss: 2308.6514 - val_mse: 31940082.0000 - val_mae: 2308.6516\n",
            "Epoch 207/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2447.2165 - mse: 36093464.0000 - mae: 2447.2166 - val_loss: 2305.6784 - val_mse: 31862582.0000 - val_mae: 2305.6785\n",
            "Epoch 208/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2446.4649 - mse: 36177132.0000 - mae: 2446.4651 - val_loss: 2302.1521 - val_mse: 31753366.0000 - val_mae: 2302.1523\n",
            "Epoch 209/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2455.5742 - mse: 36301240.0000 - mae: 2455.5742 - val_loss: 2300.7116 - val_mse: 31609614.0000 - val_mae: 2300.7117\n",
            "Epoch 210/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2442.6409 - mse: 36006492.0000 - mae: 2442.6409 - val_loss: 2301.7925 - val_mse: 31728318.0000 - val_mae: 2301.7927\n",
            "Epoch 211/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2444.0829 - mse: 36045244.0000 - mae: 2444.0830 - val_loss: 2300.4687 - val_mse: 31667006.0000 - val_mae: 2300.4685\n",
            "Epoch 212/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2448.5828 - mse: 36277152.0000 - mae: 2448.5828 - val_loss: 2298.6944 - val_mse: 31569214.0000 - val_mae: 2298.6943\n",
            "Epoch 213/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2449.1825 - mse: 35929636.0000 - mae: 2449.1826 - val_loss: 2297.9200 - val_mse: 31531356.0000 - val_mae: 2297.9202\n",
            "Epoch 214/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2442.5818 - mse: 35770432.0000 - mae: 2442.5818 - val_loss: 2303.8372 - val_mse: 31714592.0000 - val_mae: 2303.8372\n",
            "Epoch 215/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2440.8889 - mse: 35951040.0000 - mae: 2440.8889 - val_loss: 2297.6917 - val_mse: 31556728.0000 - val_mae: 2297.6917\n",
            "Epoch 216/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2441.7078 - mse: 35958008.0000 - mae: 2441.7078 - val_loss: 2309.7224 - val_mse: 31765438.0000 - val_mae: 2309.7227\n",
            "Epoch 217/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2434.8840 - mse: 35969512.0000 - mae: 2434.8840 - val_loss: 2309.3008 - val_mse: 31735544.0000 - val_mae: 2309.3008\n",
            "Epoch 218/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2430.3157 - mse: 35842304.0000 - mae: 2430.3157 - val_loss: 2295.6818 - val_mse: 31463778.0000 - val_mae: 2295.6819\n",
            "Epoch 219/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2425.9194 - mse: 35400748.0000 - mae: 2425.9194 - val_loss: 2297.4113 - val_mse: 31510646.0000 - val_mae: 2297.4111\n",
            "Epoch 220/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2447.7358 - mse: 36186420.0000 - mae: 2447.7358 - val_loss: 2298.6462 - val_mse: 31523580.0000 - val_mae: 2298.6462\n",
            "Epoch 221/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2448.7762 - mse: 35941632.0000 - mae: 2448.7761 - val_loss: 2312.9613 - val_mse: 31696160.0000 - val_mae: 2312.9612\n",
            "Epoch 222/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2436.8220 - mse: 35921336.0000 - mae: 2436.8220 - val_loss: 2304.9861 - val_mse: 31596380.0000 - val_mae: 2304.9863\n",
            "Epoch 223/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2443.4275 - mse: 36112272.0000 - mae: 2443.4275 - val_loss: 2294.5353 - val_mse: 31389308.0000 - val_mae: 2294.5352\n",
            "Epoch 224/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2442.5015 - mse: 35906840.0000 - mae: 2442.5015 - val_loss: 2295.7237 - val_mse: 31424538.0000 - val_mae: 2295.7239\n",
            "Epoch 225/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2433.7244 - mse: 35795056.0000 - mae: 2433.7244 - val_loss: 2299.5141 - val_mse: 31479460.0000 - val_mae: 2299.5139\n",
            "Epoch 226/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2424.7182 - mse: 35775064.0000 - mae: 2424.7183 - val_loss: 2308.9605 - val_mse: 31571698.0000 - val_mae: 2308.9604\n",
            "Epoch 227/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2432.7576 - mse: 35743924.0000 - mae: 2432.7576 - val_loss: 2294.4533 - val_mse: 31351296.0000 - val_mae: 2294.4534\n",
            "Epoch 228/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2440.9093 - mse: 35709192.0000 - mae: 2440.9092 - val_loss: 2300.5388 - val_mse: 31452336.0000 - val_mae: 2300.5388\n",
            "Epoch 229/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2416.9424 - mse: 35332128.0000 - mae: 2416.9424 - val_loss: 2314.8786 - val_mse: 31572298.0000 - val_mae: 2314.8789\n",
            "Epoch 230/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2428.2363 - mse: 35690868.0000 - mae: 2428.2363 - val_loss: 2298.4393 - val_mse: 31382766.0000 - val_mae: 2298.4392\n",
            "Epoch 231/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2438.6264 - mse: 35763480.0000 - mae: 2438.6265 - val_loss: 2291.7339 - val_mse: 31239106.0000 - val_mae: 2291.7339\n",
            "Epoch 232/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2443.1451 - mse: 35485484.0000 - mae: 2443.1450 - val_loss: 2292.1544 - val_mse: 31240934.0000 - val_mae: 2292.1545\n",
            "Epoch 233/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2433.7699 - mse: 35568064.0000 - mae: 2433.7700 - val_loss: 2313.0508 - val_mse: 31481458.0000 - val_mae: 2313.0508\n",
            "Epoch 234/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2428.7681 - mse: 35499184.0000 - mae: 2428.7681 - val_loss: 2314.1124 - val_mse: 31473824.0000 - val_mae: 2314.1123\n",
            "Epoch 235/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2424.4860 - mse: 35322664.0000 - mae: 2424.4861 - val_loss: 2291.2527 - val_mse: 31179366.0000 - val_mae: 2291.2529\n",
            "Epoch 236/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2413.3028 - mse: 35055288.0000 - mae: 2413.3030 - val_loss: 2288.8566 - val_mse: 30956720.0000 - val_mae: 2288.8567\n",
            "Epoch 237/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2416.2958 - mse: 35112828.0000 - mae: 2416.2959 - val_loss: 2292.7098 - val_mse: 31179740.0000 - val_mae: 2292.7097\n",
            "Epoch 238/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2409.4711 - mse: 35102168.0000 - mae: 2409.4709 - val_loss: 2301.0136 - val_mse: 31279374.0000 - val_mae: 2301.0139\n",
            "Epoch 239/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2430.2844 - mse: 35608792.0000 - mae: 2430.2844 - val_loss: 2303.4211 - val_mse: 31301928.0000 - val_mae: 2303.4209\n",
            "Epoch 240/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2413.7445 - mse: 35209332.0000 - mae: 2413.7444 - val_loss: 2309.4499 - val_mse: 31348092.0000 - val_mae: 2309.4497\n",
            "Epoch 241/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2423.9138 - mse: 35370920.0000 - mae: 2423.9138 - val_loss: 2311.5414 - val_mse: 31351010.0000 - val_mae: 2311.5413\n",
            "Epoch 242/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2422.5786 - mse: 35222328.0000 - mae: 2422.5786 - val_loss: 2312.6081 - val_mse: 31353798.0000 - val_mae: 2312.6082\n",
            "Epoch 243/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2423.4100 - mse: 35381556.0000 - mae: 2423.4099 - val_loss: 2294.1938 - val_mse: 31152124.0000 - val_mae: 2294.1938\n",
            "Epoch 244/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2411.6465 - mse: 35145244.0000 - mae: 2411.6467 - val_loss: 2293.8179 - val_mse: 31140356.0000 - val_mae: 2293.8179\n",
            "Epoch 245/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2428.3817 - mse: 35195124.0000 - mae: 2428.3816 - val_loss: 2294.3907 - val_mse: 31128774.0000 - val_mae: 2294.3909\n",
            "Epoch 246/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2413.7003 - mse: 35078024.0000 - mae: 2413.7002 - val_loss: 2286.8849 - val_mse: 30979872.0000 - val_mae: 2286.8850\n",
            "Epoch 247/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2420.2783 - mse: 35285880.0000 - mae: 2420.2783 - val_loss: 2301.8915 - val_mse: 31186506.0000 - val_mae: 2301.8914\n",
            "Epoch 248/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2424.9239 - mse: 35128544.0000 - mae: 2424.9238 - val_loss: 2292.6444 - val_mse: 31047186.0000 - val_mae: 2292.6445\n",
            "Epoch 249/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2412.5578 - mse: 34884860.0000 - mae: 2412.5579 - val_loss: 2282.9083 - val_mse: 30777354.0000 - val_mae: 2282.9082\n",
            "Epoch 250/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2423.9344 - mse: 34689968.0000 - mae: 2423.9346 - val_loss: 2304.5875 - val_mse: 31139734.0000 - val_mae: 2304.5874\n",
            "Epoch 251/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2422.2890 - mse: 35120364.0000 - mae: 2422.2888 - val_loss: 2309.7787 - val_mse: 31170882.0000 - val_mae: 2309.7788\n",
            "Epoch 252/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2410.8173 - mse: 35160564.0000 - mae: 2410.8171 - val_loss: 2307.1964 - val_mse: 31130924.0000 - val_mae: 2307.1965\n",
            "Epoch 253/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2415.0452 - mse: 34892356.0000 - mae: 2415.0452 - val_loss: 2288.5619 - val_mse: 30911540.0000 - val_mae: 2288.5618\n",
            "Epoch 254/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2409.1282 - mse: 34679552.0000 - mae: 2409.1282 - val_loss: 2295.7847 - val_mse: 30979658.0000 - val_mae: 2295.7847\n",
            "Epoch 255/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2411.0397 - mse: 35007148.0000 - mae: 2411.0398 - val_loss: 2313.5575 - val_mse: 31131018.0000 - val_mae: 2313.5579\n",
            "Epoch 256/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2405.9327 - mse: 34868888.0000 - mae: 2405.9326 - val_loss: 2285.0633 - val_mse: 30830296.0000 - val_mae: 2285.0635\n",
            "Epoch 257/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2419.1029 - mse: 34969820.0000 - mae: 2419.1028 - val_loss: 2305.3641 - val_mse: 31062026.0000 - val_mae: 2305.3640\n",
            "Epoch 258/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2412.9970 - mse: 34555492.0000 - mae: 2412.9968 - val_loss: 2297.4118 - val_mse: 30961376.0000 - val_mae: 2297.4119\n",
            "Epoch 259/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2407.9071 - mse: 34706012.0000 - mae: 2407.9070 - val_loss: 2322.9249 - val_mse: 31169990.0000 - val_mae: 2322.9248\n",
            "Epoch 260/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2421.3795 - mse: 35309688.0000 - mae: 2421.3794 - val_loss: 2304.1739 - val_mse: 31014224.0000 - val_mae: 2304.1738\n",
            "Epoch 261/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2427.4174 - mse: 35093272.0000 - mae: 2427.4175 - val_loss: 2295.5127 - val_mse: 30927624.0000 - val_mae: 2295.5127\n",
            "Epoch 262/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2409.2247 - mse: 34691692.0000 - mae: 2409.2249 - val_loss: 2315.2967 - val_mse: 31042334.0000 - val_mae: 2315.2966\n",
            "Epoch 263/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2408.3315 - mse: 34920968.0000 - mae: 2408.3315 - val_loss: 2294.8078 - val_mse: 30859158.0000 - val_mae: 2294.8081\n",
            "Epoch 264/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2410.6955 - mse: 34947736.0000 - mae: 2410.6953 - val_loss: 2285.8341 - val_mse: 30759962.0000 - val_mae: 2285.8340\n",
            "Epoch 265/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2406.7746 - mse: 34576536.0000 - mae: 2406.7747 - val_loss: 2306.8732 - val_mse: 30966666.0000 - val_mae: 2306.8733\n",
            "Epoch 266/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2404.5963 - mse: 34880944.0000 - mae: 2404.5962 - val_loss: 2285.5209 - val_mse: 30732430.0000 - val_mae: 2285.5208\n",
            "Epoch 267/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2399.0702 - mse: 34661260.0000 - mae: 2399.0701 - val_loss: 2279.6829 - val_mse: 30614544.0000 - val_mae: 2279.6829\n",
            "Epoch 268/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2424.5856 - mse: 34900636.0000 - mae: 2424.5859 - val_loss: 2287.7347 - val_mse: 30754922.0000 - val_mae: 2287.7346\n",
            "Epoch 269/500\n",
            "1130/1130 [==============================] - 0s 85us/step - loss: 2413.6148 - mse: 34974680.0000 - mae: 2413.6145 - val_loss: 2297.4075 - val_mse: 30850538.0000 - val_mae: 2297.4075\n",
            "Epoch 270/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2409.9635 - mse: 34877044.0000 - mae: 2409.9634 - val_loss: 2287.0529 - val_mse: 30715616.0000 - val_mae: 2287.0527\n",
            "Epoch 271/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2389.9497 - mse: 34004192.0000 - mae: 2389.9497 - val_loss: 2303.0408 - val_mse: 30848186.0000 - val_mae: 2303.0408\n",
            "Epoch 272/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2405.8815 - mse: 34775036.0000 - mae: 2405.8816 - val_loss: 2290.5449 - val_mse: 30718880.0000 - val_mae: 2290.5449\n",
            "Epoch 273/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2409.8733 - mse: 34793332.0000 - mae: 2409.8733 - val_loss: 2278.7278 - val_mse: 30541426.0000 - val_mae: 2278.7280\n",
            "Epoch 274/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2402.5978 - mse: 34525868.0000 - mae: 2402.5979 - val_loss: 2297.0788 - val_mse: 30740590.0000 - val_mae: 2297.0786\n",
            "Epoch 275/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2409.2578 - mse: 34606412.0000 - mae: 2409.2578 - val_loss: 2288.9945 - val_mse: 30654346.0000 - val_mae: 2288.9946\n",
            "Epoch 276/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2414.1566 - mse: 34618444.0000 - mae: 2414.1567 - val_loss: 2295.0238 - val_mse: 30697278.0000 - val_mae: 2295.0237\n",
            "Epoch 277/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2400.7061 - mse: 34416112.0000 - mae: 2400.7063 - val_loss: 2293.1549 - val_mse: 30662318.0000 - val_mae: 2293.1548\n",
            "Epoch 278/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2393.3967 - mse: 34425896.0000 - mae: 2393.3967 - val_loss: 2296.7261 - val_mse: 30689568.0000 - val_mae: 2296.7263\n",
            "Epoch 279/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2408.9665 - mse: 34380276.0000 - mae: 2408.9663 - val_loss: 2287.5062 - val_mse: 30588450.0000 - val_mae: 2287.5061\n",
            "Epoch 280/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2412.2957 - mse: 34405972.0000 - mae: 2412.2957 - val_loss: 2276.5188 - val_mse: 30425128.0000 - val_mae: 2276.5186\n",
            "Epoch 281/500\n",
            "1130/1130 [==============================] - 0s 83us/step - loss: 2392.1092 - mse: 34557924.0000 - mae: 2392.1091 - val_loss: 2314.5528 - val_mse: 30765410.0000 - val_mae: 2314.5527\n",
            "Epoch 282/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2405.9336 - mse: 34525664.0000 - mae: 2405.9336 - val_loss: 2306.2498 - val_mse: 30699560.0000 - val_mae: 2306.2498\n",
            "Epoch 283/500\n",
            "1130/1130 [==============================] - 0s 83us/step - loss: 2395.1041 - mse: 34426352.0000 - mae: 2395.1042 - val_loss: 2298.0107 - val_mse: 30622866.0000 - val_mae: 2298.0105\n",
            "Epoch 284/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2400.2323 - mse: 34317156.0000 - mae: 2400.2324 - val_loss: 2298.8825 - val_mse: 30608858.0000 - val_mae: 2298.8826\n",
            "Epoch 285/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2400.7394 - mse: 34405440.0000 - mae: 2400.7395 - val_loss: 2304.4533 - val_mse: 30657428.0000 - val_mae: 2304.4534\n",
            "Epoch 286/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2401.3083 - mse: 34363180.0000 - mae: 2401.3081 - val_loss: 2290.6853 - val_mse: 30514614.0000 - val_mae: 2290.6855\n",
            "Epoch 287/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2395.7265 - mse: 34231268.0000 - mae: 2395.7266 - val_loss: 2321.5534 - val_mse: 30746854.0000 - val_mae: 2321.5535\n",
            "Epoch 288/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2396.8269 - mse: 34270664.0000 - mae: 2396.8267 - val_loss: 2320.0960 - val_mse: 30739404.0000 - val_mae: 2320.0959\n",
            "Epoch 289/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2401.2276 - mse: 34203016.0000 - mae: 2401.2275 - val_loss: 2305.5958 - val_mse: 30639744.0000 - val_mae: 2305.5955\n",
            "Epoch 290/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2410.8264 - mse: 34557596.0000 - mae: 2410.8264 - val_loss: 2298.5271 - val_mse: 30569008.0000 - val_mae: 2298.5271\n",
            "Epoch 291/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2399.6372 - mse: 34209784.0000 - mae: 2399.6372 - val_loss: 2283.2382 - val_mse: 30428042.0000 - val_mae: 2283.2383\n",
            "Epoch 292/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2382.9148 - mse: 33807084.0000 - mae: 2382.9148 - val_loss: 2299.3956 - val_mse: 30568242.0000 - val_mae: 2299.3955\n",
            "Epoch 293/500\n",
            "1130/1130 [==============================] - 0s 90us/step - loss: 2412.3877 - mse: 34355444.0000 - mae: 2412.3877 - val_loss: 2330.8132 - val_mse: 30789592.0000 - val_mae: 2330.8135\n",
            "Epoch 294/500\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 2405.5981 - mse: 34663980.0000 - mae: 2405.5981 - val_loss: 2280.3623 - val_mse: 30367006.0000 - val_mae: 2280.3625\n",
            "Epoch 295/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2408.2040 - mse: 34283708.0000 - mae: 2408.2041 - val_loss: 2302.1114 - val_mse: 30560126.0000 - val_mae: 2302.1113\n",
            "Epoch 296/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2391.2377 - mse: 34306244.0000 - mae: 2391.2375 - val_loss: 2286.5296 - val_mse: 30428534.0000 - val_mae: 2286.5298\n",
            "Epoch 297/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2396.8077 - mse: 33963832.0000 - mae: 2396.8076 - val_loss: 2275.8297 - val_mse: 30296980.0000 - val_mae: 2275.8296\n",
            "Epoch 298/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2399.2234 - mse: 34216696.0000 - mae: 2399.2234 - val_loss: 2270.7195 - val_mse: 30205366.0000 - val_mae: 2270.7197\n",
            "Epoch 299/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2399.8405 - mse: 33784732.0000 - mae: 2399.8406 - val_loss: 2297.6192 - val_mse: 30493660.0000 - val_mae: 2297.6191\n",
            "Epoch 300/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2380.9977 - mse: 34107400.0000 - mae: 2380.9976 - val_loss: 2296.1435 - val_mse: 30483000.0000 - val_mae: 2296.1436\n",
            "Epoch 301/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2378.1703 - mse: 33951420.0000 - mae: 2378.1704 - val_loss: 2279.2229 - val_mse: 30306632.0000 - val_mae: 2279.2227\n",
            "Epoch 302/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2382.7660 - mse: 33907044.0000 - mae: 2382.7661 - val_loss: 2276.6720 - val_mse: 30247570.0000 - val_mae: 2276.6719\n",
            "Epoch 303/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2406.2425 - mse: 34392856.0000 - mae: 2406.2427 - val_loss: 2285.8804 - val_mse: 30319520.0000 - val_mae: 2285.8804\n",
            "Epoch 304/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2401.8591 - mse: 34240328.0000 - mae: 2401.8589 - val_loss: 2273.1249 - val_mse: 30182412.0000 - val_mae: 2273.1250\n",
            "Epoch 305/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2387.3096 - mse: 33998820.0000 - mae: 2387.3096 - val_loss: 2290.3372 - val_mse: 30348112.0000 - val_mae: 2290.3372\n",
            "Epoch 306/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2394.2772 - mse: 34117340.0000 - mae: 2394.2771 - val_loss: 2286.7313 - val_mse: 30288212.0000 - val_mae: 2286.7312\n",
            "Epoch 307/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2382.9622 - mse: 34007940.0000 - mae: 2382.9622 - val_loss: 2283.9005 - val_mse: 30271694.0000 - val_mae: 2283.9001\n",
            "Epoch 308/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2392.2716 - mse: 33976224.0000 - mae: 2392.2715 - val_loss: 2296.8244 - val_mse: 30359328.0000 - val_mae: 2296.8242\n",
            "Epoch 309/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2376.3310 - mse: 33410082.0000 - mae: 2376.3311 - val_loss: 2299.6118 - val_mse: 30357806.0000 - val_mae: 2299.6116\n",
            "Epoch 310/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2373.0413 - mse: 33450962.0000 - mae: 2373.0413 - val_loss: 2296.7405 - val_mse: 30315402.0000 - val_mae: 2296.7407\n",
            "Epoch 311/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2387.4329 - mse: 33909568.0000 - mae: 2387.4329 - val_loss: 2290.6760 - val_mse: 30247770.0000 - val_mae: 2290.6758\n",
            "Epoch 312/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2376.1163 - mse: 33797936.0000 - mae: 2376.1165 - val_loss: 2305.5182 - val_mse: 30327950.0000 - val_mae: 2305.5183\n",
            "Epoch 313/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2394.2410 - mse: 34253416.0000 - mae: 2394.2412 - val_loss: 2293.2693 - val_mse: 30232288.0000 - val_mae: 2293.2693\n",
            "Epoch 314/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2379.3149 - mse: 33730680.0000 - mae: 2379.3149 - val_loss: 2279.8990 - val_mse: 30100292.0000 - val_mae: 2279.8987\n",
            "Epoch 315/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2409.5271 - mse: 34213256.0000 - mae: 2409.5271 - val_loss: 2291.3998 - val_mse: 30227502.0000 - val_mae: 2291.3997\n",
            "Epoch 316/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2379.5590 - mse: 33627976.0000 - mae: 2379.5591 - val_loss: 2273.7029 - val_mse: 30062646.0000 - val_mae: 2273.7026\n",
            "Epoch 317/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2398.9066 - mse: 34018268.0000 - mae: 2398.9065 - val_loss: 2286.1320 - val_mse: 30175276.0000 - val_mae: 2286.1318\n",
            "Epoch 318/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2373.0001 - mse: 33719684.0000 - mae: 2373.0000 - val_loss: 2286.1636 - val_mse: 30162070.0000 - val_mae: 2286.1633\n",
            "Epoch 319/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2378.8421 - mse: 33713240.0000 - mae: 2378.8420 - val_loss: 2278.7898 - val_mse: 30092432.0000 - val_mae: 2278.7900\n",
            "Epoch 320/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2376.5667 - mse: 33653452.0000 - mae: 2376.5667 - val_loss: 2273.6340 - val_mse: 30005946.0000 - val_mae: 2273.6340\n",
            "Epoch 321/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2372.9876 - mse: 33703684.0000 - mae: 2372.9878 - val_loss: 2269.4555 - val_mse: 29954706.0000 - val_mae: 2269.4553\n",
            "Epoch 322/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2382.5851 - mse: 33666760.0000 - mae: 2382.5852 - val_loss: 2269.8733 - val_mse: 29947656.0000 - val_mae: 2269.8735\n",
            "Epoch 323/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2371.2067 - mse: 33658664.0000 - mae: 2371.2065 - val_loss: 2293.6876 - val_mse: 30141666.0000 - val_mae: 2293.6877\n",
            "Epoch 324/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2364.0168 - mse: 33545888.0000 - mae: 2364.0166 - val_loss: 2299.2001 - val_mse: 30188390.0000 - val_mae: 2299.1997\n",
            "Epoch 325/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2384.5225 - mse: 34037476.0000 - mae: 2384.5225 - val_loss: 2292.6959 - val_mse: 30137954.0000 - val_mae: 2292.6960\n",
            "Epoch 326/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2381.8076 - mse: 33746476.0000 - mae: 2381.8076 - val_loss: 2267.4014 - val_mse: 29895872.0000 - val_mae: 2267.4014\n",
            "Epoch 327/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2379.9215 - mse: 33591928.0000 - mae: 2379.9216 - val_loss: 2274.7242 - val_mse: 29954724.0000 - val_mae: 2274.7241\n",
            "Epoch 328/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2391.0177 - mse: 33648176.0000 - mae: 2391.0176 - val_loss: 2311.1263 - val_mse: 30265800.0000 - val_mae: 2311.1262\n",
            "Epoch 329/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2381.0819 - mse: 33932420.0000 - mae: 2381.0818 - val_loss: 2264.2500 - val_mse: 29858038.0000 - val_mae: 2264.2500\n",
            "Epoch 330/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2398.6239 - mse: 33990520.0000 - mae: 2398.6238 - val_loss: 2275.4420 - val_mse: 29987556.0000 - val_mae: 2275.4419\n",
            "Epoch 331/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2380.9949 - mse: 33823560.0000 - mae: 2380.9949 - val_loss: 2285.0398 - val_mse: 30069534.0000 - val_mae: 2285.0398\n",
            "Epoch 332/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2370.2741 - mse: 33648204.0000 - mae: 2370.2744 - val_loss: 2293.2939 - val_mse: 30092926.0000 - val_mae: 2293.2937\n",
            "Epoch 333/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2383.1617 - mse: 33959044.0000 - mae: 2383.1616 - val_loss: 2292.6101 - val_mse: 30077308.0000 - val_mae: 2292.6101\n",
            "Epoch 334/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2391.1395 - mse: 33944572.0000 - mae: 2391.1394 - val_loss: 2284.9079 - val_mse: 30006148.0000 - val_mae: 2284.9077\n",
            "Epoch 335/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2396.7343 - mse: 34129672.0000 - mae: 2396.7344 - val_loss: 2263.4928 - val_mse: 29807796.0000 - val_mae: 2263.4932\n",
            "Epoch 336/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2378.8927 - mse: 33411616.0000 - mae: 2378.8926 - val_loss: 2284.3580 - val_mse: 29994930.0000 - val_mae: 2284.3582\n",
            "Epoch 337/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2401.9185 - mse: 33944864.0000 - mae: 2401.9187 - val_loss: 2289.2807 - val_mse: 30029682.0000 - val_mae: 2289.2805\n",
            "Epoch 338/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2384.0253 - mse: 33401778.0000 - mae: 2384.0251 - val_loss: 2286.2314 - val_mse: 30013004.0000 - val_mae: 2286.2314\n",
            "Epoch 339/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2378.1044 - mse: 33484714.0000 - mae: 2378.1045 - val_loss: 2315.7946 - val_mse: 30224030.0000 - val_mae: 2315.7944\n",
            "Epoch 340/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2370.7715 - mse: 33436786.0000 - mae: 2370.7715 - val_loss: 2281.7850 - val_mse: 29960866.0000 - val_mae: 2281.7852\n",
            "Epoch 341/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2383.2506 - mse: 33437884.0000 - mae: 2383.2505 - val_loss: 2288.6438 - val_mse: 30020668.0000 - val_mae: 2288.6438\n",
            "Epoch 342/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2362.8490 - mse: 33460612.0000 - mae: 2362.8489 - val_loss: 2283.7642 - val_mse: 29962990.0000 - val_mae: 2283.7644\n",
            "Epoch 343/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2374.6488 - mse: 33251666.0000 - mae: 2374.6487 - val_loss: 2277.2575 - val_mse: 29884572.0000 - val_mae: 2277.2573\n",
            "Epoch 344/500\n",
            "1130/1130 [==============================] - 0s 85us/step - loss: 2374.9307 - mse: 33519728.0000 - mae: 2374.9304 - val_loss: 2265.3563 - val_mse: 29751988.0000 - val_mae: 2265.3560\n",
            "Epoch 345/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2391.6657 - mse: 33765300.0000 - mae: 2391.6658 - val_loss: 2272.3470 - val_mse: 29818732.0000 - val_mae: 2272.3469\n",
            "Epoch 346/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2369.9091 - mse: 33401300.0000 - mae: 2369.9092 - val_loss: 2281.3929 - val_mse: 29915586.0000 - val_mae: 2281.3926\n",
            "Epoch 347/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2374.5351 - mse: 33428142.0000 - mae: 2374.5349 - val_loss: 2267.1763 - val_mse: 29796362.0000 - val_mae: 2267.1763\n",
            "Epoch 348/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2407.3137 - mse: 34132448.0000 - mae: 2407.3135 - val_loss: 2298.5787 - val_mse: 30049168.0000 - val_mae: 2298.5789\n",
            "Epoch 349/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2387.0199 - mse: 33926020.0000 - mae: 2387.0200 - val_loss: 2260.1419 - val_mse: 29729686.0000 - val_mae: 2260.1418\n",
            "Epoch 350/500\n",
            "1130/1130 [==============================] - 0s 86us/step - loss: 2381.6088 - mse: 33273142.0000 - mae: 2381.6086 - val_loss: 2303.7311 - val_mse: 30072218.0000 - val_mae: 2303.7310\n",
            "Epoch 351/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2396.3674 - mse: 34084732.0000 - mae: 2396.3674 - val_loss: 2283.1746 - val_mse: 29906172.0000 - val_mae: 2283.1743\n",
            "Epoch 352/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2377.4302 - mse: 33556236.0000 - mae: 2377.4302 - val_loss: 2278.6141 - val_mse: 29870272.0000 - val_mae: 2278.6138\n",
            "Epoch 353/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2367.4231 - mse: 33121778.0000 - mae: 2367.4231 - val_loss: 2306.1863 - val_mse: 30077456.0000 - val_mae: 2306.1863\n",
            "Epoch 354/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2369.5715 - mse: 33170670.0000 - mae: 2369.5715 - val_loss: 2286.5001 - val_mse: 29915222.0000 - val_mae: 2286.5000\n",
            "Epoch 355/500\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 2358.9352 - mse: 33330030.0000 - mae: 2358.9351 - val_loss: 2293.8781 - val_mse: 29964702.0000 - val_mae: 2293.8782\n",
            "Epoch 356/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2371.8893 - mse: 33567660.0000 - mae: 2371.8894 - val_loss: 2285.9623 - val_mse: 29888052.0000 - val_mae: 2285.9622\n",
            "Epoch 357/500\n",
            "1130/1130 [==============================] - 0s 85us/step - loss: 2373.4478 - mse: 33570208.0000 - mae: 2373.4478 - val_loss: 2292.5616 - val_mse: 29922022.0000 - val_mae: 2292.5615\n",
            "Epoch 358/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2378.9756 - mse: 33280334.0000 - mae: 2378.9756 - val_loss: 2294.4587 - val_mse: 29945024.0000 - val_mae: 2294.4590\n",
            "Epoch 359/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2371.1290 - mse: 33135568.0000 - mae: 2371.1289 - val_loss: 2291.3098 - val_mse: 29919850.0000 - val_mae: 2291.3098\n",
            "Epoch 360/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2362.7512 - mse: 33286832.0000 - mae: 2362.7512 - val_loss: 2264.0234 - val_mse: 29679268.0000 - val_mae: 2264.0232\n",
            "Epoch 361/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2381.7676 - mse: 33579800.0000 - mae: 2381.7676 - val_loss: 2280.0839 - val_mse: 29796220.0000 - val_mae: 2280.0840\n",
            "Epoch 362/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2365.0063 - mse: 33323178.0000 - mae: 2365.0063 - val_loss: 2266.4300 - val_mse: 29691506.0000 - val_mae: 2266.4302\n",
            "Epoch 363/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2378.6697 - mse: 33077360.0000 - mae: 2378.6697 - val_loss: 2255.6640 - val_mse: 29593036.0000 - val_mae: 2255.6641\n",
            "Epoch 364/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2363.5124 - mse: 32802460.0000 - mae: 2363.5127 - val_loss: 2305.8115 - val_mse: 29977506.0000 - val_mae: 2305.8115\n",
            "Epoch 365/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2360.8001 - mse: 33312874.0000 - mae: 2360.8003 - val_loss: 2267.0944 - val_mse: 29645856.0000 - val_mae: 2267.0947\n",
            "Epoch 366/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2381.9761 - mse: 33499422.0000 - mae: 2381.9761 - val_loss: 2285.8992 - val_mse: 29794086.0000 - val_mae: 2285.8992\n",
            "Epoch 367/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2370.8787 - mse: 33248820.0000 - mae: 2370.8784 - val_loss: 2261.9753 - val_mse: 29601502.0000 - val_mae: 2261.9753\n",
            "Epoch 368/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2381.4874 - mse: 33204648.0000 - mae: 2381.4875 - val_loss: 2279.9737 - val_mse: 29734128.0000 - val_mae: 2279.9736\n",
            "Epoch 369/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2368.6124 - mse: 32778146.0000 - mae: 2368.6123 - val_loss: 2287.6502 - val_mse: 29790612.0000 - val_mae: 2287.6501\n",
            "Epoch 370/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2349.0039 - mse: 33001522.0000 - mae: 2349.0039 - val_loss: 2272.1378 - val_mse: 29678934.0000 - val_mae: 2272.1377\n",
            "Epoch 371/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2370.2709 - mse: 32995694.0000 - mae: 2370.2710 - val_loss: 2253.8473 - val_mse: 29526158.0000 - val_mae: 2253.8474\n",
            "Epoch 372/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2366.5694 - mse: 33304514.0000 - mae: 2366.5693 - val_loss: 2264.0412 - val_mse: 29611050.0000 - val_mae: 2264.0410\n",
            "Epoch 373/500\n",
            "1130/1130 [==============================] - 0s 112us/step - loss: 2379.3533 - mse: 33397442.0000 - mae: 2379.3533 - val_loss: 2282.2180 - val_mse: 29755602.0000 - val_mae: 2282.2178\n",
            "Epoch 374/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2363.0620 - mse: 33147370.0000 - mae: 2363.0620 - val_loss: 2275.5800 - val_mse: 29704052.0000 - val_mae: 2275.5798\n",
            "Epoch 375/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2351.0092 - mse: 33057628.0000 - mae: 2351.0090 - val_loss: 2271.9893 - val_mse: 29660492.0000 - val_mae: 2271.9890\n",
            "Epoch 376/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2364.9046 - mse: 32978738.0000 - mae: 2364.9045 - val_loss: 2261.7337 - val_mse: 29560696.0000 - val_mae: 2261.7336\n",
            "Epoch 377/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2369.0627 - mse: 33177238.0000 - mae: 2369.0625 - val_loss: 2276.9025 - val_mse: 29682368.0000 - val_mae: 2276.9023\n",
            "Epoch 378/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2364.9971 - mse: 33111720.0000 - mae: 2364.9971 - val_loss: 2291.7484 - val_mse: 29795002.0000 - val_mae: 2291.7488\n",
            "Epoch 379/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2359.9637 - mse: 33058634.0000 - mae: 2359.9636 - val_loss: 2260.9738 - val_mse: 29549942.0000 - val_mae: 2260.9736\n",
            "Epoch 380/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2364.1757 - mse: 33314102.0000 - mae: 2364.1758 - val_loss: 2253.4177 - val_mse: 29443812.0000 - val_mae: 2253.4180\n",
            "Epoch 381/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2368.0216 - mse: 32674270.0000 - mae: 2368.0217 - val_loss: 2271.1442 - val_mse: 29590326.0000 - val_mae: 2271.1443\n",
            "Epoch 382/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2361.1814 - mse: 33100516.0000 - mae: 2361.1814 - val_loss: 2262.9202 - val_mse: 29509944.0000 - val_mae: 2262.9204\n",
            "Epoch 383/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2369.9256 - mse: 32743268.0000 - mae: 2369.9258 - val_loss: 2277.3982 - val_mse: 29619104.0000 - val_mae: 2277.3979\n",
            "Epoch 384/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2357.9510 - mse: 32845270.0000 - mae: 2357.9512 - val_loss: 2262.2472 - val_mse: 29477692.0000 - val_mae: 2262.2473\n",
            "Epoch 385/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2370.4922 - mse: 33214032.0000 - mae: 2370.4922 - val_loss: 2263.1055 - val_mse: 29474838.0000 - val_mae: 2263.1055\n",
            "Epoch 386/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2364.8554 - mse: 32995314.0000 - mae: 2364.8552 - val_loss: 2270.4453 - val_mse: 29522890.0000 - val_mae: 2270.4453\n",
            "Epoch 387/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2365.7371 - mse: 33248516.0000 - mae: 2365.7371 - val_loss: 2258.0991 - val_mse: 29400474.0000 - val_mae: 2258.0989\n",
            "Epoch 388/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2376.4676 - mse: 32937992.0000 - mae: 2376.4675 - val_loss: 2295.9293 - val_mse: 29697866.0000 - val_mae: 2295.9292\n",
            "Epoch 389/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2353.0750 - mse: 32939492.0000 - mae: 2353.0750 - val_loss: 2254.2336 - val_mse: 29370640.0000 - val_mae: 2254.2334\n",
            "Epoch 390/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2369.3252 - mse: 32965206.0000 - mae: 2369.3252 - val_loss: 2273.2226 - val_mse: 29549016.0000 - val_mae: 2273.2224\n",
            "Epoch 391/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2372.8352 - mse: 33283160.0000 - mae: 2372.8352 - val_loss: 2266.0107 - val_mse: 29480712.0000 - val_mae: 2266.0107\n",
            "Epoch 392/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2350.1897 - mse: 32772472.0000 - mae: 2350.1899 - val_loss: 2257.2468 - val_mse: 29398908.0000 - val_mae: 2257.2471\n",
            "Epoch 393/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2359.5807 - mse: 32808206.0000 - mae: 2359.5808 - val_loss: 2269.7063 - val_mse: 29471790.0000 - val_mae: 2269.7063\n",
            "Epoch 394/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2365.0841 - mse: 32871568.0000 - mae: 2365.0840 - val_loss: 2280.0314 - val_mse: 29542174.0000 - val_mae: 2280.0315\n",
            "Epoch 395/500\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 2354.3779 - mse: 33046572.0000 - mae: 2354.3782 - val_loss: 2249.7511 - val_mse: 29294936.0000 - val_mae: 2249.7512\n",
            "Epoch 396/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2356.8049 - mse: 33054576.0000 - mae: 2356.8049 - val_loss: 2259.6420 - val_mse: 29348974.0000 - val_mae: 2259.6421\n",
            "Epoch 397/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2355.9276 - mse: 32736232.0000 - mae: 2355.9275 - val_loss: 2265.9322 - val_mse: 29420238.0000 - val_mae: 2265.9321\n",
            "Epoch 398/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2354.7691 - mse: 32820596.0000 - mae: 2354.7690 - val_loss: 2270.8310 - val_mse: 29464700.0000 - val_mae: 2270.8311\n",
            "Epoch 399/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2365.9213 - mse: 32969820.0000 - mae: 2365.9211 - val_loss: 2279.0804 - val_mse: 29513286.0000 - val_mae: 2279.0806\n",
            "Epoch 400/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2365.7389 - mse: 33167526.0000 - mae: 2365.7390 - val_loss: 2269.7176 - val_mse: 29457530.0000 - val_mae: 2269.7175\n",
            "Epoch 401/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2346.9894 - mse: 32585870.0000 - mae: 2346.9895 - val_loss: 2256.6223 - val_mse: 29330040.0000 - val_mae: 2256.6221\n",
            "Epoch 402/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2363.6973 - mse: 32559068.0000 - mae: 2363.6973 - val_loss: 2283.5983 - val_mse: 29524232.0000 - val_mae: 2283.5984\n",
            "Epoch 403/500\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 2376.2553 - mse: 33000142.0000 - mae: 2376.2554 - val_loss: 2249.3860 - val_mse: 29240460.0000 - val_mae: 2249.3860\n",
            "Epoch 404/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2362.6018 - mse: 32881118.0000 - mae: 2362.6021 - val_loss: 2281.9950 - val_mse: 29480484.0000 - val_mae: 2281.9951\n",
            "Epoch 405/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2346.3123 - mse: 32718518.0000 - mae: 2346.3125 - val_loss: 2265.6631 - val_mse: 29346414.0000 - val_mae: 2265.6631\n",
            "Epoch 406/500\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 2348.6048 - mse: 32316298.0000 - mae: 2348.6047 - val_loss: 2256.1339 - val_mse: 29254348.0000 - val_mae: 2256.1338\n",
            "Epoch 407/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2335.3878 - mse: 32119256.0000 - mae: 2335.3879 - val_loss: 2257.4204 - val_mse: 29254070.0000 - val_mae: 2257.4207\n",
            "Epoch 408/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2352.3541 - mse: 32590632.0000 - mae: 2352.3542 - val_loss: 2255.8734 - val_mse: 29205982.0000 - val_mae: 2255.8735\n",
            "Epoch 409/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2357.9499 - mse: 32320270.0000 - mae: 2357.9500 - val_loss: 2263.3193 - val_mse: 29245032.0000 - val_mae: 2263.3193\n",
            "Epoch 410/500\n",
            "1130/1130 [==============================] - 0s 83us/step - loss: 2342.6745 - mse: 32194042.0000 - mae: 2342.6746 - val_loss: 2262.7261 - val_mse: 29229948.0000 - val_mae: 2262.7261\n",
            "Epoch 411/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2355.1415 - mse: 32302198.0000 - mae: 2355.1414 - val_loss: 2262.2410 - val_mse: 29191458.0000 - val_mae: 2262.2412\n",
            "Epoch 412/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2358.4058 - mse: 32426006.0000 - mae: 2358.4058 - val_loss: 2270.4372 - val_mse: 29268006.0000 - val_mae: 2270.4373\n",
            "Epoch 413/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2352.7322 - mse: 32290120.0000 - mae: 2352.7322 - val_loss: 2279.9356 - val_mse: 29341750.0000 - val_mae: 2279.9358\n",
            "Epoch 414/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2344.3469 - mse: 32644764.0000 - mae: 2344.3469 - val_loss: 2246.0350 - val_mse: 29083618.0000 - val_mae: 2246.0352\n",
            "Epoch 415/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2351.4264 - mse: 32378224.0000 - mae: 2351.4260 - val_loss: 2267.9218 - val_mse: 29271450.0000 - val_mae: 2267.9216\n",
            "Epoch 416/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2363.7556 - mse: 32956586.0000 - mae: 2363.7559 - val_loss: 2247.5187 - val_mse: 29112638.0000 - val_mae: 2247.5188\n",
            "Epoch 417/500\n",
            "1130/1130 [==============================] - 0s 91us/step - loss: 2343.2892 - mse: 32219882.0000 - mae: 2343.2891 - val_loss: 2273.2174 - val_mse: 29332864.0000 - val_mae: 2273.2175\n",
            "Epoch 418/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2343.0572 - mse: 32551608.0000 - mae: 2343.0574 - val_loss: 2273.6196 - val_mse: 29324070.0000 - val_mae: 2273.6199\n",
            "Epoch 419/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2351.1023 - mse: 32797400.0000 - mae: 2351.1023 - val_loss: 2261.1795 - val_mse: 29213936.0000 - val_mae: 2261.1794\n",
            "Epoch 420/500\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 2372.8654 - mse: 32837410.0000 - mae: 2372.8652 - val_loss: 2261.2554 - val_mse: 29197070.0000 - val_mae: 2261.2551\n",
            "Epoch 421/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2335.9082 - mse: 32331156.0000 - mae: 2335.9082 - val_loss: 2261.9052 - val_mse: 29182304.0000 - val_mae: 2261.9053\n",
            "Epoch 422/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2357.7162 - mse: 32781890.0000 - mae: 2357.7161 - val_loss: 2235.8743 - val_mse: 28973102.0000 - val_mae: 2235.8745\n",
            "Epoch 423/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2358.5668 - mse: 32642332.0000 - mae: 2358.5669 - val_loss: 2261.4224 - val_mse: 29189430.0000 - val_mae: 2261.4224\n",
            "Epoch 424/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2333.7346 - mse: 32194690.0000 - mae: 2333.7346 - val_loss: 2261.7984 - val_mse: 29173372.0000 - val_mae: 2261.7986\n",
            "Epoch 425/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2363.3980 - mse: 33226640.0000 - mae: 2363.3979 - val_loss: 2257.2631 - val_mse: 29135144.0000 - val_mae: 2257.2632\n",
            "Epoch 426/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2345.5124 - mse: 32328188.0000 - mae: 2345.5125 - val_loss: 2244.6220 - val_mse: 29023900.0000 - val_mae: 2244.6218\n",
            "Epoch 427/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2352.1044 - mse: 32759120.0000 - mae: 2352.1042 - val_loss: 2229.6896 - val_mse: 28879926.0000 - val_mae: 2229.6897\n",
            "Epoch 428/500\n",
            "1130/1130 [==============================] - 0s 88us/step - loss: 2340.2162 - mse: 32059508.0000 - mae: 2340.2161 - val_loss: 2256.0861 - val_mse: 29082634.0000 - val_mae: 2256.0862\n",
            "Epoch 429/500\n",
            "1130/1130 [==============================] - 0s 90us/step - loss: 2354.0654 - mse: 32430540.0000 - mae: 2354.0652 - val_loss: 2273.1122 - val_mse: 29213594.0000 - val_mae: 2273.1123\n",
            "Epoch 430/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2341.5458 - mse: 32404428.0000 - mae: 2341.5459 - val_loss: 2244.4346 - val_mse: 28978092.0000 - val_mae: 2244.4346\n",
            "Epoch 431/500\n",
            "1130/1130 [==============================] - 0s 88us/step - loss: 2343.0964 - mse: 32289598.0000 - mae: 2343.0964 - val_loss: 2262.7985 - val_mse: 29089436.0000 - val_mae: 2262.7983\n",
            "Epoch 432/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2350.6344 - mse: 32115130.0000 - mae: 2350.6345 - val_loss: 2267.6573 - val_mse: 29100934.0000 - val_mae: 2267.6575\n",
            "Epoch 433/500\n",
            "1130/1130 [==============================] - 0s 88us/step - loss: 2349.6690 - mse: 32267044.0000 - mae: 2349.6689 - val_loss: 2267.7204 - val_mse: 29099368.0000 - val_mae: 2267.7205\n",
            "Epoch 434/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2358.6310 - mse: 32397668.0000 - mae: 2358.6309 - val_loss: 2234.6039 - val_mse: 28827116.0000 - val_mae: 2234.6040\n",
            "Epoch 435/500\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 2355.1248 - mse: 32120614.0000 - mae: 2355.1248 - val_loss: 2248.6869 - val_mse: 28937390.0000 - val_mae: 2248.6870\n",
            "Epoch 436/500\n",
            "1130/1130 [==============================] - 0s 88us/step - loss: 2372.5167 - mse: 32617134.0000 - mae: 2372.5166 - val_loss: 2254.7557 - val_mse: 29006460.0000 - val_mae: 2254.7556\n",
            "Epoch 437/500\n",
            "1130/1130 [==============================] - 0s 98us/step - loss: 2331.7840 - mse: 32067338.0000 - mae: 2331.7842 - val_loss: 2257.0259 - val_mse: 29006248.0000 - val_mae: 2257.0259\n",
            "Epoch 438/500\n",
            "1130/1130 [==============================] - 0s 88us/step - loss: 2364.8496 - mse: 32560590.0000 - mae: 2364.8496 - val_loss: 2275.6021 - val_mse: 29124448.0000 - val_mae: 2275.6023\n",
            "Epoch 439/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2326.8380 - mse: 32159850.0000 - mae: 2326.8379 - val_loss: 2236.1393 - val_mse: 28827832.0000 - val_mae: 2236.1394\n",
            "Epoch 440/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2333.0354 - mse: 31941706.0000 - mae: 2333.0354 - val_loss: 2260.3582 - val_mse: 28995738.0000 - val_mae: 2260.3582\n",
            "Epoch 441/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2354.8717 - mse: 32558408.0000 - mae: 2354.8718 - val_loss: 2268.1110 - val_mse: 29045330.0000 - val_mae: 2268.1108\n",
            "Epoch 442/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2345.6619 - mse: 32332504.0000 - mae: 2345.6619 - val_loss: 2238.0760 - val_mse: 28810620.0000 - val_mae: 2238.0759\n",
            "Epoch 443/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2346.8691 - mse: 31916522.0000 - mae: 2346.8691 - val_loss: 2254.7068 - val_mse: 28932924.0000 - val_mae: 2254.7070\n",
            "Epoch 444/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2350.6249 - mse: 32201254.0000 - mae: 2350.6248 - val_loss: 2258.6157 - val_mse: 28949922.0000 - val_mae: 2258.6160\n",
            "Epoch 445/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2349.2003 - mse: 32308872.0000 - mae: 2349.2004 - val_loss: 2243.6631 - val_mse: 28826034.0000 - val_mae: 2243.6631\n",
            "Epoch 446/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2325.1756 - mse: 31594340.0000 - mae: 2325.1758 - val_loss: 2269.3348 - val_mse: 29005918.0000 - val_mae: 2269.3347\n",
            "Epoch 447/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2372.0775 - mse: 32561086.0000 - mae: 2372.0776 - val_loss: 2233.3117 - val_mse: 28740396.0000 - val_mae: 2233.3115\n",
            "Epoch 448/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2337.3519 - mse: 32093708.0000 - mae: 2337.3518 - val_loss: 2256.2581 - val_mse: 28927342.0000 - val_mae: 2256.2581\n",
            "Epoch 449/500\n",
            "1130/1130 [==============================] - 0s 102us/step - loss: 2340.6528 - mse: 32030016.0000 - mae: 2340.6526 - val_loss: 2236.6181 - val_mse: 28760626.0000 - val_mae: 2236.6184\n",
            "Epoch 450/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2320.6934 - mse: 31812860.0000 - mae: 2320.6936 - val_loss: 2234.8420 - val_mse: 28725942.0000 - val_mae: 2234.8418\n",
            "Epoch 451/500\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 2371.5424 - mse: 32125116.0000 - mae: 2371.5422 - val_loss: 2260.7987 - val_mse: 28931694.0000 - val_mae: 2260.7986\n",
            "Epoch 452/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2337.1456 - mse: 32063984.0000 - mae: 2337.1455 - val_loss: 2254.8652 - val_mse: 28876540.0000 - val_mae: 2254.8650\n",
            "Epoch 453/500\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 2340.7793 - mse: 31912958.0000 - mae: 2340.7795 - val_loss: 2246.8071 - val_mse: 28797644.0000 - val_mae: 2246.8071\n",
            "Epoch 454/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2331.3103 - mse: 32351890.0000 - mae: 2331.3101 - val_loss: 2231.3858 - val_mse: 28655858.0000 - val_mae: 2231.3857\n",
            "Epoch 455/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2340.8730 - mse: 31761094.0000 - mae: 2340.8730 - val_loss: 2239.1496 - val_mse: 28729188.0000 - val_mae: 2239.1497\n",
            "Epoch 456/500\n",
            "1130/1130 [==============================] - 0s 85us/step - loss: 2332.9034 - mse: 31758278.0000 - mae: 2332.9036 - val_loss: 2258.1421 - val_mse: 28850328.0000 - val_mae: 2258.1418\n",
            "Epoch 457/500\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 2346.1868 - mse: 31992794.0000 - mae: 2346.1870 - val_loss: 2256.2826 - val_mse: 28843846.0000 - val_mae: 2256.2825\n",
            "Epoch 458/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2355.5669 - mse: 31872038.0000 - mae: 2355.5671 - val_loss: 2253.9532 - val_mse: 28826702.0000 - val_mae: 2253.9534\n",
            "Epoch 459/500\n",
            "1130/1130 [==============================] - 0s 101us/step - loss: 2328.2806 - mse: 31933090.0000 - mae: 2328.2805 - val_loss: 2218.3187 - val_mse: 28524468.0000 - val_mae: 2218.3186\n",
            "Epoch 460/500\n",
            "1130/1130 [==============================] - 0s 83us/step - loss: 2330.7003 - mse: 31820606.0000 - mae: 2330.7002 - val_loss: 2232.6019 - val_mse: 28598390.0000 - val_mae: 2232.6018\n",
            "Epoch 461/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2320.5914 - mse: 31641030.0000 - mae: 2320.5916 - val_loss: 2222.6495 - val_mse: 28518318.0000 - val_mae: 2222.6494\n",
            "Epoch 462/500\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 2341.3434 - mse: 31692886.0000 - mae: 2341.3433 - val_loss: 2274.1945 - val_mse: 28903258.0000 - val_mae: 2274.1943\n",
            "Epoch 463/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2337.9027 - mse: 31866542.0000 - mae: 2337.9026 - val_loss: 2243.9817 - val_mse: 28679358.0000 - val_mae: 2243.9814\n",
            "Epoch 464/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2342.5586 - mse: 32057236.0000 - mae: 2342.5583 - val_loss: 2239.8736 - val_mse: 28640364.0000 - val_mae: 2239.8738\n",
            "Epoch 465/500\n",
            "1130/1130 [==============================] - 0s 86us/step - loss: 2356.9728 - mse: 31902998.0000 - mae: 2356.9729 - val_loss: 2231.7209 - val_mse: 28571236.0000 - val_mae: 2231.7209\n",
            "Epoch 466/500\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 2350.2371 - mse: 32230652.0000 - mae: 2350.2371 - val_loss: 2233.2448 - val_mse: 28593360.0000 - val_mae: 2233.2446\n",
            "Epoch 467/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2348.4115 - mse: 31800042.0000 - mae: 2348.4116 - val_loss: 2232.8256 - val_mse: 28583464.0000 - val_mae: 2232.8254\n",
            "Epoch 468/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2344.1169 - mse: 31590410.0000 - mae: 2344.1169 - val_loss: 2228.9319 - val_mse: 28537444.0000 - val_mae: 2228.9319\n",
            "Epoch 469/500\n",
            "1130/1130 [==============================] - 0s 100us/step - loss: 2340.6397 - mse: 32011096.0000 - mae: 2340.6399 - val_loss: 2220.6440 - val_mse: 28466482.0000 - val_mae: 2220.6440\n",
            "Epoch 470/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2330.8095 - mse: 31547566.0000 - mae: 2330.8096 - val_loss: 2237.5585 - val_mse: 28599140.0000 - val_mae: 2237.5583\n",
            "Epoch 471/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2320.8469 - mse: 31811856.0000 - mae: 2320.8469 - val_loss: 2238.0417 - val_mse: 28605786.0000 - val_mae: 2238.0417\n",
            "Epoch 472/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2350.2317 - mse: 31805488.0000 - mae: 2350.2319 - val_loss: 2224.9283 - val_mse: 28475550.0000 - val_mae: 2224.9282\n",
            "Epoch 473/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2350.2232 - mse: 31607596.0000 - mae: 2350.2231 - val_loss: 2240.1741 - val_mse: 28609874.0000 - val_mae: 2240.1741\n",
            "Epoch 474/500\n",
            "1130/1130 [==============================] - 0s 91us/step - loss: 2328.9491 - mse: 31744214.0000 - mae: 2328.9492 - val_loss: 2255.7784 - val_mse: 28684082.0000 - val_mae: 2255.7783\n",
            "Epoch 475/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2344.6767 - mse: 31878186.0000 - mae: 2344.6768 - val_loss: 2232.9486 - val_mse: 28516130.0000 - val_mae: 2232.9487\n",
            "Epoch 476/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2345.3966 - mse: 31553816.0000 - mae: 2345.3965 - val_loss: 2225.3571 - val_mse: 28441912.0000 - val_mae: 2225.3569\n",
            "Epoch 477/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2348.5616 - mse: 31840828.0000 - mae: 2348.5615 - val_loss: 2233.1838 - val_mse: 28523236.0000 - val_mae: 2233.1836\n",
            "Epoch 478/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2335.7807 - mse: 31436408.0000 - mae: 2335.7808 - val_loss: 2222.9032 - val_mse: 28420090.0000 - val_mae: 2222.9033\n",
            "Epoch 479/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2333.9922 - mse: 31581896.0000 - mae: 2333.9922 - val_loss: 2254.8477 - val_mse: 28662758.0000 - val_mae: 2254.8477\n",
            "Epoch 480/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2357.2208 - mse: 31887044.0000 - mae: 2357.2207 - val_loss: 2233.2930 - val_mse: 28460694.0000 - val_mae: 2233.2930\n",
            "Epoch 481/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2323.0996 - mse: 31566336.0000 - mae: 2323.0996 - val_loss: 2231.4226 - val_mse: 28428326.0000 - val_mae: 2231.4226\n",
            "Epoch 482/500\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 2335.7097 - mse: 31616280.0000 - mae: 2335.7097 - val_loss: 2219.2371 - val_mse: 28326626.0000 - val_mae: 2219.2368\n",
            "Epoch 483/500\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 2343.7663 - mse: 31440422.0000 - mae: 2343.7664 - val_loss: 2247.0993 - val_mse: 28550392.0000 - val_mae: 2247.0994\n",
            "Epoch 484/500\n",
            "1130/1130 [==============================] - 0s 85us/step - loss: 2308.4154 - mse: 30973750.0000 - mae: 2308.4155 - val_loss: 2236.4608 - val_mse: 28474250.0000 - val_mae: 2236.4607\n",
            "Epoch 485/500\n",
            "1130/1130 [==============================] - 0s 99us/step - loss: 2326.0990 - mse: 31493136.0000 - mae: 2326.0991 - val_loss: 2222.0392 - val_mse: 28345014.0000 - val_mae: 2222.0393\n",
            "Epoch 486/500\n",
            "1130/1130 [==============================] - 0s 87us/step - loss: 2340.9194 - mse: 31485322.0000 - mae: 2340.9194 - val_loss: 2254.2848 - val_mse: 28586208.0000 - val_mae: 2254.2852\n",
            "Epoch 487/500\n",
            "1130/1130 [==============================] - 0s 99us/step - loss: 2328.1169 - mse: 31618894.0000 - mae: 2328.1169 - val_loss: 2237.9548 - val_mse: 28462148.0000 - val_mae: 2237.9548\n",
            "Epoch 488/500\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 2341.7793 - mse: 31812892.0000 - mae: 2341.7793 - val_loss: 2249.2922 - val_mse: 28538698.0000 - val_mae: 2249.2922\n",
            "Epoch 489/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2340.4129 - mse: 31761812.0000 - mae: 2340.4131 - val_loss: 2232.6978 - val_mse: 28420060.0000 - val_mae: 2232.6975\n",
            "Epoch 490/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2314.7893 - mse: 31355572.0000 - mae: 2314.7891 - val_loss: 2231.8893 - val_mse: 28391334.0000 - val_mae: 2231.8894\n",
            "Epoch 491/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2322.4543 - mse: 31056750.0000 - mae: 2322.4541 - val_loss: 2237.1143 - val_mse: 28436822.0000 - val_mae: 2237.1145\n",
            "Epoch 492/500\n",
            "1130/1130 [==============================] - 0s 88us/step - loss: 2326.8052 - mse: 31501810.0000 - mae: 2326.8054 - val_loss: 2229.8785 - val_mse: 28380464.0000 - val_mae: 2229.8789\n",
            "Epoch 493/500\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 2313.4077 - mse: 30632124.0000 - mae: 2313.4077 - val_loss: 2229.9295 - val_mse: 28355686.0000 - val_mae: 2229.9297\n",
            "Epoch 494/500\n",
            "1130/1130 [==============================] - 0s 88us/step - loss: 2331.7737 - mse: 31572910.0000 - mae: 2331.7737 - val_loss: 2241.3151 - val_mse: 28422304.0000 - val_mae: 2241.3149\n",
            "Epoch 495/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2323.9548 - mse: 31064246.0000 - mae: 2323.9548 - val_loss: 2237.1561 - val_mse: 28400432.0000 - val_mae: 2237.1560\n",
            "Epoch 496/500\n",
            "1130/1130 [==============================] - 0s 103us/step - loss: 2328.6004 - mse: 31677296.0000 - mae: 2328.6003 - val_loss: 2243.2558 - val_mse: 28437328.0000 - val_mae: 2243.2559\n",
            "Epoch 497/500\n",
            "1130/1130 [==============================] - 0s 98us/step - loss: 2341.8943 - mse: 31519170.0000 - mae: 2341.8943 - val_loss: 2222.8455 - val_mse: 28276140.0000 - val_mae: 2222.8452\n",
            "Epoch 498/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2341.8442 - mse: 31617796.0000 - mae: 2341.8442 - val_loss: 2234.6243 - val_mse: 28352542.0000 - val_mae: 2234.6243\n",
            "Epoch 499/500\n",
            "1130/1130 [==============================] - 0s 89us/step - loss: 2342.7684 - mse: 31327782.0000 - mae: 2342.7683 - val_loss: 2249.7845 - val_mse: 28464298.0000 - val_mae: 2249.7847\n",
            "Epoch 500/500\n",
            "1130/1130 [==============================] - 0s 99us/step - loss: 2348.0812 - mse: 31429970.0000 - mae: 2348.0813 - val_loss: 2238.1209 - val_mse: 28376800.0000 - val_mae: 2238.1211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 1500 samples\n",
            "Epoch 1/500\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 3347.3600 - mse: 68493192.0000 - mae: 3347.3601 - val_loss: 3852.4648 - val_mse: 98320360.0000 - val_mae: 3852.4646\n",
            "Epoch 2/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 3347.2397 - mse: 68492496.0000 - mae: 3347.2397 - val_loss: 3852.0328 - val_mse: 98318328.0000 - val_mae: 3852.0330\n",
            "Epoch 3/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 3345.6834 - mse: 68486384.0000 - mae: 3345.6833 - val_loss: 3848.2326 - val_mse: 98301520.0000 - val_mae: 3848.2327\n",
            "Epoch 4/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 3340.2757 - mse: 68458040.0000 - mae: 3340.2756 - val_loss: 3840.0774 - val_mse: 98256736.0000 - val_mae: 3840.0769\n",
            "Epoch 5/500\n",
            "1500/1500 [==============================] - 0s 57us/step - loss: 3331.5935 - mse: 68409264.0000 - mae: 3331.5933 - val_loss: 3829.3968 - val_mse: 98184608.0000 - val_mae: 3829.3967\n",
            "Epoch 6/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 3320.0264 - mse: 68330232.0000 - mae: 3320.0264 - val_loss: 3816.5970 - val_mse: 98089712.0000 - val_mae: 3816.5969\n",
            "Epoch 7/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 3306.7723 - mse: 68242296.0000 - mae: 3306.7722 - val_loss: 3802.4966 - val_mse: 97980136.0000 - val_mae: 3802.4966\n",
            "Epoch 8/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 3292.5942 - mse: 68142752.0000 - mae: 3292.5942 - val_loss: 3787.1248 - val_mse: 97852776.0000 - val_mae: 3787.1243\n",
            "Epoch 9/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 3278.0695 - mse: 68011920.0000 - mae: 3278.0696 - val_loss: 3770.7106 - val_mse: 97708784.0000 - val_mae: 3770.7107\n",
            "Epoch 10/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 3261.1166 - mse: 67878568.0000 - mae: 3261.1167 - val_loss: 3753.4996 - val_mse: 97548296.0000 - val_mae: 3753.4998\n",
            "Epoch 11/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 3246.1568 - mse: 67764736.0000 - mae: 3246.1567 - val_loss: 3735.7881 - val_mse: 97376544.0000 - val_mae: 3735.7881\n",
            "Epoch 12/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 3229.9021 - mse: 67603480.0000 - mae: 3229.9021 - val_loss: 3718.1698 - val_mse: 97196784.0000 - val_mae: 3718.1697\n",
            "Epoch 13/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 3213.6088 - mse: 67416056.0000 - mae: 3213.6086 - val_loss: 3701.0411 - val_mse: 97008328.0000 - val_mae: 3701.0410\n",
            "Epoch 14/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 3197.4552 - mse: 67262456.0000 - mae: 3197.4553 - val_loss: 3685.1861 - val_mse: 96818792.0000 - val_mae: 3685.1860\n",
            "Epoch 15/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 3181.2190 - mse: 67089276.0000 - mae: 3181.2190 - val_loss: 3669.9218 - val_mse: 96623376.0000 - val_mae: 3669.9221\n",
            "Epoch 16/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 3165.9387 - mse: 66893372.0000 - mae: 3165.9387 - val_loss: 3655.6766 - val_mse: 96432464.0000 - val_mae: 3655.6768\n",
            "Epoch 17/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 3157.1457 - mse: 66788952.0000 - mae: 3157.1458 - val_loss: 3642.5157 - val_mse: 96248008.0000 - val_mae: 3642.5156\n",
            "Epoch 18/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 3144.4670 - mse: 66593160.0000 - mae: 3144.4670 - val_loss: 3630.4012 - val_mse: 96066000.0000 - val_mae: 3630.4014\n",
            "Epoch 19/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 3134.8822 - mse: 66408580.0000 - mae: 3134.8821 - val_loss: 3619.7660 - val_mse: 95889312.0000 - val_mae: 3619.7664\n",
            "Epoch 20/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 3125.2454 - mse: 66265160.0000 - mae: 3125.2454 - val_loss: 3611.1086 - val_mse: 95729656.0000 - val_mae: 3611.1084\n",
            "Epoch 21/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 3122.1273 - mse: 66245884.0000 - mae: 3122.1274 - val_loss: 3603.8579 - val_mse: 95581536.0000 - val_mae: 3603.8584\n",
            "Epoch 22/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 3113.2231 - mse: 66042936.0000 - mae: 3113.2229 - val_loss: 3597.2445 - val_mse: 95435504.0000 - val_mae: 3597.2444\n",
            "Epoch 23/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 3107.9714 - mse: 65929856.0000 - mae: 3107.9714 - val_loss: 3591.7903 - val_mse: 95302656.0000 - val_mae: 3591.7903\n",
            "Epoch 24/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 3105.0423 - mse: 65885360.0000 - mae: 3105.0422 - val_loss: 3586.9069 - val_mse: 95173968.0000 - val_mae: 3586.9067\n",
            "Epoch 25/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 3099.5603 - mse: 65809276.0000 - mae: 3099.5603 - val_loss: 3582.4502 - val_mse: 95058112.0000 - val_mae: 3582.4500\n",
            "Epoch 26/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 3094.4335 - mse: 65667436.0000 - mae: 3094.4333 - val_loss: 3577.7971 - val_mse: 94933192.0000 - val_mae: 3577.7966\n",
            "Epoch 27/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 3093.7232 - mse: 65559144.0000 - mae: 3093.7234 - val_loss: 3573.5018 - val_mse: 94814416.0000 - val_mae: 3573.5017\n",
            "Epoch 28/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 3083.4337 - mse: 65416960.0000 - mae: 3083.4336 - val_loss: 3568.6822 - val_mse: 94701608.0000 - val_mae: 3568.6819\n",
            "Epoch 29/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 3082.2765 - mse: 65281184.0000 - mae: 3082.2766 - val_loss: 3561.0593 - val_mse: 94619552.0000 - val_mae: 3561.0593\n",
            "Epoch 30/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 3071.6813 - mse: 65299656.0000 - mae: 3071.6814 - val_loss: 3555.0558 - val_mse: 94491376.0000 - val_mae: 3555.0557\n",
            "Epoch 31/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 3063.1842 - mse: 65205096.0000 - mae: 3063.1841 - val_loss: 3549.7484 - val_mse: 94308576.0000 - val_mae: 3549.7480\n",
            "Epoch 32/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 3057.7243 - mse: 64984792.0000 - mae: 3057.7244 - val_loss: 3547.0256 - val_mse: 94254800.0000 - val_mae: 3547.0256\n",
            "Epoch 33/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 3056.7172 - mse: 64974536.0000 - mae: 3056.7170 - val_loss: 3539.9026 - val_mse: 94022136.0000 - val_mae: 3539.9023\n",
            "Epoch 34/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 3049.4682 - mse: 64947820.0000 - mae: 3049.4680 - val_loss: 3535.2928 - val_mse: 93912640.0000 - val_mae: 3535.2927\n",
            "Epoch 35/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 3042.2262 - mse: 64748596.0000 - mae: 3042.2261 - val_loss: 3530.5717 - val_mse: 93778344.0000 - val_mae: 3530.5718\n",
            "Epoch 36/500\n",
            "1500/1500 [==============================] - 0s 55us/step - loss: 3044.0300 - mse: 64706684.0000 - mae: 3044.0300 - val_loss: 3525.7372 - val_mse: 93605792.0000 - val_mae: 3525.7366\n",
            "Epoch 37/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 3042.2597 - mse: 64587972.0000 - mae: 3042.2600 - val_loss: 3523.4604 - val_mse: 93526680.0000 - val_mae: 3523.4604\n",
            "Epoch 38/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 3029.9955 - mse: 64412100.0000 - mae: 3029.9956 - val_loss: 3517.2019 - val_mse: 93273992.0000 - val_mae: 3517.2019\n",
            "Epoch 39/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 3030.7203 - mse: 64326376.0000 - mae: 3030.7202 - val_loss: 3513.7988 - val_mse: 93166744.0000 - val_mae: 3513.7986\n",
            "Epoch 40/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 3022.6179 - mse: 64227580.0000 - mae: 3022.6179 - val_loss: 3510.1915 - val_mse: 93050960.0000 - val_mae: 3510.1917\n",
            "Epoch 41/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 3014.9992 - mse: 63971668.0000 - mae: 3014.9993 - val_loss: 3506.1373 - val_mse: 92876104.0000 - val_mae: 3506.1375\n",
            "Epoch 42/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 3013.6007 - mse: 63956920.0000 - mae: 3013.6006 - val_loss: 3503.8731 - val_mse: 92813928.0000 - val_mae: 3503.8730\n",
            "Epoch 43/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 3008.8749 - mse: 63673156.0000 - mae: 3008.8748 - val_loss: 3500.2535 - val_mse: 92585008.0000 - val_mae: 3500.2529\n",
            "Epoch 44/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 3014.3574 - mse: 63667700.0000 - mae: 3014.3574 - val_loss: 3496.9386 - val_mse: 92451112.0000 - val_mae: 3496.9387\n",
            "Epoch 45/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 3006.6278 - mse: 63587456.0000 - mae: 3006.6277 - val_loss: 3494.7924 - val_mse: 92444544.0000 - val_mae: 3494.7920\n",
            "Epoch 46/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 3004.6181 - mse: 63620776.0000 - mae: 3004.6179 - val_loss: 3490.7758 - val_mse: 92212792.0000 - val_mae: 3490.7756\n",
            "Epoch 47/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 3005.7644 - mse: 63353116.0000 - mae: 3005.7644 - val_loss: 3487.1506 - val_mse: 92131000.0000 - val_mae: 3487.1504\n",
            "Epoch 48/500\n",
            "1500/1500 [==============================] - 0s 53us/step - loss: 2994.8483 - mse: 63275312.0000 - mae: 2994.8486 - val_loss: 3486.5096 - val_mse: 92086608.0000 - val_mae: 3486.5098\n",
            "Epoch 49/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2992.5766 - mse: 63133272.0000 - mae: 2992.5767 - val_loss: 3481.3754 - val_mse: 91887008.0000 - val_mae: 3481.3757\n",
            "Epoch 50/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2988.3011 - mse: 63095616.0000 - mae: 2988.3013 - val_loss: 3478.8631 - val_mse: 91752968.0000 - val_mae: 3478.8630\n",
            "Epoch 51/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2984.3688 - mse: 62984740.0000 - mae: 2984.3687 - val_loss: 3478.1417 - val_mse: 91556440.0000 - val_mae: 3478.1421\n",
            "Epoch 52/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2994.0871 - mse: 63021688.0000 - mae: 2994.0869 - val_loss: 3474.0292 - val_mse: 91520912.0000 - val_mae: 3474.0293\n",
            "Epoch 53/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2985.3769 - mse: 62795076.0000 - mae: 2985.3767 - val_loss: 3474.4547 - val_mse: 91560736.0000 - val_mae: 3474.4551\n",
            "Epoch 54/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2985.1222 - mse: 62735084.0000 - mae: 2985.1223 - val_loss: 3469.5998 - val_mse: 91317624.0000 - val_mae: 3469.5996\n",
            "Epoch 55/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 2985.8824 - mse: 62944792.0000 - mae: 2985.8826 - val_loss: 3467.6540 - val_mse: 91236224.0000 - val_mae: 3467.6541\n",
            "Epoch 56/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2982.4492 - mse: 62726292.0000 - mae: 2982.4492 - val_loss: 3466.9936 - val_mse: 91059352.0000 - val_mae: 3466.9937\n",
            "Epoch 57/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2972.8082 - mse: 62339128.0000 - mae: 2972.8083 - val_loss: 3465.1787 - val_mse: 90979088.0000 - val_mae: 3465.1787\n",
            "Epoch 58/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2969.0817 - mse: 62375820.0000 - mae: 2969.0818 - val_loss: 3463.1410 - val_mse: 90886424.0000 - val_mae: 3463.1406\n",
            "Epoch 59/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2972.4453 - mse: 62439828.0000 - mae: 2972.4453 - val_loss: 3462.5408 - val_mse: 90751224.0000 - val_mae: 3462.5408\n",
            "Epoch 60/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2970.0583 - mse: 62262508.0000 - mae: 2970.0583 - val_loss: 3458.7448 - val_mse: 90780896.0000 - val_mae: 3458.7451\n",
            "Epoch 61/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2967.6178 - mse: 62156920.0000 - mae: 2967.6177 - val_loss: 3458.6704 - val_mse: 90535512.0000 - val_mae: 3458.6699\n",
            "Epoch 62/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2973.4642 - mse: 62054460.0000 - mae: 2973.4641 - val_loss: 3454.4799 - val_mse: 90516752.0000 - val_mae: 3454.4797\n",
            "Epoch 63/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2966.2495 - mse: 62149860.0000 - mae: 2966.2493 - val_loss: 3452.3580 - val_mse: 90509504.0000 - val_mae: 3452.3577\n",
            "Epoch 64/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2956.3202 - mse: 61925108.0000 - mae: 2956.3203 - val_loss: 3453.6479 - val_mse: 90242712.0000 - val_mae: 3453.6479\n",
            "Epoch 65/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 2958.8825 - mse: 61735452.0000 - mae: 2958.8826 - val_loss: 3450.5369 - val_mse: 90165000.0000 - val_mae: 3450.5364\n",
            "Epoch 66/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2964.0944 - mse: 61963984.0000 - mae: 2964.0947 - val_loss: 3447.9117 - val_mse: 90083104.0000 - val_mae: 3447.9114\n",
            "Epoch 67/500\n",
            "1500/1500 [==============================] - 0s 56us/step - loss: 2955.5320 - mse: 61614676.0000 - mae: 2955.5320 - val_loss: 3444.0081 - val_mse: 90153920.0000 - val_mae: 3444.0081\n",
            "Epoch 68/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2964.0902 - mse: 61825188.0000 - mae: 2964.0901 - val_loss: 3442.3833 - val_mse: 89926696.0000 - val_mae: 3442.3833\n",
            "Epoch 69/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2952.2225 - mse: 61437336.0000 - mae: 2952.2227 - val_loss: 3437.7941 - val_mse: 89971456.0000 - val_mae: 3437.7939\n",
            "Epoch 70/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2949.1934 - mse: 61611540.0000 - mae: 2949.1934 - val_loss: 3434.9680 - val_mse: 89745000.0000 - val_mae: 3434.9680\n",
            "Epoch 71/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2947.3007 - mse: 61331188.0000 - mae: 2947.3008 - val_loss: 3431.4654 - val_mse: 89653600.0000 - val_mae: 3431.4656\n",
            "Epoch 72/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2940.0741 - mse: 61364168.0000 - mae: 2940.0740 - val_loss: 3425.4592 - val_mse: 89602472.0000 - val_mae: 3425.4590\n",
            "Epoch 73/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2949.1973 - mse: 61398776.0000 - mae: 2949.1973 - val_loss: 3421.0357 - val_mse: 89541272.0000 - val_mae: 3421.0354\n",
            "Epoch 74/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2943.6576 - mse: 61337736.0000 - mae: 2943.6577 - val_loss: 3418.9347 - val_mse: 89417168.0000 - val_mae: 3418.9343\n",
            "Epoch 75/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2937.9017 - mse: 61283436.0000 - mae: 2937.9016 - val_loss: 3415.0735 - val_mse: 89328816.0000 - val_mae: 3415.0740\n",
            "Epoch 76/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2936.3003 - mse: 61176588.0000 - mae: 2936.3003 - val_loss: 3412.2221 - val_mse: 89295568.0000 - val_mae: 3412.2224\n",
            "Epoch 77/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2934.0496 - mse: 61322084.0000 - mae: 2934.0496 - val_loss: 3409.6646 - val_mse: 89165120.0000 - val_mae: 3409.6643\n",
            "Epoch 78/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 2921.4698 - mse: 60951188.0000 - mae: 2921.4697 - val_loss: 3406.5633 - val_mse: 89031272.0000 - val_mae: 3406.5632\n",
            "Epoch 79/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2935.0185 - mse: 60912000.0000 - mae: 2935.0183 - val_loss: 3404.5575 - val_mse: 88901800.0000 - val_mae: 3404.5574\n",
            "Epoch 80/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2906.8217 - mse: 60616424.0000 - mae: 2906.8218 - val_loss: 3400.2836 - val_mse: 88873888.0000 - val_mae: 3400.2839\n",
            "Epoch 81/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2921.8425 - mse: 60835560.0000 - mae: 2921.8428 - val_loss: 3397.4440 - val_mse: 88737896.0000 - val_mae: 3397.4441\n",
            "Epoch 82/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2924.0795 - mse: 60700632.0000 - mae: 2924.0793 - val_loss: 3396.0127 - val_mse: 88598784.0000 - val_mae: 3396.0129\n",
            "Epoch 83/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2910.2524 - mse: 60400932.0000 - mae: 2910.2524 - val_loss: 3395.0533 - val_mse: 88437304.0000 - val_mae: 3395.0532\n",
            "Epoch 84/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2904.3237 - mse: 60243888.0000 - mae: 2904.3237 - val_loss: 3390.3739 - val_mse: 88483968.0000 - val_mae: 3390.3743\n",
            "Epoch 85/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2918.9625 - mse: 60468604.0000 - mae: 2918.9626 - val_loss: 3387.2729 - val_mse: 88284064.0000 - val_mae: 3387.2729\n",
            "Epoch 86/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 2904.7920 - mse: 60452044.0000 - mae: 2904.7920 - val_loss: 3388.5102 - val_mse: 88099320.0000 - val_mae: 3388.5103\n",
            "Epoch 87/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2917.2943 - mse: 60443524.0000 - mae: 2917.2944 - val_loss: 3384.3330 - val_mse: 88010688.0000 - val_mae: 3384.3330\n",
            "Epoch 88/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2906.6282 - mse: 60247260.0000 - mae: 2906.6284 - val_loss: 3378.8823 - val_mse: 87973208.0000 - val_mae: 3378.8823\n",
            "Epoch 89/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2900.2507 - mse: 60239080.0000 - mae: 2900.2507 - val_loss: 3376.3923 - val_mse: 87857432.0000 - val_mae: 3376.3923\n",
            "Epoch 90/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2897.8721 - mse: 60127712.0000 - mae: 2897.8723 - val_loss: 3376.2126 - val_mse: 87714408.0000 - val_mae: 3376.2124\n",
            "Epoch 91/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2893.7572 - mse: 59615992.0000 - mae: 2893.7573 - val_loss: 3371.5179 - val_mse: 87722800.0000 - val_mae: 3371.5181\n",
            "Epoch 92/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2897.7479 - mse: 59896872.0000 - mae: 2897.7480 - val_loss: 3370.3078 - val_mse: 87521056.0000 - val_mae: 3370.3076\n",
            "Epoch 93/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2883.5139 - mse: 59748304.0000 - mae: 2883.5137 - val_loss: 3369.7501 - val_mse: 87377640.0000 - val_mae: 3369.7502\n",
            "Epoch 94/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 2886.4262 - mse: 59709588.0000 - mae: 2886.4263 - val_loss: 3364.1972 - val_mse: 87323520.0000 - val_mae: 3364.1970\n",
            "Epoch 95/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2888.7992 - mse: 59695136.0000 - mae: 2888.7993 - val_loss: 3363.7517 - val_mse: 87154704.0000 - val_mae: 3363.7517\n",
            "Epoch 96/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2878.4347 - mse: 59288272.0000 - mae: 2878.4346 - val_loss: 3358.4955 - val_mse: 87097608.0000 - val_mae: 3358.4956\n",
            "Epoch 97/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2875.7997 - mse: 59237052.0000 - mae: 2875.7996 - val_loss: 3356.9899 - val_mse: 87006888.0000 - val_mae: 3356.9900\n",
            "Epoch 98/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2874.8834 - mse: 59390576.0000 - mae: 2874.8833 - val_loss: 3354.9788 - val_mse: 86795592.0000 - val_mae: 3354.9788\n",
            "Epoch 99/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2874.2791 - mse: 59122488.0000 - mae: 2874.2793 - val_loss: 3351.5692 - val_mse: 86751056.0000 - val_mae: 3351.5693\n",
            "Epoch 100/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2870.1278 - mse: 59039192.0000 - mae: 2870.1279 - val_loss: 3349.7868 - val_mse: 86547920.0000 - val_mae: 3349.7866\n",
            "Epoch 101/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2881.9541 - mse: 59325624.0000 - mae: 2881.9541 - val_loss: 3347.6790 - val_mse: 86558504.0000 - val_mae: 3347.6790\n",
            "Epoch 102/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2865.9261 - mse: 58912220.0000 - mae: 2865.9260 - val_loss: 3343.7691 - val_mse: 86340584.0000 - val_mae: 3343.7690\n",
            "Epoch 103/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2864.5995 - mse: 58929092.0000 - mae: 2864.5996 - val_loss: 3344.7579 - val_mse: 86104520.0000 - val_mae: 3344.7581\n",
            "Epoch 104/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2857.3652 - mse: 58699808.0000 - mae: 2857.3652 - val_loss: 3341.6128 - val_mse: 86014048.0000 - val_mae: 3341.6130\n",
            "Epoch 105/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2856.4512 - mse: 58829252.0000 - mae: 2856.4514 - val_loss: 3336.2799 - val_mse: 85982216.0000 - val_mae: 3336.2800\n",
            "Epoch 106/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 2842.0466 - mse: 58649964.0000 - mae: 2842.0466 - val_loss: 3336.1153 - val_mse: 85747400.0000 - val_mae: 3336.1152\n",
            "Epoch 107/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2846.1878 - mse: 58246344.0000 - mae: 2846.1877 - val_loss: 3335.2860 - val_mse: 85574120.0000 - val_mae: 3335.2859\n",
            "Epoch 108/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2838.2894 - mse: 58009580.0000 - mae: 2838.2893 - val_loss: 3333.6895 - val_mse: 85407576.0000 - val_mae: 3333.6895\n",
            "Epoch 109/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2831.3691 - mse: 58070332.0000 - mae: 2831.3689 - val_loss: 3326.1025 - val_mse: 85389288.0000 - val_mae: 3326.1021\n",
            "Epoch 110/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 2843.7745 - mse: 58062308.0000 - mae: 2843.7744 - val_loss: 3323.4397 - val_mse: 85340064.0000 - val_mae: 3323.4397\n",
            "Epoch 111/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2835.2241 - mse: 57922548.0000 - mae: 2835.2241 - val_loss: 3320.4204 - val_mse: 85166128.0000 - val_mae: 3320.4199\n",
            "Epoch 112/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2830.5979 - mse: 58107496.0000 - mae: 2830.5979 - val_loss: 3319.4172 - val_mse: 84996504.0000 - val_mae: 3319.4170\n",
            "Epoch 113/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2836.9832 - mse: 57948544.0000 - mae: 2836.9834 - val_loss: 3318.3184 - val_mse: 84830752.0000 - val_mae: 3318.3186\n",
            "Epoch 114/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2834.6771 - mse: 57964484.0000 - mae: 2834.6770 - val_loss: 3313.3999 - val_mse: 84750792.0000 - val_mae: 3313.3999\n",
            "Epoch 115/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2836.5284 - mse: 57493548.0000 - mae: 2836.5283 - val_loss: 3312.1878 - val_mse: 84603064.0000 - val_mae: 3312.1877\n",
            "Epoch 116/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2819.4778 - mse: 57437292.0000 - mae: 2819.4780 - val_loss: 3307.5351 - val_mse: 84552048.0000 - val_mae: 3307.5349\n",
            "Epoch 117/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2830.4485 - mse: 57259200.0000 - mae: 2830.4487 - val_loss: 3305.6142 - val_mse: 84483984.0000 - val_mae: 3305.6143\n",
            "Epoch 118/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2832.2023 - mse: 57663296.0000 - mae: 2832.2024 - val_loss: 3302.5959 - val_mse: 84337600.0000 - val_mae: 3302.5964\n",
            "Epoch 119/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 2823.6084 - mse: 57375048.0000 - mae: 2823.6084 - val_loss: 3304.0095 - val_mse: 84062320.0000 - val_mae: 3304.0093\n",
            "Epoch 120/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2815.2393 - mse: 57174512.0000 - mae: 2815.2393 - val_loss: 3297.7807 - val_mse: 84008584.0000 - val_mae: 3297.7808\n",
            "Epoch 121/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2810.3460 - mse: 56875052.0000 - mae: 2810.3459 - val_loss: 3301.6324 - val_mse: 84087056.0000 - val_mae: 3301.6323\n",
            "Epoch 122/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2809.3595 - mse: 57205120.0000 - mae: 2809.3594 - val_loss: 3294.7399 - val_mse: 83787000.0000 - val_mae: 3294.7397\n",
            "Epoch 123/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2813.8814 - mse: 56772596.0000 - mae: 2813.8816 - val_loss: 3292.2177 - val_mse: 83677936.0000 - val_mae: 3292.2180\n",
            "Epoch 124/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 2812.7228 - mse: 56770332.0000 - mae: 2812.7227 - val_loss: 3289.4859 - val_mse: 83570744.0000 - val_mae: 3289.4856\n",
            "Epoch 125/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 2805.8555 - mse: 56363132.0000 - mae: 2805.8552 - val_loss: 3287.2588 - val_mse: 83455720.0000 - val_mae: 3287.2585\n",
            "Epoch 126/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2817.1737 - mse: 56816620.0000 - mae: 2817.1736 - val_loss: 3285.2478 - val_mse: 83319232.0000 - val_mae: 3285.2483\n",
            "Epoch 127/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2799.3744 - mse: 56806308.0000 - mae: 2799.3743 - val_loss: 3283.9581 - val_mse: 83261384.0000 - val_mae: 3283.9583\n",
            "Epoch 128/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2809.3053 - mse: 56203608.0000 - mae: 2809.3054 - val_loss: 3281.9718 - val_mse: 83037960.0000 - val_mae: 3281.9719\n",
            "Epoch 129/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2812.7806 - mse: 56410396.0000 - mae: 2812.7808 - val_loss: 3278.7119 - val_mse: 82960136.0000 - val_mae: 3278.7119\n",
            "Epoch 130/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2799.3815 - mse: 56369172.0000 - mae: 2799.3813 - val_loss: 3276.2051 - val_mse: 82879736.0000 - val_mae: 3276.2051\n",
            "Epoch 131/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2792.7848 - mse: 56373520.0000 - mae: 2792.7849 - val_loss: 3274.1363 - val_mse: 82727440.0000 - val_mae: 3274.1362\n",
            "Epoch 132/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2815.1223 - mse: 56376920.0000 - mae: 2815.1223 - val_loss: 3273.0270 - val_mse: 82622312.0000 - val_mae: 3273.0271\n",
            "Epoch 133/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2795.4988 - mse: 56386160.0000 - mae: 2795.4988 - val_loss: 3269.9330 - val_mse: 82553224.0000 - val_mae: 3269.9331\n",
            "Epoch 134/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2783.7177 - mse: 55888928.0000 - mae: 2783.7178 - val_loss: 3270.2344 - val_mse: 82546888.0000 - val_mae: 3270.2346\n",
            "Epoch 135/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2780.3174 - mse: 55888512.0000 - mae: 2780.3174 - val_loss: 3265.7246 - val_mse: 82315136.0000 - val_mae: 3265.7246\n",
            "Epoch 136/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2812.2129 - mse: 56524504.0000 - mae: 2812.2129 - val_loss: 3274.6325 - val_mse: 82431472.0000 - val_mae: 3274.6326\n",
            "Epoch 137/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2784.9001 - mse: 55901844.0000 - mae: 2784.8999 - val_loss: 3261.1771 - val_mse: 82050416.0000 - val_mae: 3261.1770\n",
            "Epoch 138/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2793.7441 - mse: 55928484.0000 - mae: 2793.7441 - val_loss: 3259.8409 - val_mse: 81894432.0000 - val_mae: 3259.8411\n",
            "Epoch 139/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2780.4077 - mse: 55267792.0000 - mae: 2780.4077 - val_loss: 3256.9509 - val_mse: 81817024.0000 - val_mae: 3256.9509\n",
            "Epoch 140/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2788.6829 - mse: 55536244.0000 - mae: 2788.6831 - val_loss: 3255.3408 - val_mse: 81687552.0000 - val_mae: 3255.3406\n",
            "Epoch 141/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2769.1092 - mse: 55387772.0000 - mae: 2769.1089 - val_loss: 3254.5242 - val_mse: 81467592.0000 - val_mae: 3254.5244\n",
            "Epoch 142/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2778.2937 - mse: 55320272.0000 - mae: 2778.2939 - val_loss: 3253.0624 - val_mse: 81307288.0000 - val_mae: 3253.0620\n",
            "Epoch 143/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2767.4729 - mse: 55204292.0000 - mae: 2767.4729 - val_loss: 3248.6288 - val_mse: 81314008.0000 - val_mae: 3248.6294\n",
            "Epoch 144/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2766.8577 - mse: 55415428.0000 - mae: 2766.8577 - val_loss: 3249.2131 - val_mse: 81061624.0000 - val_mae: 3249.2134\n",
            "Epoch 145/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2766.9524 - mse: 55260080.0000 - mae: 2766.9524 - val_loss: 3248.0629 - val_mse: 80938912.0000 - val_mae: 3248.0630\n",
            "Epoch 146/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2771.1670 - mse: 55304712.0000 - mae: 2771.1670 - val_loss: 3242.7975 - val_mse: 80916712.0000 - val_mae: 3242.7976\n",
            "Epoch 147/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2755.6849 - mse: 54652484.0000 - mae: 2755.6851 - val_loss: 3246.4671 - val_mse: 80928968.0000 - val_mae: 3246.4670\n",
            "Epoch 148/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2764.4494 - mse: 54975300.0000 - mae: 2764.4495 - val_loss: 3239.9344 - val_mse: 80707088.0000 - val_mae: 3239.9343\n",
            "Epoch 149/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2764.7811 - mse: 54732844.0000 - mae: 2764.7810 - val_loss: 3237.2118 - val_mse: 80559808.0000 - val_mae: 3237.2119\n",
            "Epoch 150/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2749.0003 - mse: 54295548.0000 - mae: 2749.0002 - val_loss: 3237.1530 - val_mse: 80516312.0000 - val_mae: 3237.1531\n",
            "Epoch 151/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2760.5482 - mse: 54277488.0000 - mae: 2760.5483 - val_loss: 3235.5912 - val_mse: 80366936.0000 - val_mae: 3235.5913\n",
            "Epoch 152/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2759.4462 - mse: 54533020.0000 - mae: 2759.4460 - val_loss: 3235.4022 - val_mse: 80250872.0000 - val_mae: 3235.4021\n",
            "Epoch 153/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2737.5119 - mse: 54108648.0000 - mae: 2737.5117 - val_loss: 3229.6494 - val_mse: 80018904.0000 - val_mae: 3229.6497\n",
            "Epoch 154/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2740.7412 - mse: 54384516.0000 - mae: 2740.7412 - val_loss: 3229.5325 - val_mse: 79889480.0000 - val_mae: 3229.5327\n",
            "Epoch 155/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2734.5078 - mse: 53898292.0000 - mae: 2734.5076 - val_loss: 3227.4586 - val_mse: 79557920.0000 - val_mae: 3227.4587\n",
            "Epoch 156/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2756.0711 - mse: 53764280.0000 - mae: 2756.0710 - val_loss: 3225.1542 - val_mse: 79609176.0000 - val_mae: 3225.1541\n",
            "Epoch 157/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2740.6214 - mse: 54026484.0000 - mae: 2740.6213 - val_loss: 3226.2457 - val_mse: 79521816.0000 - val_mae: 3226.2456\n",
            "Epoch 158/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2745.4408 - mse: 53909248.0000 - mae: 2745.4407 - val_loss: 3221.2094 - val_mse: 79306000.0000 - val_mae: 3221.2092\n",
            "Epoch 159/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2735.1110 - mse: 53329532.0000 - mae: 2735.1111 - val_loss: 3219.2095 - val_mse: 79199488.0000 - val_mae: 3219.2092\n",
            "Epoch 160/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2742.1581 - mse: 54227044.0000 - mae: 2742.1580 - val_loss: 3221.9521 - val_mse: 79180888.0000 - val_mae: 3221.9519\n",
            "Epoch 161/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2748.0689 - mse: 53742752.0000 - mae: 2748.0691 - val_loss: 3217.3465 - val_mse: 78982344.0000 - val_mae: 3217.3464\n",
            "Epoch 162/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2722.8869 - mse: 53187444.0000 - mae: 2722.8867 - val_loss: 3216.8562 - val_mse: 78860200.0000 - val_mae: 3216.8560\n",
            "Epoch 163/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2740.8565 - mse: 53895572.0000 - mae: 2740.8564 - val_loss: 3229.6290 - val_mse: 78905280.0000 - val_mae: 3229.6287\n",
            "Epoch 164/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2734.4186 - mse: 53334304.0000 - mae: 2734.4187 - val_loss: 3233.0882 - val_mse: 78823296.0000 - val_mae: 3233.0884\n",
            "Epoch 165/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2723.5257 - mse: 52684424.0000 - mae: 2723.5256 - val_loss: 3216.9137 - val_mse: 78532376.0000 - val_mae: 3216.9141\n",
            "Epoch 166/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2727.6326 - mse: 52715420.0000 - mae: 2727.6328 - val_loss: 3209.2245 - val_mse: 78337336.0000 - val_mae: 3209.2244\n",
            "Epoch 167/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2723.0675 - mse: 52933264.0000 - mae: 2723.0674 - val_loss: 3207.9320 - val_mse: 78195728.0000 - val_mae: 3207.9319\n",
            "Epoch 168/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2714.5987 - mse: 53009136.0000 - mae: 2714.5986 - val_loss: 3214.6313 - val_mse: 78164232.0000 - val_mae: 3214.6313\n",
            "Epoch 169/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2723.0483 - mse: 52778116.0000 - mae: 2723.0483 - val_loss: 3206.0874 - val_mse: 77945944.0000 - val_mae: 3206.0874\n",
            "Epoch 170/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 2705.4339 - mse: 52905700.0000 - mae: 2705.4338 - val_loss: 3198.6033 - val_mse: 77571328.0000 - val_mae: 3198.6038\n",
            "Epoch 171/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2717.8673 - mse: 52780008.0000 - mae: 2717.8674 - val_loss: 3202.0524 - val_mse: 77647840.0000 - val_mae: 3202.0520\n",
            "Epoch 172/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2716.8497 - mse: 52597076.0000 - mae: 2716.8496 - val_loss: 3199.0063 - val_mse: 77499104.0000 - val_mae: 3199.0066\n",
            "Epoch 173/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 2721.9792 - mse: 52510720.0000 - mae: 2721.9792 - val_loss: 3201.6880 - val_mse: 77026608.0000 - val_mae: 3201.6880\n",
            "Epoch 174/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2722.7671 - mse: 52835784.0000 - mae: 2722.7671 - val_loss: 3204.9103 - val_mse: 77370336.0000 - val_mae: 3204.9099\n",
            "Epoch 175/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2709.4473 - mse: 51723828.0000 - mae: 2709.4473 - val_loss: 3199.4597 - val_mse: 77207424.0000 - val_mae: 3199.4600\n",
            "Epoch 176/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 2704.1276 - mse: 52300256.0000 - mae: 2704.1277 - val_loss: 3186.3205 - val_mse: 76882960.0000 - val_mae: 3186.3203\n",
            "Epoch 177/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2705.9583 - mse: 51789000.0000 - mae: 2705.9583 - val_loss: 3189.2790 - val_mse: 76837272.0000 - val_mae: 3189.2791\n",
            "Epoch 178/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2718.3161 - mse: 51797632.0000 - mae: 2718.3162 - val_loss: 3196.1720 - val_mse: 76822328.0000 - val_mae: 3196.1724\n",
            "Epoch 179/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 2694.3991 - mse: 51726532.0000 - mae: 2694.3992 - val_loss: 3191.2941 - val_mse: 76641848.0000 - val_mae: 3191.2939\n",
            "Epoch 180/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2711.2474 - mse: 51819840.0000 - mae: 2711.2476 - val_loss: 3188.0770 - val_mse: 76507368.0000 - val_mae: 3188.0769\n",
            "Epoch 181/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2688.6750 - mse: 51082140.0000 - mae: 2688.6750 - val_loss: 3196.5293 - val_mse: 76455448.0000 - val_mae: 3196.5293\n",
            "Epoch 182/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2695.1934 - mse: 51249956.0000 - mae: 2695.1934 - val_loss: 3187.9794 - val_mse: 76277968.0000 - val_mae: 3187.9792\n",
            "Epoch 183/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2683.0293 - mse: 50768676.0000 - mae: 2683.0293 - val_loss: 3175.1639 - val_mse: 75999448.0000 - val_mae: 3175.1641\n",
            "Epoch 184/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2685.4172 - mse: 51379188.0000 - mae: 2685.4172 - val_loss: 3188.5357 - val_mse: 76067464.0000 - val_mae: 3188.5356\n",
            "Epoch 185/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2677.1143 - mse: 50745264.0000 - mae: 2677.1143 - val_loss: 3175.1375 - val_mse: 75790728.0000 - val_mae: 3175.1375\n",
            "Epoch 186/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2694.2276 - mse: 51076372.0000 - mae: 2694.2278 - val_loss: 3171.2476 - val_mse: 75628696.0000 - val_mae: 3171.2473\n",
            "Epoch 187/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2675.0994 - mse: 50928628.0000 - mae: 2675.0994 - val_loss: 3175.7589 - val_mse: 75553152.0000 - val_mae: 3175.7590\n",
            "Epoch 188/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2684.3878 - mse: 51462424.0000 - mae: 2684.3879 - val_loss: 3176.7660 - val_mse: 75481976.0000 - val_mae: 3176.7664\n",
            "Epoch 189/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2675.2595 - mse: 50944152.0000 - mae: 2675.2593 - val_loss: 3166.9665 - val_mse: 75255112.0000 - val_mae: 3166.9663\n",
            "Epoch 190/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2656.3173 - mse: 50100676.0000 - mae: 2656.3169 - val_loss: 3166.2563 - val_mse: 75136488.0000 - val_mae: 3166.2563\n",
            "Epoch 191/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2686.7564 - mse: 51011028.0000 - mae: 2686.7563 - val_loss: 3167.9192 - val_mse: 75029488.0000 - val_mae: 3167.9194\n",
            "Epoch 192/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2669.6133 - mse: 49912480.0000 - mae: 2669.6135 - val_loss: 3161.8321 - val_mse: 74818904.0000 - val_mae: 3161.8323\n",
            "Epoch 193/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2661.0790 - mse: 50149864.0000 - mae: 2661.0791 - val_loss: 3156.8860 - val_mse: 74686664.0000 - val_mae: 3156.8860\n",
            "Epoch 194/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2661.9708 - mse: 49826520.0000 - mae: 2661.9709 - val_loss: 3150.7500 - val_mse: 74463368.0000 - val_mae: 3150.7502\n",
            "Epoch 195/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2676.0601 - mse: 50689208.0000 - mae: 2676.0601 - val_loss: 3155.0135 - val_mse: 74447304.0000 - val_mae: 3155.0134\n",
            "Epoch 196/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2684.3405 - mse: 50404808.0000 - mae: 2684.3406 - val_loss: 3148.9611 - val_mse: 74280776.0000 - val_mae: 3148.9614\n",
            "Epoch 197/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2656.2992 - mse: 49872760.0000 - mae: 2656.2991 - val_loss: 3156.8899 - val_mse: 74285080.0000 - val_mae: 3156.8899\n",
            "Epoch 198/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2665.2770 - mse: 50142092.0000 - mae: 2665.2771 - val_loss: 3150.0146 - val_mse: 74093672.0000 - val_mae: 3150.0144\n",
            "Epoch 199/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2641.5116 - mse: 48951804.0000 - mae: 2641.5117 - val_loss: 3143.9440 - val_mse: 73961416.0000 - val_mae: 3143.9441\n",
            "Epoch 200/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2658.4377 - mse: 49577628.0000 - mae: 2658.4377 - val_loss: 3140.8141 - val_mse: 73817176.0000 - val_mae: 3140.8140\n",
            "Epoch 201/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 2675.2986 - mse: 50615920.0000 - mae: 2675.2986 - val_loss: 3133.1430 - val_mse: 73578920.0000 - val_mae: 3133.1431\n",
            "Epoch 202/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2649.2128 - mse: 49403096.0000 - mae: 2649.2126 - val_loss: 3131.7473 - val_mse: 73474328.0000 - val_mae: 3131.7473\n",
            "Epoch 203/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2644.3203 - mse: 49529068.0000 - mae: 2644.3203 - val_loss: 3132.6581 - val_mse: 73374224.0000 - val_mae: 3132.6580\n",
            "Epoch 204/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2662.0699 - mse: 49854584.0000 - mae: 2662.0701 - val_loss: 3124.9393 - val_mse: 73149152.0000 - val_mae: 3124.9395\n",
            "Epoch 205/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2635.8116 - mse: 48596320.0000 - mae: 2635.8113 - val_loss: 3149.0161 - val_mse: 73330448.0000 - val_mae: 3149.0164\n",
            "Epoch 206/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2675.6632 - mse: 50007692.0000 - mae: 2675.6631 - val_loss: 3121.3900 - val_mse: 72906448.0000 - val_mae: 3121.3899\n",
            "Epoch 207/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2673.8618 - mse: 50055660.0000 - mae: 2673.8618 - val_loss: 3117.5338 - val_mse: 72826480.0000 - val_mae: 3117.5337\n",
            "Epoch 208/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 2670.6145 - mse: 48593100.0000 - mae: 2670.6145 - val_loss: 3115.9517 - val_mse: 72775752.0000 - val_mae: 3115.9517\n",
            "Epoch 209/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2655.6801 - mse: 49737236.0000 - mae: 2655.6799 - val_loss: 3116.6300 - val_mse: 72700904.0000 - val_mae: 3116.6299\n",
            "Epoch 210/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2645.0767 - mse: 48843040.0000 - mae: 2645.0767 - val_loss: 3114.5741 - val_mse: 72626752.0000 - val_mae: 3114.5742\n",
            "Epoch 211/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2621.1732 - mse: 48757376.0000 - mae: 2621.1733 - val_loss: 3117.1208 - val_mse: 72546992.0000 - val_mae: 3117.1211\n",
            "Epoch 212/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2675.1259 - mse: 49380064.0000 - mae: 2675.1260 - val_loss: 3114.9864 - val_mse: 72490224.0000 - val_mae: 3114.9866\n",
            "Epoch 213/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2630.9475 - mse: 48378152.0000 - mae: 2630.9473 - val_loss: 3116.1546 - val_mse: 72436384.0000 - val_mae: 3116.1550\n",
            "Epoch 214/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2636.9374 - mse: 48346184.0000 - mae: 2636.9373 - val_loss: 3103.5086 - val_mse: 72173136.0000 - val_mae: 3103.5083\n",
            "Epoch 215/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 2638.1676 - mse: 48635244.0000 - mae: 2638.1672 - val_loss: 3122.5496 - val_mse: 72343112.0000 - val_mae: 3122.5496\n",
            "Epoch 216/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2635.6277 - mse: 48305652.0000 - mae: 2635.6277 - val_loss: 3111.2690 - val_mse: 72081064.0000 - val_mae: 3111.2690\n",
            "Epoch 217/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2633.3747 - mse: 48134348.0000 - mae: 2633.3748 - val_loss: 3096.3882 - val_mse: 71822472.0000 - val_mae: 3096.3884\n",
            "Epoch 218/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2642.7062 - mse: 48506224.0000 - mae: 2642.7061 - val_loss: 3099.4660 - val_mse: 71796328.0000 - val_mae: 3099.4656\n",
            "Epoch 219/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2633.5751 - mse: 48587380.0000 - mae: 2633.5752 - val_loss: 3107.5663 - val_mse: 71789616.0000 - val_mae: 3107.5664\n",
            "Epoch 220/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2612.0331 - mse: 48103680.0000 - mae: 2612.0332 - val_loss: 3095.3866 - val_mse: 71556352.0000 - val_mae: 3095.3862\n",
            "Epoch 221/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2661.9248 - mse: 48609848.0000 - mae: 2661.9248 - val_loss: 3108.9438 - val_mse: 71633096.0000 - val_mae: 3108.9436\n",
            "Epoch 222/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2637.8858 - mse: 48136536.0000 - mae: 2637.8857 - val_loss: 3085.8415 - val_mse: 71246128.0000 - val_mae: 3085.8416\n",
            "Epoch 223/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2623.4528 - mse: 47789200.0000 - mae: 2623.4529 - val_loss: 3088.6723 - val_mse: 71299336.0000 - val_mae: 3088.6724\n",
            "Epoch 224/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2622.1169 - mse: 48049956.0000 - mae: 2622.1169 - val_loss: 3090.1509 - val_mse: 71264856.0000 - val_mae: 3090.1506\n",
            "Epoch 225/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2625.8641 - mse: 48195900.0000 - mae: 2625.8640 - val_loss: 3082.5430 - val_mse: 71099152.0000 - val_mae: 3082.5430\n",
            "Epoch 226/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2616.4515 - mse: 47911844.0000 - mae: 2616.4514 - val_loss: 3088.9255 - val_mse: 71076464.0000 - val_mae: 3088.9253\n",
            "Epoch 227/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2621.7297 - mse: 47871760.0000 - mae: 2621.7297 - val_loss: 3089.7078 - val_mse: 71018736.0000 - val_mae: 3089.7080\n",
            "Epoch 228/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2636.4817 - mse: 48543360.0000 - mae: 2636.4819 - val_loss: 3082.9605 - val_mse: 70944800.0000 - val_mae: 3082.9600\n",
            "Epoch 229/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2605.6939 - mse: 47234192.0000 - mae: 2605.6938 - val_loss: 3075.5396 - val_mse: 70750640.0000 - val_mae: 3075.5400\n",
            "Epoch 230/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2639.9566 - mse: 48223244.0000 - mae: 2639.9568 - val_loss: 3072.8635 - val_mse: 70668808.0000 - val_mae: 3072.8633\n",
            "Epoch 231/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2621.2485 - mse: 47459080.0000 - mae: 2621.2488 - val_loss: 3072.4441 - val_mse: 70625480.0000 - val_mae: 3072.4441\n",
            "Epoch 232/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2622.9847 - mse: 47693200.0000 - mae: 2622.9846 - val_loss: 3076.2695 - val_mse: 70583264.0000 - val_mae: 3076.2690\n",
            "Epoch 233/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2626.1334 - mse: 47523944.0000 - mae: 2626.1335 - val_loss: 3062.7178 - val_mse: 70337536.0000 - val_mae: 3062.7180\n",
            "Epoch 234/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2605.7095 - mse: 47619956.0000 - mae: 2605.7097 - val_loss: 3067.1542 - val_mse: 70394008.0000 - val_mae: 3067.1543\n",
            "Epoch 235/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2624.1080 - mse: 47475372.0000 - mae: 2624.1079 - val_loss: 3067.5522 - val_mse: 70402240.0000 - val_mae: 3067.5520\n",
            "Epoch 236/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 2615.4294 - mse: 47027800.0000 - mae: 2615.4294 - val_loss: 3063.0297 - val_mse: 70294544.0000 - val_mae: 3063.0298\n",
            "Epoch 237/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2616.5162 - mse: 47300880.0000 - mae: 2616.5161 - val_loss: 3067.9437 - val_mse: 70240248.0000 - val_mae: 3067.9436\n",
            "Epoch 238/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2609.9843 - mse: 46651048.0000 - mae: 2609.9844 - val_loss: 3060.4992 - val_mse: 70028080.0000 - val_mae: 3060.4990\n",
            "Epoch 239/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2611.0144 - mse: 47346532.0000 - mae: 2611.0146 - val_loss: 3056.7464 - val_mse: 69983752.0000 - val_mae: 3056.7463\n",
            "Epoch 240/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2600.8852 - mse: 47318544.0000 - mae: 2600.8853 - val_loss: 3052.8298 - val_mse: 69839768.0000 - val_mae: 3052.8301\n",
            "Epoch 241/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2617.1147 - mse: 47376832.0000 - mae: 2617.1145 - val_loss: 3043.7538 - val_mse: 69657360.0000 - val_mae: 3043.7539\n",
            "Epoch 242/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2592.6128 - mse: 47103508.0000 - mae: 2592.6130 - val_loss: 3039.1714 - val_mse: 69471048.0000 - val_mae: 3039.1714\n",
            "Epoch 243/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2635.7919 - mse: 47803336.0000 - mae: 2635.7920 - val_loss: 3046.2823 - val_mse: 69567504.0000 - val_mae: 3046.2822\n",
            "Epoch 244/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2599.0527 - mse: 46801132.0000 - mae: 2599.0527 - val_loss: 3048.6357 - val_mse: 69519384.0000 - val_mae: 3048.6357\n",
            "Epoch 245/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2602.3459 - mse: 46645288.0000 - mae: 2602.3459 - val_loss: 3057.7736 - val_mse: 69606008.0000 - val_mae: 3057.7739\n",
            "Epoch 246/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2601.9274 - mse: 47381364.0000 - mae: 2601.9275 - val_loss: 3051.8550 - val_mse: 69450080.0000 - val_mae: 3051.8547\n",
            "Epoch 247/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2586.5081 - mse: 46515108.0000 - mae: 2586.5081 - val_loss: 3046.3331 - val_mse: 69336584.0000 - val_mae: 3046.3330\n",
            "Epoch 248/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2596.7703 - mse: 47575164.0000 - mae: 2596.7703 - val_loss: 3043.4710 - val_mse: 69242624.0000 - val_mae: 3043.4709\n",
            "Epoch 249/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2555.3922 - mse: 46559280.0000 - mae: 2555.3921 - val_loss: 3024.2376 - val_mse: 68912440.0000 - val_mae: 3024.2375\n",
            "Epoch 250/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2595.5085 - mse: 47027152.0000 - mae: 2595.5085 - val_loss: 3024.4508 - val_mse: 68832928.0000 - val_mae: 3024.4507\n",
            "Epoch 251/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2598.5302 - mse: 46884224.0000 - mae: 2598.5303 - val_loss: 3036.5818 - val_mse: 68927104.0000 - val_mae: 3036.5818\n",
            "Epoch 252/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2562.8636 - mse: 46075368.0000 - mae: 2562.8638 - val_loss: 3028.9434 - val_mse: 68791664.0000 - val_mae: 3028.9436\n",
            "Epoch 253/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2584.0422 - mse: 46945260.0000 - mae: 2584.0422 - val_loss: 3022.2053 - val_mse: 68723000.0000 - val_mae: 3022.2053\n",
            "Epoch 254/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2579.5050 - mse: 46236136.0000 - mae: 2579.5049 - val_loss: 3014.0727 - val_mse: 68540432.0000 - val_mae: 3014.0728\n",
            "Epoch 255/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2585.1858 - mse: 45749032.0000 - mae: 2585.1858 - val_loss: 3028.7636 - val_mse: 68703736.0000 - val_mae: 3028.7637\n",
            "Epoch 256/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2561.5971 - mse: 46267472.0000 - mae: 2561.5972 - val_loss: 3009.2996 - val_mse: 68374472.0000 - val_mae: 3009.2996\n",
            "Epoch 257/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2579.7992 - mse: 45710520.0000 - mae: 2579.7993 - val_loss: 3024.0305 - val_mse: 68452928.0000 - val_mae: 3024.0308\n",
            "Epoch 258/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2576.8258 - mse: 46441364.0000 - mae: 2576.8259 - val_loss: 3011.7038 - val_mse: 68265208.0000 - val_mae: 3011.7036\n",
            "Epoch 259/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2577.6827 - mse: 46006568.0000 - mae: 2577.6826 - val_loss: 3015.8721 - val_mse: 68334336.0000 - val_mae: 3015.8716\n",
            "Epoch 260/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2570.9254 - mse: 46243260.0000 - mae: 2570.9253 - val_loss: 3011.2577 - val_mse: 68242088.0000 - val_mae: 3011.2581\n",
            "Epoch 261/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2574.1896 - mse: 45831912.0000 - mae: 2574.1895 - val_loss: 3002.7858 - val_mse: 68111104.0000 - val_mae: 3002.7859\n",
            "Epoch 262/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2574.2127 - mse: 46480408.0000 - mae: 2574.2129 - val_loss: 2996.4180 - val_mse: 67966368.0000 - val_mae: 2996.4180\n",
            "Epoch 263/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2558.5581 - mse: 46382396.0000 - mae: 2558.5581 - val_loss: 2989.5889 - val_mse: 67724472.0000 - val_mae: 2989.5891\n",
            "Epoch 264/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2529.3671 - mse: 45183180.0000 - mae: 2529.3674 - val_loss: 2998.5551 - val_mse: 67884224.0000 - val_mae: 2998.5549\n",
            "Epoch 265/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2549.5709 - mse: 45541800.0000 - mae: 2549.5708 - val_loss: 2990.3495 - val_mse: 67634752.0000 - val_mae: 2990.3496\n",
            "Epoch 266/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2523.9101 - mse: 45009932.0000 - mae: 2523.9099 - val_loss: 2994.1542 - val_mse: 67580984.0000 - val_mae: 2994.1543\n",
            "Epoch 267/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2519.6610 - mse: 45252748.0000 - mae: 2519.6609 - val_loss: 2983.9843 - val_mse: 67337784.0000 - val_mae: 2983.9844\n",
            "Epoch 268/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2524.5204 - mse: 44562624.0000 - mae: 2524.5203 - val_loss: 2992.9685 - val_mse: 67392720.0000 - val_mae: 2992.9688\n",
            "Epoch 269/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2545.5341 - mse: 45382584.0000 - mae: 2545.5342 - val_loss: 2984.7498 - val_mse: 67257728.0000 - val_mae: 2984.7500\n",
            "Epoch 270/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2546.4945 - mse: 45878904.0000 - mae: 2546.4946 - val_loss: 3002.4217 - val_mse: 67427576.0000 - val_mae: 3002.4216\n",
            "Epoch 271/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2535.0458 - mse: 46230496.0000 - mae: 2535.0457 - val_loss: 2985.5361 - val_mse: 67158760.0000 - val_mae: 2985.5359\n",
            "Epoch 272/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2548.0063 - mse: 45099840.0000 - mae: 2548.0063 - val_loss: 2970.2559 - val_mse: 66839824.0000 - val_mae: 2970.2561\n",
            "Epoch 273/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2520.3124 - mse: 44908048.0000 - mae: 2520.3123 - val_loss: 2975.2097 - val_mse: 66778536.0000 - val_mae: 2975.2097\n",
            "Epoch 274/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2524.5999 - mse: 45245736.0000 - mae: 2524.6001 - val_loss: 2982.2216 - val_mse: 66895456.0000 - val_mae: 2982.2217\n",
            "Epoch 275/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2526.2615 - mse: 44447308.0000 - mae: 2526.2612 - val_loss: 2980.0889 - val_mse: 66697324.0000 - val_mae: 2980.0891\n",
            "Epoch 276/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2492.8632 - mse: 44454640.0000 - mae: 2492.8633 - val_loss: 2981.1220 - val_mse: 66608464.0000 - val_mae: 2981.1221\n",
            "Epoch 277/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2535.2725 - mse: 44833996.0000 - mae: 2535.2725 - val_loss: 2971.2614 - val_mse: 66441996.0000 - val_mae: 2971.2612\n",
            "Epoch 278/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2514.5757 - mse: 44576884.0000 - mae: 2514.5757 - val_loss: 2964.6882 - val_mse: 66222032.0000 - val_mae: 2964.6880\n",
            "Epoch 279/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2526.8049 - mse: 44347196.0000 - mae: 2526.8049 - val_loss: 2956.0905 - val_mse: 66143916.0000 - val_mae: 2956.0906\n",
            "Epoch 280/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2471.3431 - mse: 43461964.0000 - mae: 2471.3430 - val_loss: 2957.2703 - val_mse: 66044740.0000 - val_mae: 2957.2703\n",
            "Epoch 281/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2529.1195 - mse: 44771548.0000 - mae: 2529.1194 - val_loss: 2967.5204 - val_mse: 66086392.0000 - val_mae: 2967.5203\n",
            "Epoch 282/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2500.6033 - mse: 44316144.0000 - mae: 2500.6033 - val_loss: 2960.2429 - val_mse: 65875820.0000 - val_mae: 2960.2427\n",
            "Epoch 283/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2502.8904 - mse: 44178608.0000 - mae: 2502.8904 - val_loss: 2958.6839 - val_mse: 65726708.0000 - val_mae: 2958.6841\n",
            "Epoch 284/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2487.4978 - mse: 44014140.0000 - mae: 2487.4978 - val_loss: 2969.0782 - val_mse: 65762620.0000 - val_mae: 2969.0784\n",
            "Epoch 285/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2516.2334 - mse: 44268628.0000 - mae: 2516.2336 - val_loss: 2947.4856 - val_mse: 65485560.0000 - val_mae: 2947.4854\n",
            "Epoch 286/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2501.1450 - mse: 44108200.0000 - mae: 2501.1450 - val_loss: 2949.2060 - val_mse: 65358168.0000 - val_mae: 2949.2061\n",
            "Epoch 287/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2494.2955 - mse: 43777684.0000 - mae: 2494.2957 - val_loss: 2969.9834 - val_mse: 65595080.0000 - val_mae: 2969.9834\n",
            "Epoch 288/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2475.7222 - mse: 43221988.0000 - mae: 2475.7224 - val_loss: 2948.4623 - val_mse: 65255112.0000 - val_mae: 2948.4624\n",
            "Epoch 289/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2466.2048 - mse: 43102580.0000 - mae: 2466.2048 - val_loss: 2951.8360 - val_mse: 65069456.0000 - val_mae: 2951.8359\n",
            "Epoch 290/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2515.8918 - mse: 44255680.0000 - mae: 2515.8916 - val_loss: 2942.3406 - val_mse: 64809288.0000 - val_mae: 2942.3406\n",
            "Epoch 291/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2513.9628 - mse: 44209008.0000 - mae: 2513.9629 - val_loss: 2957.5992 - val_mse: 64947704.0000 - val_mae: 2957.5991\n",
            "Epoch 292/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2486.7379 - mse: 43403028.0000 - mae: 2486.7378 - val_loss: 2958.9870 - val_mse: 64870308.0000 - val_mae: 2958.9871\n",
            "Epoch 293/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2451.8694 - mse: 42574952.0000 - mae: 2451.8694 - val_loss: 2963.8154 - val_mse: 64786552.0000 - val_mae: 2963.8154\n",
            "Epoch 294/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2504.8961 - mse: 43661900.0000 - mae: 2504.8962 - val_loss: 2947.1516 - val_mse: 64661252.0000 - val_mae: 2947.1516\n",
            "Epoch 295/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2453.7406 - mse: 41990140.0000 - mae: 2453.7407 - val_loss: 2941.0183 - val_mse: 64401704.0000 - val_mae: 2941.0181\n",
            "Epoch 296/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2471.1954 - mse: 43267576.0000 - mae: 2471.1953 - val_loss: 2950.6134 - val_mse: 64402540.0000 - val_mae: 2950.6133\n",
            "Epoch 297/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2463.9495 - mse: 42085820.0000 - mae: 2463.9492 - val_loss: 2952.2671 - val_mse: 64397700.0000 - val_mae: 2952.2666\n",
            "Epoch 298/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2446.2628 - mse: 42404936.0000 - mae: 2446.2629 - val_loss: 2942.2217 - val_mse: 64116092.0000 - val_mae: 2942.2217\n",
            "Epoch 299/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2481.6155 - mse: 43084792.0000 - mae: 2481.6155 - val_loss: 2947.0876 - val_mse: 64067664.0000 - val_mae: 2947.0876\n",
            "Epoch 300/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2483.7734 - mse: 43273792.0000 - mae: 2483.7734 - val_loss: 2951.9870 - val_mse: 64129672.0000 - val_mae: 2951.9873\n",
            "Epoch 301/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2483.0259 - mse: 43267772.0000 - mae: 2483.0259 - val_loss: 2938.3096 - val_mse: 63834544.0000 - val_mae: 2938.3096\n",
            "Epoch 302/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2455.9644 - mse: 42773568.0000 - mae: 2455.9644 - val_loss: 2940.7308 - val_mse: 63776972.0000 - val_mae: 2940.7307\n",
            "Epoch 303/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2473.5726 - mse: 42638372.0000 - mae: 2473.5728 - val_loss: 2926.1633 - val_mse: 63437580.0000 - val_mae: 2926.1631\n",
            "Epoch 304/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2458.3163 - mse: 41429984.0000 - mae: 2458.3164 - val_loss: 2927.2659 - val_mse: 63441820.0000 - val_mae: 2927.2661\n",
            "Epoch 305/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2448.6621 - mse: 42575808.0000 - mae: 2448.6621 - val_loss: 2932.2609 - val_mse: 63407048.0000 - val_mae: 2932.2610\n",
            "Epoch 306/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2445.0120 - mse: 42102392.0000 - mae: 2445.0120 - val_loss: 2935.2377 - val_mse: 63419104.0000 - val_mae: 2935.2375\n",
            "Epoch 307/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2497.1830 - mse: 42821156.0000 - mae: 2497.1831 - val_loss: 2935.4597 - val_mse: 63358224.0000 - val_mae: 2935.4597\n",
            "Epoch 308/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2447.5736 - mse: 41415332.0000 - mae: 2447.5737 - val_loss: 2927.9069 - val_mse: 63086232.0000 - val_mae: 2927.9070\n",
            "Epoch 309/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2470.9295 - mse: 42043532.0000 - mae: 2470.9294 - val_loss: 2921.9711 - val_mse: 62792428.0000 - val_mae: 2921.9707\n",
            "Epoch 310/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2460.1835 - mse: 42245236.0000 - mae: 2460.1836 - val_loss: 2926.6497 - val_mse: 62922188.0000 - val_mae: 2926.6497\n",
            "Epoch 311/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2452.4764 - mse: 41761660.0000 - mae: 2452.4766 - val_loss: 2928.8675 - val_mse: 62856000.0000 - val_mae: 2928.8674\n",
            "Epoch 312/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2457.5679 - mse: 41439020.0000 - mae: 2457.5681 - val_loss: 2933.6776 - val_mse: 62918436.0000 - val_mae: 2933.6777\n",
            "Epoch 313/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2444.0397 - mse: 41989320.0000 - mae: 2444.0396 - val_loss: 2928.4416 - val_mse: 62721744.0000 - val_mae: 2928.4417\n",
            "Epoch 314/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2441.2874 - mse: 41269648.0000 - mae: 2441.2874 - val_loss: 2923.8713 - val_mse: 62577348.0000 - val_mae: 2923.8713\n",
            "Epoch 315/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2471.1294 - mse: 42803276.0000 - mae: 2471.1294 - val_loss: 2921.0986 - val_mse: 62471464.0000 - val_mae: 2921.0986\n",
            "Epoch 316/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2456.2407 - mse: 40654360.0000 - mae: 2456.2407 - val_loss: 2928.5902 - val_mse: 62561140.0000 - val_mae: 2928.5903\n",
            "Epoch 317/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2423.5143 - mse: 41743620.0000 - mae: 2423.5144 - val_loss: 2932.2968 - val_mse: 62441308.0000 - val_mae: 2932.2966\n",
            "Epoch 318/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2443.0439 - mse: 42016392.0000 - mae: 2443.0439 - val_loss: 2935.6685 - val_mse: 62416064.0000 - val_mae: 2935.6687\n",
            "Epoch 319/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2428.8974 - mse: 41770280.0000 - mae: 2428.8972 - val_loss: 2919.9754 - val_mse: 62036580.0000 - val_mae: 2919.9753\n",
            "Epoch 320/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2448.1128 - mse: 42021276.0000 - mae: 2448.1130 - val_loss: 2920.4951 - val_mse: 62101520.0000 - val_mae: 2920.4951\n",
            "Epoch 321/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2430.8008 - mse: 41109436.0000 - mae: 2430.8008 - val_loss: 2918.9343 - val_mse: 61960060.0000 - val_mae: 2918.9343\n",
            "Epoch 322/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2416.8436 - mse: 40413984.0000 - mae: 2416.8438 - val_loss: 2918.3858 - val_mse: 61731024.0000 - val_mae: 2918.3857\n",
            "Epoch 323/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2441.4113 - mse: 41411968.0000 - mae: 2441.4111 - val_loss: 2917.5054 - val_mse: 61649436.0000 - val_mae: 2917.5054\n",
            "Epoch 324/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2485.2047 - mse: 42126936.0000 - mae: 2485.2046 - val_loss: 2919.0766 - val_mse: 61765676.0000 - val_mae: 2919.0767\n",
            "Epoch 325/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2437.4163 - mse: 41148308.0000 - mae: 2437.4163 - val_loss: 2937.1681 - val_mse: 61776784.0000 - val_mae: 2937.1682\n",
            "Epoch 326/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2421.1060 - mse: 40201328.0000 - mae: 2421.1060 - val_loss: 2935.7099 - val_mse: 61617640.0000 - val_mae: 2935.7100\n",
            "Epoch 327/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2422.0410 - mse: 40671648.0000 - mae: 2422.0410 - val_loss: 2911.7516 - val_mse: 61352668.0000 - val_mae: 2911.7517\n",
            "Epoch 328/500\n",
            "1500/1500 [==============================] - 0s 91us/step - loss: 2415.4009 - mse: 40343520.0000 - mae: 2415.4009 - val_loss: 2916.1828 - val_mse: 61299260.0000 - val_mae: 2916.1826\n",
            "Epoch 329/500\n",
            "1500/1500 [==============================] - 0s 87us/step - loss: 2433.9752 - mse: 41472424.0000 - mae: 2433.9751 - val_loss: 2910.0016 - val_mse: 61293476.0000 - val_mae: 2910.0017\n",
            "Epoch 330/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2429.6961 - mse: 40477468.0000 - mae: 2429.6960 - val_loss: 2910.5324 - val_mse: 61066468.0000 - val_mae: 2910.5322\n",
            "Epoch 331/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2449.9410 - mse: 41146192.0000 - mae: 2449.9409 - val_loss: 2902.2467 - val_mse: 60857008.0000 - val_mae: 2902.2471\n",
            "Epoch 332/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2398.3712 - mse: 39679636.0000 - mae: 2398.3711 - val_loss: 2898.6788 - val_mse: 60750980.0000 - val_mae: 2898.6787\n",
            "Epoch 333/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2402.1311 - mse: 39714140.0000 - mae: 2402.1311 - val_loss: 2901.4024 - val_mse: 60648828.0000 - val_mae: 2901.4026\n",
            "Epoch 334/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2412.5647 - mse: 40120524.0000 - mae: 2412.5647 - val_loss: 2900.5759 - val_mse: 60657452.0000 - val_mae: 2900.5759\n",
            "Epoch 335/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2391.4976 - mse: 39546720.0000 - mae: 2391.4976 - val_loss: 2904.8003 - val_mse: 60650948.0000 - val_mae: 2904.8000\n",
            "Epoch 336/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2407.5699 - mse: 40098388.0000 - mae: 2407.5698 - val_loss: 2903.9053 - val_mse: 60503868.0000 - val_mae: 2903.9053\n",
            "Epoch 337/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2412.3230 - mse: 40847356.0000 - mae: 2412.3228 - val_loss: 2898.5251 - val_mse: 60294484.0000 - val_mae: 2898.5249\n",
            "Epoch 338/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2407.3514 - mse: 39780020.0000 - mae: 2407.3511 - val_loss: 2898.8164 - val_mse: 60192848.0000 - val_mae: 2898.8164\n",
            "Epoch 339/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2431.3144 - mse: 40502324.0000 - mae: 2431.3145 - val_loss: 2896.3604 - val_mse: 60187912.0000 - val_mae: 2896.3604\n",
            "Epoch 340/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2397.5564 - mse: 39903696.0000 - mae: 2397.5564 - val_loss: 2900.0283 - val_mse: 60024508.0000 - val_mae: 2900.0283\n",
            "Epoch 341/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2394.2536 - mse: 38512724.0000 - mae: 2394.2537 - val_loss: 2896.4917 - val_mse: 59852192.0000 - val_mae: 2896.4917\n",
            "Epoch 342/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2394.2165 - mse: 39924052.0000 - mae: 2394.2166 - val_loss: 2890.4144 - val_mse: 59625560.0000 - val_mae: 2890.4146\n",
            "Epoch 343/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2397.5011 - mse: 39362540.0000 - mae: 2397.5010 - val_loss: 2893.9413 - val_mse: 59595800.0000 - val_mae: 2893.9414\n",
            "Epoch 344/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2406.6508 - mse: 39533400.0000 - mae: 2406.6506 - val_loss: 2899.3377 - val_mse: 59631008.0000 - val_mae: 2899.3376\n",
            "Epoch 345/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2378.0210 - mse: 39035672.0000 - mae: 2378.0208 - val_loss: 2883.3896 - val_mse: 59169668.0000 - val_mae: 2883.3896\n",
            "Epoch 346/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2375.4042 - mse: 37901564.0000 - mae: 2375.4041 - val_loss: 2892.5212 - val_mse: 59219236.0000 - val_mae: 2892.5210\n",
            "Epoch 347/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2368.0967 - mse: 38201392.0000 - mae: 2368.0969 - val_loss: 2892.3241 - val_mse: 59224060.0000 - val_mae: 2892.3240\n",
            "Epoch 348/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2413.7733 - mse: 40396120.0000 - mae: 2413.7734 - val_loss: 2893.7079 - val_mse: 59231176.0000 - val_mae: 2893.7080\n",
            "Epoch 349/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2409.3169 - mse: 39732184.0000 - mae: 2409.3169 - val_loss: 2904.3323 - val_mse: 59316468.0000 - val_mae: 2904.3323\n",
            "Epoch 350/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2395.2027 - mse: 39538684.0000 - mae: 2395.2026 - val_loss: 2881.7054 - val_mse: 58716740.0000 - val_mae: 2881.7053\n",
            "Epoch 351/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2361.2359 - mse: 38038208.0000 - mae: 2361.2358 - val_loss: 2880.4921 - val_mse: 58688460.0000 - val_mae: 2880.4924\n",
            "Epoch 352/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2395.8598 - mse: 38258804.0000 - mae: 2395.8596 - val_loss: 2878.8913 - val_mse: 58739812.0000 - val_mae: 2878.8911\n",
            "Epoch 353/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2378.6842 - mse: 38789960.0000 - mae: 2378.6841 - val_loss: 2885.8423 - val_mse: 58605020.0000 - val_mae: 2885.8423\n",
            "Epoch 354/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2362.9427 - mse: 38542304.0000 - mae: 2362.9426 - val_loss: 2885.4821 - val_mse: 58558340.0000 - val_mae: 2885.4819\n",
            "Epoch 355/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2407.3634 - mse: 39414436.0000 - mae: 2407.3635 - val_loss: 2874.5403 - val_mse: 58245848.0000 - val_mae: 2874.5408\n",
            "Epoch 356/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2363.0409 - mse: 37419892.0000 - mae: 2363.0408 - val_loss: 2877.0711 - val_mse: 58045024.0000 - val_mae: 2877.0713\n",
            "Epoch 357/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2374.4717 - mse: 38088168.0000 - mae: 2374.4717 - val_loss: 2874.9120 - val_mse: 58098400.0000 - val_mae: 2874.9121\n",
            "Epoch 358/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2329.0131 - mse: 36790444.0000 - mae: 2329.0132 - val_loss: 2868.8463 - val_mse: 57646012.0000 - val_mae: 2868.8464\n",
            "Epoch 359/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2342.6283 - mse: 38016616.0000 - mae: 2342.6284 - val_loss: 2879.9584 - val_mse: 58044416.0000 - val_mae: 2879.9583\n",
            "Epoch 360/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2356.5871 - mse: 36942004.0000 - mae: 2356.5869 - val_loss: 2876.5836 - val_mse: 57702620.0000 - val_mae: 2876.5837\n",
            "Epoch 361/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2363.8659 - mse: 37323308.0000 - mae: 2363.8657 - val_loss: 2874.9165 - val_mse: 57621164.0000 - val_mae: 2874.9163\n",
            "Epoch 362/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2345.5280 - mse: 37729116.0000 - mae: 2345.5281 - val_loss: 2861.4625 - val_mse: 57213768.0000 - val_mae: 2861.4624\n",
            "Epoch 363/500\n",
            "1500/1500 [==============================] - 0s 93us/step - loss: 2353.8732 - mse: 36980956.0000 - mae: 2353.8733 - val_loss: 2856.7560 - val_mse: 57074964.0000 - val_mae: 2856.7561\n",
            "Epoch 364/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2342.8783 - mse: 37590672.0000 - mae: 2342.8784 - val_loss: 2865.0190 - val_mse: 57162524.0000 - val_mae: 2865.0193\n",
            "Epoch 365/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2327.9456 - mse: 36940536.0000 - mae: 2327.9456 - val_loss: 2853.7084 - val_mse: 56622568.0000 - val_mae: 2853.7087\n",
            "Epoch 366/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2388.5210 - mse: 38505612.0000 - mae: 2388.5210 - val_loss: 2849.0643 - val_mse: 56444372.0000 - val_mae: 2849.0645\n",
            "Epoch 367/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2277.3373 - mse: 35228780.0000 - mae: 2277.3372 - val_loss: 2859.3669 - val_mse: 56593596.0000 - val_mae: 2859.3669\n",
            "Epoch 368/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2315.9779 - mse: 36325776.0000 - mae: 2315.9780 - val_loss: 2845.4656 - val_mse: 56133108.0000 - val_mae: 2845.4656\n",
            "Epoch 369/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2334.3422 - mse: 36705464.0000 - mae: 2334.3423 - val_loss: 2858.3924 - val_mse: 56387048.0000 - val_mae: 2858.3923\n",
            "Epoch 370/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2336.9526 - mse: 37623408.0000 - mae: 2336.9526 - val_loss: 2848.2245 - val_mse: 56164024.0000 - val_mae: 2848.2246\n",
            "Epoch 371/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2344.6131 - mse: 36243712.0000 - mae: 2344.6130 - val_loss: 2845.9649 - val_mse: 55855772.0000 - val_mae: 2845.9651\n",
            "Epoch 372/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2299.3583 - mse: 35972256.0000 - mae: 2299.3584 - val_loss: 2850.6284 - val_mse: 55944600.0000 - val_mae: 2850.6287\n",
            "Epoch 373/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2309.9897 - mse: 35938344.0000 - mae: 2309.9897 - val_loss: 2846.9303 - val_mse: 55686096.0000 - val_mae: 2846.9304\n",
            "Epoch 374/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2317.8210 - mse: 35961656.0000 - mae: 2317.8210 - val_loss: 2843.5404 - val_mse: 55503000.0000 - val_mae: 2843.5403\n",
            "Epoch 375/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2314.1896 - mse: 35485572.0000 - mae: 2314.1897 - val_loss: 2845.1039 - val_mse: 55608508.0000 - val_mae: 2845.1040\n",
            "Epoch 376/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2313.9482 - mse: 36203092.0000 - mae: 2313.9482 - val_loss: 2837.0443 - val_mse: 55218296.0000 - val_mae: 2837.0444\n",
            "Epoch 377/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2330.8706 - mse: 36116128.0000 - mae: 2330.8706 - val_loss: 2834.4350 - val_mse: 55097596.0000 - val_mae: 2834.4351\n",
            "Epoch 378/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2290.6073 - mse: 35717068.0000 - mae: 2290.6072 - val_loss: 2827.6720 - val_mse: 54925776.0000 - val_mae: 2827.6721\n",
            "Epoch 379/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2310.2413 - mse: 35838508.0000 - mae: 2310.2415 - val_loss: 2826.1437 - val_mse: 54636808.0000 - val_mae: 2826.1440\n",
            "Epoch 380/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 2296.3523 - mse: 35070112.0000 - mae: 2296.3525 - val_loss: 2828.3531 - val_mse: 54774340.0000 - val_mae: 2828.3533\n",
            "Epoch 381/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2295.6595 - mse: 35829488.0000 - mae: 2295.6594 - val_loss: 2822.9437 - val_mse: 54534468.0000 - val_mae: 2822.9436\n",
            "Epoch 382/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2301.4680 - mse: 34993940.0000 - mae: 2301.4680 - val_loss: 2824.2960 - val_mse: 54295708.0000 - val_mae: 2824.2957\n",
            "Epoch 383/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2281.4033 - mse: 35074696.0000 - mae: 2281.4033 - val_loss: 2822.8236 - val_mse: 54085284.0000 - val_mae: 2822.8237\n",
            "Epoch 384/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2276.5088 - mse: 35139632.0000 - mae: 2276.5088 - val_loss: 2828.4141 - val_mse: 53998604.0000 - val_mae: 2828.4141\n",
            "Epoch 385/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2268.1923 - mse: 34515724.0000 - mae: 2268.1924 - val_loss: 2816.1360 - val_mse: 53812564.0000 - val_mae: 2816.1360\n",
            "Epoch 386/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2311.6265 - mse: 36015852.0000 - mae: 2311.6267 - val_loss: 2815.1405 - val_mse: 53717004.0000 - val_mae: 2815.1404\n",
            "Epoch 387/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2281.2030 - mse: 34905920.0000 - mae: 2281.2029 - val_loss: 2827.3412 - val_mse: 53775816.0000 - val_mae: 2827.3413\n",
            "Epoch 388/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2310.3431 - mse: 35828408.0000 - mae: 2310.3430 - val_loss: 2809.2620 - val_mse: 53616944.0000 - val_mae: 2809.2620\n",
            "Epoch 389/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2227.1766 - mse: 32871894.0000 - mae: 2227.1765 - val_loss: 2808.4603 - val_mse: 53232264.0000 - val_mae: 2808.4604\n",
            "Epoch 390/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2283.9897 - mse: 34111776.0000 - mae: 2283.9897 - val_loss: 2812.2860 - val_mse: 53272652.0000 - val_mae: 2812.2859\n",
            "Epoch 391/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2307.5170 - mse: 36669968.0000 - mae: 2307.5171 - val_loss: 2801.2989 - val_mse: 53051544.0000 - val_mae: 2801.2991\n",
            "Epoch 392/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2268.9864 - mse: 33958552.0000 - mae: 2268.9863 - val_loss: 2796.9667 - val_mse: 52778004.0000 - val_mae: 2796.9670\n",
            "Epoch 393/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2272.9671 - mse: 34543096.0000 - mae: 2272.9673 - val_loss: 2800.5157 - val_mse: 52589512.0000 - val_mae: 2800.5161\n",
            "Epoch 394/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2296.6900 - mse: 34697092.0000 - mae: 2296.6899 - val_loss: 2805.5527 - val_mse: 52645736.0000 - val_mae: 2805.5527\n",
            "Epoch 395/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2245.2687 - mse: 33060258.0000 - mae: 2245.2686 - val_loss: 2805.8261 - val_mse: 52607440.0000 - val_mae: 2805.8259\n",
            "Epoch 396/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2303.3998 - mse: 34774796.0000 - mae: 2303.3997 - val_loss: 2804.6535 - val_mse: 52525136.0000 - val_mae: 2804.6533\n",
            "Epoch 397/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2237.0809 - mse: 33059068.0000 - mae: 2237.0808 - val_loss: 2799.5187 - val_mse: 52517476.0000 - val_mae: 2799.5186\n",
            "Epoch 398/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2230.6199 - mse: 33778072.0000 - mae: 2230.6201 - val_loss: 2790.1463 - val_mse: 51821084.0000 - val_mae: 2790.1465\n",
            "Epoch 399/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2234.6814 - mse: 33837980.0000 - mae: 2234.6814 - val_loss: 2774.9175 - val_mse: 51522256.0000 - val_mae: 2774.9175\n",
            "Epoch 400/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2253.1331 - mse: 33669196.0000 - mae: 2253.1331 - val_loss: 2786.6093 - val_mse: 51727592.0000 - val_mae: 2786.6094\n",
            "Epoch 401/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2268.5860 - mse: 34605708.0000 - mae: 2268.5859 - val_loss: 2790.9591 - val_mse: 51896904.0000 - val_mae: 2790.9592\n",
            "Epoch 402/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2229.7973 - mse: 33681888.0000 - mae: 2229.7971 - val_loss: 2771.1364 - val_mse: 51068420.0000 - val_mae: 2771.1365\n",
            "Epoch 403/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2234.1958 - mse: 33008446.0000 - mae: 2234.1956 - val_loss: 2771.1504 - val_mse: 50991460.0000 - val_mae: 2771.1504\n",
            "Epoch 404/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2223.5514 - mse: 33834524.0000 - mae: 2223.5515 - val_loss: 2774.3082 - val_mse: 51189868.0000 - val_mae: 2774.3081\n",
            "Epoch 405/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2256.1483 - mse: 34853144.0000 - mae: 2256.1482 - val_loss: 2773.6433 - val_mse: 50864512.0000 - val_mae: 2773.6433\n",
            "Epoch 406/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2197.5301 - mse: 32401580.0000 - mae: 2197.5300 - val_loss: 2791.0808 - val_mse: 51030244.0000 - val_mae: 2791.0806\n",
            "Epoch 407/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2214.7023 - mse: 32520448.0000 - mae: 2214.7024 - val_loss: 2776.9989 - val_mse: 50687100.0000 - val_mae: 2776.9993\n",
            "Epoch 408/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2242.4790 - mse: 33543032.0000 - mae: 2242.4790 - val_loss: 2772.5561 - val_mse: 50593892.0000 - val_mae: 2772.5564\n",
            "Epoch 409/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2218.1005 - mse: 32097286.0000 - mae: 2218.1006 - val_loss: 2767.9783 - val_mse: 50456296.0000 - val_mae: 2767.9783\n",
            "Epoch 410/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2254.7093 - mse: 33788692.0000 - mae: 2254.7092 - val_loss: 2771.5042 - val_mse: 50405344.0000 - val_mae: 2771.5044\n",
            "Epoch 411/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2183.1802 - mse: 32158988.0000 - mae: 2183.1802 - val_loss: 2758.2402 - val_mse: 50084724.0000 - val_mae: 2758.2400\n",
            "Epoch 412/500\n",
            "1500/1500 [==============================] - 0s 87us/step - loss: 2200.3581 - mse: 32373684.0000 - mae: 2200.3582 - val_loss: 2760.6554 - val_mse: 49970840.0000 - val_mae: 2760.6553\n",
            "Epoch 413/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2165.8484 - mse: 31859810.0000 - mae: 2165.8484 - val_loss: 2753.9091 - val_mse: 49717784.0000 - val_mae: 2753.9089\n",
            "Epoch 414/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2212.8210 - mse: 32070456.0000 - mae: 2212.8210 - val_loss: 2751.2792 - val_mse: 49321356.0000 - val_mae: 2751.2793\n",
            "Epoch 415/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2171.3533 - mse: 31419614.0000 - mae: 2171.3533 - val_loss: 2747.0877 - val_mse: 49259144.0000 - val_mae: 2747.0879\n",
            "Epoch 416/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2236.4548 - mse: 32568844.0000 - mae: 2236.4548 - val_loss: 2745.0002 - val_mse: 49222720.0000 - val_mae: 2745.0002\n",
            "Epoch 417/500\n",
            "1500/1500 [==============================] - 0s 91us/step - loss: 2171.3055 - mse: 32334072.0000 - mae: 2171.3054 - val_loss: 2744.1867 - val_mse: 49073532.0000 - val_mae: 2744.1868\n",
            "Epoch 418/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2178.3449 - mse: 32081784.0000 - mae: 2178.3450 - val_loss: 2750.1383 - val_mse: 49129572.0000 - val_mae: 2750.1384\n",
            "Epoch 419/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2205.3284 - mse: 32730742.0000 - mae: 2205.3284 - val_loss: 2757.6717 - val_mse: 49042172.0000 - val_mae: 2757.6716\n",
            "Epoch 420/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2120.9581 - mse: 30386522.0000 - mae: 2120.9583 - val_loss: 2734.4195 - val_mse: 48598172.0000 - val_mae: 2734.4194\n",
            "Epoch 421/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2216.6456 - mse: 31884358.0000 - mae: 2216.6458 - val_loss: 2750.0980 - val_mse: 48824364.0000 - val_mae: 2750.0979\n",
            "Epoch 422/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2174.1769 - mse: 30998620.0000 - mae: 2174.1770 - val_loss: 2735.0696 - val_mse: 48649016.0000 - val_mae: 2735.0696\n",
            "Epoch 423/500\n",
            "1500/1500 [==============================] - 0s 92us/step - loss: 2178.3748 - mse: 31054280.0000 - mae: 2178.3748 - val_loss: 2723.0786 - val_mse: 48117892.0000 - val_mae: 2723.0786\n",
            "Epoch 424/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2198.8041 - mse: 31517622.0000 - mae: 2198.8044 - val_loss: 2726.4683 - val_mse: 48348512.0000 - val_mae: 2726.4685\n",
            "Epoch 425/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2174.7790 - mse: 31426806.0000 - mae: 2174.7791 - val_loss: 2733.9689 - val_mse: 48398608.0000 - val_mae: 2733.9688\n",
            "Epoch 426/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2189.1946 - mse: 32273206.0000 - mae: 2189.1946 - val_loss: 2721.4152 - val_mse: 47849712.0000 - val_mae: 2721.4153\n",
            "Epoch 427/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2168.1429 - mse: 31542008.0000 - mae: 2168.1431 - val_loss: 2715.2466 - val_mse: 47545800.0000 - val_mae: 2715.2466\n",
            "Epoch 428/500\n",
            "1500/1500 [==============================] - 0s 90us/step - loss: 2167.9308 - mse: 30724664.0000 - mae: 2167.9307 - val_loss: 2721.4506 - val_mse: 47701572.0000 - val_mae: 2721.4504\n",
            "Epoch 429/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2184.3380 - mse: 31487486.0000 - mae: 2184.3379 - val_loss: 2716.6883 - val_mse: 47561840.0000 - val_mae: 2716.6882\n",
            "Epoch 430/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2182.2667 - mse: 31889682.0000 - mae: 2182.2666 - val_loss: 2711.6992 - val_mse: 47463344.0000 - val_mae: 2711.6990\n",
            "Epoch 431/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2151.5065 - mse: 31011098.0000 - mae: 2151.5066 - val_loss: 2727.0716 - val_mse: 47840544.0000 - val_mae: 2727.0718\n",
            "Epoch 432/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2131.6213 - mse: 31129794.0000 - mae: 2131.6213 - val_loss: 2701.5326 - val_mse: 47219948.0000 - val_mae: 2701.5327\n",
            "Epoch 433/500\n",
            "1500/1500 [==============================] - 0s 90us/step - loss: 2200.5527 - mse: 31676506.0000 - mae: 2200.5525 - val_loss: 2705.8786 - val_mse: 47133072.0000 - val_mae: 2705.8787\n",
            "Epoch 434/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2196.5747 - mse: 32002028.0000 - mae: 2196.5747 - val_loss: 2710.5651 - val_mse: 47076480.0000 - val_mae: 2710.5652\n",
            "Epoch 435/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2165.0474 - mse: 31538428.0000 - mae: 2165.0476 - val_loss: 2694.5595 - val_mse: 46998608.0000 - val_mae: 2694.5596\n",
            "Epoch 436/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2162.6925 - mse: 30953036.0000 - mae: 2162.6924 - val_loss: 2696.0936 - val_mse: 46816624.0000 - val_mae: 2696.0938\n",
            "Epoch 437/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2104.4559 - mse: 29700882.0000 - mae: 2104.4558 - val_loss: 2691.1227 - val_mse: 46657732.0000 - val_mae: 2691.1226\n",
            "Epoch 438/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2119.7603 - mse: 29254560.0000 - mae: 2119.7603 - val_loss: 2686.5646 - val_mse: 46477264.0000 - val_mae: 2686.5647\n",
            "Epoch 439/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2115.9890 - mse: 29166886.0000 - mae: 2115.9890 - val_loss: 2683.9777 - val_mse: 46474380.0000 - val_mae: 2683.9778\n",
            "Epoch 440/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2144.1110 - mse: 31962820.0000 - mae: 2144.1111 - val_loss: 2669.8361 - val_mse: 45954696.0000 - val_mae: 2669.8362\n",
            "Epoch 441/500\n",
            "1500/1500 [==============================] - 0s 87us/step - loss: 2120.8717 - mse: 29770804.0000 - mae: 2120.8716 - val_loss: 2674.9786 - val_mse: 46000260.0000 - val_mae: 2674.9785\n",
            "Epoch 442/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2131.8857 - mse: 29490266.0000 - mae: 2131.8857 - val_loss: 2669.9411 - val_mse: 46015916.0000 - val_mae: 2669.9414\n",
            "Epoch 443/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2144.9933 - mse: 30728318.0000 - mae: 2144.9932 - val_loss: 2671.1444 - val_mse: 45965944.0000 - val_mae: 2671.1445\n",
            "Epoch 444/500\n",
            "1500/1500 [==============================] - 0s 87us/step - loss: 2110.7435 - mse: 28508760.0000 - mae: 2110.7434 - val_loss: 2682.6318 - val_mse: 46052584.0000 - val_mae: 2682.6318\n",
            "Epoch 445/500\n",
            "1500/1500 [==============================] - 0s 92us/step - loss: 2114.2536 - mse: 30194942.0000 - mae: 2114.2537 - val_loss: 2667.4625 - val_mse: 45778700.0000 - val_mae: 2667.4624\n",
            "Epoch 446/500\n",
            "1500/1500 [==============================] - 0s 94us/step - loss: 2112.7143 - mse: 29982160.0000 - mae: 2112.7144 - val_loss: 2658.2478 - val_mse: 45375136.0000 - val_mae: 2658.2478\n",
            "Epoch 447/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2121.2774 - mse: 29170358.0000 - mae: 2121.2773 - val_loss: 2668.1616 - val_mse: 45653016.0000 - val_mae: 2668.1616\n",
            "Epoch 448/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2115.8951 - mse: 29073752.0000 - mae: 2115.8953 - val_loss: 2667.0371 - val_mse: 45456624.0000 - val_mae: 2667.0371\n",
            "Epoch 449/500\n",
            "1500/1500 [==============================] - 0s 92us/step - loss: 2133.8901 - mse: 29898354.0000 - mae: 2133.8901 - val_loss: 2649.3627 - val_mse: 45263680.0000 - val_mae: 2649.3625\n",
            "Epoch 450/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2120.4393 - mse: 30011736.0000 - mae: 2120.4395 - val_loss: 2664.7915 - val_mse: 45280336.0000 - val_mae: 2664.7915\n",
            "Epoch 451/500\n",
            "1500/1500 [==============================] - 0s 89us/step - loss: 2132.1819 - mse: 29937112.0000 - mae: 2132.1819 - val_loss: 2656.3879 - val_mse: 45385472.0000 - val_mae: 2656.3879\n",
            "Epoch 452/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2098.9393 - mse: 28602778.0000 - mae: 2098.9395 - val_loss: 2641.9051 - val_mse: 44856848.0000 - val_mae: 2641.9048\n",
            "Epoch 453/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2120.2745 - mse: 30607534.0000 - mae: 2120.2744 - val_loss: 2644.6233 - val_mse: 44843384.0000 - val_mae: 2644.6233\n",
            "Epoch 454/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2079.4140 - mse: 29132636.0000 - mae: 2079.4141 - val_loss: 2645.0924 - val_mse: 44715148.0000 - val_mae: 2645.0923\n",
            "Epoch 455/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2097.9686 - mse: 29291586.0000 - mae: 2097.9688 - val_loss: 2640.1366 - val_mse: 44504940.0000 - val_mae: 2640.1367\n",
            "Epoch 456/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2036.0188 - mse: 27156158.0000 - mae: 2036.0188 - val_loss: 2640.0857 - val_mse: 44649692.0000 - val_mae: 2640.0859\n",
            "Epoch 457/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2100.2168 - mse: 29518288.0000 - mae: 2100.2166 - val_loss: 2642.3685 - val_mse: 44527800.0000 - val_mae: 2642.3687\n",
            "Epoch 458/500\n",
            "1500/1500 [==============================] - 0s 89us/step - loss: 2038.5396 - mse: 27769998.0000 - mae: 2038.5396 - val_loss: 2627.8495 - val_mse: 44110960.0000 - val_mae: 2627.8496\n",
            "Epoch 459/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2062.3169 - mse: 28488740.0000 - mae: 2062.3169 - val_loss: 2619.9386 - val_mse: 43953272.0000 - val_mae: 2619.9387\n",
            "Epoch 460/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2143.5954 - mse: 29712050.0000 - mae: 2143.5955 - val_loss: 2629.4999 - val_mse: 44229584.0000 - val_mae: 2629.5000\n",
            "Epoch 461/500\n",
            "1500/1500 [==============================] - 0s 92us/step - loss: 2108.9718 - mse: 29072250.0000 - mae: 2108.9719 - val_loss: 2630.3051 - val_mse: 44028228.0000 - val_mae: 2630.3049\n",
            "Epoch 462/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2093.9415 - mse: 28148422.0000 - mae: 2093.9414 - val_loss: 2615.5173 - val_mse: 43669612.0000 - val_mae: 2615.5173\n",
            "Epoch 463/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2104.0529 - mse: 29633040.0000 - mae: 2104.0527 - val_loss: 2635.9034 - val_mse: 44049128.0000 - val_mae: 2635.9036\n",
            "Epoch 464/500\n",
            "1500/1500 [==============================] - 0s 93us/step - loss: 2035.6507 - mse: 28695764.0000 - mae: 2035.6509 - val_loss: 2622.2385 - val_mse: 43604548.0000 - val_mae: 2622.2385\n",
            "Epoch 465/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 1998.3763 - mse: 26682172.0000 - mae: 1998.3763 - val_loss: 2620.9708 - val_mse: 43681448.0000 - val_mae: 2620.9709\n",
            "Epoch 466/500\n",
            "1500/1500 [==============================] - 0s 92us/step - loss: 2072.7627 - mse: 28482970.0000 - mae: 2072.7627 - val_loss: 2616.8516 - val_mse: 43671704.0000 - val_mae: 2616.8516\n",
            "Epoch 467/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2050.9329 - mse: 28687900.0000 - mae: 2050.9331 - val_loss: 2602.7612 - val_mse: 43476872.0000 - val_mae: 2602.7615\n",
            "Epoch 468/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2065.0765 - mse: 28000668.0000 - mae: 2065.0764 - val_loss: 2603.8939 - val_mse: 43276920.0000 - val_mae: 2603.8936\n",
            "Epoch 469/500\n",
            "1500/1500 [==============================] - 0s 89us/step - loss: 2086.4839 - mse: 29093986.0000 - mae: 2086.4839 - val_loss: 2589.3814 - val_mse: 43072684.0000 - val_mae: 2589.3816\n",
            "Epoch 470/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2032.8994 - mse: 27109670.0000 - mae: 2032.8993 - val_loss: 2588.0186 - val_mse: 42934420.0000 - val_mae: 2588.0186\n",
            "Epoch 471/500\n",
            "1500/1500 [==============================] - 0s 97us/step - loss: 2055.4613 - mse: 27702218.0000 - mae: 2055.4614 - val_loss: 2588.8492 - val_mse: 42868140.0000 - val_mae: 2588.8494\n",
            "Epoch 472/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2056.7865 - mse: 27157200.0000 - mae: 2056.7866 - val_loss: 2595.9679 - val_mse: 43137248.0000 - val_mae: 2595.9678\n",
            "Epoch 473/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2068.5706 - mse: 28163938.0000 - mae: 2068.5706 - val_loss: 2591.3255 - val_mse: 42884136.0000 - val_mae: 2591.3257\n",
            "Epoch 474/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2003.7563 - mse: 26028742.0000 - mae: 2003.7563 - val_loss: 2597.3813 - val_mse: 42830240.0000 - val_mae: 2597.3813\n",
            "Epoch 475/500\n",
            "1500/1500 [==============================] - 0s 89us/step - loss: 2020.9626 - mse: 28112954.0000 - mae: 2020.9626 - val_loss: 2575.7317 - val_mse: 42489740.0000 - val_mae: 2575.7317\n",
            "Epoch 476/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2045.8533 - mse: 27701868.0000 - mae: 2045.8534 - val_loss: 2572.6606 - val_mse: 42358608.0000 - val_mae: 2572.6606\n",
            "Epoch 477/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2070.6503 - mse: 28272482.0000 - mae: 2070.6504 - val_loss: 2559.0055 - val_mse: 42084408.0000 - val_mae: 2559.0056\n",
            "Epoch 478/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 1971.1738 - mse: 26845496.0000 - mae: 1971.1737 - val_loss: 2560.0970 - val_mse: 42077588.0000 - val_mae: 2560.0969\n",
            "Epoch 479/500\n",
            "1500/1500 [==============================] - 0s 90us/step - loss: 2027.9677 - mse: 27559092.0000 - mae: 2027.9677 - val_loss: 2564.7448 - val_mse: 42131476.0000 - val_mae: 2564.7449\n",
            "Epoch 480/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2027.9689 - mse: 28351818.0000 - mae: 2027.9689 - val_loss: 2548.6968 - val_mse: 41795960.0000 - val_mae: 2548.6968\n",
            "Epoch 481/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2034.2190 - mse: 28772788.0000 - mae: 2034.2189 - val_loss: 2549.6434 - val_mse: 41680448.0000 - val_mae: 2549.6431\n",
            "Epoch 482/500\n",
            "1500/1500 [==============================] - 0s 93us/step - loss: 2023.8267 - mse: 27671408.0000 - mae: 2023.8267 - val_loss: 2540.3923 - val_mse: 41666272.0000 - val_mae: 2540.3926\n",
            "Epoch 483/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 1951.8979 - mse: 25867316.0000 - mae: 1951.8978 - val_loss: 2537.1420 - val_mse: 41502656.0000 - val_mae: 2537.1421\n",
            "Epoch 484/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2050.9528 - mse: 29308144.0000 - mae: 2050.9526 - val_loss: 2557.8857 - val_mse: 41793212.0000 - val_mae: 2557.8857\n",
            "Epoch 485/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2015.2309 - mse: 27343410.0000 - mae: 2015.2310 - val_loss: 2534.9536 - val_mse: 41428084.0000 - val_mae: 2534.9536\n",
            "Epoch 486/500\n",
            "1500/1500 [==============================] - 0s 90us/step - loss: 2041.5184 - mse: 28110796.0000 - mae: 2041.5183 - val_loss: 2532.5718 - val_mse: 41275028.0000 - val_mae: 2532.5718\n",
            "Epoch 487/500\n",
            "1500/1500 [==============================] - 0s 90us/step - loss: 2043.7938 - mse: 28465452.0000 - mae: 2043.7937 - val_loss: 2516.2450 - val_mse: 40895568.0000 - val_mae: 2516.2451\n",
            "Epoch 488/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2005.3057 - mse: 25899890.0000 - mae: 2005.3057 - val_loss: 2531.1046 - val_mse: 41271312.0000 - val_mae: 2531.1045\n",
            "Epoch 489/500\n",
            "1500/1500 [==============================] - 0s 90us/step - loss: 2015.7817 - mse: 26877706.0000 - mae: 2015.7816 - val_loss: 2513.1707 - val_mse: 40817116.0000 - val_mae: 2513.1709\n",
            "Epoch 490/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2028.4624 - mse: 26982264.0000 - mae: 2028.4625 - val_loss: 2517.3497 - val_mse: 40822376.0000 - val_mae: 2517.3496\n",
            "Epoch 491/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 1986.0050 - mse: 26122058.0000 - mae: 1986.0050 - val_loss: 2510.8239 - val_mse: 40709632.0000 - val_mae: 2510.8237\n",
            "Epoch 492/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 1994.5328 - mse: 26357220.0000 - mae: 1994.5327 - val_loss: 2492.3038 - val_mse: 40303856.0000 - val_mae: 2492.3037\n",
            "Epoch 493/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2010.2039 - mse: 27590238.0000 - mae: 2010.2039 - val_loss: 2498.0805 - val_mse: 40340016.0000 - val_mae: 2498.0806\n",
            "Epoch 494/500\n",
            "1500/1500 [==============================] - 0s 92us/step - loss: 2054.0516 - mse: 28302426.0000 - mae: 2054.0518 - val_loss: 2487.8872 - val_mse: 40206924.0000 - val_mae: 2487.8872\n",
            "Epoch 495/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 1968.1455 - mse: 25254734.0000 - mae: 1968.1454 - val_loss: 2489.4562 - val_mse: 40192192.0000 - val_mae: 2489.4561\n",
            "Epoch 496/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 1942.0428 - mse: 25336142.0000 - mae: 1942.0428 - val_loss: 2483.6729 - val_mse: 39986420.0000 - val_mae: 2483.6731\n",
            "Epoch 497/500\n",
            "1500/1500 [==============================] - 0s 90us/step - loss: 1992.8701 - mse: 25556006.0000 - mae: 1992.8701 - val_loss: 2487.3255 - val_mse: 40195616.0000 - val_mae: 2487.3254\n",
            "Epoch 498/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2032.8518 - mse: 27461818.0000 - mae: 2032.8518 - val_loss: 2485.6149 - val_mse: 40010524.0000 - val_mae: 2485.6150\n",
            "Epoch 499/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 1984.0784 - mse: 27347876.0000 - mae: 1984.0785 - val_loss: 2475.9459 - val_mse: 39728236.0000 - val_mae: 2475.9460\n",
            "Epoch 500/500\n",
            "1500/1500 [==============================] - 0s 89us/step - loss: 1967.1793 - mse: 26313064.0000 - mae: 1967.1793 - val_loss: 2472.6536 - val_mse: 39689512.0000 - val_mae: 2472.6536\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1880 samples, validate on 1870 samples\n",
            "Epoch 1/500\n",
            "1880/1880 [==============================] - 2s 1ms/step - loss: 4426.2392 - mse: 132218808.0000 - mae: 4426.2393 - val_loss: 4425.5934 - val_mse: 136897792.0000 - val_mae: 4425.5933\n",
            "Epoch 2/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 4422.6077 - mse: 132204440.0000 - mae: 4422.6079 - val_loss: 4415.9759 - val_mse: 136860720.0000 - val_mae: 4415.9756\n",
            "Epoch 3/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 4408.4292 - mse: 132151960.0000 - mae: 4408.4292 - val_loss: 4396.6287 - val_mse: 136781584.0000 - val_mae: 4396.6284\n",
            "Epoch 4/500\n",
            "1880/1880 [==============================] - 0s 85us/step - loss: 4390.0719 - mse: 132075872.0000 - mae: 4390.0718 - val_loss: 4376.8444 - val_mse: 136693728.0000 - val_mae: 4376.8442\n",
            "Epoch 5/500\n",
            "1880/1880 [==============================] - 0s 63us/step - loss: 4369.2856 - mse: 131992464.0000 - mae: 4369.2856 - val_loss: 4356.5874 - val_mse: 136594496.0000 - val_mae: 4356.5874\n",
            "Epoch 6/500\n",
            "1880/1880 [==============================] - 0s 63us/step - loss: 4346.4382 - mse: 131880792.0000 - mae: 4346.4385 - val_loss: 4332.6294 - val_mse: 136468880.0000 - val_mae: 4332.6294\n",
            "Epoch 7/500\n",
            "1880/1880 [==============================] - 0s 62us/step - loss: 4324.3413 - mse: 131766656.0000 - mae: 4324.3413 - val_loss: 4309.0036 - val_mse: 136330016.0000 - val_mae: 4309.0034\n",
            "Epoch 8/500\n",
            "1880/1880 [==============================] - 0s 64us/step - loss: 4299.2670 - mse: 131623792.0000 - mae: 4299.2671 - val_loss: 4285.5147 - val_mse: 136171840.0000 - val_mae: 4285.5146\n",
            "Epoch 9/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 4276.8453 - mse: 131451600.0000 - mae: 4276.8452 - val_loss: 4265.3420 - val_mse: 136009200.0000 - val_mae: 4265.3423\n",
            "Epoch 10/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4257.0360 - mse: 131299464.0000 - mae: 4257.0361 - val_loss: 4248.0402 - val_mse: 135841696.0000 - val_mae: 4248.0400\n",
            "Epoch 11/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 4240.2388 - mse: 131143968.0000 - mae: 4240.2388 - val_loss: 4233.4877 - val_mse: 135676432.0000 - val_mae: 4233.4873\n",
            "Epoch 12/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 4228.7544 - mse: 130990384.0000 - mae: 4228.7544 - val_loss: 4220.3978 - val_mse: 135508448.0000 - val_mae: 4220.3979\n",
            "Epoch 13/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 4216.0931 - mse: 130800472.0000 - mae: 4216.0933 - val_loss: 4210.0024 - val_mse: 135354176.0000 - val_mae: 4210.0024\n",
            "Epoch 14/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4203.2006 - mse: 130642584.0000 - mae: 4203.2007 - val_loss: 4200.3807 - val_mse: 135183344.0000 - val_mae: 4200.3809\n",
            "Epoch 15/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 4194.5725 - mse: 130432528.0000 - mae: 4194.5723 - val_loss: 4193.8663 - val_mse: 135039280.0000 - val_mae: 4193.8662\n",
            "Epoch 16/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 4184.1740 - mse: 130269896.0000 - mae: 4184.1738 - val_loss: 4188.1167 - val_mse: 134888448.0000 - val_mae: 4188.1167\n",
            "Epoch 17/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 4177.7779 - mse: 130163248.0000 - mae: 4177.7778 - val_loss: 4183.3236 - val_mse: 134738368.0000 - val_mae: 4183.3237\n",
            "Epoch 18/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4171.9230 - mse: 130006520.0000 - mae: 4171.9229 - val_loss: 4178.9911 - val_mse: 134585984.0000 - val_mae: 4178.9912\n",
            "Epoch 19/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 4171.5443 - mse: 129814440.0000 - mae: 4171.5439 - val_loss: 4175.5011 - val_mse: 134454672.0000 - val_mae: 4175.5010\n",
            "Epoch 20/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4167.9653 - mse: 129735296.0000 - mae: 4167.9653 - val_loss: 4172.3153 - val_mse: 134315472.0000 - val_mae: 4172.3154\n",
            "Epoch 21/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 4166.9620 - mse: 129563224.0000 - mae: 4166.9619 - val_loss: 4169.1117 - val_mse: 134186552.0000 - val_mae: 4169.1113\n",
            "Epoch 22/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 4162.1147 - mse: 129511752.0000 - mae: 4162.1147 - val_loss: 4166.0174 - val_mse: 134061320.0000 - val_mae: 4166.0176\n",
            "Epoch 23/500\n",
            "1880/1880 [==============================] - 0s 64us/step - loss: 4148.4572 - mse: 129318600.0000 - mae: 4148.4570 - val_loss: 4162.7365 - val_mse: 133911136.0000 - val_mae: 4162.7368\n",
            "Epoch 24/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 4149.4524 - mse: 129154384.0000 - mae: 4149.4526 - val_loss: 4158.6624 - val_mse: 133736776.0000 - val_mae: 4158.6626\n",
            "Epoch 25/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 4150.5214 - mse: 129091696.0000 - mae: 4150.5215 - val_loss: 4153.2372 - val_mse: 133444704.0000 - val_mae: 4153.2373\n",
            "Epoch 26/500\n",
            "1880/1880 [==============================] - 0s 88us/step - loss: 4143.7411 - mse: 128645856.0000 - mae: 4143.7407 - val_loss: 4148.5248 - val_mse: 133200904.0000 - val_mae: 4148.5249\n",
            "Epoch 27/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 4138.5681 - mse: 128448352.0000 - mae: 4138.5679 - val_loss: 4143.2547 - val_mse: 132934080.0000 - val_mae: 4143.2549\n",
            "Epoch 28/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4133.7497 - mse: 128134976.0000 - mae: 4133.7495 - val_loss: 4137.2972 - val_mse: 132613968.0000 - val_mae: 4137.2969\n",
            "Epoch 29/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 4122.4704 - mse: 127869704.0000 - mae: 4122.4702 - val_loss: 4132.9904 - val_mse: 132387064.0000 - val_mae: 4132.9902\n",
            "Epoch 30/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 4131.2852 - mse: 127590312.0000 - mae: 4131.2852 - val_loss: 4129.2307 - val_mse: 132177904.0000 - val_mae: 4129.2310\n",
            "Epoch 31/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 4115.8839 - mse: 127497528.0000 - mae: 4115.8843 - val_loss: 4125.9962 - val_mse: 131980576.0000 - val_mae: 4125.9961\n",
            "Epoch 32/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 4119.3174 - mse: 127283560.0000 - mae: 4119.3174 - val_loss: 4123.6080 - val_mse: 131828456.0000 - val_mae: 4123.6079\n",
            "Epoch 33/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 4118.8545 - mse: 127121936.0000 - mae: 4118.8545 - val_loss: 4121.9616 - val_mse: 131705856.0000 - val_mae: 4121.9619\n",
            "Epoch 34/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 4120.8435 - mse: 127058296.0000 - mae: 4120.8433 - val_loss: 4120.3569 - val_mse: 131587168.0000 - val_mae: 4120.3564\n",
            "Epoch 35/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4121.6995 - mse: 126994544.0000 - mae: 4121.6997 - val_loss: 4118.9190 - val_mse: 131470856.0000 - val_mae: 4118.9189\n",
            "Epoch 36/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 4113.0512 - mse: 126742984.0000 - mae: 4113.0513 - val_loss: 4117.1889 - val_mse: 131375048.0000 - val_mae: 4117.1890\n",
            "Epoch 37/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4108.4935 - mse: 126531168.0000 - mae: 4108.4937 - val_loss: 4115.5006 - val_mse: 131269320.0000 - val_mae: 4115.5010\n",
            "Epoch 38/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 4108.3935 - mse: 126636608.0000 - mae: 4108.3936 - val_loss: 4113.6721 - val_mse: 131153960.0000 - val_mae: 4113.6719\n",
            "Epoch 39/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 4109.4363 - mse: 126301208.0000 - mae: 4109.4360 - val_loss: 4111.7323 - val_mse: 131010768.0000 - val_mae: 4111.7324\n",
            "Epoch 40/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 4115.3950 - mse: 126298672.0000 - mae: 4115.3950 - val_loss: 4110.6320 - val_mse: 130931016.0000 - val_mae: 4110.6318\n",
            "Epoch 41/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 4102.6184 - mse: 126164912.0000 - mae: 4102.6182 - val_loss: 4108.9150 - val_mse: 130779024.0000 - val_mae: 4108.9150\n",
            "Epoch 42/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 4108.1436 - mse: 126156528.0000 - mae: 4108.1436 - val_loss: 4107.8490 - val_mse: 130752640.0000 - val_mae: 4107.8491\n",
            "Epoch 43/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 4103.6193 - mse: 126324656.0000 - mae: 4103.6196 - val_loss: 4106.8301 - val_mse: 130673744.0000 - val_mae: 4106.8301\n",
            "Epoch 44/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 4099.6402 - mse: 125995080.0000 - mae: 4099.6401 - val_loss: 4106.0340 - val_mse: 130613360.0000 - val_mae: 4106.0342\n",
            "Epoch 45/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 4108.1733 - mse: 126021024.0000 - mae: 4108.1729 - val_loss: 4104.7387 - val_mse: 130570520.0000 - val_mae: 4104.7393\n",
            "Epoch 46/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 4099.9278 - mse: 125893768.0000 - mae: 4099.9277 - val_loss: 4102.8995 - val_mse: 130511520.0000 - val_mae: 4102.8999\n",
            "Epoch 47/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 4095.9531 - mse: 125778976.0000 - mae: 4095.9529 - val_loss: 4100.8067 - val_mse: 130425320.0000 - val_mae: 4100.8066\n",
            "Epoch 48/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 4089.4468 - mse: 125728224.0000 - mae: 4089.4468 - val_loss: 4098.3348 - val_mse: 130338528.0000 - val_mae: 4098.3350\n",
            "Epoch 49/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 4095.4044 - mse: 125664928.0000 - mae: 4095.4048 - val_loss: 4094.8082 - val_mse: 130293904.0000 - val_mae: 4094.8083\n",
            "Epoch 50/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 4096.3075 - mse: 125695264.0000 - mae: 4096.3076 - val_loss: 4084.1173 - val_mse: 130281000.0000 - val_mae: 4084.1172\n",
            "Epoch 51/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 4075.6644 - mse: 125654160.0000 - mae: 4075.6643 - val_loss: 4057.3907 - val_mse: 130297376.0000 - val_mae: 4057.3904\n",
            "Epoch 52/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 4052.2800 - mse: 125585560.0000 - mae: 4052.2800 - val_loss: 4044.9939 - val_mse: 129993080.0000 - val_mae: 4044.9939\n",
            "Epoch 53/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 4042.3868 - mse: 125046880.0000 - mae: 4042.3867 - val_loss: 4035.1746 - val_mse: 129479592.0000 - val_mae: 4035.1746\n",
            "Epoch 54/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 4036.8713 - mse: 124668872.0000 - mae: 4036.8713 - val_loss: 4023.1786 - val_mse: 129161120.0000 - val_mae: 4023.1787\n",
            "Epoch 55/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 4025.8471 - mse: 124372128.0000 - mae: 4025.8474 - val_loss: 4016.1238 - val_mse: 128679896.0000 - val_mae: 4016.1235\n",
            "Epoch 56/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 4015.7965 - mse: 123964336.0000 - mae: 4015.7966 - val_loss: 4005.0720 - val_mse: 128546896.0000 - val_mae: 4005.0723\n",
            "Epoch 57/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 4002.9143 - mse: 123476632.0000 - mae: 4002.9141 - val_loss: 4005.1768 - val_mse: 128332408.0000 - val_mae: 4005.1770\n",
            "Epoch 58/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4003.3410 - mse: 123670296.0000 - mae: 4003.3411 - val_loss: 3994.7468 - val_mse: 127957808.0000 - val_mae: 3994.7471\n",
            "Epoch 59/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3990.3862 - mse: 123121744.0000 - mae: 3990.3862 - val_loss: 3979.5960 - val_mse: 127394240.0000 - val_mae: 3979.5962\n",
            "Epoch 60/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3986.3610 - mse: 122688248.0000 - mae: 3986.3608 - val_loss: 3974.1633 - val_mse: 127155608.0000 - val_mae: 3974.1633\n",
            "Epoch 61/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3984.8787 - mse: 122375808.0000 - mae: 3984.8784 - val_loss: 3967.9326 - val_mse: 126821808.0000 - val_mae: 3967.9324\n",
            "Epoch 62/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3970.0125 - mse: 121851472.0000 - mae: 3970.0122 - val_loss: 3956.5107 - val_mse: 126309696.0000 - val_mae: 3956.5110\n",
            "Epoch 63/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3957.0604 - mse: 121634392.0000 - mae: 3957.0601 - val_loss: 3954.8751 - val_mse: 126116224.0000 - val_mae: 3954.8752\n",
            "Epoch 64/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3953.6903 - mse: 121323832.0000 - mae: 3953.6904 - val_loss: 3943.4394 - val_mse: 125647952.0000 - val_mae: 3943.4390\n",
            "Epoch 65/500\n",
            "1880/1880 [==============================] - 0s 63us/step - loss: 3950.8650 - mse: 120584312.0000 - mae: 3950.8650 - val_loss: 3942.2371 - val_mse: 125425192.0000 - val_mae: 3942.2371\n",
            "Epoch 66/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3953.2751 - mse: 120611688.0000 - mae: 3953.2749 - val_loss: 3937.3524 - val_mse: 125105456.0000 - val_mae: 3937.3521\n",
            "Epoch 67/500\n",
            "1880/1880 [==============================] - 0s 63us/step - loss: 3933.8563 - mse: 120305040.0000 - mae: 3933.8564 - val_loss: 3933.7394 - val_mse: 124835344.0000 - val_mae: 3933.7393\n",
            "Epoch 68/500\n",
            "1880/1880 [==============================] - 0s 64us/step - loss: 3934.2848 - mse: 120257624.0000 - mae: 3934.2849 - val_loss: 3915.4696 - val_mse: 124312592.0000 - val_mae: 3915.4692\n",
            "Epoch 69/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3935.1794 - mse: 119746696.0000 - mae: 3935.1794 - val_loss: 3913.3757 - val_mse: 124061784.0000 - val_mae: 3913.3755\n",
            "Epoch 70/500\n",
            "1880/1880 [==============================] - 0s 62us/step - loss: 3924.9783 - mse: 119307176.0000 - mae: 3924.9785 - val_loss: 3902.2561 - val_mse: 123651232.0000 - val_mae: 3902.2559\n",
            "Epoch 71/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3931.5016 - mse: 119018992.0000 - mae: 3931.5020 - val_loss: 3893.6675 - val_mse: 123259504.0000 - val_mae: 3893.6677\n",
            "Epoch 72/500\n",
            "1880/1880 [==============================] - 0s 64us/step - loss: 3907.8648 - mse: 118516496.0000 - mae: 3907.8650 - val_loss: 3886.2236 - val_mse: 122894096.0000 - val_mae: 3886.2236\n",
            "Epoch 73/500\n",
            "1880/1880 [==============================] - 0s 61us/step - loss: 3916.5617 - mse: 118166048.0000 - mae: 3916.5615 - val_loss: 3883.6558 - val_mse: 122733808.0000 - val_mae: 3883.6562\n",
            "Epoch 74/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3888.0876 - mse: 117757552.0000 - mae: 3888.0876 - val_loss: 3873.0029 - val_mse: 122217944.0000 - val_mae: 3873.0027\n",
            "Epoch 75/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3890.8276 - mse: 117554952.0000 - mae: 3890.8274 - val_loss: 3865.5068 - val_mse: 121824240.0000 - val_mae: 3865.5063\n",
            "Epoch 76/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3899.3770 - mse: 117731464.0000 - mae: 3899.3770 - val_loss: 3863.3973 - val_mse: 121717248.0000 - val_mae: 3863.3972\n",
            "Epoch 77/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3891.9230 - mse: 116987184.0000 - mae: 3891.9231 - val_loss: 3851.6772 - val_mse: 121251048.0000 - val_mae: 3851.6772\n",
            "Epoch 78/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3883.6966 - mse: 116835360.0000 - mae: 3883.6963 - val_loss: 3847.2761 - val_mse: 121038648.0000 - val_mae: 3847.2761\n",
            "Epoch 79/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3878.5004 - mse: 116617872.0000 - mae: 3878.5002 - val_loss: 3840.0352 - val_mse: 120696424.0000 - val_mae: 3840.0349\n",
            "Epoch 80/500\n",
            "1880/1880 [==============================] - 0s 62us/step - loss: 3856.5744 - mse: 116041064.0000 - mae: 3856.5747 - val_loss: 3833.7705 - val_mse: 120349896.0000 - val_mae: 3833.7700\n",
            "Epoch 81/500\n",
            "1880/1880 [==============================] - 0s 158us/step - loss: 3872.7341 - mse: 115838224.0000 - mae: 3872.7341 - val_loss: 3827.9001 - val_mse: 119867760.0000 - val_mae: 3827.8999\n",
            "Epoch 82/500\n",
            "1880/1880 [==============================] - 0s 135us/step - loss: 3879.4849 - mse: 116235400.0000 - mae: 3879.4851 - val_loss: 3822.3880 - val_mse: 119713200.0000 - val_mae: 3822.3877\n",
            "Epoch 83/500\n",
            "1880/1880 [==============================] - 0s 150us/step - loss: 3830.9123 - mse: 114706680.0000 - mae: 3830.9124 - val_loss: 3818.5521 - val_mse: 119480128.0000 - val_mae: 3818.5518\n",
            "Epoch 84/500\n",
            "1880/1880 [==============================] - 0s 143us/step - loss: 3852.2089 - mse: 115073328.0000 - mae: 3852.2087 - val_loss: 3810.6312 - val_mse: 119014496.0000 - val_mae: 3810.6311\n",
            "Epoch 85/500\n",
            "1880/1880 [==============================] - 0s 180us/step - loss: 3838.6232 - mse: 113693048.0000 - mae: 3838.6230 - val_loss: 3810.0833 - val_mse: 118917272.0000 - val_mae: 3810.0833\n",
            "Epoch 86/500\n",
            "1880/1880 [==============================] - 0s 123us/step - loss: 3844.4365 - mse: 114672240.0000 - mae: 3844.4368 - val_loss: 3799.9321 - val_mse: 118386440.0000 - val_mae: 3799.9321\n",
            "Epoch 87/500\n",
            "1880/1880 [==============================] - 0s 106us/step - loss: 3825.5135 - mse: 113833960.0000 - mae: 3825.5137 - val_loss: 3796.2256 - val_mse: 118001184.0000 - val_mae: 3796.2256\n",
            "Epoch 88/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3831.0253 - mse: 113794008.0000 - mae: 3831.0251 - val_loss: 3790.2177 - val_mse: 117801520.0000 - val_mae: 3790.2180\n",
            "Epoch 89/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3806.3746 - mse: 113032088.0000 - mae: 3806.3745 - val_loss: 3784.4823 - val_mse: 117534792.0000 - val_mae: 3784.4824\n",
            "Epoch 90/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3834.0682 - mse: 113646176.0000 - mae: 3834.0684 - val_loss: 3783.0637 - val_mse: 117247360.0000 - val_mae: 3783.0637\n",
            "Epoch 91/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3803.3475 - mse: 112136232.0000 - mae: 3803.3472 - val_loss: 3775.5501 - val_mse: 116874928.0000 - val_mae: 3775.5503\n",
            "Epoch 92/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3785.0104 - mse: 112218328.0000 - mae: 3785.0100 - val_loss: 3770.9343 - val_mse: 116492784.0000 - val_mae: 3770.9343\n",
            "Epoch 93/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3793.0396 - mse: 112285392.0000 - mae: 3793.0393 - val_loss: 3765.1609 - val_mse: 116190632.0000 - val_mae: 3765.1609\n",
            "Epoch 94/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3789.4656 - mse: 111868192.0000 - mae: 3789.4653 - val_loss: 3758.7418 - val_mse: 115696632.0000 - val_mae: 3758.7422\n",
            "Epoch 95/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3762.8121 - mse: 110994024.0000 - mae: 3762.8123 - val_loss: 3760.2720 - val_mse: 115207848.0000 - val_mae: 3760.2720\n",
            "Epoch 96/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3778.3242 - mse: 111612176.0000 - mae: 3778.3242 - val_loss: 3747.4433 - val_mse: 115158232.0000 - val_mae: 3747.4436\n",
            "Epoch 97/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3767.2310 - mse: 110927928.0000 - mae: 3767.2312 - val_loss: 3739.2837 - val_mse: 114718416.0000 - val_mae: 3739.2837\n",
            "Epoch 98/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3735.4852 - mse: 109702784.0000 - mae: 3735.4854 - val_loss: 3743.7608 - val_mse: 113989184.0000 - val_mae: 3743.7607\n",
            "Epoch 99/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3729.5737 - mse: 109909160.0000 - mae: 3729.5740 - val_loss: 3726.6931 - val_mse: 113735920.0000 - val_mae: 3726.6934\n",
            "Epoch 100/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3739.3322 - mse: 109628928.0000 - mae: 3739.3315 - val_loss: 3717.2047 - val_mse: 113348768.0000 - val_mae: 3717.2046\n",
            "Epoch 101/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3714.8196 - mse: 109116048.0000 - mae: 3714.8196 - val_loss: 3707.0572 - val_mse: 113067200.0000 - val_mae: 3707.0571\n",
            "Epoch 102/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 3720.5421 - mse: 109089904.0000 - mae: 3720.5420 - val_loss: 3709.2255 - val_mse: 112371120.0000 - val_mae: 3709.2251\n",
            "Epoch 103/500\n",
            "1880/1880 [==============================] - 0s 64us/step - loss: 3707.3326 - mse: 107859368.0000 - mae: 3707.3328 - val_loss: 3699.0509 - val_mse: 112403056.0000 - val_mae: 3699.0505\n",
            "Epoch 104/500\n",
            "1880/1880 [==============================] - 0s 63us/step - loss: 3704.3308 - mse: 108095976.0000 - mae: 3704.3311 - val_loss: 3686.9531 - val_mse: 111658728.0000 - val_mae: 3686.9529\n",
            "Epoch 105/500\n",
            "1880/1880 [==============================] - 0s 64us/step - loss: 3681.9091 - mse: 107021968.0000 - mae: 3681.9094 - val_loss: 3677.6387 - val_mse: 111369416.0000 - val_mae: 3677.6384\n",
            "Epoch 106/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3692.8966 - mse: 107618184.0000 - mae: 3692.8967 - val_loss: 3675.2946 - val_mse: 110721376.0000 - val_mae: 3675.2944\n",
            "Epoch 107/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3657.9016 - mse: 106012312.0000 - mae: 3657.9016 - val_loss: 3667.2783 - val_mse: 110332456.0000 - val_mae: 3667.2786\n",
            "Epoch 108/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3665.9763 - mse: 105715200.0000 - mae: 3665.9766 - val_loss: 3656.4760 - val_mse: 110074008.0000 - val_mae: 3656.4763\n",
            "Epoch 109/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3648.9066 - mse: 105761592.0000 - mae: 3648.9065 - val_loss: 3650.0245 - val_mse: 109694504.0000 - val_mae: 3650.0247\n",
            "Epoch 110/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3634.9406 - mse: 105640384.0000 - mae: 3634.9407 - val_loss: 3647.8964 - val_mse: 109452592.0000 - val_mae: 3647.8962\n",
            "Epoch 111/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3635.1130 - mse: 105501168.0000 - mae: 3635.1128 - val_loss: 3637.1472 - val_mse: 108860992.0000 - val_mae: 3637.1470\n",
            "Epoch 112/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3638.3257 - mse: 105378080.0000 - mae: 3638.3257 - val_loss: 3655.5546 - val_mse: 108252184.0000 - val_mae: 3655.5549\n",
            "Epoch 113/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3616.6670 - mse: 103382272.0000 - mae: 3616.6672 - val_loss: 3625.7520 - val_mse: 107959320.0000 - val_mae: 3625.7520\n",
            "Epoch 114/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3632.3155 - mse: 104042128.0000 - mae: 3632.3152 - val_loss: 3631.2820 - val_mse: 107414496.0000 - val_mae: 3631.2820\n",
            "Epoch 115/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3589.3640 - mse: 103076424.0000 - mae: 3589.3638 - val_loss: 3614.2649 - val_mse: 107097592.0000 - val_mae: 3614.2651\n",
            "Epoch 116/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3602.0269 - mse: 102852600.0000 - mae: 3602.0269 - val_loss: 3609.7347 - val_mse: 106613968.0000 - val_mae: 3609.7349\n",
            "Epoch 117/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3577.1706 - mse: 101507328.0000 - mae: 3577.1707 - val_loss: 3617.2878 - val_mse: 105962488.0000 - val_mae: 3617.2881\n",
            "Epoch 118/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3600.7099 - mse: 102878576.0000 - mae: 3600.7104 - val_loss: 3634.9284 - val_mse: 105480064.0000 - val_mae: 3634.9282\n",
            "Epoch 119/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3615.3924 - mse: 102292192.0000 - mae: 3615.3923 - val_loss: 3599.7220 - val_mse: 105366568.0000 - val_mae: 3599.7219\n",
            "Epoch 120/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 3580.1410 - mse: 100923032.0000 - mae: 3580.1409 - val_loss: 3590.6577 - val_mse: 105067496.0000 - val_mae: 3590.6580\n",
            "Epoch 121/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3576.9776 - mse: 100472152.0000 - mae: 3576.9775 - val_loss: 3599.9300 - val_mse: 104627232.0000 - val_mae: 3599.9299\n",
            "Epoch 122/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3583.8831 - mse: 101104408.0000 - mae: 3583.8831 - val_loss: 3593.7038 - val_mse: 104379176.0000 - val_mae: 3593.7039\n",
            "Epoch 123/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3583.4242 - mse: 99985296.0000 - mae: 3583.4243 - val_loss: 3578.7804 - val_mse: 104512632.0000 - val_mae: 3578.7805\n",
            "Epoch 124/500\n",
            "1880/1880 [==============================] - 0s 63us/step - loss: 3564.3038 - mse: 98960608.0000 - mae: 3564.3037 - val_loss: 3586.8873 - val_mse: 103872392.0000 - val_mae: 3586.8872\n",
            "Epoch 125/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3578.0571 - mse: 99128376.0000 - mae: 3578.0569 - val_loss: 3586.3352 - val_mse: 103516576.0000 - val_mae: 3586.3350\n",
            "Epoch 126/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3562.8121 - mse: 99174880.0000 - mae: 3562.8120 - val_loss: 3568.1259 - val_mse: 103597424.0000 - val_mae: 3568.1260\n",
            "Epoch 127/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3577.3779 - mse: 99613256.0000 - mae: 3577.3779 - val_loss: 3569.9580 - val_mse: 103174544.0000 - val_mae: 3569.9580\n",
            "Epoch 128/500\n",
            "1880/1880 [==============================] - 0s 65us/step - loss: 3564.7084 - mse: 99395424.0000 - mae: 3564.7085 - val_loss: 3570.0910 - val_mse: 102905128.0000 - val_mae: 3570.0908\n",
            "Epoch 129/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3581.2420 - mse: 99440040.0000 - mae: 3581.2419 - val_loss: 3561.6049 - val_mse: 102745680.0000 - val_mae: 3561.6050\n",
            "Epoch 130/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3563.6931 - mse: 99150224.0000 - mae: 3563.6931 - val_loss: 3558.4692 - val_mse: 102683424.0000 - val_mae: 3558.4692\n",
            "Epoch 131/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3547.1122 - mse: 98515200.0000 - mae: 3547.1121 - val_loss: 3556.0543 - val_mse: 102361768.0000 - val_mae: 3556.0542\n",
            "Epoch 132/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3574.8518 - mse: 98687104.0000 - mae: 3574.8518 - val_loss: 3554.2366 - val_mse: 102076072.0000 - val_mae: 3554.2366\n",
            "Epoch 133/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3558.5313 - mse: 98394120.0000 - mae: 3558.5315 - val_loss: 3554.5671 - val_mse: 101604520.0000 - val_mae: 3554.5669\n",
            "Epoch 134/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3549.9388 - mse: 97669792.0000 - mae: 3549.9385 - val_loss: 3549.7132 - val_mse: 101619656.0000 - val_mae: 3549.7131\n",
            "Epoch 135/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3544.0183 - mse: 96956680.0000 - mae: 3544.0181 - val_loss: 3547.7804 - val_mse: 101386624.0000 - val_mae: 3547.7800\n",
            "Epoch 136/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3552.2480 - mse: 97578528.0000 - mae: 3552.2478 - val_loss: 3545.7847 - val_mse: 101146112.0000 - val_mae: 3545.7844\n",
            "Epoch 137/500\n",
            "1880/1880 [==============================] - 0s 62us/step - loss: 3553.6068 - mse: 97975120.0000 - mae: 3553.6067 - val_loss: 3546.9011 - val_mse: 100888168.0000 - val_mae: 3546.9011\n",
            "Epoch 138/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3530.5713 - mse: 97332216.0000 - mae: 3530.5713 - val_loss: 3548.5053 - val_mse: 100655552.0000 - val_mae: 3548.5051\n",
            "Epoch 139/500\n",
            "1880/1880 [==============================] - 0s 64us/step - loss: 3536.7885 - mse: 96731904.0000 - mae: 3536.7883 - val_loss: 3551.8565 - val_mse: 100376112.0000 - val_mae: 3551.8562\n",
            "Epoch 140/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3553.2236 - mse: 97253048.0000 - mae: 3553.2236 - val_loss: 3547.5328 - val_mse: 100292824.0000 - val_mae: 3547.5327\n",
            "Epoch 141/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3550.6347 - mse: 97262576.0000 - mae: 3550.6345 - val_loss: 3563.9350 - val_mse: 99980592.0000 - val_mae: 3563.9353\n",
            "Epoch 142/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3541.1982 - mse: 96190136.0000 - mae: 3541.1985 - val_loss: 3543.9059 - val_mse: 100252664.0000 - val_mae: 3543.9058\n",
            "Epoch 143/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3542.8651 - mse: 95877728.0000 - mae: 3542.8650 - val_loss: 3533.8725 - val_mse: 99889904.0000 - val_mae: 3533.8723\n",
            "Epoch 144/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3553.6487 - mse: 97260592.0000 - mae: 3553.6484 - val_loss: 3533.4522 - val_mse: 99958568.0000 - val_mae: 3533.4524\n",
            "Epoch 145/500\n",
            "1880/1880 [==============================] - 0s 65us/step - loss: 3510.1533 - mse: 95596168.0000 - mae: 3510.1531 - val_loss: 3531.3780 - val_mse: 99688224.0000 - val_mae: 3531.3784\n",
            "Epoch 146/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 3509.8943 - mse: 95025432.0000 - mae: 3509.8940 - val_loss: 3531.4978 - val_mse: 99471064.0000 - val_mae: 3531.4980\n",
            "Epoch 147/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3553.7630 - mse: 96651176.0000 - mae: 3553.7629 - val_loss: 3534.0059 - val_mse: 99290144.0000 - val_mae: 3534.0059\n",
            "Epoch 148/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 3514.1925 - mse: 95775984.0000 - mae: 3514.1926 - val_loss: 3532.0584 - val_mse: 99098192.0000 - val_mae: 3532.0588\n",
            "Epoch 149/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3514.8724 - mse: 95065272.0000 - mae: 3514.8723 - val_loss: 3526.1970 - val_mse: 99031824.0000 - val_mae: 3526.1968\n",
            "Epoch 150/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3513.4657 - mse: 95280800.0000 - mae: 3513.4661 - val_loss: 3542.0955 - val_mse: 98617336.0000 - val_mae: 3542.0952\n",
            "Epoch 151/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3515.5128 - mse: 93901976.0000 - mae: 3515.5129 - val_loss: 3524.0412 - val_mse: 98690504.0000 - val_mae: 3524.0417\n",
            "Epoch 152/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3514.4872 - mse: 95134296.0000 - mae: 3514.4873 - val_loss: 3522.6519 - val_mse: 98547752.0000 - val_mae: 3522.6519\n",
            "Epoch 153/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3522.9915 - mse: 93730656.0000 - mae: 3522.9912 - val_loss: 3532.1533 - val_mse: 98373400.0000 - val_mae: 3532.1533\n",
            "Epoch 154/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3524.7328 - mse: 94141496.0000 - mae: 3524.7324 - val_loss: 3531.5944 - val_mse: 98431816.0000 - val_mae: 3531.5947\n",
            "Epoch 155/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 3502.8880 - mse: 93889672.0000 - mae: 3502.8879 - val_loss: 3539.8856 - val_mse: 97852872.0000 - val_mae: 3539.8853\n",
            "Epoch 156/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3519.7047 - mse: 94962264.0000 - mae: 3519.7046 - val_loss: 3521.9283 - val_mse: 98121224.0000 - val_mae: 3521.9280\n",
            "Epoch 157/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3487.4649 - mse: 92401568.0000 - mae: 3487.4648 - val_loss: 3518.4733 - val_mse: 97973232.0000 - val_mae: 3518.4729\n",
            "Epoch 158/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3538.6451 - mse: 95176272.0000 - mae: 3538.6453 - val_loss: 3520.1264 - val_mse: 97848680.0000 - val_mae: 3520.1267\n",
            "Epoch 159/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3510.4053 - mse: 93714976.0000 - mae: 3510.4053 - val_loss: 3525.9075 - val_mse: 97745536.0000 - val_mae: 3525.9072\n",
            "Epoch 160/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3499.4299 - mse: 92704536.0000 - mae: 3499.4297 - val_loss: 3533.0903 - val_mse: 97698472.0000 - val_mae: 3533.0903\n",
            "Epoch 161/500\n",
            "1880/1880 [==============================] - 0s 63us/step - loss: 3501.3525 - mse: 93909120.0000 - mae: 3501.3525 - val_loss: 3512.9137 - val_mse: 97527680.0000 - val_mae: 3512.9136\n",
            "Epoch 162/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3477.6506 - mse: 92350280.0000 - mae: 3477.6506 - val_loss: 3522.7388 - val_mse: 97071888.0000 - val_mae: 3522.7388\n",
            "Epoch 163/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3504.2086 - mse: 94042672.0000 - mae: 3504.2085 - val_loss: 3521.9553 - val_mse: 97634368.0000 - val_mae: 3521.9553\n",
            "Epoch 164/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3533.7202 - mse: 94262928.0000 - mae: 3533.7202 - val_loss: 3577.1808 - val_mse: 97057176.0000 - val_mae: 3577.1804\n",
            "Epoch 165/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3523.9357 - mse: 93827992.0000 - mae: 3523.9355 - val_loss: 3511.8311 - val_mse: 97332616.0000 - val_mae: 3511.8313\n",
            "Epoch 166/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3514.2351 - mse: 93116536.0000 - mae: 3514.2351 - val_loss: 3513.0957 - val_mse: 97334752.0000 - val_mae: 3513.0957\n",
            "Epoch 167/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3478.4422 - mse: 92777752.0000 - mae: 3478.4424 - val_loss: 3512.8281 - val_mse: 97035912.0000 - val_mae: 3512.8281\n",
            "Epoch 168/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3495.0286 - mse: 94038384.0000 - mae: 3495.0288 - val_loss: 3513.0864 - val_mse: 96712024.0000 - val_mae: 3513.0864\n",
            "Epoch 169/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3505.0226 - mse: 92464304.0000 - mae: 3505.0229 - val_loss: 3511.3589 - val_mse: 96816568.0000 - val_mae: 3511.3589\n",
            "Epoch 170/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3481.6232 - mse: 91772432.0000 - mae: 3481.6233 - val_loss: 3507.6707 - val_mse: 96851224.0000 - val_mae: 3507.6709\n",
            "Epoch 171/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3517.2930 - mse: 93875864.0000 - mae: 3517.2930 - val_loss: 3501.5436 - val_mse: 96636232.0000 - val_mae: 3501.5432\n",
            "Epoch 172/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3503.3612 - mse: 91802496.0000 - mae: 3503.3611 - val_loss: 3501.7659 - val_mse: 96534928.0000 - val_mae: 3501.7656\n",
            "Epoch 173/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3473.9024 - mse: 92164096.0000 - mae: 3473.9023 - val_loss: 3511.0228 - val_mse: 96092280.0000 - val_mae: 3511.0227\n",
            "Epoch 174/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3502.5849 - mse: 93650840.0000 - mae: 3502.5850 - val_loss: 3502.8343 - val_mse: 96391056.0000 - val_mae: 3502.8342\n",
            "Epoch 175/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3478.6770 - mse: 92914256.0000 - mae: 3478.6772 - val_loss: 3503.8537 - val_mse: 96351824.0000 - val_mae: 3503.8540\n",
            "Epoch 176/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3503.3623 - mse: 91968712.0000 - mae: 3503.3623 - val_loss: 3499.5429 - val_mse: 96118032.0000 - val_mae: 3499.5430\n",
            "Epoch 177/500\n",
            "1880/1880 [==============================] - 0s 82us/step - loss: 3461.5508 - mse: 90454424.0000 - mae: 3461.5505 - val_loss: 3505.9172 - val_mse: 95770064.0000 - val_mae: 3505.9170\n",
            "Epoch 178/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3474.9868 - mse: 90765792.0000 - mae: 3474.9871 - val_loss: 3497.5677 - val_mse: 95857248.0000 - val_mae: 3497.5681\n",
            "Epoch 179/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3468.2925 - mse: 92172616.0000 - mae: 3468.2922 - val_loss: 3502.1267 - val_mse: 95854832.0000 - val_mae: 3502.1265\n",
            "Epoch 180/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3480.9276 - mse: 90978640.0000 - mae: 3480.9277 - val_loss: 3504.8742 - val_mse: 95959984.0000 - val_mae: 3504.8743\n",
            "Epoch 181/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3472.3955 - mse: 90887680.0000 - mae: 3472.3953 - val_loss: 3500.6011 - val_mse: 95288648.0000 - val_mae: 3500.6008\n",
            "Epoch 182/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3444.4079 - mse: 90460104.0000 - mae: 3444.4080 - val_loss: 3493.3377 - val_mse: 95086640.0000 - val_mae: 3493.3376\n",
            "Epoch 183/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3473.5196 - mse: 91076144.0000 - mae: 3473.5198 - val_loss: 3487.3654 - val_mse: 95072632.0000 - val_mae: 3487.3652\n",
            "Epoch 184/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3486.6887 - mse: 92323384.0000 - mae: 3486.6885 - val_loss: 3497.0115 - val_mse: 94666928.0000 - val_mae: 3497.0112\n",
            "Epoch 185/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3486.7724 - mse: 91376752.0000 - mae: 3486.7725 - val_loss: 3486.4859 - val_mse: 95084328.0000 - val_mae: 3486.4858\n",
            "Epoch 186/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 3485.8555 - mse: 91963120.0000 - mae: 3485.8555 - val_loss: 3483.3193 - val_mse: 94772288.0000 - val_mae: 3483.3193\n",
            "Epoch 187/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3475.0279 - mse: 91362288.0000 - mae: 3475.0276 - val_loss: 3485.0774 - val_mse: 94704440.0000 - val_mae: 3485.0776\n",
            "Epoch 188/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3486.8405 - mse: 91596080.0000 - mae: 3486.8403 - val_loss: 3478.8803 - val_mse: 94639040.0000 - val_mae: 3478.8799\n",
            "Epoch 189/500\n",
            "1880/1880 [==============================] - 0s 65us/step - loss: 3491.1610 - mse: 91113448.0000 - mae: 3491.1611 - val_loss: 3475.4369 - val_mse: 94434872.0000 - val_mae: 3475.4370\n",
            "Epoch 190/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3439.1455 - mse: 89652736.0000 - mae: 3439.1455 - val_loss: 3471.4648 - val_mse: 94134368.0000 - val_mae: 3471.4646\n",
            "Epoch 191/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3475.2738 - mse: 90315080.0000 - mae: 3475.2737 - val_loss: 3479.9547 - val_mse: 93892120.0000 - val_mae: 3479.9548\n",
            "Epoch 192/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3440.1333 - mse: 90027944.0000 - mae: 3440.1335 - val_loss: 3471.8971 - val_mse: 93706352.0000 - val_mae: 3471.8972\n",
            "Epoch 193/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3434.0591 - mse: 89403824.0000 - mae: 3434.0591 - val_loss: 3464.7570 - val_mse: 93516384.0000 - val_mae: 3464.7568\n",
            "Epoch 194/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3448.4255 - mse: 88405232.0000 - mae: 3448.4255 - val_loss: 3465.8966 - val_mse: 93650832.0000 - val_mae: 3465.8967\n",
            "Epoch 195/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3447.7676 - mse: 89299120.0000 - mae: 3447.7676 - val_loss: 3489.9525 - val_mse: 93264416.0000 - val_mae: 3489.9521\n",
            "Epoch 196/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 3454.9151 - mse: 90377640.0000 - mae: 3454.9150 - val_loss: 3489.5821 - val_mse: 93067296.0000 - val_mae: 3489.5825\n",
            "Epoch 197/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3447.4762 - mse: 89144072.0000 - mae: 3447.4758 - val_loss: 3461.2324 - val_mse: 93045528.0000 - val_mae: 3461.2327\n",
            "Epoch 198/500\n",
            "1880/1880 [==============================] - 0s 65us/step - loss: 3445.8862 - mse: 89331448.0000 - mae: 3445.8862 - val_loss: 3470.5215 - val_mse: 92798200.0000 - val_mae: 3470.5215\n",
            "Epoch 199/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3452.0562 - mse: 89168376.0000 - mae: 3452.0562 - val_loss: 3468.2088 - val_mse: 92585080.0000 - val_mae: 3468.2087\n",
            "Epoch 200/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3442.0036 - mse: 89704128.0000 - mae: 3442.0034 - val_loss: 3456.4257 - val_mse: 92640712.0000 - val_mae: 3456.4253\n",
            "Epoch 201/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3444.2669 - mse: 88956632.0000 - mae: 3444.2666 - val_loss: 3460.0611 - val_mse: 92509344.0000 - val_mae: 3460.0608\n",
            "Epoch 202/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3428.1635 - mse: 88701144.0000 - mae: 3428.1633 - val_loss: 3448.7077 - val_mse: 92454288.0000 - val_mae: 3448.7078\n",
            "Epoch 203/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3431.8841 - mse: 89147192.0000 - mae: 3431.8840 - val_loss: 3469.4886 - val_mse: 92150784.0000 - val_mae: 3469.4885\n",
            "Epoch 204/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3423.1640 - mse: 89636496.0000 - mae: 3423.1643 - val_loss: 3461.5835 - val_mse: 91971112.0000 - val_mae: 3461.5835\n",
            "Epoch 205/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 3437.5519 - mse: 89106224.0000 - mae: 3437.5522 - val_loss: 3441.9678 - val_mse: 92055616.0000 - val_mae: 3441.9680\n",
            "Epoch 206/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3423.2211 - mse: 88632760.0000 - mae: 3423.2209 - val_loss: 3446.9436 - val_mse: 91679800.0000 - val_mae: 3446.9436\n",
            "Epoch 207/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3425.5756 - mse: 87941432.0000 - mae: 3425.5754 - val_loss: 3438.4441 - val_mse: 91742360.0000 - val_mae: 3438.4441\n",
            "Epoch 208/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3413.8737 - mse: 87530520.0000 - mae: 3413.8733 - val_loss: 3435.8016 - val_mse: 91624432.0000 - val_mae: 3435.8015\n",
            "Epoch 209/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3379.1804 - mse: 86550624.0000 - mae: 3379.1807 - val_loss: 3434.7039 - val_mse: 91386728.0000 - val_mae: 3434.7039\n",
            "Epoch 210/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3404.6126 - mse: 87281296.0000 - mae: 3404.6125 - val_loss: 3431.9590 - val_mse: 91042568.0000 - val_mae: 3431.9590\n",
            "Epoch 211/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 3412.9642 - mse: 88398240.0000 - mae: 3412.9644 - val_loss: 3429.9437 - val_mse: 90882232.0000 - val_mae: 3429.9438\n",
            "Epoch 212/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3408.8252 - mse: 87262472.0000 - mae: 3408.8252 - val_loss: 3476.5875 - val_mse: 90359840.0000 - val_mae: 3476.5872\n",
            "Epoch 213/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3407.4352 - mse: 87468032.0000 - mae: 3407.4351 - val_loss: 3426.0801 - val_mse: 90543272.0000 - val_mae: 3426.0798\n",
            "Epoch 214/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 3429.1682 - mse: 88699912.0000 - mae: 3429.1680 - val_loss: 3427.3182 - val_mse: 90444480.0000 - val_mae: 3427.3181\n",
            "Epoch 215/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3375.2326 - mse: 85413568.0000 - mae: 3375.2324 - val_loss: 3431.2752 - val_mse: 90060416.0000 - val_mae: 3431.2754\n",
            "Epoch 216/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3404.8693 - mse: 86879192.0000 - mae: 3404.8691 - val_loss: 3431.2753 - val_mse: 89926336.0000 - val_mae: 3431.2751\n",
            "Epoch 217/500\n",
            "1880/1880 [==============================] - 0s 65us/step - loss: 3371.3985 - mse: 84208392.0000 - mae: 3371.3987 - val_loss: 3422.8944 - val_mse: 89691920.0000 - val_mae: 3422.8943\n",
            "Epoch 218/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3394.3013 - mse: 86239384.0000 - mae: 3394.3013 - val_loss: 3437.1189 - val_mse: 89201144.0000 - val_mae: 3437.1187\n",
            "Epoch 219/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3378.6488 - mse: 84750944.0000 - mae: 3378.6487 - val_loss: 3413.1166 - val_mse: 89065008.0000 - val_mae: 3413.1169\n",
            "Epoch 220/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3370.2179 - mse: 85904568.0000 - mae: 3370.2178 - val_loss: 3410.2130 - val_mse: 89025896.0000 - val_mae: 3410.2131\n",
            "Epoch 221/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 3404.7662 - mse: 86333384.0000 - mae: 3404.7661 - val_loss: 3415.9040 - val_mse: 88604336.0000 - val_mae: 3415.9038\n",
            "Epoch 222/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3384.0002 - mse: 85043080.0000 - mae: 3384.0000 - val_loss: 3412.3053 - val_mse: 88661160.0000 - val_mae: 3412.3054\n",
            "Epoch 223/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3391.2519 - mse: 86065136.0000 - mae: 3391.2520 - val_loss: 3408.0174 - val_mse: 88510240.0000 - val_mae: 3408.0173\n",
            "Epoch 224/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3363.4427 - mse: 84687656.0000 - mae: 3363.4426 - val_loss: 3403.4683 - val_mse: 88345752.0000 - val_mae: 3403.4685\n",
            "Epoch 225/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 3364.4232 - mse: 84162424.0000 - mae: 3364.4233 - val_loss: 3403.3354 - val_mse: 88105424.0000 - val_mae: 3403.3354\n",
            "Epoch 226/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3359.3021 - mse: 84770432.0000 - mae: 3359.3018 - val_loss: 3397.2083 - val_mse: 88011656.0000 - val_mae: 3397.2083\n",
            "Epoch 227/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3379.8198 - mse: 83356688.0000 - mae: 3379.8196 - val_loss: 3400.7249 - val_mse: 87825776.0000 - val_mae: 3400.7249\n",
            "Epoch 228/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3376.8381 - mse: 85209088.0000 - mae: 3376.8381 - val_loss: 3393.2494 - val_mse: 87807920.0000 - val_mae: 3393.2498\n",
            "Epoch 229/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3316.7685 - mse: 82367616.0000 - mae: 3316.7686 - val_loss: 3414.4274 - val_mse: 87217768.0000 - val_mae: 3414.4272\n",
            "Epoch 230/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 3350.2901 - mse: 84073728.0000 - mae: 3350.2905 - val_loss: 3387.4743 - val_mse: 87256792.0000 - val_mae: 3387.4741\n",
            "Epoch 231/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3339.1200 - mse: 83553584.0000 - mae: 3339.1201 - val_loss: 3388.0471 - val_mse: 87001592.0000 - val_mae: 3388.0471\n",
            "Epoch 232/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3370.8728 - mse: 83989328.0000 - mae: 3370.8728 - val_loss: 3387.0423 - val_mse: 86812392.0000 - val_mae: 3387.0422\n",
            "Epoch 233/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3350.8914 - mse: 84121624.0000 - mae: 3350.8916 - val_loss: 3380.4894 - val_mse: 86703168.0000 - val_mae: 3380.4893\n",
            "Epoch 234/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3354.5221 - mse: 83508392.0000 - mae: 3354.5220 - val_loss: 3385.3191 - val_mse: 86209424.0000 - val_mae: 3385.3191\n",
            "Epoch 235/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 3334.5552 - mse: 82129472.0000 - mae: 3334.5554 - val_loss: 3386.7095 - val_mse: 85929824.0000 - val_mae: 3386.7090\n",
            "Epoch 236/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3366.9096 - mse: 84172464.0000 - mae: 3366.9094 - val_loss: 3374.3204 - val_mse: 86064376.0000 - val_mae: 3374.3206\n",
            "Epoch 237/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3353.9763 - mse: 83602704.0000 - mae: 3353.9763 - val_loss: 3385.5627 - val_mse: 85465544.0000 - val_mae: 3385.5627\n",
            "Epoch 238/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 3334.3920 - mse: 81197376.0000 - mae: 3334.3921 - val_loss: 3371.5571 - val_mse: 85508320.0000 - val_mae: 3371.5569\n",
            "Epoch 239/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3310.4809 - mse: 82078288.0000 - mae: 3310.4810 - val_loss: 3376.7883 - val_mse: 85155280.0000 - val_mae: 3376.7881\n",
            "Epoch 240/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 3333.1538 - mse: 81984040.0000 - mae: 3333.1538 - val_loss: 3363.6961 - val_mse: 85163152.0000 - val_mae: 3363.6963\n",
            "Epoch 241/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3328.8051 - mse: 82039648.0000 - mae: 3328.8054 - val_loss: 3362.3237 - val_mse: 84985912.0000 - val_mae: 3362.3237\n",
            "Epoch 242/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3339.3903 - mse: 81232840.0000 - mae: 3339.3901 - val_loss: 3356.5895 - val_mse: 84716792.0000 - val_mae: 3356.5896\n",
            "Epoch 243/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3308.2331 - mse: 82049744.0000 - mae: 3308.2329 - val_loss: 3355.9314 - val_mse: 84514312.0000 - val_mae: 3355.9316\n",
            "Epoch 244/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3290.5956 - mse: 80464048.0000 - mae: 3290.5955 - val_loss: 3358.1752 - val_mse: 83987936.0000 - val_mae: 3358.1750\n",
            "Epoch 245/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3316.0505 - mse: 79922400.0000 - mae: 3316.0503 - val_loss: 3346.0280 - val_mse: 83966568.0000 - val_mae: 3346.0281\n",
            "Epoch 246/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3281.8472 - mse: 79650720.0000 - mae: 3281.8474 - val_loss: 3360.6912 - val_mse: 83597512.0000 - val_mae: 3360.6912\n",
            "Epoch 247/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3326.4185 - mse: 81322792.0000 - mae: 3326.4185 - val_loss: 3344.3355 - val_mse: 83441792.0000 - val_mae: 3344.3352\n",
            "Epoch 248/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3285.1635 - mse: 79625032.0000 - mae: 3285.1633 - val_loss: 3341.1485 - val_mse: 83275064.0000 - val_mae: 3341.1484\n",
            "Epoch 249/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3322.5553 - mse: 80806552.0000 - mae: 3322.5554 - val_loss: 3333.1445 - val_mse: 83104584.0000 - val_mae: 3333.1445\n",
            "Epoch 250/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3301.0862 - mse: 80774664.0000 - mae: 3301.0862 - val_loss: 3330.1170 - val_mse: 82956992.0000 - val_mae: 3330.1174\n",
            "Epoch 251/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3296.9352 - mse: 79634272.0000 - mae: 3296.9351 - val_loss: 3334.9011 - val_mse: 82510600.0000 - val_mae: 3334.9011\n",
            "Epoch 252/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3269.3794 - mse: 79224656.0000 - mae: 3269.3796 - val_loss: 3324.0850 - val_mse: 82242248.0000 - val_mae: 3324.0850\n",
            "Epoch 253/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3289.9473 - mse: 80293680.0000 - mae: 3289.9473 - val_loss: 3320.3373 - val_mse: 81971872.0000 - val_mae: 3320.3376\n",
            "Epoch 254/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 3255.4342 - mse: 78399472.0000 - mae: 3255.4343 - val_loss: 3313.4401 - val_mse: 81753856.0000 - val_mae: 3313.4402\n",
            "Epoch 255/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3251.5762 - mse: 78489096.0000 - mae: 3251.5762 - val_loss: 3309.4975 - val_mse: 81541400.0000 - val_mae: 3309.4973\n",
            "Epoch 256/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3269.9771 - mse: 78845296.0000 - mae: 3269.9771 - val_loss: 3314.1321 - val_mse: 81194504.0000 - val_mae: 3314.1321\n",
            "Epoch 257/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 3247.6495 - mse: 77301984.0000 - mae: 3247.6494 - val_loss: 3305.9277 - val_mse: 80871648.0000 - val_mae: 3305.9272\n",
            "Epoch 258/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3265.2223 - mse: 77999968.0000 - mae: 3265.2224 - val_loss: 3316.4992 - val_mse: 80537336.0000 - val_mae: 3316.4990\n",
            "Epoch 259/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3266.8113 - mse: 77044304.0000 - mae: 3266.8113 - val_loss: 3298.2480 - val_mse: 80568512.0000 - val_mae: 3298.2480\n",
            "Epoch 260/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3236.9514 - mse: 78100896.0000 - mae: 3236.9517 - val_loss: 3297.3472 - val_mse: 80281656.0000 - val_mae: 3297.3477\n",
            "Epoch 261/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3246.2943 - mse: 76575248.0000 - mae: 3246.2942 - val_loss: 3288.2040 - val_mse: 80043840.0000 - val_mae: 3288.2039\n",
            "Epoch 262/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3218.8657 - mse: 76462280.0000 - mae: 3218.8660 - val_loss: 3296.4310 - val_mse: 79559088.0000 - val_mae: 3296.4309\n",
            "Epoch 263/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3250.0318 - mse: 77655168.0000 - mae: 3250.0320 - val_loss: 3282.3702 - val_mse: 79481536.0000 - val_mae: 3282.3701\n",
            "Epoch 264/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3233.4938 - mse: 77246712.0000 - mae: 3233.4937 - val_loss: 3283.7579 - val_mse: 79188056.0000 - val_mae: 3283.7581\n",
            "Epoch 265/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3265.1016 - mse: 77127928.0000 - mae: 3265.1018 - val_loss: 3286.3673 - val_mse: 78873504.0000 - val_mae: 3286.3674\n",
            "Epoch 266/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3222.3980 - mse: 74840064.0000 - mae: 3222.3979 - val_loss: 3275.2259 - val_mse: 78602088.0000 - val_mae: 3275.2263\n",
            "Epoch 267/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3180.0024 - mse: 74302880.0000 - mae: 3180.0024 - val_loss: 3263.8041 - val_mse: 78381736.0000 - val_mae: 3263.8042\n",
            "Epoch 268/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3196.9759 - mse: 75539304.0000 - mae: 3196.9761 - val_loss: 3257.7565 - val_mse: 78089296.0000 - val_mae: 3257.7563\n",
            "Epoch 269/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3191.8445 - mse: 74299944.0000 - mae: 3191.8445 - val_loss: 3263.6129 - val_mse: 77694432.0000 - val_mae: 3263.6130\n",
            "Epoch 270/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3156.6831 - mse: 72246768.0000 - mae: 3156.6829 - val_loss: 3247.7671 - val_mse: 77582016.0000 - val_mae: 3247.7673\n",
            "Epoch 271/500\n",
            "1880/1880 [==============================] - 0s 85us/step - loss: 3181.7699 - mse: 73699160.0000 - mae: 3181.7700 - val_loss: 3241.5786 - val_mse: 77086000.0000 - val_mae: 3241.5789\n",
            "Epoch 272/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3155.3464 - mse: 74212576.0000 - mae: 3155.3462 - val_loss: 3238.4413 - val_mse: 76724448.0000 - val_mae: 3238.4412\n",
            "Epoch 273/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3150.0427 - mse: 71787904.0000 - mae: 3150.0425 - val_loss: 3226.8995 - val_mse: 76433128.0000 - val_mae: 3226.8997\n",
            "Epoch 274/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3143.3185 - mse: 71987776.0000 - mae: 3143.3184 - val_loss: 3223.4620 - val_mse: 75963808.0000 - val_mae: 3223.4619\n",
            "Epoch 275/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3139.1939 - mse: 71903496.0000 - mae: 3139.1936 - val_loss: 3227.7184 - val_mse: 75621560.0000 - val_mae: 3227.7185\n",
            "Epoch 276/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3192.1642 - mse: 73882296.0000 - mae: 3192.1643 - val_loss: 3213.2049 - val_mse: 75217064.0000 - val_mae: 3213.2048\n",
            "Epoch 277/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3148.2423 - mse: 72386192.0000 - mae: 3148.2422 - val_loss: 3224.7139 - val_mse: 74555360.0000 - val_mae: 3224.7141\n",
            "Epoch 278/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 3113.7614 - mse: 69549376.0000 - mae: 3113.7615 - val_loss: 3209.6100 - val_mse: 74399416.0000 - val_mae: 3209.6101\n",
            "Epoch 279/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3134.5027 - mse: 71078848.0000 - mae: 3134.5027 - val_loss: 3208.8233 - val_mse: 73908520.0000 - val_mae: 3208.8232\n",
            "Epoch 280/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3108.6162 - mse: 70369048.0000 - mae: 3108.6162 - val_loss: 3197.0214 - val_mse: 73615968.0000 - val_mae: 3197.0215\n",
            "Epoch 281/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3133.5640 - mse: 69951160.0000 - mae: 3133.5637 - val_loss: 3196.0169 - val_mse: 73118944.0000 - val_mae: 3196.0171\n",
            "Epoch 282/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3099.8904 - mse: 69797824.0000 - mae: 3099.8901 - val_loss: 3187.9206 - val_mse: 72732440.0000 - val_mae: 3187.9209\n",
            "Epoch 283/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3134.9030 - mse: 72075968.0000 - mae: 3134.9028 - val_loss: 3176.9025 - val_mse: 72548880.0000 - val_mae: 3176.9026\n",
            "Epoch 284/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3067.1951 - mse: 67754832.0000 - mae: 3067.1953 - val_loss: 3179.3616 - val_mse: 72194912.0000 - val_mae: 3179.3621\n",
            "Epoch 285/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3062.3173 - mse: 67757032.0000 - mae: 3062.3174 - val_loss: 3173.4405 - val_mse: 71491144.0000 - val_mae: 3173.4407\n",
            "Epoch 286/500\n",
            "1880/1880 [==============================] - 0s 89us/step - loss: 3101.8988 - mse: 67062984.0000 - mae: 3101.8987 - val_loss: 3164.3380 - val_mse: 71316216.0000 - val_mae: 3164.3376\n",
            "Epoch 287/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 3038.6050 - mse: 67907504.0000 - mae: 3038.6050 - val_loss: 3158.0616 - val_mse: 70973672.0000 - val_mae: 3158.0615\n",
            "Epoch 288/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 3037.6383 - mse: 65684280.0000 - mae: 3037.6382 - val_loss: 3152.7358 - val_mse: 70407592.0000 - val_mae: 3152.7358\n",
            "Epoch 289/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3045.0811 - mse: 65612660.0000 - mae: 3045.0808 - val_loss: 3150.6815 - val_mse: 69887400.0000 - val_mae: 3150.6814\n",
            "Epoch 290/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2977.7640 - mse: 63235240.0000 - mae: 2977.7639 - val_loss: 3139.7353 - val_mse: 69384064.0000 - val_mae: 3139.7354\n",
            "Epoch 291/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3063.1581 - mse: 66078440.0000 - mae: 3063.1580 - val_loss: 3135.2320 - val_mse: 68914496.0000 - val_mae: 3135.2322\n",
            "Epoch 292/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3023.6204 - mse: 66279396.0000 - mae: 3023.6204 - val_loss: 3126.8495 - val_mse: 68563856.0000 - val_mae: 3126.8499\n",
            "Epoch 293/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2958.7073 - mse: 63621120.0000 - mae: 2958.7075 - val_loss: 3121.8977 - val_mse: 67987672.0000 - val_mae: 3121.8977\n",
            "Epoch 294/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2994.2168 - mse: 63834836.0000 - mae: 2994.2168 - val_loss: 3126.7584 - val_mse: 67345208.0000 - val_mae: 3126.7585\n",
            "Epoch 295/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2934.3000 - mse: 62127444.0000 - mae: 2934.3000 - val_loss: 3117.6095 - val_mse: 66899868.0000 - val_mae: 3117.6096\n",
            "Epoch 296/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 2938.3941 - mse: 61108860.0000 - mae: 2938.3940 - val_loss: 3120.7693 - val_mse: 66292004.0000 - val_mae: 3120.7695\n",
            "Epoch 297/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2978.6257 - mse: 63766768.0000 - mae: 2978.6257 - val_loss: 3114.6177 - val_mse: 65828996.0000 - val_mae: 3114.6174\n",
            "Epoch 298/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2980.5761 - mse: 63800976.0000 - mae: 2980.5764 - val_loss: 3091.8138 - val_mse: 65822488.0000 - val_mae: 3091.8137\n",
            "Epoch 299/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2916.3830 - mse: 60725544.0000 - mae: 2916.3831 - val_loss: 3083.2270 - val_mse: 65298164.0000 - val_mae: 3083.2273\n",
            "Epoch 300/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2918.0754 - mse: 60173552.0000 - mae: 2918.0752 - val_loss: 3078.8211 - val_mse: 64673896.0000 - val_mae: 3078.8213\n",
            "Epoch 301/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 2896.6223 - mse: 59670460.0000 - mae: 2896.6223 - val_loss: 3076.8084 - val_mse: 64004132.0000 - val_mae: 3076.8083\n",
            "Epoch 302/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2914.4275 - mse: 58463732.0000 - mae: 2914.4277 - val_loss: 3061.6620 - val_mse: 63657756.0000 - val_mae: 3061.6619\n",
            "Epoch 303/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2931.0248 - mse: 61834432.0000 - mae: 2931.0247 - val_loss: 3057.3367 - val_mse: 63153380.0000 - val_mae: 3057.3367\n",
            "Epoch 304/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2929.2177 - mse: 61099516.0000 - mae: 2929.2178 - val_loss: 3070.1962 - val_mse: 62701248.0000 - val_mae: 3070.1958\n",
            "Epoch 305/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2915.8858 - mse: 59256984.0000 - mae: 2915.8857 - val_loss: 3069.6270 - val_mse: 62209956.0000 - val_mae: 3069.6272\n",
            "Epoch 306/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2920.0774 - mse: 60556008.0000 - mae: 2920.0776 - val_loss: 3078.0621 - val_mse: 61755352.0000 - val_mae: 3078.0620\n",
            "Epoch 307/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 2866.6252 - mse: 58698936.0000 - mae: 2866.6252 - val_loss: 3050.4221 - val_mse: 61325300.0000 - val_mae: 3050.4219\n",
            "Epoch 308/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2813.3528 - mse: 56196388.0000 - mae: 2813.3525 - val_loss: 3029.8925 - val_mse: 61313824.0000 - val_mae: 3029.8926\n",
            "Epoch 309/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2869.9504 - mse: 58108364.0000 - mae: 2869.9502 - val_loss: 3021.1622 - val_mse: 60730580.0000 - val_mae: 3021.1624\n",
            "Epoch 310/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2824.9527 - mse: 56613888.0000 - mae: 2824.9526 - val_loss: 3003.9732 - val_mse: 60596676.0000 - val_mae: 3003.9736\n",
            "Epoch 311/500\n",
            "1880/1880 [==============================] - 0s 83us/step - loss: 2863.6505 - mse: 56552192.0000 - mae: 2863.6506 - val_loss: 3000.9884 - val_mse: 59859184.0000 - val_mae: 3000.9883\n",
            "Epoch 312/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2796.8151 - mse: 54647280.0000 - mae: 2796.8152 - val_loss: 2999.7070 - val_mse: 59340144.0000 - val_mae: 2999.7070\n",
            "Epoch 313/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 2657.9558 - mse: 50073652.0000 - mae: 2657.9556 - val_loss: 3010.2822 - val_mse: 58573000.0000 - val_mae: 3010.2825\n",
            "Epoch 314/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 2792.9855 - mse: 54919232.0000 - mae: 2792.9854 - val_loss: 2973.8929 - val_mse: 58829944.0000 - val_mae: 2973.8928\n",
            "Epoch 315/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2828.5983 - mse: 55609804.0000 - mae: 2828.5984 - val_loss: 2968.1053 - val_mse: 58004344.0000 - val_mae: 2968.1055\n",
            "Epoch 316/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 2796.1838 - mse: 54452224.0000 - mae: 2796.1841 - val_loss: 2995.5602 - val_mse: 57353448.0000 - val_mae: 2995.5598\n",
            "Epoch 317/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2802.8263 - mse: 53985216.0000 - mae: 2802.8264 - val_loss: 2959.3754 - val_mse: 57095508.0000 - val_mae: 2959.3755\n",
            "Epoch 318/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2744.9388 - mse: 52769980.0000 - mae: 2744.9385 - val_loss: 2953.7019 - val_mse: 56623500.0000 - val_mae: 2953.7019\n",
            "Epoch 319/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2831.8002 - mse: 54921836.0000 - mae: 2831.8003 - val_loss: 2936.3487 - val_mse: 56196064.0000 - val_mae: 2936.3486\n",
            "Epoch 320/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2776.4401 - mse: 53429584.0000 - mae: 2776.4399 - val_loss: 2931.0889 - val_mse: 55672356.0000 - val_mae: 2931.0891\n",
            "Epoch 321/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2756.0453 - mse: 52620812.0000 - mae: 2756.0454 - val_loss: 2920.9460 - val_mse: 55273988.0000 - val_mae: 2920.9458\n",
            "Epoch 322/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2670.4487 - mse: 48127944.0000 - mae: 2670.4490 - val_loss: 2913.4493 - val_mse: 54860480.0000 - val_mae: 2913.4495\n",
            "Epoch 323/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2728.1455 - mse: 50688508.0000 - mae: 2728.1458 - val_loss: 2911.4793 - val_mse: 54261908.0000 - val_mae: 2911.4795\n",
            "Epoch 324/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2678.9864 - mse: 49252640.0000 - mae: 2678.9863 - val_loss: 2900.1797 - val_mse: 53873548.0000 - val_mae: 2900.1797\n",
            "Epoch 325/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 2729.1457 - mse: 51701104.0000 - mae: 2729.1458 - val_loss: 2874.6310 - val_mse: 53561192.0000 - val_mae: 2874.6311\n",
            "Epoch 326/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2726.6363 - mse: 51267700.0000 - mae: 2726.6362 - val_loss: 2868.5510 - val_mse: 53260064.0000 - val_mae: 2868.5508\n",
            "Epoch 327/500\n",
            "1880/1880 [==============================] - 0s 82us/step - loss: 2699.8066 - mse: 50166544.0000 - mae: 2699.8066 - val_loss: 2860.9181 - val_mse: 52761716.0000 - val_mae: 2860.9180\n",
            "Epoch 328/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2694.5129 - mse: 51210284.0000 - mae: 2694.5129 - val_loss: 2855.5942 - val_mse: 52313460.0000 - val_mae: 2855.5942\n",
            "Epoch 329/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 2590.8499 - mse: 45530824.0000 - mae: 2590.8501 - val_loss: 2854.3320 - val_mse: 51893540.0000 - val_mae: 2854.3320\n",
            "Epoch 330/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 2583.7210 - mse: 47175680.0000 - mae: 2583.7209 - val_loss: 2830.5640 - val_mse: 51550108.0000 - val_mae: 2830.5640\n",
            "Epoch 331/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2666.8996 - mse: 49891720.0000 - mae: 2666.8997 - val_loss: 2818.0791 - val_mse: 51135864.0000 - val_mae: 2818.0791\n",
            "Epoch 332/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2664.7042 - mse: 49712048.0000 - mae: 2664.7043 - val_loss: 2813.5396 - val_mse: 50948796.0000 - val_mae: 2813.5398\n",
            "Epoch 333/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2646.6290 - mse: 47096008.0000 - mae: 2646.6289 - val_loss: 2815.9989 - val_mse: 50424784.0000 - val_mae: 2815.9988\n",
            "Epoch 334/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 2560.4703 - mse: 44542832.0000 - mae: 2560.4702 - val_loss: 2794.5544 - val_mse: 50162272.0000 - val_mae: 2794.5542\n",
            "Epoch 335/500\n",
            "1880/1880 [==============================] - 0s 83us/step - loss: 2608.0116 - mse: 46376652.0000 - mae: 2608.0115 - val_loss: 2789.4423 - val_mse: 49479836.0000 - val_mae: 2789.4421\n",
            "Epoch 336/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2590.5124 - mse: 45967728.0000 - mae: 2590.5122 - val_loss: 2785.5309 - val_mse: 49221804.0000 - val_mae: 2785.5308\n",
            "Epoch 337/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2605.3129 - mse: 46545696.0000 - mae: 2605.3130 - val_loss: 2771.5953 - val_mse: 49026008.0000 - val_mae: 2771.5955\n",
            "Epoch 338/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2572.3708 - mse: 45856396.0000 - mae: 2572.3708 - val_loss: 2778.6500 - val_mse: 48262216.0000 - val_mae: 2778.6501\n",
            "Epoch 339/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2530.5360 - mse: 43202752.0000 - mae: 2530.5359 - val_loss: 2759.6815 - val_mse: 48013096.0000 - val_mae: 2759.6816\n",
            "Epoch 340/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 2599.9137 - mse: 45846180.0000 - mae: 2599.9138 - val_loss: 2763.4358 - val_mse: 47480552.0000 - val_mae: 2763.4358\n",
            "Epoch 341/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 2499.1254 - mse: 42335604.0000 - mae: 2499.1252 - val_loss: 2752.8479 - val_mse: 47031564.0000 - val_mae: 2752.8479\n",
            "Epoch 342/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2479.2559 - mse: 40662444.0000 - mae: 2479.2559 - val_loss: 2732.6015 - val_mse: 46957868.0000 - val_mae: 2732.6013\n",
            "Epoch 343/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2527.9257 - mse: 41922056.0000 - mae: 2527.9255 - val_loss: 2725.8378 - val_mse: 46744388.0000 - val_mae: 2725.8381\n",
            "Epoch 344/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2508.5429 - mse: 42383740.0000 - mae: 2508.5430 - val_loss: 2724.4890 - val_mse: 46062852.0000 - val_mae: 2724.4890\n",
            "Epoch 345/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2454.5686 - mse: 39861484.0000 - mae: 2454.5688 - val_loss: 2710.9560 - val_mse: 45860996.0000 - val_mae: 2710.9561\n",
            "Epoch 346/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2485.4392 - mse: 40206048.0000 - mae: 2485.4392 - val_loss: 2712.9880 - val_mse: 45166832.0000 - val_mae: 2712.9880\n",
            "Epoch 347/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2591.5522 - mse: 43582500.0000 - mae: 2591.5525 - val_loss: 2688.4767 - val_mse: 44928084.0000 - val_mae: 2688.4768\n",
            "Epoch 348/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2518.1024 - mse: 41887536.0000 - mae: 2518.1023 - val_loss: 2682.6327 - val_mse: 44590944.0000 - val_mae: 2682.6323\n",
            "Epoch 349/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 2418.5891 - mse: 39039664.0000 - mae: 2418.5891 - val_loss: 2674.2029 - val_mse: 44043044.0000 - val_mae: 2674.2026\n",
            "Epoch 350/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2470.6988 - mse: 39886264.0000 - mae: 2470.6987 - val_loss: 2671.3655 - val_mse: 43777772.0000 - val_mae: 2671.3655\n",
            "Epoch 351/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2440.7303 - mse: 41883196.0000 - mae: 2440.7302 - val_loss: 2672.3370 - val_mse: 43636336.0000 - val_mae: 2672.3369\n",
            "Epoch 352/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2399.0749 - mse: 36185084.0000 - mae: 2399.0750 - val_loss: 2663.7679 - val_mse: 42932512.0000 - val_mae: 2663.7678\n",
            "Epoch 353/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2353.9564 - mse: 35916220.0000 - mae: 2353.9563 - val_loss: 2687.6299 - val_mse: 42516124.0000 - val_mae: 2687.6296\n",
            "Epoch 354/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2368.3268 - mse: 37056808.0000 - mae: 2368.3269 - val_loss: 2634.9501 - val_mse: 42347904.0000 - val_mae: 2634.9500\n",
            "Epoch 355/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2414.9227 - mse: 39818972.0000 - mae: 2414.9226 - val_loss: 2636.5997 - val_mse: 41741612.0000 - val_mae: 2636.5999\n",
            "Epoch 356/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 2362.5666 - mse: 36789484.0000 - mae: 2362.5667 - val_loss: 2616.9198 - val_mse: 41584052.0000 - val_mae: 2616.9197\n",
            "Epoch 357/500\n",
            "1880/1880 [==============================] - 0s 84us/step - loss: 2414.8156 - mse: 37173752.0000 - mae: 2414.8157 - val_loss: 2616.7183 - val_mse: 41313852.0000 - val_mae: 2616.7185\n",
            "Epoch 358/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2359.3542 - mse: 36163884.0000 - mae: 2359.3542 - val_loss: 2620.1279 - val_mse: 40962916.0000 - val_mae: 2620.1282\n",
            "Epoch 359/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2310.9506 - mse: 34517484.0000 - mae: 2310.9504 - val_loss: 2599.2611 - val_mse: 40522888.0000 - val_mae: 2599.2610\n",
            "Epoch 360/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 2313.3023 - mse: 36599500.0000 - mae: 2313.3025 - val_loss: 2587.7283 - val_mse: 39935204.0000 - val_mae: 2587.7285\n",
            "Epoch 361/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 2242.3061 - mse: 33609912.0000 - mae: 2242.3059 - val_loss: 2567.1962 - val_mse: 39654564.0000 - val_mae: 2567.1965\n",
            "Epoch 362/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 2322.8205 - mse: 35882648.0000 - mae: 2322.8203 - val_loss: 2563.6152 - val_mse: 39603816.0000 - val_mae: 2563.6152\n",
            "Epoch 363/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2253.9466 - mse: 31587456.0000 - mae: 2253.9465 - val_loss: 2559.7368 - val_mse: 39006360.0000 - val_mae: 2559.7366\n",
            "Epoch 364/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 2277.7798 - mse: 34329228.0000 - mae: 2277.7798 - val_loss: 2582.9123 - val_mse: 38701264.0000 - val_mae: 2582.9124\n",
            "Epoch 365/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2312.5194 - mse: 34149236.0000 - mae: 2312.5195 - val_loss: 2557.0365 - val_mse: 38426748.0000 - val_mae: 2557.0364\n",
            "Epoch 366/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2269.6307 - mse: 34221296.0000 - mae: 2269.6309 - val_loss: 2578.1237 - val_mse: 38045700.0000 - val_mae: 2578.1240\n",
            "Epoch 367/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 2224.7806 - mse: 32169754.0000 - mae: 2224.7808 - val_loss: 2550.5966 - val_mse: 38013880.0000 - val_mae: 2550.5967\n",
            "Epoch 368/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2208.4687 - mse: 31698958.0000 - mae: 2208.4685 - val_loss: 2538.0802 - val_mse: 37487256.0000 - val_mae: 2538.0803\n",
            "Epoch 369/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2127.4395 - mse: 28369572.0000 - mae: 2127.4395 - val_loss: 2525.3436 - val_mse: 37611344.0000 - val_mae: 2525.3435\n",
            "Epoch 370/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2248.2242 - mse: 32649428.0000 - mae: 2248.2241 - val_loss: 2562.8952 - val_mse: 37172148.0000 - val_mae: 2562.8953\n",
            "Epoch 371/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2220.4870 - mse: 31325176.0000 - mae: 2220.4871 - val_loss: 2535.8041 - val_mse: 37011704.0000 - val_mae: 2535.8042\n",
            "Epoch 372/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 2226.1588 - mse: 33438886.0000 - mae: 2226.1587 - val_loss: 2516.5700 - val_mse: 37071548.0000 - val_mae: 2516.5701\n",
            "Epoch 373/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2150.5216 - mse: 29672532.0000 - mae: 2150.5217 - val_loss: 2514.1633 - val_mse: 36830460.0000 - val_mae: 2514.1631\n",
            "Epoch 374/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 2121.6547 - mse: 28447158.0000 - mae: 2121.6548 - val_loss: 2521.2549 - val_mse: 36426316.0000 - val_mae: 2521.2549\n",
            "Epoch 375/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 2174.6990 - mse: 29624874.0000 - mae: 2174.6987 - val_loss: 2505.5769 - val_mse: 36547100.0000 - val_mae: 2505.5769\n",
            "Epoch 376/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2162.3825 - mse: 29486666.0000 - mae: 2162.3826 - val_loss: 2521.0141 - val_mse: 36035896.0000 - val_mae: 2521.0139\n",
            "Epoch 377/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 2138.4722 - mse: 29160498.0000 - mae: 2138.4722 - val_loss: 2506.6252 - val_mse: 35934156.0000 - val_mae: 2506.6252\n",
            "Epoch 378/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 2163.2604 - mse: 29534818.0000 - mae: 2163.2603 - val_loss: 2482.9182 - val_mse: 35779532.0000 - val_mae: 2482.9182\n",
            "Epoch 379/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2138.1203 - mse: 29133006.0000 - mae: 2138.1204 - val_loss: 2492.1960 - val_mse: 35211428.0000 - val_mae: 2492.1960\n",
            "Epoch 380/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2181.3559 - mse: 31343260.0000 - mae: 2181.3560 - val_loss: 2496.8631 - val_mse: 35090912.0000 - val_mae: 2496.8630\n",
            "Epoch 381/500\n",
            "1880/1880 [==============================] - 0s 83us/step - loss: 2164.7498 - mse: 30396016.0000 - mae: 2164.7498 - val_loss: 2491.9850 - val_mse: 34861512.0000 - val_mae: 2491.9851\n",
            "Epoch 382/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2090.3200 - mse: 26840512.0000 - mae: 2090.3201 - val_loss: 2478.2495 - val_mse: 34863628.0000 - val_mae: 2478.2493\n",
            "Epoch 383/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2150.1916 - mse: 29104616.0000 - mae: 2150.1914 - val_loss: 2469.4873 - val_mse: 34983200.0000 - val_mae: 2469.4875\n",
            "Epoch 384/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 2132.7329 - mse: 29397046.0000 - mae: 2132.7329 - val_loss: 2444.2581 - val_mse: 34423088.0000 - val_mae: 2444.2578\n",
            "Epoch 385/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2125.3854 - mse: 27576764.0000 - mae: 2125.3853 - val_loss: 2434.9163 - val_mse: 34013960.0000 - val_mae: 2434.9163\n",
            "Epoch 386/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2088.7614 - mse: 27515958.0000 - mae: 2088.7615 - val_loss: 2431.8931 - val_mse: 33646708.0000 - val_mae: 2431.8931\n",
            "Epoch 387/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2063.4179 - mse: 27486950.0000 - mae: 2063.4180 - val_loss: 2469.3373 - val_mse: 33323890.0000 - val_mae: 2469.3372\n",
            "Epoch 388/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2055.1079 - mse: 27378574.0000 - mae: 2055.1079 - val_loss: 2431.2951 - val_mse: 33459952.0000 - val_mae: 2431.2949\n",
            "Epoch 389/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2048.5575 - mse: 25687278.0000 - mae: 2048.5574 - val_loss: 2402.8846 - val_mse: 33371396.0000 - val_mae: 2402.8845\n",
            "Epoch 390/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 2114.0184 - mse: 28203072.0000 - mae: 2114.0183 - val_loss: 2421.4817 - val_mse: 32663992.0000 - val_mae: 2421.4822\n",
            "Epoch 391/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 2104.7302 - mse: 28155378.0000 - mae: 2104.7300 - val_loss: 2429.2036 - val_mse: 32231512.0000 - val_mae: 2429.2034\n",
            "Epoch 392/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2046.9976 - mse: 26342858.0000 - mae: 2046.9976 - val_loss: 2391.5193 - val_mse: 32370436.0000 - val_mae: 2391.5193\n",
            "Epoch 393/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2067.4503 - mse: 26924012.0000 - mae: 2067.4504 - val_loss: 2394.1107 - val_mse: 32454154.0000 - val_mae: 2394.1111\n",
            "Epoch 394/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2088.8936 - mse: 27256734.0000 - mae: 2088.8936 - val_loss: 2388.9008 - val_mse: 32473052.0000 - val_mae: 2388.9011\n",
            "Epoch 395/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2012.8141 - mse: 24460762.0000 - mae: 2012.8141 - val_loss: 2424.7221 - val_mse: 31921880.0000 - val_mae: 2424.7222\n",
            "Epoch 396/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1984.1394 - mse: 23071910.0000 - mae: 1984.1394 - val_loss: 2390.4956 - val_mse: 32120954.0000 - val_mae: 2390.4961\n",
            "Epoch 397/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 2016.6899 - mse: 24600040.0000 - mae: 2016.6898 - val_loss: 2383.3801 - val_mse: 31918220.0000 - val_mae: 2383.3799\n",
            "Epoch 398/500\n",
            "1880/1880 [==============================] - 0s 101us/step - loss: 2081.9309 - mse: 27473690.0000 - mae: 2081.9309 - val_loss: 2368.7577 - val_mse: 31950922.0000 - val_mae: 2368.7578\n",
            "Epoch 399/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2036.7821 - mse: 26418980.0000 - mae: 2036.7821 - val_loss: 2378.0066 - val_mse: 31438774.0000 - val_mae: 2378.0066\n",
            "Epoch 400/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 1986.4237 - mse: 25432982.0000 - mae: 1986.4238 - val_loss: 2362.7714 - val_mse: 31655410.0000 - val_mae: 2362.7715\n",
            "Epoch 401/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2013.9581 - mse: 24673466.0000 - mae: 2013.9581 - val_loss: 2359.3708 - val_mse: 31672186.0000 - val_mae: 2359.3706\n",
            "Epoch 402/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1996.5138 - mse: 24669624.0000 - mae: 1996.5138 - val_loss: 2351.6421 - val_mse: 31749728.0000 - val_mae: 2351.6423\n",
            "Epoch 403/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1986.1688 - mse: 24065480.0000 - mae: 1986.1687 - val_loss: 2360.1586 - val_mse: 31500320.0000 - val_mae: 2360.1584\n",
            "Epoch 404/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2096.2163 - mse: 27830708.0000 - mae: 2096.2163 - val_loss: 2350.4685 - val_mse: 30976270.0000 - val_mae: 2350.4685\n",
            "Epoch 405/500\n",
            "1880/1880 [==============================] - 0s 93us/step - loss: 1983.9007 - mse: 24026696.0000 - mae: 1983.9005 - val_loss: 2361.4510 - val_mse: 30713426.0000 - val_mae: 2361.4512\n",
            "Epoch 406/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1967.3950 - mse: 24119520.0000 - mae: 1967.3949 - val_loss: 2344.1385 - val_mse: 31393342.0000 - val_mae: 2344.1384\n",
            "Epoch 407/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2056.8227 - mse: 25629460.0000 - mae: 2056.8228 - val_loss: 2336.0982 - val_mse: 30829664.0000 - val_mae: 2336.0981\n",
            "Epoch 408/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2024.7560 - mse: 25580184.0000 - mae: 2024.7560 - val_loss: 2328.1240 - val_mse: 30743814.0000 - val_mae: 2328.1240\n",
            "Epoch 409/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2006.0970 - mse: 24994554.0000 - mae: 2006.0969 - val_loss: 2331.2688 - val_mse: 30449950.0000 - val_mae: 2331.2688\n",
            "Epoch 410/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2061.4755 - mse: 26355620.0000 - mae: 2061.4756 - val_loss: 2312.3398 - val_mse: 29471964.0000 - val_mae: 2312.3401\n",
            "Epoch 411/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 1962.4834 - mse: 23711022.0000 - mae: 1962.4833 - val_loss: 2305.8595 - val_mse: 29917462.0000 - val_mae: 2305.8596\n",
            "Epoch 412/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 2016.1711 - mse: 23955088.0000 - mae: 2016.1711 - val_loss: 2298.5179 - val_mse: 29776546.0000 - val_mae: 2298.5178\n",
            "Epoch 413/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1926.6191 - mse: 22846546.0000 - mae: 1926.6191 - val_loss: 2292.1424 - val_mse: 29362806.0000 - val_mae: 2292.1426\n",
            "Epoch 414/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2043.4024 - mse: 25586576.0000 - mae: 2043.4022 - val_loss: 2295.9773 - val_mse: 29802026.0000 - val_mae: 2295.9773\n",
            "Epoch 415/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 1961.6336 - mse: 22604540.0000 - mae: 1961.6335 - val_loss: 2298.1724 - val_mse: 29335874.0000 - val_mae: 2298.1724\n",
            "Epoch 416/500\n",
            "1880/1880 [==============================] - 0s 83us/step - loss: 1906.2181 - mse: 21821800.0000 - mae: 1906.2183 - val_loss: 2320.7060 - val_mse: 29358472.0000 - val_mae: 2320.7058\n",
            "Epoch 417/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 1994.6510 - mse: 24169518.0000 - mae: 1994.6509 - val_loss: 2321.6153 - val_mse: 29025660.0000 - val_mae: 2321.6152\n",
            "Epoch 418/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2006.2280 - mse: 25580854.0000 - mae: 2006.2279 - val_loss: 2292.2212 - val_mse: 29296210.0000 - val_mae: 2292.2214\n",
            "Epoch 419/500\n",
            "1880/1880 [==============================] - 0s 82us/step - loss: 1917.4230 - mse: 22031416.0000 - mae: 1917.4230 - val_loss: 2308.3994 - val_mse: 29193596.0000 - val_mae: 2308.3997\n",
            "Epoch 420/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1970.9522 - mse: 24129984.0000 - mae: 1970.9523 - val_loss: 2285.3019 - val_mse: 29245318.0000 - val_mae: 2285.3022\n",
            "Epoch 421/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1973.6101 - mse: 23933914.0000 - mae: 1973.6100 - val_loss: 2305.4530 - val_mse: 29204056.0000 - val_mae: 2305.4529\n",
            "Epoch 422/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 1924.2137 - mse: 22437804.0000 - mae: 1924.2137 - val_loss: 2307.0898 - val_mse: 28812764.0000 - val_mae: 2307.0898\n",
            "Epoch 423/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 1947.6228 - mse: 21773036.0000 - mae: 1947.6227 - val_loss: 2255.8264 - val_mse: 28403708.0000 - val_mae: 2255.8264\n",
            "Epoch 424/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1995.6466 - mse: 24458198.0000 - mae: 1995.6465 - val_loss: 2260.2060 - val_mse: 28561142.0000 - val_mae: 2260.2061\n",
            "Epoch 425/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1981.4864 - mse: 24232170.0000 - mae: 1981.4866 - val_loss: 2270.2345 - val_mse: 27981288.0000 - val_mae: 2270.2344\n",
            "Epoch 426/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1936.6481 - mse: 22128814.0000 - mae: 1936.6479 - val_loss: 2265.0117 - val_mse: 27946734.0000 - val_mae: 2265.0117\n",
            "Epoch 427/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1938.1757 - mse: 22476570.0000 - mae: 1938.1757 - val_loss: 2256.6201 - val_mse: 28140432.0000 - val_mae: 2256.6201\n",
            "Epoch 428/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 1937.1147 - mse: 22929660.0000 - mae: 1937.1147 - val_loss: 2272.6644 - val_mse: 28192912.0000 - val_mae: 2272.6646\n",
            "Epoch 429/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 1968.8945 - mse: 22916684.0000 - mae: 1968.8945 - val_loss: 2243.2416 - val_mse: 27993244.0000 - val_mae: 2243.2417\n",
            "Epoch 430/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1982.3845 - mse: 23409868.0000 - mae: 1982.3843 - val_loss: 2219.8950 - val_mse: 27545360.0000 - val_mae: 2219.8950\n",
            "Epoch 431/500\n",
            "1880/1880 [==============================] - 0s 84us/step - loss: 1972.5174 - mse: 23755824.0000 - mae: 1972.5175 - val_loss: 2271.9488 - val_mse: 27639760.0000 - val_mae: 2271.9490\n",
            "Epoch 432/500\n",
            "1880/1880 [==============================] - 0s 82us/step - loss: 1945.8116 - mse: 22202276.0000 - mae: 1945.8116 - val_loss: 2257.6031 - val_mse: 27248228.0000 - val_mae: 2257.6033\n",
            "Epoch 433/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 1966.9823 - mse: 24646404.0000 - mae: 1966.9824 - val_loss: 2244.6873 - val_mse: 27344590.0000 - val_mae: 2244.6875\n",
            "Epoch 434/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1857.2998 - mse: 20355502.0000 - mae: 1857.2997 - val_loss: 2226.0261 - val_mse: 27409720.0000 - val_mae: 2226.0259\n",
            "Epoch 435/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1958.4321 - mse: 23590106.0000 - mae: 1958.4321 - val_loss: 2265.7829 - val_mse: 27474502.0000 - val_mae: 2265.7830\n",
            "Epoch 436/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 1911.2264 - mse: 22320638.0000 - mae: 1911.2263 - val_loss: 2227.0632 - val_mse: 27412716.0000 - val_mae: 2227.0630\n",
            "Epoch 437/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1871.3645 - mse: 21670258.0000 - mae: 1871.3645 - val_loss: 2230.5449 - val_mse: 27641580.0000 - val_mae: 2230.5449\n",
            "Epoch 438/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2020.9663 - mse: 25004500.0000 - mae: 2020.9662 - val_loss: 2233.1819 - val_mse: 27015620.0000 - val_mae: 2233.1819\n",
            "Epoch 439/500\n",
            "1880/1880 [==============================] - 0s 92us/step - loss: 1820.8008 - mse: 19860774.0000 - mae: 1820.8009 - val_loss: 2214.5500 - val_mse: 27072564.0000 - val_mae: 2214.5500\n",
            "Epoch 440/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 1946.6422 - mse: 22623188.0000 - mae: 1946.6423 - val_loss: 2225.0837 - val_mse: 26859660.0000 - val_mae: 2225.0837\n",
            "Epoch 441/500\n",
            "1880/1880 [==============================] - 0s 87us/step - loss: 1925.8654 - mse: 22443634.0000 - mae: 1925.8655 - val_loss: 2207.7628 - val_mse: 27049316.0000 - val_mae: 2207.7629\n",
            "Epoch 442/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 1886.7218 - mse: 22374846.0000 - mae: 1886.7218 - val_loss: 2266.5088 - val_mse: 26856408.0000 - val_mae: 2266.5088\n",
            "Epoch 443/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1948.0061 - mse: 23064136.0000 - mae: 1948.0061 - val_loss: 2204.9057 - val_mse: 27037932.0000 - val_mae: 2204.9055\n",
            "Epoch 444/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 1924.3430 - mse: 22318328.0000 - mae: 1924.3429 - val_loss: 2198.1262 - val_mse: 26725158.0000 - val_mae: 2198.1262\n",
            "Epoch 445/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1882.3305 - mse: 21176700.0000 - mae: 1882.3306 - val_loss: 2220.8691 - val_mse: 27024672.0000 - val_mae: 2220.8691\n",
            "Epoch 446/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 1928.4025 - mse: 22287438.0000 - mae: 1928.4027 - val_loss: 2208.2999 - val_mse: 26532844.0000 - val_mae: 2208.2998\n",
            "Epoch 447/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 1901.6397 - mse: 20856948.0000 - mae: 1901.6398 - val_loss: 2213.6884 - val_mse: 26575670.0000 - val_mae: 2213.6885\n",
            "Epoch 448/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1824.4978 - mse: 19825472.0000 - mae: 1824.4979 - val_loss: 2199.4560 - val_mse: 26297642.0000 - val_mae: 2199.4561\n",
            "Epoch 449/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 1879.7519 - mse: 21151740.0000 - mae: 1879.7518 - val_loss: 2179.9698 - val_mse: 26460876.0000 - val_mae: 2179.9697\n",
            "Epoch 450/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1865.8251 - mse: 21577088.0000 - mae: 1865.8253 - val_loss: 2188.4941 - val_mse: 26534240.0000 - val_mae: 2188.4941\n",
            "Epoch 451/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1941.5572 - mse: 22496616.0000 - mae: 1941.5574 - val_loss: 2192.1971 - val_mse: 25717632.0000 - val_mae: 2192.1970\n",
            "Epoch 452/500\n",
            "1880/1880 [==============================] - 0s 82us/step - loss: 1931.2306 - mse: 22997656.0000 - mae: 1931.2307 - val_loss: 2195.5605 - val_mse: 26217822.0000 - val_mae: 2195.5608\n",
            "Epoch 453/500\n",
            "1880/1880 [==============================] - 0s 89us/step - loss: 1902.5553 - mse: 22263640.0000 - mae: 1902.5553 - val_loss: 2177.3553 - val_mse: 25768820.0000 - val_mae: 2177.3552\n",
            "Epoch 454/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1843.9457 - mse: 19603746.0000 - mae: 1843.9456 - val_loss: 2175.7071 - val_mse: 25888004.0000 - val_mae: 2175.7070\n",
            "Epoch 455/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1898.7587 - mse: 22248366.0000 - mae: 1898.7588 - val_loss: 2168.2771 - val_mse: 25836052.0000 - val_mae: 2168.2771\n",
            "Epoch 456/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1913.9404 - mse: 22946212.0000 - mae: 1913.9404 - val_loss: 2182.0047 - val_mse: 25532642.0000 - val_mae: 2182.0046\n",
            "Epoch 457/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1849.1994 - mse: 19865724.0000 - mae: 1849.1996 - val_loss: 2174.4276 - val_mse: 26207526.0000 - val_mae: 2174.4277\n",
            "Epoch 458/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 1855.8929 - mse: 20769484.0000 - mae: 1855.8928 - val_loss: 2177.9936 - val_mse: 25938780.0000 - val_mae: 2177.9937\n",
            "Epoch 459/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1868.0805 - mse: 21231576.0000 - mae: 1868.0806 - val_loss: 2164.3104 - val_mse: 24983286.0000 - val_mae: 2164.3105\n",
            "Epoch 460/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1852.4970 - mse: 20145984.0000 - mae: 1852.4969 - val_loss: 2136.5413 - val_mse: 24857660.0000 - val_mae: 2136.5415\n",
            "Epoch 461/500\n",
            "1880/1880 [==============================] - 0s 83us/step - loss: 1836.7153 - mse: 20417674.0000 - mae: 1836.7155 - val_loss: 2178.1622 - val_mse: 25620706.0000 - val_mae: 2178.1621\n",
            "Epoch 462/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 1843.6448 - mse: 19984534.0000 - mae: 1843.6449 - val_loss: 2175.4553 - val_mse: 26196614.0000 - val_mae: 2175.4553\n",
            "Epoch 463/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 1828.3478 - mse: 19282470.0000 - mae: 1828.3479 - val_loss: 2153.4991 - val_mse: 25310794.0000 - val_mae: 2153.4990\n",
            "Epoch 464/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 1873.2646 - mse: 21333466.0000 - mae: 1873.2646 - val_loss: 2187.3928 - val_mse: 25224090.0000 - val_mae: 2187.3931\n",
            "Epoch 465/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1943.7649 - mse: 22821122.0000 - mae: 1943.7648 - val_loss: 2173.6896 - val_mse: 25563356.0000 - val_mae: 2173.6895\n",
            "Epoch 466/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1877.9323 - mse: 20374240.0000 - mae: 1877.9324 - val_loss: 2176.9418 - val_mse: 25104432.0000 - val_mae: 2176.9419\n",
            "Epoch 467/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1866.6839 - mse: 20694854.0000 - mae: 1866.6840 - val_loss: 2138.7670 - val_mse: 25009334.0000 - val_mae: 2138.7671\n",
            "Epoch 468/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 1954.8800 - mse: 22564890.0000 - mae: 1954.8800 - val_loss: 2170.2834 - val_mse: 25032200.0000 - val_mae: 2170.2832\n",
            "Epoch 469/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1896.7172 - mse: 21552142.0000 - mae: 1896.7172 - val_loss: 2177.3922 - val_mse: 24973800.0000 - val_mae: 2177.3923\n",
            "Epoch 470/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1848.1437 - mse: 19259218.0000 - mae: 1848.1437 - val_loss: 2153.2132 - val_mse: 25226640.0000 - val_mae: 2153.2131\n",
            "Epoch 471/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1871.2699 - mse: 21354204.0000 - mae: 1871.2699 - val_loss: 2120.6448 - val_mse: 24847458.0000 - val_mae: 2120.6448\n",
            "Epoch 472/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 1856.3001 - mse: 21076656.0000 - mae: 1856.3002 - val_loss: 2152.7495 - val_mse: 25255426.0000 - val_mae: 2152.7495\n",
            "Epoch 473/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1941.5730 - mse: 21725144.0000 - mae: 1941.5730 - val_loss: 2166.8747 - val_mse: 24997556.0000 - val_mae: 2166.8745\n",
            "Epoch 474/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1827.3568 - mse: 19552948.0000 - mae: 1827.3568 - val_loss: 2133.3004 - val_mse: 24971486.0000 - val_mae: 2133.3000\n",
            "Epoch 475/500\n",
            "1880/1880 [==============================] - 0s 87us/step - loss: 1881.2350 - mse: 20666142.0000 - mae: 1881.2349 - val_loss: 2150.8149 - val_mse: 24675416.0000 - val_mae: 2150.8147\n",
            "Epoch 476/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 1915.8807 - mse: 21827216.0000 - mae: 1915.8806 - val_loss: 2116.8778 - val_mse: 24399892.0000 - val_mae: 2116.8777\n",
            "Epoch 477/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 1828.8271 - mse: 19985160.0000 - mae: 1828.8271 - val_loss: 2133.9220 - val_mse: 24969418.0000 - val_mae: 2133.9221\n",
            "Epoch 478/500\n",
            "1880/1880 [==============================] - 0s 83us/step - loss: 1870.8643 - mse: 20768100.0000 - mae: 1870.8645 - val_loss: 2119.7406 - val_mse: 24693180.0000 - val_mae: 2119.7407\n",
            "Epoch 479/500\n",
            "1880/1880 [==============================] - 0s 85us/step - loss: 1832.1700 - mse: 20588406.0000 - mae: 1832.1699 - val_loss: 2155.3896 - val_mse: 24401438.0000 - val_mae: 2155.3896\n",
            "Epoch 480/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1812.6663 - mse: 19456522.0000 - mae: 1812.6664 - val_loss: 2148.4870 - val_mse: 24609116.0000 - val_mae: 2148.4871\n",
            "Epoch 481/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1943.4544 - mse: 24029716.0000 - mae: 1943.4543 - val_loss: 2130.1046 - val_mse: 24590148.0000 - val_mae: 2130.1047\n",
            "Epoch 482/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 1887.7774 - mse: 21401178.0000 - mae: 1887.7773 - val_loss: 2117.5047 - val_mse: 24718794.0000 - val_mae: 2117.5049\n",
            "Epoch 483/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1862.7789 - mse: 21103608.0000 - mae: 1862.7788 - val_loss: 2119.2490 - val_mse: 24631668.0000 - val_mae: 2119.2493\n",
            "Epoch 484/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 1803.4899 - mse: 18743672.0000 - mae: 1803.4899 - val_loss: 2130.9585 - val_mse: 25018526.0000 - val_mae: 2130.9585\n",
            "Epoch 485/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1927.4582 - mse: 22391826.0000 - mae: 1927.4581 - val_loss: 2120.5760 - val_mse: 24228032.0000 - val_mae: 2120.5762\n",
            "Epoch 486/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1925.4522 - mse: 23963976.0000 - mae: 1925.4521 - val_loss: 2110.3094 - val_mse: 24515294.0000 - val_mae: 2110.3096\n",
            "Epoch 487/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1890.8767 - mse: 21306142.0000 - mae: 1890.8767 - val_loss: 2136.2232 - val_mse: 24693646.0000 - val_mae: 2136.2231\n",
            "Epoch 488/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 1790.8046 - mse: 18088174.0000 - mae: 1790.8047 - val_loss: 2158.5309 - val_mse: 24175664.0000 - val_mae: 2158.5308\n",
            "Epoch 489/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 1866.9492 - mse: 20776926.0000 - mae: 1866.9492 - val_loss: 2114.9038 - val_mse: 24338300.0000 - val_mae: 2114.9038\n",
            "Epoch 490/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1875.1756 - mse: 21135702.0000 - mae: 1875.1758 - val_loss: 2111.3309 - val_mse: 24691534.0000 - val_mae: 2111.3308\n",
            "Epoch 491/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 1810.9882 - mse: 19082002.0000 - mae: 1810.9883 - val_loss: 2110.4669 - val_mse: 24617650.0000 - val_mae: 2110.4668\n",
            "Epoch 492/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1935.1754 - mse: 23120380.0000 - mae: 1935.1755 - val_loss: 2117.8572 - val_mse: 24210242.0000 - val_mae: 2117.8574\n",
            "Epoch 493/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1770.1808 - mse: 18293056.0000 - mae: 1770.1807 - val_loss: 2124.0532 - val_mse: 24433218.0000 - val_mae: 2124.0532\n",
            "Epoch 494/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 1838.8313 - mse: 20073644.0000 - mae: 1838.8313 - val_loss: 2119.2619 - val_mse: 24815874.0000 - val_mae: 2119.2620\n",
            "Epoch 495/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1936.4967 - mse: 23220230.0000 - mae: 1936.4968 - val_loss: 2107.3995 - val_mse: 24082314.0000 - val_mae: 2107.3997\n",
            "Epoch 496/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 1881.4267 - mse: 20759176.0000 - mae: 1881.4268 - val_loss: 2102.5272 - val_mse: 24258726.0000 - val_mae: 2102.5273\n",
            "Epoch 497/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1877.8921 - mse: 21389434.0000 - mae: 1877.8923 - val_loss: 2124.6236 - val_mse: 24168918.0000 - val_mae: 2124.6235\n",
            "Epoch 498/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 1846.2029 - mse: 20313840.0000 - mae: 1846.2029 - val_loss: 2111.9935 - val_mse: 24202206.0000 - val_mae: 2111.9937\n",
            "Epoch 499/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1868.8292 - mse: 21266938.0000 - mae: 1868.8290 - val_loss: 2098.1166 - val_mse: 23963224.0000 - val_mae: 2098.1167\n",
            "Epoch 500/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 1884.0337 - mse: 21675112.0000 - mae: 1884.0337 - val_loss: 2121.3892 - val_mse: 24023444.0000 - val_mae: 2121.3892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 2250 samples, validate on 2250 samples\n",
            "Epoch 1/500\n",
            "2250/2250 [==============================] - 2s 965us/step - loss: 4883.8504 - mse: 166353072.0000 - mae: 4883.8506 - val_loss: 5319.2644 - val_mse: 191311728.0000 - val_mae: 5319.2646\n",
            "Epoch 2/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4880.1178 - mse: 166338208.0000 - mae: 4880.1177 - val_loss: 5307.6227 - val_mse: 191261536.0000 - val_mae: 5307.6226\n",
            "Epoch 3/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 4859.1408 - mse: 166253744.0000 - mae: 4859.1411 - val_loss: 5273.4137 - val_mse: 191102320.0000 - val_mae: 5273.4136\n",
            "Epoch 4/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 4825.5197 - mse: 166098480.0000 - mae: 4825.5200 - val_loss: 5234.6889 - val_mse: 190890304.0000 - val_mae: 5234.6890\n",
            "Epoch 5/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4786.9626 - mse: 165883424.0000 - mae: 4786.9624 - val_loss: 5190.3081 - val_mse: 190605760.0000 - val_mae: 5190.3086\n",
            "Epoch 6/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4745.7734 - mse: 165600736.0000 - mae: 4745.7734 - val_loss: 5149.1397 - val_mse: 190282816.0000 - val_mae: 5149.1401\n",
            "Epoch 7/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4710.5349 - mse: 165323824.0000 - mae: 4710.5352 - val_loss: 5112.5593 - val_mse: 189943872.0000 - val_mae: 5112.5596\n",
            "Epoch 8/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4678.8869 - mse: 165010976.0000 - mae: 4678.8872 - val_loss: 5082.2222 - val_mse: 189596960.0000 - val_mae: 5082.2227\n",
            "Epoch 9/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4661.3894 - mse: 164741904.0000 - mae: 4661.3892 - val_loss: 5059.6337 - val_mse: 189276528.0000 - val_mae: 5059.6328\n",
            "Epoch 10/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4649.1574 - mse: 164459008.0000 - mae: 4649.1577 - val_loss: 5041.2570 - val_mse: 188966064.0000 - val_mae: 5041.2573\n",
            "Epoch 11/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4635.0069 - mse: 164158384.0000 - mae: 4635.0073 - val_loss: 5025.8634 - val_mse: 188656624.0000 - val_mae: 5025.8633\n",
            "Epoch 12/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4622.3211 - mse: 163871216.0000 - mae: 4622.3208 - val_loss: 5013.0833 - val_mse: 188350800.0000 - val_mae: 5013.0830\n",
            "Epoch 13/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4615.2822 - mse: 163547456.0000 - mae: 4615.2827 - val_loss: 5002.0004 - val_mse: 188065664.0000 - val_mae: 5002.0005\n",
            "Epoch 14/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4603.4742 - mse: 163296016.0000 - mae: 4603.4741 - val_loss: 4991.8260 - val_mse: 187777392.0000 - val_mae: 4991.8257\n",
            "Epoch 15/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4596.1300 - mse: 163001440.0000 - mae: 4596.1304 - val_loss: 4983.8046 - val_mse: 187516800.0000 - val_mae: 4983.8052\n",
            "Epoch 16/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 4586.2362 - mse: 162784432.0000 - mae: 4586.2363 - val_loss: 4977.9703 - val_mse: 187302624.0000 - val_mae: 4977.9697\n",
            "Epoch 17/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 4593.6402 - mse: 162693296.0000 - mae: 4593.6406 - val_loss: 4973.5148 - val_mse: 187117856.0000 - val_mae: 4973.5146\n",
            "Epoch 18/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4591.5138 - mse: 162510576.0000 - mae: 4591.5137 - val_loss: 4970.1864 - val_mse: 186963408.0000 - val_mae: 4970.1855\n",
            "Epoch 19/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4582.9630 - mse: 162313616.0000 - mae: 4582.9629 - val_loss: 4967.7672 - val_mse: 186847024.0000 - val_mae: 4967.7676\n",
            "Epoch 20/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4582.5913 - mse: 162152688.0000 - mae: 4582.5918 - val_loss: 4964.5649 - val_mse: 186690688.0000 - val_mae: 4964.5654\n",
            "Epoch 21/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4573.7004 - mse: 162036704.0000 - mae: 4573.7007 - val_loss: 4962.2660 - val_mse: 186577568.0000 - val_mae: 4962.2656\n",
            "Epoch 22/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4573.4169 - mse: 161974624.0000 - mae: 4573.4170 - val_loss: 4959.4836 - val_mse: 186435072.0000 - val_mae: 4959.4839\n",
            "Epoch 23/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4576.2749 - mse: 161872784.0000 - mae: 4576.2749 - val_loss: 4957.2423 - val_mse: 186316704.0000 - val_mae: 4957.2422\n",
            "Epoch 24/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4570.6306 - mse: 161734032.0000 - mae: 4570.6309 - val_loss: 4953.7739 - val_mse: 186088832.0000 - val_mae: 4953.7744\n",
            "Epoch 25/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 4574.2954 - mse: 161491808.0000 - mae: 4574.2949 - val_loss: 4950.0762 - val_mse: 185791328.0000 - val_mae: 4950.0767\n",
            "Epoch 26/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4565.9868 - mse: 161315152.0000 - mae: 4565.9868 - val_loss: 4946.7970 - val_mse: 185593120.0000 - val_mae: 4946.7974\n",
            "Epoch 27/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4562.8359 - mse: 161206624.0000 - mae: 4562.8359 - val_loss: 4942.6055 - val_mse: 185249072.0000 - val_mae: 4942.6060\n",
            "Epoch 28/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4560.8897 - mse: 160744160.0000 - mae: 4560.8896 - val_loss: 4939.3072 - val_mse: 185048320.0000 - val_mae: 4939.3066\n",
            "Epoch 29/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4551.1994 - mse: 160512512.0000 - mae: 4551.1992 - val_loss: 4935.6840 - val_mse: 184843808.0000 - val_mae: 4935.6841\n",
            "Epoch 30/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4558.0940 - mse: 160394656.0000 - mae: 4558.0938 - val_loss: 4931.4069 - val_mse: 184525136.0000 - val_mae: 4931.4067\n",
            "Epoch 31/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4555.8772 - mse: 160188464.0000 - mae: 4555.8774 - val_loss: 4927.8090 - val_mse: 184299856.0000 - val_mae: 4927.8096\n",
            "Epoch 32/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4545.7233 - mse: 159992384.0000 - mae: 4545.7231 - val_loss: 4923.8887 - val_mse: 184045936.0000 - val_mae: 4923.8887\n",
            "Epoch 33/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4536.8178 - mse: 159640048.0000 - mae: 4536.8184 - val_loss: 4919.6636 - val_mse: 183745184.0000 - val_mae: 4919.6641\n",
            "Epoch 34/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4538.0840 - mse: 159315392.0000 - mae: 4538.0835 - val_loss: 4916.4733 - val_mse: 183545824.0000 - val_mae: 4916.4731\n",
            "Epoch 35/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4530.7519 - mse: 158999520.0000 - mae: 4530.7520 - val_loss: 4914.3214 - val_mse: 183394496.0000 - val_mae: 4914.3213\n",
            "Epoch 36/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4543.9666 - mse: 159212320.0000 - mae: 4543.9668 - val_loss: 4911.3601 - val_mse: 183238240.0000 - val_mae: 4911.3604\n",
            "Epoch 37/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4527.7515 - mse: 159050464.0000 - mae: 4527.7510 - val_loss: 4900.8219 - val_mse: 182976000.0000 - val_mae: 4900.8218\n",
            "Epoch 38/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4498.3601 - mse: 158756768.0000 - mae: 4498.3599 - val_loss: 4875.2430 - val_mse: 182722192.0000 - val_mae: 4875.2432\n",
            "Epoch 39/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4472.6171 - mse: 158309424.0000 - mae: 4472.6167 - val_loss: 4873.0747 - val_mse: 182208768.0000 - val_mae: 4873.0747\n",
            "Epoch 40/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4458.3078 - mse: 157709600.0000 - mae: 4458.3076 - val_loss: 4830.3900 - val_mse: 181117296.0000 - val_mae: 4830.3901\n",
            "Epoch 41/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4449.9414 - mse: 157053024.0000 - mae: 4449.9414 - val_loss: 4830.5144 - val_mse: 180729408.0000 - val_mae: 4830.5137\n",
            "Epoch 42/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4443.6269 - mse: 156640816.0000 - mae: 4443.6265 - val_loss: 4819.3513 - val_mse: 180090512.0000 - val_mae: 4819.3516\n",
            "Epoch 43/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4419.1905 - mse: 155761776.0000 - mae: 4419.1904 - val_loss: 4775.7583 - val_mse: 178960128.0000 - val_mae: 4775.7573\n",
            "Epoch 44/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4402.9413 - mse: 155151872.0000 - mae: 4402.9409 - val_loss: 4760.0243 - val_mse: 178210096.0000 - val_mae: 4760.0244\n",
            "Epoch 45/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 4380.5629 - mse: 154061344.0000 - mae: 4380.5625 - val_loss: 4735.9059 - val_mse: 177221408.0000 - val_mae: 4735.9058\n",
            "Epoch 46/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 4374.2915 - mse: 153519856.0000 - mae: 4374.2920 - val_loss: 4721.6130 - val_mse: 176546208.0000 - val_mae: 4721.6128\n",
            "Epoch 47/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4362.6594 - mse: 152957856.0000 - mae: 4362.6592 - val_loss: 4704.7423 - val_mse: 175711504.0000 - val_mae: 4704.7427\n",
            "Epoch 48/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4343.7299 - mse: 151783376.0000 - mae: 4343.7300 - val_loss: 4687.1945 - val_mse: 174875104.0000 - val_mae: 4687.1953\n",
            "Epoch 49/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4331.4880 - mse: 151716928.0000 - mae: 4331.4878 - val_loss: 4680.7439 - val_mse: 174147488.0000 - val_mae: 4680.7437\n",
            "Epoch 50/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4317.1765 - mse: 150647760.0000 - mae: 4317.1763 - val_loss: 4650.5461 - val_mse: 173062352.0000 - val_mae: 4650.5464\n",
            "Epoch 51/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4303.7523 - mse: 149488960.0000 - mae: 4303.7524 - val_loss: 4647.1739 - val_mse: 172432384.0000 - val_mae: 4647.1743\n",
            "Epoch 52/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 4290.8866 - mse: 149177264.0000 - mae: 4290.8867 - val_loss: 4621.4260 - val_mse: 171436992.0000 - val_mae: 4621.4263\n",
            "Epoch 53/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4280.2277 - mse: 148286288.0000 - mae: 4280.2275 - val_loss: 4615.2152 - val_mse: 170762896.0000 - val_mae: 4615.2153\n",
            "Epoch 54/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4283.2910 - mse: 148033088.0000 - mae: 4283.2910 - val_loss: 4601.4727 - val_mse: 170063248.0000 - val_mae: 4601.4731\n",
            "Epoch 55/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 4270.0456 - mse: 147512944.0000 - mae: 4270.0459 - val_loss: 4587.9030 - val_mse: 169311360.0000 - val_mae: 4587.9033\n",
            "Epoch 56/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4245.4218 - mse: 146203680.0000 - mae: 4245.4219 - val_loss: 4576.3540 - val_mse: 168629072.0000 - val_mae: 4576.3540\n",
            "Epoch 57/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 4248.2166 - mse: 145804576.0000 - mae: 4248.2163 - val_loss: 4562.4620 - val_mse: 167837104.0000 - val_mae: 4562.4619\n",
            "Epoch 58/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4238.1489 - mse: 145085376.0000 - mae: 4238.1489 - val_loss: 4543.4876 - val_mse: 166972512.0000 - val_mae: 4543.4878\n",
            "Epoch 59/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 4223.9382 - mse: 144584320.0000 - mae: 4223.9380 - val_loss: 4538.6327 - val_mse: 166377648.0000 - val_mae: 4538.6323\n",
            "Epoch 60/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4211.3869 - mse: 143944976.0000 - mae: 4211.3872 - val_loss: 4535.1430 - val_mse: 165731440.0000 - val_mae: 4535.1426\n",
            "Epoch 61/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4199.4924 - mse: 142795360.0000 - mae: 4199.4927 - val_loss: 4513.8577 - val_mse: 164883808.0000 - val_mae: 4513.8579\n",
            "Epoch 62/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 4195.5599 - mse: 142996064.0000 - mae: 4195.5601 - val_loss: 4500.6046 - val_mse: 164108224.0000 - val_mae: 4500.6055\n",
            "Epoch 63/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 4178.7192 - mse: 142419552.0000 - mae: 4178.7197 - val_loss: 4488.3095 - val_mse: 163322448.0000 - val_mae: 4488.3096\n",
            "Epoch 64/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4188.8239 - mse: 142258464.0000 - mae: 4188.8242 - val_loss: 4486.6768 - val_mse: 162784832.0000 - val_mae: 4486.6768\n",
            "Epoch 65/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4169.9990 - mse: 140303184.0000 - mae: 4169.9985 - val_loss: 4470.4149 - val_mse: 161944368.0000 - val_mae: 4470.4150\n",
            "Epoch 66/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 4162.0320 - mse: 140519056.0000 - mae: 4162.0322 - val_loss: 4460.9871 - val_mse: 161189440.0000 - val_mae: 4460.9873\n",
            "Epoch 67/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4147.4449 - mse: 139306880.0000 - mae: 4147.4448 - val_loss: 4458.5066 - val_mse: 160553568.0000 - val_mae: 4458.5073\n",
            "Epoch 68/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 4140.6414 - mse: 139474224.0000 - mae: 4140.6411 - val_loss: 4439.6685 - val_mse: 159759632.0000 - val_mae: 4439.6680\n",
            "Epoch 69/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4127.9454 - mse: 138318528.0000 - mae: 4127.9453 - val_loss: 4429.3746 - val_mse: 159131216.0000 - val_mae: 4429.3750\n",
            "Epoch 70/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4114.9530 - mse: 138169008.0000 - mae: 4114.9531 - val_loss: 4415.0353 - val_mse: 158387920.0000 - val_mae: 4415.0352\n",
            "Epoch 71/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 4112.2183 - mse: 137712800.0000 - mae: 4112.2183 - val_loss: 4409.6575 - val_mse: 157822944.0000 - val_mae: 4409.6577\n",
            "Epoch 72/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4083.0698 - mse: 135741952.0000 - mae: 4083.0698 - val_loss: 4394.8834 - val_mse: 157009088.0000 - val_mae: 4394.8838\n",
            "Epoch 73/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4086.4071 - mse: 135932640.0000 - mae: 4086.4067 - val_loss: 4384.2573 - val_mse: 156300768.0000 - val_mae: 4384.2578\n",
            "Epoch 74/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4095.1858 - mse: 135619712.0000 - mae: 4095.1858 - val_loss: 4377.0456 - val_mse: 155625888.0000 - val_mae: 4377.0459\n",
            "Epoch 75/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4046.8295 - mse: 133714240.0000 - mae: 4046.8293 - val_loss: 4361.4545 - val_mse: 154762304.0000 - val_mae: 4361.4541\n",
            "Epoch 76/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4057.7376 - mse: 133656000.0000 - mae: 4057.7373 - val_loss: 4357.7265 - val_mse: 154145616.0000 - val_mae: 4357.7261\n",
            "Epoch 77/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4040.4719 - mse: 133292576.0000 - mae: 4040.4719 - val_loss: 4341.8973 - val_mse: 153352624.0000 - val_mae: 4341.8979\n",
            "Epoch 78/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4034.2041 - mse: 132365072.0000 - mae: 4034.2043 - val_loss: 4324.1834 - val_mse: 152475792.0000 - val_mae: 4324.1841\n",
            "Epoch 79/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 4007.2634 - mse: 131146096.0000 - mae: 4007.2632 - val_loss: 4320.9317 - val_mse: 151825552.0000 - val_mae: 4320.9321\n",
            "Epoch 80/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4001.6653 - mse: 131037968.0000 - mae: 4001.6653 - val_loss: 4309.0043 - val_mse: 151189856.0000 - val_mae: 4309.0049\n",
            "Epoch 81/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4003.6385 - mse: 131132384.0000 - mae: 4003.6382 - val_loss: 4311.8016 - val_mse: 150703936.0000 - val_mae: 4311.8013\n",
            "Epoch 82/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4001.5378 - mse: 130668440.0000 - mae: 4001.5378 - val_loss: 4290.1755 - val_mse: 149882672.0000 - val_mae: 4290.1758\n",
            "Epoch 83/500\n",
            "2250/2250 [==============================] - 0s 59us/step - loss: 3980.5092 - mse: 130058944.0000 - mae: 3980.5093 - val_loss: 4278.0903 - val_mse: 149072528.0000 - val_mae: 4278.0908\n",
            "Epoch 84/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 3958.7013 - mse: 128475408.0000 - mae: 3958.7014 - val_loss: 4273.5219 - val_mse: 148577664.0000 - val_mae: 4273.5215\n",
            "Epoch 85/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 3957.1961 - mse: 128203152.0000 - mae: 3957.1960 - val_loss: 4269.7286 - val_mse: 147914528.0000 - val_mae: 4269.7285\n",
            "Epoch 86/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3956.4309 - mse: 127453000.0000 - mae: 3956.4307 - val_loss: 4262.2213 - val_mse: 147277696.0000 - val_mae: 4262.2217\n",
            "Epoch 87/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 3961.3971 - mse: 127664568.0000 - mae: 3961.3972 - val_loss: 4258.3569 - val_mse: 146798912.0000 - val_mae: 4258.3569\n",
            "Epoch 88/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3937.2668 - mse: 126456008.0000 - mae: 3937.2666 - val_loss: 4248.3116 - val_mse: 146173264.0000 - val_mae: 4248.3120\n",
            "Epoch 89/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 3935.5989 - mse: 126045944.0000 - mae: 3935.5991 - val_loss: 4233.5955 - val_mse: 145472400.0000 - val_mae: 4233.5957\n",
            "Epoch 90/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 3948.5386 - mse: 125951552.0000 - mae: 3948.5386 - val_loss: 4224.9846 - val_mse: 144838592.0000 - val_mae: 4224.9854\n",
            "Epoch 91/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 3900.9678 - mse: 125225472.0000 - mae: 3900.9675 - val_loss: 4228.2401 - val_mse: 144350800.0000 - val_mae: 4228.2402\n",
            "Epoch 92/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 3906.8409 - mse: 124498304.0000 - mae: 3906.8408 - val_loss: 4205.8121 - val_mse: 143615072.0000 - val_mae: 4205.8120\n",
            "Epoch 93/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3910.8047 - mse: 124892712.0000 - mae: 3910.8049 - val_loss: 4210.7828 - val_mse: 143492192.0000 - val_mae: 4210.7827\n",
            "Epoch 94/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 3893.0538 - mse: 123939848.0000 - mae: 3893.0537 - val_loss: 4190.2693 - val_mse: 142619984.0000 - val_mae: 4190.2700\n",
            "Epoch 95/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 3898.7620 - mse: 123387352.0000 - mae: 3898.7617 - val_loss: 4193.1596 - val_mse: 142233824.0000 - val_mae: 4193.1597\n",
            "Epoch 96/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 3879.0879 - mse: 121599608.0000 - mae: 3879.0879 - val_loss: 4176.2955 - val_mse: 141498016.0000 - val_mae: 4176.2954\n",
            "Epoch 97/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 3873.1655 - mse: 121768760.0000 - mae: 3873.1658 - val_loss: 4161.1630 - val_mse: 140791040.0000 - val_mae: 4161.1631\n",
            "Epoch 98/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3858.4880 - mse: 121360912.0000 - mae: 3858.4875 - val_loss: 4153.0927 - val_mse: 140187856.0000 - val_mae: 4153.0928\n",
            "Epoch 99/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3831.7480 - mse: 120198584.0000 - mae: 3831.7480 - val_loss: 4138.1947 - val_mse: 139469680.0000 - val_mae: 4138.1948\n",
            "Epoch 100/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 3835.9297 - mse: 120346472.0000 - mae: 3835.9297 - val_loss: 4135.2499 - val_mse: 138893824.0000 - val_mae: 4135.2500\n",
            "Epoch 101/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3864.2916 - mse: 122051632.0000 - mae: 3864.2915 - val_loss: 4110.6201 - val_mse: 138107120.0000 - val_mae: 4110.6201\n",
            "Epoch 102/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3854.9375 - mse: 119629440.0000 - mae: 3854.9373 - val_loss: 4103.7969 - val_mse: 137662400.0000 - val_mae: 4103.7969\n",
            "Epoch 103/500\n",
            "2250/2250 [==============================] - 0s 60us/step - loss: 3837.0633 - mse: 119481952.0000 - mae: 3837.0632 - val_loss: 4091.1400 - val_mse: 137087328.0000 - val_mae: 4091.1399\n",
            "Epoch 104/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 3824.1013 - mse: 119229736.0000 - mae: 3824.1013 - val_loss: 4078.4168 - val_mse: 136543424.0000 - val_mae: 4078.4170\n",
            "Epoch 105/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 3811.4397 - mse: 118838144.0000 - mae: 3811.4395 - val_loss: 4072.7005 - val_mse: 136154880.0000 - val_mae: 4072.7004\n",
            "Epoch 106/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3773.2590 - mse: 118396400.0000 - mae: 3773.2590 - val_loss: 4054.3598 - val_mse: 135267184.0000 - val_mae: 4054.3601\n",
            "Epoch 107/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 3759.2539 - mse: 116972184.0000 - mae: 3759.2537 - val_loss: 4036.3255 - val_mse: 134578160.0000 - val_mae: 4036.3250\n",
            "Epoch 108/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 3742.1587 - mse: 116142872.0000 - mae: 3742.1587 - val_loss: 4011.9403 - val_mse: 133629344.0000 - val_mae: 4011.9399\n",
            "Epoch 109/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 3728.8094 - mse: 116039216.0000 - mae: 3728.8088 - val_loss: 3994.6034 - val_mse: 132702288.0000 - val_mae: 3994.6035\n",
            "Epoch 110/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 3710.7007 - mse: 114283992.0000 - mae: 3710.7009 - val_loss: 3983.9753 - val_mse: 131692568.0000 - val_mae: 3983.9751\n",
            "Epoch 111/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3716.3850 - mse: 113730152.0000 - mae: 3716.3850 - val_loss: 3984.7536 - val_mse: 130908032.0000 - val_mae: 3984.7537\n",
            "Epoch 112/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3693.7704 - mse: 113198824.0000 - mae: 3693.7703 - val_loss: 4015.2688 - val_mse: 130400080.0000 - val_mae: 4015.2688\n",
            "Epoch 113/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 3649.6962 - mse: 109954112.0000 - mae: 3649.6960 - val_loss: 3952.6917 - val_mse: 128709776.0000 - val_mae: 3952.6917\n",
            "Epoch 114/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 3657.8651 - mse: 112055928.0000 - mae: 3657.8650 - val_loss: 3980.3837 - val_mse: 128138304.0000 - val_mae: 3980.3831\n",
            "Epoch 115/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 3673.1070 - mse: 111040672.0000 - mae: 3673.1069 - val_loss: 3953.6314 - val_mse: 127015232.0000 - val_mae: 3953.6311\n",
            "Epoch 116/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 3609.3691 - mse: 108984832.0000 - mae: 3609.3691 - val_loss: 3950.4864 - val_mse: 126329376.0000 - val_mae: 3950.4866\n",
            "Epoch 117/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3636.2797 - mse: 108502680.0000 - mae: 3636.2795 - val_loss: 3920.7245 - val_mse: 124984200.0000 - val_mae: 3920.7244\n",
            "Epoch 118/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 3622.3105 - mse: 108304944.0000 - mae: 3622.3105 - val_loss: 3913.5036 - val_mse: 124156424.0000 - val_mae: 3913.5037\n",
            "Epoch 119/500\n",
            "2250/2250 [==============================] - 0s 77us/step - loss: 3637.9478 - mse: 107109952.0000 - mae: 3637.9478 - val_loss: 3887.9935 - val_mse: 122876872.0000 - val_mae: 3887.9934\n",
            "Epoch 120/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3545.0053 - mse: 103412328.0000 - mae: 3545.0054 - val_loss: 3868.8240 - val_mse: 121453552.0000 - val_mae: 3868.8240\n",
            "Epoch 121/500\n",
            "2250/2250 [==============================] - 0s 60us/step - loss: 3588.9602 - mse: 104847976.0000 - mae: 3588.9602 - val_loss: 3865.6367 - val_mse: 120731488.0000 - val_mae: 3865.6370\n",
            "Epoch 122/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 3563.6689 - mse: 103281024.0000 - mae: 3563.6689 - val_loss: 3855.4339 - val_mse: 119775192.0000 - val_mae: 3855.4343\n",
            "Epoch 123/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 3541.3438 - mse: 102705784.0000 - mae: 3541.3438 - val_loss: 3867.4553 - val_mse: 118831744.0000 - val_mae: 3867.4551\n",
            "Epoch 124/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3508.0177 - mse: 99620912.0000 - mae: 3508.0176 - val_loss: 3812.4175 - val_mse: 116903272.0000 - val_mae: 3812.4177\n",
            "Epoch 125/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 3538.0320 - mse: 100231416.0000 - mae: 3538.0322 - val_loss: 3821.8040 - val_mse: 116129984.0000 - val_mae: 3821.8040\n",
            "Epoch 126/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 3504.4792 - mse: 99015752.0000 - mae: 3504.4792 - val_loss: 3791.2045 - val_mse: 114813600.0000 - val_mae: 3791.2043\n",
            "Epoch 127/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3458.5442 - mse: 94155216.0000 - mae: 3458.5444 - val_loss: 3783.0769 - val_mse: 113658936.0000 - val_mae: 3783.0769\n",
            "Epoch 128/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3494.2080 - mse: 98118448.0000 - mae: 3494.2080 - val_loss: 3769.1471 - val_mse: 112503496.0000 - val_mae: 3769.1472\n",
            "Epoch 129/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3425.0120 - mse: 94169032.0000 - mae: 3425.0117 - val_loss: 3774.8239 - val_mse: 111388088.0000 - val_mae: 3774.8235\n",
            "Epoch 130/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 3459.9683 - mse: 95204984.0000 - mae: 3459.9683 - val_loss: 3738.0313 - val_mse: 110190312.0000 - val_mae: 3738.0315\n",
            "Epoch 131/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 3416.2478 - mse: 93985264.0000 - mae: 3416.2476 - val_loss: 3732.3283 - val_mse: 108895720.0000 - val_mae: 3732.3284\n",
            "Epoch 132/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3377.3077 - mse: 90895416.0000 - mae: 3377.3081 - val_loss: 3749.6520 - val_mse: 107789376.0000 - val_mae: 3749.6521\n",
            "Epoch 133/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3390.9492 - mse: 90863800.0000 - mae: 3390.9492 - val_loss: 3694.1183 - val_mse: 106088312.0000 - val_mae: 3694.1187\n",
            "Epoch 134/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 3382.0563 - mse: 90567440.0000 - mae: 3382.0562 - val_loss: 3652.9152 - val_mse: 104508720.0000 - val_mae: 3652.9150\n",
            "Epoch 135/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3333.0370 - mse: 86797000.0000 - mae: 3333.0369 - val_loss: 3662.2014 - val_mse: 103203392.0000 - val_mae: 3662.2017\n",
            "Epoch 136/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 3311.3912 - mse: 87294504.0000 - mae: 3311.3914 - val_loss: 3622.3421 - val_mse: 101545000.0000 - val_mae: 3622.3425\n",
            "Epoch 137/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3328.9196 - mse: 86966512.0000 - mae: 3328.9194 - val_loss: 3618.2829 - val_mse: 100403568.0000 - val_mae: 3618.2830\n",
            "Epoch 138/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3270.4508 - mse: 83728760.0000 - mae: 3270.4507 - val_loss: 3576.6741 - val_mse: 98695688.0000 - val_mae: 3576.6741\n",
            "Epoch 139/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3257.9001 - mse: 83279856.0000 - mae: 3257.9001 - val_loss: 3543.9673 - val_mse: 97219832.0000 - val_mae: 3543.9673\n",
            "Epoch 140/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 3212.5757 - mse: 80666192.0000 - mae: 3212.5757 - val_loss: 3537.6815 - val_mse: 95817552.0000 - val_mae: 3537.6816\n",
            "Epoch 141/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3131.0700 - mse: 75338704.0000 - mae: 3131.0701 - val_loss: 3508.6694 - val_mse: 94128776.0000 - val_mae: 3508.6694\n",
            "Epoch 142/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3206.4538 - mse: 80203936.0000 - mae: 3206.4539 - val_loss: 3529.0928 - val_mse: 93111104.0000 - val_mae: 3529.0930\n",
            "Epoch 143/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3208.2447 - mse: 80942168.0000 - mae: 3208.2449 - val_loss: 3444.6505 - val_mse: 91239472.0000 - val_mae: 3444.6501\n",
            "Epoch 144/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3130.8104 - mse: 75485424.0000 - mae: 3130.8105 - val_loss: 3476.3510 - val_mse: 90030416.0000 - val_mae: 3476.3506\n",
            "Epoch 145/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 3156.6396 - mse: 77615016.0000 - mae: 3156.6396 - val_loss: 3405.7891 - val_mse: 88214056.0000 - val_mae: 3405.7893\n",
            "Epoch 146/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3086.5508 - mse: 75558384.0000 - mae: 3086.5508 - val_loss: 3378.6642 - val_mse: 86614552.0000 - val_mae: 3378.6646\n",
            "Epoch 147/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3099.5237 - mse: 73740656.0000 - mae: 3099.5239 - val_loss: 3370.7396 - val_mse: 85624280.0000 - val_mae: 3370.7395\n",
            "Epoch 148/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3061.5742 - mse: 71491976.0000 - mae: 3061.5742 - val_loss: 3386.7985 - val_mse: 84472896.0000 - val_mae: 3386.7983\n",
            "Epoch 149/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3072.2789 - mse: 72678360.0000 - mae: 3072.2788 - val_loss: 3334.8094 - val_mse: 82928992.0000 - val_mae: 3334.8096\n",
            "Epoch 150/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 3029.7949 - mse: 71314816.0000 - mae: 3029.7947 - val_loss: 3287.3708 - val_mse: 81156400.0000 - val_mae: 3287.3706\n",
            "Epoch 151/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 2944.3387 - mse: 65833364.0000 - mae: 2944.3386 - val_loss: 3271.6603 - val_mse: 79709096.0000 - val_mae: 3271.6599\n",
            "Epoch 152/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 2955.2734 - mse: 68098920.0000 - mae: 2955.2734 - val_loss: 3224.5422 - val_mse: 78111424.0000 - val_mae: 3224.5425\n",
            "Epoch 153/500\n",
            "2250/2250 [==============================] - 0s 77us/step - loss: 2979.4754 - mse: 66421936.0000 - mae: 2979.4756 - val_loss: 3216.0557 - val_mse: 76895136.0000 - val_mae: 3216.0557\n",
            "Epoch 154/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2938.1821 - mse: 65171364.0000 - mae: 2938.1819 - val_loss: 3186.6146 - val_mse: 75519312.0000 - val_mae: 3186.6147\n",
            "Epoch 155/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2879.6286 - mse: 63578856.0000 - mae: 2879.6284 - val_loss: 3191.5278 - val_mse: 74655904.0000 - val_mae: 3191.5278\n",
            "Epoch 156/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2883.9187 - mse: 63763004.0000 - mae: 2883.9189 - val_loss: 3201.0213 - val_mse: 73436600.0000 - val_mae: 3201.0215\n",
            "Epoch 157/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 2871.6359 - mse: 61933056.0000 - mae: 2871.6360 - val_loss: 3144.9251 - val_mse: 71887272.0000 - val_mae: 3144.9253\n",
            "Epoch 158/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 2813.6041 - mse: 60968316.0000 - mae: 2813.6040 - val_loss: 3092.3432 - val_mse: 70329728.0000 - val_mae: 3092.3428\n",
            "Epoch 159/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2802.2177 - mse: 59636296.0000 - mae: 2802.2175 - val_loss: 3098.7153 - val_mse: 69224456.0000 - val_mae: 3098.7151\n",
            "Epoch 160/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 2785.1570 - mse: 59394132.0000 - mae: 2785.1570 - val_loss: 3042.1154 - val_mse: 67650896.0000 - val_mae: 3042.1152\n",
            "Epoch 161/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2742.4008 - mse: 56767792.0000 - mae: 2742.4006 - val_loss: 2997.2658 - val_mse: 66094620.0000 - val_mae: 2997.2656\n",
            "Epoch 162/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 2752.2550 - mse: 57132036.0000 - mae: 2752.2549 - val_loss: 3020.3557 - val_mse: 65442604.0000 - val_mae: 3020.3555\n",
            "Epoch 163/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2672.7618 - mse: 52937484.0000 - mae: 2672.7617 - val_loss: 3001.0132 - val_mse: 64314036.0000 - val_mae: 3001.0134\n",
            "Epoch 164/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2706.1783 - mse: 54728060.0000 - mae: 2706.1785 - val_loss: 2948.0798 - val_mse: 62879456.0000 - val_mae: 2948.0796\n",
            "Epoch 165/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2704.3256 - mse: 53823772.0000 - mae: 2704.3254 - val_loss: 2936.1744 - val_mse: 62037884.0000 - val_mae: 2936.1743\n",
            "Epoch 166/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2678.7656 - mse: 53935864.0000 - mae: 2678.7656 - val_loss: 2900.3946 - val_mse: 60837348.0000 - val_mae: 2900.3948\n",
            "Epoch 167/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2633.1653 - mse: 52392996.0000 - mae: 2633.1653 - val_loss: 2897.7381 - val_mse: 60033120.0000 - val_mae: 2897.7380\n",
            "Epoch 168/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2600.3300 - mse: 50809848.0000 - mae: 2600.3301 - val_loss: 2881.1365 - val_mse: 59265448.0000 - val_mae: 2881.1365\n",
            "Epoch 169/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 2612.5312 - mse: 50155200.0000 - mae: 2612.5312 - val_loss: 2863.3985 - val_mse: 58136148.0000 - val_mae: 2863.3984\n",
            "Epoch 170/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2555.6196 - mse: 48013940.0000 - mae: 2555.6196 - val_loss: 2805.1928 - val_mse: 56803484.0000 - val_mae: 2805.1929\n",
            "Epoch 171/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2545.8803 - mse: 48237176.0000 - mae: 2545.8801 - val_loss: 2818.8921 - val_mse: 56274508.0000 - val_mae: 2818.8921\n",
            "Epoch 172/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2585.4091 - mse: 47990868.0000 - mae: 2585.4094 - val_loss: 2789.9105 - val_mse: 55185392.0000 - val_mae: 2789.9106\n",
            "Epoch 173/500\n",
            "2250/2250 [==============================] - 0s 77us/step - loss: 2521.9129 - mse: 45810304.0000 - mae: 2521.9128 - val_loss: 2809.3125 - val_mse: 54736356.0000 - val_mae: 2809.3125\n",
            "Epoch 174/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2583.7153 - mse: 49684788.0000 - mae: 2583.7153 - val_loss: 2760.6894 - val_mse: 53755372.0000 - val_mae: 2760.6897\n",
            "Epoch 175/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2536.6858 - mse: 45704068.0000 - mae: 2536.6858 - val_loss: 2730.9659 - val_mse: 52939812.0000 - val_mae: 2730.9656\n",
            "Epoch 176/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 2482.6005 - mse: 44767692.0000 - mae: 2482.6006 - val_loss: 2706.4068 - val_mse: 52122152.0000 - val_mae: 2706.4067\n",
            "Epoch 177/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2418.8184 - mse: 40972008.0000 - mae: 2418.8184 - val_loss: 2707.1678 - val_mse: 51772364.0000 - val_mae: 2707.1677\n",
            "Epoch 178/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2471.4090 - mse: 44775240.0000 - mae: 2471.4089 - val_loss: 2701.5997 - val_mse: 51302836.0000 - val_mae: 2701.6001\n",
            "Epoch 179/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2520.1693 - mse: 45463892.0000 - mae: 2520.1694 - val_loss: 2693.2099 - val_mse: 50722272.0000 - val_mae: 2693.2097\n",
            "Epoch 180/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 2468.8184 - mse: 44836448.0000 - mae: 2468.8184 - val_loss: 2641.5534 - val_mse: 49904020.0000 - val_mae: 2641.5532\n",
            "Epoch 181/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2517.5664 - mse: 44799496.0000 - mae: 2517.5664 - val_loss: 2653.7429 - val_mse: 49550332.0000 - val_mae: 2653.7429\n",
            "Epoch 182/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2440.2862 - mse: 42424792.0000 - mae: 2440.2861 - val_loss: 2599.0963 - val_mse: 48549804.0000 - val_mae: 2599.0964\n",
            "Epoch 183/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 2424.0380 - mse: 41695564.0000 - mae: 2424.0378 - val_loss: 2606.8420 - val_mse: 48256460.0000 - val_mae: 2606.8416\n",
            "Epoch 184/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2465.1545 - mse: 43201912.0000 - mae: 2465.1545 - val_loss: 2564.5439 - val_mse: 47327852.0000 - val_mae: 2564.5439\n",
            "Epoch 185/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2362.1794 - mse: 40670640.0000 - mae: 2362.1794 - val_loss: 2614.6199 - val_mse: 47504664.0000 - val_mae: 2614.6199\n",
            "Epoch 186/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2367.2896 - mse: 37504660.0000 - mae: 2367.2893 - val_loss: 2572.8729 - val_mse: 46848076.0000 - val_mae: 2572.8730\n",
            "Epoch 187/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 2365.3282 - mse: 40211084.0000 - mae: 2365.3281 - val_loss: 2532.6289 - val_mse: 46130324.0000 - val_mae: 2532.6289\n",
            "Epoch 188/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2427.4553 - mse: 41079692.0000 - mae: 2427.4556 - val_loss: 2536.0296 - val_mse: 45891976.0000 - val_mae: 2536.0298\n",
            "Epoch 189/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2345.1644 - mse: 40306556.0000 - mae: 2345.1646 - val_loss: 2510.6221 - val_mse: 45212544.0000 - val_mae: 2510.6223\n",
            "Epoch 190/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2375.1833 - mse: 39112504.0000 - mae: 2375.1833 - val_loss: 2513.8703 - val_mse: 45040752.0000 - val_mae: 2513.8706\n",
            "Epoch 191/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 2368.6730 - mse: 39608948.0000 - mae: 2368.6729 - val_loss: 2498.2246 - val_mse: 44620312.0000 - val_mae: 2498.2246\n",
            "Epoch 192/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2348.5900 - mse: 37154268.0000 - mae: 2348.5898 - val_loss: 2514.4337 - val_mse: 44450776.0000 - val_mae: 2514.4341\n",
            "Epoch 193/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2359.2707 - mse: 37762160.0000 - mae: 2359.2710 - val_loss: 2491.8843 - val_mse: 43976340.0000 - val_mae: 2491.8840\n",
            "Epoch 194/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2385.1055 - mse: 39554804.0000 - mae: 2385.1052 - val_loss: 2463.5511 - val_mse: 43250100.0000 - val_mae: 2463.5510\n",
            "Epoch 195/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2324.2298 - mse: 38873576.0000 - mae: 2324.2297 - val_loss: 2464.2828 - val_mse: 43112972.0000 - val_mae: 2464.2830\n",
            "Epoch 196/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2348.4584 - mse: 37559660.0000 - mae: 2348.4583 - val_loss: 2460.9668 - val_mse: 42788540.0000 - val_mae: 2460.9668\n",
            "Epoch 197/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2339.6299 - mse: 39544200.0000 - mae: 2339.6299 - val_loss: 2444.1204 - val_mse: 42599816.0000 - val_mae: 2444.1204\n",
            "Epoch 198/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2358.4780 - mse: 38339412.0000 - mae: 2358.4780 - val_loss: 2438.0741 - val_mse: 42308284.0000 - val_mae: 2438.0740\n",
            "Epoch 199/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 2375.8424 - mse: 37578276.0000 - mae: 2375.8425 - val_loss: 2436.5130 - val_mse: 41977672.0000 - val_mae: 2436.5129\n",
            "Epoch 200/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2330.9478 - mse: 37876080.0000 - mae: 2330.9475 - val_loss: 2418.0330 - val_mse: 41496364.0000 - val_mae: 2418.0334\n",
            "Epoch 201/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 2320.7441 - mse: 38361432.0000 - mae: 2320.7439 - val_loss: 2426.0403 - val_mse: 41654448.0000 - val_mae: 2426.0405\n",
            "Epoch 202/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2331.7839 - mse: 40771716.0000 - mae: 2331.7839 - val_loss: 2417.6907 - val_mse: 41344680.0000 - val_mae: 2417.6904\n",
            "Epoch 203/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2297.0777 - mse: 35387388.0000 - mae: 2297.0776 - val_loss: 2403.9945 - val_mse: 41055824.0000 - val_mae: 2403.9946\n",
            "Epoch 204/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 2255.7049 - mse: 34599172.0000 - mae: 2255.7048 - val_loss: 2377.9026 - val_mse: 40432212.0000 - val_mae: 2377.9026\n",
            "Epoch 205/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2332.0668 - mse: 37720936.0000 - mae: 2332.0671 - val_loss: 2389.9409 - val_mse: 40484280.0000 - val_mae: 2389.9409\n",
            "Epoch 206/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2270.9116 - mse: 34989108.0000 - mae: 2270.9116 - val_loss: 2376.6326 - val_mse: 40284652.0000 - val_mae: 2376.6326\n",
            "Epoch 207/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 2278.5237 - mse: 34188796.0000 - mae: 2278.5237 - val_loss: 2375.4198 - val_mse: 40012028.0000 - val_mae: 2375.4199\n",
            "Epoch 208/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2173.2126 - mse: 31887510.0000 - mae: 2173.2126 - val_loss: 2366.7981 - val_mse: 39675812.0000 - val_mae: 2366.7983\n",
            "Epoch 209/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 2237.5714 - mse: 33236808.0000 - mae: 2237.5713 - val_loss: 2347.7754 - val_mse: 39354204.0000 - val_mae: 2347.7754\n",
            "Epoch 210/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2342.6940 - mse: 40247572.0000 - mae: 2342.6941 - val_loss: 2337.0043 - val_mse: 39034924.0000 - val_mae: 2337.0044\n",
            "Epoch 211/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2309.7305 - mse: 35347816.0000 - mae: 2309.7307 - val_loss: 2346.5541 - val_mse: 39147168.0000 - val_mae: 2346.5542\n",
            "Epoch 212/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2251.4394 - mse: 34216124.0000 - mae: 2251.4395 - val_loss: 2374.7473 - val_mse: 39390456.0000 - val_mae: 2374.7473\n",
            "Epoch 213/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2277.8297 - mse: 34840144.0000 - mae: 2277.8296 - val_loss: 2343.3135 - val_mse: 38905936.0000 - val_mae: 2343.3135\n",
            "Epoch 214/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2191.9412 - mse: 33029204.0000 - mae: 2191.9412 - val_loss: 2342.9188 - val_mse: 38728760.0000 - val_mae: 2342.9187\n",
            "Epoch 215/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2192.3535 - mse: 31726182.0000 - mae: 2192.3535 - val_loss: 2334.5668 - val_mse: 38463568.0000 - val_mae: 2334.5669\n",
            "Epoch 216/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 2275.2499 - mse: 35593780.0000 - mae: 2275.2498 - val_loss: 2330.3236 - val_mse: 38362748.0000 - val_mae: 2330.3235\n",
            "Epoch 217/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2214.8662 - mse: 33470874.0000 - mae: 2214.8662 - val_loss: 2325.0569 - val_mse: 38228280.0000 - val_mae: 2325.0566\n",
            "Epoch 218/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2304.0313 - mse: 37564192.0000 - mae: 2304.0312 - val_loss: 2328.0559 - val_mse: 38155352.0000 - val_mae: 2328.0557\n",
            "Epoch 219/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2230.7366 - mse: 34487568.0000 - mae: 2230.7366 - val_loss: 2331.1430 - val_mse: 37972892.0000 - val_mae: 2331.1428\n",
            "Epoch 220/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2180.5615 - mse: 34433396.0000 - mae: 2180.5615 - val_loss: 2305.9183 - val_mse: 37564952.0000 - val_mae: 2305.9182\n",
            "Epoch 221/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2219.2667 - mse: 32565668.0000 - mae: 2219.2666 - val_loss: 2304.2820 - val_mse: 37558852.0000 - val_mae: 2304.2817\n",
            "Epoch 222/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2212.8723 - mse: 32175788.0000 - mae: 2212.8726 - val_loss: 2332.6727 - val_mse: 37618640.0000 - val_mae: 2332.6726\n",
            "Epoch 223/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2259.4932 - mse: 34231152.0000 - mae: 2259.4934 - val_loss: 2346.4502 - val_mse: 37652312.0000 - val_mae: 2346.4504\n",
            "Epoch 224/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 2134.1112 - mse: 29998846.0000 - mae: 2134.1111 - val_loss: 2292.2768 - val_mse: 36965856.0000 - val_mae: 2292.2766\n",
            "Epoch 225/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2277.8719 - mse: 34710780.0000 - mae: 2277.8721 - val_loss: 2319.6491 - val_mse: 37113476.0000 - val_mae: 2319.6492\n",
            "Epoch 226/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 2214.9476 - mse: 31498088.0000 - mae: 2214.9475 - val_loss: 2293.5065 - val_mse: 36737612.0000 - val_mae: 2293.5063\n",
            "Epoch 227/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2136.0329 - mse: 30809290.0000 - mae: 2136.0330 - val_loss: 2295.2181 - val_mse: 36659944.0000 - val_mae: 2295.2178\n",
            "Epoch 228/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2184.2183 - mse: 32653708.0000 - mae: 2184.2183 - val_loss: 2328.8469 - val_mse: 36843020.0000 - val_mae: 2328.8469\n",
            "Epoch 229/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 2170.1110 - mse: 31624062.0000 - mae: 2170.1111 - val_loss: 2282.1539 - val_mse: 36389148.0000 - val_mae: 2282.1538\n",
            "Epoch 230/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2171.2286 - mse: 32375742.0000 - mae: 2171.2285 - val_loss: 2301.5610 - val_mse: 36413420.0000 - val_mae: 2301.5610\n",
            "Epoch 231/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2141.7611 - mse: 30279850.0000 - mae: 2141.7612 - val_loss: 2274.5713 - val_mse: 36025372.0000 - val_mae: 2274.5708\n",
            "Epoch 232/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 2197.9292 - mse: 33556916.0000 - mae: 2197.9292 - val_loss: 2283.9654 - val_mse: 36030288.0000 - val_mae: 2283.9653\n",
            "Epoch 233/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2122.2611 - mse: 30932552.0000 - mae: 2122.2612 - val_loss: 2275.8208 - val_mse: 35853120.0000 - val_mae: 2275.8210\n",
            "Epoch 234/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2159.9068 - mse: 30870100.0000 - mae: 2159.9067 - val_loss: 2257.0729 - val_mse: 35457672.0000 - val_mae: 2257.0730\n",
            "Epoch 235/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2084.2859 - mse: 27351700.0000 - mae: 2084.2859 - val_loss: 2272.3996 - val_mse: 35572712.0000 - val_mae: 2272.3997\n",
            "Epoch 236/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2226.7973 - mse: 33563768.0000 - mae: 2226.7974 - val_loss: 2261.6250 - val_mse: 35288592.0000 - val_mae: 2261.6250\n",
            "Epoch 237/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2173.0512 - mse: 32324172.0000 - mae: 2173.0510 - val_loss: 2298.8805 - val_mse: 35439300.0000 - val_mae: 2298.8806\n",
            "Epoch 238/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2143.5554 - mse: 31128060.0000 - mae: 2143.5554 - val_loss: 2286.3513 - val_mse: 35328796.0000 - val_mae: 2286.3516\n",
            "Epoch 239/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2136.9002 - mse: 30759132.0000 - mae: 2136.9004 - val_loss: 2270.8905 - val_mse: 35168284.0000 - val_mae: 2270.8904\n",
            "Epoch 240/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2176.8507 - mse: 31986046.0000 - mae: 2176.8508 - val_loss: 2271.6057 - val_mse: 35209356.0000 - val_mae: 2271.6057\n",
            "Epoch 241/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2136.4655 - mse: 30076544.0000 - mae: 2136.4656 - val_loss: 2252.8052 - val_mse: 34825468.0000 - val_mae: 2252.8054\n",
            "Epoch 242/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2093.3703 - mse: 30002500.0000 - mae: 2093.3701 - val_loss: 2257.9124 - val_mse: 34744204.0000 - val_mae: 2257.9124\n",
            "Epoch 243/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2192.7150 - mse: 31406476.0000 - mae: 2192.7151 - val_loss: 2252.5237 - val_mse: 34672064.0000 - val_mae: 2252.5234\n",
            "Epoch 244/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2156.9644 - mse: 31199608.0000 - mae: 2156.9644 - val_loss: 2281.2602 - val_mse: 35014796.0000 - val_mae: 2281.2603\n",
            "Epoch 245/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2106.7262 - mse: 29054784.0000 - mae: 2106.7263 - val_loss: 2257.1156 - val_mse: 34693468.0000 - val_mae: 2257.1155\n",
            "Epoch 246/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2140.2657 - mse: 30694422.0000 - mae: 2140.2659 - val_loss: 2257.6078 - val_mse: 34922128.0000 - val_mae: 2257.6079\n",
            "Epoch 247/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2151.5179 - mse: 29656700.0000 - mae: 2151.5181 - val_loss: 2262.5463 - val_mse: 34870756.0000 - val_mae: 2262.5464\n",
            "Epoch 248/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2087.8379 - mse: 31024612.0000 - mae: 2087.8379 - val_loss: 2252.1622 - val_mse: 34763688.0000 - val_mae: 2252.1621\n",
            "Epoch 249/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2163.1847 - mse: 30886814.0000 - mae: 2163.1846 - val_loss: 2251.4057 - val_mse: 34834776.0000 - val_mae: 2251.4055\n",
            "Epoch 250/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2121.6996 - mse: 30458292.0000 - mae: 2121.6997 - val_loss: 2262.4684 - val_mse: 34794916.0000 - val_mae: 2262.4685\n",
            "Epoch 251/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2064.4597 - mse: 29248716.0000 - mae: 2064.4595 - val_loss: 2243.7517 - val_mse: 34456640.0000 - val_mae: 2243.7520\n",
            "Epoch 252/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2076.0889 - mse: 27324854.0000 - mae: 2076.0891 - val_loss: 2237.0102 - val_mse: 34389300.0000 - val_mae: 2237.0100\n",
            "Epoch 253/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 2107.4604 - mse: 30535414.0000 - mae: 2107.4604 - val_loss: 2249.8693 - val_mse: 34389384.0000 - val_mae: 2249.8691\n",
            "Epoch 254/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2102.1734 - mse: 29692606.0000 - mae: 2102.1733 - val_loss: 2264.6822 - val_mse: 34633208.0000 - val_mae: 2264.6821\n",
            "Epoch 255/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2130.6074 - mse: 30222152.0000 - mae: 2130.6074 - val_loss: 2250.4401 - val_mse: 34333844.0000 - val_mae: 2250.4399\n",
            "Epoch 256/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2048.5978 - mse: 26732032.0000 - mae: 2048.5979 - val_loss: 2228.2414 - val_mse: 34043668.0000 - val_mae: 2228.2415\n",
            "Epoch 257/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2153.0345 - mse: 29762118.0000 - mae: 2153.0344 - val_loss: 2239.7750 - val_mse: 34143764.0000 - val_mae: 2239.7749\n",
            "Epoch 258/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2189.0570 - mse: 32489708.0000 - mae: 2189.0569 - val_loss: 2252.5511 - val_mse: 34133444.0000 - val_mae: 2252.5510\n",
            "Epoch 259/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2071.8094 - mse: 28238796.0000 - mae: 2071.8093 - val_loss: 2230.3946 - val_mse: 33835016.0000 - val_mae: 2230.3945\n",
            "Epoch 260/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2106.7527 - mse: 29352922.0000 - mae: 2106.7529 - val_loss: 2260.9889 - val_mse: 34212140.0000 - val_mae: 2260.9888\n",
            "Epoch 261/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2153.1983 - mse: 30980706.0000 - mae: 2153.1982 - val_loss: 2262.1316 - val_mse: 34135888.0000 - val_mae: 2262.1318\n",
            "Epoch 262/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2066.8821 - mse: 29496252.0000 - mae: 2066.8823 - val_loss: 2248.5615 - val_mse: 33762860.0000 - val_mae: 2248.5615\n",
            "Epoch 263/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2149.9716 - mse: 30964108.0000 - mae: 2149.9714 - val_loss: 2211.4459 - val_mse: 33418990.0000 - val_mae: 2211.4460\n",
            "Epoch 264/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2051.2551 - mse: 28010978.0000 - mae: 2051.2551 - val_loss: 2264.3795 - val_mse: 33912460.0000 - val_mae: 2264.3796\n",
            "Epoch 265/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2094.7688 - mse: 29897720.0000 - mae: 2094.7686 - val_loss: 2221.5750 - val_mse: 33381118.0000 - val_mae: 2221.5750\n",
            "Epoch 266/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2103.3594 - mse: 29238566.0000 - mae: 2103.3594 - val_loss: 2226.4045 - val_mse: 33519162.0000 - val_mae: 2226.4045\n",
            "Epoch 267/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2101.8235 - mse: 29830860.0000 - mae: 2101.8235 - val_loss: 2252.1605 - val_mse: 33751188.0000 - val_mae: 2252.1604\n",
            "Epoch 268/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2046.4281 - mse: 28865632.0000 - mae: 2046.4280 - val_loss: 2256.7811 - val_mse: 33865436.0000 - val_mae: 2256.7810\n",
            "Epoch 269/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2075.2984 - mse: 27581908.0000 - mae: 2075.2983 - val_loss: 2242.2074 - val_mse: 33585696.0000 - val_mae: 2242.2073\n",
            "Epoch 270/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2178.9430 - mse: 33688408.0000 - mae: 2178.9431 - val_loss: 2248.0988 - val_mse: 33465562.0000 - val_mae: 2248.0986\n",
            "Epoch 271/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2109.5261 - mse: 31572598.0000 - mae: 2109.5259 - val_loss: 2228.7205 - val_mse: 33232664.0000 - val_mae: 2228.7207\n",
            "Epoch 272/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2104.9545 - mse: 28807608.0000 - mae: 2104.9546 - val_loss: 2231.9404 - val_mse: 33160648.0000 - val_mae: 2231.9404\n",
            "Epoch 273/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 2026.9044 - mse: 25641956.0000 - mae: 2026.9042 - val_loss: 2217.2800 - val_mse: 32756454.0000 - val_mae: 2217.2800\n",
            "Epoch 274/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2074.0335 - mse: 28848288.0000 - mae: 2074.0334 - val_loss: 2232.7306 - val_mse: 33036334.0000 - val_mae: 2232.7307\n",
            "Epoch 275/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2107.2845 - mse: 30679888.0000 - mae: 2107.2847 - val_loss: 2215.5290 - val_mse: 32770858.0000 - val_mae: 2215.5293\n",
            "Epoch 276/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2121.4626 - mse: 30673896.0000 - mae: 2121.4626 - val_loss: 2244.9525 - val_mse: 33194134.0000 - val_mae: 2244.9521\n",
            "Epoch 277/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2052.6966 - mse: 27684404.0000 - mae: 2052.6965 - val_loss: 2249.5529 - val_mse: 33386150.0000 - val_mae: 2249.5530\n",
            "Epoch 278/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2116.0349 - mse: 29680250.0000 - mae: 2116.0349 - val_loss: 2239.5571 - val_mse: 33153046.0000 - val_mae: 2239.5569\n",
            "Epoch 279/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2020.5125 - mse: 26299340.0000 - mae: 2020.5125 - val_loss: 2273.1651 - val_mse: 33465482.0000 - val_mae: 2273.1650\n",
            "Epoch 280/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2067.0492 - mse: 29333592.0000 - mae: 2067.0493 - val_loss: 2246.4189 - val_mse: 33231962.0000 - val_mae: 2246.4187\n",
            "Epoch 281/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2098.8274 - mse: 30501072.0000 - mae: 2098.8274 - val_loss: 2224.9236 - val_mse: 32814404.0000 - val_mae: 2224.9236\n",
            "Epoch 282/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2035.8432 - mse: 26885828.0000 - mae: 2035.8431 - val_loss: 2238.1940 - val_mse: 33008546.0000 - val_mae: 2238.1941\n",
            "Epoch 283/500\n",
            "2250/2250 [==============================] - 0s 94us/step - loss: 2115.2004 - mse: 30479692.0000 - mae: 2115.2007 - val_loss: 2270.6688 - val_mse: 33141960.0000 - val_mae: 2270.6685\n",
            "Epoch 284/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2126.0185 - mse: 31543628.0000 - mae: 2126.0186 - val_loss: 2266.1498 - val_mse: 33240336.0000 - val_mae: 2266.1497\n",
            "Epoch 285/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2044.2133 - mse: 27764514.0000 - mae: 2044.2134 - val_loss: 2217.2752 - val_mse: 32833314.0000 - val_mae: 2217.2754\n",
            "Epoch 286/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 2118.6915 - mse: 30903810.0000 - mae: 2118.6914 - val_loss: 2231.2544 - val_mse: 32842690.0000 - val_mae: 2231.2542\n",
            "Epoch 287/500\n",
            "2250/2250 [==============================] - 0s 84us/step - loss: 2019.4381 - mse: 26835736.0000 - mae: 2019.4380 - val_loss: 2249.8920 - val_mse: 32942500.0000 - val_mae: 2249.8921\n",
            "Epoch 288/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 2066.4539 - mse: 30424302.0000 - mae: 2066.4541 - val_loss: 2227.7290 - val_mse: 32793556.0000 - val_mae: 2227.7290\n",
            "Epoch 289/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 2073.9053 - mse: 28683014.0000 - mae: 2073.9053 - val_loss: 2225.6432 - val_mse: 32605146.0000 - val_mae: 2225.6433\n",
            "Epoch 290/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 2086.1926 - mse: 30841004.0000 - mae: 2086.1924 - val_loss: 2227.2774 - val_mse: 32658300.0000 - val_mae: 2227.2773\n",
            "Epoch 291/500\n",
            "2250/2250 [==============================] - 0s 84us/step - loss: 2023.6679 - mse: 28696670.0000 - mae: 2023.6680 - val_loss: 2206.3909 - val_mse: 32408280.0000 - val_mae: 2206.3911\n",
            "Epoch 292/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 2011.2994 - mse: 26821748.0000 - mae: 2011.2993 - val_loss: 2212.3629 - val_mse: 32377206.0000 - val_mae: 2212.3625\n",
            "Epoch 293/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2101.1505 - mse: 29263740.0000 - mae: 2101.1506 - val_loss: 2239.9487 - val_mse: 32611674.0000 - val_mae: 2239.9490\n",
            "Epoch 294/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 2032.5167 - mse: 27496786.0000 - mae: 2032.5167 - val_loss: 2233.7018 - val_mse: 32602514.0000 - val_mae: 2233.7019\n",
            "Epoch 295/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1996.5301 - mse: 26888568.0000 - mae: 1996.5300 - val_loss: 2203.0828 - val_mse: 32378214.0000 - val_mae: 2203.0830\n",
            "Epoch 296/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 2060.0169 - mse: 30725810.0000 - mae: 2060.0168 - val_loss: 2229.3613 - val_mse: 32645746.0000 - val_mae: 2229.3616\n",
            "Epoch 297/500\n",
            "2250/2250 [==============================] - 0s 77us/step - loss: 1985.5855 - mse: 26430466.0000 - mae: 1985.5853 - val_loss: 2246.6109 - val_mse: 32572750.0000 - val_mae: 2246.6108\n",
            "Epoch 298/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2053.3465 - mse: 27497836.0000 - mae: 2053.3467 - val_loss: 2211.8013 - val_mse: 32279830.0000 - val_mae: 2211.8010\n",
            "Epoch 299/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2014.1507 - mse: 25897532.0000 - mae: 2014.1506 - val_loss: 2213.4150 - val_mse: 32228428.0000 - val_mae: 2213.4150\n",
            "Epoch 300/500\n",
            "2250/2250 [==============================] - 0s 77us/step - loss: 2067.7514 - mse: 29065914.0000 - mae: 2067.7515 - val_loss: 2239.2163 - val_mse: 32436336.0000 - val_mae: 2239.2161\n",
            "Epoch 301/500\n",
            "2250/2250 [==============================] - 0s 91us/step - loss: 2062.4542 - mse: 29483758.0000 - mae: 2062.4541 - val_loss: 2220.5643 - val_mse: 32297520.0000 - val_mae: 2220.5645\n",
            "Epoch 302/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2079.1355 - mse: 29275622.0000 - mae: 2079.1355 - val_loss: 2211.9291 - val_mse: 32177542.0000 - val_mae: 2211.9292\n",
            "Epoch 303/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2017.8472 - mse: 27324610.0000 - mae: 2017.8472 - val_loss: 2211.2433 - val_mse: 32097734.0000 - val_mae: 2211.2434\n",
            "Epoch 304/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 1989.4410 - mse: 25953102.0000 - mae: 1989.4412 - val_loss: 2217.8279 - val_mse: 32191346.0000 - val_mae: 2217.8279\n",
            "Epoch 305/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2069.7736 - mse: 27971562.0000 - mae: 2069.7734 - val_loss: 2213.8281 - val_mse: 32177964.0000 - val_mae: 2213.8281\n",
            "Epoch 306/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 2012.0313 - mse: 27056292.0000 - mae: 2012.0314 - val_loss: 2216.4585 - val_mse: 32264232.0000 - val_mae: 2216.4585\n",
            "Epoch 307/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2006.9926 - mse: 26728216.0000 - mae: 2006.9924 - val_loss: 2233.5137 - val_mse: 32372440.0000 - val_mae: 2233.5137\n",
            "Epoch 308/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 1946.4559 - mse: 24670546.0000 - mae: 1946.4561 - val_loss: 2227.5754 - val_mse: 32303860.0000 - val_mae: 2227.5754\n",
            "Epoch 309/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 2035.3115 - mse: 27910382.0000 - mae: 2035.3115 - val_loss: 2238.5779 - val_mse: 32471658.0000 - val_mae: 2238.5779\n",
            "Epoch 310/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2007.0407 - mse: 26668802.0000 - mae: 2007.0409 - val_loss: 2246.2723 - val_mse: 32612086.0000 - val_mae: 2246.2720\n",
            "Epoch 311/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2009.4357 - mse: 27817136.0000 - mae: 2009.4355 - val_loss: 2217.5399 - val_mse: 32087504.0000 - val_mae: 2217.5398\n",
            "Epoch 312/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 2078.0346 - mse: 28515542.0000 - mae: 2078.0347 - val_loss: 2216.4671 - val_mse: 32229528.0000 - val_mae: 2216.4668\n",
            "Epoch 313/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1959.8468 - mse: 24991442.0000 - mae: 1959.8469 - val_loss: 2225.7318 - val_mse: 32322330.0000 - val_mae: 2225.7314\n",
            "Epoch 314/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2087.8232 - mse: 30887554.0000 - mae: 2087.8230 - val_loss: 2205.0127 - val_mse: 31848614.0000 - val_mae: 2205.0125\n",
            "Epoch 315/500\n",
            "2250/2250 [==============================] - 0s 81us/step - loss: 2001.1326 - mse: 27469364.0000 - mae: 2001.1324 - val_loss: 2224.9739 - val_mse: 32050516.0000 - val_mae: 2224.9741\n",
            "Epoch 316/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1979.1432 - mse: 27385418.0000 - mae: 1979.1431 - val_loss: 2204.8164 - val_mse: 31656178.0000 - val_mae: 2204.8164\n",
            "Epoch 317/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 1976.1222 - mse: 26402142.0000 - mae: 1976.1219 - val_loss: 2231.1026 - val_mse: 32025630.0000 - val_mae: 2231.1028\n",
            "Epoch 318/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1951.8502 - mse: 24737546.0000 - mae: 1951.8502 - val_loss: 2258.9237 - val_mse: 32328308.0000 - val_mae: 2258.9236\n",
            "Epoch 319/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 2015.3190 - mse: 26862620.0000 - mae: 2015.3188 - val_loss: 2225.6972 - val_mse: 31920706.0000 - val_mae: 2225.6970\n",
            "Epoch 320/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2031.8936 - mse: 29124798.0000 - mae: 2031.8938 - val_loss: 2240.7334 - val_mse: 32082154.0000 - val_mae: 2240.7332\n",
            "Epoch 321/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1993.1455 - mse: 26360852.0000 - mae: 1993.1454 - val_loss: 2234.5339 - val_mse: 32254878.0000 - val_mae: 2234.5344\n",
            "Epoch 322/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2104.0286 - mse: 30260764.0000 - mae: 2104.0286 - val_loss: 2232.9122 - val_mse: 32254248.0000 - val_mae: 2232.9121\n",
            "Epoch 323/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2003.2973 - mse: 26813394.0000 - mae: 2003.2971 - val_loss: 2233.6233 - val_mse: 32043732.0000 - val_mae: 2233.6233\n",
            "Epoch 324/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2003.2425 - mse: 27925296.0000 - mae: 2003.2424 - val_loss: 2227.7849 - val_mse: 32014248.0000 - val_mae: 2227.7847\n",
            "Epoch 325/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1989.9006 - mse: 26418336.0000 - mae: 1989.9006 - val_loss: 2208.0510 - val_mse: 31919870.0000 - val_mae: 2208.0508\n",
            "Epoch 326/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 2007.9283 - mse: 26126548.0000 - mae: 2007.9282 - val_loss: 2235.4904 - val_mse: 32258538.0000 - val_mae: 2235.4905\n",
            "Epoch 327/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 1998.3120 - mse: 26107848.0000 - mae: 1998.3120 - val_loss: 2205.9982 - val_mse: 31898848.0000 - val_mae: 2205.9985\n",
            "Epoch 328/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1956.3713 - mse: 25915654.0000 - mae: 1956.3713 - val_loss: 2226.7212 - val_mse: 31921832.0000 - val_mae: 2226.7214\n",
            "Epoch 329/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1954.2205 - mse: 25821498.0000 - mae: 1954.2205 - val_loss: 2220.1875 - val_mse: 31982904.0000 - val_mae: 2220.1875\n",
            "Epoch 330/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1993.6735 - mse: 27244300.0000 - mae: 1993.6736 - val_loss: 2241.2910 - val_mse: 32174050.0000 - val_mae: 2241.2908\n",
            "Epoch 331/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 1994.1935 - mse: 27991368.0000 - mae: 1994.1936 - val_loss: 2241.8641 - val_mse: 32180112.0000 - val_mae: 2241.8643\n",
            "Epoch 332/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2009.5293 - mse: 27942300.0000 - mae: 2009.5293 - val_loss: 2226.6754 - val_mse: 32055062.0000 - val_mae: 2226.6755\n",
            "Epoch 333/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2045.9796 - mse: 29911086.0000 - mae: 2045.9796 - val_loss: 2211.1653 - val_mse: 31966324.0000 - val_mae: 2211.1655\n",
            "Epoch 334/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 2033.3949 - mse: 27975670.0000 - mae: 2033.3949 - val_loss: 2200.2993 - val_mse: 31734636.0000 - val_mae: 2200.2988\n",
            "Epoch 335/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 2029.7198 - mse: 28080270.0000 - mae: 2029.7197 - val_loss: 2225.3163 - val_mse: 32005358.0000 - val_mae: 2225.3162\n",
            "Epoch 336/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 2004.2201 - mse: 27293180.0000 - mae: 2004.2202 - val_loss: 2218.3079 - val_mse: 32116074.0000 - val_mae: 2218.3079\n",
            "Epoch 337/500\n",
            "2250/2250 [==============================] - 0s 81us/step - loss: 2040.3054 - mse: 28472040.0000 - mae: 2040.3055 - val_loss: 2226.5953 - val_mse: 32076596.0000 - val_mae: 2226.5952\n",
            "Epoch 338/500\n",
            "2250/2250 [==============================] - 0s 83us/step - loss: 2004.9944 - mse: 27264552.0000 - mae: 2004.9943 - val_loss: 2198.0388 - val_mse: 31787486.0000 - val_mae: 2198.0388\n",
            "Epoch 339/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 1901.3567 - mse: 24278354.0000 - mae: 1901.3567 - val_loss: 2223.9040 - val_mse: 31987768.0000 - val_mae: 2223.9041\n",
            "Epoch 340/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2017.5721 - mse: 27981720.0000 - mae: 2017.5720 - val_loss: 2211.4805 - val_mse: 31617646.0000 - val_mae: 2211.4805\n",
            "Epoch 341/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1988.3855 - mse: 26196298.0000 - mae: 1988.3855 - val_loss: 2196.7945 - val_mse: 31558748.0000 - val_mae: 2196.7947\n",
            "Epoch 342/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1963.5833 - mse: 25439598.0000 - mae: 1963.5831 - val_loss: 2239.1286 - val_mse: 32034802.0000 - val_mae: 2239.1284\n",
            "Epoch 343/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2005.7041 - mse: 27395588.0000 - mae: 2005.7040 - val_loss: 2211.1398 - val_mse: 31627994.0000 - val_mae: 2211.1399\n",
            "Epoch 344/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 1973.8695 - mse: 27245848.0000 - mae: 1973.8695 - val_loss: 2198.6823 - val_mse: 31551980.0000 - val_mae: 2198.6819\n",
            "Epoch 345/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1918.5145 - mse: 23310794.0000 - mae: 1918.5144 - val_loss: 2224.2015 - val_mse: 31770630.0000 - val_mae: 2224.2017\n",
            "Epoch 346/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1984.3059 - mse: 24897936.0000 - mae: 1984.3063 - val_loss: 2216.7255 - val_mse: 31710632.0000 - val_mae: 2216.7253\n",
            "Epoch 347/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2054.5621 - mse: 29236488.0000 - mae: 2054.5623 - val_loss: 2230.0997 - val_mse: 31769602.0000 - val_mae: 2230.0999\n",
            "Epoch 348/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 2059.0153 - mse: 30377690.0000 - mae: 2059.0154 - val_loss: 2225.3297 - val_mse: 31852258.0000 - val_mae: 2225.3298\n",
            "Epoch 349/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2004.1361 - mse: 28247668.0000 - mae: 2004.1362 - val_loss: 2205.3343 - val_mse: 31540092.0000 - val_mae: 2205.3345\n",
            "Epoch 350/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2013.9275 - mse: 26136112.0000 - mae: 2013.9276 - val_loss: 2214.0237 - val_mse: 31635188.0000 - val_mae: 2214.0234\n",
            "Epoch 351/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2005.2678 - mse: 27379068.0000 - mae: 2005.2678 - val_loss: 2227.4819 - val_mse: 31593020.0000 - val_mae: 2227.4817\n",
            "Epoch 352/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1981.9704 - mse: 24880766.0000 - mae: 1981.9705 - val_loss: 2187.9785 - val_mse: 31212616.0000 - val_mae: 2187.9788\n",
            "Epoch 353/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2013.3181 - mse: 26805398.0000 - mae: 2013.3180 - val_loss: 2230.8299 - val_mse: 31637246.0000 - val_mae: 2230.8298\n",
            "Epoch 354/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1911.8575 - mse: 24325990.0000 - mae: 1911.8575 - val_loss: 2223.0570 - val_mse: 31532316.0000 - val_mae: 2223.0571\n",
            "Epoch 355/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 1923.3515 - mse: 24394838.0000 - mae: 1923.3516 - val_loss: 2234.1342 - val_mse: 31731188.0000 - val_mae: 2234.1340\n",
            "Epoch 356/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1994.6489 - mse: 25837676.0000 - mae: 1994.6489 - val_loss: 2206.2761 - val_mse: 31288288.0000 - val_mae: 2206.2759\n",
            "Epoch 357/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1999.0709 - mse: 27715064.0000 - mae: 1999.0709 - val_loss: 2193.0835 - val_mse: 31002234.0000 - val_mae: 2193.0835\n",
            "Epoch 358/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1949.6640 - mse: 24982366.0000 - mae: 1949.6639 - val_loss: 2200.2807 - val_mse: 31103386.0000 - val_mae: 2200.2810\n",
            "Epoch 359/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 1889.3057 - mse: 22065458.0000 - mae: 1889.3055 - val_loss: 2207.5133 - val_mse: 31142132.0000 - val_mae: 2207.5134\n",
            "Epoch 360/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 1979.1765 - mse: 26353694.0000 - mae: 1979.1764 - val_loss: 2204.2758 - val_mse: 31252064.0000 - val_mae: 2204.2756\n",
            "Epoch 361/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 2034.9556 - mse: 28828846.0000 - mae: 2034.9556 - val_loss: 2197.3291 - val_mse: 31308808.0000 - val_mae: 2197.3289\n",
            "Epoch 362/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1951.8462 - mse: 24263192.0000 - mae: 1951.8462 - val_loss: 2218.2763 - val_mse: 31532438.0000 - val_mae: 2218.2764\n",
            "Epoch 363/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2027.9804 - mse: 29542180.0000 - mae: 2027.9805 - val_loss: 2244.1900 - val_mse: 31795978.0000 - val_mae: 2244.1899\n",
            "Epoch 364/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2044.5309 - mse: 28473314.0000 - mae: 2044.5311 - val_loss: 2202.8749 - val_mse: 31324556.0000 - val_mae: 2202.8750\n",
            "Epoch 365/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 1937.9657 - mse: 26761026.0000 - mae: 1937.9658 - val_loss: 2218.9396 - val_mse: 31456154.0000 - val_mae: 2218.9395\n",
            "Epoch 366/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 1982.7811 - mse: 25767270.0000 - mae: 1982.7811 - val_loss: 2193.9761 - val_mse: 31111218.0000 - val_mae: 2193.9763\n",
            "Epoch 367/500\n",
            "2250/2250 [==============================] - 0s 83us/step - loss: 1974.5776 - mse: 26102928.0000 - mae: 1974.5775 - val_loss: 2205.0422 - val_mse: 31436108.0000 - val_mae: 2205.0417\n",
            "Epoch 368/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1975.4081 - mse: 25653558.0000 - mae: 1975.4082 - val_loss: 2190.4085 - val_mse: 31204752.0000 - val_mae: 2190.4084\n",
            "Epoch 369/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 1978.9958 - mse: 26370416.0000 - mae: 1978.9957 - val_loss: 2193.2192 - val_mse: 31189544.0000 - val_mae: 2193.2190\n",
            "Epoch 370/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 1945.4837 - mse: 25673090.0000 - mae: 1945.4835 - val_loss: 2198.7771 - val_mse: 31235634.0000 - val_mae: 2198.7771\n",
            "Epoch 371/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1927.2757 - mse: 25921592.0000 - mae: 1927.2755 - val_loss: 2192.4106 - val_mse: 31077962.0000 - val_mae: 2192.4106\n",
            "Epoch 372/500\n",
            "2250/2250 [==============================] - 0s 81us/step - loss: 1973.1009 - mse: 26389880.0000 - mae: 1973.1008 - val_loss: 2209.9565 - val_mse: 31281000.0000 - val_mae: 2209.9565\n",
            "Epoch 373/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1902.2431 - mse: 24940274.0000 - mae: 1902.2429 - val_loss: 2207.0303 - val_mse: 31286916.0000 - val_mae: 2207.0303\n",
            "Epoch 374/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2025.1488 - mse: 27589272.0000 - mae: 2025.1489 - val_loss: 2210.2548 - val_mse: 31207220.0000 - val_mae: 2210.2551\n",
            "Epoch 375/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2108.9668 - mse: 31675806.0000 - mae: 2108.9666 - val_loss: 2203.3211 - val_mse: 31062888.0000 - val_mae: 2203.3210\n",
            "Epoch 376/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1970.1868 - mse: 26996082.0000 - mae: 1970.1869 - val_loss: 2188.0595 - val_mse: 30922172.0000 - val_mae: 2188.0596\n",
            "Epoch 377/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 1952.4397 - mse: 25720882.0000 - mae: 1952.4398 - val_loss: 2184.4440 - val_mse: 30811408.0000 - val_mae: 2184.4436\n",
            "Epoch 378/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1963.1443 - mse: 26422166.0000 - mae: 1963.1444 - val_loss: 2197.7473 - val_mse: 30893040.0000 - val_mae: 2197.7471\n",
            "Epoch 379/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1909.5431 - mse: 23524760.0000 - mae: 1909.5431 - val_loss: 2197.1989 - val_mse: 30886286.0000 - val_mae: 2197.1990\n",
            "Epoch 380/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2041.1106 - mse: 29242384.0000 - mae: 2041.1107 - val_loss: 2192.3306 - val_mse: 30889076.0000 - val_mae: 2192.3306\n",
            "Epoch 381/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 1950.8115 - mse: 27281106.0000 - mae: 1950.8113 - val_loss: 2194.0289 - val_mse: 30976370.0000 - val_mae: 2194.0288\n",
            "Epoch 382/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 1955.2344 - mse: 25420754.0000 - mae: 1955.2345 - val_loss: 2183.3695 - val_mse: 30791140.0000 - val_mae: 2183.3694\n",
            "Epoch 383/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1981.8247 - mse: 27184078.0000 - mae: 1981.8247 - val_loss: 2187.8200 - val_mse: 30798152.0000 - val_mae: 2187.8201\n",
            "Epoch 384/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1896.8130 - mse: 24093486.0000 - mae: 1896.8131 - val_loss: 2181.2558 - val_mse: 30741754.0000 - val_mae: 2181.2559\n",
            "Epoch 385/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1981.9264 - mse: 26604802.0000 - mae: 1981.9264 - val_loss: 2189.9328 - val_mse: 30816134.0000 - val_mae: 2189.9329\n",
            "Epoch 386/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1961.5283 - mse: 26893812.0000 - mae: 1961.5284 - val_loss: 2197.3676 - val_mse: 30990354.0000 - val_mae: 2197.3677\n",
            "Epoch 387/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1936.0869 - mse: 25525498.0000 - mae: 1936.0869 - val_loss: 2221.1744 - val_mse: 31145062.0000 - val_mae: 2221.1743\n",
            "Epoch 388/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 2006.6725 - mse: 27600092.0000 - mae: 2006.6726 - val_loss: 2207.5998 - val_mse: 31106414.0000 - val_mae: 2207.5999\n",
            "Epoch 389/500\n",
            "2250/2250 [==============================] - 0s 93us/step - loss: 1976.5648 - mse: 25398768.0000 - mae: 1976.5649 - val_loss: 2212.2153 - val_mse: 31192430.0000 - val_mae: 2212.2151\n",
            "Epoch 390/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2027.1156 - mse: 27698886.0000 - mae: 2027.1156 - val_loss: 2194.8919 - val_mse: 30942808.0000 - val_mae: 2194.8921\n",
            "Epoch 391/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1922.3204 - mse: 23820598.0000 - mae: 1922.3204 - val_loss: 2202.0115 - val_mse: 30878760.0000 - val_mae: 2202.0112\n",
            "Epoch 392/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1926.7319 - mse: 24925278.0000 - mae: 1926.7318 - val_loss: 2193.6668 - val_mse: 30751664.0000 - val_mae: 2193.6670\n",
            "Epoch 393/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 1952.5507 - mse: 27293938.0000 - mae: 1952.5507 - val_loss: 2206.2971 - val_mse: 30784742.0000 - val_mae: 2206.2971\n",
            "Epoch 394/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1974.2406 - mse: 25445396.0000 - mae: 1974.2407 - val_loss: 2172.4798 - val_mse: 30418732.0000 - val_mae: 2172.4797\n",
            "Epoch 395/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1931.2608 - mse: 25881838.0000 - mae: 1931.2609 - val_loss: 2200.3660 - val_mse: 30764780.0000 - val_mae: 2200.3660\n",
            "Epoch 396/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 2043.3632 - mse: 30177696.0000 - mae: 2043.3633 - val_loss: 2191.1296 - val_mse: 30638728.0000 - val_mae: 2191.1296\n",
            "Epoch 397/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1997.2066 - mse: 28013580.0000 - mae: 1997.2067 - val_loss: 2182.9292 - val_mse: 30715882.0000 - val_mae: 2182.9292\n",
            "Epoch 398/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1951.0643 - mse: 27561820.0000 - mae: 1951.0645 - val_loss: 2194.3229 - val_mse: 30883702.0000 - val_mae: 2194.3230\n",
            "Epoch 399/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 1921.2075 - mse: 24978128.0000 - mae: 1921.2073 - val_loss: 2197.7986 - val_mse: 31061130.0000 - val_mae: 2197.7986\n",
            "Epoch 400/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 1934.3828 - mse: 24590972.0000 - mae: 1934.3827 - val_loss: 2184.5238 - val_mse: 30853336.0000 - val_mae: 2184.5234\n",
            "Epoch 401/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 1977.8154 - mse: 27308632.0000 - mae: 1977.8153 - val_loss: 2193.6268 - val_mse: 30933378.0000 - val_mae: 2193.6270\n",
            "Epoch 402/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 1930.2035 - mse: 26005954.0000 - mae: 1930.2036 - val_loss: 2205.8847 - val_mse: 31152544.0000 - val_mae: 2205.8850\n",
            "Epoch 403/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1926.0644 - mse: 25820934.0000 - mae: 1926.0645 - val_loss: 2187.5876 - val_mse: 30825346.0000 - val_mae: 2187.5876\n",
            "Epoch 404/500\n",
            "2250/2250 [==============================] - 0s 84us/step - loss: 1942.5042 - mse: 26423186.0000 - mae: 1942.5043 - val_loss: 2222.6908 - val_mse: 30923248.0000 - val_mae: 2222.6909\n",
            "Epoch 405/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1964.2971 - mse: 26672380.0000 - mae: 1964.2971 - val_loss: 2173.4221 - val_mse: 30557420.0000 - val_mae: 2173.4219\n",
            "Epoch 406/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1954.5211 - mse: 25908448.0000 - mae: 1954.5211 - val_loss: 2189.0128 - val_mse: 30784328.0000 - val_mae: 2189.0127\n",
            "Epoch 407/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1935.0012 - mse: 25161380.0000 - mae: 1935.0011 - val_loss: 2176.4873 - val_mse: 30506816.0000 - val_mae: 2176.4873\n",
            "Epoch 408/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1995.8101 - mse: 27054000.0000 - mae: 1995.8102 - val_loss: 2196.1664 - val_mse: 30675548.0000 - val_mae: 2196.1663\n",
            "Epoch 409/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1995.7820 - mse: 28122478.0000 - mae: 1995.7822 - val_loss: 2177.8294 - val_mse: 30464872.0000 - val_mae: 2177.8291\n",
            "Epoch 410/500\n",
            "2250/2250 [==============================] - 0s 81us/step - loss: 1908.6888 - mse: 24387024.0000 - mae: 1908.6888 - val_loss: 2174.4583 - val_mse: 30453360.0000 - val_mae: 2174.4587\n",
            "Epoch 411/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1945.7137 - mse: 26422736.0000 - mae: 1945.7135 - val_loss: 2179.6228 - val_mse: 30681690.0000 - val_mae: 2179.6226\n",
            "Epoch 412/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1888.3041 - mse: 22838458.0000 - mae: 1888.3042 - val_loss: 2188.2788 - val_mse: 30659900.0000 - val_mae: 2188.2788\n",
            "Epoch 413/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1978.0191 - mse: 28070060.0000 - mae: 1978.0192 - val_loss: 2174.4956 - val_mse: 30522430.0000 - val_mae: 2174.4956\n",
            "Epoch 414/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 1965.1483 - mse: 27457324.0000 - mae: 1965.1482 - val_loss: 2208.5243 - val_mse: 30871924.0000 - val_mae: 2208.5242\n",
            "Epoch 415/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1961.3374 - mse: 26410266.0000 - mae: 1961.3373 - val_loss: 2194.2765 - val_mse: 30762446.0000 - val_mae: 2194.2764\n",
            "Epoch 416/500\n",
            "2250/2250 [==============================] - 0s 77us/step - loss: 1935.8917 - mse: 26008858.0000 - mae: 1935.8917 - val_loss: 2183.8989 - val_mse: 30592776.0000 - val_mae: 2183.8987\n",
            "Epoch 417/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1957.9062 - mse: 27130232.0000 - mae: 1957.9062 - val_loss: 2168.1893 - val_mse: 30504632.0000 - val_mae: 2168.1895\n",
            "Epoch 418/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1962.6489 - mse: 25127878.0000 - mae: 1962.6489 - val_loss: 2175.5330 - val_mse: 30525642.0000 - val_mae: 2175.5332\n",
            "Epoch 419/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 1960.3480 - mse: 27465356.0000 - mae: 1960.3480 - val_loss: 2197.0939 - val_mse: 30682492.0000 - val_mae: 2197.0938\n",
            "Epoch 420/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1955.0331 - mse: 26652728.0000 - mae: 1955.0333 - val_loss: 2199.3024 - val_mse: 30651046.0000 - val_mae: 2199.3025\n",
            "Epoch 421/500\n",
            "2250/2250 [==============================] - 0s 86us/step - loss: 1894.1605 - mse: 23681206.0000 - mae: 1894.1603 - val_loss: 2170.8955 - val_mse: 30294968.0000 - val_mae: 2170.8955\n",
            "Epoch 422/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 1926.2375 - mse: 24661418.0000 - mae: 1926.2375 - val_loss: 2171.0988 - val_mse: 30237320.0000 - val_mae: 2171.0986\n",
            "Epoch 423/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1919.1387 - mse: 25216770.0000 - mae: 1919.1387 - val_loss: 2165.3169 - val_mse: 30218322.0000 - val_mae: 2165.3169\n",
            "Epoch 424/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 1966.8689 - mse: 25660388.0000 - mae: 1966.8689 - val_loss: 2165.4314 - val_mse: 30252726.0000 - val_mae: 2165.4309\n",
            "Epoch 425/500\n",
            "2250/2250 [==============================] - 0s 84us/step - loss: 1914.7205 - mse: 25564044.0000 - mae: 1914.7205 - val_loss: 2171.5651 - val_mse: 30209526.0000 - val_mae: 2171.5652\n",
            "Epoch 426/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 1950.3015 - mse: 26319462.0000 - mae: 1950.3015 - val_loss: 2177.2967 - val_mse: 30273414.0000 - val_mae: 2177.2969\n",
            "Epoch 427/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1925.3246 - mse: 24717040.0000 - mae: 1925.3247 - val_loss: 2189.0824 - val_mse: 30407084.0000 - val_mae: 2189.0825\n",
            "Epoch 428/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 1863.1615 - mse: 23023872.0000 - mae: 1863.1615 - val_loss: 2165.3175 - val_mse: 30136576.0000 - val_mae: 2165.3174\n",
            "Epoch 429/500\n",
            "2250/2250 [==============================] - 0s 83us/step - loss: 1944.1193 - mse: 26714544.0000 - mae: 1944.1194 - val_loss: 2154.5008 - val_mse: 29956680.0000 - val_mae: 2154.5010\n",
            "Epoch 430/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1957.5223 - mse: 25855532.0000 - mae: 1957.5222 - val_loss: 2168.9514 - val_mse: 30245354.0000 - val_mae: 2168.9514\n",
            "Epoch 431/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1898.1519 - mse: 25110938.0000 - mae: 1898.1517 - val_loss: 2162.7447 - val_mse: 30251464.0000 - val_mae: 2162.7446\n",
            "Epoch 432/500\n",
            "2250/2250 [==============================] - 0s 83us/step - loss: 1948.4899 - mse: 26372908.0000 - mae: 1948.4897 - val_loss: 2175.3269 - val_mse: 30460234.0000 - val_mae: 2175.3271\n",
            "Epoch 433/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 1914.6702 - mse: 25464286.0000 - mae: 1914.6702 - val_loss: 2182.9863 - val_mse: 30506742.0000 - val_mae: 2182.9863\n",
            "Epoch 434/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 1927.0100 - mse: 24563598.0000 - mae: 1927.0100 - val_loss: 2189.6882 - val_mse: 30636358.0000 - val_mae: 2189.6880\n",
            "Epoch 435/500\n",
            "2250/2250 [==============================] - 0s 94us/step - loss: 1952.1389 - mse: 26387290.0000 - mae: 1952.1389 - val_loss: 2193.6475 - val_mse: 30587726.0000 - val_mae: 2193.6475\n",
            "Epoch 436/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1911.2304 - mse: 26213146.0000 - mae: 1911.2305 - val_loss: 2201.1169 - val_mse: 30518442.0000 - val_mae: 2201.1172\n",
            "Epoch 437/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 1980.2079 - mse: 27101358.0000 - mae: 1980.2078 - val_loss: 2165.1829 - val_mse: 30160788.0000 - val_mae: 2165.1831\n",
            "Epoch 438/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 1939.6023 - mse: 26549840.0000 - mae: 1939.6024 - val_loss: 2174.3627 - val_mse: 29988706.0000 - val_mae: 2174.3625\n",
            "Epoch 439/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 1894.4837 - mse: 24967410.0000 - mae: 1894.4835 - val_loss: 2162.9184 - val_mse: 30003036.0000 - val_mae: 2162.9185\n",
            "Epoch 440/500\n",
            "2250/2250 [==============================] - 0s 86us/step - loss: 1928.5916 - mse: 25507962.0000 - mae: 1928.5916 - val_loss: 2165.2586 - val_mse: 30074694.0000 - val_mae: 2165.2585\n",
            "Epoch 441/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 1958.0813 - mse: 26915626.0000 - mae: 1958.0813 - val_loss: 2194.4050 - val_mse: 30215696.0000 - val_mae: 2194.4050\n",
            "Epoch 442/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1902.0701 - mse: 24008328.0000 - mae: 1902.0699 - val_loss: 2175.5680 - val_mse: 30118054.0000 - val_mae: 2175.5681\n",
            "Epoch 443/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1910.9354 - mse: 25769702.0000 - mae: 1910.9353 - val_loss: 2195.0370 - val_mse: 30337494.0000 - val_mae: 2195.0374\n",
            "Epoch 444/500\n",
            "2250/2250 [==============================] - 0s 81us/step - loss: 1903.0790 - mse: 24037920.0000 - mae: 1903.0791 - val_loss: 2215.5790 - val_mse: 30521914.0000 - val_mae: 2215.5789\n",
            "Epoch 445/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1956.2601 - mse: 25661958.0000 - mae: 1956.2603 - val_loss: 2174.8408 - val_mse: 30284032.0000 - val_mae: 2174.8411\n",
            "Epoch 446/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1933.0716 - mse: 24990850.0000 - mae: 1933.0715 - val_loss: 2178.4036 - val_mse: 30372186.0000 - val_mae: 2178.4036\n",
            "Epoch 447/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 2025.9578 - mse: 29775084.0000 - mae: 2025.9578 - val_loss: 2179.6073 - val_mse: 30364978.0000 - val_mae: 2179.6074\n",
            "Epoch 448/500\n",
            "2250/2250 [==============================] - 0s 94us/step - loss: 1897.3616 - mse: 24906582.0000 - mae: 1897.3616 - val_loss: 2163.0827 - val_mse: 30136044.0000 - val_mae: 2163.0828\n",
            "Epoch 449/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 1842.3167 - mse: 23227062.0000 - mae: 1842.3168 - val_loss: 2157.9379 - val_mse: 30153398.0000 - val_mae: 2157.9380\n",
            "Epoch 450/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 1964.5963 - mse: 27004550.0000 - mae: 1964.5964 - val_loss: 2198.8753 - val_mse: 30446870.0000 - val_mae: 2198.8755\n",
            "Epoch 451/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1962.6670 - mse: 28236172.0000 - mae: 1962.6669 - val_loss: 2210.6595 - val_mse: 30498604.0000 - val_mae: 2210.6597\n",
            "Epoch 452/500\n",
            "2250/2250 [==============================] - 0s 94us/step - loss: 1880.4082 - mse: 22859214.0000 - mae: 1880.4080 - val_loss: 2189.5537 - val_mse: 30339796.0000 - val_mae: 2189.5535\n",
            "Epoch 453/500\n",
            "2250/2250 [==============================] - 0s 81us/step - loss: 1911.0503 - mse: 24226310.0000 - mae: 1911.0502 - val_loss: 2166.1712 - val_mse: 29964882.0000 - val_mae: 2166.1714\n",
            "Epoch 454/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1901.8138 - mse: 24755580.0000 - mae: 1901.8137 - val_loss: 2180.0909 - val_mse: 29988844.0000 - val_mae: 2180.0906\n",
            "Epoch 455/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 1930.0622 - mse: 24671058.0000 - mae: 1930.0623 - val_loss: 2156.5089 - val_mse: 29709252.0000 - val_mae: 2156.5090\n",
            "Epoch 456/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 1909.6832 - mse: 24199776.0000 - mae: 1909.6831 - val_loss: 2194.4132 - val_mse: 30018016.0000 - val_mae: 2194.4128\n",
            "Epoch 457/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1867.9810 - mse: 24990478.0000 - mae: 1867.9808 - val_loss: 2171.8384 - val_mse: 29856796.0000 - val_mae: 2171.8384\n",
            "Epoch 458/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 1910.3634 - mse: 24127214.0000 - mae: 1910.3633 - val_loss: 2158.8570 - val_mse: 29612100.0000 - val_mae: 2158.8572\n",
            "Epoch 459/500\n",
            "2250/2250 [==============================] - 0s 77us/step - loss: 1930.5051 - mse: 25192538.0000 - mae: 1930.5054 - val_loss: 2158.3337 - val_mse: 29529980.0000 - val_mae: 2158.3337\n",
            "Epoch 460/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1920.9088 - mse: 25090428.0000 - mae: 1920.9087 - val_loss: 2157.9993 - val_mse: 29532304.0000 - val_mae: 2157.9993\n",
            "Epoch 461/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1886.4433 - mse: 24989644.0000 - mae: 1886.4434 - val_loss: 2180.2867 - val_mse: 29709110.0000 - val_mae: 2180.2869\n",
            "Epoch 462/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1998.3507 - mse: 28106924.0000 - mae: 1998.3507 - val_loss: 2162.6719 - val_mse: 29535712.0000 - val_mae: 2162.6721\n",
            "Epoch 463/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1901.0423 - mse: 24235150.0000 - mae: 1901.0422 - val_loss: 2169.5656 - val_mse: 29569788.0000 - val_mae: 2169.5657\n",
            "Epoch 464/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1922.9619 - mse: 24024158.0000 - mae: 1922.9618 - val_loss: 2187.4689 - val_mse: 29683246.0000 - val_mae: 2187.4690\n",
            "Epoch 465/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 1940.7919 - mse: 25830296.0000 - mae: 1940.7920 - val_loss: 2171.3779 - val_mse: 29618040.0000 - val_mae: 2171.3777\n",
            "Epoch 466/500\n",
            "2250/2250 [==============================] - 0s 84us/step - loss: 1960.0071 - mse: 27217818.0000 - mae: 1960.0071 - val_loss: 2171.8734 - val_mse: 29712430.0000 - val_mae: 2171.8733\n",
            "Epoch 467/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1906.1185 - mse: 26115428.0000 - mae: 1906.1184 - val_loss: 2160.1220 - val_mse: 29553330.0000 - val_mae: 2160.1218\n",
            "Epoch 468/500\n",
            "2250/2250 [==============================] - 0s 86us/step - loss: 1892.3402 - mse: 23503982.0000 - mae: 1892.3402 - val_loss: 2158.2556 - val_mse: 29566762.0000 - val_mae: 2158.2556\n",
            "Epoch 469/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1922.9302 - mse: 25293480.0000 - mae: 1922.9302 - val_loss: 2174.1694 - val_mse: 29769814.0000 - val_mae: 2174.1694\n",
            "Epoch 470/500\n",
            "2250/2250 [==============================] - 0s 83us/step - loss: 1953.8246 - mse: 27124640.0000 - mae: 1953.8247 - val_loss: 2161.3516 - val_mse: 29634992.0000 - val_mae: 2161.3513\n",
            "Epoch 471/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 1990.6553 - mse: 28145252.0000 - mae: 1990.6553 - val_loss: 2167.5027 - val_mse: 29706646.0000 - val_mae: 2167.5024\n",
            "Epoch 472/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1878.6498 - mse: 23421130.0000 - mae: 1878.6500 - val_loss: 2149.8089 - val_mse: 29520464.0000 - val_mae: 2149.8086\n",
            "Epoch 473/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1899.1113 - mse: 25619144.0000 - mae: 1899.1111 - val_loss: 2180.6240 - val_mse: 29850898.0000 - val_mae: 2180.6240\n",
            "Epoch 474/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 1890.1844 - mse: 23826730.0000 - mae: 1890.1844 - val_loss: 2170.7530 - val_mse: 29798088.0000 - val_mae: 2170.7534\n",
            "Epoch 475/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 1933.3749 - mse: 25994958.0000 - mae: 1933.3751 - val_loss: 2170.8114 - val_mse: 29647096.0000 - val_mae: 2170.8115\n",
            "Epoch 476/500\n",
            "2250/2250 [==============================] - 0s 90us/step - loss: 1864.7989 - mse: 24050836.0000 - mae: 1864.7988 - val_loss: 2172.9143 - val_mse: 29582018.0000 - val_mae: 2172.9146\n",
            "Epoch 477/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1901.3442 - mse: 24747972.0000 - mae: 1901.3440 - val_loss: 2166.9171 - val_mse: 29575436.0000 - val_mae: 2166.9172\n",
            "Epoch 478/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1888.9055 - mse: 25317440.0000 - mae: 1888.9055 - val_loss: 2188.3607 - val_mse: 29701620.0000 - val_mae: 2188.3608\n",
            "Epoch 479/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1934.3365 - mse: 25319332.0000 - mae: 1934.3364 - val_loss: 2180.2628 - val_mse: 29660484.0000 - val_mae: 2180.2629\n",
            "Epoch 480/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1851.6225 - mse: 23668606.0000 - mae: 1851.6224 - val_loss: 2166.0827 - val_mse: 29594558.0000 - val_mae: 2166.0830\n",
            "Epoch 481/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1911.6592 - mse: 25256116.0000 - mae: 1911.6591 - val_loss: 2172.0245 - val_mse: 29541232.0000 - val_mae: 2172.0244\n",
            "Epoch 482/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1959.2816 - mse: 25966270.0000 - mae: 1959.2817 - val_loss: 2173.5989 - val_mse: 29732204.0000 - val_mae: 2173.5991\n",
            "Epoch 483/500\n",
            "2250/2250 [==============================] - 0s 83us/step - loss: 1909.8481 - mse: 23475240.0000 - mae: 1909.8483 - val_loss: 2165.6948 - val_mse: 29530536.0000 - val_mae: 2165.6951\n",
            "Epoch 484/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1903.4863 - mse: 25552722.0000 - mae: 1903.4862 - val_loss: 2178.0513 - val_mse: 29655056.0000 - val_mae: 2178.0513\n",
            "Epoch 485/500\n",
            "2250/2250 [==============================] - 0s 81us/step - loss: 1900.9847 - mse: 25904224.0000 - mae: 1900.9846 - val_loss: 2163.9012 - val_mse: 29563282.0000 - val_mae: 2163.9011\n",
            "Epoch 486/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 1958.9441 - mse: 26676850.0000 - mae: 1958.9440 - val_loss: 2151.5337 - val_mse: 29565508.0000 - val_mae: 2151.5334\n",
            "Epoch 487/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1866.9436 - mse: 22796982.0000 - mae: 1866.9436 - val_loss: 2176.3080 - val_mse: 30078742.0000 - val_mae: 2176.3081\n",
            "Epoch 488/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1875.1491 - mse: 23731690.0000 - mae: 1875.1492 - val_loss: 2154.1489 - val_mse: 29684498.0000 - val_mae: 2154.1487\n",
            "Epoch 489/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 1910.6133 - mse: 25390546.0000 - mae: 1910.6133 - val_loss: 2165.3821 - val_mse: 29705364.0000 - val_mae: 2165.3821\n",
            "Epoch 490/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 1896.7850 - mse: 25919146.0000 - mae: 1896.7849 - val_loss: 2166.0400 - val_mse: 29739318.0000 - val_mae: 2166.0398\n",
            "Epoch 491/500\n",
            "2250/2250 [==============================] - 0s 89us/step - loss: 1902.2803 - mse: 23534312.0000 - mae: 1902.2803 - val_loss: 2169.8551 - val_mse: 29564958.0000 - val_mae: 2169.8550\n",
            "Epoch 492/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1898.9983 - mse: 23496768.0000 - mae: 1898.9982 - val_loss: 2159.2294 - val_mse: 29438676.0000 - val_mae: 2159.2295\n",
            "Epoch 493/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1850.1024 - mse: 24056408.0000 - mae: 1850.1023 - val_loss: 2170.8259 - val_mse: 29552350.0000 - val_mae: 2170.8257\n",
            "Epoch 494/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 1915.7436 - mse: 25137914.0000 - mae: 1915.7435 - val_loss: 2186.7719 - val_mse: 29652336.0000 - val_mae: 2186.7717\n",
            "Epoch 495/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1934.6237 - mse: 24639164.0000 - mae: 1934.6235 - val_loss: 2163.2913 - val_mse: 29543768.0000 - val_mae: 2163.2913\n",
            "Epoch 496/500\n",
            "2250/2250 [==============================] - 0s 89us/step - loss: 1944.2041 - mse: 26075758.0000 - mae: 1944.2042 - val_loss: 2173.6279 - val_mse: 29622430.0000 - val_mae: 2173.6277\n",
            "Epoch 497/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 1920.3618 - mse: 25035022.0000 - mae: 1920.3618 - val_loss: 2173.5647 - val_mse: 29611544.0000 - val_mae: 2173.5647\n",
            "Epoch 498/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1970.5764 - mse: 27172844.0000 - mae: 1970.5764 - val_loss: 2150.8030 - val_mse: 29182598.0000 - val_mae: 2150.8032\n",
            "Epoch 499/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1954.8154 - mse: 26302856.0000 - mae: 1954.8153 - val_loss: 2156.3583 - val_mse: 29281642.0000 - val_mae: 2156.3584\n",
            "Epoch 500/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1880.6592 - mse: 24475998.0000 - mae: 1880.6593 - val_loss: 2148.3421 - val_mse: 29143314.0000 - val_mae: 2148.3420\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZSPkuft7YMp",
        "outputId": "71d02271-3b7a-4226-ab68-a72dc3ac6ca2"
      },
      "source": [
        "df_sklearn = calc_save_err_metric_combined(error_metrics, result_sklearn, max_of_pretrain_days, max_selected_countries, path=exp2_path, static_learner=True, alternate_batch=False, transpose=True)\n",
        "display_scores(df_sklearn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE Score\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge     LSTM\n",
            "0                      MAE        30.000     10719.064         26199.626 215350.292      7937.441       5687.090 2295.154\n",
            "1                      MAE        60.000      2998.439          3095.116   7455.608      2937.458       3861.348 3212.296\n",
            "2                      MAE        90.000      3517.174          3313.749   4953.637      3386.874       3816.471 5097.798\n",
            "3                      MAE       120.000      3309.609          2535.159   6236.067      2640.058       3885.423 4969.858\n",
            "4                      MAE       150.000      3726.906          2896.881  16026.703      3606.688       4680.568 4052.543\n",
            "5                      MAE       180.000      4657.525          4725.560  16702.749      4907.716       4403.925 5008.744\n",
            "mean                   NaN       105.000      4821.453          7127.682  44454.176      4236.039       4389.138 4106.065\n",
            "-----------------------------------------------------------------------------------\n",
            "RMSE Score\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge      LSTM\n",
            "0                     RMSE        30.000     13332.744         32452.411 650415.497     12162.997      14498.576  5524.865\n",
            "1                     RMSE        60.000      4797.720          5626.990  13858.035      6806.359       8495.825  6844.168\n",
            "2                     RMSE        90.000      7727.203          7498.593   9690.161      7958.597       9283.466 13213.493\n",
            "3                     RMSE       120.000      7129.238          5563.058  13341.399      5979.999       8465.979 11616.704\n",
            "4                     RMSE       150.000      8238.341          8712.535  41220.706      8924.896       9971.748  8608.255\n",
            "5                     RMSE       180.000      8289.249          9240.603  37313.388      9455.006       8893.869  7990.939\n",
            "mean                   NaN       105.000      8252.416         11515.698 127639.864      8547.976       9934.910  8966.404\n",
            "-----------------------------------------------------------------------------------\n",
            "MAPE Score\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge  LSTM\n",
            "0                     MAPE        30.000        65.045           151.906    588.491        38.443         12.928 1.501\n",
            "1                     MAPE        60.000        10.649             6.256     17.627         3.444          5.582 1.434\n",
            "2                     MAPE        90.000         4.535             1.610      5.625         1.624          3.862 1.325\n",
            "3                     MAPE       120.000         1.903             0.791      2.266         0.830          1.766 0.750\n",
            "4                     MAPE       150.000         1.123             0.578      1.760         0.787          1.355 0.783\n",
            "5                     MAPE       180.000         0.636             0.519      1.517         0.655          0.693 0.636\n",
            "mean                   NaN       105.000        13.982            26.943    102.881         7.631          4.365 1.071\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "U44wYr8WAqq8",
        "outputId": "681279c6-b485-4217-855e-b3bbfb16f951"
      },
      "source": [
        "save_runtime(running_time_static, path=exp2_runtime_path, static_learner=True)\n",
        "running_time_static"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PretrainDays</th>\n",
              "      <th>RandomForest</th>\n",
              "      <th>GradientBoosting</th>\n",
              "      <th>LinearSVR</th>\n",
              "      <th>DecisionTree</th>\n",
              "      <th>BayesianRidge</th>\n",
              "      <th>LSTM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30</td>\n",
              "      <td>0.212</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.017</td>\n",
              "      <td>26.291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60</td>\n",
              "      <td>0.514</td>\n",
              "      <td>0.924</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.020</td>\n",
              "      <td>35.858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>90</td>\n",
              "      <td>1.011</td>\n",
              "      <td>2.043</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.012</td>\n",
              "      <td>46.551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>120</td>\n",
              "      <td>1.512</td>\n",
              "      <td>3.219</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.014</td>\n",
              "      <td>61.248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>150</td>\n",
              "      <td>1.995</td>\n",
              "      <td>4.351</td>\n",
              "      <td>0.680</td>\n",
              "      <td>0.246</td>\n",
              "      <td>0.019</td>\n",
              "      <td>75.595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>180</td>\n",
              "      <td>2.499</td>\n",
              "      <td>5.507</td>\n",
              "      <td>0.865</td>\n",
              "      <td>0.309</td>\n",
              "      <td>0.022</td>\n",
              "      <td>85.747</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PretrainDays  RandomForest  ...  BayesianRidge   LSTM\n",
              "0            30         0.212  ...          0.017 26.291\n",
              "1            60         0.514  ...          0.020 35.858\n",
              "2            90         1.011  ...          0.012 46.551\n",
              "3           120         1.512  ...          0.014 61.248\n",
              "4           150         1.995  ...          0.019 75.595\n",
              "5           180         2.499  ...          0.022 85.747\n",
              "\n",
              "[6 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "-7NiYw19VchA",
        "outputId": "9c87251c-5023-43b7-d28e-0c88963cd02c"
      },
      "source": [
        "summary_table_static = get_summary_table(df_sklearn, running_time_static, error_metrics, static_learner=True)\n",
        "save_summary_table(summary_table_static, exp2_summary_path,static_learner=True,alternate_batch=False, transpose=True)\n",
        "summary_table_static"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RandomForest</th>\n",
              "      <th>GradientBoosting</th>\n",
              "      <th>LinearSVR</th>\n",
              "      <th>DecisionTree</th>\n",
              "      <th>BayesianRidge</th>\n",
              "      <th>LSTM</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Metric</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MAE</th>\n",
              "      <td>4821.453</td>\n",
              "      <td>7127.682</td>\n",
              "      <td>44454.176</td>\n",
              "      <td>4236.039</td>\n",
              "      <td>4389.138</td>\n",
              "      <td>4106.065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MAPE</th>\n",
              "      <td>13.982</td>\n",
              "      <td>26.943</td>\n",
              "      <td>102.881</td>\n",
              "      <td>7.631</td>\n",
              "      <td>4.365</td>\n",
              "      <td>1.071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RMSE</th>\n",
              "      <td>8252.416</td>\n",
              "      <td>11515.698</td>\n",
              "      <td>127639.864</td>\n",
              "      <td>8547.976</td>\n",
              "      <td>9934.910</td>\n",
              "      <td>8966.404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time(sec)</th>\n",
              "      <td>1.290</td>\n",
              "      <td>2.713</td>\n",
              "      <td>0.433</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.017</td>\n",
              "      <td>55.215</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           RandomForest  GradientBoosting  ...  BayesianRidge     LSTM\n",
              "Metric                                     ...                        \n",
              "MAE            4821.453          7127.682  ...       4389.138 4106.065\n",
              "MAPE             13.982            26.943  ...          4.365    1.071\n",
              "RMSE           8252.416         11515.698  ...       9934.910 8966.404\n",
              "Time(sec)         1.290             2.713  ...          0.017   55.215\n",
              "\n",
              "[4 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAdncNlrKXD8"
      },
      "source": [
        "## Incremental Learner: Alternate Batches\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGGSn84iKraj"
      },
      "source": [
        "def scikit_multiflow_alternate_batch(df, pretrain_days):\r\n",
        "\r\n",
        "    model, model_names = instantiate_regressors()\r\n",
        "\r\n",
        "    len_countries = len(df['country'].unique())\r\n",
        "\r\n",
        "    frames, running_time_frames = [], []\r\n",
        "\r\n",
        "    # Setup the evaluator\r\n",
        "    for day in pretrain_days:\r\n",
        "\r\n",
        "        df_subset = create_alternate_batch_subset(df, day, batch_size=10)\r\n",
        "\r\n",
        "        # Creating a stream from dataframe\r\n",
        "        stream = DataStream(np.array(df_subset.iloc[:, 4:-1]), y=np.array(df_subset.iloc[:, -1])) #TODO: Drop columns with name\r\n",
        "\r\n",
        "        pretrain_size = (day//2) * len_countries\r\n",
        "        max_samples = pretrain_size + 1\r\n",
        "        testing_samples_size = (day//2 + 20) * len_countries  # Testing on set one month ahead only\r\n",
        "\r\n",
        "        evaluator = EvaluatePrequential(show_plot=False,\r\n",
        "                                    pretrain_size=pretrain_size,\r\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\r\n",
        "                                    max_samples=max_samples)\r\n",
        "        # Run evaluation\r\n",
        "        evaluator.evaluate(stream=stream, model=model, model_names=model_names)\r\n",
        "\r\n",
        "        # Added Now\r\n",
        "        X = stream.X[pretrain_size: pretrain_size + testing_samples_size]\r\n",
        "        y = stream.y[pretrain_size: pretrain_size + testing_samples_size]\r\n",
        "\r\n",
        "        prediction = evaluator.predict(X)\r\n",
        "\r\n",
        "        # Since we add one extra sample, reset the evaluator\r\n",
        "        evaluator = reset_evaluator(evaluator)\r\n",
        "\r\n",
        "        evaluator = update_incremental_metrics(evaluator, y, prediction)\r\n",
        "\r\n",
        "        # Dictionary to store each iteration error scores\r\n",
        "        mdl_evaluation_scores = {}\r\n",
        "\r\n",
        "        # Adding Evaluation Measurements and pretraining days\r\n",
        "        mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\r\n",
        "        mdl_evaluation_scores['PretrainDays'] = [day//2] * len(mdl_evaluation_scores['EvaluationMeasurement'])\r\n",
        "\r\n",
        "        mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\r\n",
        "\r\n",
        "        # Errors of each model on a specific pre-train days\r\n",
        "        frames.append(mdl_evaluation_df)\r\n",
        "\r\n",
        "        # Run time for each algorithm\r\n",
        "        running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\r\n",
        "\r\n",
        "    # Final Run Time DataFrame\r\n",
        "    running_time_df = pd.concat(running_time_frames,ignore_index=True)\r\n",
        "\r\n",
        "    # Final Evaluation Score Dataframe\r\n",
        "    evaluation_scores_df = pd.concat(frames, ignore_index=True)\r\n",
        "    return evaluation_scores_df, running_time_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DpEHhhFiDfw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "918b9140-f0ea-4590-e072-d91284f6a867"
      },
      "source": [
        "# Old Script\n",
        "\"\"\"\n",
        "def scikit_multiflow_alternate_batch(df, pretrain_days):\n",
        "\n",
        "  model, model_names = instantiate_regressors()\n",
        "\n",
        "  len_countries = len(df['country'].unique())\n",
        "\n",
        "  frames , running_time_frames = [], []\n",
        "\n",
        "  # Setup the evaluator\n",
        "  for day in pretrain_days:\n",
        "\n",
        "      df_subset = create_alternate_batch_subset(df,day,batch_size=10)\n",
        "\n",
        "      # Creating a stream from dataframe\n",
        "      stream = DataStream(np.array(df_subset.iloc[:,4:-1]), y=np.array(df_subset.iloc[:,-1])) #TODO: Drop columns with name \n",
        "\n",
        "      pretrain_size = (day//2) * len_countries\n",
        "      max_samples = (day//2 + 20) * len_countries #Testing on set one month ahead only\n",
        "\n",
        "      evaluator = EvaluatePrequential(show_plot=False,\n",
        "                                    pretrain_size=pretrain_size,\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\n",
        "                                    max_samples=max_samples)\n",
        "      # Run evaluation\n",
        "      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
        "\n",
        "      # Dictionary to store each iteration error scores\n",
        "      mdl_evaluation_scores = {}\n",
        "\n",
        "      # Adding Evaluation Measurements and pretraining days\n",
        "      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\n",
        "      mdl_evaluation_scores['PretrainDays'] = [day//2] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "\n",
        "      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\n",
        "\n",
        "      # Errors of each model on a specific pre-train days\n",
        "      frames.append(mdl_evaluation_df)\n",
        "\n",
        "      # Run time for each algorithm\n",
        "      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
        "\n",
        "  # Final Run Time DataFrame\n",
        "  running_time_df = pd.concat(running_time_frames,ignore_index=True)\n",
        "\n",
        "  # Final Evaluation Score Dataframe\n",
        "  evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
        "  return evaluation_scores_df, running_time_df\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef scikit_multiflow_alternate_batch(df, pretrain_days):\\n\\n  model, model_names = instantiate_regressors()\\n\\n  len_countries = len(df['country'].unique())\\n\\n  frames , running_time_frames = [], []\\n\\n  # Setup the evaluator\\n  for day in pretrain_days:\\n\\n      df_subset = create_alternate_batch_subset(df,day,batch_size=10)\\n\\n      # Creating a stream from dataframe\\n      stream = DataStream(np.array(df_subset.iloc[:,4:-1]), y=np.array(df_subset.iloc[:,-1])) #TODO: Drop columns with name \\n\\n      pretrain_size = (day//2) * len_countries\\n      max_samples = (day//2 + 20) * len_countries #Testing on set one month ahead only\\n\\n      evaluator = EvaluatePrequential(show_plot=False,\\n                                    pretrain_size=pretrain_size,\\n                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\\n                                    max_samples=max_samples)\\n      # Run evaluation\\n      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\\n\\n      # Dictionary to store each iteration error scores\\n      mdl_evaluation_scores = {}\\n\\n      # Adding Evaluation Measurements and pretraining days\\n      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\\n      mdl_evaluation_scores['PretrainDays'] = [day//2] * len(mdl_evaluation_scores['EvaluationMeasurement'])\\n\\n      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\\n\\n      # Errors of each model on a specific pre-train days\\n      frames.append(mdl_evaluation_df)\\n\\n      # Run time for each algorithm\\n      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\\n\\n  # Final Run Time DataFrame\\n  running_time_df = pd.concat(running_time_frames,ignore_index=True)\\n\\n  # Final Evaluation Score Dataframe\\n  evaluation_scores_df = pd.concat(frames, ignore_index=True)\\n  return evaluation_scores_df, running_time_df\\n  \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv6KT2eyheiH",
        "outputId": "8e35a1d2-ab99-4b7f-9c4e-047eaad84366"
      },
      "source": [
        "result_skmlflow_alternate_batch, running_time_incremental_alternate_batch = scikit_multiflow_alternate_batch(result, pretrain_days)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 375 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.17s]\n",
            "Processed samples: 376\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 910424067.9708\n",
            "HT_Reg - MAPE          : 1.0540\n",
            "HT_Reg - MAE          : 30173.234297\n",
            "HAT_Reg - MSE          : 1275829323.4549\n",
            "HAT_Reg - MAPE          : 1.2477\n",
            "HAT_Reg - MAE          : 35718.753106\n",
            "ARF_Reg - MSE          : 2378353464.1700\n",
            "ARF_Reg - MAPE          : 1.7035\n",
            "ARF_Reg - MAE          : 48768.365404\n",
            "PA_Reg - MSE          : 307079836609.3058\n",
            "PA_Reg - MAPE          : 19.3564\n",
            "PA_Reg - MAE          : 554147.847248\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 750 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [13.10s]\n",
            "Processed samples: 751\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1072431014199.4456\n",
            "HT_Reg - MAPE          : 48.5788\n",
            "HT_Reg - MAE          : 1035582.451666\n",
            "HAT_Reg - MSE          : 1072430891961.0686\n",
            "HAT_Reg - MAPE          : 48.5788\n",
            "HAT_Reg - MAE          : 1035582.392647\n",
            "ARF_Reg - MSE          : 84049121968.8927\n",
            "ARF_Reg - MAPE          : 13.5997\n",
            "ARF_Reg - MAE          : 289912.265986\n",
            "PA_Reg - MSE          : 232267102451343.0938\n",
            "PA_Reg - MAPE          : 714.9169\n",
            "PA_Reg - MAE          : 15240311.757026\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 1125 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [20.59s]\n",
            "Processed samples: 1126\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 42322325469251.2891\n",
            "HT_Reg - MAPE          : 197.0749\n",
            "HT_Reg - MAE          : 6505561.118708\n",
            "HAT_Reg - MSE          : 42322325393609.4453\n",
            "HAT_Reg - MAPE          : 197.0749\n",
            "HAT_Reg - MAE          : 6505561.112895\n",
            "ARF_Reg - MSE          : 22829047621088.2617\n",
            "ARF_Reg - MAPE          : 144.7406\n",
            "ARF_Reg - MAE          : 4777975.263759\n",
            "PA_Reg - MSE          : 848581836522646.2500\n",
            "PA_Reg - MAPE          : 882.4568\n",
            "PA_Reg - MAE          : 29130428.018185\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 1500 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [29.65s]\n",
            "Processed samples: 1501\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 14675022635.5407\n",
            "HT_Reg - MAPE          : 1.8416\n",
            "HT_Reg - MAE          : 121140.507823\n",
            "HAT_Reg - MSE          : 14675022635.5407\n",
            "HAT_Reg - MAPE          : 1.8416\n",
            "HAT_Reg - MAE          : 121140.507823\n",
            "ARF_Reg - MSE          : 18501818049.0357\n",
            "ARF_Reg - MAPE          : 2.0678\n",
            "ARF_Reg - MAE          : 136021.388204\n",
            "PA_Reg - MSE          : 361585348499.0104\n",
            "PA_Reg - MAPE          : 9.1414\n",
            "PA_Reg - MAE          : 601319.672470\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 1875 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [38.68s]\n",
            "Processed samples: 1876\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3201498650.6280\n",
            "HT_Reg - MAPE          : 1.3541\n",
            "HT_Reg - MAE          : 56581.787270\n",
            "HAT_Reg - MSE          : 3201498650.6280\n",
            "HAT_Reg - MAPE          : 1.3541\n",
            "HAT_Reg - MAE          : 56581.787270\n",
            "ARF_Reg - MSE          : 4814708885.6661\n",
            "ARF_Reg - MAPE          : 1.6605\n",
            "ARF_Reg - MAE          : 69388.103344\n",
            "PA_Reg - MSE          : 139561355856.8973\n",
            "PA_Reg - MAPE          : 8.9402\n",
            "PA_Reg - MAE          : 373579.115927\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 2250 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [46.89s]\n",
            "Processed samples: 2251\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3276201565.8252\n",
            "HT_Reg - MAPE          : 1.3512\n",
            "HT_Reg - MAE          : 57238.112878\n",
            "HAT_Reg - MSE          : 3276201565.8252\n",
            "HAT_Reg - MAPE          : 1.3512\n",
            "HAT_Reg - MAE          : 57238.112878\n",
            "ARF_Reg - MSE          : 10571217760.8724\n",
            "ARF_Reg - MAPE          : 2.4272\n",
            "ARF_Reg - MAE          : 102816.427485\n",
            "PA_Reg - MSE          : 19303308.2842\n",
            "PA_Reg - MAPE          : 0.1037\n",
            "PA_Reg - MAE          : 4393.553036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfluozdk7zod"
      },
      "source": [
        "df_alternate_batch = calc_save_err_metric_combined(error_metrics, result_skmlflow_alternate_batch, max_of_pretrain_days, max_selected_countries, path=exp2_path, static_learner=False, alternate_batch=True, transpose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL_NOS6va8bo",
        "outputId": "7c253b39-c61c-4088-941a-6f4763cd2b06"
      },
      "source": [
        "display_scores(df_alternate_batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE Score\n",
            "     EvaluationMeasurement  PretrainDays      HT_Reg     HAT_Reg     ARF_Reg      PA_Reg\n",
            "0                      MAE        15.000 1762087.482 1672923.625  128648.402  611473.408\n",
            "1                      MAE        30.000 1164499.952 1164499.952 1083883.275 1656716.234\n",
            "2                      MAE        45.000   16673.452   16673.452   13683.411  467890.062\n",
            "3                      MAE        60.000   17526.079   17526.079    9861.444  174974.998\n",
            "4                      MAE        75.000   17731.968   17731.968   14270.845  111131.619\n",
            "5                      MAE        90.000   16295.151   16295.151   12610.185   96585.100\n",
            "mean                   NaN        52.500  499135.681  484275.038  210492.927  519795.237\n",
            "-----------------------------------------------------------------------------------\n",
            "RMSE Score\n",
            "     EvaluationMeasurement  PretrainDays      HT_Reg     HAT_Reg     ARF_Reg      PA_Reg\n",
            "0                     RMSE        15.000 4635819.019 4422191.293  354625.472 1673381.609\n",
            "1                     RMSE        30.000 3225576.592 3225576.592 2979108.614 3987600.105\n",
            "2                     RMSE        45.000   35005.519   35005.519   30999.965 1056687.792\n",
            "3                     RMSE        60.000   35725.755   35725.755   26777.169  436286.706\n",
            "4                     RMSE        75.000   43014.504   43014.504   40325.897  270509.876\n",
            "5                     RMSE        90.000   37216.907   37216.907   29183.663  231309.228\n",
            "mean                   NaN        52.500 1335393.049 1299788.428  576836.797 1275962.553\n",
            "-----------------------------------------------------------------------------------\n",
            "MAPE Score\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                     MAPE        15.000 4238.249 4047.296  400.269 1504.420\n",
            "1                     MAPE        30.000 3105.156 3105.156 2925.655 3962.616\n",
            "2                     MAPE        45.000   21.919   21.919   14.978  260.919\n",
            "3                     MAPE        60.000    8.760    8.760    1.680   51.456\n",
            "4                     MAPE        75.000    4.398    4.398    1.685   31.511\n",
            "5                     MAPE        90.000    1.709    1.709    0.973   10.935\n",
            "mean                   NaN        52.500 1230.032 1198.206  557.540  970.310\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "zNBEQbIeA-7H",
        "outputId": "b328bc8f-f09b-4079-cb81-5d52dc961432"
      },
      "source": [
        "save_runtime(running_time_incremental_alternate_batch, path=exp2_runtime_path, static_learner=False, alternate_batch=True)\n",
        "running_time_incremental_alternate_batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PretrainDays</th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.198</td>\n",
              "      <td>5.873</td>\n",
              "      <td>0.002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.546</td>\n",
              "      <td>12.257</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>90</td>\n",
              "      <td>0.686</td>\n",
              "      <td>2.160</td>\n",
              "      <td>17.760</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>120</td>\n",
              "      <td>1.042</td>\n",
              "      <td>3.553</td>\n",
              "      <td>25.074</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>150</td>\n",
              "      <td>1.677</td>\n",
              "      <td>3.929</td>\n",
              "      <td>33.092</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>180</td>\n",
              "      <td>2.183</td>\n",
              "      <td>5.229</td>\n",
              "      <td>39.505</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
              "0            30   0.119    0.198    5.873   0.002\n",
              "1            60   0.318    0.546   12.257   0.003\n",
              "2            90   0.686    2.160   17.760   0.003\n",
              "3           120   1.042    3.553   25.074   0.003\n",
              "4           150   1.677    3.929   33.092   0.003\n",
              "5           180   2.183    5.229   39.505   0.003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "1RuBt5n7Vqzf",
        "outputId": "9c405439-0248-4d1b-820e-fb93b7864395"
      },
      "source": [
        "summary_table_incremental_alternate = get_summary_table(df_alternate_batch, running_time_incremental_alternate_batch, error_metrics, static_learner=False)\n",
        "save_summary_table(summary_table_incremental_alternate, exp2_summary_path,static_learner=False,alternate_batch=True, transpose=True)\n",
        "summary_table_incremental_alternate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Metric</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MAE</th>\n",
              "      <td>499135.681</td>\n",
              "      <td>484275.038</td>\n",
              "      <td>210492.927</td>\n",
              "      <td>519795.237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MAPE</th>\n",
              "      <td>1230.032</td>\n",
              "      <td>1198.206</td>\n",
              "      <td>557.540</td>\n",
              "      <td>970.310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RMSE</th>\n",
              "      <td>1335393.049</td>\n",
              "      <td>1299788.428</td>\n",
              "      <td>576836.797</td>\n",
              "      <td>1275962.553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time(sec)</th>\n",
              "      <td>1.004</td>\n",
              "      <td>2.602</td>\n",
              "      <td>22.260</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               HT_Reg     HAT_Reg    ARF_Reg      PA_Reg\n",
              "Metric                                                  \n",
              "MAE        499135.681  484275.038 210492.927  519795.237\n",
              "MAPE         1230.032    1198.206    557.540     970.310\n",
              "RMSE      1335393.049 1299788.428 576836.797 1275962.553\n",
              "Time(sec)       1.004       2.602     22.260       0.003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQYy8fGQ_EJ5"
      },
      "source": [
        "## Significance tests for Experiment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "3yH-IbaPBk2D",
        "outputId": "27ecc410-88d2-42d9-85a9-9a0e715707c6"
      },
      "source": [
        "## EXP2\n",
        "# Significance results for Experiment 2\n",
        "err_metric_for_significance = 'MAPE'\n",
        "significance_thresh = 0.05\n",
        "plot_pop = False\n",
        "\n",
        "# Concatenating a population of all results (as in boxplot) for experiment 2\n",
        "# This is done for runs per batch for experiment 2. But for experiment 1 it's done for runs per country (their final averages, like the result sent to the boxplots).\n",
        "static = df_sklearn[df_sklearn['EvaluationMeasurement'] == err_metric_for_significance].drop(columns=['EvaluationMeasurement'], axis=1).transpose()\n",
        "incremental = df_alternate_batch[df_alternate_batch['EvaluationMeasurement'] == err_metric_for_significance].drop(columns=['EvaluationMeasurement', 'PretrainDays'], axis=1).transpose()\n",
        "concated_df = pd.concat([static, incremental]).transpose()\n",
        "concated_df.set_index('PretrainDays', inplace=True, drop=True)\n",
        "concated_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RandomForest</th>\n",
              "      <th>GradientBoosting</th>\n",
              "      <th>LinearSVR</th>\n",
              "      <th>DecisionTree</th>\n",
              "      <th>BayesianRidge</th>\n",
              "      <th>LSTM</th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PretrainDays</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30.000</th>\n",
              "      <td>65.045</td>\n",
              "      <td>151.906</td>\n",
              "      <td>588.491</td>\n",
              "      <td>38.443</td>\n",
              "      <td>12.928</td>\n",
              "      <td>1.501</td>\n",
              "      <td>4238.249</td>\n",
              "      <td>4047.296</td>\n",
              "      <td>400.269</td>\n",
              "      <td>1504.420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60.000</th>\n",
              "      <td>10.649</td>\n",
              "      <td>6.256</td>\n",
              "      <td>17.627</td>\n",
              "      <td>3.444</td>\n",
              "      <td>5.582</td>\n",
              "      <td>1.434</td>\n",
              "      <td>3105.156</td>\n",
              "      <td>3105.156</td>\n",
              "      <td>2925.655</td>\n",
              "      <td>3962.616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90.000</th>\n",
              "      <td>4.535</td>\n",
              "      <td>1.610</td>\n",
              "      <td>5.625</td>\n",
              "      <td>1.624</td>\n",
              "      <td>3.862</td>\n",
              "      <td>1.325</td>\n",
              "      <td>21.919</td>\n",
              "      <td>21.919</td>\n",
              "      <td>14.978</td>\n",
              "      <td>260.919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120.000</th>\n",
              "      <td>1.903</td>\n",
              "      <td>0.791</td>\n",
              "      <td>2.266</td>\n",
              "      <td>0.830</td>\n",
              "      <td>1.766</td>\n",
              "      <td>0.750</td>\n",
              "      <td>8.760</td>\n",
              "      <td>8.760</td>\n",
              "      <td>1.680</td>\n",
              "      <td>51.456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150.000</th>\n",
              "      <td>1.123</td>\n",
              "      <td>0.578</td>\n",
              "      <td>1.760</td>\n",
              "      <td>0.787</td>\n",
              "      <td>1.355</td>\n",
              "      <td>0.783</td>\n",
              "      <td>4.398</td>\n",
              "      <td>4.398</td>\n",
              "      <td>1.685</td>\n",
              "      <td>31.511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180.000</th>\n",
              "      <td>0.636</td>\n",
              "      <td>0.519</td>\n",
              "      <td>1.517</td>\n",
              "      <td>0.655</td>\n",
              "      <td>0.693</td>\n",
              "      <td>0.636</td>\n",
              "      <td>1.709</td>\n",
              "      <td>1.709</td>\n",
              "      <td>0.973</td>\n",
              "      <td>10.935</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              RandomForest  GradientBoosting  ...  ARF_Reg   PA_Reg\n",
              "PretrainDays                                  ...                  \n",
              "30.000              65.045           151.906  ...  400.269 1504.420\n",
              "60.000              10.649             6.256  ... 2925.655 3962.616\n",
              "90.000               4.535             1.610  ...   14.978  260.919\n",
              "120.000              1.903             0.791  ...    1.680   51.456\n",
              "150.000              1.123             0.578  ...    1.685   31.511\n",
              "180.000              0.636             0.519  ...    0.973   10.935\n",
              "\n",
              "[6 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kbQZ10ONBwJZ",
        "outputId": "c8d1dbe9-1aac-4f23-c6fe-180e708989ac"
      },
      "source": [
        "# Selecting the best algorithm for statistical comparisons\n",
        "# We want to know if the best is statistically significantly better compared to the rest.\n",
        "best_algo = concated_df.mean().sort_values(ascending=True).index[0]\n",
        "best_algo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'LSTM'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i0TvT9BNHwS",
        "outputId": "dafb6efa-d061-4312-f216-d599e382896b"
      },
      "source": [
        "print('AVG results across countries')\n",
        "concated_df.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AVG results across countries\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForest         13.982\n",
              "GradientBoosting     26.943\n",
              "LinearSVR           102.881\n",
              "DecisionTree          7.631\n",
              "BayesianRidge         4.365\n",
              "LSTM                  1.071\n",
              "HT_Reg             1230.032\n",
              "HAT_Reg            1198.206\n",
              "ARF_Reg             557.540\n",
              "PA_Reg              970.310\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v6za-agNIrF",
        "outputId": "c7374304-0eed-42f7-8b34-674891d9c98d"
      },
      "source": [
        "print('STEDEV across countries')\n",
        "concated_df.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STEDEV across countries\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForest         25.287\n",
              "GradientBoosting     61.258\n",
              "LinearSVR           237.978\n",
              "DecisionTree         15.131\n",
              "BayesianRidge         4.569\n",
              "LSTM                  0.389\n",
              "HT_Reg             1924.966\n",
              "HAT_Reg            1865.958\n",
              "ARF_Reg            1170.880\n",
              "PA_Reg             1574.068\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG_TSWRXBwtM",
        "outputId": "4b6c3888-aa99-408f-ee8e-626052aef3be"
      },
      "source": [
        "# Iterate through all the other algorithms to see if the difference in results is significant\n",
        "competitors = list(concated_df.columns)\n",
        "competitors.remove(best_algo)\n",
        "for significance_thresh in [0.01,0.05]:\n",
        "  print(f'Running significane at: {significance_thresh}')\n",
        "  for competitor in competitors:\n",
        "    # print(competitor)\n",
        "    pval, significant = check_significance(concated_df[best_algo], concated_df[competitor], significance_at=significance_thresh)\n",
        "    print(f'Comparison of {best_algo} to {competitor} pvalue: {pval}   /  significant?: {significant}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running significane at: 0.01\n",
            "Population too small.\n",
            "Comparison of LSTM to RandomForest pvalue: 0.046399461870904594   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to GradientBoosting pvalue: 0.24886387493792206   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to LinearSVR pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to DecisionTree pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to BayesianRidge pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to HT_Reg pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to HAT_Reg pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to ARF_Reg pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to PA_Reg pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Running significane at: 0.05\n",
            "Population too small.\n",
            "Comparison of LSTM to RandomForest pvalue: 0.046399461870904594   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to GradientBoosting pvalue: 0.24886387493792206   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to LinearSVR pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to DecisionTree pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to BayesianRidge pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to HT_Reg pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to HAT_Reg pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to ARF_Reg pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to PA_Reg pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
            "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsx2eNV0WqxC",
        "outputId": "eb732304-25c9-46da-ddde-17792c6bd051"
      },
      "source": [
        "# Iterate through all the other algorithms to see if the difference in results is significant\n",
        "best_algo2 = concated_df.mean().sort_values(ascending=True).index[1]\n",
        "competitors = list(concated_df.columns)\n",
        "competitors.remove(best_algo2)\n",
        "for significance_thresh in [0.01,0.05]:\n",
        "  print(f'Running significance at: {significance_thresh}')\n",
        "  for competitor in competitors:\n",
        "    # print(competitor)\n",
        "    pval, significant = check_significance(concated_df[best_algo2], concated_df[competitor], significance_at=significance_thresh)\n",
        "    print(f'Comparison of {best_algo2} to {competitor} pvalue: {pval}   /  significant?: {significant}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running significance at: 0.01\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to RandomForest pvalue: 0.17295491798842066   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to GradientBoosting pvalue: 0.6001794871405538   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to LinearSVR pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to DecisionTree pvalue: 0.3454475304692257   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to LSTM pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to HT_Reg pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to HAT_Reg pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to ARF_Reg pvalue: 0.046399461870904594   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to PA_Reg pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Running significance at: 0.05\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to RandomForest pvalue: 0.17295491798842066   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to GradientBoosting pvalue: 0.6001794871405538   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to LinearSVR pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to DecisionTree pvalue: 0.3454475304692257   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to LSTM pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to HT_Reg pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to HAT_Reg pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to ARF_Reg pvalue: 0.046399461870904594   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to PA_Reg pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
            "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPwMeUDwn5Tk"
      },
      "source": [
        "# Download Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-fVUOfpj78ES",
        "outputId": "e04ef632-e706-45d8-cb96-8fe98878ae95"
      },
      "source": [
        "\n",
        "!zip -r /content/Result.zip /content/Result\n",
        "from google.colab import files\n",
        "files.download(\"/content/Result.zip\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/Result/ (stored 0%)\n",
            "  adding: content/Result/exp2/ (stored 0%)\n",
            "  adding: content/Result/exp2/combined25country_MAPE_incremental_alternate_batch.csv (deflated 42%)\n",
            "  adding: content/Result/exp2/combined25country_MAPE_static.tex (deflated 55%)\n",
            "  adding: content/Result/exp2/summary/ (stored 0%)\n",
            "  adding: content/Result/exp2/summary/combined25country_summary_table_incremental_alternate_batch.tex (deflated 40%)\n",
            "  adding: content/Result/exp2/summary/combined25country_summary_table_incremental_alternate_batch.csv (deflated 28%)\n",
            "  adding: content/Result/exp2/summary/combined25country_summary_table_incremental.csv (deflated 26%)\n",
            "  adding: content/Result/exp2/summary/combined25country_summary_table_static.csv (deflated 25%)\n",
            "  adding: content/Result/exp2/summary/combined25country_summary_table_static.tex (deflated 41%)\n",
            "  adding: content/Result/exp2/summary/combined25country_summary_table_incremental.tex (deflated 39%)\n",
            "  adding: content/Result/exp2/combined25country_MAE_incremental.csv (deflated 41%)\n",
            "  adding: content/Result/exp2/combined25country_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp2/combined25country_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp2/combined25country_MAPE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp2/combined25country_MAPE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp2/combined25country_RMSE_incremental_alternate_batch.tex (deflated 59%)\n",
            "  adding: content/Result/exp2/combined25country_RMSE_incremental.csv (deflated 43%)\n",
            "  adding: content/Result/exp2/combined25country_MAPE_incremental_alternate_batch.tex (deflated 58%)\n",
            "  adding: content/Result/exp2/combined25country_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp2/combined25country_MAE_incremental_alternate_batch.tex (deflated 58%)\n",
            "  adding: content/Result/exp2/combined25country_MAE_incremental_alternate_batch.csv (deflated 42%)\n",
            "  adding: content/Result/exp2/runtime/ (stored 0%)\n",
            "  adding: content/Result/exp2/runtime/combined25country_runtime_incremental_alternate_batch.tex (deflated 54%)\n",
            "  adding: content/Result/exp2/runtime/combined25country_runtime_incremental_alternate_batch.csv (deflated 33%)\n",
            "  adding: content/Result/exp2/runtime/combined25country_runtime_incremental.tex (deflated 52%)\n",
            "  adding: content/Result/exp2/runtime/combined25country_runtime_static.csv (deflated 34%)\n",
            "  adding: content/Result/exp2/runtime/combined25country_runtime_incremental.csv (deflated 33%)\n",
            "  adding: content/Result/exp2/runtime/combined25country_runtime_static.tex (deflated 62%)\n",
            "  adding: content/Result/exp2/combined25country_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp2/combined25country_RMSE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp2/combined25country_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp2/combined25country_MAE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp2/combined25country_RMSE_incremental_alternate_batch.csv (deflated 44%)\n",
            "  adding: content/Result/exp1/ (stored 0%)\n",
            "  adding: content/Result/exp1/France_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Switzerland_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Israel_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Indonesia_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_RMSE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Israel_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Italy_MAPE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Czechia_MAPE_incremental.csv (deflated 46%)\n",
            "  adding: content/Result/exp1/Pakistan_RMSE_incremental.tex (deflated 55%)\n",
            "  adding: content/Result/exp1/Mexico_MAE_static.tex (deflated 54%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Israel_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Italy_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Spain_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Brazil_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Israel_MAE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Canada_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Belgium_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Spain_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Ecuador_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Russia_MAE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Israel_RMSE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Germany_MAPE_incremental.csv (deflated 43%)\n",
            "  adding: content/Result/exp1/Iran_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Germany_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Spain_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Belgium_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Ecuador_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Brazil_MAPE_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/Philippines_RMSE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Spain_MAE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Mexico_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/India_MAPE_incremental.csv (deflated 43%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/United_Kingdom_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/Nepal_RMSE_incremental.csv (deflated 43%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAPE_incremental.tex (deflated 60%)\n",
            "  adding: content/Result/exp1/Germany_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Nepal_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Iraq_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/Indonesia_MAE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Switzerland_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Italy_MAE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/France_MAPE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/Germany_MAPE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Belgium_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Germany_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Switzerland_MAPE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Pakistan_MAE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Netherlands_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Ecuador_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAPE_incremental.tex (deflated 60%)\n",
            "  adding: content/Result/exp1/Canada_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Canada_MAPE_static.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Belgium_MAPE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Iraq_RMSE_incremental.csv (deflated 43%)\n",
            "  adding: content/Result/exp1/United_States_of_America_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Italy_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Nepal_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Brazil_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Spain_MAPE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Indonesia_MAPE_static.csv (deflated 43%)\n",
            "  adding: content/Result/exp1/Canada_MAE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/France_MAPE_static.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Russia_MAPE_incremental.csv (deflated 45%)\n",
            "  adding: content/Result/exp1/Israel_MAPE_incremental.tex (deflated 60%)\n",
            "  adding: content/Result/exp1/Nepal_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Romania_MAPE_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Nepal_MAPE_incremental.csv (deflated 45%)\n",
            "  adding: content/Result/exp1/Germany_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Belgium_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Pakistan_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Belgium_MAE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Mexico_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Indonesia_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Romania_MAPE_incremental.tex (deflated 61%)\n",
            "  adding: content/Result/exp1/Indonesia_RMSE_static.tex (deflated 53%)\n",
            "  adding: content/Result/exp1/Czechia_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Russia_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Switzerland_MAE_incremental.tex (deflated 55%)\n",
            "  adding: content/Result/exp1/Switzerland_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Mexico_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Czechia_MAE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Romania_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Russia_MAPE_static.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Germany_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/India_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Iran_MAPE_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAE_static.tex (deflated 53%)\n",
            "  adding: content/Result/exp1/Mexico_MAPE_incremental.tex (deflated 61%)\n",
            "  adding: content/Result/exp1/Czechia_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Pakistan_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Canada_MAPE_incremental.tex (deflated 60%)\n",
            "  adding: content/Result/exp1/Mexico_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Romania_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Indonesia_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Iran_MAPE_incremental.tex (deflated 61%)\n",
            "  adding: content/Result/exp1/Russia_MAE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Iraq_MAPE_incremental.csv (deflated 45%)\n",
            "  adding: content/Result/exp1/Ecuador_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Romania_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/Ecuador_MAPE_static.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/summary/ (stored 0%)\n",
            "  adding: content/Result/exp1/summary/top_countries_MAPE_summary_table_incremental.csv (deflated 49%)\n",
            "  adding: content/Result/exp1/summary/summary_table_combined_mean_incremental.tex (deflated 39%)\n",
            "  adding: content/Result/exp1/summary/summary_table_combined_mean_static.tex (deflated 42%)\n",
            "  adding: content/Result/exp1/summary/top_countries_MAPE_summary_table_static.csv (deflated 49%)\n",
            "  adding: content/Result/exp1/summary/top_countries_MAPE_summary_table_static.tex (deflated 62%)\n",
            "  adding: content/Result/exp1/summary/summary_table_combined_mean_static.csv (deflated 25%)\n",
            "  adding: content/Result/exp1/summary/top_countries_MAPE_summary_table_incremental.tex (deflated 62%)\n",
            "  adding: content/Result/exp1/summary/summary_table_combined_mean_incremental.csv (deflated 25%)\n",
            "  adding: content/Result/exp1/Nepal_MAE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/United_Kingdom_RMSE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Iraq_MAPE_static.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Iran_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Netherlands_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/France_RMSE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Brazil_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Ecuador_MAPE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Canada_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Pakistan_MAPE_incremental.tex (deflated 60%)\n",
            "  adding: content/Result/exp1/Pakistan_MAPE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Indonesia_MAE_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Israel_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Mexico_MAPE_incremental.csv (deflated 44%)\n",
            "  adding: content/Result/exp1/Spain_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Romania_MAE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Mexico_RMSE_static.tex (deflated 53%)\n",
            "  adding: content/Result/exp1/Germany_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Nepal_MAPE_static.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Russia_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Russia_MAE_static.tex (deflated 53%)\n",
            "  adding: content/Result/exp1/India_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Philippines_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Philippines_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Germany_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Belgium_MAPE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/Pakistan_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Netherlands_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Netherlands_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Brazil_MAPE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/Brazil_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Ecuador_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/Mexico_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Philippines_RMSE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Italy_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Brazil_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Nepal_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/Netherlands_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Russia_RMSE_incremental.csv (deflated 44%)\n",
            "  adding: content/Result/exp1/Belgium_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Indonesia_MAPE_incremental.tex (deflated 62%)\n",
            "  adding: content/Result/exp1/Italy_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Iraq_MAPE_incremental.tex (deflated 62%)\n",
            "  adding: content/Result/exp1/India_RMSE_static.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Israel_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/Philippines_MAPE_static.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Russia_MAPE_incremental.tex (deflated 61%)\n",
            "  adding: content/Result/exp1/Romania_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Pakistan_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Canada_MAPE_incremental.csv (deflated 44%)\n",
            "  adding: content/Result/exp1/Netherlands_MAE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Romania_MAPE_incremental.csv (deflated 44%)\n",
            "  adding: content/Result/exp1/Spain_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Russia_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Netherlands_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Ecuador_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Brazil_MAE_incremental.tex (deflated 55%)\n",
            "  adding: content/Result/exp1/Spain_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Germany_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Romania_RMSE_incremental.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Pakistan_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/Israel_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Switzerland_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/France_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Pakistan_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/France_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Switzerland_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/France_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Netherlands_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Canada_MAE_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/France_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Czechia_MAPE_static.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Czechia_MAE_incremental.tex (deflated 55%)\n",
            "  adding: content/Result/exp1/Czechia_RMSE_incremental.tex (deflated 55%)\n",
            "  adding: content/Result/exp1/United_States_of_America_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Ecuador_MAE_incremental.tex (deflated 55%)\n",
            "  adding: content/Result/exp1/Italy_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Russia_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Philippines_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/India_MAPE_static.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Indonesia_MAPE_incremental.csv (deflated 46%)\n",
            "  adding: content/Result/exp1/Canada_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAPE_incremental.csv (deflated 44%)\n",
            "  adding: content/Result/exp1/Spain_RMSE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Brazil_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Ecuador_MAPE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Spain_MAPE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/France_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Nepal_MAPE_incremental.tex (deflated 61%)\n",
            "  adding: content/Result/exp1/Philippines_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Mexico_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Iran_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Mexico_MAPE_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/Iraq_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Brazil_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Czechia_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Philippines_MAE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Italy_MAPE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/India_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Iran_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Germany_MAPE_static.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Israel_MAPE_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/Czechia_MAPE_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/Czechia_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Romania_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Philippines_MAPE_incremental.csv (deflated 44%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAPE_static.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Romania_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Philippines_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Belgium_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Czechia_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/India_MAPE_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/United_States_of_America_RMSE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/France_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Iran_MAE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAPE_static.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAPE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/United_Kingdom_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/runtime/ (stored 0%)\n",
            "  adding: content/Result/exp1/runtime/Netherlands_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Iraq_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Switzerland_runtime_static.tex (deflated 66%)\n",
            "  adding: content/Result/exp1/runtime/Romania_runtime_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/runtime/Nepal_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Ecuador_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/India_runtime_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/runtime/India_runtime_static.csv (deflated 43%)\n",
            "  adding: content/Result/exp1/runtime/Mexico_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/United_States_of_America_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Canada_runtime_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/runtime/Romania_runtime_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/runtime/Belgium_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Indonesia_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Iraq_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Pakistan_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Philippines_runtime_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/runtime/Indonesia_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/United_States_of_America_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Nepal_runtime_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/runtime/Belgium_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Spain_runtime_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/runtime/Netherlands_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/India_runtime_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/runtime/Italy_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Germany_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/India_runtime_static.tex (deflated 66%)\n",
            "  adding: content/Result/exp1/runtime/Romania_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Israel_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/United_Kingdom_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Israel_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Nepal_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/United_Kingdom_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Germany_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/United_Arab_Emirates_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Iran_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/France_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Canada_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Nepal_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Italy_runtime_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/runtime/Netherlands_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Czechia_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Pakistan_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Iran_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Iraq_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Iran_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Spain_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Ecuador_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Ecuador_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Germany_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/United_Kingdom_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Pakistan_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Romania_runtime_static.tex (deflated 66%)\n",
            "  adding: content/Result/exp1/runtime/United_States_of_America_runtime_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/runtime/Italy_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Czechia_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/France_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Switzerland_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Brazil_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Brazil_runtime_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/runtime/Iran_runtime_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/runtime/Spain_runtime_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/runtime/Switzerland_runtime_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/runtime/Russia_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/United_States_of_America_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Belgium_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Ecuador_runtime_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/runtime/United_Arab_Emirates_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Israel_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Germany_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/United_Arab_Emirates_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Mexico_runtime_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/runtime/France_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Russia_runtime_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/runtime/Mexico_runtime_static.tex (deflated 66%)\n",
            "  adding: content/Result/exp1/runtime/Russia_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Canada_runtime_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/runtime/Israel_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Czechia_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Iraq_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Belgium_runtime_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/runtime/France_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Pakistan_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Philippines_runtime_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/runtime/Czechia_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Switzerland_runtime_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/runtime/United_Kingdom_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Indonesia_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Netherlands_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Brazil_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/United_Arab_Emirates_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Russia_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Italy_runtime_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/runtime/Brazil_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Spain_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Mexico_runtime_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/runtime/Philippines_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Philippines_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Canada_runtime_static.tex (deflated 64%)\n",
            "  adding: content/Result/exp1/runtime/Indonesia_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Switzerland_MAPE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Romania_MAPE_static.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Ecuador_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Indonesia_MAPE_static.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/Switzerland_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Canada_MAPE_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/India_MAPE_incremental.tex (deflated 60%)\n",
            "  adding: content/Result/exp1/Iraq_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Iran_RMSE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Brazil_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Indonesia_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Iran_MAPE_incremental.csv (deflated 45%)\n",
            "  adding: content/Result/exp1/Iraq_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAPE_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/Belgium_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Belgium_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Spain_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Netherlands_MAPE_static.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Israel_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAPE_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Belgium_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Italy_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Philippines_MAPE_incremental.tex (deflated 61%)\n",
            "  adding: content/Result/exp1/Canada_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/United_Kingdom_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/India_RMSE_incremental.tex (deflated 55%)\n",
            "  adding: content/Result/exp1/Ecuador_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Iran_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Switzerland_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Iraq_MAE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/India_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAPE_incremental.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/Iraq_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/France_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Iran_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Nepal_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Italy_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Netherlands_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Mexico_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Philippines_MAPE_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/United_States_of_America_RMSE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/Pakistan_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Indonesia_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Germany_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Iraq_MAPE_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/Switzerland_MAPE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Switzerland_MAE_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Czechia_MAPE_incremental.tex (deflated 61%)\n",
            "  adding: content/Result/exp1/Russia_RMSE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Netherlands_MAPE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/Nepal_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/France_RMSE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Pakistan_RMSE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Iraq_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Italy_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Netherlands_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Brazil_RMSE_static.tex (deflated 53%)\n",
            "  adding: content/Result/exp1/Iran_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Canada_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Spain_MAPE_static.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Israel_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Italy_MAPE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/India_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Nepal_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/India_MAE_incremental.tex (deflated 55%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_08c75791-6148-4580-bade-5ddd1b9dfdfa\", \"Result.zip\", 233109)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X-m0P0s0hKNA",
        "outputId": "c4d6d7c8-7e99-4f73-ea3e-137ae2bcd0bf"
      },
      "source": [
        "\"\"\"\n",
        "!zip -r /content/csv_files.zip /content/csv_files\n",
        "from google.colab import files\n",
        "files.download(\"/content/csv_files.zip\")\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/csv_files/ (stored 0%)\n",
            "  adding: content/csv_files/processed/ (stored 0%)\n",
            "  adding: content/csv_files/processed/Monaco.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/Denmark.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Indonesia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Malaysia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Japan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/United_States_of_America.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/Finland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Australia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Taiwan.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/Vietnam.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/Ireland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Iraq.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Lithuania.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Sri_Lanka.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Italy.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Afghanistan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Mexico.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Pakistan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Oman.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Azerbaijan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Germany.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Philippines.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Thailand.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/Iran.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Israel.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Brazil.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Russia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Luxembourg.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Canada.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Czechia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Singapore.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Qatar.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Belarus.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/San_Marino.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/South_Korea.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/United_Arab_Emirates.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Spain.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Lebanon.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Switzerland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Belgium.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Bahrain.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Georgia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/United_Kingdom.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Ecuador.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Norway.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Dominican_Republic.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Netherlands.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Estonia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Algeria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/China.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Sweden.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/New_Zealand.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/Egypt.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Cambodia.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/Greece.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/India.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Romania.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Croatia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Armenia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/North_Macedonia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Iceland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Nepal.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Austria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/France.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Nigeria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Kuwait.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/ (stored 0%)\n",
            "  adding: content/csv_files/processed_null/Monaco.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Denmark.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Indonesia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Mali.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Malaysia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Japan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Uzbekistan.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/United_States_of_America.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Uganda.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Finland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Namibia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Australia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Equatorial_Guinea.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Aruba.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Kazakhstan.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Chile.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Taiwan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Tajikistan.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Jordan.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Vietnam.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Ukraine.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Ireland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Antigua_and_Barbuda.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Iraq.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Tunisia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Northern_Mariana_Islands.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Jamaica.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Malta.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Sudan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Lithuania.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Sri_Lanka.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Italy.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Benin.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Cyprus.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Turkey.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Trinidad_and_Tobago.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Afghanistan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Mexico.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Montserrat.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Falkland_Islands_(Malvinas).csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Pakistan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Syria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Zimbabwe.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Oman.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Jersey.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Bhutan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Azerbaijan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/French_Polynesia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/South_Africa.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Liechtenstein.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Somalia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Andorra.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Ethiopia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Germany.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/South_Sudan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Philippines.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Bahamas.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Rwanda.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Thailand.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Djibouti.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Laos.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Bolivia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Cote_dIvoire.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Burkina_Faso.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Iran.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Greenland.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Eritrea.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Israel.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Brazil.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Russia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Faroe_Islands.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Luxembourg.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Senegal.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Saint_Lucia.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Chad.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Holy_See.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Bermuda.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Serbia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Poland.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Angola.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Slovenia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Canada.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Slovakia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Czechia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Singapore.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Eswatini.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Marshall_Islands.csv (deflated 80%)\n",
            "  adding: content/csv_files/processed_null/Honduras.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Haiti.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Qatar.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Myanmar.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Barbados.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Latvia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Guinea.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Saudi_Arabia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Timor_Leste.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Montenegro.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Belarus.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Isle_of_Man.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Mauritania.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Fiji.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/San_Marino.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/South_Korea.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Guernsey.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/United_States_Virgin_Islands.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Saint_Kitts_and_Nevis.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/United_Arab_Emirates.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Spain.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Lebanon.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Bonaire, Saint Eustatius and Saba.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Seychelles.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Wallis_and_Futuna.csv (deflated 86%)\n",
            "  adding: content/csv_files/processed_null/Cuba.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Saint_Vincent_and_the_Grenadines.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Guyana.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Switzerland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Paraguay.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Belgium.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Mongolia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Bahrain.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Guinea_Bissau.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Dominica.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Georgia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Morocco.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Kosovo.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Moldova.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Western_Sahara.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Mauritius.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Niger.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Turks_and_Caicos_islands.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Kyrgyzstan.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Bangladesh.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/United_Kingdom.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Gabon.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Ecuador.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Sierra_Leone.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Norway.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Costa_Rica.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Dominican_Republic.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Netherlands.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Botswana.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/New_Caledonia.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Estonia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Portugal.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Algeria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/China.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Colombia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Peru.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Grenada.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Argentina.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Anguilla.csv (deflated 96%)\n",
            "  adding: content/csv_files/processed_null/Sweden.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Congo.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Bosnia_and_Herzegovina.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Cases_on_an_international_conveyance_Japan.csv (deflated 92%)\n",
            "  adding: content/csv_files/processed_null/Zambia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Cayman_Islands.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/New_Zealand.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Egypt.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Guatemala.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Nicaragua.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Ghana.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Mozambique.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Kenya.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Hungary.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/United_Republic_of_Tanzania.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Cambodia.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Greece.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Comoros.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/India.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Gibraltar.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Romania.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Togo.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Croatia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Democratic_Republic_of_the_Congo.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Maldives.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Armenia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Gambia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Papua_New_Guinea.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Guam.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Belize.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Uruguay.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/North_Macedonia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Curaçao.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Cameroon.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Cape_Verde.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Sint_Maarten.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Panama.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Iceland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Nepal.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Venezuela.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Austria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Bulgaria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/France.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Burundi.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Suriname.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/El_Salvador.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Madagascar.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/British_Virgin_Islands.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Solomon_Islands.csv (deflated 86%)\n",
            "  adding: content/csv_files/processed_null/Liberia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Nigeria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Sao_Tome_and_Principe.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Brunei_Darussalam.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Lesotho.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Albania.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Palestine.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Malawi.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Kuwait.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Libya.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Central_African_Republic.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Yemen.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Puerto_Rico.csv (deflated 93%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c68c5803-b1e9-4972-a4bd-e535276b249f\", \"csv_files.zip\", 1053581)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDICI28viZYg"
      },
      "source": [
        "# Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DR7Z02IGVCk"
      },
      "source": [
        "## Bar Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkebu9hm0GMy"
      },
      "source": [
        "\"\"\"\n",
        "def get_filenames(pattern):\n",
        "  filenames=[]\n",
        "  for name in glob.glob(f\"{exp1_path}/*{pattern}*.csv\"):\n",
        "    filenames.append(name)\n",
        "  return filenames\n",
        "\n",
        "def get_countryname(filename):\n",
        "  country_name = filename.split('/')[-1].split('.')[0].split('_')[:-2]\n",
        "  return '_'.join(country_name)\n",
        "\n",
        "def get_error_stat_name(pattern):\n",
        "  return pattern.split('_')\n",
        "\n",
        "def plot_graph(df, statistics, metric):\n",
        "  sns.set_theme(style=\"darkgrid\")\n",
        "  df.plot(kind='bar',figsize=(12,10))\n",
        "  plt.title(f'{statistics}')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yscale('log')\n",
        "  plt.ylabel(metric)\n",
        "  ax = plt.gca()\n",
        "  ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))  # Changing scientific notation to plain text\n",
        "  plt.show()\n",
        "\n",
        "def  get_list_of_error_metric_per_country(filenames):\n",
        "  metric_dataframes_list = []\n",
        "  for filename in filenames:\n",
        "    country_name = get_countryname(filename)\n",
        "    df = pd.read_csv(filename)\n",
        "    df = df.set_index('Unnamed: 0')\n",
        "    df['country'] = country_name\n",
        "    metric_dataframes_list.append(df)\n",
        "  return metric_dataframes_list\n",
        "\n",
        "\n",
        "def get_mean_error_dataframes(metric_dataframes_list, learner_type):\n",
        "  mean_error_dataframes=[]\n",
        "  start_row = 'mean'\n",
        "  if learner_type == 'static':\n",
        "    start_col = 'RandomForest'\n",
        "  elif learner_type=='incremental':\n",
        "    start_col='HT_Reg'\n",
        "\n",
        "  for i in range(len(metric_dataframes_list)):\n",
        "    row = pd.DataFrame([metric_dataframes_list[i].loc[start_row][start_col:]])\n",
        "    mean_error_dataframes.append(row) # Passing values as a list for grouped bar chart\n",
        "    \n",
        "  result_df = pd.concat(mean_error_dataframes, ignore_index=True)\n",
        "  result_df.set_index('country', inplace=True)\n",
        "\n",
        "  return result_df\n",
        "\n",
        "filename_patterns = ['MAE_static','MAPE_static','MAE_incremental','MAPE_incremental']\n",
        "error_metric_mapper = {\n",
        "    'MAE': 'Mean Absolute Error(MAE)', \n",
        "    'MAPE':'Mean Absolute Percentage Error(MAPE)',\n",
        "    'RMSE':'Root Mean Square Error(RMSE)'}\n",
        "\n",
        "for pattern in filename_patterns:\n",
        "  filenames = get_filenames(pattern)\n",
        "  stat,learner_type = get_error_stat_name(pattern)\n",
        "  metric_dataframes_list = get_list_of_error_metric_per_country(filenames)\n",
        "  mean_error_dataframes = get_mean_error_dataframes(metric_dataframes_list,learner_type)\n",
        "  plot_graph(mean_error_dataframes,pattern,error_metric_mapper[stat])\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otlR4XEz2Kmf"
      },
      "source": [
        "\"\"\"!zip -r /content/summary_1.zip /content/Result/exp1/summary\n",
        "!zip -r /content/summary_2.zip /content/Result/exp2/summary\n",
        "from google.colab import files\n",
        "files.download(\"/content/summary_1.zip\")\n",
        "files.download(\"/content/summary_2.zip\")\"\"\"\n",
        "\n",
        "\"\"\"!zip -r /content/Result.zip /content/Result\n",
        "from google.colab import files\n",
        "files.download(\"/content/Result.zip\")\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmQXDjmrXAiS"
      },
      "source": [
        "!zip -r /content/csv_files.zip /content/csv_files\n",
        "from google.colab import files\n",
        "files.download(\"/content/csv_files.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQRkdX0oGaec"
      },
      "source": [
        "## Box Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oL2TxDdHRwM"
      },
      "source": [
        "def preprocess_data(df, metric_type, learner_type, col_mapper):\n",
        "  # Only pretrain days records are required not the mean row\n",
        "  df.drop(['mean'], axis=1, inplace=True)\n",
        "\n",
        "  # Renaming the Algorithm Columns\n",
        "  df.rename(columns={'Unnamed: 0': 'Algorithms'}, inplace=True)  \n",
        "\n",
        "  # Dropping first two rows: \"EvaluationMeasurement\" & \"PretrainDays\"\n",
        "  df.drop([0,1], axis=0, inplace=True)  \n",
        "\n",
        "  # Renaming columns based on mapper\n",
        "  df['Algorithms'].replace(col_mapper, inplace=True)  \n",
        "\n",
        "  # Melting the dataframe based on 'Algorithms'\n",
        "  df_melt = df.melt(id_vars=['Algorithms'])  \n",
        "\n",
        "  # Dropping unwanted varibale column(created bcoz of index)\n",
        "  df_melt.drop('variable', axis=1, inplace=True)  \n",
        "\n",
        "  # Renaming the value column by metric type\n",
        "  df_melt.rename(columns={'value':metric_type}, inplace=True)  \n",
        "\n",
        "  # Converting to float value bcoz by default the values are of type object\n",
        "  df_melt[metric_type] = df_melt[metric_type].astype('float64')  \n",
        "  \n",
        "  df_melt['Learner Type']= learner_type  # Adding the learner type\n",
        "\n",
        "  return df_melt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKugGWSVBPH6"
      },
      "source": [
        "def order_by_median(df,reverse = False):\n",
        "    grouped_df = df.groupby('Algorithms')\n",
        "    algo_medians = {}\n",
        "    for cur_group in grouped_df.groups.keys():\n",
        "        df_cur_grp = grouped_df.get_group(cur_group)\n",
        "        algo_medians[cur_group] = df_cur_grp['MAPE'].median()\n",
        "    sorted_algo_medians = dict(sorted(algo_medians.items(), key=lambda kv: kv[1], reverse=reverse))\n",
        "    return list(sorted_algo_medians.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-GmwJSgmvw6"
      },
      "source": [
        "# If value is less than zero return float value otherwise an integer value\n",
        "def format_values(y_val,pos):\n",
        "    if y_val < 1:\n",
        "        return format(float(y_val))\n",
        "    else:\n",
        "        return format(int(y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1676SLhHSe_"
      },
      "source": [
        "def draw_boxplot(df, save_filename):\n",
        "  colors = ['#1f77b4', '#ff7f0e','#2ca02c']\n",
        "  # Setting custom color palette\n",
        "  sns.set_palette(sns.color_palette(colors))\n",
        "\n",
        "  plt.figure(figsize=(10,6),dpi=90)\n",
        "  ordered_algo_list = order_by_median(df, reverse= False)\n",
        "\n",
        "  ax = sns.boxplot(x=\"Algorithms\", y=metric_type, hue='Learner Type', data=df, order=ordered_algo_list, dodge =False, width=0.5) #, hue_order=['Incremental','Static'])\n",
        "  ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
        "  ax.set(yscale = 'log')\n",
        "  ax.set_ylim(top =100)\n",
        "  ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(format_values))  # lambda x, p: format(int(x), ',')\n",
        "  ax.tick_params(axis='both', which='major', labelsize=16)\n",
        "  ax.set_ylabel(metric_type,fontsize=18)\n",
        "  ax.legend(loc='upper left')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(f'{box_plot_path}/{save_filename}.pdf')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKWFUip8HZd3"
      },
      "source": [
        "def read_preprocess_plot_graph(filenames, col_mapper, save_filename, metric_type='MAPE'):\n",
        "  metric_type = metric_type\n",
        "  frames = []\n",
        "  for filename in filenames:\n",
        "      if 'static' in filename:\n",
        "          learner_type = 'Static'\n",
        "      else:\n",
        "          learner_type = 'Incremental'\n",
        "\n",
        "      df = pd.read_csv(filename)\n",
        "      df_melt = preprocess_data(df, metric_type, learner_type, col_mapper)\n",
        "      frames.append(df_melt)\n",
        "\n",
        "  final_df = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "  # Updating LSTM learner type as Sequential\n",
        "  final_df.loc[final_df['Algorithms'] == 'LSTM', 'Learner Type'] = 'Sequential'\n",
        "\n",
        "  # Sorting final dataframe\n",
        "  final_df = final_df.sort_values(by=['MAPE'])\n",
        "  \n",
        "  draw_boxplot(final_df, save_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeltxQL_JnUH"
      },
      "source": [
        "\n",
        "col_mapper = {'HT_Reg':'Hoeffding Trees',\n",
        "              'HAT_Reg':'Hoeffding Adapt Tr',\n",
        "              'ARF_Reg':'Adaptive RF',\n",
        "              'PA_Reg':'Pass Agg Regr',\n",
        "              'RandomForest':'Random Forest',\n",
        "              'GradientBoosting': 'Gradient Boosting',\n",
        "              'DecisionTree': 'Decision Trees',\n",
        "              'LinearSVR': 'Linear SVR',\n",
        "              'BayesianRidge':'Bayesian Ridge'\n",
        "              }\n",
        "\n",
        "metric_type = 'MAPE'\n",
        "exp1_filenames = glob.glob(f'{exp1_path}/*{metric_type}*.csv')\n",
        "exp2_filenames = []\n",
        "\n",
        "for filename in glob.glob(f'{exp2_path}/*{metric_type}*.csv'):\n",
        "  if 'alternate' in filename or 'static' in filename:\n",
        "    exp2_filenames.append(filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uXMLcXK_ZkQ"
      },
      "source": [
        "'''\n",
        "col_mapper = {'HT_Reg':'HT',\n",
        "              'HAT_Reg':'HAT',\n",
        "              'ARF_Reg':'ARF',\n",
        "              'PA_Reg':'PA',\n",
        "              'RandomForest':'Random Forest',\n",
        "              'GradientBoosting': 'Gradient Boosting',\n",
        "              'DecisionTree': 'Decision Tree',\n",
        "              'LinearSVR': 'Linear SVR',\n",
        "              'BayesianRidge':'Bayesian Ridge',\n",
        "              'LSTM': 'LSTM'\n",
        "              }\n",
        "\n",
        "metric_type = 'MAPE'\n",
        "exp1_filenames = glob.glob(f'{exp1_path}/*{metric_type}*.csv')\n",
        "exp2_filenames = []\n",
        "\n",
        "for filename in glob.glob(f'{exp2_path}/*{metric_type}*.csv'):\n",
        "  if 'alternate' in filename or 'static' in filename:\n",
        "    exp2_filenames.append(filename)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "yFw4A4RoXGhs",
        "outputId": "8157bf23-36f6-4316-db2b-2f48bd783a51"
      },
      "source": [
        "save_filename = 'fig1'\n",
        "read_preprocess_plot_graph(exp1_filenames, col_mapper, save_filename, metric_type)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 900x540 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "r9MYJNUYjvfZ",
        "outputId": "4a26e0a8-dd3c-4797-c240-09da4241c332"
      },
      "source": [
        "save_filename = 'fig2'\n",
        "read_preprocess_plot_graph(exp2_filenames, col_mapper, save_filename,  metric_type)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 900x540 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "eA9pc2xtzDUL",
        "outputId": "300ec9f4-e058-494d-c0d5-df1412dd6d7c"
      },
      "source": [
        "!zip -r /content/Plots.zip /content/Plots\n",
        "from google.colab import files\n",
        "files.download(\"/content/Plots.zip\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: content/Plots/ (stored 0%)\n",
            "updating: content/Plots/boxplots/ (stored 0%)\n",
            "updating: content/Plots/boxplots/fig2.pdf (deflated 30%)\n",
            "updating: content/Plots/boxplots/fig1.pdf (deflated 36%)\n",
            "updating: content/Plots/barplot/ (stored 0%)\n",
            "updating: content/Plots/barplot/top_selected_country_cases.pdf (deflated 32%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5bf61ebf-84a2-462a-8dab-7697d67fdf5d\", \"Plots.zip\", 35625)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPCN6qhVzOLb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
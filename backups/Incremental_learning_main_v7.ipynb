{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Incremental_learning_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riu9BVr_-nl2"
      },
      "source": [
        "# Covid Dataset\n",
        "\n",
        "**Required Dataset features and target**\n",
        "\n",
        "The dataset has 53 columns; 1 to represent the country, 1 to represent the day (it will be an integer), 50 floats to represent the positive cases of the 50 previous days, and 1 column to represent the output that is the average of a full week of cases.\n",
        "\n",
        "![required_features.jpg](https://drive.google.com/uc?id=1smUwSHRwMT8h-M8kjG3ymmxdhQbe1HvY)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "ZhjnZH15MMQA",
        "outputId": "61905543-469f-4030-c686-1e6f43bacbbe"
      },
      "source": [
        "# Installing Incremental learner: Scikit-Multiflow\n",
        "!pip install scikit-multiflow\n",
        "\n",
        "\n",
        "# Overdiding some files from scikit multiflow library\n",
        "!gdown https://drive.google.com/uc?id=1f5GgBqjAsTUFnubHODmfqn6qiNyDBqIw\n",
        "!unzip /content/src.zip -d /content/src\n",
        "!cp -r /content/src/src /content/\n",
        "!rm -r /content/src/src\n",
        "\n",
        "# Creating a seperate directory to store all csv's\n",
        "! mkdir -p /content/csv_files\n",
        "! mkdir -p /content/csv_files/processed_null\n",
        "! mkdir -p /content/csv_files/processed\n",
        "\n",
        "! mkdir -p /content/Result/exp1\n",
        "! mkdir -p /content/Result/exp2\n",
        "! mkdir -p /content/Result/exp3\n",
        "\n",
        "! mkdir -p /content/Result/exp1/runtime\n",
        "! mkdir -p /content/Result/exp2/runtime\n",
        "! mkdir -p /content/Result/exp3/runtime\n",
        "\n",
        "! mkdir -p /content/Result/exp1/summary\n",
        "! mkdir -p /content/Result/exp2/summary\n",
        "! mkdir -p /content/Result/exp3/summary\n",
        "\n",
        "! mkdir -p /content/Plots\n",
        "! mkdir -p /content/Plots/barplot\n",
        "! mkdir -p /content/Plots/boxplots\n",
        "\n",
        "! mkdir -p /content/Result/exp1/united_dataframe\n",
        "! mkdir -p /content/Result/exp1/united_dataframe/incremental\n",
        "\n",
        "! mkdir -p /content/Result/exp1/united_dataframe/static\n",
        "! mkdir -p /content/Result/exp2/united_dataframe\n",
        "\n",
        "! mkdir -p /content/Result/exp2/united_dataframe/incremental\n",
        "! mkdir -p /content/Result/exp2/united_dataframe/static\n",
        "\n",
        "! mkdir -p /content/Result/exp3/united_dataframe\n",
        "! mkdir -p /content/Result/exp3/united_dataframe/incremental_alternate\n",
        "\n",
        "# Download the zip file\n",
        "\"\"\"\n",
        "!zip -r /content/file.zip /content/csv_files\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-multiflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/b8/dc05e1232cb261429258da43dc6882b4da8debbb485f968f06426e0bf41a/scikit_multiflow-0.5.3-cp37-cp37m-manylinux2010_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-multiflow) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from scikit-multiflow) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from scikit-multiflow) (1.19.5)\n",
            "Requirement already satisfied: sortedcontainers>=1.5.7 in /usr/local/lib/python3.7/dist-packages (from scikit-multiflow) (2.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from scikit-multiflow) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-multiflow) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->scikit-multiflow) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->scikit-multiflow) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.0.0->scikit-multiflow) (1.15.0)\n",
            "Installing collected packages: scikit-multiflow\n",
            "Successfully installed scikit-multiflow-0.5.3\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1f5GgBqjAsTUFnubHODmfqn6qiNyDBqIw\n",
            "To: /content/src.zip\n",
            "100% 26.1k/26.1k [00:00<00:00, 9.29MB/s]\n",
            "Archive:  /content/src.zip\n",
            "  inflating: /content/src/src/_classification_performance_evaluator.py  \n",
            "  inflating: /content/src/src/base_evaluator.py  \n",
            "  inflating: /content/src/src/constants.py  \n",
            "  inflating: /content/src/src/evaluate_prequential.py  \n",
            "  inflating: /content/src/src/evaluation_data_buffer.py  \n",
            "  inflating: /content/src/src/measure_collection.py  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!zip -r /content/file.zip /content/csv_files\\nfrom google.colab import files\\nfiles.download(\"/content/file.zip\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "cz3gWpvxYPLo",
        "outputId": "f4ccc26b-e570-441b-c5f0-8e5fc14b2caf"
      },
      "source": [
        "# For Box plot: Run this only if manually uploaded the results\n",
        "'''\n",
        "!unzip /content/Result.zip -d /content/Result\n",
        "!cp -r /content/Result/content/Result /content/\n",
        "!rm -r /content/Result/content/Result\n",
        "!rm -r /content/Result/content\n",
        "\n",
        "csv_processed_path = '/content/csv_files/processed'\n",
        "csv_processed_with_null_path = '/content/csv_files/processed_null'\n",
        "exp1_path = '/content/Result/exp1'\n",
        "exp2_path = '/content/Result/exp2'\n",
        "exp1_runtime_path = '/content/Result/exp1/runtime'\n",
        "exp2_runtime_path = '/content/Result/exp2/runtime'\n",
        "exp1_summary_path = '/content/Result/exp1/summary'\n",
        "exp2_summary_path = '/content/Result/exp2/summary'\n",
        "bar_plot_path = r'/content/Plots/barplot'\n",
        "box_plot_path = r'/content/Plots/boxplots'\n",
        "exp1_static_united_df_path = '/content/Result/exp1/united_dataframe/static'\n",
        "exp1_inc_united_df_path = '/content/Result/exp1/united_dataframe/incremental'\n",
        "exp2_static_united_df_path = '/content/Result/exp2/united_dataframe/static'\n",
        "exp2_inc_united_df_path = '/content/Result/exp2/united_dataframe/incremental'\n",
        "exp3_inc_alt_united_df_path = '/content/Result/exp2/united_dataframe/incremental_alternate'\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!unzip /content/Result.zip -d /content/Result\\n!cp -r /content/Result/content/Result /content/\\n!rm -r /content/Result/content/Result\\n!rm -r /content/Result/content\\n\\ncsv_processed_path = '/content/csv_files/processed'\\ncsv_processed_with_null_path = '/content/csv_files/processed_null'\\nexp1_path = '/content/Result/exp1'\\nexp2_path = '/content/Result/exp2'\\nexp1_runtime_path = '/content/Result/exp1/runtime'\\nexp2_runtime_path = '/content/Result/exp2/runtime'\\nexp1_summary_path = '/content/Result/exp1/summary'\\nexp2_summary_path = '/content/Result/exp2/summary'\\nbar_plot_path = r'/content/Plots/barplot'\\nbox_plot_path = r'/content/Plots/boxplots'\\nexp1_static_united_df_path = '/content/Result/exp1/united_dataframe/static'\\nexp1_inc_united_df_path = '/content/Result/exp1/united_dataframe/incremental'\\nexp2_static_united_df_path = '/content/Result/exp2/united_dataframe/static'\\nexp2_inc_united_df_path = '/content/Result/exp2/united_dataframe/incremental'\\nexp3_inc_alt_united_df_path = '/content/Result/exp2/united_dataframe/incremental_alternate'\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqTR_N3fpMON",
        "outputId": "62ded99e-4bdc-4f5e-d4e2-7dcf2621144c"
      },
      "source": [
        "#!pip uninstall keras\n",
        "#!pip uninstall tensorflow\n",
        "\n",
        "!pip install keras==2.3.1\n",
        "!pip install tensorflow==2.1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\r\u001b[K     |▉                               | 10kB 13.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 11.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 8.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 6.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n",
            "Installing collected packages: keras-applications, keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.3.1 keras-applications-1.0.8\n",
            "Collecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/56/0dbdae2a3c527a119bec0d5cf441655fe030ce1daa6fa6b9542f7dbd8664/tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 38kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 36.7MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 37.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.32.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.19.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.10.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.12.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (54.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.27.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.7.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.2.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=b4a6d97b53e4ddb99d3eeb6bfffa66b8e0cb11cbb05872b347dc2c612688b154\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, gast, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvDISgy5CUOC",
        "outputId": "825b24c5-76a8-4356-e424-43615b982862"
      },
      "source": [
        "# General Imports \n",
        "import pandas as pd\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "import warnings\n",
        "from pandas.core.common import SettingWithCopyWarning\n",
        "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "sns.set_theme(style='darkgrid')\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "# Imports for incremental learner\n",
        "from skmultiflow.data import DataStream\n",
        "from skmultiflow.trees import HoeffdingTreeRegressor\n",
        "from src.evaluate_prequential import EvaluatePrequential\n",
        "from skmultiflow.meta import AdaptiveRandomForestRegressor\n",
        "from skmultiflow.trees import HoeffdingAdaptiveTreeRegressor\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.linear_model import PassiveAggressiveRegressor\n",
        "\n",
        "# Imports for static Learner\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from time import perf_counter as pc_timer\n",
        "from functools import wraps\n",
        "\n",
        "import keras\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "#from tensorflow.keras import Sequential\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# For significance tests\n",
        "from scipy.stats import normaltest\n",
        "from scipy import stats \n",
        "# pd.set_option('display.max_colwidth', 500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "wHwu0MJOCm68",
        "outputId": "67d6931f-9337-4dde-e18a-67b8c2a9414a"
      },
      "source": [
        "url = 'https://drive.google.com/file/d/1eYy56fHe1XsWgPkBGVc0i6d6A5EU3nxj/view?usp=sharing'\n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "df = pd.read_csv(path)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dateRep</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "      <th>cases</th>\n",
              "      <th>deaths</th>\n",
              "      <th>countriesAndTerritories</th>\n",
              "      <th>geoId</th>\n",
              "      <th>countryterritoryCode</th>\n",
              "      <th>popData2019</th>\n",
              "      <th>continentExp</th>\n",
              "      <th>Cumulative_number_for_14_days_of_COVID-19_cases_per_100000</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30/11/2020</td>\n",
              "      <td>30</td>\n",
              "      <td>11</td>\n",
              "      <td>2020</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>6.417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29/11/2020</td>\n",
              "      <td>29</td>\n",
              "      <td>11</td>\n",
              "      <td>2020</td>\n",
              "      <td>228</td>\n",
              "      <td>11</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>6.845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28/11/2020</td>\n",
              "      <td>28</td>\n",
              "      <td>11</td>\n",
              "      <td>2020</td>\n",
              "      <td>214</td>\n",
              "      <td>15</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>6.785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>27/11/2020</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2020</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>6.396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>26/11/2020</td>\n",
              "      <td>26</td>\n",
              "      <td>11</td>\n",
              "      <td>2020</td>\n",
              "      <td>200</td>\n",
              "      <td>12</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>7.342</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      dateRep  ...  Cumulative_number_for_14_days_of_COVID-19_cases_per_100000\n",
              "0  30/11/2020  ...                                              6.417         \n",
              "1  29/11/2020  ...                                              6.845         \n",
              "2  28/11/2020  ...                                              6.785         \n",
              "3  27/11/2020  ...                                              6.396         \n",
              "4  26/11/2020  ...                                              7.342         \n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6NQxu_LuoTV"
      },
      "source": [
        "# Grouping countries together for analysis\n",
        "total_countries = df['countriesAndTerritories'].unique()\n",
        "df_grouped = df.groupby('countriesAndTerritories')\n",
        "pretrain_days = [30,60,90,120,150,180,210,240]  # TODO: Make it full list later\n",
        "valid_countries = []\n",
        "decimal = 3  # Specify the scale of decimal places \n",
        "error_metrics = ['MAE','MAPE', 'RMSE']\n",
        "\n",
        "# Setting path variables for both experiments\n",
        "csv_processed_path = '/content/csv_files/processed'\n",
        "csv_processed_with_null_path = '/content/csv_files/processed_null'\n",
        "\n",
        "exp1_path = '/content/Result/exp1'\n",
        "exp2_path = '/content/Result/exp2'\n",
        "exp3_path = '/content/Result/exp3'\n",
        "\n",
        "exp1_runtime_path = '/content/Result/exp1/runtime'\n",
        "exp2_runtime_path = '/content/Result/exp2/runtime'\n",
        "exp3_runtime_path = '/content/Result/exp3/runtime'\n",
        "\n",
        "exp1_summary_path = '/content/Result/exp1/summary'\n",
        "exp2_summary_path = '/content/Result/exp2/summary'\n",
        "exp3_summary_path = '/content/Result/exp3/summary'\n",
        "\n",
        "bar_plot_path = r'/content/Plots/barplot'\n",
        "box_plot_path = r'/content/Plots/boxplots'\n",
        "\n",
        "exp1_static_united_df_path = '/content/Result/exp1/united_dataframe/static'\n",
        "exp1_inc_united_df_path = '/content/Result/exp1/united_dataframe/incremental'\n",
        "\n",
        "exp2_static_united_df_path = '/content/Result/exp2/united_dataframe/static'\n",
        "exp2_inc_united_df_path = '/content/Result/exp2/united_dataframe/incremental'\n",
        "\n",
        "exp3_inc_alt_united_df_path = '/content/Result/exp2/united_dataframe/incremental_alternate'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oubf_WqnATmM"
      },
      "source": [
        "## Feature Set with Individual Countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0vj8668gNrP"
      },
      "source": [
        "# Create lags\n",
        "def create_features_with_lags(df):\n",
        "  for i in range(89, 0, -1):  # Loop in reverse order for creating ordered lags eg: cases_t-10, cases_t-9... cases_t-1. t=current cases\n",
        "    df[f'cases_t-{i}'] = df['cases'].shift(i, axis=0)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G23w6M6HdyEb",
        "outputId": "eb44edc8-424b-4f27-95d8-a33ef7b22b42"
      },
      "source": [
        "# Pre-Processing dataset and saving them into csv's.\n",
        "for country in total_countries:\n",
        "  df = df_grouped.get_group(country)\n",
        "\n",
        "  # Selecting required features\n",
        "  df= df[['dateRep','cases','countriesAndTerritories']]\n",
        "\n",
        "  # Rename features\n",
        "  df.rename(columns={'countriesAndTerritories':'country', 'dateRep':'date'}, inplace=True)\n",
        "\n",
        "  # Convert to date, sort and set index\n",
        "  df['date'] = pd.to_datetime(df['date'],format='%d/%m/%Y')\n",
        "  df.sort_values('date', inplace=True)\n",
        "  df.set_index('date', inplace=True)\n",
        "\n",
        "  # Adding feature\n",
        "  df['day_no']= pd.Series([i for i in range(1,len(df)+1)], index=df.index)\n",
        "\n",
        "  # Reordering features\n",
        "  df = df[['day_no', 'country','cases']]\n",
        "\n",
        "  # Adding features through lags\n",
        "  df = create_features_with_lags(df)\n",
        "\n",
        "  # Creating target with last 10 days cases\n",
        "  df['target'] = df.iloc[:,[2]+[i*-1 for i in range(1,10)]].mean(axis=1)\n",
        "\n",
        "  # Dropping mid columns\n",
        "  drop_columns = list(df.loc[:,'cases_t-39':'cases_t-1'].columns)  #list(df.loc[:,'cases_t-38':'cases_t-1'].columns)\n",
        "  df.drop(drop_columns, axis=1, inplace=True)\n",
        "\n",
        "  # Country name\n",
        "  filename = df['country'].unique()[0]\n",
        "\n",
        "  # Saving file\n",
        "  df.to_csv(f'{csv_processed_with_null_path}/{filename}.csv')\n",
        "\n",
        "  # Dropping null records\n",
        "  df.dropna(how='any', axis=0, inplace=True)\n",
        "\n",
        "  # Valid countries that have records more than max of pretrain\n",
        "  if len(df)>max(pretrain_days):\n",
        "    valid_countries.append(country)  \n",
        "    df.to_csv(f'{csv_processed_path}/{filename}.csv')\n",
        "  \n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMZCoXFwPsHA"
      },
      "source": [
        "## Total cases of top selected countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClOLDLs6480A"
      },
      "source": [
        "# Top countries to select for experiment 1\n",
        "number_of_countries = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDWlSV8OJUQP"
      },
      "source": [
        "# Replaces underscore from country names\n",
        "def format_names(list_countries):\n",
        "  updated_country_list = []\n",
        "  for country_name in list_countries:\n",
        "    updated_country_list.append(country_name.replace(\"_\",\" \"))\n",
        "  return updated_country_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQfWz6LlAyHf"
      },
      "source": [
        "# A dictionary of all countries\n",
        "dict_countries = Counter(valid_countries)\n",
        "\n",
        "for country in valid_countries:\n",
        "  dict_countries[country] = df_grouped.get_group(country)['cases'].sum()\n",
        "\n",
        "# Select top_countries and order(Ascending/Decending) \n",
        "top_countries = sorted(dict_countries.items(), key=lambda dict_countries: dict_countries[1], reverse=True) [0:number_of_countries]\n",
        "\n",
        "# Creating dataframe of top selected countries\n",
        "df_top_countries = pd.DataFrame.from_dict(dict(top_countries), orient='index', columns=['Total Cases'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_U3MIv16lLfX",
        "outputId": "1e96c734-0b1d-4c02-f9dc-4b7302c4d80b"
      },
      "source": [
        "# Number of countries to plot on bar graph\n",
        "num_country_plot = 50\n",
        "\n",
        "df_plot_countries = df_top_countries.iloc[0:num_country_plot]\n",
        "\n",
        "# Plotting graph\n",
        "sns.set_theme(style='white')\n",
        "plt.figure(figsize=(12,16))\n",
        "top_countries_list = format_names(df_plot_countries.index)\n",
        "plt.barh(top_countries_list[::-1], df_plot_countries['Total Cases'].values[::-1]) # Reversing the order to have heighest values at the top of bar chart\n",
        "#plt.title(f'Top {len(top_countries)} Countries with Most Cases')\n",
        "plt.xscale('log')\n",
        "ax = plt.axes() # for updating axes values to plain text\n",
        "ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "plt.margins(y=0)\n",
        "ax.tick_params(axis='both', which='major', labelsize=18)\n",
        "plt.xlabel('Number of Cases', fontsize=20)\n",
        "plt.ylabel('Countries',fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{bar_plot_path}/top_selected_country_cases.pdf')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x1152 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "qbgnoDsBEsoQ",
        "outputId": "a4217f75-684d-47ef-94a2-25f30d8b08b5"
      },
      "source": [
        "\"\"\"\n",
        "# Plotting graph\n",
        "sns.set_theme(style='white')\n",
        "plt.figure(figsize=(12,16), dpi=90)\n",
        "\n",
        "#plt.bar(df_top_countries.index, df_top_countries['Total Cases'].values)\n",
        "top_countries_list = format_names(df_top_countries.index)\n",
        "plt.barh(top_countries_list, df_top_countries['Total Cases'].values)\n",
        "\n",
        "#plt.axvline(Number_of_countries-0.5, 0,1, ls='--', c='black')\n",
        "plt.axhline(y=(Number_of_countries-0.5), ls='--', c='black')\n",
        "\n",
        "plt.title(f'Top {len(top_countries)} Countries with Most Cases')\n",
        "\n",
        "plt.xscale('log')\n",
        "ax = plt.axes() # for updating \n",
        "ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "\n",
        "#plt.gcf().axes[0].yaxis.get_major_formatter().set_scientific(False)  # To get labels in plain text \n",
        "#plt.xticks(rotation=90)\n",
        "plt.margins(y=0)\n",
        "plt.xlabel('Number of Cases')\n",
        "plt.ylabel('Countries')\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Plotting graph\\nsns.set_theme(style='white')\\nplt.figure(figsize=(12,16), dpi=90)\\n\\n#plt.bar(df_top_countries.index, df_top_countries['Total Cases'].values)\\ntop_countries_list = format_names(df_top_countries.index)\\nplt.barh(top_countries_list, df_top_countries['Total Cases'].values)\\n\\n#plt.axvline(Number_of_countries-0.5, 0,1, ls='--', c='black')\\nplt.axhline(y=(Number_of_countries-0.5), ls='--', c='black')\\n\\nplt.title(f'Top {len(top_countries)} Countries with Most Cases')\\n\\nplt.xscale('log')\\nax = plt.axes() # for updating \\nax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\\n\\n#plt.gcf().axes[0].yaxis.get_major_formatter().set_scientific(False)  # To get labels in plain text \\n#plt.xticks(rotation=90)\\nplt.margins(y=0)\\nplt.xlabel('Number of Cases')\\nplt.ylabel('Countries')\\nplt.show()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0TQucg2Pwu1"
      },
      "source": [
        "## Average cases of top selected countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6kOjiJOSRCtG",
        "outputId": "6da90646-12e0-423a-f014-141048babb6d"
      },
      "source": [
        "dict_countries_avg = Counter(total_countries)\n",
        "\n",
        "for country in dict_countries.keys():\n",
        "  dict_countries_avg[country] = df_grouped.get_group(country)['cases'].mean()\n",
        "\n",
        "# Average cases for all countries\n",
        "df_avg_cases_countries = pd.DataFrame.from_dict(dict_countries_avg, orient='index', columns=['Average'])\n",
        "\n",
        "# List of top selected countries\n",
        "top_countries = list(df_top_countries.index)\n",
        "\n",
        "# Average of selected top countries\n",
        "avg_df = df_avg_cases_countries[df_avg_cases_countries.index.isin(top_countries)]\n",
        "avg_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Algeria</th>\n",
              "      <td>248.402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Australia</th>\n",
              "      <td>83.015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Austria</th>\n",
              "      <td>827.923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bahrain</th>\n",
              "      <td>259.066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Belgium</th>\n",
              "      <td>1719.735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Brazil</th>\n",
              "      <td>18793.869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Canada</th>\n",
              "      <td>1102.018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>China</th>\n",
              "      <td>273.256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Croatia</th>\n",
              "      <td>380.216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Czechia</th>\n",
              "      <td>1546.795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Denmark</th>\n",
              "      <td>236.167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ecuador</th>\n",
              "      <td>580.414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Egypt</th>\n",
              "      <td>348.015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Estonia</th>\n",
              "      <td>36.411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Finland</th>\n",
              "      <td>74.184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>France</th>\n",
              "      <td>6602.628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Georgia</th>\n",
              "      <td>407.159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Germany</th>\n",
              "      <td>3136.515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Greece</th>\n",
              "      <td>312.057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iceland</th>\n",
              "      <td>16.015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>India</th>\n",
              "      <td>28154.301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iran</th>\n",
              "      <td>2823.658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iraq</th>\n",
              "      <td>1648.009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ireland</th>\n",
              "      <td>216.290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Israel</th>\n",
              "      <td>1008.949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Italy</th>\n",
              "      <td>4717.792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Japan</th>\n",
              "      <td>436.786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Kuwait</th>\n",
              "      <td>427.706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lebanon</th>\n",
              "      <td>382.361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Malaysia</th>\n",
              "      <td>192.493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mexico</th>\n",
              "      <td>3294.854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Netherlands</th>\n",
              "      <td>1542.479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Norway</th>\n",
              "      <td>103.414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Oman</th>\n",
              "      <td>374.194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pakistan</th>\n",
              "      <td>1202.489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Philippines</th>\n",
              "      <td>1294.771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Qatar</th>\n",
              "      <td>417.614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Romania</th>\n",
              "      <td>1411.784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Russia</th>\n",
              "      <td>6753.917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>San_Marino</th>\n",
              "      <td>4.734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Singapore</th>\n",
              "      <td>173.253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>South_Korea</th>\n",
              "      <td>101.789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Spain</th>\n",
              "      <td>4905.318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sweden</th>\n",
              "      <td>766.554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Switzerland</th>\n",
              "      <td>943.503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Taiwan</th>\n",
              "      <td>2.009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>United_Arab_Emirates</th>\n",
              "      <td>508.342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>United_Kingdom</th>\n",
              "      <td>4813.473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>United_States_of_America</th>\n",
              "      <td>39831.312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Vietnam</th>\n",
              "      <td>4.045</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           Average\n",
              "Algeria                    248.402\n",
              "Australia                   83.015\n",
              "Austria                    827.923\n",
              "Bahrain                    259.066\n",
              "Belgium                   1719.735\n",
              "Brazil                   18793.869\n",
              "Canada                    1102.018\n",
              "China                      273.256\n",
              "Croatia                    380.216\n",
              "Czechia                   1546.795\n",
              "Denmark                    236.167\n",
              "Ecuador                    580.414\n",
              "Egypt                      348.015\n",
              "Estonia                     36.411\n",
              "Finland                     74.184\n",
              "France                    6602.628\n",
              "Georgia                    407.159\n",
              "Germany                   3136.515\n",
              "Greece                     312.057\n",
              "Iceland                     16.015\n",
              "India                    28154.301\n",
              "Iran                      2823.658\n",
              "Iraq                      1648.009\n",
              "Ireland                    216.290\n",
              "Israel                    1008.949\n",
              "Italy                     4717.792\n",
              "Japan                      436.786\n",
              "Kuwait                     427.706\n",
              "Lebanon                    382.361\n",
              "Malaysia                   192.493\n",
              "Mexico                    3294.854\n",
              "Netherlands               1542.479\n",
              "Norway                     103.414\n",
              "Oman                       374.194\n",
              "Pakistan                  1202.489\n",
              "Philippines               1294.771\n",
              "Qatar                      417.614\n",
              "Romania                   1411.784\n",
              "Russia                    6753.917\n",
              "San_Marino                   4.734\n",
              "Singapore                  173.253\n",
              "South_Korea                101.789\n",
              "Spain                     4905.318\n",
              "Sweden                     766.554\n",
              "Switzerland                943.503\n",
              "Taiwan                       2.009\n",
              "United_Arab_Emirates       508.342\n",
              "United_Kingdom            4813.473\n",
              "United_States_of_America 39831.312\n",
              "Vietnam                      4.045"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkabfg44B2iE"
      },
      "source": [
        "# Common Methods for All Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwvQ0-1b7Nk5"
      },
      "source": [
        "countries = top_countries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jkIXUcMCpwy"
      },
      "source": [
        "## Common Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHYQO1XBALXe"
      },
      "source": [
        "# Return a combined dataframe for a each error statistics(MAE,RMSE,MAPE etc) along with the newly added mean row.\n",
        "def get_metric_with_mean(result: pd.DataFrame, error_metric: str)->pd.DataFrame:\n",
        "  df_grouped = result.groupby('EvaluationMeasurement')\n",
        "  df = df_grouped.get_group(error_metric).reset_index(drop=True)\n",
        "  df = df.append(df.describe().loc['mean'])\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AVGVaKLwhHu"
      },
      "source": [
        "def calc_mean_to_max_error(df, max_of_pretrain_days, max_of_df):\n",
        "  i=-1\n",
        "  for row_num in range(len(df)-1):  # Go before mean row\n",
        "    i += 1\n",
        "    for col_num in df.columns[2:]:\n",
        "      df.loc[row_num,col_num] = df.loc[row_num,col_num]/max_of_pretrain_days[i] \n",
        "  \n",
        "  for col in df.columns[2:]:\n",
        "      df.loc['mean',col] = df.loc['mean',col]/max_of_df\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgvjK5nJkUaG"
      },
      "source": [
        "# Note: Do not change the filenames, since they are later being used for visualizations \n",
        "def save_runtime(df,path,country=None,static_learner=True,alternate_batch=False, transpose=False):\n",
        "  df = df.apply(pd.to_numeric,errors='ignore') # Converting the dataframe to numeric\n",
        "  df = df.round(decimal) # Setting the precision\n",
        "  \n",
        "  # if transpose flag is set to true\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "\n",
        "  if country==None:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/combined_country_runtime_static.tex')\n",
        "      df.to_csv(f'{path}/combined_country_runtime_static.csv')\n",
        "    else:\n",
        "      if alternate_batch:\n",
        "         df.to_latex(f'{path}/combined_country_runtime_incremental_alternate_batch.tex')\n",
        "         df.to_csv(f'{path}/combined_country_runtime_incremental_alternate_batch.csv')\n",
        "      else:\n",
        "        df.to_latex(f'{path}/combined_country_runtime_incremental.tex')\n",
        "        df.to_csv(f'{path}/combined_country_runtime_incremental.csv')\n",
        "  else:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/{country}_runtime_static.tex')\n",
        "      df.to_csv(f'{path}/{country}_runtime_static.csv')\n",
        "    else:\n",
        "      df.to_latex(f'{path}/{country}_runtime_incremental.tex')\n",
        "      df.to_csv(f'{path}/{country}_runtime_incremental.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJCVpF7oT7r0"
      },
      "source": [
        "# Note: Do not change the filenames, since they are later being used for visualizations \n",
        "def save_summary_table(df,path,country=False,static_learner=True,alternate_batch=False, transpose=False):\n",
        "\n",
        "  df = df.apply(pd.to_numeric,errors='ignore') # Converting the dataframe to numeric\n",
        "  df = df.round(decimal) # Setting the precision\n",
        "  \n",
        "  # if transpose flag is set to true\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "\n",
        "  if country:\n",
        "    metric = df.loc['EvaluationMeasurement'].unique()[0]\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/top_countries_{metric}_summary_table_static.tex')\n",
        "      df.to_csv(f'{path}/top_countries_{metric}_summary_table_static.csv')\n",
        "    else:\n",
        "      df.to_latex(f'{path}/top_countries_{metric}_summary_table_incremental.tex')\n",
        "      df.to_csv(f'{path}/top_countries_{metric}_summary_table_incremental.csv')\n",
        "    \n",
        "  else:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/combined_country_summary_table_static.tex')\n",
        "      df.to_csv(f'{path}/combined_country_summary_table_static.csv')\n",
        "    else:\n",
        "      if alternate_batch:\n",
        "         df.to_latex(f'{path}/combined_country_summary_table_incremental_alternate_batch.tex')\n",
        "         df.to_csv(f'{path}/combined_country_summary_table_incremental_alternate_batch.csv')\n",
        "      else:\n",
        "        df.to_latex(f'{path}/combined_country_summary_table_incremental.tex')\n",
        "        df.to_csv(f'{path}/combined_country_summary_table_incremental.csv')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnS1UsFdKRbW"
      },
      "source": [
        "# Note: Do not change the filenames since they are later being used for visualizations\n",
        "def save_metrics(df, path, country=None, static_learner=True, alternate_batch=False, transpose=False): \n",
        "  df = df.apply(pd.to_numeric,errors='ignore') # Converting the dataframe to numeric\n",
        "  df = df.round(decimal) # Setting the precision\n",
        "  \n",
        "  # if transpose flag is set to true\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "\n",
        "  metric_type = df.loc['EvaluationMeasurement'].unique()[0]\n",
        "  if country==None:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/combined_country_{metric_type}_static.tex')\n",
        "      df.to_csv(f'{path}/combined_country_{metric_type}_static.csv')\n",
        "    else:\n",
        "      if alternate_batch:\n",
        "         df.to_latex(f'{path}/combined_country_{metric_type}_incremental_alternate_batch.tex')\n",
        "         df.to_csv(f'{path}/combined_country_{metric_type}_incremental_alternate_batch.csv')\n",
        "      else:\n",
        "        df.to_latex(f'{path}/combined_country_{metric_type}_incremental.tex')\n",
        "        df.to_csv(f'{path}/combined_country_{metric_type}_incremental.csv')\n",
        "  else:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/{country}_{metric_type}_static.tex')\n",
        "      df.to_csv(f'{path}/{country}_{metric_type}_static.csv')\n",
        "    else:\n",
        "      df.to_latex(f'{path}/{country}_{metric_type}_incremental.tex')\n",
        "      df.to_csv(f'{path}/{country}_{metric_type}_incremental.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQFR3NKzuSK-"
      },
      "source": [
        "def save_combined_summary_table(df, path, static_learner=False,transpose=False):\n",
        "  df = df.apply(pd.to_numeric,errors='ignore')\n",
        "  df = df.round(decimal)\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "  \n",
        "  if static_learner:\n",
        "    save_path = f'{path}/summary_table_combined_mean_static'\n",
        "  else:\n",
        "    save_path = f'{path}/summary_table_combined_mean_incremental'\n",
        "\n",
        "  df.to_csv(f'{save_path}.csv')\n",
        "  df.to_latex(f'{save_path}.tex')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umh96O9TSp2Z"
      },
      "source": [
        "def save_united_df(df, path, country=None):\n",
        "    if country:\n",
        "        df.to_csv(f'{path}/{country}.csv')\n",
        "    else:\n",
        "        df.to_csv(f'{path}/united_df.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUrUegDEPqof"
      },
      "source": [
        "def display_runtime_per_country(results_runtime,countries):\n",
        "  for i in range(len(countries)):\n",
        "    print(f'_____________Running Time for {countries[i]}________________')\n",
        "    print(results_runtime[i].to_string())\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI4IiENZtLFG"
      },
      "source": [
        "def calc_save_err_metric_countrywise(countries, error_metrics, results, max_of_pretrain_per_country, max_cases_per_country, path, static_learner, transpose):\n",
        "  countrywise_error_scores={}\n",
        "  for i in range(len(countries)):\n",
        "    country_error_score = []\n",
        "    for error_metric in error_metrics:\n",
        "      \n",
        "      df_error_metric = get_metric_with_mean(results[i], error_metric=error_metric)\n",
        "\n",
        "      #if error_metric != 'MAPE':\n",
        "      #  df_error_metric = calc_mean_to_max_error(df_error_metric, max_of_pretrain_per_country[i], max_cases_per_country[i])\n",
        "\n",
        "      country_error_score.append(df_error_metric)\n",
        "      display_countrywise_scores(countries[i],df_error_metric)\n",
        "\n",
        "      # Transposing the metrics while saving\n",
        "      save_metrics(df_error_metric, path=path, country=countries[i], static_learner=static_learner, transpose=transpose)\n",
        "      \n",
        "    countrywise_error_scores[countries[i]] = pd.concat(country_error_score,ignore_index=True)\n",
        "    \n",
        "  return countrywise_error_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e4uCM0f65tm"
      },
      "source": [
        "def calc_save_err_metric_combined(error_metrics, results, max_of_pretrain_days, max_selected_countries, path, static_learner, alternate_batch, transpose):\n",
        "  combined_err_metric = []\n",
        "  for error_metric in error_metrics:\n",
        "    df_error_metric = get_metric_with_mean(results, error_metric=error_metric)\n",
        "\n",
        "    #if error_metric != 'MAPE':\n",
        "    #  df_error_metric = calc_mean_to_max_error(df_error_metric, max_of_pretrain_days, max_selected_countries)\n",
        "\n",
        "    # Transposing the metrics while saving\n",
        "    save_metrics(df_error_metric, path=path, static_learner=static_learner, alternate_batch=alternate_batch, transpose=transpose)\n",
        "    \n",
        "    combined_err_metric.append(df_error_metric)\n",
        "  return (pd.concat(combined_err_metric, ignore_index=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7nnCdGu8QVD"
      },
      "source": [
        "def get_summary_table(df_result, df_runtime_result, error_metrics, static_learner=True):\n",
        "  sum_metric=[]\n",
        "  measure_col_name = 'Metric'\n",
        "  \n",
        "  # Setting start row and column for static and incremental learner\n",
        "  for metric in error_metrics:\n",
        "    start_row = 'mean'\n",
        "    if static_learner:\n",
        "      start_col='RandomForest'\n",
        "    else:\n",
        "      start_col='HT_Reg'\n",
        "\n",
        "    df_metric = get_metric_with_mean(df_result, metric)\n",
        "    df_row = pd.DataFrame([df_metric.loc[start_row][start_col:]])\n",
        "    \n",
        "    df_row[measure_col_name] = str(metric)    \n",
        "    sum_metric.append(df_row)\n",
        "\n",
        "  # Adding run time\n",
        "  df_runtime_row = pd.DataFrame([df_runtime_result.describe().loc[start_row][start_col:]])\n",
        "  df_runtime_row[measure_col_name]='Time(sec)'\n",
        "  sum_metric.append(df_runtime_row)\n",
        "\n",
        "  df_summary = pd.concat(sum_metric, ignore_index=True)\n",
        "  df_summary.set_index(measure_col_name, inplace=True)\n",
        "\n",
        "  return df_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLrQqgWXAPgi"
      },
      "source": [
        "def get_summary_table_countrywise(df_result_dict, error_metrics, static_learner=True):  #df_runtime_result,\n",
        "  summary_metric=[]\n",
        "  measure_col_name = f'Country({str(error_metrics[0])})'\n",
        "  eval_measure_col = 'EvaluationMeasurement'\n",
        "  start_row = 'mean'\n",
        "  if static_learner:\n",
        "    start_col='RandomForest'\n",
        "  else:\n",
        "    start_col='HT_Reg'\n",
        "\n",
        "  for country in df_result_dict.keys():\n",
        "    df_result = df_result_dict[country]\n",
        "\n",
        "    # Setting start row and column for static and incremental learner\n",
        "    for metric in error_metrics:      \n",
        "      df_metric = get_metric_with_mean(df_result, metric)\n",
        "      df_row = pd.DataFrame([df_metric.loc[start_row][start_col:]])\n",
        "      df_row[eval_measure_col] = metric\n",
        "      df_row[measure_col_name] = country\n",
        "      summary_metric.append(df_row)\n",
        "\n",
        "  df_summary = pd.concat(summary_metric, ignore_index=True)\n",
        "  df_summary.set_index(measure_col_name, inplace=True)\n",
        "\n",
        "  return df_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSX0nYK3SW2Q"
      },
      "source": [
        "def get_sum_table_combined_mean(countrywise_error_score_incremental,results_runtime, static_learner=False):\n",
        "  sum_table_combined_mean=[]\n",
        "  measure_col_name = 'Metric'\n",
        "  start_row = 'mean'\n",
        "  if static_learner:\n",
        "    start_col = 'RandomForest'\n",
        "  else:\n",
        "    start_col= 'HT_Reg'\n",
        "\n",
        "  for metric in error_metrics:\n",
        "    df_sum_cur_metric = get_summary_table_countrywise(countrywise_error_score_incremental, [metric], static_learner=static_learner)\n",
        "    df_row = pd.DataFrame([df_sum_cur_metric.describe().loc[start_row]])\n",
        "\n",
        "    df_row[measure_col_name] = metric\n",
        "    sum_table_combined_mean.append(df_row)\n",
        "\n",
        "  # Adding run time\n",
        "  df_runtime = pd.concat(results_runtime, ignore_index=True).describe().loc[start_row][start_col:]\n",
        "  df_runtime_row = pd.DataFrame([df_runtime])\n",
        "  df_runtime_row[measure_col_name]='Time(sec)'\n",
        "  sum_table_combined_mean.append(df_runtime_row)\n",
        "\n",
        "  # Concating results to one dataframe\n",
        "  sum_table_combined_mean = pd.concat(sum_table_combined_mean, ignore_index=True)\n",
        "  sum_table_combined_mean.set_index(measure_col_name, inplace=True)\n",
        "  return sum_table_combined_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AKhvSFwAzUj"
      },
      "source": [
        "def check_significance(target_pop, competitor_pop, significance_at: float):\n",
        "    \"\"\"\n",
        "    Comparing algorithms per batch or per country pairs (exp 2 or 1 respectively), \n",
        "      so for each pair, we compare the significance of the best algo to all of the the other algos.\n",
        "    Ttest performed if the distribution is normal, otherwise we perform a non-parametric test.\n",
        "    \"\"\"\n",
        "    model_pop, population = target_pop, competitor_pop  \n",
        "    \n",
        "    # Normality tests\n",
        "    if len(model_pop) >= 8:  # skew test not valid for smaller populations\n",
        "      value_mdl, p_mdl = normaltest(model_pop.values)\n",
        "      value_pop, p_pop = normaltest(population.values)\n",
        "      if (p_mdl >= 0.05) & (p_pop >= 0.05):\n",
        "          # print('It is likely that both populations are normal. Thus, running T-Test...')\n",
        "          tset, pval = stats.ttest_ind(model_pop, population)\n",
        "          if pval < significance_at:    # alpha value is 0.05 or 5%\n",
        "              significant = 'Significant (Ttest)'\n",
        "          else:\n",
        "              significant = 'Not Significant (Ttest)'\n",
        "      else:\n",
        "          # print('It is unlikely that the result is normal. Thus, running Wilcoxon test...')\n",
        "          if np.sum(np.subtract(list(model_pop), list(competitor_pop))) != 0.0:  # if values are identical the test will crash, but we now it's not significant\n",
        "              tset, pval = stats.wilcoxon(model_pop, population)\n",
        "              if pval < significance_at:    # alpha value is 0.05 or 5%\n",
        "                  significant = 'Significant (Wilcox Test)'\n",
        "              else:\n",
        "                  significant = 'Not Significant (Wilcox Test)'\n",
        "          else:\n",
        "              # print('Warning: results are identical')\n",
        "              tset, pval = stats.ttest_ind(model_pop, population)\n",
        "              significant = 'Not Significant (Wilcox Test)'\n",
        "    else:\n",
        "      print('Population too small.')\n",
        "      if np.sum(np.subtract(list(model_pop), list(competitor_pop))) != 0.0:  # if values are identical the test will crash, but we now it's not significant\n",
        "          tset, pval = stats.wilcoxon(model_pop, population)\n",
        "          if pval < significance_at:    # alpha value is 0.05 or 5%\n",
        "              significant = 'Significant (Wilcox Test)'\n",
        "          else:\n",
        "              significant = 'Not Significant (Wilcox Test)'\n",
        "    return pval, significant "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuW-RWNMWGE5"
      },
      "source": [
        "def unit_incremental_df(country_name, evaluator, date, milestone):  # Added Now\n",
        "    frame = {}\n",
        "    frame['date'] = date\n",
        "    if type(country_name) == pd.Series:\n",
        "        frame['Country'] = country_name\n",
        "    else:\n",
        "        frame['Country'] = [country_name] * len(date)\n",
        "    frame['Milestone'] = [milestone] * len(date)\n",
        "    frame['y_true'] = evaluator.mean_eval_measurements[0].y_true_vector\n",
        "    for i in range(len(evaluator.model_names)):\n",
        "        frame[f'pred_{evaluator.model_names[i]}'] = evaluator.mean_eval_measurements[i].y_pred_vector\n",
        "    return pd.DataFrame(frame)\n",
        "\n",
        "\n",
        "\n",
        "def unit_static_df(country_name, date, y_true,  milestone, model_predictions):  # Added Now\n",
        "    frame = {}\n",
        "    frame['date'] = date\n",
        "    if type(country_name) == pd.Series:\n",
        "        frame['Country'] = country_name\n",
        "    else:\n",
        "        frame['Country'] = [country_name] * len(date)\n",
        "\n",
        "    frame['Milestone'] = [milestone] * len(date)\n",
        "    frame['y_true'] = y_true\n",
        "\n",
        "    for algo, y_pred in model_predictions.items():  # Updated Now\n",
        "        if algo == 'LSTM':\n",
        "            frame[f'pred_{algo}'] = y_pred.flatten().tolist()\n",
        "        else:\n",
        "            frame[f'pred_{algo}'] = y_pred\n",
        "\n",
        "    return pd.DataFrame(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZYx2PYrRBMV"
      },
      "source": [
        "## Combining Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g11ZDONMQvHT"
      },
      "source": [
        "'''\n",
        "def sortby_date_and_set_index(df):\n",
        "  df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y')\n",
        "  df.sort_values('date', inplace=True)\n",
        "  df.set_index('date', inplace=True)\n",
        "  return df\n",
        "'''\n",
        "\n",
        "def sortby_date_and_set_index(df):  # Updated\n",
        "    df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y')\n",
        "    df.sort_values('date', inplace=True)\n",
        "    # df.set_index('date', inplace=True) #TODO:  Not setting date as idx; Might need to remove this line later\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZr-9CCDwIVo"
      },
      "source": [
        "def get_dataset_with_target(countries, df_grouped):\n",
        "    # Empty list to store Dataframes of each country\n",
        "    frames = []\n",
        "\n",
        "    for country in countries:\n",
        "        df = df_grouped.get_group(country)\n",
        "\n",
        "        # Creating feature 'day_no'\n",
        "        df['day_no'] = pd.Series([i for i in range(1, len(df) + 1)], index=df.index)\n",
        "\n",
        "        # Reordering features\n",
        "        # df = df[['day_no', 'country', 'cases']]\n",
        "        df = df[['date', 'day_no', 'country', 'cases']]  # Added: Date column\n",
        "\n",
        "        # Adding features through lags\n",
        "        df = create_features_with_lags(df)\n",
        "\n",
        "        # Creating target with last 10 days cases\n",
        "        idx_cases = list(df.columns).index('cases')  # Added: Earlier hard coded idx\n",
        "        df['target'] = df.iloc[:, [idx_cases] + [i * -1 for i in range(1, 10)]].mean(axis=1)  # Updated: Replacing idx with idx_cases\n",
        "\n",
        "        # Dropping null columns\n",
        "        df.dropna(how='any', axis=0, inplace=True)\n",
        "\n",
        "        # Dropping mid columns\n",
        "        drop_columns = list(df.loc[:, 'cases_t-39':'cases_t-1'].columns)  # Updated: cases_t-38 to t-39 for exact 50 columns of lags\n",
        "        df.drop(drop_columns, axis=1, inplace=True)\n",
        "\n",
        "        frames.append(df)\n",
        "\n",
        "    return (pd.concat(frames, ignore_index=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sit5iHT_0UWl"
      },
      "source": [
        "def reshape_dataframe(*data: np.ndarray):\n",
        "    # This function adds an extra dimension which is necessary in the LSTM\n",
        "    arr = []\n",
        "    for d in data:\n",
        "        arr.append(np.reshape(np.array(d), (d.shape[0], 1, d.shape[1])))\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1buCiuYoliN"
      },
      "source": [
        "def get_countries_sortedby_cases(valid_countries, df_grouped):\n",
        "  # A dictionary of all countries\n",
        "  dict_countries = Counter(valid_countries)\n",
        "\n",
        "  for country in dict_countries.keys():\n",
        "    dict_countries[country] = df_grouped.get_group(country)['cases'].sum()\n",
        "\n",
        "  # Sorting countries based on number of cases\n",
        "  countries_sortedby_cases = sorted(dict_countries.items(), key=lambda dict_countries: dict_countries[1], reverse=True)\n",
        "\n",
        "  # Creating dataframe \n",
        "  df_countries_sortedbycases = pd.DataFrame.from_dict(dict(countries_sortedby_cases), orient='index', columns=['Total Cases'])\n",
        "  \n",
        "  return df_countries_sortedbycases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvuRHEVArNJ0"
      },
      "source": [
        "# Getting a list of valid countries\n",
        "def get_countries_with_valid_size(df):\n",
        "  total_countries = list(df_grouped.groups.keys())\n",
        "\n",
        "  # A list for countries with required datasize\n",
        "  valid_countries = []\n",
        "\n",
        "  # List of countries with more than 230 records. Because, max training size = 150, lags removed = 50, prediction = 30.\n",
        "  for country in total_countries:\n",
        "    if len(df_grouped.get_group(country)) >= 230:\n",
        "      valid_countries.append(country)\n",
        "\n",
        "  return valid_countries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tqHxnYhwsbk"
      },
      "source": [
        "def preprocess_dataset(df):  \n",
        "  # Selecting required features\n",
        "  df= df[['dateRep','cases','countriesAndTerritories']]\n",
        "\n",
        "  # Rename features\n",
        "  df.rename(columns={'countriesAndTerritories':'country', 'dateRep':'date'}, inplace=True)\n",
        "\n",
        "  # Convert to date, sort and set index\n",
        "  df = sortby_date_and_set_index(df)\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6nW5zkHuFLn"
      },
      "source": [
        "# Calculating maximum of dataframe for every pretrain size\n",
        "def calc_max_of_pretrain_days(pretrain_days,df)->list:\n",
        "  max_of_pretrain_days = []\n",
        "  \n",
        "  for day in pretrain_days:\n",
        "    df_subset = create_subset(df,day)\n",
        "    max_of_pretrain_days.append(df_subset['cases'].max())\n",
        "  \n",
        "  return max_of_pretrain_days"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze3Ju2mOFltA"
      },
      "source": [
        "def display_scores(results):\n",
        "  #print(f'_________________________________{country}____________________________________________')\n",
        "  df_MAE = get_metric_with_mean(results,'MAE' )\n",
        "  df_RMSE = get_metric_with_mean(results,'RMSE')\n",
        "  df_MAPE = get_metric_with_mean(results,'MAPE')\n",
        "  print('MAE Score')\n",
        "  print(df_MAE.to_string())\n",
        "  print('-----------------------------------------------------------------------------------')\n",
        "  print('RMSE Score')\n",
        "  print(df_RMSE.to_string())\n",
        "  print('-----------------------------------------------------------------------------------')\n",
        "  print('MAPE Score')\n",
        "  print(df_MAPE.to_string())\n",
        "  print('\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZbb27ZOpQmT"
      },
      "source": [
        "## Alternate Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SKzw1NdWenN"
      },
      "source": [
        "def get_alternate_batch_records_idx(batch_size,total_records): \n",
        "  total_batches = total_records//batch_size\n",
        "  current_batch=1\n",
        "  start_idx = 0\n",
        "  end_idx = batch_size\n",
        "  idx_list = []\n",
        "  \n",
        "  while current_batch <= total_batches:\n",
        "    if current_batch%2!=0:\n",
        "      idx_list.extend([x for x in range(start_idx,end_idx)])\n",
        "      start_idx = idx_list[-1]+(batch_size+1)\n",
        "      end_idx = start_idx + batch_size\n",
        "    current_batch += 1\n",
        "\n",
        "  return idx_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1Sdy4o7oufO"
      },
      "source": [
        "def create_alternate_batch_subset(df,days,batch_size):\n",
        "  df_grouped = df.groupby('country')\n",
        "  countries = df['country'].unique()\n",
        "  frame1,frame2 = [],[]\n",
        "\n",
        "  for country in countries:\n",
        "    df_cur_country = df_grouped.get_group(country)\n",
        "\n",
        "    df1 = df_cur_country.iloc[0:days//2]\n",
        "    df2 = df_cur_country.iloc[days:days+30]  # Adding 30 for a testing batch that is one month ahead\n",
        "    \n",
        "    # Selecting alternate batches\n",
        "    idx = get_alternate_batch_records_idx(batch_size,total_records=len(df2))\n",
        "    df2 = df2.iloc[idx]\n",
        "\n",
        "    # Appending dataframes\n",
        "    frame1.append(df1)\n",
        "    frame2.append(df2)\n",
        "\n",
        "  r1 = pd.concat(frame1, ignore_index=True)\n",
        "  r2 = pd.concat(frame2, ignore_index=True)\n",
        "  r = r1.append(r2, ignore_index=True)\n",
        "  \n",
        "  return (r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPViJY9uDZ0q"
      },
      "source": [
        "## Incremental Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9DZn8PjUter"
      },
      "source": [
        "def instantiate_regressors():\n",
        "  ht_reg = HoeffdingTreeRegressor()\n",
        "  hat_reg = HoeffdingAdaptiveTreeRegressor()\n",
        "  arf_reg = AdaptiveRandomForestRegressor()\n",
        "  pa_reg = PassiveAggressiveRegressor(max_iter=1, random_state=0, tol=1e-3)\n",
        "\n",
        "  model = [ht_reg, hat_reg, arf_reg, pa_reg]\n",
        "  model_names = ['HT_Reg', 'HAT_Reg', 'ARF_Reg', 'PA_Reg']\n",
        "\n",
        "  return model, model_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NwhtdOBXBUy"
      },
      "source": [
        "def get_error_scores_per_model(evaluator, mdl_evaluation_scores)-> pd.DataFrame:\n",
        "  \n",
        "  for i in range(len(evaluator.model_names)):\n",
        "    # Desired error metrics\n",
        "    mse = evaluator.mean_eval_measurements[i].get_mean_square_error()\n",
        "    mae = evaluator.mean_eval_measurements[i].get_average_error()\n",
        "    mape = evaluator.mean_eval_measurements[i].get_mean_absolute_percentage_error()\n",
        "    rmse = sqrt(mse)\n",
        "\n",
        "    # Dictionary of errors per model\n",
        "    mdl_evaluation_scores[str(evaluator.model_names[i])] = [rmse, mae, mape]\n",
        "\n",
        "  return(pd.DataFrame(mdl_evaluation_scores))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSq_iuypN2BE"
      },
      "source": [
        "def get_running_time_per_model_incremental_learner(evaluator,day):\n",
        "    cols = ['PretrainDays']  # Adding pretrain as first column\n",
        "    cols += evaluator.model_names  # Adding remaining columns of different algorithm\n",
        "    running_time = []\n",
        "    running_time.append(day)\n",
        "    for i in range(len(evaluator.model_names)):\n",
        "        running_time.append(evaluator.running_time_measurements[i]._total_time)\n",
        "\n",
        "    return (pd.DataFrame([running_time],columns=cols))  # Passing running_time as a list of list to insert it as a row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VbOOY4WfUzT"
      },
      "source": [
        "def display_countrywise_scores(country,df_error_metric):\n",
        "  print(f'_________________________________{country}____________________________________________')\n",
        "  print(df_error_metric.to_string())\n",
        "  print('\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC8vD-Dl3uNs"
      },
      "source": [
        "# Create a dataframe of all countries with pre-train size = pretrain days and test&train size = pretrain days\n",
        "def create_subset(result,days):\n",
        "  result_grouped = result.groupby('country')\n",
        "  countries = result['country'].unique()\n",
        "  frame1,frame2 = [],[]\n",
        "  for country in countries:\n",
        "    df = result_grouped.get_group(country) \n",
        "    df1 = df.iloc[0:days]\n",
        "    df2 = df.iloc[days:days+30]\n",
        "    frame1.append(df1)\n",
        "    frame2.append(df2)\n",
        "\n",
        "  r1 = pd.concat(frame1, ignore_index=True)\n",
        "  r2 = pd.concat(frame2, ignore_index=True)\n",
        "  r = r1.append(r2, ignore_index=True)\n",
        "  \n",
        "  return (r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGzbZRfVDc-c"
      },
      "source": [
        "## Static Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKVRq7O9onX7"
      },
      "source": [
        "def mean_absolute_percentage_error(actual, predicted):\n",
        "    \"\"\"\n",
        "    Mean absolute percentage error (MAPE).\n",
        "    :return error\n",
        "    \"\"\"\n",
        "    actual =  np.array(actual) \n",
        "    predicted = np.array(predicted) \n",
        "\n",
        "    mask = actual != 0\n",
        "    return (np.fabs(actual - predicted) / np.fabs(actual))[mask].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olpCsifni4p-"
      },
      "source": [
        "def get_scores(y_true, model_predictions, days):\n",
        "    mdl_evaluation_scores = {}\n",
        "    mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE', 'MAE', 'MAPE']\n",
        "    mdl_evaluation_scores['PretrainDays'] = [days] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "\n",
        "    for model in model_predictions:\n",
        "        y_pred = model_predictions[model]\n",
        "        if model == 'LSTM':\n",
        "            rmse = mean_squared_error(y_true[:, np.newaxis], y_pred, squared=False)\n",
        "            mae = mean_absolute_error(y_true[:, np.newaxis], y_pred)\n",
        "            mape = mean_absolute_percentage_error(y_true[:, np.newaxis], y_pred)\n",
        "        else:\n",
        "            rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "            mae = mean_absolute_error(y_true, y_pred)\n",
        "            mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "\n",
        "        mdl_evaluation_scores[model] = [rmse, mae, mape]\n",
        "    return pd.DataFrame(mdl_evaluation_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBKHhy5I0sWy"
      },
      "source": [
        "def get_running_time_per_model_static_learner(model_predictions,total_execution_time):\n",
        "    cols = ['PretrainDays']\n",
        "    cols += model_predictions.keys()\n",
        "    return pd.DataFrame(total_execution_time, columns=cols)\n",
        "\n",
        "\n",
        "def measure(wrapped_func):\n",
        "    @wraps(wrapped_func)\n",
        "    def _time_it(*args, **kwargs):\n",
        "        start = pc_timer()\n",
        "        try:\n",
        "            model_predictions = wrapped_func(*args, **kwargs)\n",
        "        finally:\n",
        "            end_ = pc_timer() - start\n",
        "            return model_predictions, end_\n",
        "    return _time_it\n",
        "\n",
        "\n",
        "@measure\n",
        "def train_test_model(regressor, X_train, y_train, X_test):\n",
        "    regressor.fit(X_train, y_train)\n",
        "    return regressor.predict(X_test)\n",
        "\n",
        "\n",
        "@measure\n",
        "def train_test_lstm(regressor, X_train_lstm, y_train, X_val_lstm, y_val, X_test_lstm, patience, epochs, batch_size_lstm):\n",
        "    regressor.compile(loss='mae', optimizer='adagrad', metrics=['mse', 'mae'])\n",
        "\n",
        "    history = regressor.fit(\n",
        "        X_train_lstm,\n",
        "        y_train,\n",
        "        validation_data=(X_val_lstm, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size_lstm,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss',\n",
        "                                 mode='min',\n",
        "                                 patience=patience)])\n",
        "\n",
        "    return regressor.predict(X_test_lstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB-GhumBoRRH"
      },
      "source": [
        "def define_lstm_model(x_train_lstm, layers, activations, patience):\n",
        "    # Start defining the model\n",
        "    input_shape = x_train_lstm.shape\n",
        "\n",
        "    # Definining model first with LSTM n layers\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(layers[0], input_shape=input_shape[1:], activation=activations[0], return_sequences=True))\n",
        "\n",
        "    # Adding middle layers\n",
        "    for l in range(1, len(layers)-1):\n",
        "      model.add(LSTM(layers[l], activation=activations[l], return_sequences=True))\n",
        "      model.add(Dropout(0.2))\n",
        "\n",
        "    # Add last Dense and LSTMs layers\n",
        "    if len(layers) > 1:\n",
        "      model.add(Dense(layers[-1], activation=activations[-1]))\n",
        "      model.add(Dropout(0.2))\n",
        "      model.add(LSTM(layers[-1], activation=activations[-1]))\n",
        "\n",
        "    model.add(Dense(1))  # output layer. Since we have only 1 output value\n",
        "    # End defining model\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjnk6w9_cqG6"
      },
      "source": [
        "def normalize_dataset(*dataframes):\n",
        "    arr = []\n",
        "    for df in dataframes:\n",
        "        arr.append(StandardScaler().fit_transform(df))\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GQGxLLNw1z5"
      },
      "source": [
        "def get_validation_set(df_train, batch_size=10):  # Updated Now\n",
        "    '''\n",
        "    lst_idx = -1\n",
        "    total_batches = len(df_train) // batch_size\n",
        "    train_set, val_set = [], []\n",
        "\n",
        "    for cur_batch in range(total_batches):\n",
        "        start = lst_idx + 1\n",
        "        end = start + batch_size\n",
        "        if cur_batch % 2 == 0:\n",
        "            train_set.append(df_train.iloc[start:end])\n",
        "        else:\n",
        "            val_set.append(df_train.iloc[start:end])\n",
        "\n",
        "        lst_idx = end - 1  # adjusting last index because we add 1 in starting\n",
        "    '''\n",
        "    train_set, val_set = [], []\n",
        "    countries = df_train['country'].unique()\n",
        "    for country in countries:\n",
        "        train_set.append(df_train[df_train['country'] == country].iloc[:-batch_size, :])\n",
        "        val_set.append(df_train[df_train['country'] == country].iloc[-batch_size:])\n",
        "    return pd.concat(train_set, ignore_index=True), pd.concat(val_set, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMyZ5jcy_m6j"
      },
      "source": [
        "# Experiment 1\n",
        "Training and testing with five countries individually. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDnpy-TycyY9"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "* cases(t): Number of cases on current day(Column='cases') \n",
        "\n",
        "* cases(t-n): Number of cases 'n' days before current day 't'\n",
        "\n",
        "* 30 day gap: Training from day number t-89 to t-39(50 days). Then a gap of 30 days and then creating target by averaging t to t-9(10 Days).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "SeJJB0sdNnIq",
        "outputId": "7145ffbf-caab-4f18-b7ae-5768defb26b5"
      },
      "source": [
        "# Sample set for understanding dataset\n",
        "sample_df = pd.read_csv(f'{csv_processed_path}/United_States_of_America.csv')\n",
        "sample_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>day_no</th>\n",
              "      <th>country</th>\n",
              "      <th>cases</th>\n",
              "      <th>cases_t-89</th>\n",
              "      <th>cases_t-88</th>\n",
              "      <th>cases_t-87</th>\n",
              "      <th>cases_t-86</th>\n",
              "      <th>cases_t-85</th>\n",
              "      <th>cases_t-84</th>\n",
              "      <th>cases_t-83</th>\n",
              "      <th>cases_t-82</th>\n",
              "      <th>cases_t-81</th>\n",
              "      <th>cases_t-80</th>\n",
              "      <th>cases_t-79</th>\n",
              "      <th>cases_t-78</th>\n",
              "      <th>cases_t-77</th>\n",
              "      <th>cases_t-76</th>\n",
              "      <th>cases_t-75</th>\n",
              "      <th>cases_t-74</th>\n",
              "      <th>cases_t-73</th>\n",
              "      <th>cases_t-72</th>\n",
              "      <th>cases_t-71</th>\n",
              "      <th>cases_t-70</th>\n",
              "      <th>cases_t-69</th>\n",
              "      <th>cases_t-68</th>\n",
              "      <th>cases_t-67</th>\n",
              "      <th>cases_t-66</th>\n",
              "      <th>cases_t-65</th>\n",
              "      <th>cases_t-64</th>\n",
              "      <th>cases_t-63</th>\n",
              "      <th>cases_t-62</th>\n",
              "      <th>cases_t-61</th>\n",
              "      <th>cases_t-60</th>\n",
              "      <th>cases_t-59</th>\n",
              "      <th>cases_t-58</th>\n",
              "      <th>cases_t-57</th>\n",
              "      <th>cases_t-56</th>\n",
              "      <th>cases_t-55</th>\n",
              "      <th>cases_t-54</th>\n",
              "      <th>cases_t-53</th>\n",
              "      <th>cases_t-52</th>\n",
              "      <th>cases_t-51</th>\n",
              "      <th>cases_t-50</th>\n",
              "      <th>cases_t-49</th>\n",
              "      <th>cases_t-48</th>\n",
              "      <th>cases_t-47</th>\n",
              "      <th>cases_t-46</th>\n",
              "      <th>cases_t-45</th>\n",
              "      <th>cases_t-44</th>\n",
              "      <th>cases_t-43</th>\n",
              "      <th>cases_t-42</th>\n",
              "      <th>cases_t-41</th>\n",
              "      <th>cases_t-40</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>90</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>19979</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>11525.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-30</td>\n",
              "      <td>91</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>18360</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>12877.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-03-31</td>\n",
              "      <td>92</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>21595</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>14499.600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-04-01</td>\n",
              "      <td>93</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>24998</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>16287.100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-04-02</td>\n",
              "      <td>94</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>27103</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>19.000</td>\n",
              "      <td>18151.500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         date  day_no  ... cases_t-40    target\n",
              "0  2020-03-29      90  ...      0.000 11525.000\n",
              "1  2020-03-30      91  ...      0.000 12877.500\n",
              "2  2020-03-31      92  ...      0.000 14499.600\n",
              "3  2020-04-01      93  ...      1.000 16287.100\n",
              "4  2020-04-02      94  ...     19.000 18151.500\n",
              "\n",
              "[5 rows x 55 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJiGVxO_hsNv"
      },
      "source": [
        "## Incremental Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZrg_v5CLReb"
      },
      "source": [
        "def reset_evaluator(evaluator):  # Added Now\n",
        "  for j in range(evaluator.n_models):\n",
        "      evaluator.mean_eval_measurements[j].reset()\n",
        "      evaluator.current_eval_measurements[j].reset()\n",
        "  return evaluator\n",
        "\n",
        "\n",
        "def update_incremental_metrics(evaluator, y, prediction):  # Added Now\n",
        "  for j in range(evaluator.n_models):\n",
        "    for i in range(len(prediction[0])):\n",
        "      evaluator.mean_eval_measurements[j].add_result(y[i], prediction[j][i])\n",
        "      evaluator.current_eval_measurements[j].add_result(y[i], prediction[j][i])\n",
        "\n",
        "    # Adding result manually causes y_true_vector to have a objects inserted like array([123.45]) in a list.\n",
        "    # For calculating metrics we have to convert them into flat list.\n",
        "    evaluator.mean_eval_measurements[j].y_true_vector = np.array(evaluator.mean_eval_measurements[j].y_true_vector).flatten().tolist()\n",
        "    evaluator.current_eval_measurements[j].y_true_vector = np.array(evaluator.current_eval_measurements[j].y_true_vector).flatten().tolist()\n",
        "  return evaluator\n",
        "\n",
        "\n",
        "def get_error_scores_per_model(evaluator, mdl_evaluation_scores, inc_alt_batches=False) -> pd.DataFrame:\n",
        "  for i in range(len(evaluator.model_names)):\n",
        "    # Desired error metrics\n",
        "    mse = evaluator.mean_eval_measurements[i].get_mean_square_error()\n",
        "    mae = evaluator.mean_eval_measurements[i].get_average_error()\n",
        "    if not inc_alt_batches:\n",
        "      mae = mae[0]  # get_average_error() is returning a List instead of single value.\n",
        "    mape = evaluator.mean_eval_measurements[i].get_mean_absolute_percentage_error()\n",
        "    rmse = sqrt(mse)\n",
        "\n",
        "    # Dictionary of errors per model\n",
        "    mdl_evaluation_scores[str(evaluator.model_names[i])] = [rmse, mae, mape]\n",
        "  return (pd.DataFrame(mdl_evaluation_scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "pw3R1IKXR05k",
        "outputId": "9944fa99-d813-45f2-ee3e-3a77b122f70a"
      },
      "source": [
        "\"\"\"\n",
        "# Old\n",
        "\n",
        "def scikit_multiflow(df, pretrain_days):\n",
        "\n",
        "  # Creating a stream from dataframe\n",
        "  stream = DataStream(np.array(df.iloc[:,4:-1]), y=np.array(df.iloc[:,-1])) # Selecting features x=[t-89:t-39] and y=[target]. TODO: Drop columns with name \n",
        "\n",
        "  model, model_names = instantiate_regressors()\n",
        "\n",
        "  frames, running_time_frames = [], []\n",
        "\n",
        "  # Setup the evaluator\n",
        "  for day in pretrain_days:\n",
        "\n",
        "      pretrain_days = day\n",
        "      max_samples = pretrain_days + 30  #Testing on set one month ahead only\n",
        "\n",
        "      '''evaluator = EvaluatePrequential(show_plot=False,\n",
        "                                    pretrain_size=day,\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'], \n",
        "                                    max_samples=max_samples)'''\n",
        "      \n",
        "      evaluator = EvaluatePrequential(show_plot=False,\n",
        "                                    pretrain_size=day-1,\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'], \n",
        "                                    max_samples=day)\n",
        "\n",
        "\n",
        "      # Run evaluation\n",
        "      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
        "\n",
        "      # Dictionary to store each iteration error scores\n",
        "      mdl_evaluation_scores = {}\n",
        "\n",
        "      # Adding Evaluation Measurements and pretraining days\n",
        "      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE']\n",
        "      mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\n",
        "\n",
        "      # Errors of each model on a specific pre-train days\n",
        "      frames.append(mdl_evaluation_df)\n",
        "\n",
        "      # Run time for each algorithm\n",
        "      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
        "\n",
        "  # Final Run Time DataFrame\n",
        "  running_time_df = pd.concat(running_time_frames,ignore_index=True)\n",
        "\n",
        "  # Final Evaluation Score Dataframe\n",
        "  evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
        "  return evaluation_scores_df, running_time_df\n",
        "\n",
        "for country in countries:\n",
        "  # Read each country  \n",
        "  df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\n",
        "\n",
        "  # Get evaluation scores and running time for country\n",
        "  evaluation_scores_df, running_time_df = scikit_multiflow(df_country,pretrain_days)\n",
        "\n",
        "  # Appending evaluation scores and runtime for each country\n",
        "  results_incremental.append(evaluation_scores_df)\n",
        "  results_runtime_incremental.append(running_time_df)\n",
        "\n",
        "  # Get max of each pretrain subset and for each country dataset\n",
        "  max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days,df_country))\n",
        "  max_cases_per_country.append(df_country['cases'].max())\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Old\\n\\ndef scikit_multiflow(df, pretrain_days):\\n\\n  # Creating a stream from dataframe\\n  stream = DataStream(np.array(df.iloc[:,4:-1]), y=np.array(df.iloc[:,-1])) # Selecting features x=[t-89:t-39] and y=[target]. TODO: Drop columns with name \\n\\n  model, model_names = instantiate_regressors()\\n\\n  frames, running_time_frames = [], []\\n\\n  # Setup the evaluator\\n  for day in pretrain_days:\\n\\n      pretrain_days = day\\n      max_samples = pretrain_days + 30  #Testing on set one month ahead only\\n\\n      '''evaluator = EvaluatePrequential(show_plot=False,\\n                                    pretrain_size=day,\\n                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'], \\n                                    max_samples=max_samples)'''\\n      \\n      evaluator = EvaluatePrequential(show_plot=False,\\n                                    pretrain_size=day-1,\\n                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'], \\n                                    max_samples=day)\\n\\n\\n      # Run evaluation\\n      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\\n\\n      # Dictionary to store each iteration error scores\\n      mdl_evaluation_scores = {}\\n\\n      # Adding Evaluation Measurements and pretraining days\\n      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE']\\n      mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\\n      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\\n\\n      # Errors of each model on a specific pre-train days\\n      frames.append(mdl_evaluation_df)\\n\\n      # Run time for each algorithm\\n      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\\n\\n  # Final Run Time DataFrame\\n  running_time_df = pd.concat(running_time_frames,ignore_index=True)\\n\\n  # Final Evaluation Score Dataframe\\n  evaluation_scores_df = pd.concat(frames, ignore_index=True)\\n  return evaluation_scores_df, running_time_df\\n\\nfor country in countries:\\n  # Read each country  \\n  df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\\n\\n  # Get evaluation scores and running time for country\\n  evaluation_scores_df, running_time_df = scikit_multiflow(df_country,pretrain_days)\\n\\n  # Appending evaluation scores and runtime for each country\\n  results_incremental.append(evaluation_scores_df)\\n  results_runtime_incremental.append(running_time_df)\\n\\n  # Get max of each pretrain subset and for each country dataset\\n  max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days,df_country))\\n  max_cases_per_country.append(df_country['cases'].max())\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ3htjYaL-1o"
      },
      "source": [
        "def scikit_multiflow(df, pretrain_days, country):  # Added Country in parameter\n",
        "    # Creating a stream from dataframe\n",
        "    stream = DataStream(np.array(df.iloc[:, 4:-1]), y=np.array(df.iloc[:, -1]))  # Selecting features x=[t-89:t-39] and y=[target].\n",
        "\n",
        "    model, model_names = instantiate_regressors()\n",
        "\n",
        "    frames, running_time_frames = [], []\n",
        "\n",
        "    united_dataframe = []  # Added Now\n",
        "\n",
        "    # Setup the evaluator\n",
        "    for day in pretrain_days:\n",
        "        pretrain_days = day\n",
        "        # max_samples = pretrain_days + 30  # Training and then testing on set one month ahead only\n",
        "        max_samples = pretrain_days + 1\n",
        "        testing_samples_size = 30\n",
        "\n",
        "        evaluator = EvaluatePrequential(show_plot=False,\n",
        "                                        pretrain_size=pretrain_days,\n",
        "                                        metrics=['mean_square_error', 'mean_absolute_error',\n",
        "                                                 'mean_absolute_percentage_error'],\n",
        "                                        max_samples=max_samples)\n",
        "\n",
        "        # Run evaluation\n",
        "        evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
        "\n",
        "        X = stream.X[pretrain_days: pretrain_days + testing_samples_size]\n",
        "        y = stream.y[pretrain_days: pretrain_days + testing_samples_size]\n",
        "        target_dates = df.iloc[pretrain_days: pretrain_days +testing_samples_size, 0]  # Added Now\n",
        "\n",
        "        prediction = evaluator.predict(X)\n",
        "\n",
        "        # Since we add one extra sample, reset the evaluator\n",
        "        evaluator = reset_evaluator(evaluator)\n",
        "\n",
        "        evaluator = update_incremental_metrics(evaluator, y, prediction)\n",
        "\n",
        "        united_dataframe.append(unit_incremental_df(country, evaluator, target_dates, pretrain_days))  # Added now\n",
        "\n",
        "        # Dictionary to store each iteration error scores\n",
        "        mdl_evaluation_scores = {}\n",
        "\n",
        "        # Adding Evaluation Measurements and pretraining days\n",
        "        mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE', 'MAE', 'MAPE']\n",
        "        mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "        mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\n",
        "\n",
        "        # Errors of each model on a specific pre-train days\n",
        "        frames.append(mdl_evaluation_df)\n",
        "\n",
        "        # Run time for each algorithm\n",
        "        running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
        "\n",
        "    # Final Run Time DataFrame\n",
        "    running_time_df = pd.concat(running_time_frames, ignore_index=True)\n",
        "\n",
        "    # Final Evaluation Score Dataframe\n",
        "    evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    united_dataframe = pd.concat(united_dataframe, ignore_index=True)  # Added Now\n",
        "    return evaluation_scores_df, running_time_df, united_dataframe  # Added united_dataframe in return statement\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUw_30g9T4fS",
        "outputId": "78329270-21df-49a3-cfb0-a01c7da82054"
      },
      "source": [
        "# Training all countries\n",
        "results_incremental = []\n",
        "results_runtime_incremental = []\n",
        "max_of_pretrain_per_country = []\n",
        "max_cases_per_country = []\n",
        "\n",
        "for country in countries:\n",
        "    # Read each country\n",
        "    df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\n",
        "\n",
        "    # Get evaluation scores and running time for country\n",
        "    evaluation_scores_df, running_time_df, united_dataframe = scikit_multiflow(df_country,pretrain_days, country)\n",
        "\n",
        "    save_united_df(united_dataframe, exp1_inc_united_df_path, country=country)\n",
        "\n",
        "    # Appending evaluation scores and runtime for each country\n",
        "    results_incremental.append(evaluation_scores_df)\n",
        "\n",
        "    results_runtime_incremental.append(running_time_df)\n",
        "\n",
        "    # Get max of each pretrain subset and for each country dataset\n",
        "    max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days,df_country))\n",
        "    max_cases_per_country.append(df_country['cases'].max())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.89s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.9070\n",
            "HT_Reg - MAPE          : 0.0019\n",
            "HT_Reg - MAE          : 0.952376\n",
            "HAT_Reg - MSE          : 1.1017\n",
            "HAT_Reg - MAPE          : 0.0021\n",
            "HAT_Reg - MAE          : 1.049625\n",
            "ARF_Reg - MSE          : 3459.9496\n",
            "ARF_Reg - MAPE          : 0.1167\n",
            "ARF_Reg - MAE          : 58.821337\n",
            "PA_Reg - MSE          : 13628.7425\n",
            "PA_Reg - MAPE          : 0.2317\n",
            "PA_Reg - MAE          : 116.742205\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.29s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1549611.7514\n",
            "HT_Reg - MAPE          : 0.6315\n",
            "HT_Reg - MAE          : 1244.834026\n",
            "HAT_Reg - MSE          : 1549536.2037\n",
            "HAT_Reg - MAPE          : 0.6315\n",
            "HAT_Reg - MAE          : 1244.803681\n",
            "ARF_Reg - MSE          : 1658129.9661\n",
            "ARF_Reg - MAPE          : 0.6532\n",
            "ARF_Reg - MAE          : 1287.683954\n",
            "PA_Reg - MSE          : 2674858.3423\n",
            "PA_Reg - MAPE          : 0.8297\n",
            "PA_Reg - MAE          : 1635.499417\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.03s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 43992843.3609\n",
            "HT_Reg - MAPE          : 0.7822\n",
            "HT_Reg - MAE          : 6632.710107\n",
            "HAT_Reg - MSE          : 43992843.3621\n",
            "HAT_Reg - MAPE          : 0.7822\n",
            "HAT_Reg - MAE          : 6632.710107\n",
            "ARF_Reg - MSE          : 27729290.8822\n",
            "ARF_Reg - MAPE          : 0.6210\n",
            "ARF_Reg - MAE          : 5265.860887\n",
            "PA_Reg - MSE          : 168392979.0151\n",
            "PA_Reg - MAPE          : 1.5303\n",
            "PA_Reg - MAE          : 12976.632037\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.24s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 7995606.0391\n",
            "HT_Reg - MAPE          : 0.5315\n",
            "HT_Reg - MAE          : 2827.650268\n",
            "HAT_Reg - MSE          : 7995606.0390\n",
            "HAT_Reg - MAPE          : 0.5315\n",
            "HAT_Reg - MAE          : 2827.650268\n",
            "ARF_Reg - MSE          : 943.4255\n",
            "ARF_Reg - MAPE          : 0.0058\n",
            "ARF_Reg - MAE          : 30.715232\n",
            "PA_Reg - MSE          : 3636178558.0887\n",
            "PA_Reg - MAPE          : 11.3349\n",
            "PA_Reg - MAE          : 60300.734308\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.51s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 42.1366\n",
            "HT_Reg - MAPE          : 0.0201\n",
            "HT_Reg - MAE          : 6.491271\n",
            "HAT_Reg - MSE          : 264.3550\n",
            "HAT_Reg - MAPE          : 0.0503\n",
            "HAT_Reg - MAE          : 16.258999\n",
            "ARF_Reg - MSE          : 1124.7859\n",
            "ARF_Reg - MAPE          : 0.1038\n",
            "ARF_Reg - MAE          : 33.537828\n",
            "PA_Reg - MSE          : 26693.2477\n",
            "PA_Reg - MAPE          : 0.5055\n",
            "PA_Reg - MAE          : 163.380683\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.10s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1063.4147\n",
            "HT_Reg - MAPE          : 0.1821\n",
            "HT_Reg - MAE          : 32.610040\n",
            "HAT_Reg - MSE          : 34.4989\n",
            "HAT_Reg - MAPE          : 0.0328\n",
            "HAT_Reg - MAE          : 5.873579\n",
            "ARF_Reg - MSE          : 33495.0854\n",
            "ARF_Reg - MAPE          : 1.0219\n",
            "ARF_Reg - MAE          : 183.016626\n",
            "PA_Reg - MSE          : 7167.5058\n",
            "PA_Reg - MAPE          : 0.4727\n",
            "PA_Reg - MAE          : 84.661123\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.77s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 614.6817\n",
            "HT_Reg - MAPE          : 0.0767\n",
            "HT_Reg - MAE          : 24.792775\n",
            "HAT_Reg - MSE          : 156.3900\n",
            "HAT_Reg - MAPE          : 0.0387\n",
            "HAT_Reg - MAE          : 12.505598\n",
            "ARF_Reg - MSE          : 2294.6662\n",
            "ARF_Reg - MAPE          : 0.1482\n",
            "ARF_Reg - MAE          : 47.902675\n",
            "PA_Reg - MSE          : 3500895.1081\n",
            "PA_Reg - MAPE          : 5.7874\n",
            "PA_Reg - MAE          : 1871.067906\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.04s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 301732.7453\n",
            "HT_Reg - MAPE          : 0.5301\n",
            "HT_Reg - MAE          : 549.302053\n",
            "HAT_Reg - MSE          : 301695.3891\n",
            "HAT_Reg - MAPE          : 0.5301\n",
            "HAT_Reg - MAE          : 549.268048\n",
            "ARF_Reg - MSE          : 309308.7990\n",
            "ARF_Reg - MAPE          : 0.5367\n",
            "ARF_Reg - MAE          : 556.155373\n",
            "PA_Reg - MSE          : 17507821.7739\n",
            "PA_Reg - MAPE          : 4.0381\n",
            "PA_Reg - MAE          : 4184.234909\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.92s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 99518.4623\n",
            "HT_Reg - MAPE          : 0.2639\n",
            "HT_Reg - MAE          : 315.465469\n",
            "HAT_Reg - MSE          : 99518.4624\n",
            "HAT_Reg - MAPE          : 0.2639\n",
            "HAT_Reg - MAE          : 315.465469\n",
            "ARF_Reg - MSE          : 2047.7638\n",
            "ARF_Reg - MAPE          : 0.0378\n",
            "ARF_Reg - MAE          : 45.252224\n",
            "PA_Reg - MSE          : 125032402.8435\n",
            "PA_Reg - MAPE          : 9.3524\n",
            "PA_Reg - MAE          : 11181.788893\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.38s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 51.2777\n",
            "HT_Reg - MAPE          : 0.0050\n",
            "HT_Reg - MAE          : 7.160845\n",
            "HAT_Reg - MSE          : 51.2777\n",
            "HAT_Reg - MAPE          : 0.0050\n",
            "HAT_Reg - MAE          : 7.160845\n",
            "ARF_Reg - MSE          : 270166.0386\n",
            "ARF_Reg - MAPE          : 0.3654\n",
            "ARF_Reg - MAE          : 519.774988\n",
            "PA_Reg - MSE          : 6135245.3746\n",
            "PA_Reg - MAPE          : 1.7414\n",
            "PA_Reg - MAE          : 2476.942748\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.49s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1345381.6046\n",
            "HT_Reg - MAPE          : 0.2902\n",
            "HT_Reg - MAE          : 1159.905860\n",
            "HAT_Reg - MSE          : 1345381.6046\n",
            "HAT_Reg - MAPE          : 0.2902\n",
            "HAT_Reg - MAE          : 1159.905860\n",
            "ARF_Reg - MSE          : 1026086.6363\n",
            "ARF_Reg - MAPE          : 0.2534\n",
            "ARF_Reg - MAE          : 1012.959346\n",
            "PA_Reg - MSE          : 3032602.0846\n",
            "PA_Reg - MAPE          : 0.4356\n",
            "PA_Reg - MAE          : 1741.436787\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [8.08s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3488961.9035\n",
            "HT_Reg - MAPE          : 0.2333\n",
            "HT_Reg - MAE          : 1867.876308\n",
            "HAT_Reg - MSE          : 3488961.9035\n",
            "HAT_Reg - MAPE          : 0.2333\n",
            "HAT_Reg - MAE          : 1867.876308\n",
            "ARF_Reg - MSE          : 2484180.6618\n",
            "ARF_Reg - MAPE          : 0.1969\n",
            "ARF_Reg - MAE          : 1576.128377\n",
            "PA_Reg - MSE          : 274999330.9085\n",
            "PA_Reg - MAPE          : 2.0713\n",
            "PA_Reg - MAE          : 16583.103778\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.55s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.2886\n",
            "HT_Reg - MAPE          : 0.0025\n",
            "HT_Reg - MAE          : 0.537201\n",
            "HAT_Reg - MSE          : 618.1635\n",
            "HAT_Reg - MAPE          : 0.1144\n",
            "HAT_Reg - MAE          : 24.862894\n",
            "ARF_Reg - MSE          : 1.3763\n",
            "ARF_Reg - MAPE          : 0.0054\n",
            "ARF_Reg - MAE          : 1.173141\n",
            "PA_Reg - MSE          : 786871.9603\n",
            "PA_Reg - MAPE          : 4.0822\n",
            "PA_Reg - MAE          : 887.058037\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.15s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 6124.7415\n",
            "HT_Reg - MAPE          : 0.1682\n",
            "HT_Reg - MAE          : 78.260728\n",
            "HAT_Reg - MSE          : 5318.1579\n",
            "HAT_Reg - MAPE          : 0.1568\n",
            "HAT_Reg - MAE          : 72.925701\n",
            "ARF_Reg - MSE          : 3736.4112\n",
            "ARF_Reg - MAPE          : 0.1314\n",
            "ARF_Reg - MAE          : 61.126190\n",
            "PA_Reg - MSE          : 7490720.4898\n",
            "PA_Reg - MAPE          : 5.8833\n",
            "PA_Reg - MAE          : 2736.918064\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.73s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 54306.5431\n",
            "HT_Reg - MAPE          : 0.2872\n",
            "HT_Reg - MAE          : 233.037643\n",
            "HAT_Reg - MSE          : 54310.5550\n",
            "HAT_Reg - MAPE          : 0.2872\n",
            "HAT_Reg - MAE          : 233.046251\n",
            "ARF_Reg - MSE          : 55782.4610\n",
            "ARF_Reg - MAPE          : 0.2911\n",
            "ARF_Reg - MAE          : 236.183109\n",
            "PA_Reg - MSE          : 5765809.3801\n",
            "PA_Reg - MAPE          : 2.9593\n",
            "PA_Reg - MAE          : 2401.209983\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.58s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 406978.9799\n",
            "HT_Reg - MAPE          : 0.3116\n",
            "HT_Reg - MAE          : 637.949042\n",
            "HAT_Reg - MSE          : 406978.9801\n",
            "HAT_Reg - MAPE          : 0.3116\n",
            "HAT_Reg - MAE          : 637.949042\n",
            "ARF_Reg - MSE          : 365440.6399\n",
            "ARF_Reg - MAPE          : 0.2952\n",
            "ARF_Reg - MAE          : 604.516865\n",
            "PA_Reg - MSE          : 21139511.3509\n",
            "PA_Reg - MAPE          : 2.2454\n",
            "PA_Reg - MAE          : 4597.772434\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.23s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 154702.9825\n",
            "HT_Reg - MAPE          : 0.0999\n",
            "HT_Reg - MAE          : 393.323000\n",
            "HAT_Reg - MSE          : 154702.9825\n",
            "HAT_Reg - MAPE          : 0.0999\n",
            "HAT_Reg - MAE          : 393.323000\n",
            "ARF_Reg - MSE          : 19438.5976\n",
            "ARF_Reg - MAPE          : 0.0354\n",
            "ARF_Reg - MAE          : 139.422371\n",
            "PA_Reg - MSE          : 766295.0495\n",
            "PA_Reg - MAPE          : 0.2224\n",
            "PA_Reg - MAE          : 875.382802\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.94s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 7786500.1854\n",
            "HT_Reg - MAPE          : 1.0051\n",
            "HT_Reg - MAE          : 2790.430108\n",
            "HAT_Reg - MSE          : 7786500.1854\n",
            "HAT_Reg - MAPE          : 1.0051\n",
            "HAT_Reg - MAE          : 2790.430108\n",
            "ARF_Reg - MSE          : 11132059.0768\n",
            "ARF_Reg - MAPE          : 1.2018\n",
            "ARF_Reg - MAE          : 3336.474049\n",
            "PA_Reg - MSE          : 24432130.4386\n",
            "PA_Reg - MAPE          : 1.7805\n",
            "PA_Reg - MAE          : 4942.886853\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.87s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 43146713.8116\n",
            "HT_Reg - MAPE          : 3.5387\n",
            "HT_Reg - MAE          : 6568.615822\n",
            "HAT_Reg - MSE          : 43146713.8116\n",
            "HAT_Reg - MAPE          : 3.5387\n",
            "HAT_Reg - MAE          : 6568.615822\n",
            "ARF_Reg - MSE          : 7523556.4247\n",
            "ARF_Reg - MAPE          : 1.4777\n",
            "ARF_Reg - MAE          : 2742.910211\n",
            "PA_Reg - MSE          : 317208474.5880\n",
            "PA_Reg - MAPE          : 9.5951\n",
            "PA_Reg - MAE          : 17810.347402\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.86s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 19168.7400\n",
            "HT_Reg - MAPE          : 0.0911\n",
            "HT_Reg - MAE          : 138.451219\n",
            "HAT_Reg - MSE          : 19168.7400\n",
            "HAT_Reg - MAPE          : 0.0911\n",
            "HAT_Reg - MAE          : 138.451219\n",
            "ARF_Reg - MSE          : 4119.7855\n",
            "ARF_Reg - MAPE          : 0.0422\n",
            "ARF_Reg - MAE          : 64.185555\n",
            "PA_Reg - MSE          : 130966632.2011\n",
            "PA_Reg - MAPE          : 7.5290\n",
            "PA_Reg - MAE          : 11444.065370\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.49s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 17666.2563\n",
            "HT_Reg - MAPE          : 0.1421\n",
            "HT_Reg - MAE          : 132.914470\n",
            "HAT_Reg - MSE          : 21609.5147\n",
            "HAT_Reg - MAPE          : 0.1572\n",
            "HAT_Reg - MAE          : 147.001751\n",
            "ARF_Reg - MSE          : 270300.3211\n",
            "ARF_Reg - MAPE          : 0.5558\n",
            "ARF_Reg - MAE          : 519.904146\n",
            "PA_Reg - MSE          : 16543413.3573\n",
            "PA_Reg - MAPE          : 4.3483\n",
            "PA_Reg - MAE          : 4067.359507\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.09s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 440018.1339\n",
            "HT_Reg - MAPE          : 0.2768\n",
            "HT_Reg - MAE          : 663.338627\n",
            "HAT_Reg - MSE          : 138891.3579\n",
            "HAT_Reg - MAPE          : 0.1555\n",
            "HAT_Reg - MAE          : 372.681309\n",
            "ARF_Reg - MSE          : 7234.1071\n",
            "ARF_Reg - MAPE          : 0.0355\n",
            "ARF_Reg - MAE          : 85.053554\n",
            "PA_Reg - MSE          : 78513202.3210\n",
            "PA_Reg - MAPE          : 3.6980\n",
            "PA_Reg - MAE          : 8860.767592\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.75s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1752939.1286\n",
            "HT_Reg - MAPE          : 0.3606\n",
            "HT_Reg - MAE          : 1323.986076\n",
            "HAT_Reg - MSE          : 1746922.9229\n",
            "HAT_Reg - MAPE          : 0.3599\n",
            "HAT_Reg - MAE          : 1321.712118\n",
            "ARF_Reg - MSE          : 63384048.9581\n",
            "ARF_Reg - MAPE          : 2.1681\n",
            "ARF_Reg - MAE          : 7961.409986\n",
            "PA_Reg - MSE          : 1289795.3841\n",
            "PA_Reg - MAPE          : 0.3093\n",
            "PA_Reg - MAE          : 1135.691588\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.40s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 55897417.4195\n",
            "HT_Reg - MAPE          : 6.8736\n",
            "HT_Reg - MAE          : 7476.457545\n",
            "HAT_Reg - MSE          : 54826721.3741\n",
            "HAT_Reg - MAPE          : 6.8075\n",
            "HAT_Reg - MAE          : 7404.506829\n",
            "ARF_Reg - MSE          : 50868608.8295\n",
            "ARF_Reg - MAPE          : 6.5572\n",
            "ARF_Reg - MAE          : 7132.223274\n",
            "PA_Reg - MSE          : 3721001925.0202\n",
            "PA_Reg - MAPE          : 56.0817\n",
            "PA_Reg - MAE          : 61000.015779\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.15s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 11865.5551\n",
            "HT_Reg - MAPE          : 0.2556\n",
            "HT_Reg - MAE          : 108.929129\n",
            "HAT_Reg - MSE          : 12031.0423\n",
            "HAT_Reg - MAPE          : 0.2574\n",
            "HAT_Reg - MAE          : 109.686108\n",
            "ARF_Reg - MSE          : 38142208.5135\n",
            "ARF_Reg - MAPE          : 14.4941\n",
            "ARF_Reg - MAE          : 6175.937865\n",
            "PA_Reg - MSE          : 1565655898.0642\n",
            "PA_Reg - MAPE          : 92.8617\n",
            "PA_Reg - MAE          : 39568.369919\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.79s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 147855.2692\n",
            "HT_Reg - MAPE          : 0.5833\n",
            "HT_Reg - MAE          : 384.519530\n",
            "HAT_Reg - MSE          : 148055.3418\n",
            "HAT_Reg - MAPE          : 0.5837\n",
            "HAT_Reg - MAE          : 384.779602\n",
            "ARF_Reg - MSE          : 8886.2141\n",
            "ARF_Reg - MAPE          : 0.1430\n",
            "ARF_Reg - MAE          : 94.266718\n",
            "PA_Reg - MSE          : 16776245.7321\n",
            "PA_Reg - MAPE          : 6.2134\n",
            "PA_Reg - MAE          : 4095.881557\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.61s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 547122.5153\n",
            "HT_Reg - MAPE          : 0.8469\n",
            "HT_Reg - MAE          : 739.677305\n",
            "HAT_Reg - MSE          : 545879.4519\n",
            "HAT_Reg - MAPE          : 0.8459\n",
            "HAT_Reg - MAE          : 738.836553\n",
            "ARF_Reg - MSE          : 22518.6532\n",
            "ARF_Reg - MAPE          : 0.1718\n",
            "ARF_Reg - MAE          : 150.062165\n",
            "PA_Reg - MSE          : 12947262.5352\n",
            "PA_Reg - MAPE          : 4.1198\n",
            "PA_Reg - MAE          : 3598.230473\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.18s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1123236.6892\n",
            "HT_Reg - MAPE          : 0.3622\n",
            "HT_Reg - MAE          : 1059.828613\n",
            "HAT_Reg - MSE          : 1123424.0363\n",
            "HAT_Reg - MAPE          : 0.3623\n",
            "HAT_Reg - MAE          : 1059.916995\n",
            "ARF_Reg - MSE          : 113275.0623\n",
            "ARF_Reg - MAPE          : 0.1150\n",
            "ARF_Reg - MAE          : 336.563608\n",
            "PA_Reg - MSE          : 23841489.7392\n",
            "PA_Reg - MAPE          : 1.6689\n",
            "PA_Reg - MAE          : 4882.774799\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.54s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 30673.0970\n",
            "HT_Reg - MAPE          : 0.1054\n",
            "HT_Reg - MAE          : 175.137366\n",
            "HAT_Reg - MSE          : 86781.4503\n",
            "HAT_Reg - MAPE          : 0.1773\n",
            "HAT_Reg - MAE          : 294.586915\n",
            "ARF_Reg - MSE          : 1768.2322\n",
            "ARF_Reg - MAPE          : 0.0253\n",
            "ARF_Reg - MAE          : 42.050354\n",
            "PA_Reg - MSE          : 38909259.1002\n",
            "PA_Reg - MAPE          : 3.7538\n",
            "PA_Reg - MAE          : 6237.728681\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.16s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 433.8882\n",
            "HT_Reg - MAPE          : 0.0198\n",
            "HT_Reg - MAE          : 20.829983\n",
            "HAT_Reg - MSE          : 55095.1470\n",
            "HAT_Reg - MAPE          : 0.2232\n",
            "HAT_Reg - MAE          : 234.723554\n",
            "ARF_Reg - MSE          : 3695081.2355\n",
            "ARF_Reg - MAPE          : 1.8278\n",
            "ARF_Reg - MAE          : 1922.259409\n",
            "PA_Reg - MSE          : 1376373508.7363\n",
            "PA_Reg - MAPE          : 35.2758\n",
            "PA_Reg - MAE          : 37099.508201\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.79s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 124.2062\n",
            "HT_Reg - MAPE          : 0.0335\n",
            "HT_Reg - MAE          : 11.144786\n",
            "HAT_Reg - MSE          : 522.4783\n",
            "HAT_Reg - MAPE          : 0.0687\n",
            "HAT_Reg - MAE          : 22.857783\n",
            "ARF_Reg - MSE          : 737364.7044\n",
            "ARF_Reg - MAPE          : 2.5810\n",
            "ARF_Reg - MAE          : 858.699426\n",
            "PA_Reg - MSE          : 832150027.8544\n",
            "PA_Reg - MAPE          : 86.7058\n",
            "PA_Reg - MAE          : 28847.010726\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.17s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3640.8155\n",
            "HT_Reg - MAPE          : 0.1299\n",
            "HT_Reg - MAE          : 60.339170\n",
            "HAT_Reg - MSE          : 3976.0884\n",
            "HAT_Reg - MAPE          : 0.1358\n",
            "HAT_Reg - MAE          : 63.056232\n",
            "ARF_Reg - MSE          : 899.4134\n",
            "ARF_Reg - MAPE          : 0.0646\n",
            "ARF_Reg - MAE          : 29.990221\n",
            "PA_Reg - MSE          : 177324408.9944\n",
            "PA_Reg - MAPE          : 28.6681\n",
            "PA_Reg - MAE          : 13316.321151\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.32s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 103004.8882\n",
            "HT_Reg - MAPE          : 0.7866\n",
            "HT_Reg - MAE          : 320.943746\n",
            "HAT_Reg - MSE          : 103642.6792\n",
            "HAT_Reg - MAPE          : 0.7891\n",
            "HAT_Reg - MAE          : 321.935831\n",
            "ARF_Reg - MSE          : 8434.8188\n",
            "ARF_Reg - MAPE          : 0.2251\n",
            "ARF_Reg - MAE          : 91.841270\n",
            "PA_Reg - MSE          : 103754641.7536\n",
            "PA_Reg - MAPE          : 24.9657\n",
            "PA_Reg - MAE          : 10186.002246\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.26s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 108453.9253\n",
            "HT_Reg - MAPE          : 0.2971\n",
            "HT_Reg - MAE          : 329.323436\n",
            "HAT_Reg - MSE          : 108498.8827\n",
            "HAT_Reg - MAPE          : 0.2972\n",
            "HAT_Reg - MAE          : 329.391686\n",
            "ARF_Reg - MSE          : 9998.2380\n",
            "ARF_Reg - MAPE          : 0.0902\n",
            "ARF_Reg - MAE          : 99.991190\n",
            "PA_Reg - MSE          : 21008810.7321\n",
            "PA_Reg - MAPE          : 4.1353\n",
            "PA_Reg - MAE          : 4583.536924\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.63s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1490089.7651\n",
            "HT_Reg - MAPE          : 0.4968\n",
            "HT_Reg - MAE          : 1220.692330\n",
            "HAT_Reg - MSE          : 1490090.0227\n",
            "HAT_Reg - MAPE          : 0.4968\n",
            "HAT_Reg - MAE          : 1220.692436\n",
            "ARF_Reg - MSE          : 1739837.0719\n",
            "ARF_Reg - MAPE          : 0.5368\n",
            "ARF_Reg - MAE          : 1319.028837\n",
            "PA_Reg - MSE          : 812788290.1899\n",
            "PA_Reg - MAPE          : 11.6024\n",
            "PA_Reg - MAE          : 28509.442123\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.88s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 6800880.9498\n",
            "HT_Reg - MAPE          : 0.5191\n",
            "HT_Reg - MAE          : 2607.849871\n",
            "HAT_Reg - MSE          : 6800880.9746\n",
            "HAT_Reg - MAPE          : 0.5191\n",
            "HAT_Reg - MAE          : 2607.849876\n",
            "ARF_Reg - MSE          : 6818871.3395\n",
            "ARF_Reg - MAPE          : 0.5198\n",
            "ARF_Reg - MAE          : 2611.296869\n",
            "PA_Reg - MSE          : 2472717285.3031\n",
            "PA_Reg - MAPE          : 9.8984\n",
            "PA_Reg - MAE          : 49726.424417\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.49s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2121.8930\n",
            "HT_Reg - MAPE          : 0.2063\n",
            "HT_Reg - MAE          : 46.064010\n",
            "HAT_Reg - MSE          : 9055.5449\n",
            "HAT_Reg - MAPE          : 0.4262\n",
            "HAT_Reg - MAE          : 95.160627\n",
            "ARF_Reg - MSE          : 16677.6363\n",
            "ARF_Reg - MAPE          : 0.5783\n",
            "ARF_Reg - MAE          : 129.141923\n",
            "PA_Reg - MSE          : 12053352.3179\n",
            "PA_Reg - MAPE          : 15.5477\n",
            "PA_Reg - MAE          : 3471.793818\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.09s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 5327.7833\n",
            "HT_Reg - MAPE          : 2.1343\n",
            "HT_Reg - MAE          : 72.991666\n",
            "HAT_Reg - MSE          : 177.1254\n",
            "HAT_Reg - MAPE          : 0.3891\n",
            "HAT_Reg - MAE          : 13.308848\n",
            "ARF_Reg - MSE          : 95750.1053\n",
            "ARF_Reg - MAPE          : 9.0478\n",
            "ARF_Reg - MAE          : 309.435139\n",
            "PA_Reg - MSE          : 505728.7019\n",
            "PA_Reg - MAPE          : 20.7937\n",
            "PA_Reg - MAE          : 711.146048\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.63s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2076.2059\n",
            "HT_Reg - MAPE          : 0.1111\n",
            "HT_Reg - MAE          : 45.565403\n",
            "HAT_Reg - MSE          : 4459.0698\n",
            "HAT_Reg - MAPE          : 0.1628\n",
            "HAT_Reg - MAE          : 66.776267\n",
            "ARF_Reg - MSE          : 142997.7975\n",
            "ARF_Reg - MAPE          : 0.9219\n",
            "ARF_Reg - MAE          : 378.150496\n",
            "PA_Reg - MSE          : 5485833.5287\n",
            "PA_Reg - MAPE          : 5.7099\n",
            "PA_Reg - MAE          : 2342.185631\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.23s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1548704.6494\n",
            "HT_Reg - MAPE          : 0.6813\n",
            "HT_Reg - MAE          : 1244.469626\n",
            "HAT_Reg - MSE          : 1548846.1335\n",
            "HAT_Reg - MAPE          : 0.6813\n",
            "HAT_Reg - MAE          : 1244.526470\n",
            "ARF_Reg - MSE          : 1497765.9224\n",
            "ARF_Reg - MAPE          : 0.6700\n",
            "ARF_Reg - MAE          : 1223.832473\n",
            "PA_Reg - MSE          : 27770.5275\n",
            "PA_Reg - MAPE          : 0.0912\n",
            "PA_Reg - MAE          : 166.644915\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.15s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 8846.4667\n",
            "HT_Reg - MAPE          : 0.0603\n",
            "HT_Reg - MAE          : 94.055658\n",
            "HAT_Reg - MSE          : 8845.6272\n",
            "HAT_Reg - MAPE          : 0.0603\n",
            "HAT_Reg - MAE          : 94.051195\n",
            "ARF_Reg - MSE          : 537025.3288\n",
            "ARF_Reg - MAPE          : 0.4700\n",
            "ARF_Reg - MAE          : 732.820120\n",
            "PA_Reg - MSE          : 1568532.7095\n",
            "PA_Reg - MAPE          : 0.8033\n",
            "PA_Reg - MAE          : 1252.410759\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [6.56s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1568865.4853\n",
            "HT_Reg - MAPE          : 0.2316\n",
            "HT_Reg - MAE          : 1252.543606\n",
            "HAT_Reg - MSE          : 1568865.4748\n",
            "HAT_Reg - MAPE          : 0.2316\n",
            "HAT_Reg - MAE          : 1252.543602\n",
            "ARF_Reg - MSE          : 1493369.4822\n",
            "ARF_Reg - MAPE          : 0.2259\n",
            "ARF_Reg - MAE          : 1222.034976\n",
            "PA_Reg - MSE          : 112428976.8908\n",
            "PA_Reg - MAPE          : 1.9602\n",
            "PA_Reg - MAE          : 10603.253128\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [8.14s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 33944.2340\n",
            "HT_Reg - MAPE          : 0.2004\n",
            "HT_Reg - MAE          : 184.239610\n",
            "HAT_Reg - MSE          : 33944.2340\n",
            "HAT_Reg - MAPE          : 0.2004\n",
            "HAT_Reg - MAE          : 184.239610\n",
            "ARF_Reg - MSE          : 18919939.6183\n",
            "ARF_Reg - MAPE          : 4.7321\n",
            "ARF_Reg - MAE          : 4349.705693\n",
            "PA_Reg - MSE          : 606818375.6630\n",
            "PA_Reg - MAPE          : 26.7990\n",
            "PA_Reg - MAE          : 24633.683762\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [9.18s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 34581263.3045\n",
            "HT_Reg - MAPE          : 6.9940\n",
            "HT_Reg - MAE          : 5880.583585\n",
            "HAT_Reg - MSE          : 34581263.3045\n",
            "HAT_Reg - MAPE          : 6.9940\n",
            "HAT_Reg - MAE          : 5880.583585\n",
            "ARF_Reg - MSE          : 34968319.4076\n",
            "ARF_Reg - MAPE          : 7.0331\n",
            "ARF_Reg - MAE          : 5913.401678\n",
            "PA_Reg - MSE          : 8272704806.7529\n",
            "PA_Reg - MAPE          : 108.1760\n",
            "PA_Reg - MAE          : 90954.410595\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.49s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 79539.9745\n",
            "HT_Reg - MAPE          : 1.3533\n",
            "HT_Reg - MAE          : 282.028322\n",
            "HAT_Reg - MSE          : 77937.7466\n",
            "HAT_Reg - MAPE          : 1.3396\n",
            "HAT_Reg - MAE          : 279.173327\n",
            "ARF_Reg - MSE          : 39058.3818\n",
            "ARF_Reg - MAPE          : 0.9483\n",
            "ARF_Reg - MAE          : 197.631935\n",
            "PA_Reg - MSE          : 903588371.1531\n",
            "PA_Reg - MAPE          : 144.2406\n",
            "PA_Reg - MAE          : 30059.746691\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.11s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 72408.6362\n",
            "HT_Reg - MAPE          : 15.4649\n",
            "HT_Reg - MAE          : 269.088529\n",
            "HAT_Reg - MSE          : 113744.1063\n",
            "HAT_Reg - MAPE          : 19.3827\n",
            "HAT_Reg - MAE          : 337.259702\n",
            "ARF_Reg - MSE          : 144391.0553\n",
            "ARF_Reg - MAPE          : 21.8384\n",
            "ARF_Reg - MAE          : 379.988230\n",
            "PA_Reg - MSE          : 157145466.4462\n",
            "PA_Reg - MAPE          : 720.4464\n",
            "PA_Reg - MAE          : 12535.767485\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.77s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 6831.5267\n",
            "HT_Reg - MAPE          : 2.4310\n",
            "HT_Reg - MAE          : 82.653050\n",
            "HAT_Reg - MSE          : 7396.8179\n",
            "HAT_Reg - MAPE          : 2.5296\n",
            "HAT_Reg - MAE          : 86.004755\n",
            "ARF_Reg - MSE          : 157.6109\n",
            "ARF_Reg - MAPE          : 0.3692\n",
            "ARF_Reg - MAE          : 12.554316\n",
            "PA_Reg - MSE          : 59761576.6528\n",
            "PA_Reg - MAPE          : 227.3694\n",
            "PA_Reg - MAE          : 7730.561212\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.72s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 15551.5620\n",
            "HT_Reg - MAPE          : 1.1134\n",
            "HT_Reg - MAE          : 124.705902\n",
            "HAT_Reg - MSE          : 15542.0362\n",
            "HAT_Reg - MAPE          : 1.1131\n",
            "HAT_Reg - MAE          : 124.667703\n",
            "ARF_Reg - MSE          : 36.5493\n",
            "ARF_Reg - MAPE          : 0.0540\n",
            "ARF_Reg - MAE          : 6.045600\n",
            "PA_Reg - MSE          : 23203.3319\n",
            "PA_Reg - MAPE          : 1.3601\n",
            "PA_Reg - MAE          : 152.326399\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.09s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 949.7496\n",
            "HT_Reg - MAPE          : 0.1323\n",
            "HT_Reg - MAE          : 30.818007\n",
            "HAT_Reg - MSE          : 981.4898\n",
            "HAT_Reg - MAPE          : 0.1345\n",
            "HAT_Reg - MAE          : 31.328737\n",
            "ARF_Reg - MSE          : 8.6480\n",
            "ARF_Reg - MAPE          : 0.0126\n",
            "ARF_Reg - MAE          : 2.940745\n",
            "PA_Reg - MSE          : 27891.2622\n",
            "PA_Reg - MAPE          : 0.7168\n",
            "PA_Reg - MAE          : 167.006773\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.01s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 24910.8593\n",
            "HT_Reg - MAPE          : 0.3788\n",
            "HT_Reg - MAE          : 157.831744\n",
            "HAT_Reg - MSE          : 24774.7892\n",
            "HAT_Reg - MAPE          : 0.3777\n",
            "HAT_Reg - MAE          : 157.400093\n",
            "ARF_Reg - MSE          : 2534.7994\n",
            "ARF_Reg - MAPE          : 0.1208\n",
            "ARF_Reg - MAE          : 50.346791\n",
            "PA_Reg - MSE          : 9761712.9518\n",
            "PA_Reg - MAPE          : 7.4979\n",
            "PA_Reg - MAE          : 3124.374010\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.40s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 8390412.9949\n",
            "HT_Reg - MAPE          : 0.8306\n",
            "HT_Reg - MAE          : 2896.620962\n",
            "HAT_Reg - MSE          : 8389765.6482\n",
            "HAT_Reg - MAPE          : 0.8306\n",
            "HAT_Reg - MAE          : 2896.509218\n",
            "ARF_Reg - MSE          : 8483772.1602\n",
            "ARF_Reg - MAPE          : 0.8352\n",
            "ARF_Reg - MAE          : 2912.691566\n",
            "PA_Reg - MSE          : 4439528.3694\n",
            "PA_Reg - MAPE          : 0.6042\n",
            "PA_Reg - MAE          : 2107.018835\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [7.29s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2992951.5341\n",
            "HT_Reg - MAPE          : 0.4025\n",
            "HT_Reg - MAE          : 1730.014894\n",
            "HAT_Reg - MSE          : 2992951.9074\n",
            "HAT_Reg - MAPE          : 0.4025\n",
            "HAT_Reg - MAE          : 1730.015002\n",
            "ARF_Reg - MSE          : 1315009.1287\n",
            "ARF_Reg - MAPE          : 0.2668\n",
            "ARF_Reg - MAE          : 1146.738474\n",
            "PA_Reg - MSE          : 2403615044.4760\n",
            "PA_Reg - MAPE          : 11.4055\n",
            "PA_Reg - MAE          : 49026.676866\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.50s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 10873.2820\n",
            "HT_Reg - MAPE          : 1.5969\n",
            "HT_Reg - MAE          : 104.275030\n",
            "HAT_Reg - MSE          : 9615.6940\n",
            "HAT_Reg - MAPE          : 1.5017\n",
            "HAT_Reg - MAE          : 98.059645\n",
            "ARF_Reg - MSE          : 1576073.2176\n",
            "ARF_Reg - MAPE          : 19.2254\n",
            "ARF_Reg - MAE          : 1255.417547\n",
            "PA_Reg - MSE          : 115256703.6808\n",
            "PA_Reg - MAPE          : 164.4069\n",
            "PA_Reg - MAE          : 10735.767494\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.15s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 4410.4305\n",
            "HT_Reg - MAPE          : 1.8396\n",
            "HT_Reg - MAE          : 66.411072\n",
            "HAT_Reg - MSE          : 3127.1078\n",
            "HAT_Reg - MAPE          : 1.5490\n",
            "HAT_Reg - MAE          : 55.920549\n",
            "ARF_Reg - MSE          : 1263711.0225\n",
            "ARF_Reg - MAPE          : 31.1399\n",
            "ARF_Reg - MAE          : 1124.149021\n",
            "PA_Reg - MSE          : 20415491.0016\n",
            "PA_Reg - MAPE          : 125.1621\n",
            "PA_Reg - MAE          : 4518.350474\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.77s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2003.4589\n",
            "HT_Reg - MAPE          : 1.1190\n",
            "HT_Reg - MAE          : 44.760015\n",
            "HAT_Reg - MSE          : 1363.0625\n",
            "HAT_Reg - MAPE          : 0.9230\n",
            "HAT_Reg - MAE          : 36.919676\n",
            "ARF_Reg - MSE          : 13.9893\n",
            "ARF_Reg - MAPE          : 0.0935\n",
            "ARF_Reg - MAE          : 3.740230\n",
            "PA_Reg - MSE          : 478043.1329\n",
            "PA_Reg - MAPE          : 17.2852\n",
            "PA_Reg - MAE          : 691.406634\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.31s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 458.4451\n",
            "HT_Reg - MAPE          : 0.1758\n",
            "HT_Reg - MAE          : 21.411331\n",
            "HAT_Reg - MSE          : 644.1857\n",
            "HAT_Reg - MAPE          : 0.2084\n",
            "HAT_Reg - MAE          : 25.380814\n",
            "ARF_Reg - MSE          : 0.2811\n",
            "ARF_Reg - MAPE          : 0.0044\n",
            "ARF_Reg - MAE          : 0.530225\n",
            "PA_Reg - MSE          : 254009.5973\n",
            "PA_Reg - MAPE          : 4.1379\n",
            "PA_Reg - MAE          : 503.993648\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.50s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 13104.9471\n",
            "HT_Reg - MAPE          : 0.4358\n",
            "HT_Reg - MAE          : 114.476841\n",
            "HAT_Reg - MSE          : 12999.2727\n",
            "HAT_Reg - MAPE          : 0.4340\n",
            "HAT_Reg - MAE          : 114.014353\n",
            "ARF_Reg - MSE          : 412.7721\n",
            "ARF_Reg - MAPE          : 0.0773\n",
            "ARF_Reg - MAE          : 20.316794\n",
            "PA_Reg - MSE          : 3966552.1351\n",
            "PA_Reg - MAPE          : 7.5813\n",
            "PA_Reg - MAE          : 1991.620480\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.45s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 237551.3273\n",
            "HT_Reg - MAPE          : 0.6620\n",
            "HT_Reg - MAE          : 487.392375\n",
            "HAT_Reg - MSE          : 238182.7984\n",
            "HAT_Reg - MAPE          : 0.6629\n",
            "HAT_Reg - MAE          : 488.039751\n",
            "ARF_Reg - MSE          : 185095.7645\n",
            "ARF_Reg - MAPE          : 0.5844\n",
            "ARF_Reg - MAE          : 430.227573\n",
            "PA_Reg - MSE          : 64593.8031\n",
            "PA_Reg - MAPE          : 0.3452\n",
            "PA_Reg - MAE          : 254.153109\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.81s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2143960.2200\n",
            "HT_Reg - MAPE          : 0.7177\n",
            "HT_Reg - MAE          : 1464.226834\n",
            "HAT_Reg - MSE          : 2143733.8660\n",
            "HAT_Reg - MAPE          : 0.7177\n",
            "HAT_Reg - MAE          : 1464.149537\n",
            "ARF_Reg - MSE          : 2005818.7211\n",
            "ARF_Reg - MAPE          : 0.6942\n",
            "ARF_Reg - MAE          : 1416.269297\n",
            "PA_Reg - MSE          : 1174705.3014\n",
            "PA_Reg - MAPE          : 0.5312\n",
            "PA_Reg - MAE          : 1083.838227\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.61s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 7930087.3564\n",
            "HT_Reg - MAPE          : 0.4839\n",
            "HT_Reg - MAE          : 2816.041079\n",
            "HAT_Reg - MSE          : 7930087.3564\n",
            "HAT_Reg - MAPE          : 0.4839\n",
            "HAT_Reg - MAE          : 2816.041079\n",
            "ARF_Reg - MSE          : 419798.7055\n",
            "ARF_Reg - MAPE          : 0.1113\n",
            "ARF_Reg - MAE          : 647.918749\n",
            "PA_Reg - MSE          : 31431826.7525\n",
            "PA_Reg - MAPE          : 0.9635\n",
            "PA_Reg - MAE          : 5606.409435\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.52s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 129.7342\n",
            "HT_Reg - MAPE          : 0.0200\n",
            "HT_Reg - MAE          : 11.390093\n",
            "HAT_Reg - MSE          : 12764.1930\n",
            "HAT_Reg - MAPE          : 0.1988\n",
            "HAT_Reg - MAE          : 112.978728\n",
            "ARF_Reg - MSE          : 12174.8871\n",
            "ARF_Reg - MAPE          : 0.1941\n",
            "ARF_Reg - MAE          : 110.339871\n",
            "PA_Reg - MSE          : 21790965.6120\n",
            "PA_Reg - MAPE          : 8.2127\n",
            "PA_Reg - MAE          : 4668.079435\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.11s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1461.5442\n",
            "HT_Reg - MAPE          : 0.0671\n",
            "HT_Reg - MAE          : 38.230147\n",
            "HAT_Reg - MSE          : 4380.7702\n",
            "HAT_Reg - MAPE          : 0.1162\n",
            "HAT_Reg - MAE          : 66.187387\n",
            "ARF_Reg - MSE          : 3562.6722\n",
            "ARF_Reg - MAPE          : 0.1048\n",
            "ARF_Reg - MAE          : 59.688125\n",
            "PA_Reg - MSE          : 18322134.0107\n",
            "PA_Reg - MAPE          : 7.5148\n",
            "PA_Reg - MAE          : 4280.436194\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.64s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2076.4500\n",
            "HT_Reg - MAPE          : 0.0397\n",
            "HT_Reg - MAE          : 45.568081\n",
            "HAT_Reg - MSE          : 2028.8856\n",
            "HAT_Reg - MAPE          : 0.0393\n",
            "HAT_Reg - MAE          : 45.043153\n",
            "ARF_Reg - MSE          : 21.6376\n",
            "ARF_Reg - MAPE          : 0.0041\n",
            "ARF_Reg - MAE          : 4.651622\n",
            "PA_Reg - MSE          : 830047859.8175\n",
            "PA_Reg - MAPE          : 25.1204\n",
            "PA_Reg - MAE          : 28810.551189\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.27s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3075.9327\n",
            "HT_Reg - MAPE          : 0.2917\n",
            "HT_Reg - MAE          : 55.461091\n",
            "HAT_Reg - MSE          : 3499.9703\n",
            "HAT_Reg - MAPE          : 0.3112\n",
            "HAT_Reg - MAE          : 59.160547\n",
            "ARF_Reg - MSE          : 3.0478\n",
            "ARF_Reg - MAPE          : 0.0092\n",
            "ARF_Reg - MAE          : 1.745786\n",
            "PA_Reg - MSE          : 79786998.0915\n",
            "PA_Reg - MAPE          : 46.9877\n",
            "PA_Reg - MAE          : 8932.356805\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.55s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 556.1713\n",
            "HT_Reg - MAPE          : 0.1099\n",
            "HT_Reg - MAE          : 23.583284\n",
            "HAT_Reg - MSE          : 884.8537\n",
            "HAT_Reg - MAPE          : 0.1386\n",
            "HAT_Reg - MAE          : 29.746490\n",
            "ARF_Reg - MSE          : 66419.0294\n",
            "ARF_Reg - MAPE          : 1.2009\n",
            "ARF_Reg - MAE          : 257.718896\n",
            "PA_Reg - MSE          : 52525415.6687\n",
            "PA_Reg - MAPE          : 33.7719\n",
            "PA_Reg - MAE          : 7247.442009\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.69s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 927.6325\n",
            "HT_Reg - MAPE          : 0.0833\n",
            "HT_Reg - MAE          : 30.457060\n",
            "HAT_Reg - MSE          : 921.7634\n",
            "HAT_Reg - MAPE          : 0.0830\n",
            "HAT_Reg - MAE          : 30.360556\n",
            "ARF_Reg - MSE          : 2352.8261\n",
            "ARF_Reg - MAPE          : 0.1327\n",
            "ARF_Reg - MAE          : 48.505939\n",
            "PA_Reg - MSE          : 1730627.7889\n",
            "PA_Reg - MAPE          : 3.5983\n",
            "PA_Reg - MAE          : 1315.533272\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.16s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 320264.0550\n",
            "HT_Reg - MAPE          : 0.4820\n",
            "HT_Reg - MAE          : 565.918771\n",
            "HAT_Reg - MSE          : 320132.9965\n",
            "HAT_Reg - MAPE          : 0.4819\n",
            "HAT_Reg - MAE          : 565.802966\n",
            "ARF_Reg - MSE          : 213695.4085\n",
            "ARF_Reg - MAPE          : 0.3937\n",
            "ARF_Reg - MAE          : 462.272007\n",
            "PA_Reg - MSE          : 11504279.4516\n",
            "PA_Reg - MAPE          : 2.8888\n",
            "PA_Reg - MAE          : 3391.795904\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [7.55s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 9245725.1216\n",
            "HT_Reg - MAPE          : 0.7430\n",
            "HT_Reg - MAE          : 3040.678398\n",
            "HAT_Reg - MSE          : 9245726.9063\n",
            "HAT_Reg - MAPE          : 0.7430\n",
            "HAT_Reg - MAE          : 3040.678692\n",
            "ARF_Reg - MSE          : 9132912.2217\n",
            "ARF_Reg - MAPE          : 0.7384\n",
            "ARF_Reg - MAE          : 3022.070850\n",
            "PA_Reg - MSE          : 86371448.3535\n",
            "PA_Reg - MAPE          : 2.2708\n",
            "PA_Reg - MAE          : 9293.624070\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.48s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 71525.8088\n",
            "HT_Reg - MAPE          : 0.1610\n",
            "HT_Reg - MAE          : 267.443095\n",
            "HAT_Reg - MSE          : 4084.0960\n",
            "HAT_Reg - MAPE          : 0.0385\n",
            "HAT_Reg - MAE          : 63.906932\n",
            "ARF_Reg - MSE          : 165930.6220\n",
            "ARF_Reg - MAPE          : 0.2452\n",
            "ARF_Reg - MAE          : 407.345826\n",
            "PA_Reg - MSE          : 15831409.5059\n",
            "PA_Reg - MAPE          : 2.3949\n",
            "PA_Reg - MAE          : 3978.870381\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.11s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 22285.0676\n",
            "HT_Reg - MAPE          : 0.3583\n",
            "HT_Reg - MAE          : 149.281839\n",
            "HAT_Reg - MSE          : 190196.6341\n",
            "HAT_Reg - MAPE          : 1.0468\n",
            "HAT_Reg - MAE          : 436.115391\n",
            "ARF_Reg - MSE          : 14285833.0552\n",
            "ARF_Reg - MAPE          : 9.0726\n",
            "ARF_Reg - MAE          : 3779.660442\n",
            "PA_Reg - MSE          : 26593578.4132\n",
            "PA_Reg - MAPE          : 12.3785\n",
            "PA_Reg - MAE          : 5156.896200\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.72s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 41062.8553\n",
            "HT_Reg - MAPE          : 0.2660\n",
            "HT_Reg - MAE          : 202.639718\n",
            "HAT_Reg - MSE          : 83834.4094\n",
            "HAT_Reg - MAPE          : 0.3801\n",
            "HAT_Reg - MAE          : 289.541723\n",
            "ARF_Reg - MSE          : 1039637.7899\n",
            "ARF_Reg - MAPE          : 1.3386\n",
            "ARF_Reg - MAE          : 1019.626299\n",
            "PA_Reg - MSE          : 51623856.0099\n",
            "PA_Reg - MAPE          : 9.4328\n",
            "PA_Reg - MAE          : 7184.974322\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.45s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 50583.6866\n",
            "HT_Reg - MAPE          : 0.2461\n",
            "HT_Reg - MAE          : 224.908174\n",
            "HAT_Reg - MSE          : 49476.1763\n",
            "HAT_Reg - MAPE          : 0.2434\n",
            "HAT_Reg - MAE          : 222.432408\n",
            "ARF_Reg - MSE          : 5204.6319\n",
            "ARF_Reg - MAPE          : 0.0789\n",
            "ARF_Reg - MAE          : 72.143135\n",
            "PA_Reg - MSE          : 538265678.8680\n",
            "PA_Reg - MAPE          : 25.3891\n",
            "PA_Reg - MAE          : 23200.553417\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.65s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 5127.7514\n",
            "HT_Reg - MAPE          : 0.0880\n",
            "HT_Reg - MAE          : 71.608319\n",
            "HAT_Reg - MSE          : 4328.5729\n",
            "HAT_Reg - MAPE          : 0.0808\n",
            "HAT_Reg - MAE          : 65.791891\n",
            "ARF_Reg - MSE          : 7644.4130\n",
            "ARF_Reg - MAPE          : 0.1074\n",
            "ARF_Reg - MAE          : 87.432334\n",
            "PA_Reg - MSE          : 127312811.8477\n",
            "PA_Reg - MAPE          : 13.8615\n",
            "PA_Reg - MAE          : 11283.297915\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.28s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 6434.1196\n",
            "HT_Reg - MAPE          : 0.0792\n",
            "HT_Reg - MAE          : 80.212964\n",
            "HAT_Reg - MSE          : 6891.8401\n",
            "HAT_Reg - MAPE          : 0.0820\n",
            "HAT_Reg - MAE          : 83.017107\n",
            "ARF_Reg - MSE          : 237.6194\n",
            "ARF_Reg - MAPE          : 0.0152\n",
            "ARF_Reg - MAE          : 15.414908\n",
            "PA_Reg - MSE          : 9995272.4453\n",
            "PA_Reg - MAPE          : 3.1213\n",
            "PA_Reg - MAE          : 3161.530080\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.65s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 89394.6095\n",
            "HT_Reg - MAPE          : 0.2322\n",
            "HT_Reg - MAE          : 298.989313\n",
            "HAT_Reg - MSE          : 90785.0324\n",
            "HAT_Reg - MAPE          : 0.2340\n",
            "HAT_Reg - MAE          : 301.305547\n",
            "ARF_Reg - MSE          : 75975.4013\n",
            "ARF_Reg - MAPE          : 0.2140\n",
            "ARF_Reg - MAE          : 275.636357\n",
            "PA_Reg - MSE          : 3953151.7927\n",
            "PA_Reg - MAPE          : 1.5438\n",
            "PA_Reg - MAE          : 1988.253453\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [7.09s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2190.4744\n",
            "HT_Reg - MAPE          : 0.0541\n",
            "HT_Reg - MAE          : 46.802504\n",
            "HAT_Reg - MSE          : 2195.2943\n",
            "HAT_Reg - MAPE          : 0.0541\n",
            "HAT_Reg - MAE          : 46.853968\n",
            "ARF_Reg - MSE          : 8075.6951\n",
            "ARF_Reg - MAPE          : 0.1038\n",
            "ARF_Reg - MAE          : 89.864871\n",
            "PA_Reg - MSE          : 2338593356.3939\n",
            "PA_Reg - MAPE          : 55.8483\n",
            "PA_Reg - MAE          : 48359.004915\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.52s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 43.6805\n",
            "HT_Reg - MAPE          : 0.0124\n",
            "HT_Reg - MAE          : 6.609125\n",
            "HAT_Reg - MSE          : 228.5798\n",
            "HAT_Reg - MAPE          : 0.0283\n",
            "HAT_Reg - MAE          : 15.118855\n",
            "ARF_Reg - MSE          : 7.8701\n",
            "ARF_Reg - MAPE          : 0.0052\n",
            "ARF_Reg - MAE          : 2.805373\n",
            "PA_Reg - MSE          : 3202365.9871\n",
            "PA_Reg - MAPE          : 3.3443\n",
            "PA_Reg - MAE          : 1789.515573\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.09s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 829.7873\n",
            "HT_Reg - MAPE          : 0.0407\n",
            "HT_Reg - MAE          : 28.806029\n",
            "HAT_Reg - MSE          : 483.7104\n",
            "HAT_Reg - MAPE          : 0.0310\n",
            "HAT_Reg - MAE          : 21.993417\n",
            "ARF_Reg - MSE          : 946106.9490\n",
            "ARF_Reg - MAPE          : 1.3731\n",
            "ARF_Reg - MAE          : 972.680291\n",
            "PA_Reg - MSE          : 32543333.5976\n",
            "PA_Reg - MAPE          : 8.0529\n",
            "PA_Reg - MAE          : 5704.676467\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.84s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 142341.9051\n",
            "HT_Reg - MAPE          : 0.9056\n",
            "HT_Reg - MAE          : 377.282262\n",
            "HAT_Reg - MSE          : 172422.4405\n",
            "HAT_Reg - MAPE          : 0.9967\n",
            "HAT_Reg - MAE          : 415.237812\n",
            "ARF_Reg - MSE          : 545763.7507\n",
            "ARF_Reg - MAPE          : 1.7733\n",
            "ARF_Reg - MAE          : 738.758249\n",
            "PA_Reg - MSE          : 875330845.0961\n",
            "PA_Reg - MAPE          : 71.0177\n",
            "PA_Reg - MAE          : 29585.990690\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.79s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 102.6802\n",
            "HT_Reg - MAPE          : 0.0335\n",
            "HT_Reg - MAE          : 10.133125\n",
            "HAT_Reg - MSE          : 232.2839\n",
            "HAT_Reg - MAPE          : 0.0504\n",
            "HAT_Reg - MAE          : 15.240864\n",
            "ARF_Reg - MSE          : 143962.1053\n",
            "ARF_Reg - MAPE          : 1.2539\n",
            "ARF_Reg - MAE          : 379.423385\n",
            "PA_Reg - MSE          : 34650167.0717\n",
            "PA_Reg - MAPE          : 19.4529\n",
            "PA_Reg - MAE          : 5886.439252\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.30s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1853.1600\n",
            "HT_Reg - MAPE          : 0.1066\n",
            "HT_Reg - MAE          : 43.048345\n",
            "HAT_Reg - MSE          : 1860.4034\n",
            "HAT_Reg - MAPE          : 0.1068\n",
            "HAT_Reg - MAE          : 43.132393\n",
            "ARF_Reg - MSE          : 199.2079\n",
            "ARF_Reg - MAPE          : 0.0350\n",
            "ARF_Reg - MAE          : 14.114103\n",
            "PA_Reg - MSE          : 108298012.6893\n",
            "PA_Reg - MAPE          : 25.7718\n",
            "PA_Reg - MAE          : 10406.633110\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.23s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 33511.0809\n",
            "HT_Reg - MAPE          : 0.1974\n",
            "HT_Reg - MAE          : 183.060320\n",
            "HAT_Reg - MSE          : 33713.1428\n",
            "HAT_Reg - MAPE          : 0.1980\n",
            "HAT_Reg - MAE          : 183.611391\n",
            "ARF_Reg - MSE          : 88761.0667\n",
            "ARF_Reg - MAPE          : 0.3213\n",
            "ARF_Reg - MAE          : 297.927956\n",
            "PA_Reg - MSE          : 16359134.4625\n",
            "PA_Reg - MAPE          : 4.3613\n",
            "PA_Reg - MAE          : 4044.642687\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.56s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 302821.2572\n",
            "HT_Reg - MAPE          : 0.3955\n",
            "HT_Reg - MAE          : 550.291975\n",
            "HAT_Reg - MSE          : 302834.7993\n",
            "HAT_Reg - MAPE          : 0.3955\n",
            "HAT_Reg - MAE          : 550.304279\n",
            "ARF_Reg - MSE          : 283935.1617\n",
            "ARF_Reg - MAPE          : 0.3830\n",
            "ARF_Reg - MAE          : 532.855667\n",
            "PA_Reg - MSE          : 13755.5151\n",
            "PA_Reg - MAPE          : 0.0843\n",
            "PA_Reg - MAE          : 117.283908\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.96s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.6765\n",
            "HT_Reg - MAPE          : 0.0007\n",
            "HT_Reg - MAE          : 0.822480\n",
            "HAT_Reg - MSE          : 0.6761\n",
            "HAT_Reg - MAPE          : 0.0007\n",
            "HAT_Reg - MAE          : 0.822272\n",
            "ARF_Reg - MSE          : 420.4485\n",
            "ARF_Reg - MAPE          : 0.0164\n",
            "ARF_Reg - MAE          : 20.504840\n",
            "PA_Reg - MSE          : 11145172.9874\n",
            "PA_Reg - MAPE          : 2.6710\n",
            "PA_Reg - MAE          : 3338.438705\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.57s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 25627.6758\n",
            "HT_Reg - MAPE          : 0.4234\n",
            "HT_Reg - MAE          : 160.086464\n",
            "HAT_Reg - MSE          : 8971.9644\n",
            "HAT_Reg - MAPE          : 0.2505\n",
            "HAT_Reg - MAE          : 94.720454\n",
            "ARF_Reg - MSE          : 52654.6383\n",
            "ARF_Reg - MAPE          : 0.6069\n",
            "ARF_Reg - MAE          : 229.465985\n",
            "PA_Reg - MSE          : 479300.9390\n",
            "PA_Reg - MAPE          : 1.8310\n",
            "PA_Reg - MAE          : 692.315635\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.15s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2704.3180\n",
            "HT_Reg - MAPE          : 1.5030\n",
            "HT_Reg - MAE          : 52.003058\n",
            "HAT_Reg - MSE          : 3263.8133\n",
            "HAT_Reg - MAPE          : 1.6512\n",
            "HAT_Reg - MAE          : 57.129794\n",
            "ARF_Reg - MSE          : 117292.7093\n",
            "ARF_Reg - MAPE          : 9.8983\n",
            "ARF_Reg - MAE          : 342.480232\n",
            "PA_Reg - MSE          : 133594.1678\n",
            "PA_Reg - MAPE          : 10.5637\n",
            "PA_Reg - MAE          : 365.505359\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.77s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 11.4312\n",
            "HT_Reg - MAPE          : 0.0505\n",
            "HT_Reg - MAE          : 3.381005\n",
            "HAT_Reg - MSE          : 243.7258\n",
            "HAT_Reg - MAPE          : 0.2334\n",
            "HAT_Reg - MAE          : 15.611721\n",
            "ARF_Reg - MSE          : 763212.0711\n",
            "ARF_Reg - MAPE          : 13.0586\n",
            "ARF_Reg - MAE          : 873.620095\n",
            "PA_Reg - MSE          : 18788724.2459\n",
            "PA_Reg - MAPE          : 64.7922\n",
            "PA_Reg - MAE          : 4334.596203\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.14s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 91014.1157\n",
            "HT_Reg - MAPE          : 0.4630\n",
            "HT_Reg - MAE          : 301.685458\n",
            "HAT_Reg - MSE          : 91116.7509\n",
            "HAT_Reg - MAPE          : 0.4633\n",
            "HAT_Reg - MAE          : 301.855513\n",
            "ARF_Reg - MSE          : 64203.3862\n",
            "ARF_Reg - MAPE          : 0.3889\n",
            "ARF_Reg - MAE          : 253.383871\n",
            "PA_Reg - MSE          : 186305.5778\n",
            "PA_Reg - MAPE          : 0.6624\n",
            "PA_Reg - MAE          : 431.631298\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.70s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 146749.8524\n",
            "HT_Reg - MAPE          : 0.4206\n",
            "HT_Reg - MAE          : 383.079434\n",
            "HAT_Reg - MSE          : 146740.8873\n",
            "HAT_Reg - MAPE          : 0.4206\n",
            "HAT_Reg - MAE          : 383.067732\n",
            "ARF_Reg - MSE          : 107172.3879\n",
            "ARF_Reg - MAPE          : 0.3594\n",
            "ARF_Reg - MAE          : 327.371941\n",
            "PA_Reg - MSE          : 31903399.7967\n",
            "PA_Reg - MAPE          : 6.2015\n",
            "PA_Reg - MAE          : 5648.309464\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.35s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3985.8478\n",
            "HT_Reg - MAPE          : 0.1391\n",
            "HT_Reg - MAE          : 63.133571\n",
            "HAT_Reg - MSE          : 3992.1316\n",
            "HAT_Reg - MAPE          : 0.1392\n",
            "HAT_Reg - MAE          : 63.183317\n",
            "ARF_Reg - MSE          : 128832.1979\n",
            "ARF_Reg - MAPE          : 0.7908\n",
            "ARF_Reg - MAE          : 358.932024\n",
            "PA_Reg - MSE          : 2929845.6949\n",
            "PA_Reg - MAPE          : 3.7710\n",
            "PA_Reg - MAE          : 1711.679203\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.51s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 279.0666\n",
            "HT_Reg - MAPE          : 0.0287\n",
            "HT_Reg - MAE          : 16.705286\n",
            "HAT_Reg - MSE          : 279.3900\n",
            "HAT_Reg - MAPE          : 0.0287\n",
            "HAT_Reg - MAE          : 16.714965\n",
            "ARF_Reg - MSE          : 1255246.0246\n",
            "ARF_Reg - MAPE          : 1.9237\n",
            "ARF_Reg - MAE          : 1120.377626\n",
            "PA_Reg - MSE          : 38139326.8604\n",
            "PA_Reg - MAPE          : 10.6039\n",
            "PA_Reg - MAE          : 6175.704564\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.10s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 977406.0874\n",
            "HT_Reg - MAPE          : 0.5218\n",
            "HT_Reg - MAE          : 988.638502\n",
            "HAT_Reg - MSE          : 977406.0910\n",
            "HAT_Reg - MAPE          : 0.5218\n",
            "HAT_Reg - MAE          : 988.638504\n",
            "ARF_Reg - MSE          : 1160438.3944\n",
            "ARF_Reg - MAPE          : 0.5686\n",
            "ARF_Reg - MAE          : 1077.236462\n",
            "PA_Reg - MSE          : 312135.3080\n",
            "PA_Reg - MAPE          : 0.2949\n",
            "PA_Reg - MAE          : 558.690709\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.53s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1815.2908\n",
            "HT_Reg - MAPE          : 0.2100\n",
            "HT_Reg - MAE          : 42.606229\n",
            "HAT_Reg - MSE          : 3386.1875\n",
            "HAT_Reg - MAPE          : 0.2868\n",
            "HAT_Reg - MAE          : 58.190957\n",
            "ARF_Reg - MSE          : 38.1380\n",
            "ARF_Reg - MAPE          : 0.0304\n",
            "ARF_Reg - MAE          : 6.175599\n",
            "PA_Reg - MSE          : 974.0778\n",
            "PA_Reg - MAPE          : 0.1538\n",
            "PA_Reg - MAE          : 31.210220\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.07s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 19095.7792\n",
            "HT_Reg - MAPE          : 0.1602\n",
            "HT_Reg - MAE          : 138.187479\n",
            "HAT_Reg - MSE          : 19049.8218\n",
            "HAT_Reg - MAPE          : 0.1600\n",
            "HAT_Reg - MAE          : 138.021092\n",
            "ARF_Reg - MSE          : 17021.2419\n",
            "ARF_Reg - MAPE          : 0.1513\n",
            "ARF_Reg - MAE          : 130.465482\n",
            "PA_Reg - MSE          : 138339.2181\n",
            "PA_Reg - MAPE          : 0.4313\n",
            "PA_Reg - MAE          : 371.939804\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.75s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 14841.8534\n",
            "HT_Reg - MAPE          : 0.1780\n",
            "HT_Reg - MAE          : 121.827146\n",
            "HAT_Reg - MSE          : 15099.2101\n",
            "HAT_Reg - MAPE          : 0.1795\n",
            "HAT_Reg - MAE          : 122.878843\n",
            "ARF_Reg - MSE          : 3328140.7556\n",
            "ARF_Reg - MAPE          : 2.6648\n",
            "ARF_Reg - MAE          : 1824.319258\n",
            "PA_Reg - MSE          : 443606387.0948\n",
            "PA_Reg - MAPE          : 30.7654\n",
            "PA_Reg - MAE          : 21061.965414\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.35s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 793121.2608\n",
            "HT_Reg - MAPE          : 1.3294\n",
            "HT_Reg - MAE          : 890.573557\n",
            "HAT_Reg - MSE          : 793167.3480\n",
            "HAT_Reg - MAPE          : 1.3295\n",
            "HAT_Reg - MAE          : 890.599432\n",
            "ARF_Reg - MSE          : 1650470.5707\n",
            "ARF_Reg - MAPE          : 1.9178\n",
            "ARF_Reg - MAE          : 1284.706414\n",
            "PA_Reg - MSE          : 45775312.3081\n",
            "PA_Reg - MAPE          : 10.0996\n",
            "PA_Reg - MAE          : 6765.745510\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.43s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 25.1629\n",
            "HT_Reg - MAPE          : 0.0082\n",
            "HT_Reg - MAE          : 5.016266\n",
            "HAT_Reg - MSE          : 25.1972\n",
            "HAT_Reg - MAPE          : 0.0082\n",
            "HAT_Reg - MAE          : 5.019685\n",
            "ARF_Reg - MSE          : 27884.4899\n",
            "ARF_Reg - MAPE          : 0.2734\n",
            "ARF_Reg - MAE          : 166.986496\n",
            "PA_Reg - MSE          : 174131170.3335\n",
            "PA_Reg - MAPE          : 21.6043\n",
            "PA_Reg - MAE          : 13195.877020\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.78s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 8170.9180\n",
            "HT_Reg - MAPE          : 0.1580\n",
            "HT_Reg - MAE          : 90.393130\n",
            "HAT_Reg - MSE          : 8170.2861\n",
            "HAT_Reg - MAPE          : 0.1580\n",
            "HAT_Reg - MAE          : 90.389635\n",
            "ARF_Reg - MSE          : 2293.7386\n",
            "ARF_Reg - MAPE          : 0.0837\n",
            "ARF_Reg - MAE          : 47.892991\n",
            "PA_Reg - MSE          : 166104336.8326\n",
            "PA_Reg - MAPE          : 22.5317\n",
            "PA_Reg - MAE          : 12888.147145\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.27s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 726.8838\n",
            "HT_Reg - MAPE          : 0.0354\n",
            "HT_Reg - MAE          : 26.960784\n",
            "HAT_Reg - MSE          : 726.9699\n",
            "HAT_Reg - MAPE          : 0.0354\n",
            "HAT_Reg - MAE          : 26.962379\n",
            "ARF_Reg - MSE          : 116.8802\n",
            "ARF_Reg - MAPE          : 0.0142\n",
            "ARF_Reg - MAE          : 10.811117\n",
            "PA_Reg - MSE          : 14659203.5179\n",
            "PA_Reg - MAPE          : 5.0318\n",
            "PA_Reg - MAE          : 3828.733931\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.13s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 19419.5264\n",
            "HT_Reg - MAPE          : 0.3304\n",
            "HT_Reg - MAE          : 139.353961\n",
            "HAT_Reg - MSE          : 19417.6201\n",
            "HAT_Reg - MAPE          : 0.3304\n",
            "HAT_Reg - MAE          : 139.347121\n",
            "ARF_Reg - MSE          : 41443.2341\n",
            "ARF_Reg - MAPE          : 0.4826\n",
            "ARF_Reg - MAE          : 203.576114\n",
            "PA_Reg - MSE          : 1554880.4193\n",
            "PA_Reg - MAPE          : 2.9563\n",
            "PA_Reg - MAE          : 1246.948443\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.50s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1884463.1199\n",
            "HT_Reg - MAPE          : 1.8151\n",
            "HT_Reg - MAE          : 1372.757488\n",
            "HAT_Reg - MSE          : 1119371.6920\n",
            "HAT_Reg - MAPE          : 1.3989\n",
            "HAT_Reg - MAE          : 1058.003635\n",
            "ARF_Reg - MSE          : 3236435.6574\n",
            "ARF_Reg - MAPE          : 2.3787\n",
            "ARF_Reg - MAE          : 1799.009632\n",
            "PA_Reg - MSE          : 32555.1232\n",
            "PA_Reg - MAPE          : 0.2386\n",
            "PA_Reg - MAE          : 180.430383\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.06s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 5277676.6517\n",
            "HT_Reg - MAPE          : 1.2582\n",
            "HT_Reg - MAE          : 2297.319449\n",
            "HAT_Reg - MSE          : 5219661.7991\n",
            "HAT_Reg - MAPE          : 1.2513\n",
            "HAT_Reg - MAE          : 2284.657917\n",
            "ARF_Reg - MSE          : 1778045.0755\n",
            "ARF_Reg - MAPE          : 0.7303\n",
            "ARF_Reg - MAE          : 1333.433566\n",
            "PA_Reg - MSE          : 6762869.7263\n",
            "PA_Reg - MAPE          : 1.4243\n",
            "PA_Reg - MAE          : 2600.551812\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.61s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 123710.9879\n",
            "HT_Reg - MAPE          : 0.3664\n",
            "HT_Reg - MAE          : 351.725728\n",
            "HAT_Reg - MSE          : 125762.3686\n",
            "HAT_Reg - MAPE          : 0.3694\n",
            "HAT_Reg - MAE          : 354.629904\n",
            "ARF_Reg - MSE          : 4920717.7600\n",
            "ARF_Reg - MAPE          : 2.3107\n",
            "ARF_Reg - MAE          : 2218.269091\n",
            "PA_Reg - MSE          : 234866314.0896\n",
            "PA_Reg - MAPE          : 15.9639\n",
            "PA_Reg - MAE          : 15325.348743\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.60s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 515.2174\n",
            "HT_Reg - MAPE          : 0.0663\n",
            "HT_Reg - MAE          : 22.698401\n",
            "HAT_Reg - MSE          : 506.8399\n",
            "HAT_Reg - MAPE          : 0.0658\n",
            "HAT_Reg - MAE          : 22.513106\n",
            "ARF_Reg - MSE          : 2984163.5700\n",
            "ARF_Reg - MAPE          : 5.0467\n",
            "ARF_Reg - MAE          : 1727.473175\n",
            "PA_Reg - MSE          : 92340419.3229\n",
            "PA_Reg - MAPE          : 28.0730\n",
            "PA_Reg - MAE          : 9609.392245\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.96s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 9139.6323\n",
            "HT_Reg - MAPE          : 0.3901\n",
            "HT_Reg - MAE          : 95.601424\n",
            "HAT_Reg - MSE          : 9140.6484\n",
            "HAT_Reg - MAPE          : 0.3901\n",
            "HAT_Reg - MAE          : 95.606738\n",
            "ARF_Reg - MSE          : 100626.4885\n",
            "ARF_Reg - MAPE          : 1.2942\n",
            "ARF_Reg - MAE          : 317.216785\n",
            "PA_Reg - MSE          : 362807975.9810\n",
            "PA_Reg - MAPE          : 77.7133\n",
            "PA_Reg - MAE          : 19047.518893\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.28s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 112807.7751\n",
            "HT_Reg - MAPE          : 1.4030\n",
            "HT_Reg - MAE          : 335.868687\n",
            "HAT_Reg - MSE          : 112805.8971\n",
            "HAT_Reg - MAPE          : 1.4029\n",
            "HAT_Reg - MAE          : 335.865892\n",
            "ARF_Reg - MSE          : 3700.9414\n",
            "ARF_Reg - MAPE          : 0.2541\n",
            "ARF_Reg - MAE          : 60.835363\n",
            "PA_Reg - MSE          : 51215672.2622\n",
            "PA_Reg - MAPE          : 29.8935\n",
            "PA_Reg - MAE          : 7156.512577\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.38s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 60124.8143\n",
            "HT_Reg - MAPE          : 0.9777\n",
            "HT_Reg - MAE          : 245.203618\n",
            "HAT_Reg - MSE          : 60123.3047\n",
            "HAT_Reg - MAPE          : 0.9777\n",
            "HAT_Reg - MAE          : 245.200540\n",
            "ARF_Reg - MSE          : 383.2645\n",
            "ARF_Reg - MAPE          : 0.0781\n",
            "ARF_Reg - MAE          : 19.577142\n",
            "PA_Reg - MSE          : 419559.5869\n",
            "PA_Reg - MAPE          : 2.5827\n",
            "PA_Reg - MAE          : 647.734195\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.73s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 19722.8514\n",
            "HT_Reg - MAPE          : 0.6925\n",
            "HT_Reg - MAE          : 140.438070\n",
            "HAT_Reg - MSE          : 19722.6907\n",
            "HAT_Reg - MAPE          : 0.6925\n",
            "HAT_Reg - MAE          : 140.437498\n",
            "ARF_Reg - MSE          : 31.4759\n",
            "ARF_Reg - MAPE          : 0.0277\n",
            "ARF_Reg - MAE          : 5.610339\n",
            "PA_Reg - MSE          : 5472.6970\n",
            "PA_Reg - MAPE          : 0.3648\n",
            "PA_Reg - MAE          : 73.977679\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.51s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.3079\n",
            "HT_Reg - MAPE          : 0.0405\n",
            "HT_Reg - MAE          : 0.554926\n",
            "HAT_Reg - MSE          : 1.0400\n",
            "HAT_Reg - MAPE          : 0.0744\n",
            "HAT_Reg - MAE          : 1.019810\n",
            "ARF_Reg - MSE          : 11.0321\n",
            "ARF_Reg - MAPE          : 0.2424\n",
            "ARF_Reg - MAE          : 3.321460\n",
            "PA_Reg - MSE          : 1649.2277\n",
            "PA_Reg - MAPE          : 2.9643\n",
            "PA_Reg - MAE          : 40.610685\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.08s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 39.2051\n",
            "HT_Reg - MAPE          : 1.4230\n",
            "HT_Reg - MAE          : 6.261400\n",
            "HAT_Reg - MSE          : 18.8427\n",
            "HAT_Reg - MAPE          : 0.9866\n",
            "HAT_Reg - MAE          : 4.340823\n",
            "ARF_Reg - MSE          : 7.0883\n",
            "ARF_Reg - MAPE          : 0.6051\n",
            "ARF_Reg - MAE          : 2.662394\n",
            "PA_Reg - MSE          : 3439.3924\n",
            "PA_Reg - MAPE          : 13.3287\n",
            "PA_Reg - MAE          : 58.646333\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.60s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.1125\n",
            "HT_Reg - MAPE          : 0.1048\n",
            "HT_Reg - MAE          : 0.335404\n",
            "HAT_Reg - MSE          : 0.2699\n",
            "HAT_Reg - MAPE          : 0.1623\n",
            "HAT_Reg - MAE          : 0.519504\n",
            "ARF_Reg - MSE          : 22.0458\n",
            "ARF_Reg - MAPE          : 1.4673\n",
            "ARF_Reg - MAE          : 4.695293\n",
            "PA_Reg - MSE          : 919.9747\n",
            "PA_Reg - MAPE          : 9.4785\n",
            "PA_Reg - MAE          : 30.331085\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.18s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 7.0093\n",
            "HT_Reg - MAPE          : 0.2085\n",
            "HT_Reg - MAE          : 2.647503\n",
            "HAT_Reg - MSE          : 6.4152\n",
            "HAT_Reg - MAPE          : 0.1994\n",
            "HAT_Reg - MAE          : 2.532818\n",
            "ARF_Reg - MSE          : 6.3420\n",
            "ARF_Reg - MAPE          : 0.1983\n",
            "ARF_Reg - MAE          : 2.518339\n",
            "PA_Reg - MSE          : 135.7990\n",
            "PA_Reg - MAPE          : 0.9176\n",
            "PA_Reg - MAE          : 11.653282\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.95s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.3602\n",
            "HT_Reg - MAPE          : 0.0594\n",
            "HT_Reg - MAE          : 0.600181\n",
            "HAT_Reg - MSE          : 0.3710\n",
            "HAT_Reg - MAPE          : 0.0603\n",
            "HAT_Reg - MAE          : 0.609134\n",
            "ARF_Reg - MSE          : 0.8346\n",
            "ARF_Reg - MAPE          : 0.0905\n",
            "ARF_Reg - MAE          : 0.913560\n",
            "PA_Reg - MSE          : 108.8736\n",
            "PA_Reg - MAPE          : 1.0331\n",
            "PA_Reg - MAE          : 10.434251\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.46s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 43230.1568\n",
            "HT_Reg - MAPE          : 0.8546\n",
            "HT_Reg - MAE          : 207.918630\n",
            "HAT_Reg - MSE          : 43230.1775\n",
            "HAT_Reg - MAPE          : 0.8546\n",
            "HAT_Reg - MAE          : 207.918680\n",
            "ARF_Reg - MSE          : 35221.5568\n",
            "ARF_Reg - MAPE          : 0.7714\n",
            "ARF_Reg - MAE          : 187.674071\n",
            "PA_Reg - MSE          : 35819.7309\n",
            "PA_Reg - MAPE          : 0.7779\n",
            "PA_Reg - MAE          : 189.261013\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.70s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 753625.5013\n",
            "HT_Reg - MAPE          : 0.4940\n",
            "HT_Reg - MAE          : 868.116064\n",
            "HAT_Reg - MSE          : 753625.5013\n",
            "HAT_Reg - MAPE          : 0.4940\n",
            "HAT_Reg - MAE          : 868.116064\n",
            "ARF_Reg - MSE          : 439430.3912\n",
            "ARF_Reg - MAPE          : 0.3772\n",
            "ARF_Reg - MAE          : 662.895460\n",
            "PA_Reg - MSE          : 4066154.2062\n",
            "PA_Reg - MAPE          : 1.1475\n",
            "PA_Reg - MAE          : 2016.470730\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.15s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 59606.5961\n",
            "HT_Reg - MAPE          : 0.0645\n",
            "HT_Reg - MAE          : 244.144621\n",
            "HAT_Reg - MSE          : 59606.5961\n",
            "HAT_Reg - MAPE          : 0.0645\n",
            "HAT_Reg - MAE          : 244.144621\n",
            "ARF_Reg - MSE          : 17247.4057\n",
            "ARF_Reg - MAPE          : 0.0347\n",
            "ARF_Reg - MAE          : 131.329379\n",
            "PA_Reg - MSE          : 11576437.9778\n",
            "PA_Reg - MAPE          : 0.8983\n",
            "PA_Reg - MAE          : 3402.416491\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.53s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 55.5673\n",
            "HT_Reg - MAPE          : 1.4335\n",
            "HT_Reg - MAE          : 7.454345\n",
            "HAT_Reg - MSE          : 23.7298\n",
            "HAT_Reg - MAPE          : 0.9368\n",
            "HAT_Reg - MAE          : 4.871328\n",
            "ARF_Reg - MSE          : 10.7571\n",
            "ARF_Reg - MAPE          : 0.6307\n",
            "ARF_Reg - MAE          : 3.279802\n",
            "PA_Reg - MSE          : 66623.4620\n",
            "PA_Reg - MAPE          : 49.6375\n",
            "PA_Reg - MAE          : 258.115211\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.09s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 57.0901\n",
            "HT_Reg - MAPE          : 0.3855\n",
            "HT_Reg - MAE          : 7.555801\n",
            "HAT_Reg - MSE          : 23.2289\n",
            "HAT_Reg - MAPE          : 0.2459\n",
            "HAT_Reg - MAE          : 4.819641\n",
            "ARF_Reg - MSE          : 443.3828\n",
            "ARF_Reg - MAPE          : 1.0743\n",
            "ARF_Reg - MAE          : 21.056658\n",
            "PA_Reg - MSE          : 1.7022\n",
            "PA_Reg - MAPE          : 0.0666\n",
            "PA_Reg - MAE          : 1.304667\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.73s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 64.8100\n",
            "HT_Reg - MAPE          : 0.3327\n",
            "HT_Reg - MAE          : 8.050466\n",
            "HAT_Reg - MSE          : 73.2285\n",
            "HAT_Reg - MAPE          : 0.3536\n",
            "HAT_Reg - MAE          : 8.557364\n",
            "ARF_Reg - MSE          : 32.2167\n",
            "ARF_Reg - MAPE          : 0.2345\n",
            "ARF_Reg - MAE          : 5.675977\n",
            "PA_Reg - MSE          : 40.4912\n",
            "PA_Reg - MAPE          : 0.2629\n",
            "PA_Reg - MAE          : 6.363266\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.51s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 10740.1363\n",
            "HT_Reg - MAPE          : 0.7252\n",
            "HT_Reg - MAE          : 103.634629\n",
            "HAT_Reg - MSE          : 10740.1399\n",
            "HAT_Reg - MAPE          : 0.7252\n",
            "HAT_Reg - MAE          : 103.634646\n",
            "ARF_Reg - MSE          : 10346.0156\n",
            "ARF_Reg - MAPE          : 0.7118\n",
            "ARF_Reg - MAE          : 101.715365\n",
            "PA_Reg - MSE          : 10266.3847\n",
            "PA_Reg - MAPE          : 0.7090\n",
            "PA_Reg - MAE          : 101.323170\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.29s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 92196.4161\n",
            "HT_Reg - MAPE          : 0.5122\n",
            "HT_Reg - MAE          : 303.638627\n",
            "HAT_Reg - MSE          : 92196.4161\n",
            "HAT_Reg - MAPE          : 0.5122\n",
            "HAT_Reg - MAE          : 303.638627\n",
            "ARF_Reg - MSE          : 207.9361\n",
            "ARF_Reg - MAPE          : 0.0243\n",
            "ARF_Reg - MAE          : 14.419990\n",
            "PA_Reg - MSE          : 506276.4046\n",
            "PA_Reg - MAPE          : 1.2003\n",
            "PA_Reg - MAE          : 711.531029\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.58s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 4668.5050\n",
            "HT_Reg - MAPE          : 0.0700\n",
            "HT_Reg - MAE          : 68.326459\n",
            "HAT_Reg - MSE          : 4668.5050\n",
            "HAT_Reg - MAPE          : 0.0700\n",
            "HAT_Reg - MAE          : 68.326459\n",
            "ARF_Reg - MSE          : 12315.1301\n",
            "ARF_Reg - MAPE          : 0.1138\n",
            "ARF_Reg - MAE          : 110.973556\n",
            "PA_Reg - MSE          : 1239309.9889\n",
            "PA_Reg - MAPE          : 1.1413\n",
            "PA_Reg - MAE          : 1113.243005\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.34s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 6143110.9944\n",
            "HT_Reg - MAPE          : 1.8280\n",
            "HT_Reg - MAE          : 2478.530007\n",
            "HAT_Reg - MSE          : 6143110.9944\n",
            "HAT_Reg - MAPE          : 1.8280\n",
            "HAT_Reg - MAE          : 2478.530007\n",
            "ARF_Reg - MSE          : 1164346.5861\n",
            "ARF_Reg - MAPE          : 0.7958\n",
            "ARF_Reg - MAE          : 1079.048927\n",
            "PA_Reg - MSE          : 1733400.0936\n",
            "PA_Reg - MAPE          : 0.9710\n",
            "PA_Reg - MAE          : 1316.586531\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.46s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 865170.3239\n",
            "HT_Reg - MAPE          : 0.5803\n",
            "HT_Reg - MAE          : 930.145324\n",
            "HAT_Reg - MSE          : 865170.3239\n",
            "HAT_Reg - MAPE          : 0.5803\n",
            "HAT_Reg - MAE          : 930.145324\n",
            "ARF_Reg - MSE          : 305063.3191\n",
            "ARF_Reg - MAPE          : 0.3446\n",
            "ARF_Reg - MAE          : 552.325374\n",
            "PA_Reg - MSE          : 189631547.9226\n",
            "PA_Reg - MAPE          : 8.5911\n",
            "PA_Reg - MAE          : 13770.677105\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.52s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 374.9385\n",
            "HT_Reg - MAPE          : 0.9930\n",
            "HT_Reg - MAE          : 19.363328\n",
            "HAT_Reg - MSE          : 147.2612\n",
            "HAT_Reg - MAPE          : 0.6223\n",
            "HAT_Reg - MAE          : 12.135122\n",
            "ARF_Reg - MSE          : 764.2165\n",
            "ARF_Reg - MAPE          : 1.4177\n",
            "ARF_Reg - MAE          : 27.644465\n",
            "PA_Reg - MSE          : 208788.1571\n",
            "PA_Reg - MAPE          : 23.4325\n",
            "PA_Reg - MAE          : 456.933427\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.07s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 103.9615\n",
            "HT_Reg - MAPE          : 8.4968\n",
            "HT_Reg - MAE          : 10.196153\n",
            "HAT_Reg - MSE          : 25.7182\n",
            "HAT_Reg - MAPE          : 4.2261\n",
            "HAT_Reg - MAE          : 5.071315\n",
            "ARF_Reg - MSE          : 20546.7747\n",
            "ARF_Reg - MAPE          : 119.4512\n",
            "ARF_Reg - MAE          : 143.341462\n",
            "PA_Reg - MSE          : 22075.3402\n",
            "PA_Reg - MAPE          : 123.8148\n",
            "PA_Reg - MAE          : 148.577724\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.74s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 149.2078\n",
            "HT_Reg - MAPE          : 0.2745\n",
            "HT_Reg - MAE          : 12.215063\n",
            "HAT_Reg - MSE          : 199.6757\n",
            "HAT_Reg - MAPE          : 0.3175\n",
            "HAT_Reg - MAE          : 14.130664\n",
            "ARF_Reg - MSE          : 36.0762\n",
            "ARF_Reg - MAPE          : 0.1350\n",
            "ARF_Reg - MAE          : 6.006343\n",
            "PA_Reg - MSE          : 18457.2464\n",
            "PA_Reg - MAPE          : 3.0530\n",
            "PA_Reg - MAE          : 135.857449\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.07s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 153.2963\n",
            "HT_Reg - MAPE          : 0.1938\n",
            "HT_Reg - MAE          : 12.381289\n",
            "HAT_Reg - MSE          : 152.9824\n",
            "HAT_Reg - MAPE          : 0.1936\n",
            "HAT_Reg - MAE          : 12.368605\n",
            "ARF_Reg - MSE          : 112.0229\n",
            "ARF_Reg - MAPE          : 0.1656\n",
            "ARF_Reg - MAE          : 10.584086\n",
            "PA_Reg - MSE          : 2634.5638\n",
            "PA_Reg - MAPE          : 0.8033\n",
            "PA_Reg - MAE          : 51.328002\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.73s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 26345.8518\n",
            "HT_Reg - MAPE          : 0.6025\n",
            "HT_Reg - MAE          : 162.314053\n",
            "HAT_Reg - MSE          : 26345.8169\n",
            "HAT_Reg - MAPE          : 0.6025\n",
            "HAT_Reg - MAE          : 162.313945\n",
            "ARF_Reg - MSE          : 23403.4797\n",
            "ARF_Reg - MAPE          : 0.5679\n",
            "ARF_Reg - MAE          : 152.981959\n",
            "PA_Reg - MSE          : 374769.1000\n",
            "PA_Reg - MAPE          : 2.2724\n",
            "PA_Reg - MAE          : 612.183878\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.28s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 50.1868\n",
            "HT_Reg - MAPE          : 0.0369\n",
            "HT_Reg - MAE          : 7.084262\n",
            "HAT_Reg - MSE          : 50.1868\n",
            "HAT_Reg - MAPE          : 0.0369\n",
            "HAT_Reg - MAE          : 7.084262\n",
            "ARF_Reg - MSE          : 7.5025\n",
            "ARF_Reg - MAPE          : 0.0143\n",
            "ARF_Reg - MAE          : 2.739061\n",
            "PA_Reg - MSE          : 349.7371\n",
            "PA_Reg - MAPE          : 0.0975\n",
            "PA_Reg - MAE          : 18.701260\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.02s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 489305.4413\n",
            "HT_Reg - MAPE          : 0.5047\n",
            "HT_Reg - MAE          : 699.503711\n",
            "HAT_Reg - MSE          : 489305.4413\n",
            "HAT_Reg - MAPE          : 0.5047\n",
            "HAT_Reg - MAE          : 699.503711\n",
            "ARF_Reg - MSE          : 463183.8607\n",
            "ARF_Reg - MAPE          : 0.4910\n",
            "ARF_Reg - MAE          : 680.576124\n",
            "PA_Reg - MSE          : 2823950.4588\n",
            "PA_Reg - MAPE          : 1.2125\n",
            "PA_Reg - MAE          : 1680.461383\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [7.81s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 617494.1556\n",
            "HT_Reg - MAPE          : 0.2610\n",
            "HT_Reg - MAE          : 785.807964\n",
            "HAT_Reg - MSE          : 617494.1556\n",
            "HAT_Reg - MAPE          : 0.2610\n",
            "HAT_Reg - MAE          : 785.807964\n",
            "ARF_Reg - MSE          : 386599.2182\n",
            "ARF_Reg - MAPE          : 0.2065\n",
            "ARF_Reg - MAE          : 621.771034\n",
            "PA_Reg - MSE          : 8235287.3484\n",
            "PA_Reg - MAPE          : 0.9532\n",
            "PA_Reg - MAE          : 2869.719036\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.51s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 325.9616\n",
            "HT_Reg - MAPE          : 0.2119\n",
            "HT_Reg - MAE          : 18.054408\n",
            "HAT_Reg - MSE          : 106.3539\n",
            "HAT_Reg - MAPE          : 0.1210\n",
            "HAT_Reg - MAE          : 10.312804\n",
            "ARF_Reg - MSE          : 881.5704\n",
            "ARF_Reg - MAPE          : 0.3485\n",
            "ARF_Reg - MAE          : 29.691251\n",
            "PA_Reg - MSE          : 76565.4168\n",
            "PA_Reg - MAPE          : 3.2477\n",
            "PA_Reg - MAE          : 276.704566\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.09s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 4342.8903\n",
            "HT_Reg - MAPE          : 0.1189\n",
            "HT_Reg - MAE          : 65.900609\n",
            "HAT_Reg - MSE          : 4271.2069\n",
            "HAT_Reg - MAPE          : 0.1179\n",
            "HAT_Reg - MAE          : 65.354471\n",
            "ARF_Reg - MSE          : 198142.7660\n",
            "ARF_Reg - MAPE          : 0.8032\n",
            "ARF_Reg - MAE          : 445.132302\n",
            "PA_Reg - MSE          : 2499424.1890\n",
            "PA_Reg - MAPE          : 2.8527\n",
            "PA_Reg - MAE          : 1580.956732\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.76s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 36672.9485\n",
            "HT_Reg - MAPE          : 0.1668\n",
            "HT_Reg - MAE          : 191.501824\n",
            "HAT_Reg - MSE          : 36696.5339\n",
            "HAT_Reg - MAPE          : 0.1669\n",
            "HAT_Reg - MAE          : 191.563394\n",
            "ARF_Reg - MSE          : 772022.4195\n",
            "ARF_Reg - MAPE          : 0.7654\n",
            "ARF_Reg - MAE          : 878.648063\n",
            "PA_Reg - MSE          : 63318.5604\n",
            "PA_Reg - MAPE          : 0.2192\n",
            "PA_Reg - MAE          : 251.631795\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.32s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 67192.4933\n",
            "HT_Reg - MAPE          : 0.3405\n",
            "HT_Reg - MAE          : 259.215149\n",
            "HAT_Reg - MSE          : 67149.1180\n",
            "HAT_Reg - MAPE          : 0.3404\n",
            "HAT_Reg - MAE          : 259.131469\n",
            "ARF_Reg - MSE          : 3359760.3698\n",
            "ARF_Reg - MAPE          : 2.4080\n",
            "ARF_Reg - MAE          : 1832.964912\n",
            "PA_Reg - MSE          : 94058253.3672\n",
            "PA_Reg - MAPE          : 12.7409\n",
            "PA_Reg - MAE          : 9698.363438\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.01s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 6447584.6873\n",
            "HT_Reg - MAPE          : 13.0016\n",
            "HT_Reg - MAE          : 2539.209461\n",
            "HAT_Reg - MSE          : 6447582.6032\n",
            "HAT_Reg - MAPE          : 13.0016\n",
            "HAT_Reg - MAE          : 2539.209051\n",
            "ARF_Reg - MSE          : 877126.0491\n",
            "ARF_Reg - MAPE          : 4.7954\n",
            "ARF_Reg - MAE          : 936.550078\n",
            "PA_Reg - MSE          : 17775271.6242\n",
            "PA_Reg - MAPE          : 21.5877\n",
            "PA_Reg - MAE          : 4216.073010\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.02s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 24320.3994\n",
            "HT_Reg - MAPE          : 0.3052\n",
            "HT_Reg - MAE          : 155.949990\n",
            "HAT_Reg - MSE          : 24320.4202\n",
            "HAT_Reg - MAPE          : 0.3052\n",
            "HAT_Reg - MAE          : 155.950057\n",
            "ARF_Reg - MSE          : 34848.9857\n",
            "ARF_Reg - MAPE          : 0.3653\n",
            "ARF_Reg - MAE          : 186.678830\n",
            "PA_Reg - MSE          : 41377978.2623\n",
            "PA_Reg - MAPE          : 12.5882\n",
            "PA_Reg - MAE          : 6432.571668\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.80s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 201565.0013\n",
            "HT_Reg - MAPE          : 1.3201\n",
            "HT_Reg - MAE          : 448.959911\n",
            "HAT_Reg - MSE          : 201565.0636\n",
            "HAT_Reg - MAPE          : 1.3201\n",
            "HAT_Reg - MAE          : 448.959980\n",
            "ARF_Reg - MSE          : 37770.5272\n",
            "ARF_Reg - MAPE          : 0.5714\n",
            "ARF_Reg - MAE          : 194.346410\n",
            "PA_Reg - MSE          : 285915.8684\n",
            "PA_Reg - MAPE          : 1.5722\n",
            "PA_Reg - MAE          : 534.711014\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.78s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 104528.7956\n",
            "HT_Reg - MAPE          : 1.5222\n",
            "HT_Reg - MAE          : 323.309133\n",
            "HAT_Reg - MSE          : 104528.7142\n",
            "HAT_Reg - MAPE          : 1.5222\n",
            "HAT_Reg - MAE          : 323.309007\n",
            "ARF_Reg - MSE          : 13886.6740\n",
            "ARF_Reg - MAPE          : 0.5548\n",
            "ARF_Reg - MAE          : 117.841733\n",
            "PA_Reg - MSE          : 30804713.7584\n",
            "PA_Reg - MAPE          : 26.1309\n",
            "PA_Reg - MAE          : 5550.199434\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.54s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 76.7119\n",
            "HT_Reg - MAPE          : 0.0364\n",
            "HT_Reg - MAE          : 8.758534\n",
            "HAT_Reg - MSE          : 179.3043\n",
            "HAT_Reg - MAPE          : 0.0557\n",
            "HAT_Reg - MAE          : 13.390454\n",
            "ARF_Reg - MSE          : 3321.7501\n",
            "ARF_Reg - MAPE          : 0.2396\n",
            "ARF_Reg - MAE          : 57.634626\n",
            "PA_Reg - MSE          : 48843.9312\n",
            "PA_Reg - MAPE          : 0.9189\n",
            "PA_Reg - MAE          : 221.006632\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.13s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 6375.9187\n",
            "HT_Reg - MAPE          : 0.0800\n",
            "HT_Reg - MAE          : 79.849350\n",
            "HAT_Reg - MSE          : 10111.1803\n",
            "HAT_Reg - MAPE          : 0.1007\n",
            "HAT_Reg - MAE          : 100.554365\n",
            "ARF_Reg - MSE          : 358897.1350\n",
            "ARF_Reg - MAPE          : 0.6002\n",
            "ARF_Reg - MAE          : 599.080241\n",
            "PA_Reg - MSE          : 569862.3458\n",
            "PA_Reg - MAPE          : 0.7563\n",
            "PA_Reg - MAE          : 754.892274\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.65s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 12258.7023\n",
            "HT_Reg - MAPE          : 0.0761\n",
            "HT_Reg - MAE          : 110.719024\n",
            "HAT_Reg - MSE          : 13126.1864\n",
            "HAT_Reg - MAPE          : 0.0787\n",
            "HAT_Reg - MAE          : 114.569570\n",
            "ARF_Reg - MSE          : 5595364.5851\n",
            "ARF_Reg - MAPE          : 1.6254\n",
            "ARF_Reg - MAE          : 2365.452300\n",
            "PA_Reg - MSE          : 4690826.3730\n",
            "PA_Reg - MAPE          : 1.4882\n",
            "PA_Reg - MAE          : 2165.831566\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.44s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.6816\n",
            "HT_Reg - MAPE          : 0.0015\n",
            "HT_Reg - MAE          : 0.825596\n",
            "HAT_Reg - MSE          : 1837.1832\n",
            "HAT_Reg - MAPE          : 0.0800\n",
            "HAT_Reg - MAE          : 42.862375\n",
            "ARF_Reg - MSE          : 5618635.3337\n",
            "ARF_Reg - MAPE          : 4.4265\n",
            "ARF_Reg - MAE          : 2370.366076\n",
            "PA_Reg - MSE          : 660692.5990\n",
            "PA_Reg - MAPE          : 1.5179\n",
            "PA_Reg - MAE          : 812.829994\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.49s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 85.6620\n",
            "HT_Reg - MAPE          : 0.0585\n",
            "HT_Reg - MAE          : 9.255375\n",
            "HAT_Reg - MSE          : 85.4435\n",
            "HAT_Reg - MAPE          : 0.0584\n",
            "HAT_Reg - MAE          : 9.243567\n",
            "ARF_Reg - MSE          : 145194.9493\n",
            "ARF_Reg - MAPE          : 2.4071\n",
            "ARF_Reg - MAE          : 381.044550\n",
            "PA_Reg - MSE          : 175409722.9485\n",
            "PA_Reg - MAPE          : 83.6654\n",
            "PA_Reg - MAE          : 13244.233573\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.87s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 13598.6761\n",
            "HT_Reg - MAPE          : 0.9857\n",
            "HT_Reg - MAE          : 116.613362\n",
            "HAT_Reg - MSE          : 13570.5801\n",
            "HAT_Reg - MAPE          : 0.9847\n",
            "HAT_Reg - MAE          : 116.492833\n",
            "ARF_Reg - MSE          : 1188.4034\n",
            "ARF_Reg - MAPE          : 0.2914\n",
            "ARF_Reg - MAE          : 34.473227\n",
            "PA_Reg - MSE          : 1677807.9883\n",
            "PA_Reg - MAPE          : 10.9493\n",
            "PA_Reg - MAE          : 1295.302277\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.29s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 173689.6927\n",
            "HT_Reg - MAPE          : 2.5950\n",
            "HT_Reg - MAE          : 416.760954\n",
            "HAT_Reg - MSE          : 173686.1556\n",
            "HAT_Reg - MAPE          : 2.5950\n",
            "HAT_Reg - MAE          : 416.756710\n",
            "ARF_Reg - MSE          : 30.7090\n",
            "ARF_Reg - MAPE          : 0.0345\n",
            "ARF_Reg - MAE          : 5.541570\n",
            "PA_Reg - MSE          : 197345.2638\n",
            "PA_Reg - MAPE          : 2.7661\n",
            "PA_Reg - MAE          : 444.235595\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.37s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 621.1290\n",
            "HT_Reg - MAPE          : 0.0702\n",
            "HT_Reg - MAE          : 24.922460\n",
            "HAT_Reg - MSE          : 621.1863\n",
            "HAT_Reg - MAPE          : 0.0702\n",
            "HAT_Reg - MAE          : 24.923609\n",
            "ARF_Reg - MSE          : 70.7887\n",
            "ARF_Reg - MAPE          : 0.0237\n",
            "ARF_Reg - MAE          : 8.413603\n",
            "PA_Reg - MSE          : 35647.5988\n",
            "PA_Reg - MAPE          : 0.5321\n",
            "PA_Reg - MAE          : 188.805717\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.50s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2.4930\n",
            "HT_Reg - MAPE          : 0.0463\n",
            "HT_Reg - MAE          : 1.578923\n",
            "HAT_Reg - MSE          : 47.7208\n",
            "HAT_Reg - MAPE          : 0.2026\n",
            "HAT_Reg - MAE          : 6.908028\n",
            "ARF_Reg - MSE          : 0.7540\n",
            "ARF_Reg - MAPE          : 0.0255\n",
            "ARF_Reg - MAE          : 0.868332\n",
            "PA_Reg - MSE          : 506.0305\n",
            "PA_Reg - MAPE          : 0.6597\n",
            "PA_Reg - MAE          : 22.495122\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.07s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 79.4925\n",
            "HT_Reg - MAPE          : 1.2922\n",
            "HT_Reg - MAE          : 8.915855\n",
            "HAT_Reg - MSE          : 22.4231\n",
            "HAT_Reg - MAPE          : 0.6863\n",
            "HAT_Reg - MAE          : 4.735302\n",
            "ARF_Reg - MSE          : 653.5861\n",
            "ARF_Reg - MAPE          : 3.7051\n",
            "ARF_Reg - MAE          : 25.565331\n",
            "PA_Reg - MSE          : 25628.8739\n",
            "PA_Reg - MAPE          : 23.2015\n",
            "PA_Reg - MAE          : 160.090206\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.70s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 42.1941\n",
            "HT_Reg - MAPE          : 0.4360\n",
            "HT_Reg - MAE          : 6.495701\n",
            "HAT_Reg - MSE          : 89.5288\n",
            "HAT_Reg - MAPE          : 0.6350\n",
            "HAT_Reg - MAE          : 9.461967\n",
            "ARF_Reg - MSE          : 6.9052\n",
            "ARF_Reg - MAPE          : 0.1764\n",
            "ARF_Reg - MAE          : 2.627784\n",
            "PA_Reg - MSE          : 8233.9049\n",
            "PA_Reg - MAPE          : 6.0900\n",
            "PA_Reg - MAE          : 90.740867\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.32s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 4.2833\n",
            "HT_Reg - MAPE          : 0.0699\n",
            "HT_Reg - MAE          : 2.069625\n",
            "HAT_Reg - MSE          : 4.2741\n",
            "HAT_Reg - MAPE          : 0.0698\n",
            "HAT_Reg - MAE          : 2.067380\n",
            "ARF_Reg - MSE          : 8.5326\n",
            "ARF_Reg - MAPE          : 0.0987\n",
            "ARF_Reg - MAE          : 2.921069\n",
            "PA_Reg - MSE          : 46175.1290\n",
            "PA_Reg - MAPE          : 7.2596\n",
            "PA_Reg - MAE          : 214.883990\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.64s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 26485.0144\n",
            "HT_Reg - MAPE          : 0.7048\n",
            "HT_Reg - MAE          : 162.742171\n",
            "HAT_Reg - MSE          : 26487.4721\n",
            "HAT_Reg - MAPE          : 0.7048\n",
            "HAT_Reg - MAE          : 162.749722\n",
            "ARF_Reg - MSE          : 28535.5364\n",
            "ARF_Reg - MAPE          : 0.7316\n",
            "ARF_Reg - MAE          : 168.924647\n",
            "PA_Reg - MSE          : 173714.9059\n",
            "PA_Reg - MAPE          : 1.8051\n",
            "PA_Reg - MAE          : 416.791202\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [6.16s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1362.8850\n",
            "HT_Reg - MAPE          : 0.1158\n",
            "HT_Reg - MAE          : 36.917273\n",
            "HAT_Reg - MSE          : 1362.7991\n",
            "HAT_Reg - MAPE          : 0.1158\n",
            "HAT_Reg - MAE          : 36.916110\n",
            "ARF_Reg - MSE          : 1792.7717\n",
            "ARF_Reg - MAPE          : 0.1329\n",
            "ARF_Reg - MAE          : 42.341135\n",
            "PA_Reg - MSE          : 203663.5473\n",
            "PA_Reg - MAPE          : 1.4160\n",
            "PA_Reg - MAE          : 451.290979\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.38s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 41219.0694\n",
            "HT_Reg - MAPE          : 0.2881\n",
            "HT_Reg - MAE          : 203.024800\n",
            "HAT_Reg - MSE          : 41219.0728\n",
            "HAT_Reg - MAPE          : 0.2881\n",
            "HAT_Reg - MAE          : 203.024808\n",
            "ARF_Reg - MSE          : 19885.8312\n",
            "ARF_Reg - MAPE          : 0.2001\n",
            "ARF_Reg - MAE          : 141.017131\n",
            "PA_Reg - MSE          : 560736.0374\n",
            "PA_Reg - MAPE          : 1.0628\n",
            "PA_Reg - MAE          : 748.823102\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.50s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 370336.9244\n",
            "HT_Reg - MAPE          : 0.2636\n",
            "HT_Reg - MAE          : 608.553140\n",
            "HAT_Reg - MSE          : 370336.9244\n",
            "HAT_Reg - MAPE          : 0.2636\n",
            "HAT_Reg - MAE          : 608.553140\n",
            "ARF_Reg - MSE          : 92077.4207\n",
            "ARF_Reg - MAPE          : 0.1315\n",
            "ARF_Reg - MAE          : 303.442615\n",
            "PA_Reg - MSE          : 874052.2728\n",
            "PA_Reg - MAPE          : 0.4050\n",
            "PA_Reg - MAE          : 934.907628\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.58s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1562.1543\n",
            "HT_Reg - MAPE          : 2.5833\n",
            "HT_Reg - MAE          : 39.524097\n",
            "HAT_Reg - MSE          : 1398.6021\n",
            "HAT_Reg - MAPE          : 2.4443\n",
            "HAT_Reg - MAE          : 37.397889\n",
            "ARF_Reg - MSE          : 906.0520\n",
            "ARF_Reg - MAPE          : 1.9674\n",
            "ARF_Reg - MAE          : 30.100697\n",
            "PA_Reg - MSE          : 430.3167\n",
            "PA_Reg - MAPE          : 1.3558\n",
            "PA_Reg - MAE          : 20.744077\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.19s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 651.7145\n",
            "HT_Reg - MAPE          : 4.9094\n",
            "HT_Reg - MAE          : 25.528699\n",
            "HAT_Reg - MSE          : 641.6355\n",
            "HAT_Reg - MAPE          : 4.8713\n",
            "HAT_Reg - MAE          : 25.330524\n",
            "ARF_Reg - MSE          : 358.8401\n",
            "ARF_Reg - MAPE          : 3.6429\n",
            "ARF_Reg - MAE          : 18.943076\n",
            "PA_Reg - MSE          : 0.5108\n",
            "PA_Reg - MAPE          : 0.1374\n",
            "PA_Reg - MAE          : 0.714708\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.83s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 104.9036\n",
            "HT_Reg - MAPE          : 0.3380\n",
            "HT_Reg - MAE          : 10.242246\n",
            "HAT_Reg - MSE          : 104.9807\n",
            "HAT_Reg - MAPE          : 0.3382\n",
            "HAT_Reg - MAE          : 10.246011\n",
            "ARF_Reg - MSE          : 5.2358\n",
            "ARF_Reg - MAPE          : 0.0755\n",
            "ARF_Reg - MAE          : 2.288183\n",
            "PA_Reg - MSE          : 498.0640\n",
            "PA_Reg - MAPE          : 0.7365\n",
            "PA_Reg - MAE          : 22.317348\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.46s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 7223.0130\n",
            "HT_Reg - MAPE          : 0.6815\n",
            "HT_Reg - MAE          : 84.988311\n",
            "HAT_Reg - MSE          : 7222.6171\n",
            "HAT_Reg - MAPE          : 0.6815\n",
            "HAT_Reg - MAE          : 84.985982\n",
            "ARF_Reg - MSE          : 3312.3695\n",
            "ARF_Reg - MAPE          : 0.4615\n",
            "ARF_Reg - MAE          : 57.553188\n",
            "PA_Reg - MSE          : 29.0744\n",
            "PA_Reg - MAPE          : 0.0432\n",
            "PA_Reg - MAE          : 5.392068\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.32s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 847.1509\n",
            "HT_Reg - MAPE          : 0.6153\n",
            "HT_Reg - MAE          : 29.105857\n",
            "HAT_Reg - MSE          : 847.1326\n",
            "HAT_Reg - MAPE          : 0.6153\n",
            "HAT_Reg - MAE          : 29.105543\n",
            "ARF_Reg - MSE          : 555.2789\n",
            "ARF_Reg - MAPE          : 0.4982\n",
            "ARF_Reg - MAE          : 23.564357\n",
            "PA_Reg - MSE          : 4226.4061\n",
            "PA_Reg - MAPE          : 1.3744\n",
            "PA_Reg - MAE          : 65.010815\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.27s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1179.6217\n",
            "HT_Reg - MAPE          : 1.6754\n",
            "HT_Reg - MAE          : 34.345621\n",
            "HAT_Reg - MSE          : 1179.6150\n",
            "HAT_Reg - MAPE          : 1.6754\n",
            "HAT_Reg - MAE          : 34.345524\n",
            "ARF_Reg - MSE          : 9.1860\n",
            "ARF_Reg - MAPE          : 0.1478\n",
            "ARF_Reg - MAE          : 3.030840\n",
            "PA_Reg - MSE          : 43371.9615\n",
            "PA_Reg - MAPE          : 10.1590\n",
            "PA_Reg - MAE          : 208.259361\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.03s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 48.7586\n",
            "HT_Reg - MAPE          : 0.2728\n",
            "HT_Reg - MAE          : 6.982734\n",
            "HAT_Reg - MSE          : 48.7596\n",
            "HAT_Reg - MAPE          : 0.2728\n",
            "HAT_Reg - MAE          : 6.982809\n",
            "ARF_Reg - MSE          : 2.0811\n",
            "ARF_Reg - MAPE          : 0.0564\n",
            "ARF_Reg - MAE          : 1.442615\n",
            "PA_Reg - MSE          : 6038.7694\n",
            "PA_Reg - MAPE          : 3.0355\n",
            "PA_Reg - MAE          : 77.709519\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.57s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 185.1422\n",
            "HT_Reg - MAPE          : 1.0079\n",
            "HT_Reg - MAE          : 13.606696\n",
            "HAT_Reg - MSE          : 185.1431\n",
            "HAT_Reg - MAPE          : 1.0079\n",
            "HAT_Reg - MAE          : 13.606731\n",
            "ARF_Reg - MSE          : 0.1100\n",
            "ARF_Reg - MAPE          : 0.0246\n",
            "ARF_Reg - MAE          : 0.331647\n",
            "PA_Reg - MSE          : 4846.1448\n",
            "PA_Reg - MAPE          : 5.1566\n",
            "PA_Reg - MAE          : 69.614257\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.51s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 187.8023\n",
            "HT_Reg - MAPE          : 0.1320\n",
            "HT_Reg - MAE          : 13.704098\n",
            "HAT_Reg - MSE          : 40.5232\n",
            "HAT_Reg - MAPE          : 0.0613\n",
            "HAT_Reg - MAE          : 6.365782\n",
            "ARF_Reg - MSE          : 314.7318\n",
            "ARF_Reg - MAPE          : 0.1709\n",
            "ARF_Reg - MAE          : 17.740683\n",
            "PA_Reg - MSE          : 27295.2364\n",
            "PA_Reg - MAPE          : 1.5916\n",
            "PA_Reg - MAE          : 165.212700\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.12s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 485.4504\n",
            "HT_Reg - MAPE          : 0.0768\n",
            "HT_Reg - MAE          : 22.032938\n",
            "HAT_Reg - MSE          : 467.8834\n",
            "HAT_Reg - MAPE          : 0.0754\n",
            "HAT_Reg - MAE          : 21.630612\n",
            "ARF_Reg - MSE          : 790.4249\n",
            "ARF_Reg - MAPE          : 0.0980\n",
            "ARF_Reg - MAE          : 28.114497\n",
            "PA_Reg - MSE          : 25356.1773\n",
            "PA_Reg - MAPE          : 0.5552\n",
            "PA_Reg - MAE          : 159.236231\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.72s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 682.5152\n",
            "HT_Reg - MAPE          : 0.0492\n",
            "HT_Reg - MAE          : 26.124991\n",
            "HAT_Reg - MSE          : 677.5772\n",
            "HAT_Reg - MAPE          : 0.0491\n",
            "HAT_Reg - MAE          : 26.030314\n",
            "ARF_Reg - MSE          : 780915.0096\n",
            "ARF_Reg - MAPE          : 1.6655\n",
            "ARF_Reg - MAE          : 883.693957\n",
            "PA_Reg - MSE          : 2041766.8377\n",
            "PA_Reg - MAPE          : 2.6930\n",
            "PA_Reg - MAE          : 1428.904069\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.36s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1.3430\n",
            "HT_Reg - MAPE          : 0.0029\n",
            "HT_Reg - MAE          : 1.158878\n",
            "HAT_Reg - MSE          : 1.2479\n",
            "HAT_Reg - MAPE          : 0.0028\n",
            "HAT_Reg - MAE          : 1.117103\n",
            "ARF_Reg - MSE          : 256029.9462\n",
            "ARF_Reg - MAPE          : 1.2621\n",
            "ARF_Reg - MAE          : 505.994018\n",
            "PA_Reg - MSE          : 740330.6776\n",
            "PA_Reg - MAPE          : 2.1462\n",
            "PA_Reg - MAE          : 860.424708\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.27s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 445.8572\n",
            "HT_Reg - MAPE          : 0.0533\n",
            "HT_Reg - MAE          : 21.115330\n",
            "HAT_Reg - MSE          : 448.6206\n",
            "HAT_Reg - MAPE          : 0.0534\n",
            "HAT_Reg - MAE          : 21.180665\n",
            "ARF_Reg - MSE          : 269200.0593\n",
            "ARF_Reg - MAPE          : 1.3092\n",
            "ARF_Reg - MAE          : 518.844928\n",
            "PA_Reg - MSE          : 13204571.6719\n",
            "PA_Reg - MAPE          : 9.1693\n",
            "PA_Reg - MAE          : 3633.809526\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.30s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 950.6619\n",
            "HT_Reg - MAPE          : 0.0458\n",
            "HT_Reg - MAE          : 30.832805\n",
            "HAT_Reg - MSE          : 950.6197\n",
            "HAT_Reg - MAPE          : 0.0458\n",
            "HAT_Reg - MAE          : 30.832121\n",
            "ARF_Reg - MSE          : 245.2415\n",
            "ARF_Reg - MAPE          : 0.0232\n",
            "ARF_Reg - MAE          : 15.660190\n",
            "PA_Reg - MSE          : 2233866.8992\n",
            "PA_Reg - MAPE          : 2.2188\n",
            "PA_Reg - MAE          : 1494.612625\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.67s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 220.4430\n",
            "HT_Reg - MAPE          : 0.0450\n",
            "HT_Reg - MAE          : 14.847324\n",
            "HAT_Reg - MSE          : 220.2701\n",
            "HAT_Reg - MAPE          : 0.0450\n",
            "HAT_Reg - MAE          : 14.841498\n",
            "ARF_Reg - MSE          : 3532.7648\n",
            "ARF_Reg - MAPE          : 0.1801\n",
            "ARF_Reg - MAE          : 59.437066\n",
            "PA_Reg - MSE          : 1914476.4700\n",
            "PA_Reg - MAPE          : 4.1916\n",
            "PA_Reg - MAE          : 1383.646078\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.97s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 50.1425\n",
            "HT_Reg - MAPE          : 0.0474\n",
            "HT_Reg - MAE          : 7.081140\n",
            "HAT_Reg - MSE          : 50.0905\n",
            "HAT_Reg - MAPE          : 0.0474\n",
            "HAT_Reg - MAE          : 7.077467\n",
            "ARF_Reg - MSE          : 7515.2108\n",
            "ARF_Reg - MAPE          : 0.5806\n",
            "ARF_Reg - MAE          : 86.690315\n",
            "PA_Reg - MSE          : 769185.2550\n",
            "PA_Reg - MAPE          : 5.8743\n",
            "PA_Reg - MAE          : 877.032072\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.48s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 121.8591\n",
            "HT_Reg - MAPE          : 0.0797\n",
            "HT_Reg - MAE          : 11.038982\n",
            "HAT_Reg - MSE          : 240.4267\n",
            "HAT_Reg - MAPE          : 0.1120\n",
            "HAT_Reg - MAE          : 15.505699\n",
            "ARF_Reg - MSE          : 0.7065\n",
            "ARF_Reg - MAPE          : 0.0061\n",
            "ARF_Reg - MAE          : 0.840530\n",
            "PA_Reg - MSE          : 628886.3224\n",
            "PA_Reg - MAPE          : 5.7258\n",
            "PA_Reg - MAE          : 793.023532\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.07s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 32.1690\n",
            "HT_Reg - MAPE          : 0.0356\n",
            "HT_Reg - MAE          : 5.671776\n",
            "HAT_Reg - MSE          : 128.3175\n",
            "HAT_Reg - MAPE          : 0.0710\n",
            "HAT_Reg - MAE          : 11.327732\n",
            "ARF_Reg - MSE          : 16008.1086\n",
            "ARF_Reg - MAPE          : 0.7932\n",
            "ARF_Reg - MAE          : 126.523155\n",
            "PA_Reg - MSE          : 330013.0057\n",
            "PA_Reg - MAPE          : 3.6017\n",
            "PA_Reg - MAE          : 574.467585\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.74s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1058.3719\n",
            "HT_Reg - MAPE          : 0.1301\n",
            "HT_Reg - MAE          : 32.532628\n",
            "HAT_Reg - MSE          : 947.0333\n",
            "HAT_Reg - MAPE          : 0.1230\n",
            "HAT_Reg - MAE          : 30.773906\n",
            "ARF_Reg - MSE          : 285.4634\n",
            "ARF_Reg - MAPE          : 0.0676\n",
            "ARF_Reg - MAE          : 16.895661\n",
            "PA_Reg - MSE          : 8080401.0435\n",
            "PA_Reg - MAPE          : 11.3659\n",
            "PA_Reg - MAE          : 2842.604623\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.50s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 23382.0885\n",
            "HT_Reg - MAPE          : 0.2500\n",
            "HT_Reg - MAE          : 152.912029\n",
            "HAT_Reg - MSE          : 23382.0882\n",
            "HAT_Reg - MAPE          : 0.2500\n",
            "HAT_Reg - MAE          : 152.912028\n",
            "ARF_Reg - MSE          : 22989.9039\n",
            "ARF_Reg - MAPE          : 0.2479\n",
            "ARF_Reg - MAE          : 151.624219\n",
            "PA_Reg - MSE          : 963109.5137\n",
            "PA_Reg - MAPE          : 1.6046\n",
            "PA_Reg - MAE          : 981.381431\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.76s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 167.3634\n",
            "HT_Reg - MAPE          : 0.0333\n",
            "HT_Reg - MAE          : 12.936900\n",
            "HAT_Reg - MSE          : 167.3634\n",
            "HAT_Reg - MAPE          : 0.0333\n",
            "HAT_Reg - MAE          : 12.936900\n",
            "ARF_Reg - MSE          : 140858.7557\n",
            "ARF_Reg - MAPE          : 0.9653\n",
            "ARF_Reg - MAE          : 375.311545\n",
            "PA_Reg - MSE          : 6243698.7496\n",
            "PA_Reg - MAPE          : 6.4268\n",
            "PA_Reg - MAE          : 2498.739432\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.10s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 114.6733\n",
            "HT_Reg - MAPE          : 0.0614\n",
            "HT_Reg - MAE          : 10.708563\n",
            "HAT_Reg - MSE          : 114.6733\n",
            "HAT_Reg - MAPE          : 0.0614\n",
            "HAT_Reg - MAE          : 10.708563\n",
            "ARF_Reg - MSE          : 539602.0737\n",
            "ARF_Reg - MAPE          : 4.2096\n",
            "ARF_Reg - MAE          : 734.576118\n",
            "PA_Reg - MSE          : 2147807.8908\n",
            "PA_Reg - MAPE          : 8.3985\n",
            "PA_Reg - MAE          : 1465.540136\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.44s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 132.4225\n",
            "HT_Reg - MAPE          : 0.0424\n",
            "HT_Reg - MAE          : 11.507497\n",
            "HAT_Reg - MSE          : 132.4225\n",
            "HAT_Reg - MAPE          : 0.0424\n",
            "HAT_Reg - MAE          : 11.507497\n",
            "ARF_Reg - MSE          : 140995.6426\n",
            "ARF_Reg - MAPE          : 1.3825\n",
            "ARF_Reg - MAE          : 375.493865\n",
            "PA_Reg - MSE          : 528831.9147\n",
            "PA_Reg - MAPE          : 2.6775\n",
            "PA_Reg - MAE          : 727.208302\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.53s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 354437.5365\n",
            "HT_Reg - MAPE          : 0.5625\n",
            "HT_Reg - MAE          : 595.346568\n",
            "HAT_Reg - MSE          : 354437.5365\n",
            "HAT_Reg - MAPE          : 0.5625\n",
            "HAT_Reg - MAE          : 595.346568\n",
            "ARF_Reg - MSE          : 242903.4048\n",
            "ARF_Reg - MAPE          : 0.4657\n",
            "ARF_Reg - MAE          : 492.852315\n",
            "PA_Reg - MSE          : 86385.2597\n",
            "PA_Reg - MAPE          : 0.2777\n",
            "PA_Reg - MAE          : 293.913694\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.53s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 6429.1203\n",
            "HT_Reg - MAPE          : 0.4934\n",
            "HT_Reg - MAE          : 80.181795\n",
            "HAT_Reg - MSE          : 2841.5928\n",
            "HAT_Reg - MAPE          : 0.3280\n",
            "HAT_Reg - MAE          : 53.306593\n",
            "ARF_Reg - MSE          : 5386.2202\n",
            "ARF_Reg - MAPE          : 0.4516\n",
            "ARF_Reg - MAE          : 73.390873\n",
            "PA_Reg - MSE          : 2129441.4191\n",
            "PA_Reg - MAPE          : 8.9801\n",
            "PA_Reg - MAE          : 1459.260573\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.10s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2664.6793\n",
            "HT_Reg - MAPE          : 0.9335\n",
            "HT_Reg - MAE          : 51.620532\n",
            "HAT_Reg - MSE          : 473.7344\n",
            "HAT_Reg - MAPE          : 0.3936\n",
            "HAT_Reg - MAE          : 21.765440\n",
            "ARF_Reg - MSE          : 287494.0012\n",
            "ARF_Reg - MAPE          : 9.6959\n",
            "ARF_Reg - MAE          : 536.184671\n",
            "PA_Reg - MSE          : 703419.2289\n",
            "PA_Reg - MAPE          : 15.1664\n",
            "PA_Reg - MAE          : 838.700917\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.76s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 13.2593\n",
            "HT_Reg - MAPE          : 0.0857\n",
            "HT_Reg - MAE          : 3.641334\n",
            "HAT_Reg - MSE          : 3.9774\n",
            "HAT_Reg - MAPE          : 0.0469\n",
            "HAT_Reg - MAE          : 1.994334\n",
            "ARF_Reg - MSE          : 24191.0460\n",
            "ARF_Reg - MAPE          : 3.6596\n",
            "ARF_Reg - MAE          : 155.534710\n",
            "PA_Reg - MSE          : 14540679.8677\n",
            "PA_Reg - MAPE          : 89.7229\n",
            "PA_Reg - MAE          : 3813.224340\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.67s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1338.7627\n",
            "HT_Reg - MAPE          : 1.1653\n",
            "HT_Reg - MAE          : 36.589107\n",
            "HAT_Reg - MSE          : 1381.0809\n",
            "HAT_Reg - MAPE          : 1.1835\n",
            "HAT_Reg - MAE          : 37.162896\n",
            "ARF_Reg - MSE          : 215.8043\n",
            "ARF_Reg - MAPE          : 0.4678\n",
            "ARF_Reg - MAE          : 14.690278\n",
            "PA_Reg - MSE          : 577674.7284\n",
            "PA_Reg - MAPE          : 24.2054\n",
            "PA_Reg - MAE          : 760.049162\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.74s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 275.6930\n",
            "HT_Reg - MAPE          : 0.1665\n",
            "HT_Reg - MAE          : 16.604006\n",
            "HAT_Reg - MSE          : 272.6696\n",
            "HAT_Reg - MAPE          : 0.1656\n",
            "HAT_Reg - MAE          : 16.512711\n",
            "ARF_Reg - MSE          : 324.5550\n",
            "ARF_Reg - MAPE          : 0.1807\n",
            "ARF_Reg - MAE          : 18.015409\n",
            "PA_Reg - MSE          : 643515.7042\n",
            "PA_Reg - MAPE          : 8.0461\n",
            "PA_Reg - MAE          : 802.194306\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.82s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 122298.2211\n",
            "HT_Reg - MAPE          : 0.6836\n",
            "HT_Reg - MAE          : 349.711626\n",
            "HAT_Reg - MSE          : 122308.0060\n",
            "HAT_Reg - MAPE          : 0.6836\n",
            "HAT_Reg - MAE          : 349.725615\n",
            "ARF_Reg - MSE          : 134026.2520\n",
            "ARF_Reg - MAPE          : 0.7156\n",
            "ARF_Reg - MAE          : 366.095960\n",
            "PA_Reg - MSE          : 264980.3035\n",
            "PA_Reg - MAPE          : 1.0062\n",
            "PA_Reg - MAE          : 514.762376\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.47s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 126259.9401\n",
            "HT_Reg - MAPE          : 0.6107\n",
            "HT_Reg - MAE          : 355.330747\n",
            "HAT_Reg - MSE          : 126259.9742\n",
            "HAT_Reg - MAPE          : 0.6107\n",
            "HAT_Reg - MAE          : 355.330795\n",
            "ARF_Reg - MSE          : 158968.2620\n",
            "ARF_Reg - MAPE          : 0.6853\n",
            "ARF_Reg - MAE          : 398.708242\n",
            "PA_Reg - MSE          : 4172358.3677\n",
            "PA_Reg - MAPE          : 3.5109\n",
            "PA_Reg - MAE          : 2042.635153\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.78s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 202370.9006\n",
            "HT_Reg - MAPE          : 0.3860\n",
            "HT_Reg - MAE          : 449.856533\n",
            "HAT_Reg - MSE          : 202370.9006\n",
            "HAT_Reg - MAPE          : 0.3860\n",
            "HAT_Reg - MAE          : 449.856533\n",
            "ARF_Reg - MSE          : 19957.5858\n",
            "ARF_Reg - MAPE          : 0.1212\n",
            "ARF_Reg - MAE          : 141.271320\n",
            "PA_Reg - MSE          : 2868443.4650\n",
            "PA_Reg - MAPE          : 1.4533\n",
            "PA_Reg - MAE          : 1693.647976\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.50s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3970.0669\n",
            "HT_Reg - MAPE          : 0.1260\n",
            "HT_Reg - MAE          : 63.008467\n",
            "HAT_Reg - MSE          : 18827.6241\n",
            "HAT_Reg - MAPE          : 0.2743\n",
            "HAT_Reg - MAE          : 137.213790\n",
            "ARF_Reg - MSE          : 2939.5320\n",
            "ARF_Reg - MAPE          : 0.1084\n",
            "ARF_Reg - MAE          : 54.217451\n",
            "PA_Reg - MSE          : 5964769.2483\n",
            "PA_Reg - MAPE          : 4.8826\n",
            "PA_Reg - MAE          : 2442.287708\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.09s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 6602.5705\n",
            "HT_Reg - MAPE          : 1.3001\n",
            "HT_Reg - MAE          : 81.256203\n",
            "HAT_Reg - MSE          : 111976.6583\n",
            "HAT_Reg - MAPE          : 5.3541\n",
            "HAT_Reg - MAE          : 334.629135\n",
            "ARF_Reg - MSE          : 884730.4189\n",
            "ARF_Reg - MAPE          : 15.0496\n",
            "ARF_Reg - MAE          : 940.601094\n",
            "PA_Reg - MSE          : 1030212866.8756\n",
            "PA_Reg - MAPE          : 513.5509\n",
            "PA_Reg - MAE          : 32096.929244\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.77s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 8930.2384\n",
            "HT_Reg - MAPE          : 11.2500\n",
            "HT_Reg - MAE          : 94.499939\n",
            "HAT_Reg - MSE          : 4275.6018\n",
            "HAT_Reg - MAPE          : 7.7843\n",
            "HAT_Reg - MAE          : 65.388086\n",
            "ARF_Reg - MSE          : 2031213.4534\n",
            "ARF_Reg - MAPE          : 169.6674\n",
            "ARF_Reg - MAE          : 1425.206460\n",
            "PA_Reg - MSE          : 43532.9145\n",
            "PA_Reg - MAPE          : 24.8387\n",
            "PA_Reg - MAE          : 208.645428\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.27s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 24132.2867\n",
            "HT_Reg - MAPE          : 8.6785\n",
            "HT_Reg - MAE          : 155.345701\n",
            "HAT_Reg - MSE          : 23401.0459\n",
            "HAT_Reg - MAPE          : 8.5460\n",
            "HAT_Reg - MAE          : 152.974004\n",
            "ARF_Reg - MSE          : 2541.3615\n",
            "ARF_Reg - MAPE          : 2.8163\n",
            "ARF_Reg - MAE          : 50.411918\n",
            "PA_Reg - MSE          : 12606.4322\n",
            "PA_Reg - MAPE          : 6.2725\n",
            "PA_Reg - MAE          : 112.278369\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.70s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2320.5038\n",
            "HT_Reg - MAPE          : 0.4226\n",
            "HT_Reg - MAE          : 48.171607\n",
            "HAT_Reg - MSE          : 2298.0178\n",
            "HAT_Reg - MAPE          : 0.4205\n",
            "HAT_Reg - MAE          : 47.937645\n",
            "ARF_Reg - MSE          : 34.9689\n",
            "ARF_Reg - MAPE          : 0.0519\n",
            "ARF_Reg - MAE          : 5.913447\n",
            "PA_Reg - MSE          : 128719.4333\n",
            "PA_Reg - MAPE          : 3.1471\n",
            "PA_Reg - MAE          : 358.774906\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.61s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 6798.4661\n",
            "HT_Reg - MAPE          : 0.2986\n",
            "HT_Reg - MAE          : 82.452811\n",
            "HAT_Reg - MSE          : 6805.9411\n",
            "HAT_Reg - MAPE          : 0.2988\n",
            "HAT_Reg - MAE          : 82.498128\n",
            "ARF_Reg - MSE          : 34.3677\n",
            "ARF_Reg - MAPE          : 0.0212\n",
            "ARF_Reg - MAE          : 5.862393\n",
            "PA_Reg - MSE          : 2369540.4731\n",
            "PA_Reg - MAPE          : 5.5753\n",
            "PA_Reg - MAE          : 1539.331177\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.51s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 472788.6573\n",
            "HT_Reg - MAPE          : 0.6462\n",
            "HT_Reg - MAE          : 687.596289\n",
            "HAT_Reg - MSE          : 472627.4339\n",
            "HAT_Reg - MAPE          : 0.6461\n",
            "HAT_Reg - MAE          : 687.479043\n",
            "ARF_Reg - MSE          : 512544.2044\n",
            "ARF_Reg - MAPE          : 0.6729\n",
            "ARF_Reg - MAE          : 715.921926\n",
            "PA_Reg - MSE          : 11837.4422\n",
            "PA_Reg - MAPE          : 0.1023\n",
            "PA_Reg - MAE          : 108.800010\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.76s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3079.3707\n",
            "HT_Reg - MAPE          : 0.1690\n",
            "HT_Reg - MAE          : 55.492078\n",
            "HAT_Reg - MSE          : 3077.9415\n",
            "HAT_Reg - MAPE          : 0.1689\n",
            "HAT_Reg - MAE          : 55.479199\n",
            "ARF_Reg - MSE          : 136692.3625\n",
            "ARF_Reg - MAPE          : 1.1258\n",
            "ARF_Reg - MAE          : 369.719302\n",
            "PA_Reg - MSE          : 100776.4210\n",
            "PA_Reg - MAPE          : 0.9667\n",
            "PA_Reg - MAE          : 317.453022\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.56s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 33.7084\n",
            "HT_Reg - MAPE          : 0.1063\n",
            "HT_Reg - MAE          : 5.805892\n",
            "HAT_Reg - MSE          : 21.2440\n",
            "HAT_Reg - MAPE          : 0.0844\n",
            "HAT_Reg - MAE          : 4.609120\n",
            "ARF_Reg - MSE          : 330.9257\n",
            "ARF_Reg - MAPE          : 0.3332\n",
            "ARF_Reg - MAE          : 18.191363\n",
            "PA_Reg - MSE          : 1355882.7776\n",
            "PA_Reg - MAPE          : 21.3264\n",
            "PA_Reg - MAE          : 1164.423796\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.13s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 4.5464\n",
            "HT_Reg - MAPE          : 0.0310\n",
            "HT_Reg - MAE          : 2.132236\n",
            "HAT_Reg - MSE          : 42.9877\n",
            "HAT_Reg - MAPE          : 0.0953\n",
            "HAT_Reg - MAE          : 6.556502\n",
            "ARF_Reg - MSE          : 21025.1033\n",
            "ARF_Reg - MAPE          : 2.1076\n",
            "ARF_Reg - MAE          : 145.000356\n",
            "PA_Reg - MSE          : 34047179.6898\n",
            "PA_Reg - MAPE          : 84.8110\n",
            "PA_Reg - MAE          : 5834.996117\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.76s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 248.9243\n",
            "HT_Reg - MAPE          : 1.5621\n",
            "HT_Reg - MAE          : 15.777335\n",
            "HAT_Reg - MSE          : 236.5329\n",
            "HAT_Reg - MAPE          : 1.5227\n",
            "HAT_Reg - MAE          : 15.379625\n",
            "ARF_Reg - MSE          : 4.6136\n",
            "ARF_Reg - MAPE          : 0.2127\n",
            "ARF_Reg - MAE          : 2.147938\n",
            "PA_Reg - MSE          : 157430.6146\n",
            "PA_Reg - MAPE          : 39.2847\n",
            "PA_Reg - MAE          : 396.775270\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.48s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 882.6888\n",
            "HT_Reg - MAPE          : 1.9940\n",
            "HT_Reg - MAE          : 29.710080\n",
            "HAT_Reg - MSE          : 888.0640\n",
            "HAT_Reg - MAPE          : 2.0000\n",
            "HAT_Reg - MAE          : 29.800402\n",
            "ARF_Reg - MSE          : 887.4191\n",
            "ARF_Reg - MAPE          : 1.9993\n",
            "ARF_Reg - MAE          : 29.789580\n",
            "PA_Reg - MSE          : 5813385.4609\n",
            "PA_Reg - MAPE          : 161.8185\n",
            "PA_Reg - MAE          : 2411.096319\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.99s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 369.5457\n",
            "HT_Reg - MAPE          : 2.1125\n",
            "HT_Reg - MAE          : 19.223572\n",
            "HAT_Reg - MSE          : 370.1413\n",
            "HAT_Reg - MAPE          : 2.1142\n",
            "HAT_Reg - MAE          : 19.239057\n",
            "ARF_Reg - MSE          : 172.7008\n",
            "ARF_Reg - MAPE          : 1.4441\n",
            "ARF_Reg - MAE          : 13.141569\n",
            "PA_Reg - MSE          : 946753.0432\n",
            "PA_Reg - MAPE          : 106.9244\n",
            "PA_Reg - MAE          : 973.012355\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.38s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 147.6901\n",
            "HT_Reg - MAPE          : 0.2002\n",
            "HT_Reg - MAE          : 12.152780\n",
            "HAT_Reg - MSE          : 147.6628\n",
            "HAT_Reg - MAPE          : 0.2002\n",
            "HAT_Reg - MAE          : 12.151658\n",
            "ARF_Reg - MSE          : 0.0877\n",
            "ARF_Reg - MAPE          : 0.0049\n",
            "ARF_Reg - MAE          : 0.296127\n",
            "PA_Reg - MSE          : 1275.5718\n",
            "PA_Reg - MAPE          : 0.5884\n",
            "PA_Reg - MAE          : 35.715148\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.75s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 495941.6195\n",
            "HT_Reg - MAPE          : 0.8348\n",
            "HT_Reg - MAE          : 704.231226\n",
            "HAT_Reg - MSE          : 495941.6220\n",
            "HAT_Reg - MAPE          : 0.8348\n",
            "HAT_Reg - MAE          : 704.231228\n",
            "ARF_Reg - MSE          : 492942.4123\n",
            "ARF_Reg - MAPE          : 0.8323\n",
            "ARF_Reg - MAE          : 702.098577\n",
            "PA_Reg - MSE          : 2403872.6774\n",
            "PA_Reg - MAPE          : 1.8379\n",
            "PA_Reg - MAE          : 1550.442736\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [7.21s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 52011.0512\n",
            "HT_Reg - MAPE          : 0.1805\n",
            "HT_Reg - MAE          : 228.059315\n",
            "HAT_Reg - MSE          : 52011.0512\n",
            "HAT_Reg - MAPE          : 0.1805\n",
            "HAT_Reg - MAE          : 228.059315\n",
            "ARF_Reg - MSE          : 16538.5110\n",
            "ARF_Reg - MAPE          : 0.1018\n",
            "ARF_Reg - MAE          : 128.602142\n",
            "PA_Reg - MSE          : 7275154.5703\n",
            "PA_Reg - MAPE          : 2.1342\n",
            "PA_Reg - MAE          : 2697.249445\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.55s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 17626.9395\n",
            "HT_Reg - MAPE          : 0.1416\n",
            "HT_Reg - MAE          : 132.766485\n",
            "HAT_Reg - MSE          : 16403.6279\n",
            "HAT_Reg - MAPE          : 0.1366\n",
            "HAT_Reg - MAE          : 128.076649\n",
            "ARF_Reg - MSE          : 6081.5975\n",
            "ARF_Reg - MAPE          : 0.0832\n",
            "ARF_Reg - MAE          : 77.984598\n",
            "PA_Reg - MSE          : 92487.9065\n",
            "PA_Reg - MAPE          : 0.3245\n",
            "PA_Reg - MAE          : 304.118244\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.14s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 791.6835\n",
            "HT_Reg - MAPE          : 0.0582\n",
            "HT_Reg - MAE          : 28.136871\n",
            "HAT_Reg - MSE          : 746.8701\n",
            "HAT_Reg - MAPE          : 0.0565\n",
            "HAT_Reg - MAE          : 27.328924\n",
            "ARF_Reg - MSE          : 1300916.4154\n",
            "ARF_Reg - MAPE          : 2.3575\n",
            "ARF_Reg - MAE          : 1140.577229\n",
            "PA_Reg - MSE          : 17388074.6485\n",
            "PA_Reg - MAPE          : 8.6191\n",
            "PA_Reg - MAE          : 4169.901036\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.79s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 27752.9305\n",
            "HT_Reg - MAPE          : 0.8388\n",
            "HT_Reg - MAE          : 166.592108\n",
            "HAT_Reg - MSE          : 23561.3487\n",
            "HAT_Reg - MAPE          : 0.7729\n",
            "HAT_Reg - MAE          : 153.497064\n",
            "ARF_Reg - MSE          : 1119156.2520\n",
            "ARF_Reg - MAPE          : 5.3268\n",
            "ARF_Reg - MAE          : 1057.901816\n",
            "PA_Reg - MSE          : 6346152539.6743\n",
            "PA_Reg - MAPE          : 401.1216\n",
            "PA_Reg - MAE          : 79662.742482\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.00s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 930.0720\n",
            "HT_Reg - MAPE          : 0.0940\n",
            "HT_Reg - MAE          : 30.497082\n",
            "HAT_Reg - MSE          : 726.3283\n",
            "HAT_Reg - MAPE          : 0.0831\n",
            "HAT_Reg - MAE          : 26.950478\n",
            "ARF_Reg - MSE          : 856.7861\n",
            "ARF_Reg - MAPE          : 0.0903\n",
            "ARF_Reg - MAE          : 29.270908\n",
            "PA_Reg - MSE          : 33939585.1797\n",
            "PA_Reg - MAPE          : 17.9641\n",
            "PA_Reg - MAE          : 5825.769063\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.71s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 43090.5598\n",
            "HT_Reg - MAPE          : 2.6819\n",
            "HT_Reg - MAE          : 207.582658\n",
            "HAT_Reg - MSE          : 43276.8039\n",
            "HAT_Reg - MAPE          : 2.6877\n",
            "HAT_Reg - MAE          : 208.030776\n",
            "ARF_Reg - MSE          : 2610.8028\n",
            "ARF_Reg - MAPE          : 0.6602\n",
            "ARF_Reg - MAE          : 51.096015\n",
            "PA_Reg - MSE          : 272519785.8765\n",
            "PA_Reg - MAPE          : 213.2839\n",
            "PA_Reg - MAE          : 16508.173305\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.53s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 55206.9357\n",
            "HT_Reg - MAPE          : 11.7481\n",
            "HT_Reg - MAE          : 234.961562\n",
            "HAT_Reg - MSE          : 55162.6107\n",
            "HAT_Reg - MAPE          : 11.7434\n",
            "HAT_Reg - MAE          : 234.867219\n",
            "ARF_Reg - MSE          : 13098.7931\n",
            "ARF_Reg - MAPE          : 5.7225\n",
            "ARF_Reg - MAE          : 114.449959\n",
            "PA_Reg - MSE          : 86122009.1811\n",
            "PA_Reg - MAPE          : 464.0097\n",
            "PA_Reg - MAE          : 9280.194458\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.33s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 20599.1257\n",
            "HT_Reg - MAPE          : 18.8847\n",
            "HT_Reg - MAE          : 143.523955\n",
            "HAT_Reg - MSE          : 20586.0635\n",
            "HAT_Reg - MAPE          : 18.8787\n",
            "HAT_Reg - MAE          : 143.478443\n",
            "ARF_Reg - MSE          : 6718.4760\n",
            "ARF_Reg - MAPE          : 10.7850\n",
            "ARF_Reg - MAE          : 81.966310\n",
            "PA_Reg - MSE          : 80179733.3688\n",
            "PA_Reg - MAPE          : 1178.1992\n",
            "PA_Reg - MAE          : 8954.313674\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.11s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 7639.7206\n",
            "HT_Reg - MAPE          : 17.1383\n",
            "HT_Reg - MAE          : 87.405495\n",
            "HAT_Reg - MSE          : 7639.8238\n",
            "HAT_Reg - MAPE          : 17.1384\n",
            "HAT_Reg - MAE          : 87.406085\n",
            "ARF_Reg - MSE          : 1.3362\n",
            "ARF_Reg - MAPE          : 0.2267\n",
            "ARF_Reg - MAE          : 1.155960\n",
            "PA_Reg - MSE          : 459823.9756\n",
            "PA_Reg - MAPE          : 132.9614\n",
            "PA_Reg - MAE          : 678.103219\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.50s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 82.7195\n",
            "HT_Reg - MAPE          : 0.1226\n",
            "HT_Reg - MAE          : 9.095024\n",
            "HAT_Reg - MSE          : 287.9216\n",
            "HAT_Reg - MAPE          : 0.2287\n",
            "HAT_Reg - MAE          : 16.968251\n",
            "ARF_Reg - MSE          : 488.7897\n",
            "ARF_Reg - MAPE          : 0.2980\n",
            "ARF_Reg - MAE          : 22.108589\n",
            "PA_Reg - MSE          : 4582941.5539\n",
            "PA_Reg - MAPE          : 28.8515\n",
            "PA_Reg - MAE          : 2140.780595\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.09s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1576.1784\n",
            "HT_Reg - MAPE          : 2.1345\n",
            "HT_Reg - MAE          : 39.701114\n",
            "HAT_Reg - MSE          : 106.7960\n",
            "HAT_Reg - MAPE          : 0.5556\n",
            "HAT_Reg - MAE          : 10.334214\n",
            "ARF_Reg - MSE          : 23326.1719\n",
            "ARF_Reg - MAPE          : 8.2112\n",
            "ARF_Reg - MAE          : 152.729080\n",
            "PA_Reg - MSE          : 1078212.7220\n",
            "PA_Reg - MAPE          : 55.8264\n",
            "PA_Reg - MAE          : 1038.370224\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.76s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 787.3932\n",
            "HT_Reg - MAPE          : 1.5250\n",
            "HT_Reg - MAE          : 28.060528\n",
            "HAT_Reg - MSE          : 738.3766\n",
            "HAT_Reg - MAPE          : 1.4768\n",
            "HAT_Reg - MAE          : 27.173086\n",
            "ARF_Reg - MSE          : 200.4288\n",
            "ARF_Reg - MAPE          : 0.7694\n",
            "ARF_Reg - MAE          : 14.157289\n",
            "PA_Reg - MSE          : 390621.0681\n",
            "PA_Reg - MAPE          : 33.9672\n",
            "PA_Reg - MAE          : 624.996854\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.65s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1391.9989\n",
            "HT_Reg - MAPE          : 5.0418\n",
            "HT_Reg - MAE          : 37.309502\n",
            "HAT_Reg - MSE          : 1420.6907\n",
            "HAT_Reg - MAPE          : 5.0935\n",
            "HAT_Reg - MAE          : 37.692050\n",
            "ARF_Reg - MSE          : 135.1135\n",
            "ARF_Reg - MAPE          : 1.5708\n",
            "ARF_Reg - MAE          : 11.623834\n",
            "PA_Reg - MSE          : 295112.8189\n",
            "PA_Reg - MAPE          : 73.4112\n",
            "PA_Reg - MAE          : 543.242873\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.34s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 52.7690\n",
            "HT_Reg - MAPE          : 0.1333\n",
            "HT_Reg - MAE          : 7.264225\n",
            "HAT_Reg - MSE          : 50.7288\n",
            "HAT_Reg - MAPE          : 0.1307\n",
            "HAT_Reg - MAE          : 7.122419\n",
            "ARF_Reg - MSE          : 43.7854\n",
            "ARF_Reg - MAPE          : 0.1214\n",
            "ARF_Reg - MAE          : 6.617052\n",
            "PA_Reg - MSE          : 93.6184\n",
            "PA_Reg - MAPE          : 0.1775\n",
            "PA_Reg - MAE          : 9.675662\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.31s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 981.0378\n",
            "HT_Reg - MAPE          : 0.2797\n",
            "HT_Reg - MAE          : 31.321522\n",
            "HAT_Reg - MSE          : 980.4545\n",
            "HAT_Reg - MAPE          : 0.2796\n",
            "HAT_Reg - MAE          : 31.312211\n",
            "ARF_Reg - MSE          : 61.8695\n",
            "ARF_Reg - MAPE          : 0.0702\n",
            "ARF_Reg - MAE          : 7.865718\n",
            "PA_Reg - MSE          : 281257.5024\n",
            "PA_Reg - MAPE          : 4.7352\n",
            "PA_Reg - MAE          : 530.337159\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.09s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3682.3193\n",
            "HT_Reg - MAPE          : 0.4211\n",
            "HT_Reg - MAE          : 60.682117\n",
            "HAT_Reg - MSE          : 3683.9594\n",
            "HAT_Reg - MAPE          : 0.4212\n",
            "HAT_Reg - MAE          : 60.695629\n",
            "ARF_Reg - MSE          : 1114.1146\n",
            "ARF_Reg - MAPE          : 0.2316\n",
            "ARF_Reg - MAE          : 33.378355\n",
            "PA_Reg - MSE          : 655134.1712\n",
            "PA_Reg - MAPE          : 5.6170\n",
            "PA_Reg - MAE          : 809.403590\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [7.03s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 162394.5846\n",
            "HT_Reg - MAPE          : 0.7273\n",
            "HT_Reg - MAE          : 402.982115\n",
            "HAT_Reg - MSE          : 162394.5407\n",
            "HAT_Reg - MAPE          : 0.7273\n",
            "HAT_Reg - MAE          : 402.982060\n",
            "ARF_Reg - MSE          : 107366.9801\n",
            "ARF_Reg - MAPE          : 0.5914\n",
            "ARF_Reg - MAE          : 327.669010\n",
            "PA_Reg - MSE          : 550467.0994\n",
            "PA_Reg - MAPE          : 1.3390\n",
            "PA_Reg - MAE          : 741.934700\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.51s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.2439\n",
            "HT_Reg - MAPE          : 0.0499\n",
            "HT_Reg - MAE          : 0.493861\n",
            "HAT_Reg - MSE          : 4.6534\n",
            "HAT_Reg - MAPE          : 0.2179\n",
            "HAT_Reg - MAE          : 2.157173\n",
            "ARF_Reg - MSE          : 89.7825\n",
            "ARF_Reg - MAPE          : 0.9571\n",
            "ARF_Reg - MAE          : 9.475365\n",
            "PA_Reg - MSE          : 1140232.2858\n",
            "PA_Reg - MAPE          : 107.8603\n",
            "PA_Reg - MAE          : 1067.816597\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.17s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 44.5177\n",
            "HT_Reg - MAPE          : 0.2391\n",
            "HT_Reg - MAE          : 6.672160\n",
            "HAT_Reg - MSE          : 59.3691\n",
            "HAT_Reg - MAPE          : 0.2762\n",
            "HAT_Reg - MAE          : 7.705137\n",
            "ARF_Reg - MSE          : 196.8943\n",
            "ARF_Reg - MAPE          : 0.5029\n",
            "ARF_Reg - MAE          : 14.031904\n",
            "PA_Reg - MSE          : 447373.3285\n",
            "PA_Reg - MAPE          : 23.9735\n",
            "PA_Reg - MAE          : 668.859723\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.73s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 5.9723\n",
            "HT_Reg - MAPE          : 0.0537\n",
            "HT_Reg - MAE          : 2.443830\n",
            "HAT_Reg - MSE          : 6.5137\n",
            "HAT_Reg - MAPE          : 0.0561\n",
            "HAT_Reg - MAE          : 2.552190\n",
            "ARF_Reg - MSE          : 0.2068\n",
            "ARF_Reg - MAPE          : 0.0100\n",
            "ARF_Reg - MAE          : 0.454782\n",
            "PA_Reg - MSE          : 32925.5070\n",
            "PA_Reg - MAPE          : 3.9880\n",
            "PA_Reg - MAE          : 181.453870\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.52s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 8.9357\n",
            "HT_Reg - MAPE          : 0.0594\n",
            "HT_Reg - MAE          : 2.989259\n",
            "HAT_Reg - MSE          : 8.8849\n",
            "HAT_Reg - MAPE          : 0.0593\n",
            "HAT_Reg - MAE          : 2.980760\n",
            "ARF_Reg - MSE          : 4.1858\n",
            "ARF_Reg - MAPE          : 0.0407\n",
            "ARF_Reg - MAE          : 2.045912\n",
            "PA_Reg - MSE          : 90280.8686\n",
            "PA_Reg - MAPE          : 5.9735\n",
            "PA_Reg - MAE          : 300.467750\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.56s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 52284.0660\n",
            "HT_Reg - MAPE          : 0.7759\n",
            "HT_Reg - MAE          : 228.657093\n",
            "HAT_Reg - MSE          : 52283.3830\n",
            "HAT_Reg - MAPE          : 0.7759\n",
            "HAT_Reg - MAE          : 228.655599\n",
            "ARF_Reg - MSE          : 50275.7328\n",
            "ARF_Reg - MAPE          : 0.7609\n",
            "ARF_Reg - MAE          : 224.222507\n",
            "PA_Reg - MSE          : 10679.5992\n",
            "PA_Reg - MAPE          : 0.3507\n",
            "PA_Reg - MAE          : 103.342146\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.70s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 200.2202\n",
            "HT_Reg - MAPE          : 0.1330\n",
            "HT_Reg - MAE          : 14.149917\n",
            "HAT_Reg - MSE          : 200.2198\n",
            "HAT_Reg - MAPE          : 0.1330\n",
            "HAT_Reg - MAE          : 14.149904\n",
            "ARF_Reg - MSE          : 156.7009\n",
            "ARF_Reg - MAPE          : 0.1177\n",
            "ARF_Reg - MAE          : 12.518022\n",
            "PA_Reg - MSE          : 114262.3728\n",
            "PA_Reg - MAPE          : 3.1769\n",
            "PA_Reg - MAE          : 338.027178\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.75s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 102.3270\n",
            "HT_Reg - MAPE          : 0.1193\n",
            "HT_Reg - MAE          : 10.115679\n",
            "HAT_Reg - MSE          : 102.3270\n",
            "HAT_Reg - MAPE          : 0.1193\n",
            "HAT_Reg - MAE          : 10.115681\n",
            "ARF_Reg - MSE          : 0.4103\n",
            "ARF_Reg - MAPE          : 0.0076\n",
            "ARF_Reg - MAE          : 0.640568\n",
            "PA_Reg - MSE          : 528324.2758\n",
            "PA_Reg - MAPE          : 8.5715\n",
            "PA_Reg - MAE          : 726.859186\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [7.57s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 24129.3113\n",
            "HT_Reg - MAPE          : 0.5152\n",
            "HT_Reg - MAE          : 155.336124\n",
            "HAT_Reg - MSE          : 24129.3112\n",
            "HAT_Reg - MAPE          : 0.5152\n",
            "HAT_Reg - MAE          : 155.336123\n",
            "ARF_Reg - MSE          : 27484.9558\n",
            "ARF_Reg - MAPE          : 0.5499\n",
            "ARF_Reg - MAE          : 165.785873\n",
            "PA_Reg - MSE          : 37374.9813\n",
            "PA_Reg - MAPE          : 0.6412\n",
            "PA_Reg - MAE          : 193.326101\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.54s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1671.2252\n",
            "HT_Reg - MAPE          : 2.1292\n",
            "HT_Reg - MAE          : 40.880621\n",
            "HAT_Reg - MSE          : 732.1243\n",
            "HAT_Reg - MAPE          : 1.4093\n",
            "HAT_Reg - MAE          : 27.057795\n",
            "ARF_Reg - MSE          : 104.1363\n",
            "ARF_Reg - MAPE          : 0.5315\n",
            "ARF_Reg - MAE          : 10.204720\n",
            "PA_Reg - MSE          : 1656543.9318\n",
            "PA_Reg - MAPE          : 67.0348\n",
            "PA_Reg - MAE          : 1287.067959\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.27s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1357.6663\n",
            "HT_Reg - MAPE          : 3.9198\n",
            "HT_Reg - MAE          : 36.846523\n",
            "HAT_Reg - MSE          : 1980.5124\n",
            "HAT_Reg - MAPE          : 4.7344\n",
            "HAT_Reg - MAE          : 44.502948\n",
            "ARF_Reg - MSE          : 514675.3187\n",
            "ARF_Reg - MAPE          : 76.3201\n",
            "ARF_Reg - MAE          : 717.408753\n",
            "PA_Reg - MSE          : 802328156.9897\n",
            "PA_Reg - MAPE          : 3013.3402\n",
            "PA_Reg - MAE          : 28325.397738\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.68s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 115.6256\n",
            "HT_Reg - MAPE          : 0.4336\n",
            "HT_Reg - MAE          : 10.752934\n",
            "HAT_Reg - MSE          : 89.6682\n",
            "HAT_Reg - MAPE          : 0.3818\n",
            "HAT_Reg - MAE          : 9.469328\n",
            "ARF_Reg - MSE          : 248.5995\n",
            "ARF_Reg - MAPE          : 0.6358\n",
            "ARF_Reg - MAE          : 15.767039\n",
            "PA_Reg - MSE          : 455477.3371\n",
            "PA_Reg - MAPE          : 27.2133\n",
            "PA_Reg - MAE          : 674.890611\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.58s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 46665.4402\n",
            "HT_Reg - MAPE          : 0.6012\n",
            "HT_Reg - MAE          : 216.021851\n",
            "HAT_Reg - MSE          : 46595.6620\n",
            "HAT_Reg - MAPE          : 0.6008\n",
            "HAT_Reg - MAE          : 215.860284\n",
            "ARF_Reg - MSE          : 35377.9862\n",
            "ARF_Reg - MAPE          : 0.5235\n",
            "ARF_Reg - MAE          : 188.090367\n",
            "PA_Reg - MSE          : 3739.6066\n",
            "PA_Reg - MAPE          : 0.1702\n",
            "PA_Reg - MAE          : 61.152323\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.33s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 160.0816\n",
            "HT_Reg - MAPE          : 0.0627\n",
            "HT_Reg - MAE          : 12.652335\n",
            "HAT_Reg - MSE          : 159.5477\n",
            "HAT_Reg - MAPE          : 0.0626\n",
            "HAT_Reg - MAE          : 12.631221\n",
            "ARF_Reg - MSE          : 780.1218\n",
            "ARF_Reg - MAPE          : 0.1384\n",
            "ARF_Reg - MAE          : 27.930661\n",
            "PA_Reg - MSE          : 5653985.8545\n",
            "PA_Reg - MAPE          : 11.7830\n",
            "PA_Reg - MAE          : 2377.811148\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.49s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 352.0639\n",
            "HT_Reg - MAPE          : 0.6448\n",
            "HT_Reg - MAE          : 18.763367\n",
            "HAT_Reg - MSE          : 352.1333\n",
            "HAT_Reg - MAPE          : 0.6449\n",
            "HAT_Reg - MAE          : 18.765216\n",
            "ARF_Reg - MSE          : 209336.9678\n",
            "ARF_Reg - MAPE          : 15.7228\n",
            "ARF_Reg - MAE          : 457.533570\n",
            "PA_Reg - MSE          : 294215.7223\n",
            "PA_Reg - MAPE          : 18.6397\n",
            "PA_Reg - MAE          : 542.416558\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.29s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 65.4000\n",
            "HT_Reg - MAPE          : 0.5118\n",
            "HT_Reg - MAE          : 8.087025\n",
            "HAT_Reg - MSE          : 65.0527\n",
            "HAT_Reg - MAPE          : 0.5105\n",
            "HAT_Reg - MAE          : 8.065523\n",
            "ARF_Reg - MSE          : 226738.0648\n",
            "ARF_Reg - MAPE          : 30.1374\n",
            "ARF_Reg - MAE          : 476.170206\n",
            "PA_Reg - MSE          : 6689136.7353\n",
            "PA_Reg - MAPE          : 163.6922\n",
            "PA_Reg - MAE          : 2586.336547\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [7.00s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 24154.8018\n",
            "HT_Reg - MAPE          : 11.7741\n",
            "HT_Reg - MAE          : 155.418151\n",
            "HAT_Reg - MSE          : 24154.8541\n",
            "HAT_Reg - MAPE          : 11.7741\n",
            "HAT_Reg - MAE          : 155.418320\n",
            "ARF_Reg - MSE          : 30.0279\n",
            "ARF_Reg - MAPE          : 0.4151\n",
            "ARF_Reg - MAE          : 5.479770\n",
            "PA_Reg - MSE          : 516246.7929\n",
            "PA_Reg - MAPE          : 54.4321\n",
            "PA_Reg - MAE          : 718.503161\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.52s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.0522\n",
            "HT_Reg - MAPE          : 0.0022\n",
            "HT_Reg - MAE          : 0.228579\n",
            "HAT_Reg - MSE          : 2.6986\n",
            "HAT_Reg - MAPE          : 0.0158\n",
            "HAT_Reg - MAE          : 1.642737\n",
            "ARF_Reg - MSE          : 681.8495\n",
            "ARF_Reg - MAPE          : 0.2518\n",
            "ARF_Reg - MAE          : 26.112248\n",
            "PA_Reg - MSE          : 2782981.8162\n",
            "PA_Reg - MAPE          : 16.0871\n",
            "PA_Reg - MAE          : 1668.227148\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.09s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 15.1165\n",
            "HT_Reg - MAPE          : 0.1062\n",
            "HT_Reg - MAE          : 3.887994\n",
            "HAT_Reg - MSE          : 86.0460\n",
            "HAT_Reg - MAPE          : 0.2534\n",
            "HAT_Reg - MAE          : 9.276100\n",
            "ARF_Reg - MSE          : 7093.1824\n",
            "ARF_Reg - MAPE          : 2.3011\n",
            "ARF_Reg - MAE          : 84.221033\n",
            "PA_Reg - MSE          : 860610.4708\n",
            "PA_Reg - MAPE          : 25.3467\n",
            "PA_Reg - MAE          : 927.690935\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.78s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 16.7797\n",
            "HT_Reg - MAPE          : 0.5689\n",
            "HT_Reg - MAE          : 4.096305\n",
            "HAT_Reg - MSE          : 13.1829\n",
            "HAT_Reg - MAPE          : 0.5043\n",
            "HAT_Reg - MAE          : 3.630831\n",
            "ARF_Reg - MSE          : 61.5604\n",
            "ARF_Reg - MAPE          : 1.0897\n",
            "ARF_Reg - MAE          : 7.846042\n",
            "PA_Reg - MSE          : 107319.4659\n",
            "PA_Reg - MAPE          : 45.4995\n",
            "PA_Reg - MAE          : 327.596499\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.50s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 546.4221\n",
            "HT_Reg - MAPE          : 2.8163\n",
            "HT_Reg - MAE          : 23.375674\n",
            "HAT_Reg - MSE          : 550.0124\n",
            "HAT_Reg - MAPE          : 2.8256\n",
            "HAT_Reg - MAE          : 23.452342\n",
            "ARF_Reg - MSE          : 172.6305\n",
            "ARF_Reg - MAPE          : 1.5830\n",
            "ARF_Reg - MAE          : 13.138894\n",
            "PA_Reg - MSE          : 1703.1968\n",
            "PA_Reg - MAPE          : 4.9723\n",
            "PA_Reg - MAE          : 41.269804\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.97s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 254.7943\n",
            "HT_Reg - MAPE          : 0.6542\n",
            "HT_Reg - MAE          : 15.962278\n",
            "HAT_Reg - MSE          : 254.7363\n",
            "HAT_Reg - MAPE          : 0.6541\n",
            "HAT_Reg - MAE          : 15.960460\n",
            "ARF_Reg - MSE          : 15.2770\n",
            "ARF_Reg - MAPE          : 0.1602\n",
            "ARF_Reg - MAE          : 3.908574\n",
            "PA_Reg - MSE          : 561.6458\n",
            "PA_Reg - MAPE          : 0.9713\n",
            "PA_Reg - MAE          : 23.699067\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [6.27s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1116.1605\n",
            "HT_Reg - MAPE          : 0.3814\n",
            "HT_Reg - MAE          : 33.408988\n",
            "HAT_Reg - MSE          : 1116.1607\n",
            "HAT_Reg - MAPE          : 0.3814\n",
            "HAT_Reg - MAE          : 33.408992\n",
            "ARF_Reg - MSE          : 133.6755\n",
            "ARF_Reg - MAPE          : 0.1320\n",
            "ARF_Reg - MAE          : 11.561814\n",
            "PA_Reg - MSE          : 11.2480\n",
            "PA_Reg - MAPE          : 0.0383\n",
            "PA_Reg - MAE          : 3.353807\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.21s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 10944.4508\n",
            "HT_Reg - MAPE          : 0.5354\n",
            "HT_Reg - MAE          : 104.615729\n",
            "HAT_Reg - MSE          : 10944.4504\n",
            "HAT_Reg - MAPE          : 0.5354\n",
            "HAT_Reg - MAE          : 104.615727\n",
            "ARF_Reg - MSE          : 11117.9734\n",
            "ARF_Reg - MAPE          : 0.5396\n",
            "ARF_Reg - MAE          : 105.441801\n",
            "PA_Reg - MSE          : 45100.2261\n",
            "PA_Reg - MAPE          : 1.0868\n",
            "PA_Reg - MAE          : 212.368138\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [7.38s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 43169.0066\n",
            "HT_Reg - MAPE          : 0.5044\n",
            "HT_Reg - MAE          : 207.771525\n",
            "HAT_Reg - MSE          : 43169.0066\n",
            "HAT_Reg - MAPE          : 0.5044\n",
            "HAT_Reg - MAE          : 207.771525\n",
            "ARF_Reg - MSE          : 42629.8628\n",
            "ARF_Reg - MAPE          : 0.5013\n",
            "ARF_Reg - MAE          : 206.470004\n",
            "PA_Reg - MSE          : 1167921.9061\n",
            "PA_Reg - MAPE          : 2.6237\n",
            "PA_Reg - MAE          : 1080.704356\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.45s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 85.9873\n",
            "HT_Reg - MAPE          : 0.6624\n",
            "HT_Reg - MAE          : 9.272933\n",
            "HAT_Reg - MSE          : 87.8195\n",
            "HAT_Reg - MAPE          : 0.6694\n",
            "HAT_Reg - MAE          : 9.371205\n",
            "ARF_Reg - MSE          : 65.7523\n",
            "ARF_Reg - MAPE          : 0.5792\n",
            "ARF_Reg - MAE          : 8.108779\n",
            "PA_Reg - MSE          : 49469.2481\n",
            "PA_Reg - MAPE          : 15.8869\n",
            "PA_Reg - MAE          : 222.416834\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.11s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.6213\n",
            "HT_Reg - MAPE          : 0.1251\n",
            "HT_Reg - MAE          : 0.788212\n",
            "HAT_Reg - MSE          : 357.6407\n",
            "HAT_Reg - MAPE          : 3.0018\n",
            "HAT_Reg - MAE          : 18.911391\n",
            "ARF_Reg - MSE          : 16664.9609\n",
            "ARF_Reg - MAPE          : 20.4909\n",
            "ARF_Reg - MAE          : 129.092838\n",
            "PA_Reg - MSE          : 804.5108\n",
            "PA_Reg - MAPE          : 4.5022\n",
            "PA_Reg - MAE          : 28.363900\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.75s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 24.3391\n",
            "HT_Reg - MAPE          : 6.1668\n",
            "HT_Reg - MAE          : 4.933470\n",
            "HAT_Reg - MSE          : 26.3996\n",
            "HAT_Reg - MAPE          : 6.4226\n",
            "HAT_Reg - MAE          : 5.138056\n",
            "ARF_Reg - MSE          : 29.3585\n",
            "ARF_Reg - MAPE          : 6.7729\n",
            "ARF_Reg - MAE          : 5.418347\n",
            "PA_Reg - MSE          : 124.5418\n",
            "PA_Reg - MAPE          : 13.9498\n",
            "PA_Reg - MAE          : 11.159830\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.95s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 49.6291\n",
            "HT_Reg - MAPE          : 1.6773\n",
            "HT_Reg - MAE          : 7.044794\n",
            "HAT_Reg - MSE          : 49.5103\n",
            "HAT_Reg - MAPE          : 1.6753\n",
            "HAT_Reg - MAE          : 7.036353\n",
            "ARF_Reg - MSE          : 0.4514\n",
            "ARF_Reg - MAPE          : 0.1600\n",
            "ARF_Reg - MAE          : 0.671850\n",
            "PA_Reg - MSE          : 151.3062\n",
            "PA_Reg - MAPE          : 2.9287\n",
            "PA_Reg - MAE          : 12.300657\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.81s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 34.1998\n",
            "HT_Reg - MAPE          : 0.4006\n",
            "HT_Reg - MAE          : 5.848056\n",
            "HAT_Reg - MSE          : 34.1566\n",
            "HAT_Reg - MAPE          : 0.4003\n",
            "HAT_Reg - MAE          : 5.844362\n",
            "ARF_Reg - MSE          : 4.2340\n",
            "ARF_Reg - MAPE          : 0.1409\n",
            "ARF_Reg - MAE          : 2.057667\n",
            "PA_Reg - MSE          : 468.7710\n",
            "PA_Reg - MAPE          : 1.4830\n",
            "PA_Reg - MAE          : 21.651119\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [6.51s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 500.1882\n",
            "HT_Reg - MAPE          : 0.5083\n",
            "HT_Reg - MAE          : 22.364889\n",
            "HAT_Reg - MSE          : 500.2420\n",
            "HAT_Reg - MAPE          : 0.5083\n",
            "HAT_Reg - MAE          : 22.366090\n",
            "ARF_Reg - MSE          : 319.5947\n",
            "ARF_Reg - MAPE          : 0.4063\n",
            "ARF_Reg - MAE          : 17.877211\n",
            "PA_Reg - MSE          : 928.7607\n",
            "PA_Reg - MAPE          : 0.6926\n",
            "PA_Reg - MAE          : 30.475576\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [7.43s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1128.3467\n",
            "HT_Reg - MAPE          : 0.5732\n",
            "HT_Reg - MAE          : 33.590872\n",
            "HAT_Reg - MSE          : 1128.3395\n",
            "HAT_Reg - MAPE          : 0.5732\n",
            "HAT_Reg - MAE          : 33.590766\n",
            "ARF_Reg - MSE          : 894.7379\n",
            "ARF_Reg - MAPE          : 0.5104\n",
            "ARF_Reg - MAE          : 29.912170\n",
            "PA_Reg - MSE          : 2422.1876\n",
            "PA_Reg - MAPE          : 0.8399\n",
            "PA_Reg - MAE          : 49.215725\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [8.30s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 73019.5459\n",
            "HT_Reg - MAPE          : 0.7962\n",
            "HT_Reg - MAE          : 270.221291\n",
            "HAT_Reg - MSE          : 73019.5459\n",
            "HAT_Reg - MAPE          : 0.7962\n",
            "HAT_Reg - MAE          : 270.221291\n",
            "ARF_Reg - MSE          : 73398.6493\n",
            "ARF_Reg - MAPE          : 0.7982\n",
            "ARF_Reg - MAE          : 270.921851\n",
            "PA_Reg - MSE          : 121524.8893\n",
            "PA_Reg - MAPE          : 1.0271\n",
            "PA_Reg - MAE          : 348.604201\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.50s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 9.3085\n",
            "HT_Reg - MAPE          : 0.8029\n",
            "HT_Reg - MAE          : 3.050991\n",
            "HAT_Reg - MSE          : 53.7400\n",
            "HAT_Reg - MAPE          : 1.9291\n",
            "HAT_Reg - MAE          : 7.330757\n",
            "ARF_Reg - MSE          : 268.3425\n",
            "ARF_Reg - MAPE          : 4.3108\n",
            "ARF_Reg - MAE          : 16.381162\n",
            "PA_Reg - MSE          : 2573.1938\n",
            "PA_Reg - MAPE          : 13.3491\n",
            "PA_Reg - MAE          : 50.726657\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.08s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 77.3378\n",
            "HT_Reg - MAPE          : 29.3140\n",
            "HT_Reg - MAE          : 8.794192\n",
            "HAT_Reg - MSE          : 145.4464\n",
            "HAT_Reg - MAPE          : 40.2004\n",
            "HAT_Reg - MAE          : 12.060118\n",
            "ARF_Reg - MSE          : 12224.2298\n",
            "ARF_Reg - MAPE          : 368.5441\n",
            "ARF_Reg - MAE          : 110.563239\n",
            "PA_Reg - MSE          : 131505.1166\n",
            "PA_Reg - MAPE          : 1208.7878\n",
            "PA_Reg - MAE          : 362.636342\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.77s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 39.2084\n",
            "HT_Reg - MAPE          : 7.8271\n",
            "HT_Reg - MAE          : 6.261662\n",
            "HAT_Reg - MSE          : 37.5179\n",
            "HAT_Reg - MAPE          : 7.6565\n",
            "HAT_Reg - MAE          : 6.125186\n",
            "ARF_Reg - MSE          : 62.9451\n",
            "ARF_Reg - MAPE          : 9.9172\n",
            "ARF_Reg - MAE          : 7.933794\n",
            "PA_Reg - MSE          : 349.1952\n",
            "PA_Reg - MAPE          : 23.3585\n",
            "PA_Reg - MAE          : 18.686765\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.32s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 58.1056\n",
            "HT_Reg - MAPE          : 6.9297\n",
            "HT_Reg - MAE          : 7.622703\n",
            "HAT_Reg - MSE          : 53.1124\n",
            "HAT_Reg - MAPE          : 6.6253\n",
            "HAT_Reg - MAE          : 7.287822\n",
            "ARF_Reg - MSE          : 0.0618\n",
            "ARF_Reg - MAPE          : 0.2260\n",
            "ARF_Reg - MAE          : 0.248593\n",
            "PA_Reg - MSE          : 359.3328\n",
            "PA_Reg - MAPE          : 17.2328\n",
            "PA_Reg - MAE          : 18.956076\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.84s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.1730\n",
            "HT_Reg - MAPE          : 0.0533\n",
            "HT_Reg - MAE          : 0.415947\n",
            "HAT_Reg - MSE          : 0.1032\n",
            "HAT_Reg - MAPE          : 0.0412\n",
            "HAT_Reg - MAE          : 0.321184\n",
            "ARF_Reg - MSE          : 0.0000\n",
            "ARF_Reg - MAPE          : 0.0005\n",
            "ARF_Reg - MAE          : 0.003715\n",
            "PA_Reg - MSE          : 93.1629\n",
            "PA_Reg - MAPE          : 1.2374\n",
            "PA_Reg - MAE          : 9.652093\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.79s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 687.9730\n",
            "HT_Reg - MAPE          : 0.7625\n",
            "HT_Reg - MAE          : 26.229239\n",
            "HAT_Reg - MSE          : 679.4837\n",
            "HAT_Reg - MAPE          : 0.7578\n",
            "HAT_Reg - MAE          : 26.066908\n",
            "ARF_Reg - MSE          : 247.1207\n",
            "ARF_Reg - MAPE          : 0.4570\n",
            "ARF_Reg - MAE          : 15.720072\n",
            "PA_Reg - MSE          : 2.6528\n",
            "PA_Reg - MAPE          : 0.0473\n",
            "PA_Reg - MAE          : 1.628739\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.23s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1293.2105\n",
            "HT_Reg - MAPE          : 0.5645\n",
            "HT_Reg - MAE          : 35.961236\n",
            "HAT_Reg - MSE          : 1292.8823\n",
            "HAT_Reg - MAPE          : 0.5645\n",
            "HAT_Reg - MAE          : 35.956673\n",
            "ARF_Reg - MSE          : 1144.0206\n",
            "ARF_Reg - MAPE          : 0.5310\n",
            "ARF_Reg - MAE          : 33.823374\n",
            "PA_Reg - MSE          : 3940.2300\n",
            "PA_Reg - MAPE          : 0.9854\n",
            "PA_Reg - MAE          : 62.771251\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [7.19s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.1522\n",
            "HT_Reg - MAPE          : 0.0328\n",
            "HT_Reg - MAE          : 0.390135\n",
            "HAT_Reg - MSE          : 0.1554\n",
            "HAT_Reg - MAPE          : 0.0331\n",
            "HAT_Reg - MAE          : 0.394152\n",
            "ARF_Reg - MSE          : 2.9851\n",
            "ARF_Reg - MAPE          : 0.1452\n",
            "ARF_Reg - MAE          : 1.727750\n",
            "PA_Reg - MSE          : 1879.5196\n",
            "PA_Reg - MAPE          : 3.6431\n",
            "PA_Reg - MAE          : 43.353426\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.48s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.8796\n",
            "HT_Reg - MAPE          : 0.0957\n",
            "HT_Reg - MAE          : 0.937861\n",
            "HAT_Reg - MSE          : 23.7648\n",
            "HAT_Reg - MAPE          : 0.4974\n",
            "HAT_Reg - MAE          : 4.874919\n",
            "ARF_Reg - MSE          : 6.0783\n",
            "ARF_Reg - MAPE          : 0.2516\n",
            "ARF_Reg - MAE          : 2.465417\n",
            "PA_Reg - MSE          : 871.3951\n",
            "PA_Reg - MAPE          : 3.0122\n",
            "PA_Reg - MAE          : 29.519403\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.09s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 9.9908\n",
            "HT_Reg - MAPE          : 1.9755\n",
            "HT_Reg - MAE          : 3.160825\n",
            "HAT_Reg - MSE          : 0.2017\n",
            "HAT_Reg - MAPE          : 0.2807\n",
            "HAT_Reg - MAE          : 0.449112\n",
            "ARF_Reg - MSE          : 6.9424\n",
            "ARF_Reg - MAPE          : 1.6468\n",
            "ARF_Reg - MAE          : 2.634838\n",
            "PA_Reg - MSE          : 471.0121\n",
            "PA_Reg - MAPE          : 13.5643\n",
            "PA_Reg - MAE          : 21.702812\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.78s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1.9705\n",
            "HT_Reg - MAPE          : 7.0187\n",
            "HT_Reg - MAE          : 1.403732\n",
            "HAT_Reg - MSE          : 2.2892\n",
            "HAT_Reg - MAPE          : 7.5650\n",
            "HAT_Reg - MAE          : 1.513002\n",
            "ARF_Reg - MSE          : 3.0229\n",
            "ARF_Reg - MAPE          : 8.6933\n",
            "ARF_Reg - MAE          : 1.738653\n",
            "PA_Reg - MSE          : 44.5744\n",
            "PA_Reg - MAPE          : 33.3820\n",
            "PA_Reg - MAE          : 6.676407\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            " #################### [99%] [2.36s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 12.2335\n",
            "HT_Reg - MAPE          : nan\n",
            "HT_Reg - MAE          : 3.497648\n",
            "HAT_Reg - MSE          : 12.1959\n",
            "HAT_Reg - MAPE          : nan\n",
            "HAT_Reg - MAE          : 3.492264\n",
            "ARF_Reg - MSE          : 8.5091\n",
            "ARF_Reg - MAPE          : nan\n",
            "ARF_Reg - MAE          : 2.917038\n",
            "PA_Reg - MSE          : 21.5770\n",
            "PA_Reg - MAPE          : nan\n",
            "PA_Reg - MAE          : 4.645105\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n",
            "/content/src/measure_collection.py:1163: RuntimeWarning: Mean of empty slice.\n",
            "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/content/src/measure_collection.py:1286: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n",
            "/content/src/measure_collection.py:1286: RuntimeWarning: Mean of empty slice.\n",
            "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n",
            "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            "\r #################### [99%] [2.90s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3.4766\n",
            "HT_Reg - MAPE          : nan\n",
            "HT_Reg - MAE          : 1.864557\n",
            "HAT_Reg - MSE          : 3.4769\n",
            "HAT_Reg - MAPE          : nan\n",
            "HAT_Reg - MAE          : 1.864636\n",
            "ARF_Reg - MSE          : 1.7450\n",
            "ARF_Reg - MAPE          : nan\n",
            "ARF_Reg - MAE          : 1.320995\n",
            "PA_Reg - MSE          : 145.4689\n",
            "PA_Reg - MAPE          : nan\n",
            "PA_Reg - MAE          : 12.061051\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.53s]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n",
            "/content/src/measure_collection.py:1163: RuntimeWarning: Mean of empty slice.\n",
            "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/content/src/measure_collection.py:1286: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n",
            "/content/src/measure_collection.py:1286: RuntimeWarning: Mean of empty slice.\n",
            "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n",
            "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.3915\n",
            "HT_Reg - MAPE          : 1.5643\n",
            "HT_Reg - MAE          : 0.625728\n",
            "HAT_Reg - MSE          : 0.3915\n",
            "HAT_Reg - MAPE          : 1.5643\n",
            "HAT_Reg - MAE          : 0.625720\n",
            "ARF_Reg - MSE          : 0.0814\n",
            "ARF_Reg - MAPE          : 0.7134\n",
            "ARF_Reg - MAE          : 0.285367\n",
            "PA_Reg - MSE          : 144.0585\n",
            "PA_Reg - MAPE          : 30.0061\n",
            "PA_Reg - MAE          : 12.002436\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.04s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 17.7751\n",
            "HT_Reg - MAPE          : 0.7027\n",
            "HT_Reg - MAE          : 4.216054\n",
            "HAT_Reg - MSE          : 17.7751\n",
            "HAT_Reg - MAPE          : 0.7027\n",
            "HAT_Reg - MAE          : 4.216054\n",
            "ARF_Reg - MSE          : 11.8703\n",
            "ARF_Reg - MAPE          : 0.5742\n",
            "ARF_Reg - MAE          : 3.445324\n",
            "PA_Reg - MSE          : 67.2202\n",
            "PA_Reg - MAPE          : 1.3665\n",
            "PA_Reg - MAE          : 8.198795\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.51s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 92.2397\n",
            "HT_Reg - MAPE          : 0.5488\n",
            "HT_Reg - MAE          : 9.604150\n",
            "HAT_Reg - MSE          : 92.2397\n",
            "HAT_Reg - MAPE          : 0.5488\n",
            "HAT_Reg - MAE          : 9.604150\n",
            "ARF_Reg - MSE          : 101.3413\n",
            "ARF_Reg - MAPE          : 0.5752\n",
            "ARF_Reg - MAE          : 10.066844\n",
            "PA_Reg - MSE          : 72.0527\n",
            "PA_Reg - MAPE          : 0.4851\n",
            "PA_Reg - MAE          : 8.488384\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.52s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 5.6759\n",
            "HT_Reg - MAPE          : 11.9120\n",
            "HT_Reg - MAE          : 2.382407\n",
            "HAT_Reg - MSE          : 4.5860\n",
            "HAT_Reg - MAPE          : 10.7074\n",
            "HAT_Reg - MAE          : 2.141486\n",
            "ARF_Reg - MSE          : 0.0006\n",
            "ARF_Reg - MAPE          : 0.1238\n",
            "ARF_Reg - MAE          : 0.024762\n",
            "PA_Reg - MSE          : 69.2108\n",
            "PA_Reg - MAPE          : 41.5965\n",
            "PA_Reg - MAE          : 8.319302\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.11s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1.2399\n",
            "HT_Reg - MAPE          : 2.7837\n",
            "HT_Reg - MAE          : 1.113492\n",
            "HAT_Reg - MSE          : 0.7964\n",
            "HAT_Reg - MAPE          : 2.2310\n",
            "HAT_Reg - MAE          : 0.892395\n",
            "ARF_Reg - MSE          : 32.6626\n",
            "ARF_Reg - MAPE          : 14.2878\n",
            "ARF_Reg - MAE          : 5.715123\n",
            "PA_Reg - MSE          : 532.0987\n",
            "PA_Reg - MAPE          : 57.6682\n",
            "PA_Reg - MAE          : 23.067265\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.71s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2.1968\n",
            "HT_Reg - MAPE          : 2.4703\n",
            "HT_Reg - MAE          : 1.482151\n",
            "HAT_Reg - MSE          : 2.2257\n",
            "HAT_Reg - MAPE          : 2.4865\n",
            "HAT_Reg - MAE          : 1.491890\n",
            "ARF_Reg - MSE          : 1.7956\n",
            "ARF_Reg - MAPE          : 2.2333\n",
            "ARF_Reg - MAE          : 1.340008\n",
            "PA_Reg - MSE          : 13.5822\n",
            "PA_Reg - MAPE          : 6.1423\n",
            "PA_Reg - MAE          : 3.685403\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.28s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 141.8958\n",
            "HT_Reg - MAPE          : 0.8759\n",
            "HT_Reg - MAE          : 11.912000\n",
            "HAT_Reg - MSE          : 141.5213\n",
            "HAT_Reg - MAPE          : 0.8747\n",
            "HAT_Reg - MAE          : 11.896272\n",
            "ARF_Reg - MSE          : 133.3945\n",
            "ARF_Reg - MAPE          : 0.8492\n",
            "ARF_Reg - MAE          : 11.549653\n",
            "PA_Reg - MSE          : 180.6551\n",
            "PA_Reg - MAPE          : 0.9883\n",
            "PA_Reg - MAE          : 13.440801\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.09s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 8.1900\n",
            "HT_Reg - MAPE          : 0.6221\n",
            "HT_Reg - MAE          : 2.861817\n",
            "HAT_Reg - MSE          : 8.1848\n",
            "HAT_Reg - MAPE          : 0.6219\n",
            "HAT_Reg - MAE          : 2.860901\n",
            "ARF_Reg - MSE          : 2.7022\n",
            "ARF_Reg - MAPE          : 0.3574\n",
            "ARF_Reg - MAE          : 1.643840\n",
            "PA_Reg - MSE          : 1.7511\n",
            "PA_Reg - MAPE          : 0.2877\n",
            "PA_Reg - MAE          : 1.323308\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.85s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.9571\n",
            "HT_Reg - MAPE          : 1.0870\n",
            "HT_Reg - MAE          : 0.978298\n",
            "HAT_Reg - MSE          : 0.9567\n",
            "HAT_Reg - MAPE          : 1.0868\n",
            "HAT_Reg - MAE          : 0.978108\n",
            "ARF_Reg - MSE          : 222.8393\n",
            "ARF_Reg - MAPE          : 16.5864\n",
            "ARF_Reg - MAE          : 14.927803\n",
            "PA_Reg - MSE          : 10061.7400\n",
            "PA_Reg - MAPE          : 111.4536\n",
            "PA_Reg - MAE          : 100.308225\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [4.55s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.4397\n",
            "HT_Reg - MAPE          : 0.1700\n",
            "HT_Reg - MAE          : 0.663098\n",
            "HAT_Reg - MSE          : 0.4397\n",
            "HAT_Reg - MAPE          : 0.1700\n",
            "HAT_Reg - MAE          : 0.663065\n",
            "ARF_Reg - MSE          : 535.6713\n",
            "ARF_Reg - MAPE          : 5.9345\n",
            "ARF_Reg - MAE          : 23.144575\n",
            "PA_Reg - MSE          : 1843.2283\n",
            "PA_Reg - MAPE          : 11.0084\n",
            "PA_Reg - MAE          : 42.932835\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [5.37s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 6.5659\n",
            "HT_Reg - MAPE          : 0.5024\n",
            "HT_Reg - MAE          : 2.562395\n",
            "HAT_Reg - MSE          : 6.5658\n",
            "HAT_Reg - MAPE          : 0.5024\n",
            "HAT_Reg - MAE          : 2.562373\n",
            "ARF_Reg - MSE          : 0.0452\n",
            "ARF_Reg - MAPE          : 0.0417\n",
            "ARF_Reg - MAE          : 0.212544\n",
            "PA_Reg - MSE          : 31.3775\n",
            "PA_Reg - MAPE          : 1.0983\n",
            "PA_Reg - MAE          : 5.601558\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.54s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3.3777\n",
            "HT_Reg - MAPE          : 0.5405\n",
            "HT_Reg - MAE          : 1.837860\n",
            "HAT_Reg - MSE          : 0.5324\n",
            "HAT_Reg - MAPE          : 0.2146\n",
            "HAT_Reg - MAE          : 0.729624\n",
            "ARF_Reg - MSE          : 0.6445\n",
            "ARF_Reg - MAPE          : 0.2361\n",
            "ARF_Reg - MAE          : 0.802788\n",
            "PA_Reg - MSE          : 49.2132\n",
            "PA_Reg - MAPE          : 2.0633\n",
            "PA_Reg - MAE          : 7.015212\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            " #################### [98%] [1.13s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.4894\n",
            "HT_Reg - MAPE          : 6.9959\n",
            "HT_Reg - MAE          : 0.699586\n",
            "HAT_Reg - MSE          : 1.2750\n",
            "HAT_Reg - MAPE          : 11.2915\n",
            "HAT_Reg - MAE          : 1.129154\n",
            "ARF_Reg - MSE          : 98.8212\n",
            "ARF_Reg - MAPE          : 99.4088\n",
            "ARF_Reg - MAE          : 9.940884\n",
            "PA_Reg - MSE          : 10952.5169\n",
            "PA_Reg - MAPE          : 1046.5427\n",
            "PA_Reg - MAE          : 104.654273\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.68s]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 4.5476\n",
            "HT_Reg - MAPE          : 10.6625\n",
            "HT_Reg - MAE          : 2.132504\n",
            "HAT_Reg - MSE          : 4.2305\n",
            "HAT_Reg - MAPE          : 10.2841\n",
            "HAT_Reg - MAE          : 2.056821\n",
            "ARF_Reg - MSE          : 0.9460\n",
            "ARF_Reg - MAPE          : 4.8631\n",
            "ARF_Reg - MAE          : 0.972620\n",
            "PA_Reg - MSE          : 1720.3498\n",
            "PA_Reg - MAPE          : 207.3855\n",
            "PA_Reg - MAE          : 41.477100\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.34s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.9387\n",
            "HT_Reg - MAPE          : 1.2111\n",
            "HT_Reg - MAE          : 0.968861\n",
            "HAT_Reg - MSE          : 0.8848\n",
            "HAT_Reg - MAPE          : 1.1758\n",
            "HAT_Reg - MAE          : 0.940659\n",
            "ARF_Reg - MSE          : 0.0216\n",
            "ARF_Reg - MAPE          : 0.1836\n",
            "ARF_Reg - MAE          : 0.146882\n",
            "PA_Reg - MSE          : 35.2381\n",
            "PA_Reg - MAPE          : 7.4202\n",
            "PA_Reg - MAE          : 5.936170\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.75s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.3385\n",
            "HT_Reg - MAPE          : 1.9393\n",
            "HT_Reg - MAE          : 0.581796\n",
            "HAT_Reg - MSE          : 0.3577\n",
            "HAT_Reg - MAPE          : 1.9935\n",
            "HAT_Reg - MAE          : 0.598042\n",
            "ARF_Reg - MSE          : 0.0558\n",
            "ARF_Reg - MAPE          : 0.7874\n",
            "ARF_Reg - MAE          : 0.236221\n",
            "PA_Reg - MSE          : 15.7950\n",
            "PA_Reg - MAPE          : 13.2476\n",
            "PA_Reg - MAE          : 3.974293\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.95s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.0868\n",
            "HT_Reg - MAPE          : 0.2947\n",
            "HT_Reg - MAE          : 0.294676\n",
            "HAT_Reg - MSE          : 0.0923\n",
            "HAT_Reg - MAPE          : 0.3038\n",
            "HAT_Reg - MAE          : 0.303779\n",
            "ARF_Reg - MSE          : 0.0193\n",
            "ARF_Reg - MAPE          : 0.1389\n",
            "ARF_Reg - MAE          : 0.138908\n",
            "PA_Reg - MSE          : 0.0754\n",
            "PA_Reg - MAPE          : 0.2746\n",
            "PA_Reg - MAE          : 0.274608\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 210 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.68s]\n",
            "Processed samples: 211\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1.5773\n",
            "HT_Reg - MAPE          : 0.6280\n",
            "HT_Reg - MAE          : 1.255923\n",
            "HAT_Reg - MSE          : 1.5969\n",
            "HAT_Reg - MAPE          : 0.6318\n",
            "HAT_Reg - MAE          : 1.263685\n",
            "ARF_Reg - MSE          : 0.7719\n",
            "ARF_Reg - MAPE          : 0.4393\n",
            "ARF_Reg - MAE          : 0.878591\n",
            "PA_Reg - MSE          : 0.9737\n",
            "PA_Reg - MAPE          : 0.4934\n",
            "PA_Reg - MAE          : 0.986737\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 240 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [7.51s]\n",
            "Processed samples: 241\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 0.0704\n",
            "HT_Reg - MAPE          : 0.1474\n",
            "HT_Reg - MAE          : 0.265394\n",
            "HAT_Reg - MSE          : 0.0691\n",
            "HAT_Reg - MAPE          : 0.1461\n",
            "HAT_Reg - MAE          : 0.262920\n",
            "ARF_Reg - MSE          : 0.2451\n",
            "ARF_Reg - MAPE          : 0.2751\n",
            "ARF_Reg - MAE          : 0.495102\n",
            "PA_Reg - MSE          : 0.0264\n",
            "PA_Reg - MAPE          : 0.0903\n",
            "PA_Reg - MAE          : 0.162617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HQdWByW7GTn",
        "outputId": "4ade03fa-0ad7-42dd-a702-b19b9fa5c942"
      },
      "source": [
        "# Save the running time for each country\n",
        "for i in range(len(countries)):\n",
        "  save_runtime(results_runtime_incremental[i], path=exp1_runtime_path, country = countries[i], static_learner=False)\n",
        "\n",
        "# Display countrywise running time complexity\n",
        "display_runtime_per_country(results_runtime_incremental, countries)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_____________Running Time for United_States_of_America________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.013    0.018    0.534   0.005\n",
            "1            60   0.026    0.042    1.063   0.002\n",
            "2            90   0.051    0.070    1.730   0.002\n",
            "3           120   0.130    0.159    3.117   0.001\n",
            "4           150   0.172    0.200    4.907   0.002\n",
            "5           180   0.222    0.263    4.332   0.002\n",
            "6           210   0.258    0.321    5.453   0.002\n",
            "7           240   0.308    0.467    5.880   0.001\n",
            "\n",
            "\n",
            "_____________Running Time for India________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.017    0.512   0.002\n",
            "1            60   0.022    0.038    1.057   0.002\n",
            "2            90   0.038    0.073    1.654   0.002\n",
            "3           120   0.081    0.124    2.751   0.002\n",
            "4           150   0.136    0.161    4.106   0.001\n",
            "5           180   0.178    0.226    4.585   0.002\n",
            "6           210   0.236    0.294    4.854   0.001\n",
            "7           240   0.318    0.368    6.250   0.001\n",
            "\n",
            "\n",
            "_____________Running Time for Brazil________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.011    0.016    0.484   0.002\n",
            "1            60   0.021    0.037    1.054   0.002\n",
            "2            90   0.037    0.065    1.677   0.002\n",
            "3           120   0.078    0.117    2.709   0.002\n",
            "4           150   0.126    0.171    4.561   0.002\n",
            "5           180   0.167    0.216    4.402   0.001\n",
            "6           210   0.204    0.265    3.934   0.002\n",
            "7           240   0.251    0.317    5.380   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Russia________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.017    0.491   0.002\n",
            "1            60   0.022    0.039    1.095   0.002\n",
            "2            90   0.040    0.069    1.542   0.002\n",
            "3           120   0.086    0.137    2.242   0.002\n",
            "4           150   0.150    0.187    2.781   0.001\n",
            "5           180   0.205    0.246    3.121   0.002\n",
            "6           210   0.317    0.356    3.739   0.002\n",
            "7           240   0.397    0.448    4.063   0.001\n",
            "\n",
            "\n",
            "_____________Running Time for France________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.012    0.019    0.566   0.008\n",
            "1            60   0.026    0.040    1.108   0.002\n",
            "2            90   0.039    0.067    1.728   0.002\n",
            "3           120   0.093    0.128    3.029   0.002\n",
            "4           150   0.145    0.171    4.107   0.001\n",
            "5           180   0.195    0.241    3.629   0.001\n",
            "6           210   0.187    0.252    5.870   0.001\n",
            "7           240   0.251    0.351    5.051   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Spain________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.011    0.018    0.542   0.002\n",
            "1            60   0.026    0.042    1.045   0.001\n",
            "2            90   0.045    0.070    1.665   0.001\n",
            "3           120   0.117    0.142    2.911   0.002\n",
            "4           150   0.251    0.190    4.648   0.001\n",
            "5           180   0.192    0.225    4.322   0.002\n",
            "6           210   0.264    0.293    4.537   0.002\n",
            "7           240   0.224    0.298    5.250   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for United_Kingdom________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.011    0.018    0.513   0.002\n",
            "1            60   0.026    0.041    1.099   0.002\n",
            "2            90   0.045    0.183    1.701   0.002\n",
            "3           120   0.096    0.135    2.787   0.001\n",
            "4           150   0.153    0.189    5.261   0.002\n",
            "5           180   0.190    0.246    4.104   0.001\n",
            "6           210   0.231    0.279    3.731   0.001\n",
            "7           240   0.301    0.358    5.687   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Italy________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.011    0.018    0.518   0.002\n",
            "1            60   0.026    0.041    1.104   0.002\n",
            "2            90   0.042    0.074    1.715   0.003\n",
            "3           120   0.101    0.140    2.425   0.002\n",
            "4           150   0.132    0.178    4.285   0.002\n",
            "5           180   0.191    0.216    5.381   0.002\n",
            "6           210   0.224    0.293    5.849   0.002\n",
            "7           240   0.399    0.322    6.705   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Mexico________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.011    0.017    0.516   0.002\n",
            "1            60   0.023    0.039    1.088   0.002\n",
            "2            90   0.038    0.062    1.640   0.001\n",
            "3           120   0.097    0.120    2.795   0.002\n",
            "4           150   0.127    0.171    3.996   0.002\n",
            "5           180   0.188    0.211    3.870   0.001\n",
            "6           210   0.202    0.243    4.553   0.002\n",
            "7           240   0.250    0.294    5.409   0.003\n",
            "\n",
            "\n",
            "_____________Running Time for Germany________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.012    0.019    0.536   0.002\n",
            "1            60   0.025    0.040    1.087   0.003\n",
            "2            90   0.046    0.068    1.686   0.002\n",
            "3           120   0.105    0.138    2.697   0.001\n",
            "4           150   0.150    0.178    4.246   0.001\n",
            "5           180   0.201    0.252    3.861   0.002\n",
            "6           210   0.234    0.276    3.662   0.002\n",
            "7           240   0.383    0.325    4.951   0.001\n",
            "\n",
            "\n",
            "_____________Running Time for Iran________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.011    0.018    0.495   0.002\n",
            "1            60   0.026    0.042    1.057   0.002\n",
            "2            90   0.047    0.072    1.572   0.001\n",
            "3           120   0.130    0.147    2.166   0.001\n",
            "4           150   0.164    0.186    2.871   0.001\n",
            "5           180   0.227    0.254    3.633   0.001\n",
            "6           210   0.386    0.318    4.771   0.002\n",
            "7           240   0.325    0.367    6.682   0.001\n",
            "\n",
            "\n",
            "_____________Running Time for Belgium________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.017    0.505   0.002\n",
            "1            60   0.025    0.040    1.103   0.002\n",
            "2            90   0.043    0.064    1.673   0.004\n",
            "3           120   0.088    0.229    2.515   0.002\n",
            "4           150   0.147    0.169    3.550   0.001\n",
            "5           180   0.195    0.230    3.481   0.001\n",
            "6           210   0.233    0.258    3.995   0.002\n",
            "7           240   0.277    0.337    4.788   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Iraq________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.016    0.477   0.002\n",
            "1            60   0.026    0.038    1.020   0.002\n",
            "2            90   0.036    0.061    1.513   0.002\n",
            "3           120   0.076    0.203    3.168   0.002\n",
            "4           150   0.125    0.152    4.723   0.001\n",
            "5           180   0.175    0.220    3.407   0.001\n",
            "6           210   0.207    0.252    4.100   0.001\n",
            "7           240   0.253    0.305    4.564   0.001\n",
            "\n",
            "\n",
            "_____________Running Time for Czechia________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.016    0.465   0.001\n",
            "1            60   0.023    0.039    1.047   0.002\n",
            "2            90   0.043    0.071    1.718   0.002\n",
            "3           120   0.093    0.118    2.813   0.001\n",
            "4           150   0.137    0.171    5.077   0.001\n",
            "5           180   0.204    0.228    3.671   0.001\n",
            "6           210   0.220    0.273    5.715   0.002\n",
            "7           240   0.271    0.314    5.190   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Netherlands________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.011    0.018    0.489   0.002\n",
            "1            60   0.024    0.039    1.048   0.002\n",
            "2            90   0.144    0.066    1.744   0.002\n",
            "3           120   0.093    0.123    3.335   0.002\n",
            "4           150   0.149    0.178    3.589   0.002\n",
            "5           180   0.203    0.237    3.883   0.001\n",
            "6           210   0.222    0.289    5.548   0.001\n",
            "7           240   0.292    0.352    4.625   0.001\n",
            "\n",
            "\n",
            "_____________Running Time for Romania________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.017    0.503   0.002\n",
            "1            60   0.023    0.040    1.066   0.002\n",
            "2            90   0.047    0.074    1.679   0.002\n",
            "3           120   0.089    0.123    2.874   0.001\n",
            "4           150   0.146    0.181    4.611   0.001\n",
            "5           180   0.197    0.242    2.960   0.002\n",
            "6           210   0.241    0.279    4.002   0.001\n",
            "7           240   0.404    0.339    7.370   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Philippines________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.011    0.018    0.529   0.002\n",
            "1            60   0.024    0.040    1.090   0.002\n",
            "2            90   0.045    0.071    1.641   0.003\n",
            "3           120   0.087    0.133    2.388   0.002\n",
            "4           150   0.126    0.155    2.970   0.001\n",
            "5           180   0.177    0.201    3.587   0.001\n",
            "6           210   0.224    0.270    4.416   0.002\n",
            "7           240   0.390    0.328    5.173   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Pakistan________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.016    0.477   0.001\n",
            "1            60   0.029    0.038    1.035   0.002\n",
            "2            90   0.042    0.068    1.669   0.002\n",
            "3           120   0.074    0.109    2.243   0.001\n",
            "4           150   0.122    0.164    2.895   0.002\n",
            "5           180   0.176    0.209    4.428   0.004\n",
            "6           210   0.259    0.318    4.062   0.002\n",
            "7           240   0.332    0.494    5.384   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Canada________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.012    0.019    0.531   0.002\n",
            "1            60   0.025    0.041    1.120   0.002\n",
            "2            90   0.043    0.072    1.700   0.002\n",
            "3           120   0.096    0.132    2.984   0.002\n",
            "4           150   0.149    0.182    4.018   0.001\n",
            "5           180   0.217    0.238    3.835   0.002\n",
            "6           210   0.229    0.267    4.163   0.001\n",
            "7           240   0.272    0.318    6.330   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Israel________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.011    0.017    0.480   0.002\n",
            "1            60   0.023    0.039    1.049   0.002\n",
            "2            90   0.042    0.069    1.530   0.002\n",
            "3           120   0.091    0.121    3.067   0.002\n",
            "4           150   0.150    0.167    4.875   0.002\n",
            "5           180   0.203    0.353    6.047   0.002\n",
            "6           210   0.269    0.299    7.619   0.001\n",
            "7           240   0.282    0.331    8.620   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Switzerland________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.016    0.485   0.001\n",
            "1            60   0.024    0.038    1.069   0.002\n",
            "2            90   0.041    0.070    1.674   0.002\n",
            "3           120   0.089    0.129    2.537   0.001\n",
            "4           150   0.128    0.158    3.835   0.001\n",
            "5           180   0.184    0.233    4.629   0.002\n",
            "6           210   0.206    0.244    4.981   0.002\n",
            "7           240   0.258    0.318    6.754   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Austria________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.012    0.018    0.497   0.002\n",
            "1            60   0.025    0.040    1.130   0.002\n",
            "2            90   0.043    0.071    1.671   0.002\n",
            "3           120   0.103    0.139    3.125   0.002\n",
            "4           150   0.146    0.185    5.199   0.001\n",
            "5           180   0.186    0.231    4.059   0.001\n",
            "6           210   0.222    0.246    5.396   0.002\n",
            "7           240   0.256    0.304    5.080   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Sweden________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.011    0.018    0.519   0.002\n",
            "1            60   0.025    0.039    1.069   0.002\n",
            "2            90   0.039    0.067    1.556   0.001\n",
            "3           120   0.099    0.146    2.057   0.001\n",
            "4           150   0.135    0.175    3.269   0.001\n",
            "5           180   0.186    0.221    4.318   0.001\n",
            "6           210   0.243    0.274    5.681   0.001\n",
            "7           240   0.279    0.319    7.006   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Ecuador________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.017    0.478   0.002\n",
            "1            60   0.021    0.036    1.071   0.002\n",
            "2            90   0.044    0.065    1.627   0.001\n",
            "3           120   0.103    0.130    2.250   0.002\n",
            "4           150   0.246    0.169    3.253   0.001\n",
            "5           180   0.203    0.238    4.889   0.001\n",
            "6           210   0.244    0.279    6.171   0.002\n",
            "7           240   0.306    0.453    6.374   0.001\n",
            "\n",
            "\n",
            "_____________Running Time for United_Arab_Emirates________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.011    0.018    0.520   0.002\n",
            "1            60   0.024    0.040    1.055   0.001\n",
            "2            90   0.050    0.079    1.743   0.002\n",
            "3           120   0.105    0.138    2.585   0.001\n",
            "4           150   0.141    0.171    4.020   0.002\n",
            "5           180   0.207    0.243    3.828   0.002\n",
            "6           210   0.352    0.304    4.940   0.002\n",
            "7           240   0.295    0.317    5.376   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Japan________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.013    0.020    0.564   0.002\n",
            "1            60   0.031    0.043    1.113   0.002\n",
            "2            90   0.045    0.068    1.686   0.002\n",
            "3           120   0.102    0.133    2.958   0.001\n",
            "4           150   0.164    0.294    5.286   0.001\n",
            "5           180   0.230    0.265    3.880   0.001\n",
            "6           210   0.268    0.312    3.992   0.001\n",
            "7           240   0.292    0.343    5.490   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Kuwait________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.012    0.017    0.525   0.002\n",
            "1            60   0.021    0.037    1.041   0.001\n",
            "2            90   0.037    0.060    1.677   0.002\n",
            "3           120   0.087    0.112    2.189   0.001\n",
            "4           150   0.135    0.172    3.142   0.001\n",
            "5           180   0.188    0.211    3.406   0.002\n",
            "6           210   0.257    0.287    3.756   0.001\n",
            "7           240   0.276    0.443    4.443   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Qatar________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.017    0.493   0.003\n",
            "1            60   0.024    0.038    1.030   0.002\n",
            "2            90   0.041    0.070    1.519   0.001\n",
            "3           120   0.089    0.116    3.427   0.002\n",
            "4           150   0.150    0.156    4.684   0.002\n",
            "5           180   0.207    0.236    3.872   0.002\n",
            "6           210   0.275    0.332    3.810   0.002\n",
            "7           240   0.430    0.384    4.961   0.001\n",
            "\n",
            "\n",
            "_____________Running Time for Georgia________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.017    0.501   0.002\n",
            "1            60   0.020    0.036    1.046   0.002\n",
            "2            90   0.034    0.058    1.530   0.002\n",
            "3           120   0.076    0.095    2.035   0.001\n",
            "4           150   0.091    0.128    2.751   0.002\n",
            "5           180   0.108    0.148    3.235   0.001\n",
            "6           210   0.122    0.184    5.415   0.002\n",
            "7           240   0.135    0.204    4.869   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Lebanon________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.011    0.018    0.526   0.002\n",
            "1            60   0.022    0.038    1.060   0.002\n",
            "2            90   0.038    0.061    1.650   0.002\n",
            "3           120   0.078    0.102    2.358   0.002\n",
            "4           150   0.090    0.134    3.082   0.001\n",
            "5           180   0.122    0.160    3.328   0.003\n",
            "6           210   0.149    0.209    4.015   0.002\n",
            "7           240   0.199    0.248    5.051   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Croatia________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.022    0.503   0.002\n",
            "1            60   0.023    0.039    1.045   0.002\n",
            "2            90   0.038    0.063    1.664   0.002\n",
            "3           120   0.082    0.119    2.908   0.001\n",
            "4           150   0.116    0.148    4.491   0.001\n",
            "5           180   0.149    0.193    4.971   0.001\n",
            "6           210   0.285    0.214    5.552   0.002\n",
            "7           240   0.230    0.270    7.359   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Oman________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.017    0.497   0.002\n",
            "1            60   0.029    0.036    1.052   0.002\n",
            "2            90   0.039    0.063    1.679   0.002\n",
            "3           120   0.071    0.104    2.177   0.003\n",
            "4           150   0.115    0.149    2.764   0.001\n",
            "5           180   0.152    0.174    3.715   0.002\n",
            "6           210   0.195    0.240    4.393   0.001\n",
            "7           240   0.224    0.290    4.309   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Egypt________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.018    0.528   0.002\n",
            "1            60   0.024    0.040    1.102   0.002\n",
            "2            90   0.036    0.063    1.567   0.001\n",
            "3           120   0.091    0.120    2.243   0.002\n",
            "4           150   0.116    0.175    3.233   0.002\n",
            "5           180   0.168    0.212    3.524   0.001\n",
            "6           210   0.235    0.279    3.805   0.002\n",
            "7           240   0.356    0.395    5.655   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Greece________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.016    0.486   0.002\n",
            "1            60   0.032    0.039    1.029   0.002\n",
            "2            90   0.038    0.064    1.627   0.002\n",
            "3           120   0.075    0.128    2.152   0.002\n",
            "4           150   0.112    0.155    4.414   0.001\n",
            "5           180   0.143    0.184    5.877   0.002\n",
            "6           210   0.185    0.237    5.994   0.002\n",
            "7           240   0.232    0.386    5.921   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for China________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.018    0.026    0.564   0.002\n",
            "1            60   0.041    0.066    1.114   0.002\n",
            "2            90   0.071    0.096    1.692   0.002\n",
            "3           120   0.187    0.218    2.095   0.002\n",
            "4           150   0.211    0.243    2.906   0.002\n",
            "5           180   0.213    0.264    4.834   0.001\n",
            "6           210   0.347    0.290    5.429   0.002\n",
            "7           240   0.282    0.332    5.989   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Bahrain________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.017    0.496   0.003\n",
            "1            60   0.023    0.040    1.089   0.003\n",
            "2            90   0.041    0.063    1.640   0.002\n",
            "3           120   0.067    0.111    2.216   0.002\n",
            "4           150   0.113    0.153    3.035   0.001\n",
            "5           180   0.152    0.193    3.981   0.001\n",
            "6           210   0.174    0.228    5.294   0.001\n",
            "7           240   0.239    0.266    4.503   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Algeria________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.017    0.473   0.002\n",
            "1            60   0.023    0.038    1.041   0.002\n",
            "2            90   0.039    0.064    1.662   0.002\n",
            "3           120   0.102    0.125    2.303   0.002\n",
            "4           150   0.146    0.178    3.464   0.001\n",
            "5           180   0.202    0.225    4.707   0.002\n",
            "6           210   0.247    0.372    5.860   0.002\n",
            "7           240   0.317    0.397    5.863   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Denmark________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.017    0.511   0.002\n",
            "1            60   0.034    0.039    1.062   0.002\n",
            "2            90   0.043    0.064    1.676   0.002\n",
            "3           120   0.084    0.118    2.496   0.001\n",
            "4           150   0.116    0.157    4.500   0.001\n",
            "5           180   0.163    0.193    4.488   0.002\n",
            "6           210   0.184    0.333    3.986   0.001\n",
            "7           240   0.232    0.276    6.313   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Ireland________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.017    0.496   0.002\n",
            "1            60   0.024    0.040    1.048   0.002\n",
            "2            90   0.051    0.068    1.670   0.002\n",
            "3           120   0.098    0.128    3.107   0.002\n",
            "4           150   0.135    0.181    5.420   0.002\n",
            "5           180   0.179    0.200    4.260   0.002\n",
            "6           210   0.206    0.335    3.999   0.002\n",
            "7           240   0.247    0.301    5.253   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Malaysia________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.012    0.019    0.557   0.004\n",
            "1            60   0.026    0.046    1.084   0.002\n",
            "2            90   0.043    0.071    1.677   0.002\n",
            "3           120   0.103    0.140    2.266   0.001\n",
            "4           150   0.136    0.173    4.716   0.001\n",
            "5           180   0.192    0.233    4.986   0.002\n",
            "6           210   0.302    0.236    4.244   0.001\n",
            "7           240   0.231    0.281    6.747   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Singapore________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.012    0.019    0.536   0.001\n",
            "1            60   0.025    0.040    1.096   0.002\n",
            "2            90   0.041    0.070    1.709   0.001\n",
            "3           120   0.115    0.136    2.794   0.001\n",
            "4           150   0.167    0.186    4.386   0.001\n",
            "5           180   0.225    0.254    4.078   0.002\n",
            "6           210   0.259    0.291    3.815   0.001\n",
            "7           240   0.309    0.346    5.491   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Norway________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.017    0.494   0.002\n",
            "1            60   0.030    0.044    1.035   0.002\n",
            "2            90   0.050    0.070    1.666   0.002\n",
            "3           120   0.089    0.120    2.471   0.002\n",
            "4           150   0.118    0.159    4.086   0.001\n",
            "5           180   0.151    0.192    5.002   0.002\n",
            "6           210   0.169    0.221    5.743   0.001\n",
            "7           240   0.201    0.258    6.619   0.001\n",
            "\n",
            "\n",
            "_____________Running Time for South_Korea________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.013    0.020    0.505   0.002\n",
            "1            60   0.030    0.048    1.124   0.002\n",
            "2            90   0.045    0.072    1.629   0.001\n",
            "3           120   0.123    0.164    2.259   0.001\n",
            "4           150   0.157    0.281    4.149   0.001\n",
            "5           180   0.200    0.238    5.311   0.002\n",
            "6           210   0.210    0.240    6.345   0.001\n",
            "7           240   0.235    0.285    7.102   0.001\n",
            "\n",
            "\n",
            "_____________Running Time for Australia________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.012    0.019    0.526   0.002\n",
            "1            60   0.026    0.042    1.233   0.002\n",
            "2            90   0.042    0.070    1.588   0.002\n",
            "3           120   0.103    0.128    3.409   0.002\n",
            "4           150   0.131    0.157    4.062   0.001\n",
            "5           180   0.187    0.199    4.149   0.002\n",
            "6           210   0.200    0.247    4.874   0.001\n",
            "7           240   0.219    0.381    6.435   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Finland________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.011    0.018    0.518   0.002\n",
            "1            60   0.025    0.041    1.061   0.001\n",
            "2            90   0.056    0.080    1.674   0.002\n",
            "3           120   0.088    0.121    3.344   0.002\n",
            "4           150   0.130    0.159    4.712   0.001\n",
            "5           180   0.173    0.208    5.931   0.002\n",
            "6           210   0.179    0.213    5.861   0.001\n",
            "7           240   0.312    0.254    6.876   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Estonia________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.016    0.441   0.002\n",
            "1            60   0.024    0.039    1.061   0.002\n",
            "2            90   0.046    0.069    1.651   0.002\n",
            "3           120   0.084    0.120    2.789   0.002\n",
            "4           150   0.102    0.139    4.600   0.001\n",
            "5           180   0.128    0.179    6.261   0.002\n",
            "6           210   0.128    0.188    7.156   0.001\n",
            "7           240   0.145    0.213    7.998   0.001\n",
            "\n",
            "\n",
            "_____________Running Time for Iceland________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.017    0.490   0.001\n",
            "1            60   0.030    0.037    1.028   0.002\n",
            "2            90   0.043    0.067    1.692   0.002\n",
            "3           120   0.068    0.115    2.173   0.001\n",
            "4           150   0.112    0.149    4.614   0.002\n",
            "5           180   0.131    0.181    5.525   0.001\n",
            "6           210   0.137    0.186    5.961   0.001\n",
            "7           240   0.139    0.210    6.888   0.001\n",
            "\n",
            "\n",
            "_____________Running Time for San_Marino________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.017    0.481   0.002\n",
            "1            60   0.023    0.039    1.050   0.002\n",
            "2            90   0.037    0.065    1.702   0.002\n",
            "3           120   0.064    0.098    2.215   0.002\n",
            "4           150   0.080    0.120    2.719   0.002\n",
            "5           180   0.100    0.142    3.306   0.002\n",
            "6           210   0.103    0.157    3.794   0.001\n",
            "7           240   0.106    0.178    4.247   0.001\n",
            "\n",
            "\n",
            "_____________Running Time for Vietnam________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.011    0.019    0.509   0.002\n",
            "1            60   0.022    0.037    1.067   0.002\n",
            "2            90   0.035    0.062    1.627   0.002\n",
            "3           120   0.063    0.089    2.148   0.002\n",
            "4           150   0.077    0.113    2.929   0.002\n",
            "5           180   0.081    0.134    4.661   0.002\n",
            "6           210   0.099    0.158    4.310   0.002\n",
            "7           240   0.122    0.194    5.074   0.001\n",
            "\n",
            "\n",
            "_____________Running Time for Taiwan________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.011    0.018    0.535   0.003\n",
            "1            60   0.024    0.041    1.080   0.002\n",
            "2            90   0.035    0.065    1.598   0.002\n",
            "3           120   0.059    0.097    2.198   0.002\n",
            "4           150   0.077    0.114    3.581   0.001\n",
            "5           180   0.086    0.144    4.752   0.001\n",
            "6           210   0.106    0.156    6.450   0.001\n",
            "7           240   0.111    0.179    7.267   0.001\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMH2CG_nuloA",
        "outputId": "a18c7146-3a2e-42e9-e5be-19a0a9c3015f"
      },
      "source": [
        "countrywise_error_score_incremental = calc_save_err_metric_countrywise(countries, error_metrics, results_incremental, max_of_pretrain_per_country, max_cases_per_country, path=exp1_path, static_learner=False, transpose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________United_States_of_America____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays      HT_Reg     HAT_Reg     ARF_Reg      PA_Reg\n",
            "0                      MAE        30.000  390176.928  646039.937 1517913.706 1819162.372\n",
            "1                      MAE        60.000 1441573.023 1415719.650 1464594.064 5818965.043\n",
            "2                      MAE        90.000   20005.990   20005.990   43009.400  432785.063\n",
            "3                      MAE       120.000   11559.992   11559.992   11951.760 1818992.710\n",
            "4                      MAE       150.000   20166.300   20166.300   27738.292 7566808.707\n",
            "5                      MAE       180.000    4860.379    4860.379   10321.769  992635.876\n",
            "6                      MAE       210.000   56106.643   56106.643   51564.129 1617244.383\n",
            "7                      MAE       240.000  105473.050  105473.050   88631.021 1010290.376\n",
            "mean                   NaN       135.000  256240.288  284991.493  401965.518 2634610.566\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_States_of_America____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  17.379   28.631   66.635  79.974\n",
            "1                     MAPE        60.000  57.854   56.899   58.844 249.268\n",
            "2                     MAPE        90.000   0.341    0.341    0.798   8.037\n",
            "3                     MAPE       120.000   0.194    0.194    0.201  35.335\n",
            "4                     MAPE       150.000   0.519    0.519    0.709 194.068\n",
            "5                     MAPE       180.000   0.103    0.103    0.204  20.012\n",
            "6                     MAPE       210.000   0.448    0.448    0.405  17.645\n",
            "7                     MAPE       240.000   0.620    0.620    0.521   5.898\n",
            "mean                   NaN       135.000   9.682   10.970   16.040  76.280\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_States_of_America____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays      HT_Reg     HAT_Reg     ARF_Reg      PA_Reg\n",
            "0                     RMSE        30.000  717828.590 1096020.216 2253839.539 2682115.944\n",
            "1                     RMSE        60.000 2112224.854 2061883.811 2138102.504 6646783.857\n",
            "2                     RMSE        90.000   23125.696   23125.696   44307.226  528386.433\n",
            "3                     RMSE       120.000   13663.692   13663.692   14017.081 2307348.646\n",
            "4                     RMSE       150.000   22518.172   22518.172   28555.150 8524420.726\n",
            "5                     RMSE       180.000    5778.620    5778.620   12125.288 1228151.989\n",
            "6                     RMSE       210.000   68235.262   68235.262   63944.791 2017617.677\n",
            "7                     RMSE       240.000  105517.554  105517.554   88921.063 1435471.576\n",
            "mean                   NaN       135.000  383611.555  424592.878  580476.580 3171287.106\n",
            "\n",
            "\n",
            "\n",
            "_________________________________India____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays     HT_Reg    HAT_Reg    ARF_Reg      PA_Reg\n",
            "0                      MAE        30.000   2718.178   2184.079   8784.880   36026.852\n",
            "1                      MAE        60.000  31040.354  32653.236  57708.669   29684.748\n",
            "2                      MAE        90.000   6103.608   4437.247  69252.059   19643.547\n",
            "3                      MAE       120.000  14363.950  14411.126  19585.510  221941.660\n",
            "4                      MAE       150.000   6726.792  22459.162  24194.483  410557.501\n",
            "5                      MAE       180.000 248118.100 250558.017 106670.812 1319724.742\n",
            "6                      MAE       210.000 188707.075 188712.256 122243.188  801787.488\n",
            "7                      MAE       240.000  21966.720  21983.558   1027.171  140150.862\n",
            "mean                   NaN       135.000  64968.097  67174.835  51183.346  372439.675\n",
            "\n",
            "\n",
            "\n",
            "_________________________________India____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   0.661    0.513    1.844   8.123\n",
            "1                     MAPE        60.000   2.445    2.599    4.750   2.608\n",
            "2                     MAPE        90.000   0.226    0.162    2.552   0.719\n",
            "3                     MAPE       120.000   0.232    0.233    0.321   3.773\n",
            "4                     MAPE       150.000   0.086    0.256    0.282   4.680\n",
            "5                     MAPE       180.000   3.505    3.541    1.559  19.889\n",
            "6                     MAPE       210.000   4.188    4.188    2.726  18.334\n",
            "7                     MAPE       240.000   0.516    0.516    0.024   3.252\n",
            "mean                   NaN       135.000   1.482    1.501    1.757   7.672\n",
            "\n",
            "\n",
            "\n",
            "_________________________________India____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays     HT_Reg    HAT_Reg    ARF_Reg      PA_Reg\n",
            "0                     RMSE        30.000   4418.745   3623.689  13764.339   47997.804\n",
            "1                     RMSE        60.000  50480.306  51396.264  82018.528   41134.832\n",
            "2                     RMSE        90.000   6650.587   5004.170  74188.303   25765.992\n",
            "3                     RMSE       120.000  19389.144  19365.982  24636.149  243010.217\n",
            "4                     RMSE       150.000   8028.172  27253.142  27704.557  535538.088\n",
            "5                     RMSE       180.000 249535.687 252096.233 113447.346 1603274.278\n",
            "6                     RMSE       210.000 188898.194 188903.442 122922.413  925599.865\n",
            "7                     RMSE       240.000  22254.186  22270.814   1454.668  160788.750\n",
            "mean                   NaN       135.000  68706.878  71239.217  57517.038  447888.728\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Brazil____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays     HT_Reg    HAT_Reg   ARF_Reg     PA_Reg\n",
            "0                      MAE        30.000  59948.492  85878.802 42766.683  19764.431\n",
            "1                      MAE        60.000 141062.302  97880.844 67763.146  63852.384\n",
            "2                      MAE        90.000  16075.510  16035.467 82592.864 225313.012\n",
            "3                      MAE       120.000 132992.886 132993.243 75038.491 352417.677\n",
            "4                      MAE       150.000  32895.656  32895.691 47855.158 690891.821\n",
            "5                      MAE       180.000   3785.643   3785.625  5586.021 585389.598\n",
            "6                      MAE       210.000   3292.051   3292.050  2854.850 526153.741\n",
            "7                      MAE       240.000   2949.282   2949.284  2278.813 378345.038\n",
            "mean                   NaN       135.000  49125.228  46963.876 40842.004 355265.963\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Brazil____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   5.050    6.978    3.395   1.876\n",
            "1                     MAPE        60.000   4.930    3.461    2.419   2.433\n",
            "2                     MAPE        90.000   0.433    0.432    2.240   6.058\n",
            "3                     MAPE       120.000   3.121    3.121    1.780   8.418\n",
            "4                     MAPE       150.000   1.021    1.021    1.458  19.766\n",
            "5                     MAPE       180.000   0.163    0.163    0.242  24.702\n",
            "6                     MAPE       210.000   0.135    0.135    0.131  23.499\n",
            "7                     MAPE       240.000   0.091    0.091    0.070  11.582\n",
            "mean                   NaN       135.000   1.868    1.925    1.467  12.292\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Brazil____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays     HT_Reg    HAT_Reg    ARF_Reg     PA_Reg\n",
            "0                     RMSE        30.000  83940.426 124645.921  68095.179  26928.168\n",
            "1                     RMSE        60.000 229490.809 158927.657 101269.261  92941.595\n",
            "2                     RMSE        90.000  18010.888  17989.722  97138.539 282261.207\n",
            "3                     RMSE       120.000 135463.343 135463.748  80798.643 457331.020\n",
            "4                     RMSE       150.000  34227.988  34227.996  53356.929 930866.369\n",
            "5                     RMSE       180.000   4829.739   4829.718   6309.947 747161.566\n",
            "6                     RMSE       210.000   4323.200   4323.200   3498.578 678523.804\n",
            "7                     RMSE       240.000   3266.364   3266.366   2787.329 494368.723\n",
            "mean                   NaN       135.000  64194.095  60459.291  51656.800 463797.807\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Russia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays     HT_Reg    HAT_Reg    ARF_Reg      PA_Reg\n",
            "0                      MAE        30.000 158169.643 146384.633  35239.780  196166.536\n",
            "1                      MAE        60.000 129227.722 151452.976 342490.511  883443.165\n",
            "2                      MAE        90.000  83519.413  83519.413  79411.529 1400600.915\n",
            "3                      MAE       120.000   2196.961   2196.961    759.365  102094.475\n",
            "4                      MAE       150.000    144.777    144.777    124.164   57119.191\n",
            "5                      MAE       180.000   4431.450   4431.450   4212.389   34966.571\n",
            "6                      MAE       210.000   9365.504   9365.504  10351.515   80012.968\n",
            "7                      MAE       240.000  11179.751  11179.751  10880.776   73038.575\n",
            "mean                   NaN       135.000  49779.403  51084.433  60433.754  353430.300\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Russia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  16.579   15.348    3.647  20.509\n",
            "1                     MAPE        60.000  15.939   18.669   41.518 105.468\n",
            "2                     MAPE        90.000  12.894   12.894   12.257 216.406\n",
            "3                     MAPE       120.000   0.415    0.415    0.148  19.613\n",
            "4                     MAPE       150.000   0.028    0.028    0.024  11.585\n",
            "5                     MAPE       180.000   0.375    0.375    0.352   3.794\n",
            "6                     MAPE       210.000   0.484    0.484    0.531   3.709\n",
            "7                     MAPE       240.000   0.462    0.462    0.450   2.970\n",
            "mean                   NaN       135.000   5.897    6.085    7.366  48.007\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Russia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays     HT_Reg    HAT_Reg    ARF_Reg      PA_Reg\n",
            "0                     RMSE        30.000 255851.798 237805.009  45970.689  301024.766\n",
            "1                     RMSE        60.000 228843.504 266527.168 495819.798 1080081.736\n",
            "2                     RMSE        90.000  93169.738  93169.738  88403.429 1576512.428\n",
            "3                     RMSE       120.000   2209.424   2209.424    895.750  116001.716\n",
            "4                     RMSE       150.000    171.533    171.533    136.791   75001.304\n",
            "5                     RMSE       180.000   5264.007   5264.007   5085.515   39017.680\n",
            "6                     RMSE       210.000   9555.229   9555.229  10709.826  139459.615\n",
            "7                     RMSE       240.000  11182.896  11182.896  10883.488   88101.631\n",
            "mean                   NaN       135.000  75781.016  78235.625  82238.161  426900.109\n",
            "\n",
            "\n",
            "\n",
            "_________________________________France____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg     PA_Reg\n",
            "0                      MAE        30.000  7085.740 25122.677  9795.778 127673.029\n",
            "1                      MAE        60.000 50824.770 33805.771 77290.010 433158.445\n",
            "2                      MAE        90.000   768.505   773.975  1445.484  46638.073\n",
            "3                      MAE       120.000   620.586   622.776   679.357  31022.742\n",
            "4                      MAE       150.000  5057.448  5057.722  5079.094  40366.293\n",
            "5                      MAE       180.000 12389.657 12389.694 12145.618 116914.667\n",
            "6                      MAE       210.000 18631.078 18631.079 13595.619 250826.102\n",
            "7                      MAE       240.000 11375.835 11375.835 34094.754 282448.947\n",
            "mean                   NaN       135.000 13344.202 13472.441 19265.714 166131.037\n",
            "\n",
            "\n",
            "\n",
            "_________________________________France____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  16.023   59.979   23.258 213.220\n",
            "1                     MAPE        60.000 114.304   75.585  172.530 897.989\n",
            "2                     MAPE        90.000   1.334    1.344    2.611  84.952\n",
            "3                     MAPE       120.000   0.346    0.347    0.309  21.126\n",
            "4                     MAPE       150.000   0.711    0.711    0.714   5.615\n",
            "5                     MAPE       180.000   0.771    0.771    0.760   7.621\n",
            "6                     MAPE       210.000   0.463    0.463    0.357   7.417\n",
            "7                     MAPE       240.000   0.678    0.678    2.041  17.102\n",
            "mean                   NaN       135.000  16.829   17.485   25.323 156.880\n",
            "\n",
            "\n",
            "\n",
            "_________________________________France____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg    ARF_Reg     PA_Reg\n",
            "0                     RMSE        30.000 11700.961 41133.416  18968.221 181383.898\n",
            "1                     RMSE        60.000 69280.555 44340.098 100023.732 468451.241\n",
            "2                     RMSE        90.000   846.175   850.809   1492.205  69085.039\n",
            "3                     RMSE       120.000   808.434   810.480    928.971  38265.476\n",
            "4                     RMSE       150.000  5474.280  5474.680   5492.799  51868.049\n",
            "5                     RMSE       180.000 13385.800 13385.835  13057.278 140061.843\n",
            "6                     RMSE       210.000 20658.608 20658.608  14557.434 334302.014\n",
            "7                     RMSE       240.000 12073.465 12073.465  34488.384 315257.715\n",
            "mean                   NaN       135.000 16778.535 17340.924  23626.128 199834.409\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Spain____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays     HT_Reg    HAT_Reg    ARF_Reg     PA_Reg\n",
            "0                      MAE        30.000 161891.244 708189.766 757049.829 436505.926\n",
            "1                      MAE        60.000  96103.525 336289.809 248519.975  69732.887\n",
            "2                      MAE        90.000   1534.009   1703.017   1115.621  35769.526\n",
            "3                      MAE       120.000   1544.089   1537.582   1787.134  13663.925\n",
            "4                      MAE       150.000   5928.471   5928.092   5538.790 134236.348\n",
            "5                      MAE       180.000   5966.082   5966.117   4800.507 146078.124\n",
            "6                      MAE       210.000   5809.525   5809.525   4256.561 206279.153\n",
            "7                      MAE       240.000   4473.968   4473.968  11087.390 373217.228\n",
            "mean                   NaN       135.000  35406.364 133737.235 129269.476 176935.390\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Spain____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000 239.252 1152.425 1165.516 530.556\n",
            "1                     MAPE        60.000 284.858 1000.956  740.527 186.768\n",
            "2                     MAPE        90.000   2.824    3.114    2.024  85.319\n",
            "3                     MAPE       120.000   0.341    0.339    0.412   3.395\n",
            "4                     MAPE       150.000   0.637    0.637    0.591  14.359\n",
            "5                     MAPE       180.000   0.532    0.532    0.424  14.291\n",
            "6                     MAPE       210.000   0.303    0.303    0.226  11.474\n",
            "7                     MAPE       240.000   0.404    0.404    0.932  33.963\n",
            "mean                   NaN       135.000  66.144  269.839  238.831 110.016\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Spain____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays     HT_Reg     HAT_Reg     ARF_Reg     PA_Reg\n",
            "0                     RMSE        30.000 228934.470 1202836.345 1112894.256 509527.040\n",
            "1                     RMSE        60.000 117351.371  428591.319  321881.520  87887.955\n",
            "2                     RMSE        90.000   1594.419    1771.068    1192.203  51708.083\n",
            "3                     RMSE       120.000   2000.677    1993.300    2198.967  18279.577\n",
            "4                     RMSE       150.000   6218.261    6218.163    5880.481 179172.263\n",
            "5                     RMSE       180.000   6208.585    6208.614    5346.704 190414.270\n",
            "6                     RMSE       210.000   6522.874    6522.875    4949.059 268382.460\n",
            "7                     RMSE       240.000   5045.453    5045.453   13355.305 460284.986\n",
            "mean                   NaN       135.000  46734.514  207398.392  183462.312 220707.079\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_Kingdom____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg     PA_Reg\n",
            "0                      MAE        30.000 15053.956 62858.904  3436.848  37442.053\n",
            "1                      MAE        60.000 11720.034 32667.018  2475.266 235144.773\n",
            "2                      MAE        90.000   652.154   648.598  6634.644  43558.917\n",
            "3                      MAE       120.000  1283.656  1286.474   911.065  24640.109\n",
            "4                      MAE       150.000   771.250   771.096   939.915  17882.953\n",
            "5                      MAE       180.000  8914.228  8914.216  8604.907  53547.501\n",
            "6                      MAE       210.000 17616.695 17616.695 17147.350 380994.285\n",
            "7                      MAE       240.000  9118.037  9118.037 15462.248 234048.997\n",
            "mean                   NaN       135.000  8141.251 16735.130  6951.530 128407.449\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_Kingdom____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   5.788   24.647    1.278  12.409\n",
            "1                     MAPE        60.000  11.852   33.423    2.290 214.955\n",
            "2                     MAPE        90.000   1.051    1.047   10.334  69.295\n",
            "3                     MAPE       120.000   1.533    1.537    1.047  33.240\n",
            "4                     MAPE       150.000   0.402    0.402    0.342   8.195\n",
            "5                     MAPE       180.000   0.704    0.704    0.665   4.458\n",
            "6                     MAPE       210.000   0.776    0.776    0.756  16.192\n",
            "7                     MAPE       240.000   0.509    0.509    0.854  13.346\n",
            "mean                   NaN       135.000   2.827    7.881    2.196  46.511\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_Kingdom____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg    HAT_Reg   ARF_Reg     PA_Reg\n",
            "0                     RMSE        30.000 23927.803 102293.114  4776.197  48656.568\n",
            "1                     RMSE        60.000 17199.860  50184.291  3272.520 263994.748\n",
            "2                     RMSE        90.000   772.948    762.855  6642.037  57477.989\n",
            "3                     RMSE       120.000  1319.885   1322.534   984.978  30896.894\n",
            "4                     RMSE       150.000   868.609    868.511  1242.285  25109.018\n",
            "5                     RMSE       180.000 10208.972  10208.953  9999.383  62525.495\n",
            "6                     RMSE       210.000 17640.269  17640.269 17166.305 524543.205\n",
            "7                     RMSE       240.000  9714.309   9714.309 15821.218 287256.219\n",
            "mean                   NaN       135.000 10206.582  24124.354  7488.115 162557.517\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Italy____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg     PA_Reg\n",
            "0                      MAE        30.000 23932.885 87764.676 38896.672 136396.284\n",
            "1                      MAE        60.000   881.958  5352.237 19997.701  44186.999\n",
            "2                      MAE        90.000  1514.084  1509.136   976.645   6436.408\n",
            "3                      MAE       120.000  1259.368  1258.193   227.829   9619.354\n",
            "4                      MAE       150.000   469.736   469.523   493.128   5694.896\n",
            "5                      MAE       180.000  2738.561  2738.490  2763.701  16773.578\n",
            "6                      MAE       210.000 25264.858 25264.856 25361.645  28468.958\n",
            "7                      MAE       240.000 14545.131 14545.131 15135.859  66873.554\n",
            "mean                   NaN       135.000  8825.822 17362.780 12981.647  39306.254\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Italy____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  30.412  108.722   43.867 116.693\n",
            "1                     MAPE        60.000   2.806   19.149   70.908 135.528\n",
            "2                     MAPE        90.000   7.392    7.366    4.792  31.487\n",
            "3                     MAPE       120.000   3.640    3.635    0.750  30.984\n",
            "4                     MAPE       150.000   0.345    0.345    0.363   4.221\n",
            "5                     MAPE       180.000   0.482    0.482    0.463   5.924\n",
            "6                     MAPE       210.000   0.894    0.894    0.895   0.962\n",
            "7                     MAPE       240.000   0.478    0.478    0.498   2.312\n",
            "mean                   NaN       135.000   5.806   17.634   15.317  41.014\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Italy____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg    HAT_Reg   ARF_Reg     PA_Reg\n",
            "0                     RMSE        30.000 40416.020 123708.596 47461.351 159134.851\n",
            "1                     RMSE        60.000  1049.234   6087.734 22199.722  50973.839\n",
            "2                     RMSE        90.000  1616.958   1608.190  1113.344   7663.244\n",
            "3                     RMSE       120.000  1264.838   1263.731   246.917  12938.322\n",
            "4                     RMSE       150.000   496.075    495.847   519.601   7672.579\n",
            "5                     RMSE       180.000  4008.767   4008.719  4127.866  22005.502\n",
            "6                     RMSE       210.000 26329.806  26329.803 26483.582  47136.192\n",
            "7                     RMSE       240.000 15025.608  15025.608 15586.302  93535.256\n",
            "mean                   NaN       135.000 11275.913  22316.028 14717.336  50132.473\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Mexico____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg  HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000 10579.634 8978.131 10328.803 15765.228\n",
            "1                      MAE        60.000  6456.005 1085.347 37551.137  7879.143\n",
            "2                      MAE        90.000  1108.612 5154.113 19690.620 20370.203\n",
            "3                      MAE       120.000  2033.668 1179.161 15677.441 67272.797\n",
            "4                      MAE       150.000   547.249  456.725  9884.846 28484.257\n",
            "5                      MAE       180.000   619.393  607.184   506.534 89046.419\n",
            "6                      MAE       210.000  1611.008 1611.406  1240.158 38280.245\n",
            "7                      MAE       240.000  2069.965 2071.859  2019.256 69486.183\n",
            "mean                   NaN       135.000  3128.192 2642.991 12112.349 42073.059\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Mexico____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   3.944    3.361    3.888   6.063\n",
            "1                     MAPE        60.000   1.325    0.230    7.745   1.674\n",
            "2                     MAPE        90.000   0.169    0.789    3.050   3.148\n",
            "3                     MAPE       120.000   0.358    0.209    2.698  12.059\n",
            "4                     MAPE       150.000   0.119    0.099    2.035   5.974\n",
            "5                     MAPE       180.000   0.154    0.153    0.145  22.983\n",
            "6                     MAPE       210.000   0.300    0.300    0.229   7.259\n",
            "7                     MAPE       240.000   0.276    0.276    0.271   9.510\n",
            "mean                   NaN       135.000   0.831    0.677    2.508   8.584\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Mexico____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000 14267.258 11971.477 13561.867 20027.461\n",
            "1                     RMSE        60.000  8206.554  1544.418 45369.521 10195.795\n",
            "2                     RMSE        90.000  1366.903  5694.252 20645.412 24040.668\n",
            "3                     RMSE       120.000  2299.743  1426.432 15768.224 78429.868\n",
            "4                     RMSE       150.000   692.689   569.856 10450.508 37405.920\n",
            "5                     RMSE       180.000   671.062   652.907   685.308 97596.542\n",
            "6                     RMSE       210.000  1680.069  1680.553  1333.874 48283.755\n",
            "7                     RMSE       240.000  2450.415  2453.355  2343.949 80082.292\n",
            "mean                   NaN       135.000  3954.337  3249.156 13769.833 49507.788\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Germany____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays     HT_Reg    HAT_Reg    ARF_Reg     PA_Reg\n",
            "0                      MAE        30.000 121277.158 109350.486  77811.781 211547.762\n",
            "1                      MAE        60.000  62728.175  64455.898 134963.909 320168.320\n",
            "2                      MAE        90.000   1298.528   1289.961   1318.093  13129.473\n",
            "3                      MAE       120.000    710.821    707.991    178.629  21244.273\n",
            "4                      MAE       150.000    193.601    193.148    156.110  46497.447\n",
            "5                      MAE       180.000   2183.802   2183.996   1914.033  35464.654\n",
            "6                      MAE       210.000  13772.964  13773.200  13418.658  40783.134\n",
            "7                      MAE       240.000  11076.495  11076.495  10018.928  31923.728\n",
            "mean                   NaN       135.000  26655.193  25378.897  29972.518  90094.849\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Germany____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000 205.243  184.161  133.155 244.513\n",
            "1                     MAPE        60.000 153.342  157.014  328.815 813.846\n",
            "2                     MAPE        90.000   3.231    3.211    3.252  30.787\n",
            "3                     MAPE       120.000   0.923    0.920    0.179  28.202\n",
            "4                     MAPE       150.000   0.131    0.130    0.107  33.952\n",
            "5                     MAPE       180.000   0.523    0.523    0.421   9.891\n",
            "6                     MAPE       210.000   0.858    0.858    0.836   2.452\n",
            "7                     MAPE       240.000   0.611    0.611    0.552   1.732\n",
            "mean                   NaN       135.000  45.608   43.428   58.415 145.672\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Germany____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays     HT_Reg    HAT_Reg    ARF_Reg     PA_Reg\n",
            "0                     RMSE        30.000 228507.465 202220.795 154743.267 244503.545\n",
            "1                     RMSE        60.000  80819.435  84137.067 172823.885 339232.066\n",
            "2                     RMSE        90.000   1415.456   1406.596   1374.618  17362.220\n",
            "3                     RMSE       120.000    721.752    718.927    240.775  25205.402\n",
            "4                     RMSE       150.000    254.554    254.143    209.437  52080.444\n",
            "5                     RMSE       180.000   2901.584   2901.773   2718.009  46423.682\n",
            "6                     RMSE       210.000  14197.837  14198.072  13825.956  53733.430\n",
            "7                     RMSE       240.000  11111.719  11111.719  10071.157  39043.742\n",
            "mean                   NaN       135.000  42491.225  39618.637  44500.888 102198.066\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Iran____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000 18329.573 11139.785 35700.573 18731.632\n",
            "1                      MAE        60.000   503.957   322.297   842.454 13270.076\n",
            "2                      MAE        90.000   377.262   383.993   134.460 22393.566\n",
            "3                      MAE       120.000   345.618   346.263   279.746 13608.778\n",
            "4                      MAE       150.000   221.450   221.703   251.631 11638.398\n",
            "5                      MAE       180.000  1225.344  1225.657  1181.683 37745.040\n",
            "6                      MAE       210.000  5740.968  5740.968  5814.921 43954.973\n",
            "7                      MAE       240.000  8220.216  8220.216  8221.856 14800.547\n",
            "mean                   NaN       135.000  4370.548  3450.110  6553.416 22017.876\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Iran____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  10.684    6.827   21.309  13.700\n",
            "1                     MAPE        60.000   0.207    0.129    0.344   5.379\n",
            "2                     MAPE        90.000   0.153    0.156    0.054   9.165\n",
            "3                     MAPE       120.000   0.149    0.149    0.122   5.532\n",
            "4                     MAPE       150.000   0.092    0.092    0.100   5.147\n",
            "5                     MAPE       180.000   0.309    0.309    0.298  10.191\n",
            "6                     MAPE       210.000   0.635    0.635    0.644   4.810\n",
            "7                     MAPE       240.000   0.620    0.620    0.620   1.143\n",
            "mean                   NaN       135.000   1.606    1.115    2.936   6.883\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Iran____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000 23195.918 13584.430 47362.302 25879.225\n",
            "1                     RMSE        60.000   603.259   404.747   966.501 15570.863\n",
            "2                     RMSE        90.000   393.589   399.914   213.213 27210.322\n",
            "3                     RMSE       120.000   431.093   431.367   379.830 17533.330\n",
            "4                     RMSE       150.000   304.593   304.934   374.735 15002.066\n",
            "5                     RMSE       180.000  1302.469  1302.749  1255.899 58095.363\n",
            "6                     RMSE       210.000  6185.810  6185.810  6260.638 50528.266\n",
            "7                     RMSE       240.000  8253.395  8253.395  8256.259 18102.946\n",
            "mean                   NaN       135.000  5083.766  3858.418  8133.672 28490.298\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Belgium____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000 21139.600 24679.298 11557.338 38800.688\n",
            "1                      MAE        60.000 12271.166  1576.404 17478.828 31426.548\n",
            "2                      MAE        90.000   294.166   301.114   501.347  4890.978\n",
            "3                      MAE       120.000    84.355    84.051   119.610  1014.575\n",
            "4                      MAE       150.000   304.601   304.509   320.720  4755.831\n",
            "5                      MAE       180.000  4776.663  4776.609  4715.899  9034.796\n",
            "6                      MAE       210.000  6922.671  6922.671  7063.598 32736.005\n",
            "7                      MAE       240.000  4039.679  4039.679  2948.464 49497.931\n",
            "mean                   NaN       135.000  6229.113  5335.542  5588.225 21519.669\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Belgium____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  84.981   98.372   45.886 131.005\n",
            "1                     MAPE        60.000 125.769   16.205  172.914 285.969\n",
            "2                     MAPE        90.000   2.187    2.260    4.680  45.538\n",
            "3                     MAPE       120.000   0.184    0.183    0.217   2.083\n",
            "4                     MAPE       150.000   0.336    0.335    0.299   6.625\n",
            "5                     MAPE       180.000   0.798    0.798    0.790   2.718\n",
            "6                     MAPE       210.000   0.631    0.631    0.657   5.530\n",
            "7                     MAPE       240.000   1.559    1.559    0.990  19.627\n",
            "mean                   NaN       135.000  27.056   15.043   28.304  62.387\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Belgium____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000 34492.744 39332.662 18581.432 50320.864\n",
            "1                     RMSE        60.000 18620.135  2375.365 21916.250 35680.299\n",
            "2                     RMSE        90.000   335.239   340.311   547.203  5860.658\n",
            "3                     RMSE       120.000   110.124   109.777   138.544  1336.079\n",
            "4                     RMSE       150.000   418.304   418.221   482.579  6134.523\n",
            "5                     RMSE       180.000  6176.327  6176.298  6095.365 11088.226\n",
            "6                     RMSE       210.000  8465.829  8465.829  8424.256 41584.861\n",
            "7                     RMSE       240.000  4615.169  4615.169  4554.921 56851.474\n",
            "mean                   NaN       135.000  9154.234  7729.204  7592.569 26107.123\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Iraq____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000   75.150   74.950  102.737   560.389\n",
            "1                      MAE        60.000  576.945  579.363  548.519   496.787\n",
            "2                      MAE        90.000 1109.550 1109.550 1175.960  2720.575\n",
            "3                      MAE       120.000 7057.821 7057.821 8184.254  4581.219\n",
            "4                      MAE       150.000 8530.072 8530.072  155.272 25052.336\n",
            "5                      MAE       180.000 3029.890 3029.890 6645.887  5473.282\n",
            "6                      MAE       210.000  740.953  740.953 6055.557 14171.157\n",
            "7                      MAE       240.000   57.000   57.000   56.692  4924.641\n",
            "mean                   NaN       135.000 2647.173 2647.450 2865.610  7247.548\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Iraq____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   0.779    0.777    1.067   5.708\n",
            "1                     MAPE        60.000   0.529    0.531    0.507   0.609\n",
            "2                     MAPE        90.000   0.492    0.492    0.521   1.187\n",
            "3                     MAPE       120.000   2.009    2.009    2.380   1.323\n",
            "4                     MAPE       150.000   2.061    2.061    0.038   6.009\n",
            "5                     MAPE       180.000   0.829    0.829    1.763   1.452\n",
            "6                     MAPE       210.000   0.258    0.258    1.965   4.626\n",
            "7                     MAPE       240.000   0.026    0.026    0.026   2.275\n",
            "mean                   NaN       135.000   0.873    0.873    1.033   2.899\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Iraq____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000   87.104   86.651  119.507   720.867\n",
            "1                     RMSE        60.000  670.451  673.447  633.173   563.519\n",
            "2                     RMSE        90.000 1342.218 1342.218 1453.662  4035.698\n",
            "3                     RMSE       120.000 8457.286 8457.286 9247.258  5952.940\n",
            "4                     RMSE       150.000 8875.918 8875.918  186.879 28308.026\n",
            "5                     RMSE       180.000 3182.349 3182.349 6655.064  6809.620\n",
            "6                     RMSE       210.000  826.920  826.920 6062.607 15758.854\n",
            "7                     RMSE       240.000   78.364   78.364   87.706  7101.631\n",
            "mean                   NaN       135.000 2940.076 2940.394 3055.732  8656.394\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Czechia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000   334.368   777.803   112.318  1362.424\n",
            "1                      MAE        60.000   323.622   280.207   523.463   624.748\n",
            "2                      MAE        90.000    32.640    32.388    20.678   529.426\n",
            "3                      MAE       120.000   120.951   120.176    82.286  1259.266\n",
            "4                      MAE       150.000   691.437   691.794   648.401  1055.031\n",
            "5                      MAE       180.000  3920.339  3920.339  3744.421  2183.749\n",
            "6                      MAE       210.000  7134.428  7134.428 10923.363 18211.420\n",
            "7                      MAE       240.000 18049.402 18049.402 11692.291  3905.946\n",
            "mean                   NaN       135.000  3825.898  3875.817  3468.403  3641.501\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Czechia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   6.414   14.291    2.076  25.061\n",
            "1                     MAPE        60.000   5.465    4.828    8.681  11.709\n",
            "2                     MAPE        90.000   0.226    0.225    0.163   4.105\n",
            "3                     MAPE       120.000   0.535    0.532    0.358   5.613\n",
            "4                     MAPE       150.000   0.723    0.724    0.655   1.565\n",
            "5                     MAPE       180.000   0.832    0.832    0.789   0.386\n",
            "6                     MAPE       210.000   1.062    1.062    1.637   2.620\n",
            "7                     MAPE       240.000   4.181    4.181    2.736   0.869\n",
            "mean                   NaN       135.000   2.430    3.334    2.137   6.491\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Czechia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000   406.914   954.865   161.742  1624.005\n",
            "1                     RMSE        60.000   505.679   404.213   759.041   733.771\n",
            "2                     RMSE        90.000    41.832    41.429    26.277   619.241\n",
            "3                     RMSE       120.000   125.452   124.556    88.931  1718.748\n",
            "4                     RMSE       150.000   878.254   878.314   838.519  1322.580\n",
            "5                     RMSE       180.000  4619.733  4619.733  4429.260  3111.530\n",
            "6                     RMSE       210.000  9748.593  9748.593 14984.670 23866.649\n",
            "7                     RMSE       240.000 18203.977 18203.977 14864.921  6016.054\n",
            "mean                   NaN       135.000  4316.304  4371.960  4519.170  4876.572\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Netherlands____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000 8574.798 4109.370  2386.971 26770.655\n",
            "1                      MAE        60.000 1969.299 6151.000 49602.401 11627.631\n",
            "2                      MAE        90.000  247.658  258.374   272.291  2102.599\n",
            "3                      MAE       120.000  104.183  104.173   180.070  1182.805\n",
            "4                      MAE       150.000  373.557  373.537   386.857  2524.618\n",
            "5                      MAE       180.000 4104.381 4104.370  4143.095  3715.303\n",
            "6                      MAE       210.000 4149.432 4149.432  3574.126 32260.324\n",
            "7                      MAE       240.000 6438.924 6438.924   375.408  9890.508\n",
            "mean                   NaN       135.000 3245.279 3211.148  7615.152 11259.305\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Netherlands____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  42.450   19.882   10.886 120.234\n",
            "1                     MAPE        60.000  13.815   47.665  368.990  76.581\n",
            "2                     MAPE        90.000   3.079    3.232    3.585  27.925\n",
            "3                     MAPE       120.000   0.370    0.370    0.373   3.230\n",
            "4                     MAPE       150.000   0.308    0.308    0.318   2.667\n",
            "5                     MAPE       180.000   0.832    0.832    0.843   0.986\n",
            "6                     MAPE       210.000   0.502    0.502    0.478   5.277\n",
            "7                     MAPE       240.000   1.251    1.251    0.073   1.926\n",
            "mean                   NaN       135.000   7.826    9.255   48.193  29.853\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Netherlands____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg  HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000 12208.613 5556.126  3422.275 33053.105\n",
            "1                     RMSE        60.000  2325.789 9310.607 67749.588 14193.801\n",
            "2                     RMSE        90.000   277.561  285.795   319.798  2412.118\n",
            "3                     RMSE       120.000   119.814  119.900   215.706  1656.229\n",
            "4                     RMSE       150.000   555.369  555.370   578.878  3228.020\n",
            "5                     RMSE       180.000  4627.758 4627.751  4659.327  4817.189\n",
            "6                     RMSE       210.000  4832.157 4832.157  4001.115 40438.127\n",
            "7                     RMSE       240.000  6455.182 6455.182   471.177 12537.801\n",
            "mean                   NaN       135.000  3925.280 3967.861 10177.233 14042.049\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Romania____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                      MAE        30.000  925.405  637.583  276.853 3752.673\n",
            "1                      MAE        60.000  993.786  356.858 2388.230 1157.136\n",
            "2                      MAE        90.000  220.565  207.012  262.040 1631.980\n",
            "3                      MAE       120.000  722.652  722.637  721.421 1533.802\n",
            "4                      MAE       150.000  276.320  276.320   40.017 9210.443\n",
            "5                      MAE       180.000  912.709  912.709 3644.341 2402.664\n",
            "6                      MAE       210.000 3847.965 3847.965 3725.151 5027.761\n",
            "7                      MAE       240.000 1628.513 1628.513 1304.841 5570.817\n",
            "mean                   NaN       135.000 1190.989 1073.700 1545.362 3785.910\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Romania____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   4.755    3.183    1.404  16.361\n",
            "1                     MAPE        60.000   3.891    1.554    8.818   5.816\n",
            "2                     MAPE        90.000   0.318    0.294    0.414   3.575\n",
            "3                     MAPE       120.000   0.607    0.607    0.606   1.284\n",
            "4                     MAPE       150.000   0.223    0.223    0.031   7.475\n",
            "5                     MAPE       180.000   0.286    0.286    1.449   1.065\n",
            "6                     MAPE       210.000   0.535    0.535    0.515   0.716\n",
            "7                     MAPE       240.000   0.198    0.198    0.158   0.674\n",
            "mean                   NaN       135.000   1.352    0.860    1.674   4.621\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Romania____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000 1402.177  912.978  456.554  4192.086\n",
            "1                     RMSE        60.000 1379.802  497.672 3821.212  1374.300\n",
            "2                     RMSE        90.000  305.308  292.775  326.633  1918.002\n",
            "3                     RMSE       120.000  726.289  726.274  725.226  1881.718\n",
            "4                     RMSE       150.000  319.882  319.882   59.378 10924.179\n",
            "5                     RMSE       180.000 1198.888 1198.888 3809.908  2903.885\n",
            "6                     RMSE       210.000 4123.387 4123.387 4016.013  6698.724\n",
            "7                     RMSE       240.000 1672.302 1672.302 1355.485  7519.047\n",
            "mean                   NaN       135.000 1391.004 1218.020 1821.301  4676.493\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Philippines____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000  359.383  395.645  948.401  3926.325\n",
            "1                      MAE        60.000 1124.727 1200.756 1436.746  6576.268\n",
            "2                      MAE        90.000  843.497  843.490  839.668  3025.848\n",
            "3                      MAE       120.000 1754.684 1754.684 1737.891  8168.251\n",
            "4                      MAE       150.000 3689.086 3689.086  632.217  9666.245\n",
            "5                      MAE       180.000 5288.007 5288.007 2205.901 17188.228\n",
            "6                      MAE       210.000 5206.463 5206.463 4524.382  8928.610\n",
            "7                      MAE       240.000  131.629  131.629   39.708  2688.112\n",
            "mean                   NaN       135.000 2299.684 2313.720 1545.614  7520.986\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Philippines____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   1.354    1.444    3.727  15.595\n",
            "1                     MAPE        60.000   1.799    1.911    2.285  11.088\n",
            "2                     MAPE        90.000   0.544    0.544    0.541   1.902\n",
            "3                     MAPE       120.000   0.456    0.456    0.450   2.007\n",
            "4                     MAPE       150.000   1.138    1.138    0.204   2.951\n",
            "5                     MAPE       180.000   2.200    2.200    0.967   7.157\n",
            "6                     MAPE       210.000   2.982    2.982    2.592   5.086\n",
            "7                     MAPE       240.000   0.082    0.082    0.024   1.712\n",
            "mean                   NaN       135.000   1.319    1.345    1.349   5.937\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Philippines____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000  728.327  782.011 1537.504  4940.936\n",
            "1                     RMSE        60.000 1589.930 1671.665 2065.425  6850.770\n",
            "2                     RMSE        90.000  898.661  898.654  895.525  3735.195\n",
            "3                     RMSE       120.000 1835.514 1835.514 1822.040 11381.493\n",
            "4                     RMSE       150.000 4310.765 4310.765  737.285 12190.881\n",
            "5                     RMSE       180.000 5335.203 5335.203 3365.768 19282.736\n",
            "6                     RMSE       210.000 5239.844 5239.844 4552.464 11023.141\n",
            "7                     RMSE       240.000  160.491  160.491   64.056  4637.422\n",
            "mean                   NaN       135.000 2512.342 2529.268 1880.008  9255.322\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Pakistan____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000 7692.772 8009.150 10671.477  3353.911\n",
            "1                      MAE        60.000 3298.678 2166.587 20995.288 21939.632\n",
            "2                      MAE        90.000 3741.440 3719.515  9705.723 40399.783\n",
            "3                      MAE       120.000 9115.511 9003.819  1237.587 57762.958\n",
            "4                      MAE       150.000  251.901  253.604  6325.062 35493.938\n",
            "5                      MAE       180.000 1289.390 1288.930   723.536  5990.845\n",
            "6                      MAE       210.000  515.142  515.118   823.553  4443.890\n",
            "7                      MAE       240.000 1028.308 1028.394   308.183  1142.850\n",
            "mean                   NaN       135.000 3366.643 3248.140  6348.801 21315.976\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Pakistan____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   4.237    4.383    5.871   1.943\n",
            "1                     MAPE        60.000   0.727    0.515    4.598   4.824\n",
            "2                     MAPE        90.000   2.226    2.213    5.349  19.564\n",
            "3                     MAPE       120.000  14.266   14.091    1.917  88.185\n",
            "4                     MAPE       150.000   0.566    0.566   14.094  72.557\n",
            "5                     MAPE       180.000   2.062    2.062    1.156   9.682\n",
            "6                     MAPE       210.000   0.362    0.362    0.397   2.514\n",
            "7                     MAPE       240.000   0.351    0.351    0.105   0.389\n",
            "mean                   NaN       135.000   3.100    3.068    4.186  24.957\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Pakistan____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000 10033.444 10497.713 13804.829  4433.781\n",
            "1                     RMSE        60.000  4968.592  3526.155 27747.217 23769.095\n",
            "2                     RMSE        90.000  4796.777  4769.136 11091.587 49216.803\n",
            "3                     RMSE       120.000  9259.556  9145.934  1245.621 61940.906\n",
            "4                     RMSE       150.000   306.172   304.121  6723.186 37460.058\n",
            "5                     RMSE       180.000  1363.272  1362.647   792.345  7068.350\n",
            "6                     RMSE       210.000   582.807   582.831  1044.684  5484.947\n",
            "7                     RMSE       240.000  1028.329  1028.415   316.245  1616.164\n",
            "mean                   NaN       135.000  4042.369  3902.119  7845.714 23873.763\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Canada____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg  ARF_Reg     PA_Reg\n",
            "0                      MAE        30.000 12886.223  7198.641 2710.456  66285.868\n",
            "1                      MAE        60.000 15024.052 11757.905 1815.243 160550.929\n",
            "2                      MAE        90.000    43.467    46.303  605.836  25846.431\n",
            "3                      MAE       120.000   383.531   389.514  325.994   7966.147\n",
            "4                      MAE       150.000   261.969   262.384  142.329   6853.923\n",
            "5                      MAE       180.000  1165.450  1165.452  911.025   6163.381\n",
            "6                      MAE       210.000  2520.243  2520.239 2612.601  23920.633\n",
            "7                      MAE       240.000  2587.493  2587.493 2565.694  16762.126\n",
            "mean                   NaN       135.000  4359.053  3240.992 1461.147  39293.680\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Canada____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  10.853    5.931    2.134  55.211\n",
            "1                     MAPE        60.000  37.320   30.346    4.160 341.038\n",
            "2                     MAPE        90.000   0.121    0.134    1.801  79.097\n",
            "3                     MAPE       120.000   0.979    0.994    0.836  19.715\n",
            "4                     MAPE       150.000   0.504    0.505    0.240  11.915\n",
            "5                     MAPE       180.000   0.578    0.578    0.435   3.162\n",
            "6                     MAPE       210.000   0.671    0.671    0.698   6.464\n",
            "7                     MAPE       240.000   0.498    0.498    0.493   3.235\n",
            "mean                   NaN       135.000   6.440    4.957    1.350  64.980\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Canada____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg  ARF_Reg     PA_Reg\n",
            "0                     RMSE        30.000 17987.321  9863.720 3432.601  87007.660\n",
            "1                     RMSE        60.000 22292.908 19721.724 2368.031 181286.643\n",
            "2                     RMSE        90.000    64.161    56.791  609.106  29010.091\n",
            "3                     RMSE       120.000   425.491   431.045  370.802   9235.230\n",
            "4                     RMSE       150.000   286.148   286.627  168.328   8860.268\n",
            "5                     RMSE       180.000  1259.175  1259.171 1032.870   8559.682\n",
            "6                     RMSE       210.000  2699.035  2699.032 2784.647  28481.183\n",
            "7                     RMSE       240.000  2589.954  2589.954 2579.406  19637.267\n",
            "mean                   NaN       135.000  5950.524  4613.508 1668.224  46509.753\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Israel____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000 4916.178 6188.315 4217.941  5006.700\n",
            "1                      MAE        60.000 2037.425 2307.327  413.689  1595.190\n",
            "2                      MAE        90.000  909.055  913.642  817.270  1178.408\n",
            "3                      MAE       120.000 1060.846 1060.543  850.839  5674.349\n",
            "4                      MAE       150.000 2150.206 2150.216 3589.596  9785.023\n",
            "5                      MAE       180.000 1951.627 1951.627 1976.551  8151.618\n",
            "6                      MAE       210.000 1722.810 1722.810 6858.877 27363.877\n",
            "7                      MAE       240.000 6834.671 6834.671 6800.197 18339.573\n",
            "mean                   NaN       135.000 2697.852 2891.144 3190.620  9636.842\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Israel____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000 249.597  316.635  206.454 180.402\n",
            "1                     MAPE        60.000   9.823    9.863    2.356  12.517\n",
            "2                     MAPE        90.000   0.679    0.685    0.615   0.815\n",
            "3                     MAPE       120.000   0.711    0.710    0.566   3.954\n",
            "4                     MAPE       150.000   0.563    0.563    1.116   3.460\n",
            "5                     MAPE       180.000   1.103    1.103    1.127   3.224\n",
            "6                     MAPE       210.000   2.469    2.469    9.977  41.103\n",
            "7                     MAPE       240.000   8.149    8.149    8.087  21.639\n",
            "mean                   NaN       135.000  34.137   42.522   28.787  33.389\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Israel____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000 7031.791 8451.657 5950.045  5432.592\n",
            "1                     RMSE        60.000 2807.838 3498.793  544.174  1956.046\n",
            "2                     RMSE        90.000 1026.528 1029.068  942.505  1656.765\n",
            "3                     RMSE       120.000 1071.164 1070.838  863.179  7159.492\n",
            "4                     RMSE       150.000 2721.891 2721.906 3934.880 10933.366\n",
            "5                     RMSE       180.000 2291.693 2291.693 2342.375 10044.192\n",
            "6                     RMSE       210.000 2240.562 2240.562 6969.331 34960.794\n",
            "7                     RMSE       240.000 6951.992 6951.992 6871.161 22463.457\n",
            "mean                   NaN       135.000 3267.932 3532.064 3552.206 11825.838\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Switzerland____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg   HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000 6150.289 17081.029 15090.674 13881.193\n",
            "1                      MAE        60.000 2508.623  8143.700   848.702  2637.172\n",
            "2                      MAE        90.000  243.368   248.669   164.936   263.938\n",
            "3                      MAE       120.000   89.759    87.142    24.798   847.732\n",
            "4                      MAE       150.000  151.585   152.169   127.128   835.116\n",
            "5                      MAE       180.000  879.668   879.564   784.563  3733.895\n",
            "6                      MAE       210.000 5764.920  5764.908  5849.357  6781.406\n",
            "7                      MAE       240.000 1365.622  1365.622  1042.465 39222.908\n",
            "mean                   NaN       135.000 2144.229  4215.351  2991.578  8525.420\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Switzerland____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000 178.794  509.513  461.453 259.118\n",
            "1                     MAPE        60.000 125.087  368.817   42.940 146.476\n",
            "2                     MAPE        90.000   2.918    3.005    1.858   3.506\n",
            "3                     MAPE       120.000   0.588    0.572    0.122   4.796\n",
            "4                     MAPE       150.000   0.422    0.424    0.348   2.317\n",
            "5                     MAPE       180.000   0.590    0.590    0.454   5.082\n",
            "6                     MAPE       210.000   0.893    0.893    0.909   1.139\n",
            "7                     MAPE       240.000   0.323    0.323    0.276   9.346\n",
            "mean                   NaN       135.000  38.702  110.517   63.545  53.972\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Switzerland____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg   HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000 9483.817 24797.442 24893.821 16048.882\n",
            "1                     RMSE        60.000 3707.404 10691.415   895.181  3314.946\n",
            "2                     RMSE        90.000  257.619   261.814   186.061   376.129\n",
            "3                     RMSE       120.000   94.879    92.370    35.713  1042.112\n",
            "4                     RMSE       150.000  165.426   166.020   144.098  1108.656\n",
            "5                     RMSE       180.000 1338.054  1337.953  1262.743  4107.394\n",
            "6                     RMSE       210.000 5954.274  5954.260  6028.255  8672.820\n",
            "7                     RMSE       240.000 1396.215  1396.216  1386.859 46916.982\n",
            "mean                   NaN       135.000 2799.711  5587.186  4354.091 10198.490\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Austria____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg   ARF_Reg   PA_Reg\n",
            "0                      MAE        30.000 2410.097 1793.220 33886.340 5885.078\n",
            "1                      MAE        60.000 3300.796 5777.400 18293.657  424.638\n",
            "2                      MAE        90.000   81.830   77.506    56.397  503.039\n",
            "3                      MAE       120.000   44.641   44.987    43.927  326.093\n",
            "4                      MAE       150.000  295.390  295.079   214.422 1253.134\n",
            "5                      MAE       180.000  763.541  763.618   664.300 1428.194\n",
            "6                      MAE       210.000 4219.567 4219.394  4113.648 2346.306\n",
            "7                      MAE       240.000 2065.966 2065.966   420.907 3215.157\n",
            "mean                   NaN       135.000 1647.729 1879.646  7211.700 1922.705\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Austria____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  56.369   40.564  771.671 133.421\n",
            "1                     MAPE        60.000 105.242  191.321  607.276  13.792\n",
            "2                     MAPE        90.000   1.011    0.960    0.668   6.149\n",
            "3                     MAPE       120.000   0.264    0.267    0.218   2.226\n",
            "4                     MAPE       150.000   0.621    0.621    0.396   3.115\n",
            "5                     MAPE       180.000   0.750    0.750    0.652   1.575\n",
            "6                     MAPE       210.000   0.818    0.818    0.795   0.463\n",
            "7                     MAPE       240.000   0.373    0.373    0.080   0.591\n",
            "mean                   NaN       135.000  20.681   29.459  172.719  20.166\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Austria____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg   ARF_Reg   PA_Reg\n",
            "0                     RMSE        30.000 3936.750 2916.292 46252.394 7135.772\n",
            "1                     RMSE        60.000 5627.156 8127.760 23937.423  543.096\n",
            "2                     RMSE        90.000   83.687   79.246    60.747  562.995\n",
            "3                     RMSE       120.000   57.796   57.694    68.867  468.197\n",
            "4                     RMSE       150.000  348.193  347.695   288.348 1385.805\n",
            "5                     RMSE       180.000  837.273  837.448   729.141 1680.617\n",
            "6                     RMSE       210.000 4534.759 4534.584  4429.012 2823.712\n",
            "7                     RMSE       240.000 2111.551 2111.551   637.856 3590.639\n",
            "mean                   NaN       135.000 2192.146 2376.534  9550.474 2273.854\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Sweden____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000 1806.454 1139.463  525.738  5119.128\n",
            "1                      MAE        60.000 1016.510  521.697  417.276  2944.053\n",
            "2                      MAE        90.000  598.779  597.947  714.132 12217.785\n",
            "3                      MAE       120.000  123.165  132.761   94.736  5920.772\n",
            "4                      MAE       150.000  151.182  150.888  109.577  5759.436\n",
            "5                      MAE       180.000  107.262  107.372  120.974  1208.142\n",
            "6                      MAE       210.000 2605.896 2605.981 2530.129  2149.844\n",
            "7                      MAE       240.000 3701.411 3701.411 3891.503  4706.274\n",
            "mean                   NaN       135.000 1263.832 1119.690 1050.508  5003.180\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Sweden____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   3.209    2.062    0.961   9.286\n",
            "1                     MAPE        60.000   1.084    0.552    0.451   3.476\n",
            "2                     MAPE        90.000   1.988    1.986    2.291  37.016\n",
            "3                     MAPE       120.000   0.499    0.538    0.376  25.257\n",
            "4                     MAPE       150.000   0.712    0.710    0.516  26.285\n",
            "5                     MAPE       180.000   0.145    0.145    0.155   2.176\n",
            "6                     MAPE       210.000   0.775    0.775    0.743   0.740\n",
            "7                     MAPE       240.000   0.764    0.764    0.801   0.973\n",
            "mean                   NaN       135.000   1.147    0.941    0.787  13.151\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Sweden____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000 2540.829 1705.099  721.873  6064.321\n",
            "1                     RMSE        60.000 1223.655  633.939  472.859  3538.363\n",
            "2                     RMSE        90.000  690.537  689.896  793.880 14482.810\n",
            "3                     RMSE       120.000  144.205  154.041  109.133  7314.301\n",
            "4                     RMSE       150.000  168.013  167.595  133.902  6437.692\n",
            "5                     RMSE       180.000  173.913  174.045  196.625  1601.737\n",
            "6                     RMSE       210.000 2880.167 2880.263 2818.050  2638.036\n",
            "7                     RMSE       240.000 3729.357 3729.357 3937.949  5996.699\n",
            "mean                   NaN       135.000 1443.835 1266.779 1148.034  6009.245\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Ecuador____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000 20403.880 24161.144 29252.420  7684.498\n",
            "1                      MAE        60.000  3237.376 18658.316 21058.733 32523.786\n",
            "2                      MAE        90.000   429.931   810.505  1509.278 13049.336\n",
            "3                      MAE       120.000   233.014   227.680   170.966 21222.821\n",
            "4                      MAE       150.000   445.371   445.393   451.060 10417.033\n",
            "5                      MAE       180.000    95.040    96.629   116.024 14053.887\n",
            "6                      MAE       210.000    86.453    85.912   116.196 14829.557\n",
            "7                      MAE       240.000    52.634    52.792     9.341  8047.195\n",
            "mean                   NaN       135.000  3122.962  5567.296  6585.502 15228.514\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Ecuador____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  67.835   78.175   93.869  24.177\n",
            "1                     MAPE        60.000   5.771   32.790   38.801  64.427\n",
            "2                     MAPE        90.000   0.476    0.897    1.687  14.617\n",
            "3                     MAPE       120.000   0.236    0.230    0.172  22.341\n",
            "4                     MAPE       150.000   2.966    2.972    2.862  48.506\n",
            "5                     MAPE       180.000   0.100    0.102    0.131  15.025\n",
            "6                     MAPE       210.000   0.091    0.090    0.129  16.762\n",
            "7                     MAPE       240.000   0.060    0.060    0.011   9.071\n",
            "mean                   NaN       135.000   9.692   14.415   17.208  26.866\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Ecuador____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000 25921.622 30328.426 39015.459 10116.184\n",
            "1                     RMSE        60.000  4551.382 26030.946 36326.713 57202.774\n",
            "2                     RMSE        90.000   554.268  1070.672  1957.334 15268.012\n",
            "3                     RMSE       120.000   249.095   245.049   193.783 23356.775\n",
            "4                     RMSE       150.000   579.763   580.145   562.053 12333.841\n",
            "5                     RMSE       180.000   114.182   116.050   139.376 21701.209\n",
            "6                     RMSE       210.000   117.406   116.353   141.991 19119.220\n",
            "7                     RMSE       240.000    56.990    57.189    13.202 11380.382\n",
            "mean                   NaN       135.000  4018.089  7318.104  9793.739 21309.800\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_Arab_Emirates____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000 1437.751 1864.147 1738.222  9930.261\n",
            "1                      MAE        60.000 2678.061 2234.658 3811.690 35074.874\n",
            "2                      MAE        90.000  295.984  319.650 1148.808 10026.896\n",
            "3                      MAE       120.000   96.781   93.892  247.049 13206.566\n",
            "4                      MAE       150.000  314.744  314.497  287.500 11901.249\n",
            "5                      MAE       180.000  445.341  445.824  624.601  3734.542\n",
            "6                      MAE       210.000  217.732  217.731  518.913  4957.910\n",
            "7                      MAE       240.000    1.077    1.077    0.232     0.100\n",
            "mean                   NaN       135.000  685.934  686.434 1047.127 11104.050\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_Arab_Emirates____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   1.842    2.394    2.226  13.000\n",
            "1                     MAPE        60.000   6.483    5.407    8.858  81.234\n",
            "2                     MAPE        90.000   0.857    0.921    3.231  24.735\n",
            "3                     MAPE       120.000   0.369    0.359    0.851  42.886\n",
            "4                     MAPE       150.000   0.423    0.423    0.379  17.971\n",
            "5                     MAPE       180.000   0.371    0.371    0.520   3.107\n",
            "6                     MAPE       210.000   0.176    0.176    0.424   4.130\n",
            "7                     MAPE       240.000   0.001    0.001    0.000   0.000\n",
            "mean                   NaN       135.000   1.315    1.256    2.061  23.383\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_Arab_Emirates____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000 1936.064 2627.994 2411.868 12380.757\n",
            "1                     RMSE        60.000 3850.953 3075.245 4657.137 40556.202\n",
            "2                     RMSE        90.000  309.889  332.828 1170.760 11431.809\n",
            "3                     RMSE       120.000  113.651  110.717  265.691 15437.788\n",
            "4                     RMSE       150.000  341.825  341.417  320.474 14078.089\n",
            "5                     RMSE       180.000  465.868  466.369  652.888  5488.086\n",
            "6                     RMSE       210.000  253.719  253.719  564.573  6630.815\n",
            "7                     RMSE       240.000    1.077    1.077    0.232     0.100\n",
            "mean                   NaN       135.000  909.131  901.171 1255.453 13250.456\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Japan____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                      MAE        30.000 1211.382 1121.612 1185.724 1644.152\n",
            "1                      MAE        60.000 1486.479 1410.170 1197.098 8168.327\n",
            "2                      MAE        90.000  107.138  156.513  341.383 2361.612\n",
            "3                      MAE       120.000  811.578  812.771  778.739  680.355\n",
            "4                      MAE       150.000  153.940  153.940  376.509 2538.891\n",
            "5                      MAE       180.000  614.480  614.432  546.165 3626.225\n",
            "6                      MAE       210.000  366.580  366.580 1176.701 1647.503\n",
            "7                      MAE       240.000 1164.900 1164.900 1235.153 1602.147\n",
            "mean                   NaN       135.000  739.560  725.115  854.684 2783.652\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Japan____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  19.304   17.636   18.410  29.204\n",
            "1                     MAPE        60.000  30.898   29.219   24.688 167.307\n",
            "2                     MAPE        90.000   0.294    0.460    1.952  12.006\n",
            "3                     MAPE       120.000   0.725    0.726    0.691   0.621\n",
            "4                     MAPE       150.000   0.221    0.221    0.676   4.671\n",
            "5                     MAPE       180.000   1.184    1.184    1.104   7.176\n",
            "6                     MAPE       210.000   0.293    0.293    1.291   1.956\n",
            "7                     MAPE       240.000   0.569    0.569    0.603   0.766\n",
            "mean                   NaN       135.000   6.686    6.288    6.177  27.963\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Japan____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                     RMSE        30.000 1521.457 1391.426 1446.463 2345.500\n",
            "1                     RMSE        60.000 1714.444 1632.371 1346.265 9674.068\n",
            "2                     RMSE        90.000  157.107  202.643  413.266 2529.511\n",
            "3                     RMSE       120.000  851.872  853.031  821.938  854.016\n",
            "4                     RMSE       150.000  194.721  194.702  456.265 3324.247\n",
            "5                     RMSE       180.000  694.092  694.048  735.040 4429.192\n",
            "6                     RMSE       210.000  514.264  514.263 1274.883 2060.624\n",
            "7                     RMSE       240.000 1172.930 1172.930 1244.078 1928.908\n",
            "mean                   NaN       135.000  852.611  831.927  967.275 3393.258\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Kuwait____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                      MAE        30.000  343.610  334.690  291.114  359.377\n",
            "1                      MAE        60.000 2895.191 2895.352 2757.238 3963.768\n",
            "2                      MAE        90.000  634.170  633.762  279.887 1969.235\n",
            "3                      MAE       120.000 1097.531 1097.690 2033.577 8658.215\n",
            "4                      MAE       150.000  106.432  106.431  426.377 2277.867\n",
            "5                      MAE       180.000   68.503   68.503   74.941 1923.379\n",
            "6                      MAE       210.000   59.536   59.539   60.753 2105.146\n",
            "7                      MAE       240.000  130.925  130.919  197.143  870.647\n",
            "mean                   NaN       135.000  666.987  665.861  765.129 2765.954\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Kuwait____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   0.529    0.516    0.427   0.502\n",
            "1                     MAPE        60.000   4.712    4.712    4.381   6.462\n",
            "2                     MAPE        90.000   0.953    0.952    0.406   2.862\n",
            "3                     MAPE       120.000   1.835    1.836    3.409  14.534\n",
            "4                     MAPE       150.000   0.151    0.151    0.640   3.363\n",
            "5                     MAPE       180.000   0.119    0.119    0.122   3.013\n",
            "6                     MAPE       210.000   0.110    0.110    0.110   3.175\n",
            "7                     MAPE       240.000   0.337    0.337    0.520   2.375\n",
            "mean                   NaN       135.000   1.093    1.092    1.252   4.536\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Kuwait____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                     RMSE        30.000  384.168  373.856  332.415  482.823\n",
            "1                     RMSE        60.000 3763.315 3763.268 3168.494 4898.836\n",
            "2                     RMSE        90.000  749.676  747.622  285.838 2337.038\n",
            "3                     RMSE       120.000 1112.649 1112.809 2038.788 9063.851\n",
            "4                     RMSE       150.000  126.508  126.506  489.029 2542.662\n",
            "5                     RMSE       180.000   77.604   77.603   83.997 2408.276\n",
            "6                     RMSE       210.000   96.862   96.866   95.728 2482.147\n",
            "7                     RMSE       240.000  131.716  131.709  228.259 1371.821\n",
            "mean                   NaN       135.000  805.312  803.780  840.318 3198.432\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Qatar____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000 2349.777 2022.811 4228.953  4991.316\n",
            "1                      MAE        60.000 6406.527 6224.812 1504.581  3252.160\n",
            "2                      MAE        90.000  779.677  779.994  363.025 32705.243\n",
            "3                      MAE       120.000  127.478  126.533 1693.868  9991.261\n",
            "4                      MAE       150.000  368.011  367.916  339.513  3017.159\n",
            "5                      MAE       180.000  489.752  489.735  103.224   764.820\n",
            "6                      MAE       210.000  269.084  269.082   13.199   383.657\n",
            "7                      MAE       240.000  138.001  138.001    7.866   158.580\n",
            "mean                   NaN       135.000 1366.038 1302.360 1031.779  6908.024\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Qatar____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   1.804    1.567    3.309   3.328\n",
            "1                     MAPE        60.000   5.282    5.147    1.064   2.755\n",
            "2                     MAPE        90.000   1.650    1.651    0.789  76.770\n",
            "3                     MAPE       120.000   0.461    0.458    6.101  36.375\n",
            "4                     MAPE       150.000   1.587    1.586    1.462  13.074\n",
            "5                     MAPE       180.000   2.256    2.256    0.479   3.513\n",
            "6                     MAPE       210.000   1.236    1.236    0.062   1.757\n",
            "7                     MAPE       240.000   0.683    0.683    0.039   0.790\n",
            "mean                   NaN       135.000   1.870    1.823    1.663  17.295\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Qatar____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000 3643.990 3092.774 6180.491  6825.213\n",
            "1                     RMSE        60.000 8306.282 8112.081 2511.958  4541.385\n",
            "2                     RMSE        90.000  946.485  947.254  413.201 43211.058\n",
            "3                     RMSE       120.000  143.616  142.670 1708.189 11296.908\n",
            "4                     RMSE       150.000  398.609  398.528  377.493  3431.750\n",
            "5                     RMSE       180.000  493.883  493.868  104.677   892.798\n",
            "6                     RMSE       210.000  269.563  269.561   15.792   506.636\n",
            "7                     RMSE       240.000  138.016  138.016    8.244   215.581\n",
            "mean                   NaN       135.000 1792.556 1699.344 1415.006  8865.166\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Georgia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                      MAE        30.000   6.119    6.349    2.743   47.576\n",
            "1                      MAE        60.000  13.718   12.442   30.446   74.303\n",
            "2                      MAE        90.000   1.676    1.690    9.018   16.193\n",
            "3                      MAE       120.000   1.960    1.951    2.009    9.027\n",
            "4                      MAE       150.000  78.685   78.683   78.219   80.060\n",
            "5                      MAE       180.000 583.131  583.131  570.299  266.990\n",
            "6                      MAE       210.000 804.601  804.601 1622.298 1257.921\n",
            "7                      MAE       240.000 899.516  899.516  120.803  925.668\n",
            "mean                   NaN       135.000 298.676  298.545  304.479  334.717\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Georgia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   1.002    1.047    0.403   8.236\n",
            "1                     MAPE        60.000   2.499    2.187    5.828  12.704\n",
            "2                     MAPE        90.000   0.250    0.264    2.124   3.271\n",
            "3                     MAPE       120.000   0.209    0.207    0.230   0.960\n",
            "4                     MAPE       150.000   0.673    0.672    0.657   0.795\n",
            "5                     MAPE       180.000   0.909    0.909    0.876   0.446\n",
            "6                     MAPE       210.000   0.274    0.274    0.547   0.446\n",
            "7                     MAPE       240.000   0.216    0.216    0.028   0.218\n",
            "mean                   NaN       135.000   0.754    0.722    1.337   3.384\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Georgia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                     RMSE        30.000    6.998    7.244    3.371   58.809\n",
            "1                     RMSE        60.000   15.096   13.703   32.264   83.599\n",
            "2                     RMSE        90.000    2.296    2.340   10.624   19.267\n",
            "3                     RMSE       120.000    2.310    2.326    2.452   10.611\n",
            "4                     RMSE       150.000  109.875  109.876  109.426  108.770\n",
            "5                     RMSE       180.000  685.515  685.515  678.634  304.226\n",
            "6                     RMSE       210.000 1091.309 1091.309 2154.410 1737.784\n",
            "7                     RMSE       240.000  936.002  936.002  155.145 1096.809\n",
            "mean                   NaN       135.000  356.175  356.039  393.291  427.484\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Lebanon____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                      MAE        30.000   11.485   21.039   15.787  90.883\n",
            "1                      MAE        60.000    7.573   12.047   33.333  12.459\n",
            "2                      MAE        90.000   44.568   44.611   44.251  48.413\n",
            "3                      MAE       120.000  268.347  268.347  271.022 199.534\n",
            "4                      MAE       150.000  199.638  199.638   74.954 762.784\n",
            "5                      MAE       180.000  661.545  661.545 1098.809 686.680\n",
            "6                      MAE       210.000 1536.273 1536.273 2778.171 937.935\n",
            "7                      MAE       240.000 3508.213 3508.213  250.701 638.273\n",
            "mean                   NaN       135.000  779.705  781.464  570.878 422.120\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Lebanon____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   0.924    1.487    1.051   7.054\n",
            "1                     MAPE        60.000   0.458    0.734    1.957   0.660\n",
            "2                     MAPE        90.000   0.579    0.580    0.544   0.744\n",
            "3                     MAPE       120.000   0.830    0.830    0.839   0.555\n",
            "4                     MAPE       150.000   0.307    0.307    0.112   1.167\n",
            "5                     MAPE       180.000   0.535    0.535    0.904   0.555\n",
            "6                     MAPE       210.000   0.973    0.973    1.743   0.590\n",
            "7                     MAPE       240.000   2.255    2.255    0.164   0.422\n",
            "mean                   NaN       135.000   0.858    0.963    0.914   1.468\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Lebanon____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                     RMSE        30.000   14.779   27.007   21.942  100.277\n",
            "1                     RMSE        60.000    9.140   14.146   39.829   17.508\n",
            "2                     RMSE        90.000   56.779   56.817   57.580   60.604\n",
            "3                     RMSE       120.000  302.675  302.675  305.547  248.881\n",
            "4                     RMSE       150.000  223.227  223.227   99.033  923.000\n",
            "5                     RMSE       180.000  752.712  752.712 1225.714  868.096\n",
            "6                     RMSE       210.000 1644.265 1644.265 3166.899 1104.942\n",
            "7                     RMSE       240.000 3511.920 3511.920  307.370  893.972\n",
            "mean                   NaN       135.000  814.437  816.596  652.989  527.160\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Croatia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                      MAE        30.000  284.882  280.108  290.041  661.754\n",
            "1                      MAE        60.000   83.625  228.544   82.016  108.537\n",
            "2                      MAE        90.000   36.570   36.752   33.806   25.122\n",
            "3                      MAE       120.000   82.858   82.923   92.269  360.594\n",
            "4                      MAE       150.000  142.200  142.200  135.408   96.793\n",
            "5                      MAE       180.000  234.952  234.952  233.945  314.525\n",
            "6                      MAE       210.000 1508.045 1508.045 1483.311 1063.763\n",
            "7                      MAE       240.000  895.516  895.516  780.565  580.598\n",
            "mean                   NaN       135.000  408.581  426.130  391.420  401.461\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Croatia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  72.166   69.893   51.370 125.544\n",
            "1                     MAPE        60.000 123.318  272.273   86.871 285.693\n",
            "2                     MAPE        90.000   0.481    0.483    0.438   0.380\n",
            "3                     MAPE       120.000   0.574    0.575    0.689   2.677\n",
            "4                     MAPE       150.000   0.594    0.594    0.567   0.422\n",
            "5                     MAPE       180.000   0.313    0.313    0.312   0.658\n",
            "6                     MAPE       210.000   0.654    0.654    0.644   0.454\n",
            "7                     MAPE       240.000   0.278    0.278    0.242   0.177\n",
            "mean                   NaN       135.000  24.797   43.133   17.642  52.001\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Croatia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                     RMSE        30.000  329.595  321.563  357.694  724.647\n",
            "1                     RMSE        60.000   98.117  272.633  191.333  130.859\n",
            "2                     RMSE        90.000   38.318   38.552   36.203   38.807\n",
            "3                     RMSE       120.000  115.484  115.572  122.656  495.804\n",
            "4                     RMSE       150.000  144.601  144.601  137.588  119.275\n",
            "5                     RMSE       180.000  377.258  377.258  374.322  440.781\n",
            "6                     RMSE       210.000 1542.027 1542.027 1515.393 1179.219\n",
            "7                     RMSE       240.000  910.841  910.841  808.023  739.352\n",
            "mean                   NaN       135.000  444.530  465.381  442.902  483.593\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Oman____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                      MAE        30.000   30.286   34.906   59.055  417.630\n",
            "1                      MAE        60.000  661.645  665.175  480.014 1831.875\n",
            "2                      MAE        90.000 1214.814 1215.117 1252.415 1745.413\n",
            "3                      MAE       120.000  541.665  541.553 3967.908 1801.649\n",
            "4                      MAE       150.000 2241.447 2241.446  528.021 9818.020\n",
            "5                      MAE       180.000  186.911  186.911  101.915 1988.720\n",
            "6                      MAE       210.000  453.893  453.893  218.346 2698.710\n",
            "7                      MAE       240.000  256.424  256.424    0.786    0.100\n",
            "mean                   NaN       135.000  698.386  699.428  826.057 2537.765\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Oman____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   0.155    0.163    0.254   1.538\n",
            "1                     MAPE        60.000   0.644    0.648    0.510   2.032\n",
            "2                     MAPE        90.000   0.962    0.962    0.938   1.428\n",
            "3                     MAPE       120.000   2.085    2.084   19.189   7.712\n",
            "4                     MAPE       150.000   6.950    6.950    1.596  23.085\n",
            "5                     MAPE       180.000   0.383    0.383    0.204   4.012\n",
            "6                     MAPE       210.000   1.619    1.619    0.832   9.418\n",
            "7                     MAPE       240.000   1.207    1.207    0.004   0.000\n",
            "mean                   NaN       135.000   1.751    1.752    2.941   6.153\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Oman____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000   37.740   42.952   87.180   643.973\n",
            "1                     RMSE        60.000  898.726  902.977  622.848  2205.606\n",
            "2                     RMSE        90.000 1679.919 1680.216 1474.803  2480.539\n",
            "3                     RMSE       120.000  591.702  591.596 4227.820  2190.872\n",
            "4                     RMSE       150.000 2249.524 2249.524  684.553 11808.306\n",
            "5                     RMSE       180.000  234.911  234.911  139.695  2575.679\n",
            "6                     RMSE       210.000  460.977  460.976  244.197  3356.058\n",
            "7                     RMSE       240.000  256.424  256.424    0.786     0.100\n",
            "mean                   NaN       135.000  801.240  802.447  935.235  3157.642\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Egypt____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                      MAE        30.000  618.634  315.372  205.295  365.172\n",
            "1                      MAE        60.000 1480.795 1383.124 2605.986  579.217\n",
            "2                      MAE        90.000 1409.882 1414.689  540.792 1622.910\n",
            "3                      MAE       120.000  960.738  907.940 3290.451 5887.533\n",
            "4                      MAE       150.000  301.737  300.893 1168.276  527.426\n",
            "5                      MAE       180.000  466.270  466.100  265.565  960.487\n",
            "6                      MAE       210.000  340.146  340.150   68.164  155.713\n",
            "7                      MAE       240.000   23.241   23.241    1.043   20.446\n",
            "mean                   NaN       135.000  700.181  643.939 1018.196 1264.863\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Egypt____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   1.138    0.657    0.362   0.742\n",
            "1                     MAPE        60.000   1.008    0.943    1.784   0.404\n",
            "2                     MAPE        90.000   1.902    1.907    0.478   2.005\n",
            "3                     MAPE       120.000   6.976    6.548   20.239  40.823\n",
            "4                     MAPE       150.000   1.921    1.915    7.316   3.345\n",
            "5                     MAPE       180.000   3.580    3.579    2.034   7.516\n",
            "6                     MAPE       210.000   1.662    1.662    0.260   0.779\n",
            "7                     MAPE       240.000   0.065    0.065    0.003   0.057\n",
            "mean                   NaN       135.000   2.281    2.160    4.059   6.959\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Egypt____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                     RMSE        30.000  782.522  390.417  310.480  498.114\n",
            "1                     RMSE        60.000 1976.065 1852.395 3493.411  766.367\n",
            "2                     RMSE        90.000 1833.592 1838.415  623.514 2092.538\n",
            "3                     RMSE       120.000 1318.578 1248.465 3357.316 7009.153\n",
            "4                     RMSE       150.000  316.581  315.714 1216.311  641.377\n",
            "5                     RMSE       180.000  505.202  505.007  295.841 1070.404\n",
            "6                     RMSE       210.000  347.013  347.016   92.147  207.389\n",
            "7                     RMSE       240.000   23.265   23.265    1.221   26.319\n",
            "mean                   NaN       135.000  887.852  815.087 1173.780 1538.958\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Greece____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                      MAE        30.000   63.514   23.138   76.826  387.776\n",
            "1                      MAE        60.000  192.983  108.159  399.363  259.083\n",
            "2                      MAE        90.000    6.399    6.249    5.165  104.550\n",
            "3                      MAE       120.000  108.211  108.251  113.235   88.212\n",
            "4                      MAE       150.000  168.557  168.569  175.234  315.264\n",
            "5                      MAE       180.000   54.098   54.097   51.496  249.039\n",
            "6                      MAE       210.000 1324.837 1324.837 1257.403 1058.784\n",
            "7                      MAE       240.000  403.123  403.123  187.628  611.691\n",
            "mean                   NaN       135.000  290.215  274.553  283.294  384.300\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Greece____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   6.279    1.869    6.689  30.016\n",
            "1                     MAPE        60.000  12.615    7.024   27.963  28.014\n",
            "2                     MAPE        90.000   0.291    0.279    0.186   4.227\n",
            "3                     MAPE       120.000   0.665    0.665    0.713   0.817\n",
            "4                     MAPE       150.000   0.697    0.697    0.727   1.233\n",
            "5                     MAPE       180.000   0.108    0.108    0.104   0.580\n",
            "6                     MAPE       210.000   0.643    0.643    0.603   0.499\n",
            "7                     MAPE       240.000   0.180    0.180    0.087   0.296\n",
            "mean                   NaN       135.000   2.685    1.433    4.634   8.210\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Greece____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                     RMSE        30.000   86.735   29.267   88.966  458.691\n",
            "1                     RMSE        60.000  226.278  125.986  490.183  321.589\n",
            "2                     RMSE        90.000    7.206    6.797    6.249  131.153\n",
            "3                     RMSE       120.000  129.151  129.234  133.632  117.784\n",
            "4                     RMSE       150.000  171.988  172.004  178.297  388.423\n",
            "5                     RMSE       180.000   95.131   95.130   90.193  355.907\n",
            "6                     RMSE       210.000 1446.970 1446.970 1384.202 1198.661\n",
            "7                     RMSE       240.000  444.337  444.337  204.380  734.190\n",
            "mean                   NaN       135.000  325.974  306.216  322.013  463.300\n",
            "\n",
            "\n",
            "\n",
            "_________________________________China____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                      MAE        30.000  50.300   48.331   41.923   4.289\n",
            "1                      MAE        60.000  16.064   15.994   13.099  13.345\n",
            "2                      MAE        90.000  21.228   21.231   19.017  31.578\n",
            "3                      MAE       120.000  81.317   81.314   60.117  71.028\n",
            "4                      MAE       150.000  49.262   49.261   32.050 108.664\n",
            "5                      MAE       180.000  30.355   30.354    2.408  77.398\n",
            "6                      MAE       210.000  10.296   10.296    5.365  15.287\n",
            "7                      MAE       240.000  13.558   13.558    2.929  12.331\n",
            "mean                   NaN       135.000  34.047   33.793   22.113  41.740\n",
            "\n",
            "\n",
            "\n",
            "_________________________________China____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   7.788    7.484    6.410   0.611\n",
            "1                     MAPE        60.000   2.161    2.148    1.557   0.664\n",
            "2                     MAPE        90.000   0.372    0.372    0.428   0.644\n",
            "3                     MAPE       120.000   0.588    0.588    0.413   0.656\n",
            "4                     MAPE       150.000   2.027    2.027    1.323   4.419\n",
            "5                     MAPE       180.000   1.408    1.408    0.120   3.712\n",
            "6                     MAPE       210.000   0.642    0.642    0.348   0.805\n",
            "7                     MAPE       240.000   1.033    1.033    0.226   0.951\n",
            "mean                   NaN       135.000   2.002    1.963    1.353   1.558\n",
            "\n",
            "\n",
            "\n",
            "_________________________________China____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     RMSE        30.000  52.923   51.038   45.819   4.701\n",
            "1                     RMSE        60.000  18.241   18.131   13.896  18.402\n",
            "2                     RMSE        90.000  32.959   32.963   27.840  43.596\n",
            "3                     RMSE       120.000  93.351   93.348   73.414  76.717\n",
            "4                     RMSE       150.000  49.647   49.647   33.975 134.284\n",
            "5                     RMSE       180.000  30.583   30.582    3.036  94.431\n",
            "6                     RMSE       210.000  12.359   12.359    6.976  18.767\n",
            "7                     RMSE       240.000  13.570   13.570    3.681  16.725\n",
            "mean                   NaN       135.000  37.954   37.705   26.080  50.953\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Bahrain____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                      MAE        30.000  57.617   65.350   68.751   86.712\n",
            "1                      MAE        60.000 147.715  147.828  285.059  373.964\n",
            "2                      MAE        90.000 373.323  373.275  618.548  495.755\n",
            "3                      MAE       120.000 174.831  174.779  711.246 2346.602\n",
            "4                      MAE       150.000 201.732  201.712  883.491 1303.190\n",
            "5                      MAE       180.000 151.379  151.380  142.832 1117.310\n",
            "6                      MAE       210.000  93.824   93.825  196.917 2021.445\n",
            "7                      MAE       240.000  11.243   11.242   65.540  402.734\n",
            "mean                   NaN       135.000 151.458  152.424  371.548 1018.464\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Bahrain____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   0.245    0.268    0.288   0.380\n",
            "1                     MAPE        60.000   0.309    0.309    0.621   0.808\n",
            "2                     MAPE        90.000   0.771    0.771    1.277   0.990\n",
            "3                     MAPE       120.000   0.502    0.502    1.982   6.621\n",
            "4                     MAPE       150.000   0.323    0.323    1.737   2.332\n",
            "5                     MAPE       180.000   0.396    0.396    0.361   2.736\n",
            "6                     MAPE       210.000   0.466    0.466    1.023  10.735\n",
            "7                     MAPE       240.000   0.076    0.076    0.445   2.720\n",
            "mean                   NaN       135.000   0.386    0.389    0.967   3.415\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Bahrain____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                     RMSE        30.000  71.248   82.067   83.175  133.319\n",
            "1                     RMSE        60.000 210.096  210.331  304.690  436.069\n",
            "2                     RMSE        90.000 429.397  429.346  697.733  623.969\n",
            "3                     RMSE       120.000 196.813  196.768  854.528 2761.093\n",
            "4                     RMSE       150.000 256.896  256.858  898.546 1472.409\n",
            "5                     RMSE       180.000 184.223  184.223  176.092 1363.073\n",
            "6                     RMSE       210.000 102.455  102.455  222.665 2494.785\n",
            "7                     RMSE       240.000  16.162   16.160   73.836  539.578\n",
            "mean                   NaN       135.000 183.411  184.776  413.908 1228.037\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Algeria____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                      MAE        30.000 168.159  298.981  432.156  713.437\n",
            "1                      MAE        60.000 277.820   74.359 1139.352  282.248\n",
            "2                      MAE        90.000 261.801  259.470  221.257  532.154\n",
            "3                      MAE       120.000  71.501   71.501   68.531 1597.599\n",
            "4                      MAE       150.000 444.745  444.745  632.579 1584.034\n",
            "5                      MAE       180.000 133.724  133.724  570.526  337.796\n",
            "6                      MAE       210.000 355.442  355.442  322.161  398.524\n",
            "7                      MAE       240.000 593.935  593.935  485.374    8.369\n",
            "mean                   NaN       135.000 288.391  279.020  483.992  681.770\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Algeria____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   0.933    1.665    2.405   3.936\n",
            "1                     MAPE        60.000   1.867    0.467    8.285   2.197\n",
            "2                     MAPE        90.000   0.514    0.509    0.435   1.053\n",
            "3                     MAPE       120.000   0.137    0.137    0.132   3.554\n",
            "4                     MAPE       150.000   1.933    1.933    2.505   6.484\n",
            "5                     MAPE       180.000   0.790    0.790    3.385   2.065\n",
            "6                     MAPE       210.000   0.445    0.445    0.563   0.589\n",
            "7                     MAPE       240.000   0.562    0.562    0.459   0.008\n",
            "mean                   NaN       135.000   0.897    0.813    2.271   2.486\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Algeria____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                     RMSE        30.000 217.824  414.304  592.792  948.368\n",
            "1                     RMSE        60.000 417.461  133.808 1411.106  339.468\n",
            "2                     RMSE        90.000 285.119  282.889  240.596  630.231\n",
            "3                     RMSE       120.000  82.790   82.790   79.495 1986.953\n",
            "4                     RMSE       150.000 542.731  542.731  654.538 1802.038\n",
            "5                     RMSE       180.000 145.444  145.444  572.815  404.395\n",
            "6                     RMSE       210.000 441.963  441.963  366.224  443.479\n",
            "7                     RMSE       240.000 593.944  593.944  485.375   11.765\n",
            "mean                   NaN       135.000 340.910  329.734  550.367  820.837\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Denmark____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                      MAE        30.000 1141.786  422.821 1428.455 1183.133\n",
            "1                      MAE        60.000 1137.098 2640.763 5719.618  903.118\n",
            "2                      MAE        90.000   62.853   59.251  122.840  798.431\n",
            "3                      MAE       120.000   23.496   23.533   38.406  340.303\n",
            "4                      MAE       150.000  101.214  101.208  102.901 1147.212\n",
            "5                      MAE       180.000  318.157  318.161  326.387  560.589\n",
            "6                      MAE       210.000  598.578  598.578  680.892 1563.854\n",
            "7                      MAE       240.000  421.731  421.731   74.638 1402.451\n",
            "mean                   NaN       135.000  475.614  573.256 1061.767  987.386\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Denmark____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  13.027    3.947   17.247  12.457\n",
            "1                     MAPE        60.000  29.073   67.145  145.046  22.980\n",
            "2                     MAPE        90.000   2.625    2.482    4.908  30.505\n",
            "3                     MAPE       120.000   0.386    0.387    0.428   4.436\n",
            "4                     MAPE       150.000   0.424    0.424    0.398   5.679\n",
            "5                     MAPE       180.000   0.683    0.683    0.701   1.194\n",
            "6                     MAPE       210.000   0.609    0.609    0.683   1.524\n",
            "7                     MAPE       240.000   0.348    0.348    0.062   1.154\n",
            "mean                   NaN       135.000   5.897    9.503   21.184   9.991\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Denmark____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                     RMSE        30.000 1363.131  615.537 1679.320 1441.577\n",
            "1                     RMSE        60.000 1457.470 3189.904 6709.349 1072.964\n",
            "2                     RMSE        90.000   70.234   67.360  123.138  920.927\n",
            "3                     RMSE       120.000   26.013   26.066   47.293  405.058\n",
            "4                     RMSE       150.000  143.148  143.145  149.476 1642.171\n",
            "5                     RMSE       180.000  323.034  323.038  331.795  692.366\n",
            "6                     RMSE       210.000  606.969  606.969  702.140 2063.538\n",
            "7                     RMSE       240.000  425.622  425.622   92.190 1627.695\n",
            "mean                   NaN       135.000  551.953  674.705 1229.338 1233.287\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Ireland____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg  HAT_Reg   ARF_Reg   PA_Reg\n",
            "0                      MAE        30.000  8782.922 3333.179 23679.306 1054.320\n",
            "1                      MAE        60.000 10530.409 4822.095 28579.865 4477.192\n",
            "2                      MAE        90.000    94.387  104.227   314.708 2019.115\n",
            "3                      MAE       120.000   199.448  198.550    78.181  198.600\n",
            "4                      MAE       150.000    51.461   51.449    52.935  762.061\n",
            "5                      MAE       180.000   458.559  458.592   406.512 2695.312\n",
            "6                      MAE       210.000   228.774  228.771   282.108 2311.882\n",
            "7                      MAE       240.000    59.707   59.707   640.047 2839.875\n",
            "mean                   NaN       135.000  2550.708 1157.071  6754.208 2044.795\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Ireland____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  82.662   29.959  225.126   9.289\n",
            "1                     MAPE        60.000 867.921  368.391 2314.653 261.997\n",
            "2                     MAPE        90.000   6.560    8.005   22.358 153.208\n",
            "3                     MAPE       120.000   3.684    3.667    1.394   4.593\n",
            "4                     MAPE       150.000   0.329    0.328    0.241   4.360\n",
            "5                     MAPE       180.000   0.628    0.628    0.519   4.541\n",
            "6                     MAPE       210.000   0.342    0.342    0.471   5.334\n",
            "7                     MAPE       240.000   0.200    0.200    2.138   9.986\n",
            "mean                   NaN       135.000 120.291   51.440  320.863  56.663\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Ireland____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg  HAT_Reg   ARF_Reg   PA_Reg\n",
            "0                     RMSE        30.000 11645.440 4289.171 31603.260 1663.765\n",
            "1                     RMSE        60.000 14266.139 6345.477 37565.755 5682.064\n",
            "2                     RMSE        90.000   112.795  130.075   353.446 2306.619\n",
            "3                     RMSE       120.000   200.843  199.937    80.108  267.166\n",
            "4                     RMSE       150.000    57.056   57.059    74.553  974.371\n",
            "5                     RMSE       180.000   548.938  548.984   513.064 3010.676\n",
            "6                     RMSE       210.000   299.374  299.370   334.257 3116.739\n",
            "7                     RMSE       240.000    60.549   60.550   665.320 4266.898\n",
            "mean                   NaN       135.000  3398.892 1491.328  8898.720 2661.037\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Malaysia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000 189.997  239.365  146.398 13703.149\n",
            "1                      MAE        60.000 480.262  479.729  355.661  5537.043\n",
            "2                      MAE        90.000  41.835   41.796   38.842  1121.949\n",
            "3                      MAE       120.000  35.945   35.947   29.392  1400.121\n",
            "4                      MAE       150.000  16.788   16.788   18.896   286.299\n",
            "5                      MAE       180.000 337.781  337.780  325.360   304.781\n",
            "6                      MAE       210.000 865.487  865.487  859.993  1895.034\n",
            "7                      MAE       240.000 112.645  112.645  108.604   928.415\n",
            "mean                   NaN       135.000 260.093  266.192  235.393  3147.099\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Malaysia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   4.082    5.183    2.649 289.950\n",
            "1                     MAPE        60.000  22.764   22.646   16.118 181.612\n",
            "2                     MAPE        90.000   5.310    5.307    4.955 149.398\n",
            "3                     MAPE       120.000   2.658    2.658    2.169 101.281\n",
            "4                     MAPE       150.000   1.399    1.399    0.867  16.237\n",
            "5                     MAPE       180.000   0.768    0.768    0.710   0.691\n",
            "6                     MAPE       210.000   0.880    0.880    0.874   1.835\n",
            "7                     MAPE       240.000   0.090    0.090    0.086   0.743\n",
            "mean                   NaN       135.000   4.744    4.866    3.554  92.718\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Malaysia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000 239.198  314.690  192.604 17238.953\n",
            "1                     RMSE        60.000 592.536  589.446  415.421  6053.962\n",
            "2                     RMSE        90.000  44.377   44.345   42.891  1417.648\n",
            "3                     RMSE       120.000  36.536   36.539   30.012  1838.528\n",
            "4                     RMSE       150.000  20.951   20.951   21.079   380.489\n",
            "5                     RMSE       180.000 416.209  416.209  406.251   489.579\n",
            "6                     RMSE       210.000 873.218  873.218  867.331  2495.872\n",
            "7                     RMSE       240.000 126.524  126.524  161.588  1119.813\n",
            "mean                   NaN       135.000 293.694  302.740  267.147  3879.356\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Singapore____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000 1154.561 1209.894 1660.221  2444.525\n",
            "1                      MAE        60.000 1077.237 1070.704  752.244  8256.395\n",
            "2                      MAE        90.000 2611.594 2565.702 3495.387 62558.836\n",
            "3                      MAE       120.000  168.191  168.159  257.107  1499.900\n",
            "4                      MAE       150.000  297.504  297.827  186.671  2402.973\n",
            "5                      MAE       180.000  208.237  207.896  133.158  6102.904\n",
            "6                      MAE       210.000  220.669  220.687   82.540  1397.673\n",
            "7                      MAE       240.000   83.355   83.351    0.878   204.263\n",
            "mean                   NaN       135.000  727.668  728.027  821.026 10608.434\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Singapore____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   1.975    2.066    2.781   4.284\n",
            "1                     MAPE        60.000   3.417    3.410    1.980  28.953\n",
            "2                     MAPE        90.000  12.130   11.920   16.690 295.426\n",
            "3                     MAPE       120.000   1.481    1.482    1.931   9.313\n",
            "4                     MAPE       150.000   6.687    6.694    4.416  50.179\n",
            "5                     MAPE       180.000  20.162   20.130   13.352 595.640\n",
            "6                     MAPE       210.000  31.143   31.146   11.604 194.040\n",
            "7                     MAPE       240.000  12.515   12.514    0.127  29.395\n",
            "mean                   NaN       135.000  11.189   11.170    6.610 150.904\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Singapore____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000 1767.273 1837.397 2306.294  4260.763\n",
            "1                     RMSE        60.000 1553.004 1544.855 1385.231 11966.785\n",
            "2                     RMSE        90.000 3061.164 3007.943 3622.693 65858.081\n",
            "3                     RMSE       120.000  228.664  228.826  317.453  1748.310\n",
            "4                     RMSE       150.000  300.702  301.022  197.947  3035.708\n",
            "5                     RMSE       180.000  209.076  208.738  134.078  7354.271\n",
            "6                     RMSE       210.000  224.273  224.290   82.708  1681.953\n",
            "7                     RMSE       240.000   83.358   83.354    1.124   276.448\n",
            "mean                   NaN       135.000  928.440  929.553 1005.941 12022.790\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Norway____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                      MAE        30.000 569.669  212.431 1718.050 2116.590\n",
            "1                      MAE        60.000 124.773 1270.207 1828.930  769.096\n",
            "2                      MAE        90.000  70.803   70.017   67.010  213.305\n",
            "3                      MAE       120.000  20.782   20.760   16.538  153.853\n",
            "4                      MAE       150.000  39.698   39.670   35.810  230.614\n",
            "5                      MAE       180.000  51.165   51.151   36.891  849.818\n",
            "6                      MAE       210.000 366.960  366.957  325.185  408.026\n",
            "7                      MAE       240.000 360.232  360.232  306.114  379.799\n",
            "mean                   NaN       135.000 200.510  298.928  541.816  640.138\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Norway____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  26.459    7.750   79.673  75.104\n",
            "1                     MAPE        60.000   8.952   90.469  134.007  55.481\n",
            "2                     MAPE        90.000   7.765    7.692    7.399  21.810\n",
            "3                     MAPE       120.000   1.112    1.111    0.424   6.002\n",
            "4                     MAPE       150.000   0.406    0.406    0.367   2.406\n",
            "5                     MAPE       180.000   0.384    0.384    0.271   6.607\n",
            "6                     MAPE       210.000   0.791    0.791    0.684   1.052\n",
            "7                     MAPE       240.000   0.670    0.670    0.565   0.742\n",
            "mean                   NaN       135.000   5.817   13.659   27.924  21.150\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Norway____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                     RMSE        30.000 795.216  326.450 2089.817 2544.687\n",
            "1                     RMSE        60.000 171.111 1560.273 2589.024  865.599\n",
            "2                     RMSE        90.000  74.076   73.490   74.385  261.181\n",
            "3                     RMSE       120.000  23.789   23.756   20.452  184.404\n",
            "4                     RMSE       150.000  47.702   47.669   43.370  328.486\n",
            "5                     RMSE       180.000  56.931   56.928   44.589 1038.120\n",
            "6                     RMSE       210.000 403.129  403.129  363.209  525.226\n",
            "7                     RMSE       240.000 369.704  369.704  318.448  472.123\n",
            "mean                   NaN       135.000 242.707  357.675  692.912  777.478\n",
            "\n",
            "\n",
            "\n",
            "_________________________________South_Korea____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                      MAE        30.000 1543.250 1566.155 1363.069 393.589\n",
            "1                      MAE        60.000    6.219    6.208    2.739 193.790\n",
            "2                      MAE        90.000    2.963    2.962    3.363 150.552\n",
            "3                      MAE       120.000   53.298   53.299   53.476 363.874\n",
            "4                      MAE       150.000  142.001  142.001  137.745 225.707\n",
            "5                      MAE       180.000   12.292   12.292   15.424 604.149\n",
            "6                      MAE       210.000   73.028   73.028   84.766 385.203\n",
            "7                      MAE       240.000  230.822  230.822  246.891 245.711\n",
            "mean                   NaN       135.000  257.984  260.846  238.434 320.322\n",
            "\n",
            "\n",
            "\n",
            "_________________________________South_Korea____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  81.037   87.252   73.126  35.593\n",
            "1                     MAPE        60.000   0.143    0.143    0.064   4.932\n",
            "2                     MAPE        90.000   0.061    0.061    0.066   3.121\n",
            "3                     MAPE       120.000   0.396    0.396    0.434   7.245\n",
            "4                     MAPE       150.000   0.616    0.616    0.592   1.295\n",
            "5                     MAPE       180.000   0.150    0.150    0.190   7.606\n",
            "6                     MAPE       210.000   0.459    0.459    0.535   3.137\n",
            "7                     MAPE       240.000   0.613    0.613    0.653   0.622\n",
            "mean                   NaN       135.000  10.434   11.211    9.458   7.944\n",
            "\n",
            "\n",
            "\n",
            "_________________________________South_Korea____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     RMSE        30.000 2060.155 1977.575 1888.744 534.857\n",
            "1                     RMSE        60.000    7.209    7.152    3.353 272.557\n",
            "2                     RMSE        90.000    3.405    3.408    4.304 195.736\n",
            "3                     RMSE       120.000   92.928   92.927   91.132 395.085\n",
            "4                     RMSE       150.000  164.557  164.557  160.919 294.393\n",
            "5                     RMSE       180.000   14.985   14.985   18.477 717.526\n",
            "6                     RMSE       210.000   91.287   91.287  105.071 508.084\n",
            "7                     RMSE       240.000  235.835  235.835  253.826 334.011\n",
            "mean                   NaN       135.000  333.795  323.466  315.728 406.531\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Australia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                      MAE        30.000 1524.145 1002.001 1638.411 15019.659\n",
            "1                      MAE        60.000 2664.513 2878.774 4659.432 14301.971\n",
            "2                      MAE        90.000  102.780  103.737   98.228   197.470\n",
            "3                      MAE       120.000  261.124  261.296  233.985  1033.261\n",
            "4                      MAE       150.000   28.819   28.822   41.134  4138.083\n",
            "5                      MAE       180.000   88.232   88.223  112.112  4250.368\n",
            "6                      MAE       210.000  122.189  122.188   76.380   608.736\n",
            "7                      MAE       240.000  159.830  159.830    9.741   223.166\n",
            "mean                   NaN       135.000  618.954  580.609  858.678  4971.589\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Australia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                     MAPE        30.000 119.339   72.296  131.874 1167.857\n",
            "1                     MAPE        60.000 251.496  267.048  420.765 1417.003\n",
            "2                     MAPE        90.000   0.515    0.510    0.482    2.067\n",
            "3                     MAPE       120.000   0.662    0.663    0.588    3.341\n",
            "4                     MAPE       150.000   0.506    0.506    0.647   75.148\n",
            "5                     MAPE       180.000   5.006    5.005    5.966  248.214\n",
            "6                     MAPE       210.000  11.659   11.659    6.710   48.424\n",
            "7                     MAPE       240.000  13.303   13.303    0.838   18.646\n",
            "mean                   NaN       135.000  50.311   46.374   70.984  372.587\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Australia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg    PA_Reg\n",
            "0                     RMSE        30.000 2247.844 1254.037 2667.561 22198.971\n",
            "1                     RMSE        60.000 3218.316 3467.963 5498.636 18145.208\n",
            "2                     RMSE        90.000  135.371  136.767  130.211   232.042\n",
            "3                     RMSE       120.000  269.813  269.970  243.909  1474.802\n",
            "4                     RMSE       150.000   34.818   34.821   49.129  4896.659\n",
            "5                     RMSE       180.000   94.457   94.448  125.673  4905.888\n",
            "6                     RMSE       210.000  139.909  139.906   88.296   911.156\n",
            "7                     RMSE       240.000  159.945  159.945   10.676   258.199\n",
            "mean                   NaN       135.000  787.559  694.732 1101.761  6627.866\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Finland____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                      MAE        30.000 184.776  461.238  173.262  523.982\n",
            "1                      MAE        60.000 183.529  196.293  949.790 1148.181\n",
            "2                      MAE        90.000  28.318   28.380   16.166  279.210\n",
            "3                      MAE       120.000  37.113   37.120   22.689  101.924\n",
            "4                      MAE       150.000  13.633   13.633   15.139   61.918\n",
            "5                      MAE       180.000 113.249  113.249   97.290   77.969\n",
            "6                      MAE       210.000 159.300  159.300  160.000  175.359\n",
            "7                      MAE       240.000 215.778  215.778  210.859  212.675\n",
            "mean                   NaN       135.000 116.962  153.124  205.649  322.652\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Finland____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   3.979    9.403    3.363   8.997\n",
            "1                     MAPE        60.000  18.652   19.477   92.961  95.764\n",
            "2                     MAPE        90.000   5.780    5.793    2.841  49.106\n",
            "3                     MAPE       120.000   2.211    2.212    1.329   6.279\n",
            "4                     MAPE       150.000   0.351    0.351    0.276   1.441\n",
            "5                     MAPE       180.000   0.634    0.634    0.525   0.476\n",
            "6                     MAPE       210.000   0.661    0.661    0.665   0.774\n",
            "7                     MAPE       240.000   0.504    0.504    0.492   0.488\n",
            "mean                   NaN       135.000   4.097    4.879   12.807  20.416\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Finland____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                     RMSE        30.000 264.268  574.282  288.270  648.159\n",
            "1                     RMSE        60.000 243.856  242.988 1066.245 1251.071\n",
            "2                     RMSE        90.000  32.489   32.587   18.780  336.034\n",
            "3                     RMSE       120.000  37.588   37.594   23.339  122.133\n",
            "4                     RMSE       150.000  16.871   16.871   21.549   76.178\n",
            "5                     RMSE       180.000 122.468  122.468  108.964   92.601\n",
            "6                     RMSE       210.000 170.232  170.232  170.511  255.692\n",
            "7                     RMSE       240.000 216.336  216.336  212.081  260.714\n",
            "mean                   NaN       135.000 138.014  176.670  238.717  380.323\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Estonia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                      MAE        30.000  62.333   58.591   86.778 125.746\n",
            "1                      MAE        60.000  76.838   97.131  186.480  74.895\n",
            "2                      MAE        90.000  14.179   14.207   13.631  26.068\n",
            "3                      MAE       120.000   2.654    2.643    4.649   7.567\n",
            "4                      MAE       150.000  14.257   14.251   13.672  12.387\n",
            "5                      MAE       180.000  22.942   22.941   21.995  11.050\n",
            "6                      MAE       210.000 161.418  161.418  152.947 113.850\n",
            "7                      MAE       240.000 262.107  262.107  263.245   1.353\n",
            "mean                   NaN       135.000  77.091   79.161   92.925  46.615\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Estonia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  10.083    9.610   14.098  20.236\n",
            "1                     MAPE        60.000  44.543   66.648  126.493  29.134\n",
            "2                     MAPE        90.000   9.941    9.966    9.447  16.446\n",
            "3                     MAPE       120.000   0.396    0.393    0.475   0.933\n",
            "4                     MAPE       150.000   0.513    0.513    0.452   0.423\n",
            "5                     MAPE       180.000   0.533    0.533    0.514   0.254\n",
            "6                     MAPE       210.000   0.832    0.832    0.781   0.499\n",
            "7                     MAPE       240.000   0.779    0.779    0.782   0.004\n",
            "mean                   NaN       135.000   8.453   11.159   19.130   8.491\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Estonia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     RMSE        30.000  79.119   85.660  103.273 183.738\n",
            "1                     RMSE        60.000  94.203  140.906  295.486  87.521\n",
            "2                     RMSE        90.000  14.812   14.825   14.371  32.228\n",
            "3                     RMSE       120.000   3.316    3.309    5.691   8.890\n",
            "4                     RMSE       150.000  16.584   16.582   17.523  15.853\n",
            "5                     RMSE       180.000  24.035   24.034   23.451  13.698\n",
            "6                     RMSE       210.000 183.648  183.648  175.205 152.009\n",
            "7                     RMSE       240.000 262.107  262.107  263.276   1.848\n",
            "mean                   NaN       135.000  84.728   91.384  112.285  61.973\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Iceland____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                      MAE        30.000  94.220  200.274  447.411 203.502\n",
            "1                      MAE        60.000 266.281   27.776   38.383  35.552\n",
            "2                      MAE        90.000  19.496   19.242   15.673   8.036\n",
            "3                      MAE       120.000   2.150    1.903    5.210   3.721\n",
            "4                      MAE       150.000   3.753    3.787    3.976  15.232\n",
            "5                      MAE       180.000  50.087   50.008   41.517  31.665\n",
            "6                      MAE       210.000  20.039   20.000   18.941 116.894\n",
            "7                      MAE       240.000   5.468    5.471    9.705  42.243\n",
            "mean                   NaN       135.000  57.687   41.058   72.602  57.106\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Iceland____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000 378.036  983.583 2233.269 916.529\n",
            "1                     MAPE        60.000 723.765  115.986  151.748 171.410\n",
            "2                     MAPE        90.000  31.051   30.502   25.503  12.837\n",
            "3                     MAPE       120.000   0.773    0.711    0.733   0.848\n",
            "4                     MAPE       150.000   0.309    0.318    0.365   1.937\n",
            "5                     MAPE       180.000   0.829    0.828    0.663   0.537\n",
            "6                     MAPE       210.000   0.462    0.460    0.438   6.301\n",
            "7                     MAPE       240.000   0.401    0.401    0.716   3.329\n",
            "mean                   NaN       135.000 141.953  141.599  301.679 139.216\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Iceland____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     RMSE        30.000 105.516  236.273  554.341 241.745\n",
            "1                     RMSE        60.000 340.048   38.222   51.633  50.103\n",
            "2                     RMSE        90.000  20.505   20.150   17.205   9.221\n",
            "3                     RMSE       120.000   3.012    2.776    5.671   4.651\n",
            "4                     RMSE       150.000   7.272    7.262    7.106  20.847\n",
            "5                     RMSE       180.000  53.152   53.072   45.627  34.971\n",
            "6                     RMSE       210.000  24.552   24.525   23.293 158.736\n",
            "7                     RMSE       240.000   6.788    6.790   11.825  48.851\n",
            "mean                   NaN       135.000  70.106   48.634   89.588  71.141\n",
            "\n",
            "\n",
            "\n",
            "_________________________________San_Marino____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                      MAE        30.000   5.877    9.039    6.189  19.287\n",
            "1                      MAE        60.000   6.888    6.719   11.497  11.162\n",
            "2                      MAE        90.000   3.167    3.176    2.449   6.760\n",
            "3                      MAE       120.000   4.295    4.295    2.919   5.188\n",
            "4                      MAE       150.000   1.661    1.661    0.786   8.216\n",
            "5                      MAE       180.000   1.165    1.165    1.206   6.529\n",
            "6                      MAE       210.000  16.667   16.667   13.890  13.332\n",
            "7                      MAE       240.000  14.664   14.664   15.053  16.516\n",
            "mean                   NaN       135.000   6.798    7.173    6.749  10.874\n",
            "\n",
            "\n",
            "\n",
            "_________________________________San_Marino____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   2.212    3.233    1.628   6.171\n",
            "1                     MAPE        60.000   8.186    8.396   14.444  13.215\n",
            "2                     MAPE        90.000  23.489   23.492   17.740  68.057\n",
            "3                     MAPE       120.000   3.478    3.478    2.208   3.975\n",
            "4                     MAPE       150.000   6.385    6.385    3.125  31.217\n",
            "5                     MAPE       180.000   0.635    0.635    0.545   6.618\n",
            "6                     MAPE       210.000   0.873    0.873    0.694   0.711\n",
            "7                     MAPE       240.000   0.642    0.642    0.657   0.705\n",
            "mean                   NaN       135.000   5.738    5.892    5.130  16.334\n",
            "\n",
            "\n",
            "\n",
            "_________________________________San_Marino____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     RMSE        30.000   6.973   10.226    8.045  25.179\n",
            "1                     RMSE        60.000   8.920    7.724   13.223  13.919\n",
            "2                     RMSE        90.000   3.561    3.563    2.968   8.716\n",
            "3                     RMSE       120.000   4.357    4.357    2.993   5.749\n",
            "4                     RMSE       150.000   1.692    1.692    0.856   8.633\n",
            "5                     RMSE       180.000   1.862    1.862    1.940   7.535\n",
            "6                     RMSE       210.000  17.743   17.743   15.389  15.499\n",
            "7                     RMSE       240.000  14.912   14.912   15.374  18.981\n",
            "mean                   NaN       135.000   7.502    7.760    7.598  13.026\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Vietnam____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                      MAE        30.000   1.775    3.284    3.534  44.456\n",
            "1                      MAE        60.000   4.208    4.891    3.838   9.218\n",
            "2                      MAE        90.000   2.013    2.029    1.555  10.834\n",
            "3                      MAE       120.000  17.379   17.377   17.093  16.005\n",
            "4                      MAE       150.000   3.043    3.041    3.268  29.177\n",
            "5                      MAE       180.000   9.321    9.320    3.369  39.159\n",
            "6                      MAE       210.000   2.202    2.202    2.573  10.017\n",
            "7                      MAE       240.000   3.248    3.248    0.885   2.129\n",
            "mean                   NaN       135.000   5.399    5.674    4.514  20.125\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Vietnam____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   5.423    6.166    4.617  51.006\n",
            "1                     MAPE        60.000   7.254    8.351    7.684  21.695\n",
            "2                     MAPE        90.000   2.241    2.232    1.622  14.816\n",
            "3                     MAPE       120.000   0.817    0.818    0.798   0.768\n",
            "4                     MAPE       150.000   2.629    2.627    2.734  24.502\n",
            "5                     MAPE       180.000   3.826    3.826    1.433  16.521\n",
            "6                     MAPE       210.000   0.486    0.486    0.565   2.235\n",
            "7                     MAPE       240.000   0.769    0.769    0.220   0.537\n",
            "mean                   NaN       135.000   2.931    3.159    2.459  16.510\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Vietnam____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     RMSE        30.000   2.127    4.171    4.343  51.596\n",
            "1                     RMSE        60.000   4.630    5.203    4.222  10.924\n",
            "2                     RMSE        90.000   2.331    2.352    2.047  15.464\n",
            "3                     RMSE       120.000  19.461   19.450   19.277  18.712\n",
            "4                     RMSE       150.000   3.328    3.327    3.506  35.580\n",
            "5                     RMSE       180.000  10.218   10.218    5.052  46.257\n",
            "6                     RMSE       210.000   2.503    2.503    3.617  13.716\n",
            "7                     RMSE       240.000   3.298    3.298    1.052   2.866\n",
            "mean                   NaN       135.000   5.987    6.315    5.389  24.390\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Taiwan____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                      MAE        30.000   3.962    4.680    7.018  58.864\n",
            "1                      MAE        60.000  21.572   22.830   16.683  72.934\n",
            "2                      MAE        90.000   3.391    3.511    2.547   5.791\n",
            "3                      MAE       120.000   0.912    0.930    0.440   3.534\n",
            "4                      MAE       150.000   0.366    0.371    0.314   5.376\n",
            "5                      MAE       180.000   0.573    0.572    0.461   2.927\n",
            "6                      MAE       210.000   1.114    1.121    0.750   2.023\n",
            "7                      MAE       240.000   1.625    1.628    1.394   4.044\n",
            "mean                   NaN       135.000   4.189    4.455    3.701  19.437\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Taiwan____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  16.727   21.741   33.819 413.127\n",
            "1                     MAPE        60.000 106.693  111.164  105.653 344.854\n",
            "2                     MAPE        90.000  13.299   13.817    9.648  29.976\n",
            "3                     MAPE       120.000   1.323    1.348    0.413   4.604\n",
            "4                     MAPE       150.000   1.410    1.441    0.674  10.961\n",
            "5                     MAPE       180.000   0.418    0.417    0.368   3.099\n",
            "6                     MAPE       210.000   0.451    0.455    0.293   0.967\n",
            "7                     MAPE       240.000   0.366    0.367    0.297   1.096\n",
            "mean                   NaN       135.000  17.586   18.844   18.896 101.086\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Taiwan____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     RMSE        30.000   4.774    5.561    8.620  76.043\n",
            "1                     RMSE        60.000  24.630   26.562   17.832  86.724\n",
            "2                     RMSE        90.000   3.423    3.547    2.637   8.393\n",
            "3                     RMSE       120.000   1.036    1.054    0.545   4.087\n",
            "4                     RMSE       150.000   0.447    0.454    0.362   6.647\n",
            "5                     RMSE       180.000   0.756    0.756    0.586   3.670\n",
            "6                     RMSE       210.000   1.264    1.270    0.944   2.538\n",
            "7                     RMSE       240.000   2.242    2.245    2.041   4.825\n",
            "mean                   NaN       135.000   4.821    5.181    4.196  24.116\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T6hi5rPZF4Uf",
        "outputId": "f71b6e0a-5609-4e8f-b505-7a536cb4dc5f"
      },
      "source": [
        "# Get summary table for each country for specified metric\n",
        "summary_table_countrywise_incremental = get_summary_table_countrywise(countrywise_error_score_incremental, ['MAPE'], static_learner=False)\n",
        "\n",
        "# Saving the summary table\n",
        "save_summary_table(summary_table_countrywise_incremental, exp1_summary_path,country=True, static_learner=False,alternate_batch=False,transpose=True)\n",
        "\n",
        "summary_table_countrywise_incremental"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "      <th>EvaluationMeasurement</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Country(MAPE)</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>United_States_of_America</th>\n",
              "      <td>9.682</td>\n",
              "      <td>10.970</td>\n",
              "      <td>16.040</td>\n",
              "      <td>76.280</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>India</th>\n",
              "      <td>1.482</td>\n",
              "      <td>1.501</td>\n",
              "      <td>1.757</td>\n",
              "      <td>7.672</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Brazil</th>\n",
              "      <td>1.868</td>\n",
              "      <td>1.925</td>\n",
              "      <td>1.467</td>\n",
              "      <td>12.292</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Russia</th>\n",
              "      <td>5.897</td>\n",
              "      <td>6.085</td>\n",
              "      <td>7.366</td>\n",
              "      <td>48.007</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>France</th>\n",
              "      <td>16.829</td>\n",
              "      <td>17.485</td>\n",
              "      <td>25.323</td>\n",
              "      <td>156.880</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Spain</th>\n",
              "      <td>66.144</td>\n",
              "      <td>269.839</td>\n",
              "      <td>238.831</td>\n",
              "      <td>110.016</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>United_Kingdom</th>\n",
              "      <td>2.827</td>\n",
              "      <td>7.881</td>\n",
              "      <td>2.196</td>\n",
              "      <td>46.511</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Italy</th>\n",
              "      <td>5.806</td>\n",
              "      <td>17.634</td>\n",
              "      <td>15.317</td>\n",
              "      <td>41.014</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mexico</th>\n",
              "      <td>0.831</td>\n",
              "      <td>0.677</td>\n",
              "      <td>2.508</td>\n",
              "      <td>8.584</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Germany</th>\n",
              "      <td>45.608</td>\n",
              "      <td>43.428</td>\n",
              "      <td>58.415</td>\n",
              "      <td>145.672</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iran</th>\n",
              "      <td>1.606</td>\n",
              "      <td>1.115</td>\n",
              "      <td>2.936</td>\n",
              "      <td>6.883</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Belgium</th>\n",
              "      <td>27.056</td>\n",
              "      <td>15.043</td>\n",
              "      <td>28.304</td>\n",
              "      <td>62.387</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iraq</th>\n",
              "      <td>0.873</td>\n",
              "      <td>0.873</td>\n",
              "      <td>1.033</td>\n",
              "      <td>2.899</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Czechia</th>\n",
              "      <td>2.430</td>\n",
              "      <td>3.334</td>\n",
              "      <td>2.137</td>\n",
              "      <td>6.491</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Netherlands</th>\n",
              "      <td>7.826</td>\n",
              "      <td>9.255</td>\n",
              "      <td>48.193</td>\n",
              "      <td>29.853</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Romania</th>\n",
              "      <td>1.352</td>\n",
              "      <td>0.860</td>\n",
              "      <td>1.674</td>\n",
              "      <td>4.621</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Philippines</th>\n",
              "      <td>1.319</td>\n",
              "      <td>1.345</td>\n",
              "      <td>1.349</td>\n",
              "      <td>5.937</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pakistan</th>\n",
              "      <td>3.100</td>\n",
              "      <td>3.068</td>\n",
              "      <td>4.186</td>\n",
              "      <td>24.957</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Canada</th>\n",
              "      <td>6.440</td>\n",
              "      <td>4.957</td>\n",
              "      <td>1.350</td>\n",
              "      <td>64.980</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Israel</th>\n",
              "      <td>34.137</td>\n",
              "      <td>42.522</td>\n",
              "      <td>28.787</td>\n",
              "      <td>33.389</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Switzerland</th>\n",
              "      <td>38.702</td>\n",
              "      <td>110.517</td>\n",
              "      <td>63.545</td>\n",
              "      <td>53.972</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Austria</th>\n",
              "      <td>20.681</td>\n",
              "      <td>29.459</td>\n",
              "      <td>172.719</td>\n",
              "      <td>20.166</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sweden</th>\n",
              "      <td>1.147</td>\n",
              "      <td>0.941</td>\n",
              "      <td>0.787</td>\n",
              "      <td>13.151</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ecuador</th>\n",
              "      <td>9.692</td>\n",
              "      <td>14.415</td>\n",
              "      <td>17.208</td>\n",
              "      <td>26.866</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>United_Arab_Emirates</th>\n",
              "      <td>1.315</td>\n",
              "      <td>1.256</td>\n",
              "      <td>2.061</td>\n",
              "      <td>23.383</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Japan</th>\n",
              "      <td>6.686</td>\n",
              "      <td>6.288</td>\n",
              "      <td>6.177</td>\n",
              "      <td>27.963</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Kuwait</th>\n",
              "      <td>1.093</td>\n",
              "      <td>1.092</td>\n",
              "      <td>1.252</td>\n",
              "      <td>4.536</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Qatar</th>\n",
              "      <td>1.870</td>\n",
              "      <td>1.823</td>\n",
              "      <td>1.663</td>\n",
              "      <td>17.295</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Georgia</th>\n",
              "      <td>0.754</td>\n",
              "      <td>0.722</td>\n",
              "      <td>1.337</td>\n",
              "      <td>3.384</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lebanon</th>\n",
              "      <td>0.858</td>\n",
              "      <td>0.963</td>\n",
              "      <td>0.914</td>\n",
              "      <td>1.468</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Croatia</th>\n",
              "      <td>24.797</td>\n",
              "      <td>43.133</td>\n",
              "      <td>17.642</td>\n",
              "      <td>52.001</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Oman</th>\n",
              "      <td>1.751</td>\n",
              "      <td>1.752</td>\n",
              "      <td>2.941</td>\n",
              "      <td>6.153</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Egypt</th>\n",
              "      <td>2.281</td>\n",
              "      <td>2.160</td>\n",
              "      <td>4.059</td>\n",
              "      <td>6.959</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Greece</th>\n",
              "      <td>2.685</td>\n",
              "      <td>1.433</td>\n",
              "      <td>4.634</td>\n",
              "      <td>8.210</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>China</th>\n",
              "      <td>2.002</td>\n",
              "      <td>1.963</td>\n",
              "      <td>1.353</td>\n",
              "      <td>1.558</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bahrain</th>\n",
              "      <td>0.386</td>\n",
              "      <td>0.389</td>\n",
              "      <td>0.967</td>\n",
              "      <td>3.415</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Algeria</th>\n",
              "      <td>0.897</td>\n",
              "      <td>0.813</td>\n",
              "      <td>2.271</td>\n",
              "      <td>2.486</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Denmark</th>\n",
              "      <td>5.897</td>\n",
              "      <td>9.503</td>\n",
              "      <td>21.184</td>\n",
              "      <td>9.991</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ireland</th>\n",
              "      <td>120.291</td>\n",
              "      <td>51.440</td>\n",
              "      <td>320.863</td>\n",
              "      <td>56.663</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Malaysia</th>\n",
              "      <td>4.744</td>\n",
              "      <td>4.866</td>\n",
              "      <td>3.554</td>\n",
              "      <td>92.718</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Singapore</th>\n",
              "      <td>11.189</td>\n",
              "      <td>11.170</td>\n",
              "      <td>6.610</td>\n",
              "      <td>150.904</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Norway</th>\n",
              "      <td>5.817</td>\n",
              "      <td>13.659</td>\n",
              "      <td>27.924</td>\n",
              "      <td>21.150</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>South_Korea</th>\n",
              "      <td>10.434</td>\n",
              "      <td>11.211</td>\n",
              "      <td>9.458</td>\n",
              "      <td>7.944</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Australia</th>\n",
              "      <td>50.311</td>\n",
              "      <td>46.374</td>\n",
              "      <td>70.984</td>\n",
              "      <td>372.587</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Finland</th>\n",
              "      <td>4.097</td>\n",
              "      <td>4.879</td>\n",
              "      <td>12.807</td>\n",
              "      <td>20.416</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Estonia</th>\n",
              "      <td>8.453</td>\n",
              "      <td>11.159</td>\n",
              "      <td>19.130</td>\n",
              "      <td>8.491</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iceland</th>\n",
              "      <td>141.953</td>\n",
              "      <td>141.599</td>\n",
              "      <td>301.679</td>\n",
              "      <td>139.216</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>San_Marino</th>\n",
              "      <td>5.738</td>\n",
              "      <td>5.892</td>\n",
              "      <td>5.130</td>\n",
              "      <td>16.334</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Vietnam</th>\n",
              "      <td>2.931</td>\n",
              "      <td>3.159</td>\n",
              "      <td>2.459</td>\n",
              "      <td>16.510</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Taiwan</th>\n",
              "      <td>17.586</td>\n",
              "      <td>18.844</td>\n",
              "      <td>18.896</td>\n",
              "      <td>101.086</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          HT_Reg  HAT_Reg  ...  PA_Reg  EvaluationMeasurement\n",
              "Country(MAPE)                              ...                               \n",
              "United_States_of_America   9.682   10.970  ...  76.280                   MAPE\n",
              "India                      1.482    1.501  ...   7.672                   MAPE\n",
              "Brazil                     1.868    1.925  ...  12.292                   MAPE\n",
              "Russia                     5.897    6.085  ...  48.007                   MAPE\n",
              "France                    16.829   17.485  ... 156.880                   MAPE\n",
              "Spain                     66.144  269.839  ... 110.016                   MAPE\n",
              "United_Kingdom             2.827    7.881  ...  46.511                   MAPE\n",
              "Italy                      5.806   17.634  ...  41.014                   MAPE\n",
              "Mexico                     0.831    0.677  ...   8.584                   MAPE\n",
              "Germany                   45.608   43.428  ... 145.672                   MAPE\n",
              "Iran                       1.606    1.115  ...   6.883                   MAPE\n",
              "Belgium                   27.056   15.043  ...  62.387                   MAPE\n",
              "Iraq                       0.873    0.873  ...   2.899                   MAPE\n",
              "Czechia                    2.430    3.334  ...   6.491                   MAPE\n",
              "Netherlands                7.826    9.255  ...  29.853                   MAPE\n",
              "Romania                    1.352    0.860  ...   4.621                   MAPE\n",
              "Philippines                1.319    1.345  ...   5.937                   MAPE\n",
              "Pakistan                   3.100    3.068  ...  24.957                   MAPE\n",
              "Canada                     6.440    4.957  ...  64.980                   MAPE\n",
              "Israel                    34.137   42.522  ...  33.389                   MAPE\n",
              "Switzerland               38.702  110.517  ...  53.972                   MAPE\n",
              "Austria                   20.681   29.459  ...  20.166                   MAPE\n",
              "Sweden                     1.147    0.941  ...  13.151                   MAPE\n",
              "Ecuador                    9.692   14.415  ...  26.866                   MAPE\n",
              "United_Arab_Emirates       1.315    1.256  ...  23.383                   MAPE\n",
              "Japan                      6.686    6.288  ...  27.963                   MAPE\n",
              "Kuwait                     1.093    1.092  ...   4.536                   MAPE\n",
              "Qatar                      1.870    1.823  ...  17.295                   MAPE\n",
              "Georgia                    0.754    0.722  ...   3.384                   MAPE\n",
              "Lebanon                    0.858    0.963  ...   1.468                   MAPE\n",
              "Croatia                   24.797   43.133  ...  52.001                   MAPE\n",
              "Oman                       1.751    1.752  ...   6.153                   MAPE\n",
              "Egypt                      2.281    2.160  ...   6.959                   MAPE\n",
              "Greece                     2.685    1.433  ...   8.210                   MAPE\n",
              "China                      2.002    1.963  ...   1.558                   MAPE\n",
              "Bahrain                    0.386    0.389  ...   3.415                   MAPE\n",
              "Algeria                    0.897    0.813  ...   2.486                   MAPE\n",
              "Denmark                    5.897    9.503  ...   9.991                   MAPE\n",
              "Ireland                  120.291   51.440  ...  56.663                   MAPE\n",
              "Malaysia                   4.744    4.866  ...  92.718                   MAPE\n",
              "Singapore                 11.189   11.170  ... 150.904                   MAPE\n",
              "Norway                     5.817   13.659  ...  21.150                   MAPE\n",
              "South_Korea               10.434   11.211  ...   7.944                   MAPE\n",
              "Australia                 50.311   46.374  ... 372.587                   MAPE\n",
              "Finland                    4.097    4.879  ...  20.416                   MAPE\n",
              "Estonia                    8.453   11.159  ...   8.491                   MAPE\n",
              "Iceland                  141.953  141.599  ... 139.216                   MAPE\n",
              "San_Marino                 5.738    5.892  ...  16.334                   MAPE\n",
              "Vietnam                    2.931    3.159  ...  16.510                   MAPE\n",
              "Taiwan                    17.586   18.844  ... 101.086                   MAPE\n",
              "\n",
              "[50 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jEmM1GgdAqq"
      },
      "source": [
        "sum_inc_countrywise_mean = get_sum_table_combined_mean(countrywise_error_score_incremental,results_runtime_incremental)\n",
        "save_combined_summary_table(sum_inc_countrywise_mean, exp1_summary_path, static_learner=False, transpose=True) \n",
        "sum_inc_countrywise_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEBzePUFh8FO"
      },
      "source": [
        "## Static Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtk8rnQGTPgy"
      },
      "source": [
        "def scikit_learn(df, training_days, country):  #Added country now\n",
        "    frames = []\n",
        "    model_predictions = {\n",
        "        'RandomForest': [],\n",
        "        'GradientBoosting': [],\n",
        "        'LinearSVR': [],\n",
        "        'DecisionTree': [],\n",
        "        'BayesianRidge': [],\n",
        "        'LSTM': [] \n",
        "      # 'MLPRegressor': [],\n",
        "      # 'LinearRegression': []\n",
        "    }\n",
        "    total_execution_time = []\n",
        "\n",
        "    # params (others like epoch and batch size are also hardcoded in train_test_lstm())\n",
        "    layers = [50, 30, 20, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\n",
        "    activations = ['tanh', 'tanh', 'relu']\n",
        "    epochs = 200  # Previously: 500\n",
        "    patience = 20\n",
        "    batch_size_lstm = 10\n",
        "    united_dataframe = []  # Added Now\n",
        "\n",
        "    for day in training_days:\n",
        "        print(f\"~~~~~~~~~~~~~~~~~~~~*Pretraining Day: {day}~~~~~~~~~~~~~~~~~~~~\")\n",
        "        testing_samples_size = 30  # Added Now\n",
        "        cur_exec_time = [day]  # Keeping running time for each pre-train set\n",
        "        target_dates = df.iloc[day: day + testing_samples_size, 0]  # Added Now\n",
        "        train = df.iloc[:day, :]\n",
        "        test = df.iloc[day:day + testing_samples_size, :]  # Testing on set one month ahead only, hence day+30.\n",
        "\n",
        "        # training and test sets for all models except LSTM\n",
        "        X_train, y_train = train.iloc[:, 4:-1], train.iloc[:, -1]\n",
        "        X_test, y_test = test.iloc[:, 4:-1], test.iloc[:, -1]\n",
        "\n",
        "        # Seperating validation set from train set\n",
        "        train_df, val_df = get_validation_set(train, batch_size=10)\n",
        "\n",
        "        # Splitting test and validation into dependent and independent sets\n",
        "        X_train_batch, y_train_batch = train_df.iloc[:, 4:-1], train_df.iloc[:, -1]  # Consist only odd batches\n",
        "        X_val_batch, y_val_batch = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\n",
        "\n",
        "        # Normalizing dataset\n",
        "        X_train_lstm_norm, X_test_lstm_norm, X_val_lstm_norm = normalize_dataset(X_train_batch, X_test, X_val_batch)\n",
        "\n",
        "        # Reshaping the dataframes\n",
        "        X_train_lstm, X_val_lstm, X_test_lstm = reshape_dataframe(X_train_lstm_norm, X_val_lstm_norm, X_test_lstm_norm)\n",
        "\n",
        "        rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "        model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        gb_reg = GradientBoostingRegressor(random_state=0)\n",
        "        model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        lsv_reg = LinearSVR(random_state=0, tol=1e-5)\n",
        "        model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        dt_reg = DecisionTreeRegressor(random_state=0)\n",
        "        model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        br_reg = BayesianRidge()\n",
        "        model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\n",
        "        model_predictions['LSTM'], exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train_batch, X_val_lstm, y_val_batch, X_test_lstm, patience, epochs, batch_size_lstm)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        united_dataframe.append(unit_static_df(country, target_dates, y_test, day, model_predictions))  # Added now\n",
        "\n",
        "        mdl_evaluation_df = get_scores(y_test, model_predictions, day)\n",
        "        total_execution_time.append(cur_exec_time)\n",
        "        frames.append(mdl_evaluation_df)\n",
        "\n",
        "    evaluation_score_df = pd.concat(frames, ignore_index=True)\n",
        "    united_dataframe = pd.concat(united_dataframe, ignore_index=True)  # Added Now\n",
        "    running_time_df = get_running_time_per_model_static_learner(model_predictions, total_execution_time)\n",
        "    return evaluation_score_df, running_time_df, united_dataframe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCrlBMu3TZMm",
        "outputId": "20d3c94b-f4ac-499f-f726-114af9b7edd5"
      },
      "source": [
        "results_static = []\n",
        "results_runtime_static = []\n",
        "max_of_pretrain_per_country = []\n",
        "max_cases_per_country = []\n",
        "\n",
        "pretrain_days = [30,60]  # TODO: Remove this line \n",
        "\n",
        "for country in countries[0:2]:  #Remove list slicing\n",
        "    # Read country wise csv file\n",
        "    df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\n",
        "\n",
        "    print(f\"*******************Processing {country}************************\")\n",
        "    # Evaluation scores and running time of each algorithm over different pre-training days\n",
        "    evaluation_scores_df, running_time_df, united_dataframe = scikit_learn(df_country, pretrain_days, country)  # Returning united_dataframe also\n",
        "\n",
        "    save_united_df(united_dataframe, exp1_static_united_df_path, country=country)\n",
        "\n",
        "    # Append result of each pretrain size in results\n",
        "    results_static.append(evaluation_scores_df)\n",
        "\n",
        "    # Appending every country runtime\n",
        "    # results_runtime_static.append(running_time_df)\n",
        "\n",
        "    # Saving runtime for each country\n",
        "    save_runtime(running_time_df, path=exp1_runtime_path, country = country, static_learner=True)\n",
        "\n",
        "    # Calculating max cases per country based on pre-train size\n",
        "    max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days, df_country))\n",
        "\n",
        "    # Maximum case of each country\n",
        "    max_cases_per_country.append(df_country['cases'].max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*******************Processing United_States_of_America************************\n",
            "~~~~~~~~~~~~~~~~~~~~*Pretraining Day: 30~~~~~~~~~~~~~~~~~~~~\n",
            "Train on 20 samples, validate on 10 samples\n",
            "Epoch 1/200\n",
            "20/20 [==============================] - 2s 108ms/step - loss: 24611.8682 - mse: 649051904.0000 - mae: 24611.8691 - val_loss: 29278.5430 - val_mse: 857781504.0000 - val_mae: 29278.5430\n",
            "Epoch 2/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24611.8418 - mse: 649050624.0000 - mae: 24611.8418 - val_loss: 29278.5254 - val_mse: 857780352.0000 - val_mae: 29278.5254\n",
            "Epoch 3/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24611.8223 - mse: 649049600.0000 - mae: 24611.8223 - val_loss: 29278.5039 - val_mse: 857779200.0000 - val_mae: 29278.5039\n",
            "Epoch 4/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24611.8057 - mse: 649048768.0000 - mae: 24611.8066 - val_loss: 29278.4805 - val_mse: 857778048.0000 - val_mae: 29278.4805\n",
            "Epoch 5/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24611.7803 - mse: 649047552.0000 - mae: 24611.7793 - val_loss: 29278.4570 - val_mse: 857776512.0000 - val_mae: 29278.4570\n",
            "Epoch 6/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24611.7451 - mse: 649045888.0000 - mae: 24611.7461 - val_loss: 29278.4180 - val_mse: 857774208.0000 - val_mae: 29278.4180\n",
            "Epoch 7/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24611.6934 - mse: 649043712.0000 - mae: 24611.6934 - val_loss: 29278.3477 - val_mse: 857770112.0000 - val_mae: 29278.3477\n",
            "Epoch 8/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24611.5566 - mse: 649037632.0000 - mae: 24611.5566 - val_loss: 29278.1934 - val_mse: 857760832.0000 - val_mae: 29278.1934\n",
            "Epoch 9/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24611.3125 - mse: 649027200.0000 - mae: 24611.3125 - val_loss: 29277.7930 - val_mse: 857737152.0000 - val_mae: 29277.7930\n",
            "Epoch 10/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24610.8760 - mse: 649011840.0000 - mae: 24610.8750 - val_loss: 29276.8750 - val_mse: 857682560.0000 - val_mae: 29276.8750\n",
            "Epoch 11/200\n",
            "20/20 [==============================] - 0s 997us/step - loss: 24609.4814 - mse: 648951040.0000 - mae: 24609.4805 - val_loss: 29274.9629 - val_mse: 857569280.0000 - val_mae: 29274.9629\n",
            "Epoch 12/200\n",
            "20/20 [==============================] - 0s 972us/step - loss: 24606.4531 - mse: 648831872.0000 - mae: 24606.4531 - val_loss: 29271.8320 - val_mse: 857383808.0000 - val_mae: 29271.8320\n",
            "Epoch 13/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24602.4990 - mse: 648656768.0000 - mae: 24602.5000 - val_loss: 29268.1680 - val_mse: 857167104.0000 - val_mae: 29268.1680\n",
            "Epoch 14/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24596.2812 - mse: 648355840.0000 - mae: 24596.2812 - val_loss: 29264.1758 - val_mse: 856931264.0000 - val_mae: 29264.1758\n",
            "Epoch 15/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24593.7627 - mse: 648281088.0000 - mae: 24593.7617 - val_loss: 29260.3750 - val_mse: 856706944.0000 - val_mae: 29260.3750\n",
            "Epoch 16/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24585.5654 - mse: 647919936.0000 - mae: 24585.5664 - val_loss: 29255.9375 - val_mse: 856444992.0000 - val_mae: 29255.9375\n",
            "Epoch 17/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24575.6982 - mse: 647448768.0000 - mae: 24575.6992 - val_loss: 29251.2344 - val_mse: 856167552.0000 - val_mae: 29251.2344\n",
            "Epoch 18/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24568.5625 - mse: 647115136.0000 - mae: 24568.5625 - val_loss: 29246.2090 - val_mse: 855871744.0000 - val_mae: 29246.2090\n",
            "Epoch 19/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24567.1230 - mse: 647084608.0000 - mae: 24567.1211 - val_loss: 29241.3965 - val_mse: 855588672.0000 - val_mae: 29241.3965\n",
            "Epoch 20/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24565.4062 - mse: 646979008.0000 - mae: 24565.4062 - val_loss: 29236.7910 - val_mse: 855317504.0000 - val_mae: 29236.7910\n",
            "Epoch 21/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24547.4658 - mse: 646149184.0000 - mae: 24547.4648 - val_loss: 29230.9570 - val_mse: 854974400.0000 - val_mae: 29230.9570\n",
            "Epoch 22/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24549.1865 - mse: 646119296.0000 - mae: 24549.1875 - val_loss: 29225.7148 - val_mse: 854666368.0000 - val_mae: 29225.7148\n",
            "Epoch 23/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24530.5840 - mse: 645346816.0000 - mae: 24530.5840 - val_loss: 29219.5059 - val_mse: 854301568.0000 - val_mae: 29219.5059\n",
            "Epoch 24/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24511.9531 - mse: 644299904.0000 - mae: 24511.9531 - val_loss: 29212.3652 - val_mse: 853882240.0000 - val_mae: 29212.3652\n",
            "Epoch 25/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24505.7295 - mse: 644057920.0000 - mae: 24505.7305 - val_loss: 29205.2695 - val_mse: 853466112.0000 - val_mae: 29205.2695\n",
            "Epoch 26/200\n",
            "20/20 [==============================] - 0s 886us/step - loss: 24492.8799 - mse: 643608448.0000 - mae: 24492.8809 - val_loss: 29198.1035 - val_mse: 853045632.0000 - val_mae: 29198.1035\n",
            "Epoch 27/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24493.8721 - mse: 643531648.0000 - mae: 24493.8711 - val_loss: 29191.1660 - val_mse: 852639040.0000 - val_mae: 29191.1660\n",
            "Epoch 28/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24482.8125 - mse: 642962240.0000 - mae: 24482.8125 - val_loss: 29183.8594 - val_mse: 852210944.0000 - val_mae: 29183.8594\n",
            "Epoch 29/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24455.6875 - mse: 641589440.0000 - mae: 24455.6875 - val_loss: 29175.3945 - val_mse: 851715392.0000 - val_mae: 29175.3945\n",
            "Epoch 30/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 24459.3857 - mse: 641531328.0000 - mae: 24459.3867 - val_loss: 29167.6094 - val_mse: 851259712.0000 - val_mae: 29167.6094\n",
            "Epoch 31/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 24457.0059 - mse: 641646272.0000 - mae: 24457.0059 - val_loss: 29160.1211 - val_mse: 850821312.0000 - val_mae: 29160.1211\n",
            "Epoch 32/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24441.9727 - mse: 640725632.0000 - mae: 24441.9727 - val_loss: 29152.1562 - val_mse: 850355392.0000 - val_mae: 29152.1562\n",
            "Epoch 33/200\n",
            "20/20 [==============================] - 0s 911us/step - loss: 24434.2217 - mse: 640912256.0000 - mae: 24434.2227 - val_loss: 29143.9336 - val_mse: 849874688.0000 - val_mae: 29143.9336\n",
            "Epoch 34/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24412.1514 - mse: 639351936.0000 - mae: 24412.1523 - val_loss: 29135.3125 - val_mse: 849370816.0000 - val_mae: 29135.3125\n",
            "Epoch 35/200\n",
            "20/20 [==============================] - 0s 934us/step - loss: 24398.1875 - mse: 638992064.0000 - mae: 24398.1875 - val_loss: 29126.4062 - val_mse: 848850496.0000 - val_mae: 29126.4062\n",
            "Epoch 36/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24378.4248 - mse: 637598784.0000 - mae: 24378.4258 - val_loss: 29116.6445 - val_mse: 848280640.0000 - val_mae: 29116.6445\n",
            "Epoch 37/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 24384.4297 - mse: 638213120.0000 - mae: 24384.4297 - val_loss: 29107.5195 - val_mse: 847748224.0000 - val_mae: 29107.5195\n",
            "Epoch 38/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24358.1162 - mse: 636788352.0000 - mae: 24358.1152 - val_loss: 29097.6191 - val_mse: 847171008.0000 - val_mae: 29097.6191\n",
            "Epoch 39/200\n",
            "20/20 [==============================] - 0s 993us/step - loss: 24360.8320 - mse: 637090688.0000 - mae: 24360.8320 - val_loss: 29088.3555 - val_mse: 846630784.0000 - val_mae: 29088.3555\n",
            "Epoch 40/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24324.0332 - mse: 635508224.0000 - mae: 24324.0332 - val_loss: 29077.7930 - val_mse: 846015616.0000 - val_mae: 29077.7930\n",
            "Epoch 41/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24318.9482 - mse: 635097536.0000 - mae: 24318.9473 - val_loss: 29067.4785 - val_mse: 845415040.0000 - val_mae: 29067.4785\n",
            "Epoch 42/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24291.8369 - mse: 633816576.0000 - mae: 24291.8379 - val_loss: 29056.4648 - val_mse: 844773760.0000 - val_mae: 29056.4648\n",
            "Epoch 43/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24324.4277 - mse: 635275648.0000 - mae: 24324.4277 - val_loss: 29046.6621 - val_mse: 844203392.0000 - val_mae: 29046.6621\n",
            "Epoch 44/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24261.8721 - mse: 632601600.0000 - mae: 24261.8711 - val_loss: 29035.0156 - val_mse: 843526464.0000 - val_mae: 29035.0156\n",
            "Epoch 45/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24310.2246 - mse: 634615040.0000 - mae: 24310.2246 - val_loss: 29025.3965 - val_mse: 842967168.0000 - val_mae: 29025.3965\n",
            "Epoch 46/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24260.1387 - mse: 632304320.0000 - mae: 24260.1367 - val_loss: 29014.2500 - val_mse: 842319744.0000 - val_mae: 29014.2500\n",
            "Epoch 47/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24173.6924 - mse: 627899008.0000 - mae: 24173.6934 - val_loss: 29000.8555 - val_mse: 841542272.0000 - val_mae: 29000.8555\n",
            "Epoch 48/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24196.5205 - mse: 629485248.0000 - mae: 24196.5215 - val_loss: 28988.7539 - val_mse: 840839808.0000 - val_mae: 28988.7539\n",
            "Epoch 49/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24240.4629 - mse: 631437184.0000 - mae: 24240.4629 - val_loss: 28977.8320 - val_mse: 840206400.0000 - val_mae: 28977.8320\n",
            "Epoch 50/200\n",
            "20/20 [==============================] - 0s 997us/step - loss: 24139.2129 - mse: 626920512.0000 - mae: 24139.2129 - val_loss: 28964.4375 - val_mse: 839429824.0000 - val_mae: 28964.4375\n",
            "Epoch 51/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24196.2061 - mse: 629032960.0000 - mae: 24196.2070 - val_loss: 28952.6133 - val_mse: 838744832.0000 - val_mae: 28952.6133\n",
            "Epoch 52/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24134.0078 - mse: 626027648.0000 - mae: 24134.0078 - val_loss: 28939.0977 - val_mse: 837962112.0000 - val_mae: 28939.0977\n",
            "Epoch 53/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24094.3779 - mse: 624144320.0000 - mae: 24094.3789 - val_loss: 28925.0625 - val_mse: 837149824.0000 - val_mae: 28925.0625\n",
            "Epoch 54/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24101.3457 - mse: 625028480.0000 - mae: 24101.3457 - val_loss: 28911.6250 - val_mse: 836372608.0000 - val_mae: 28911.6250\n",
            "Epoch 55/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24112.8447 - mse: 625110080.0000 - mae: 24112.8438 - val_loss: 28898.7500 - val_mse: 835628032.0000 - val_mae: 28898.7500\n",
            "Epoch 56/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24045.4873 - mse: 621337920.0000 - mae: 24045.4883 - val_loss: 28884.2500 - val_mse: 834790080.0000 - val_mae: 28884.2500\n",
            "Epoch 57/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24020.5938 - mse: 620645760.0000 - mae: 24020.5938 - val_loss: 28869.3320 - val_mse: 833929024.0000 - val_mae: 28869.3320\n",
            "Epoch 58/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24085.2559 - mse: 623318656.0000 - mae: 24085.2559 - val_loss: 28856.0996 - val_mse: 833165504.0000 - val_mae: 28856.0996\n",
            "Epoch 59/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 23983.7949 - mse: 619184832.0000 - mae: 23983.7949 - val_loss: 28841.0195 - val_mse: 832295552.0000 - val_mae: 28841.0195\n",
            "Epoch 60/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24015.9883 - mse: 620257152.0000 - mae: 24015.9883 - val_loss: 28826.5996 - val_mse: 831464320.0000 - val_mae: 28826.5996\n",
            "Epoch 61/200\n",
            "20/20 [==============================] - 0s 941us/step - loss: 23976.8281 - mse: 618443072.0000 - mae: 23976.8281 - val_loss: 28811.3809 - val_mse: 830587968.0000 - val_mae: 28811.3809\n",
            "Epoch 62/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23970.6279 - mse: 618562048.0000 - mae: 23970.6289 - val_loss: 28796.8691 - val_mse: 829752384.0000 - val_mae: 28796.8691\n",
            "Epoch 63/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23994.3438 - mse: 620014912.0000 - mae: 23994.3438 - val_loss: 28783.0371 - val_mse: 828956544.0000 - val_mae: 28783.0371\n",
            "Epoch 64/200\n",
            "20/20 [==============================] - 0s 979us/step - loss: 23930.6494 - mse: 616918016.0000 - mae: 23930.6504 - val_loss: 28768.0840 - val_mse: 828096896.0000 - val_mae: 28768.0840\n",
            "Epoch 65/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23870.4189 - mse: 613516416.0000 - mae: 23870.4180 - val_loss: 28752.0098 - val_mse: 827173376.0000 - val_mae: 28752.0098\n",
            "Epoch 66/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 23856.3818 - mse: 613640896.0000 - mae: 23856.3809 - val_loss: 28735.5684 - val_mse: 826229568.0000 - val_mae: 28735.5684\n",
            "Epoch 67/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23948.5020 - mse: 617152384.0000 - mae: 23948.5039 - val_loss: 28720.8867 - val_mse: 825386880.0000 - val_mae: 28720.8867\n",
            "Epoch 68/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23909.6143 - mse: 615382208.0000 - mae: 23909.6133 - val_loss: 28705.7285 - val_mse: 824517248.0000 - val_mae: 28705.7285\n",
            "Epoch 69/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 23797.5879 - mse: 610053120.0000 - mae: 23797.5879 - val_loss: 28688.4336 - val_mse: 823526464.0000 - val_mae: 28688.4336\n",
            "Epoch 70/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23865.3223 - mse: 614384512.0000 - mae: 23865.3223 - val_loss: 28672.9121 - val_mse: 822636992.0000 - val_mae: 28672.9121\n",
            "Epoch 71/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23764.9473 - mse: 607915392.0000 - mae: 23764.9473 - val_loss: 28655.3438 - val_mse: 821631680.0000 - val_mae: 28655.3438\n",
            "Epoch 72/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23766.2275 - mse: 607697024.0000 - mae: 23766.2285 - val_loss: 28637.2383 - val_mse: 820595520.0000 - val_mae: 28637.2383\n",
            "Epoch 73/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 23731.3262 - mse: 608543040.0000 - mae: 23731.3262 - val_loss: 28619.1973 - val_mse: 819564416.0000 - val_mae: 28619.1973\n",
            "Epoch 74/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23724.6787 - mse: 606093120.0000 - mae: 23724.6777 - val_loss: 28600.7402 - val_mse: 818510144.0000 - val_mae: 28600.7402\n",
            "Epoch 75/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23653.3760 - mse: 602518208.0000 - mae: 23653.3750 - val_loss: 28580.6289 - val_mse: 817362560.0000 - val_mae: 28580.6289\n",
            "Epoch 76/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23584.9385 - mse: 599918272.0000 - mae: 23584.9395 - val_loss: 28559.9434 - val_mse: 816182848.0000 - val_mae: 28559.9434\n",
            "Epoch 77/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23571.3828 - mse: 599380032.0000 - mae: 23571.3828 - val_loss: 28539.4336 - val_mse: 815014144.0000 - val_mae: 28539.4336\n",
            "Epoch 78/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23594.7871 - mse: 600919168.0000 - mae: 23594.7871 - val_loss: 28519.4688 - val_mse: 813877632.0000 - val_mae: 28519.4688\n",
            "Epoch 79/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23647.9727 - mse: 604490880.0000 - mae: 23647.9727 - val_loss: 28500.9004 - val_mse: 812821632.0000 - val_mae: 28500.9004\n",
            "Epoch 80/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23666.9141 - mse: 604538688.0000 - mae: 23666.9160 - val_loss: 28482.3809 - val_mse: 811768704.0000 - val_mae: 28482.3809\n",
            "Epoch 81/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 23587.8203 - mse: 600554688.0000 - mae: 23587.8203 - val_loss: 28462.9629 - val_mse: 810665600.0000 - val_mae: 28462.9629\n",
            "Epoch 82/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23564.2422 - mse: 599647104.0000 - mae: 23564.2402 - val_loss: 28443.0742 - val_mse: 809536512.0000 - val_mae: 28443.0742\n",
            "Epoch 83/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23594.4971 - mse: 600317120.0000 - mae: 23594.4961 - val_loss: 28423.8809 - val_mse: 808448128.0000 - val_mae: 28423.8809\n",
            "Epoch 84/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23414.9209 - mse: 592000000.0000 - mae: 23414.9219 - val_loss: 28401.5059 - val_mse: 807180224.0000 - val_mae: 28401.5059\n",
            "Epoch 85/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23403.1523 - mse: 590847104.0000 - mae: 23403.1523 - val_loss: 28379.1406 - val_mse: 805913920.0000 - val_mae: 28379.1406\n",
            "Epoch 86/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23495.7197 - mse: 596298368.0000 - mae: 23495.7207 - val_loss: 28358.7402 - val_mse: 804759872.0000 - val_mae: 28358.7402\n",
            "Epoch 87/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23358.9717 - mse: 589676928.0000 - mae: 23358.9727 - val_loss: 28335.1875 - val_mse: 803428224.0000 - val_mae: 28335.1875\n",
            "Epoch 88/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23299.4434 - mse: 586008896.0000 - mae: 23299.4434 - val_loss: 28311.2559 - val_mse: 802076480.0000 - val_mae: 28311.2559\n",
            "Epoch 89/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23261.3203 - mse: 585511296.0000 - mae: 23261.3184 - val_loss: 28287.4316 - val_mse: 800732288.0000 - val_mae: 28287.4316\n",
            "Epoch 90/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23334.7354 - mse: 587632000.0000 - mae: 23334.7344 - val_loss: 28264.4570 - val_mse: 799436800.0000 - val_mae: 28264.4570\n",
            "Epoch 91/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23296.8809 - mse: 586060864.0000 - mae: 23296.8809 - val_loss: 28241.8535 - val_mse: 798164288.0000 - val_mae: 28241.8535\n",
            "Epoch 92/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23087.5928 - mse: 577898688.0000 - mae: 23087.5918 - val_loss: 28215.7090 - val_mse: 796693696.0000 - val_mae: 28215.7090\n",
            "Epoch 93/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23248.8154 - mse: 583289216.0000 - mae: 23248.8164 - val_loss: 28192.1602 - val_mse: 795369984.0000 - val_mae: 28192.1602\n",
            "Epoch 94/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23188.3418 - mse: 582060416.0000 - mae: 23188.3398 - val_loss: 28167.7383 - val_mse: 793998464.0000 - val_mae: 28167.7383\n",
            "Epoch 95/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23231.4238 - mse: 582266112.0000 - mae: 23231.4258 - val_loss: 28144.3438 - val_mse: 792686080.0000 - val_mae: 28144.3438\n",
            "Epoch 96/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23056.0342 - mse: 575366528.0000 - mae: 23056.0352 - val_loss: 28118.2969 - val_mse: 791225920.0000 - val_mae: 28118.2969\n",
            "Epoch 97/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23146.1807 - mse: 578164608.0000 - mae: 23146.1816 - val_loss: 28093.8555 - val_mse: 789857792.0000 - val_mae: 28093.8555\n",
            "Epoch 98/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 23147.3213 - mse: 577949504.0000 - mae: 23147.3223 - val_loss: 28069.9316 - val_mse: 788519424.0000 - val_mae: 28069.9316\n",
            "Epoch 99/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22971.4121 - mse: 571551552.0000 - mae: 22971.4121 - val_loss: 28043.5527 - val_mse: 787045120.0000 - val_mae: 28043.5527\n",
            "Epoch 100/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22932.8750 - mse: 570208640.0000 - mae: 22932.8750 - val_loss: 28017.0430 - val_mse: 785565696.0000 - val_mae: 28017.0430\n",
            "Epoch 101/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23000.4033 - mse: 575249152.0000 - mae: 23000.4023 - val_loss: 27991.8066 - val_mse: 784157824.0000 - val_mae: 27991.8066\n",
            "Epoch 102/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22952.9824 - mse: 569959104.0000 - mae: 22952.9824 - val_loss: 27965.2441 - val_mse: 782677568.0000 - val_mae: 27965.2441\n",
            "Epoch 103/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22920.1592 - mse: 570091840.0000 - mae: 22920.1602 - val_loss: 27939.0625 - val_mse: 781220608.0000 - val_mae: 27939.0625\n",
            "Epoch 104/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22850.4697 - mse: 564531968.0000 - mae: 22850.4688 - val_loss: 27911.7812 - val_mse: 779703616.0000 - val_mae: 27911.7812\n",
            "Epoch 105/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22797.6680 - mse: 564439936.0000 - mae: 22797.6680 - val_loss: 27883.8086 - val_mse: 778149696.0000 - val_mae: 27883.8086\n",
            "Epoch 106/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22727.8145 - mse: 562387840.0000 - mae: 22727.8145 - val_loss: 27854.5000 - val_mse: 776523136.0000 - val_mae: 27854.5000\n",
            "Epoch 107/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22665.3652 - mse: 557285568.0000 - mae: 22665.3652 - val_loss: 27825.3320 - val_mse: 774906816.0000 - val_mae: 27825.3320\n",
            "Epoch 108/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 22766.3311 - mse: 563703296.0000 - mae: 22766.3320 - val_loss: 27798.1348 - val_mse: 773402624.0000 - val_mae: 27798.1348\n",
            "Epoch 109/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22618.5479 - mse: 552572928.0000 - mae: 22618.5469 - val_loss: 27768.4883 - val_mse: 771763072.0000 - val_mae: 27768.4883\n",
            "Epoch 110/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22774.2812 - mse: 563302528.0000 - mae: 22774.2812 - val_loss: 27741.0254 - val_mse: 770246272.0000 - val_mae: 27741.0254\n",
            "Epoch 111/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22542.3516 - mse: 551624064.0000 - mae: 22542.3516 - val_loss: 27710.7656 - val_mse: 768577344.0000 - val_mae: 27710.7656\n",
            "Epoch 112/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 22635.0859 - mse: 557307776.0000 - mae: 22635.0859 - val_loss: 27681.7715 - val_mse: 766979264.0000 - val_mae: 27681.7715\n",
            "Epoch 113/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 22661.8213 - mse: 559288640.0000 - mae: 22661.8223 - val_loss: 27653.7383 - val_mse: 765435648.0000 - val_mae: 27653.7383\n",
            "Epoch 114/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22468.9814 - mse: 549725376.0000 - mae: 22468.9805 - val_loss: 27623.6719 - val_mse: 763782528.0000 - val_mae: 27623.6719\n",
            "Epoch 115/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22480.6113 - mse: 547110208.0000 - mae: 22480.6113 - val_loss: 27592.9023 - val_mse: 762092480.0000 - val_mae: 27592.9023\n",
            "Epoch 116/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22560.2148 - mse: 556653376.0000 - mae: 22560.2148 - val_loss: 27563.8086 - val_mse: 760495744.0000 - val_mae: 27563.8086\n",
            "Epoch 117/200\n",
            "20/20 [==============================] - 0s 982us/step - loss: 22230.9922 - mse: 539042944.0000 - mae: 22230.9922 - val_loss: 27530.0664 - val_mse: 758646336.0000 - val_mae: 27530.0664\n",
            "Epoch 118/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22501.6475 - mse: 549276992.0000 - mae: 22501.6465 - val_loss: 27500.9160 - val_mse: 757051776.0000 - val_mae: 27500.9160\n",
            "Epoch 119/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 22283.7178 - mse: 539937280.0000 - mae: 22283.7168 - val_loss: 27469.0195 - val_mse: 755308288.0000 - val_mae: 27469.0195\n",
            "Epoch 120/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22520.4287 - mse: 550478016.0000 - mae: 22520.4277 - val_loss: 27440.3320 - val_mse: 753742592.0000 - val_mae: 27440.3320\n",
            "Epoch 121/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22363.9014 - mse: 546987328.0000 - mae: 22363.9004 - val_loss: 27409.9648 - val_mse: 752086400.0000 - val_mae: 27409.9648\n",
            "Epoch 122/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22247.1758 - mse: 542205248.0000 - mae: 22247.1758 - val_loss: 27378.1562 - val_mse: 750353792.0000 - val_mae: 27378.1562\n",
            "Epoch 123/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22425.1035 - mse: 547375424.0000 - mae: 22425.1035 - val_loss: 27349.3809 - val_mse: 748789632.0000 - val_mae: 27349.3809\n",
            "Epoch 124/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22158.2207 - mse: 532636160.0000 - mae: 22158.2207 - val_loss: 27316.6309 - val_mse: 747009728.0000 - val_mae: 27316.6309\n",
            "Epoch 125/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22280.4980 - mse: 539964928.0000 - mae: 22280.4980 - val_loss: 27285.5371 - val_mse: 745321664.0000 - val_mae: 27285.5371\n",
            "Epoch 126/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21914.2520 - mse: 525650112.0000 - mae: 21914.2539 - val_loss: 27250.2188 - val_mse: 743407232.0000 - val_mae: 27250.2188\n",
            "Epoch 127/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22005.1973 - mse: 527473856.0000 - mae: 22005.1973 - val_loss: 27215.7129 - val_mse: 741538688.0000 - val_mae: 27215.7129\n",
            "Epoch 128/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22467.4121 - mse: 549382016.0000 - mae: 22467.4121 - val_loss: 27187.6465 - val_mse: 740021056.0000 - val_mae: 27187.6465\n",
            "Epoch 129/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21929.5908 - mse: 519798080.0000 - mae: 21929.5898 - val_loss: 27152.4941 - val_mse: 738122624.0000 - val_mae: 27152.4941\n",
            "Epoch 130/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 22170.0859 - mse: 537268544.0000 - mae: 22170.0859 - val_loss: 27121.4590 - val_mse: 736450048.0000 - val_mae: 27121.4590\n",
            "Epoch 131/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21973.8799 - mse: 530196800.0000 - mae: 21973.8789 - val_loss: 27087.1602 - val_mse: 734601024.0000 - val_mae: 27087.1602\n",
            "Epoch 132/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21941.9473 - mse: 522600288.0000 - mae: 21941.9473 - val_loss: 27052.8867 - val_mse: 732756608.0000 - val_mae: 27052.8867\n",
            "Epoch 133/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21803.1328 - mse: 517064896.0000 - mae: 21803.1328 - val_loss: 27017.3633 - val_mse: 730848128.0000 - val_mae: 27017.3633\n",
            "Epoch 134/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21779.7461 - mse: 518339072.0000 - mae: 21779.7461 - val_loss: 26982.0469 - val_mse: 728954496.0000 - val_mae: 26982.0469\n",
            "Epoch 135/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 21738.2656 - mse: 515509856.0000 - mae: 21738.2656 - val_loss: 26946.3711 - val_mse: 727043072.0000 - val_mae: 26946.3711\n",
            "Epoch 136/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 21634.4209 - mse: 509363200.0000 - mae: 21634.4199 - val_loss: 26909.8125 - val_mse: 725088896.0000 - val_mae: 26909.8125\n",
            "Epoch 137/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21794.3271 - mse: 520662688.0000 - mae: 21794.3281 - val_loss: 26875.8008 - val_mse: 723273088.0000 - val_mae: 26875.8008\n",
            "Epoch 138/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 21652.3672 - mse: 511882656.0000 - mae: 21652.3672 - val_loss: 26839.2754 - val_mse: 721324416.0000 - val_mae: 26839.2754\n",
            "Epoch 139/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21704.1523 - mse: 515836864.0000 - mae: 21704.1523 - val_loss: 26803.9180 - val_mse: 719440768.0000 - val_mae: 26803.9180\n",
            "Epoch 140/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21565.8418 - mse: 506126400.0000 - mae: 21565.8418 - val_loss: 26767.5195 - val_mse: 717505216.0000 - val_mae: 26767.5195\n",
            "Epoch 141/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21458.7676 - mse: 506690144.0000 - mae: 21458.7676 - val_loss: 26729.8711 - val_mse: 715505920.0000 - val_mae: 26729.8711\n",
            "Epoch 142/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21735.3584 - mse: 515355840.0000 - mae: 21735.3594 - val_loss: 26695.2090 - val_mse: 713666048.0000 - val_mae: 26695.2090\n",
            "Epoch 143/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21661.9971 - mse: 512729664.0000 - mae: 21661.9961 - val_loss: 26660.1211 - val_mse: 711807872.0000 - val_mae: 26660.1211\n",
            "Epoch 144/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 21004.7861 - mse: 485951488.0000 - mae: 21004.7852 - val_loss: 26617.7910 - val_mse: 709569280.0000 - val_mae: 26617.7910\n",
            "Epoch 145/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 21379.5352 - mse: 498173376.0000 - mae: 21379.5352 - val_loss: 26580.1406 - val_mse: 707582272.0000 - val_mae: 26580.1406\n",
            "Epoch 146/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21574.5625 - mse: 510666816.0000 - mae: 21574.5625 - val_loss: 26545.2441 - val_mse: 705742848.0000 - val_mae: 26545.2441\n",
            "Epoch 147/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 21262.6953 - mse: 499813120.0000 - mae: 21262.6973 - val_loss: 26506.8066 - val_mse: 703719936.0000 - val_mae: 26506.8066\n",
            "Epoch 148/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21359.9404 - mse: 496720896.0000 - mae: 21359.9414 - val_loss: 26468.9121 - val_mse: 701726848.0000 - val_mae: 26468.9121\n",
            "Epoch 149/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21068.6826 - mse: 488726944.0000 - mae: 21068.6836 - val_loss: 26428.3184 - val_mse: 699595776.0000 - val_mae: 26428.3184\n",
            "Epoch 150/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21115.9707 - mse: 491637760.0000 - mae: 21115.9707 - val_loss: 26388.5508 - val_mse: 697511936.0000 - val_mae: 26388.5508\n",
            "Epoch 151/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21005.3184 - mse: 484263840.0000 - mae: 21005.3184 - val_loss: 26348.5996 - val_mse: 695422592.0000 - val_mae: 26348.5996\n",
            "Epoch 152/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21010.5898 - mse: 487798624.0000 - mae: 21010.5898 - val_loss: 26308.9023 - val_mse: 693350272.0000 - val_mae: 26308.9023\n",
            "Epoch 153/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20992.9805 - mse: 487465472.0000 - mae: 20992.9805 - val_loss: 26269.1035 - val_mse: 691275520.0000 - val_mae: 26269.1035\n",
            "Epoch 154/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20858.3701 - mse: 481674144.0000 - mae: 20858.3711 - val_loss: 26227.8594 - val_mse: 689128640.0000 - val_mae: 26227.8594\n",
            "Epoch 155/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20680.0186 - mse: 472273088.0000 - mae: 20680.0195 - val_loss: 26184.0723 - val_mse: 686850176.0000 - val_mae: 26184.0723\n",
            "Epoch 156/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20931.9561 - mse: 486237888.0000 - mae: 20931.9570 - val_loss: 26143.9141 - val_mse: 684765568.0000 - val_mae: 26143.9141\n",
            "Epoch 157/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20842.5312 - mse: 480760928.0000 - mae: 20842.5312 - val_loss: 26103.0488 - val_mse: 682649984.0000 - val_mae: 26103.0488\n",
            "Epoch 158/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20789.9219 - mse: 477315264.0000 - mae: 20789.9219 - val_loss: 26062.1934 - val_mse: 680537344.0000 - val_mae: 26062.1934\n",
            "Epoch 159/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20806.0439 - mse: 475243008.0000 - mae: 20806.0430 - val_loss: 26021.5039 - val_mse: 678435456.0000 - val_mae: 26021.5039\n",
            "Epoch 160/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20540.4688 - mse: 467480320.0000 - mae: 20540.4688 - val_loss: 25977.1289 - val_mse: 676145792.0000 - val_mae: 25977.1289\n",
            "Epoch 161/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20755.9229 - mse: 476395424.0000 - mae: 20755.9219 - val_loss: 25936.0156 - val_mse: 674030080.0000 - val_mae: 25936.0156\n",
            "Epoch 162/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20307.0264 - mse: 456270336.0000 - mae: 20307.0273 - val_loss: 25890.4902 - val_mse: 671690624.0000 - val_mae: 25890.4902\n",
            "Epoch 163/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21043.1279 - mse: 486166528.0000 - mae: 21043.1289 - val_loss: 25852.9727 - val_mse: 669767424.0000 - val_mae: 25852.9727\n",
            "Epoch 164/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20515.0244 - mse: 469623616.0000 - mae: 20515.0254 - val_loss: 25809.8945 - val_mse: 667561600.0000 - val_mae: 25809.8945\n",
            "Epoch 165/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20402.7998 - mse: 465377216.0000 - mae: 20402.8008 - val_loss: 25766.7344 - val_mse: 665356288.0000 - val_mae: 25766.7344\n",
            "Epoch 166/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20514.7910 - mse: 474031936.0000 - mae: 20514.7910 - val_loss: 25724.7402 - val_mse: 663215040.0000 - val_mae: 25724.7402\n",
            "Epoch 167/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20035.6758 - mse: 445261632.0000 - mae: 20035.6758 - val_loss: 25677.5879 - val_mse: 660813312.0000 - val_mae: 25677.5879\n",
            "Epoch 168/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20417.0391 - mse: 458621024.0000 - mae: 20417.0391 - val_loss: 25634.6875 - val_mse: 658631168.0000 - val_mae: 25634.6875\n",
            "Epoch 169/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 20336.9961 - mse: 461782784.0000 - mae: 20336.9961 - val_loss: 25591.0586 - val_mse: 656416640.0000 - val_mae: 25591.0586\n",
            "Epoch 170/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20199.1719 - mse: 453553056.0000 - mae: 20199.1719 - val_loss: 25546.2480 - val_mse: 654147200.0000 - val_mae: 25546.2480\n",
            "Epoch 171/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 19922.4521 - mse: 439091552.0000 - mae: 19922.4531 - val_loss: 25499.5254 - val_mse: 651785600.0000 - val_mae: 25499.5254\n",
            "Epoch 172/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 20064.5713 - mse: 444291424.0000 - mae: 20064.5723 - val_loss: 25452.8555 - val_mse: 649427392.0000 - val_mae: 25452.8555\n",
            "Epoch 173/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19187.2178 - mse: 417501440.0000 - mae: 19187.2168 - val_loss: 25398.8340 - val_mse: 646705408.0000 - val_mae: 25398.8340\n",
            "Epoch 174/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 19638.3242 - mse: 429104192.0000 - mae: 19638.3242 - val_loss: 25348.5020 - val_mse: 644171072.0000 - val_mae: 25348.5020\n",
            "Epoch 175/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19928.1621 - mse: 443162432.0000 - mae: 19928.1621 - val_loss: 25302.5410 - val_mse: 641865344.0000 - val_mae: 25302.5410\n",
            "Epoch 176/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19494.2422 - mse: 420874560.0000 - mae: 19494.2422 - val_loss: 25253.4473 - val_mse: 639409536.0000 - val_mae: 25253.4473\n",
            "Epoch 177/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 19498.9258 - mse: 423044736.0000 - mae: 19498.9258 - val_loss: 25203.4062 - val_mse: 636907264.0000 - val_mae: 25203.4062\n",
            "Epoch 178/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20027.2061 - mse: 453535424.0000 - mae: 20027.2070 - val_loss: 25159.8125 - val_mse: 634736320.0000 - val_mae: 25159.8125\n",
            "Epoch 179/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19832.0967 - mse: 441921280.0000 - mae: 19832.0977 - val_loss: 25114.8906 - val_mse: 632502272.0000 - val_mae: 25114.8906\n",
            "Epoch 180/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19829.6230 - mse: 438931904.0000 - mae: 19829.6211 - val_loss: 25069.9160 - val_mse: 630268480.0000 - val_mae: 25069.9160\n",
            "Epoch 181/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19873.9092 - mse: 441836448.0000 - mae: 19873.9102 - val_loss: 25024.9199 - val_mse: 628037312.0000 - val_mae: 25024.9199\n",
            "Epoch 182/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19173.9043 - mse: 408894912.0000 - mae: 19173.9023 - val_loss: 24973.3477 - val_mse: 625486336.0000 - val_mae: 24973.3477\n",
            "Epoch 183/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20034.4238 - mse: 447250016.0000 - mae: 20034.4238 - val_loss: 24930.2949 - val_mse: 623362048.0000 - val_mae: 24930.2949\n",
            "Epoch 184/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19220.9609 - mse: 420382464.0000 - mae: 19220.9609 - val_loss: 24879.8691 - val_mse: 620876800.0000 - val_mae: 24879.8691\n",
            "Epoch 185/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 19730.0762 - mse: 433330272.0000 - mae: 19730.0762 - val_loss: 24833.8477 - val_mse: 618612096.0000 - val_mae: 24833.8477\n",
            "Epoch 186/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18843.4805 - mse: 399119360.0000 - mae: 18843.4805 - val_loss: 24779.8242 - val_mse: 615958720.0000 - val_mae: 24779.8242\n",
            "Epoch 187/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19650.9268 - mse: 429070688.0000 - mae: 19650.9277 - val_loss: 24733.7012 - val_mse: 613699136.0000 - val_mae: 24733.7012\n",
            "Epoch 188/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18994.4531 - mse: 405841408.0000 - mae: 18994.4531 - val_loss: 24681.9277 - val_mse: 611166784.0000 - val_mae: 24681.9277\n",
            "Epoch 189/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 19340.1465 - mse: 425783936.0000 - mae: 19340.1465 - val_loss: 24633.4375 - val_mse: 608802688.0000 - val_mae: 24633.4375\n",
            "Epoch 190/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19883.2900 - mse: 445814272.0000 - mae: 19883.2910 - val_loss: 24590.2695 - val_mse: 606701696.0000 - val_mae: 24590.2695\n",
            "Epoch 191/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 18802.5303 - mse: 398435712.0000 - mae: 18802.5312 - val_loss: 24536.5957 - val_mse: 604089664.0000 - val_mae: 24536.5957\n",
            "Epoch 192/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 18727.5225 - mse: 398223808.0000 - mae: 18727.5215 - val_loss: 24482.8262 - val_mse: 601481152.0000 - val_mae: 24482.8262\n",
            "Epoch 193/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 19221.1348 - mse: 419300800.0000 - mae: 19221.1348 - val_loss: 24434.6562 - val_mse: 599152320.0000 - val_mae: 24434.6562\n",
            "Epoch 194/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18563.3252 - mse: 394761280.0000 - mae: 18563.3242 - val_loss: 24379.7441 - val_mse: 596498752.0000 - val_mae: 24379.7441\n",
            "Epoch 195/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18956.6079 - mse: 413747072.0000 - mae: 18956.6074 - val_loss: 24329.8848 - val_mse: 594097472.0000 - val_mae: 24329.8848\n",
            "Epoch 196/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17751.8164 - mse: 360360096.0000 - mae: 17751.8164 - val_loss: 24269.4688 - val_mse: 591196160.0000 - val_mae: 24269.4688\n",
            "Epoch 197/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18212.3984 - mse: 376612960.0000 - mae: 18212.3984 - val_loss: 24213.5059 - val_mse: 588513728.0000 - val_mae: 24213.5059\n",
            "Epoch 198/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 18026.4902 - mse: 370535232.0000 - mae: 18026.4902 - val_loss: 24156.1191 - val_mse: 585769856.0000 - val_mae: 24156.1191\n",
            "Epoch 199/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18209.5254 - mse: 375897760.0000 - mae: 18209.5254 - val_loss: 24100.7344 - val_mse: 583128384.0000 - val_mae: 24100.7344\n",
            "Epoch 200/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18447.6689 - mse: 391140512.0000 - mae: 18447.6680 - val_loss: 24046.9043 - val_mse: 580565824.0000 - val_mae: 24046.9043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~*Pretraining Day: 60~~~~~~~~~~~~~~~~~~~~\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50 samples, validate on 10 samples\n",
            "Epoch 1/200\n",
            "50/50 [==============================] - 2s 45ms/step - loss: 26302.4602 - mse: 713713984.0000 - mae: 26302.4609 - val_loss: 22458.7109 - val_mse: 504741792.0000 - val_mae: 22458.7109\n",
            "Epoch 2/200\n",
            "50/50 [==============================] - 0s 714us/step - loss: 26302.1691 - mse: 713698240.0000 - mae: 26302.1699 - val_loss: 22457.6758 - val_mse: 504695200.0000 - val_mae: 22457.6758\n",
            "Epoch 3/200\n",
            "50/50 [==============================] - 0s 677us/step - loss: 26297.8781 - mse: 713469952.0000 - mae: 26297.8770 - val_loss: 22450.9102 - val_mse: 504391232.0000 - val_mae: 22450.9102\n",
            "Epoch 4/200\n",
            "50/50 [==============================] - 0s 609us/step - loss: 26284.6828 - mse: 712792256.0000 - mae: 26284.6816 - val_loss: 22440.1816 - val_mse: 503909056.0000 - val_mae: 22440.1816\n",
            "Epoch 5/200\n",
            "50/50 [==============================] - 0s 735us/step - loss: 26269.9141 - mse: 711968576.0000 - mae: 26269.9141 - val_loss: 22428.5527 - val_mse: 503386688.0000 - val_mae: 22428.5527\n",
            "Epoch 6/200\n",
            "50/50 [==============================] - 0s 662us/step - loss: 26251.9949 - mse: 711025856.0000 - mae: 26251.9941 - val_loss: 22414.5430 - val_mse: 502757984.0000 - val_mae: 22414.5430\n",
            "Epoch 7/200\n",
            "50/50 [==============================] - 0s 666us/step - loss: 26226.2324 - mse: 709681152.0000 - mae: 26226.2324 - val_loss: 22397.7520 - val_mse: 502005312.0000 - val_mae: 22397.7520\n",
            "Epoch 8/200\n",
            "50/50 [==============================] - 0s 876us/step - loss: 26202.4234 - mse: 708425344.0000 - mae: 26202.4219 - val_loss: 22379.2129 - val_mse: 501175104.0000 - val_mae: 22379.2129\n",
            "Epoch 9/200\n",
            "50/50 [==============================] - 0s 751us/step - loss: 26176.7387 - mse: 707041344.0000 - mae: 26176.7402 - val_loss: 22357.8184 - val_mse: 500218272.0000 - val_mae: 22357.8184\n",
            "Epoch 10/200\n",
            "50/50 [==============================] - 0s 817us/step - loss: 26144.9004 - mse: 705444800.0000 - mae: 26144.9023 - val_loss: 22333.5742 - val_mse: 499135392.0000 - val_mae: 22333.5742\n",
            "Epoch 11/200\n",
            "50/50 [==============================] - 0s 783us/step - loss: 26107.9605 - mse: 703247808.0000 - mae: 26107.9609 - val_loss: 22306.1289 - val_mse: 497911392.0000 - val_mae: 22306.1289\n",
            "Epoch 12/200\n",
            "50/50 [==============================] - 0s 972us/step - loss: 26053.1539 - mse: 700485888.0000 - mae: 26053.1523 - val_loss: 22274.5059 - val_mse: 496503616.0000 - val_mae: 22274.5059\n",
            "Epoch 13/200\n",
            "50/50 [==============================] - 0s 753us/step - loss: 26033.7074 - mse: 699459328.0000 - mae: 26033.7070 - val_loss: 22242.7383 - val_mse: 495091872.0000 - val_mae: 22242.7383\n",
            "Epoch 14/200\n",
            "50/50 [==============================] - 0s 805us/step - loss: 25961.6852 - mse: 695829440.0000 - mae: 25961.6855 - val_loss: 22204.2344 - val_mse: 493384384.0000 - val_mae: 22204.2344\n",
            "Epoch 15/200\n",
            "50/50 [==============================] - 0s 728us/step - loss: 25923.3766 - mse: 693862656.0000 - mae: 25923.3750 - val_loss: 22164.1289 - val_mse: 491610208.0000 - val_mae: 22164.1289\n",
            "Epoch 16/200\n",
            "50/50 [==============================] - 0s 725us/step - loss: 25876.8660 - mse: 691322368.0000 - mae: 25876.8652 - val_loss: 22121.4414 - val_mse: 489725440.0000 - val_mae: 22121.4414\n",
            "Epoch 17/200\n",
            "50/50 [==============================] - 0s 996us/step - loss: 25796.1426 - mse: 687422848.0000 - mae: 25796.1426 - val_loss: 22073.6055 - val_mse: 487618656.0000 - val_mae: 22073.6055\n",
            "Epoch 18/200\n",
            "50/50 [==============================] - 0s 814us/step - loss: 25716.1863 - mse: 683024128.0000 - mae: 25716.1855 - val_loss: 22021.5117 - val_mse: 485331648.0000 - val_mae: 22021.5117\n",
            "Epoch 19/200\n",
            "50/50 [==============================] - 0s 724us/step - loss: 25645.1574 - mse: 679886592.0000 - mae: 25645.1602 - val_loss: 21966.9531 - val_mse: 482944416.0000 - val_mae: 21966.9531\n",
            "Epoch 20/200\n",
            "50/50 [==============================] - 0s 619us/step - loss: 25546.7750 - mse: 674425408.0000 - mae: 25546.7754 - val_loss: 21908.0977 - val_mse: 480378208.0000 - val_mae: 21908.0977\n",
            "Epoch 21/200\n",
            "50/50 [==============================] - 0s 705us/step - loss: 25456.8234 - mse: 669378048.0000 - mae: 25456.8223 - val_loss: 21845.5742 - val_mse: 477663040.0000 - val_mae: 21845.5742\n",
            "Epoch 22/200\n",
            "50/50 [==============================] - 0s 686us/step - loss: 25454.3449 - mse: 669868160.0000 - mae: 25454.3457 - val_loss: 21785.9023 - val_mse: 475081536.0000 - val_mae: 21785.9023\n",
            "Epoch 23/200\n",
            "50/50 [==============================] - 0s 897us/step - loss: 25334.5734 - mse: 663167488.0000 - mae: 25334.5742 - val_loss: 21719.7344 - val_mse: 472229024.0000 - val_mae: 21719.7344\n",
            "Epoch 24/200\n",
            "50/50 [==============================] - 0s 779us/step - loss: 25201.4551 - mse: 657008960.0000 - mae: 25201.4551 - val_loss: 21648.8125 - val_mse: 469185632.0000 - val_mae: 21648.8125\n",
            "Epoch 25/200\n",
            "50/50 [==============================] - 0s 941us/step - loss: 25251.8336 - mse: 658840192.0000 - mae: 25251.8320 - val_loss: 21581.7969 - val_mse: 466319456.0000 - val_mae: 21581.7969\n",
            "Epoch 26/200\n",
            "50/50 [==============================] - 0s 817us/step - loss: 25058.5543 - mse: 650142336.0000 - mae: 25058.5547 - val_loss: 21504.1055 - val_mse: 463011136.0000 - val_mae: 21504.1055\n",
            "Epoch 27/200\n",
            "50/50 [==============================] - 0s 768us/step - loss: 24997.3738 - mse: 646177984.0000 - mae: 24997.3730 - val_loss: 21424.3945 - val_mse: 459632736.0000 - val_mae: 21424.3945\n",
            "Epoch 28/200\n",
            "50/50 [==============================] - 0s 701us/step - loss: 24784.0633 - mse: 636038912.0000 - mae: 24784.0625 - val_loss: 21336.9336 - val_mse: 455948736.0000 - val_mae: 21336.9336\n",
            "Epoch 29/200\n",
            "50/50 [==============================] - 0s 726us/step - loss: 24766.6977 - mse: 634973376.0000 - mae: 24766.6973 - val_loss: 21250.8730 - val_mse: 452340416.0000 - val_mae: 21250.8730\n",
            "Epoch 30/200\n",
            "50/50 [==============================] - 0s 810us/step - loss: 24671.7668 - mse: 630274048.0000 - mae: 24671.7656 - val_loss: 21160.2559 - val_mse: 448559616.0000 - val_mae: 21160.2559\n",
            "Epoch 31/200\n",
            "50/50 [==============================] - 0s 745us/step - loss: 24488.9070 - mse: 620377152.0000 - mae: 24488.9082 - val_loss: 21063.7012 - val_mse: 444555072.0000 - val_mae: 21063.7012\n",
            "Epoch 32/200\n",
            "50/50 [==============================] - 0s 747us/step - loss: 24371.7535 - mse: 614468224.0000 - mae: 24371.7559 - val_loss: 20963.7617 - val_mse: 440434592.0000 - val_mae: 20963.7617\n",
            "Epoch 33/200\n",
            "50/50 [==============================] - 0s 694us/step - loss: 24323.5801 - mse: 613874432.0000 - mae: 24323.5801 - val_loss: 20863.8086 - val_mse: 436337600.0000 - val_mae: 20863.8086\n",
            "Epoch 34/200\n",
            "50/50 [==============================] - 0s 925us/step - loss: 24066.4801 - mse: 600600640.0000 - mae: 24066.4805 - val_loss: 20754.7383 - val_mse: 431897088.0000 - val_mae: 20754.7383\n",
            "Epoch 35/200\n",
            "50/50 [==============================] - 0s 723us/step - loss: 24064.1336 - mse: 600517312.0000 - mae: 24064.1348 - val_loss: 20647.1367 - val_mse: 427541568.0000 - val_mae: 20647.1367\n",
            "Epoch 36/200\n",
            "50/50 [==============================] - 0s 753us/step - loss: 23851.9602 - mse: 590458624.0000 - mae: 23851.9609 - val_loss: 20533.7559 - val_mse: 422985664.0000 - val_mae: 20533.7559\n",
            "Epoch 37/200\n",
            "50/50 [==============================] - 0s 699us/step - loss: 23664.9070 - mse: 579316672.0000 - mae: 23664.9082 - val_loss: 20414.5137 - val_mse: 418225760.0000 - val_mae: 20414.5137\n",
            "Epoch 38/200\n",
            "50/50 [==============================] - 0s 667us/step - loss: 23687.2355 - mse: 582852224.0000 - mae: 23687.2344 - val_loss: 20297.9766 - val_mse: 413603936.0000 - val_mae: 20297.9766\n",
            "Epoch 39/200\n",
            "50/50 [==============================] - 0s 822us/step - loss: 23516.1750 - mse: 574720064.0000 - mae: 23516.1758 - val_loss: 20178.4414 - val_mse: 408899904.0000 - val_mae: 20178.4414\n",
            "Epoch 40/200\n",
            "50/50 [==============================] - 0s 718us/step - loss: 23125.4594 - mse: 556755776.0000 - mae: 23125.4570 - val_loss: 20044.9473 - val_mse: 403684832.0000 - val_mae: 20044.9473\n",
            "Epoch 41/200\n",
            "50/50 [==============================] - 0s 701us/step - loss: 23020.0582 - mse: 550152384.0000 - mae: 23020.0605 - val_loss: 19909.9492 - val_mse: 398449184.0000 - val_mae: 19909.9492\n",
            "Epoch 42/200\n",
            "50/50 [==============================] - 0s 893us/step - loss: 22932.0266 - mse: 546977472.0000 - mae: 22932.0273 - val_loss: 19774.2930 - val_mse: 393231968.0000 - val_mae: 19774.2930\n",
            "Epoch 43/200\n",
            "50/50 [==============================] - 0s 738us/step - loss: 22703.8863 - mse: 535651872.0000 - mae: 22703.8848 - val_loss: 19633.3281 - val_mse: 387857216.0000 - val_mae: 19633.3281\n",
            "Epoch 44/200\n",
            "50/50 [==============================] - 0s 715us/step - loss: 22604.2145 - mse: 533939328.0000 - mae: 22604.2148 - val_loss: 19491.5801 - val_mse: 382494304.0000 - val_mae: 19491.5801\n",
            "Epoch 45/200\n",
            "50/50 [==============================] - 0s 758us/step - loss: 22278.0316 - mse: 521331104.0000 - mae: 22278.0332 - val_loss: 19339.9609 - val_mse: 376802528.0000 - val_mae: 19339.9609\n",
            "Epoch 46/200\n",
            "50/50 [==============================] - 0s 771us/step - loss: 22240.6328 - mse: 517420448.0000 - mae: 22240.6328 - val_loss: 19190.1035 - val_mse: 371229280.0000 - val_mae: 19190.1035\n",
            "Epoch 47/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 21963.0418 - mse: 504091360.0000 - mae: 21963.0430 - val_loss: 19035.0625 - val_mse: 365518016.0000 - val_mae: 19035.0625\n",
            "Epoch 48/200\n",
            "50/50 [==============================] - 0s 764us/step - loss: 21868.2160 - mse: 499578784.0000 - mae: 21868.2148 - val_loss: 18877.5781 - val_mse: 359764384.0000 - val_mae: 18877.5781\n",
            "Epoch 49/200\n",
            "50/50 [==============================] - 0s 799us/step - loss: 21453.5648 - mse: 482782400.0000 - mae: 21453.5645 - val_loss: 18710.2812 - val_mse: 353720480.0000 - val_mae: 18710.2812\n",
            "Epoch 50/200\n",
            "50/50 [==============================] - 0s 743us/step - loss: 21687.9422 - mse: 494006912.0000 - mae: 21687.9434 - val_loss: 18553.4805 - val_mse: 348114272.0000 - val_mae: 18553.4805\n",
            "Epoch 51/200\n",
            "50/50 [==============================] - 0s 733us/step - loss: 21016.8043 - mse: 467275040.0000 - mae: 21016.8047 - val_loss: 18378.9102 - val_mse: 341947488.0000 - val_mae: 18378.9102\n",
            "Epoch 52/200\n",
            "50/50 [==============================] - 0s 768us/step - loss: 20907.2359 - mse: 461126464.0000 - mae: 20907.2363 - val_loss: 18204.6094 - val_mse: 335865408.0000 - val_mae: 18204.6094\n",
            "Epoch 53/200\n",
            "50/50 [==============================] - 0s 744us/step - loss: 20465.5656 - mse: 442355392.0000 - mae: 20465.5664 - val_loss: 18022.2969 - val_mse: 329581216.0000 - val_mae: 18022.2969\n",
            "Epoch 54/200\n",
            "50/50 [==============================] - 0s 689us/step - loss: 20445.9650 - mse: 441952160.0000 - mae: 20445.9668 - val_loss: 17841.3984 - val_mse: 323413408.0000 - val_mae: 17841.3984\n",
            "Epoch 55/200\n",
            "50/50 [==============================] - 0s 859us/step - loss: 20175.5332 - mse: 431769920.0000 - mae: 20175.5332 - val_loss: 17656.9102 - val_mse: 317207616.0000 - val_mae: 17656.9102\n",
            "Epoch 56/200\n",
            "50/50 [==============================] - 0s 617us/step - loss: 19602.8895 - mse: 407780768.0000 - mae: 19602.8887 - val_loss: 17459.8164 - val_mse: 310668928.0000 - val_mae: 17459.8164\n",
            "Epoch 57/200\n",
            "50/50 [==============================] - 0s 945us/step - loss: 19572.8168 - mse: 405358016.0000 - mae: 19572.8164 - val_loss: 17260.5938 - val_mse: 304125600.0000 - val_mae: 17260.5938\n",
            "Epoch 58/200\n",
            "50/50 [==============================] - 0s 818us/step - loss: 19760.7289 - mse: 412943200.0000 - mae: 19760.7305 - val_loss: 17069.7402 - val_mse: 297954304.0000 - val_mae: 17069.7402\n",
            "Epoch 59/200\n",
            "50/50 [==============================] - 0s 769us/step - loss: 19267.6477 - mse: 396271648.0000 - mae: 19267.6484 - val_loss: 16869.4648 - val_mse: 291576960.0000 - val_mae: 16869.4648\n",
            "Epoch 60/200\n",
            "50/50 [==============================] - 0s 859us/step - loss: 19147.9732 - mse: 391415968.0000 - mae: 19147.9746 - val_loss: 16666.1426 - val_mse: 285181888.0000 - val_mae: 16666.1426\n",
            "Epoch 61/200\n",
            "50/50 [==============================] - 0s 746us/step - loss: 19033.3129 - mse: 384656128.0000 - mae: 19033.3125 - val_loss: 16464.6602 - val_mse: 278945728.0000 - val_mae: 16464.6602\n",
            "Epoch 62/200\n",
            "50/50 [==============================] - 0s 785us/step - loss: 18307.0477 - mse: 363442816.0000 - mae: 18307.0469 - val_loss: 16246.0371 - val_mse: 272288256.0000 - val_mae: 16246.0371\n",
            "Epoch 63/200\n",
            "50/50 [==============================] - 0s 720us/step - loss: 17929.8180 - mse: 347028352.0000 - mae: 17929.8184 - val_loss: 16018.3535 - val_mse: 265436320.0000 - val_mae: 16018.3535\n",
            "Epoch 64/200\n",
            "50/50 [==============================] - 0s 729us/step - loss: 17650.0477 - mse: 335886976.0000 - mae: 17650.0469 - val_loss: 15788.4082 - val_mse: 258640432.0000 - val_mae: 15788.4082\n",
            "Epoch 65/200\n",
            "50/50 [==============================] - 0s 710us/step - loss: 17553.1494 - mse: 329320928.0000 - mae: 17553.1484 - val_loss: 15559.0596 - val_mse: 251981568.0000 - val_mae: 15559.0596\n",
            "Epoch 66/200\n",
            "50/50 [==============================] - 0s 651us/step - loss: 17132.0752 - mse: 314306112.0000 - mae: 17132.0742 - val_loss: 15322.5488 - val_mse: 245234384.0000 - val_mae: 15322.5488\n",
            "Epoch 67/200\n",
            "50/50 [==============================] - 0s 766us/step - loss: 16990.2605 - mse: 313228480.0000 - mae: 16990.2617 - val_loss: 15083.9277 - val_mse: 238539216.0000 - val_mae: 15083.9277\n",
            "Epoch 68/200\n",
            "50/50 [==============================] - 0s 833us/step - loss: 16652.8969 - mse: 302675200.0000 - mae: 16652.8984 - val_loss: 14840.2861 - val_mse: 231828144.0000 - val_mae: 14840.2861\n",
            "Epoch 69/200\n",
            "50/50 [==============================] - 0s 720us/step - loss: 16499.3438 - mse: 297838272.0000 - mae: 16499.3438 - val_loss: 14608.0801 - val_mse: 225538208.0000 - val_mae: 14608.0801\n",
            "Epoch 70/200\n",
            "50/50 [==============================] - 0s 765us/step - loss: 16172.9107 - mse: 289261536.0000 - mae: 16172.9111 - val_loss: 14360.7031 - val_mse: 218995248.0000 - val_mae: 14360.7031\n",
            "Epoch 71/200\n",
            "50/50 [==============================] - 0s 767us/step - loss: 15320.5445 - mse: 263696544.0000 - mae: 15320.5439 - val_loss: 14091.9404 - val_mse: 211984976.0000 - val_mae: 14091.9404\n",
            "Epoch 72/200\n",
            "50/50 [==============================] - 0s 898us/step - loss: 15426.7000 - mse: 266841328.0000 - mae: 15426.7002 - val_loss: 13851.8623 - val_mse: 205843376.0000 - val_mae: 13851.8623\n",
            "Epoch 73/200\n",
            "50/50 [==============================] - 0s 766us/step - loss: 15024.8387 - mse: 255625872.0000 - mae: 15024.8398 - val_loss: 13604.9580 - val_mse: 199660400.0000 - val_mae: 13604.9580\n",
            "Epoch 74/200\n",
            "50/50 [==============================] - 0s 654us/step - loss: 14836.9061 - mse: 243667392.0000 - mae: 14836.9062 - val_loss: 13343.4814 - val_mse: 193237776.0000 - val_mae: 13343.4814\n",
            "Epoch 75/200\n",
            "50/50 [==============================] - 0s 913us/step - loss: 15043.2879 - mse: 249176112.0000 - mae: 15043.2871 - val_loss: 13101.4102 - val_mse: 187446944.0000 - val_mae: 13101.4102\n",
            "Epoch 76/200\n",
            "50/50 [==============================] - 0s 864us/step - loss: 13429.1650 - mse: 208125296.0000 - mae: 13429.1650 - val_loss: 12840.7578 - val_mse: 181375824.0000 - val_mae: 12840.7578\n",
            "Epoch 77/200\n",
            "50/50 [==============================] - 0s 842us/step - loss: 13273.4209 - mse: 208020928.0000 - mae: 13273.4209 - val_loss: 12558.8105 - val_mse: 175032528.0000 - val_mae: 12558.8105\n",
            "Epoch 78/200\n",
            "50/50 [==============================] - 0s 762us/step - loss: 13907.8529 - mse: 218337696.0000 - mae: 13907.8525 - val_loss: 12305.0840 - val_mse: 169460976.0000 - val_mae: 12305.0840\n",
            "Epoch 79/200\n",
            "50/50 [==============================] - 0s 955us/step - loss: 12963.1506 - mse: 191370016.0000 - mae: 12963.1504 - val_loss: 12036.2090 - val_mse: 163744512.0000 - val_mae: 12036.2090\n",
            "Epoch 80/200\n",
            "50/50 [==============================] - 0s 744us/step - loss: 12938.4902 - mse: 189838576.0000 - mae: 12938.4902 - val_loss: 11773.6611 - val_mse: 158343040.0000 - val_mae: 11773.6611\n",
            "Epoch 81/200\n",
            "50/50 [==============================] - 0s 811us/step - loss: 12764.4844 - mse: 182428752.0000 - mae: 12764.4854 - val_loss: 11509.3457 - val_mse: 153047904.0000 - val_mae: 11509.3457\n",
            "Epoch 82/200\n",
            "50/50 [==============================] - 0s 757us/step - loss: 11776.6211 - mse: 157770256.0000 - mae: 11776.6211 - val_loss: 11237.7217 - val_mse: 147795472.0000 - val_mae: 11237.7217\n",
            "Epoch 83/200\n",
            "50/50 [==============================] - 0s 769us/step - loss: 11958.6521 - mse: 170155728.0000 - mae: 11958.6523 - val_loss: 10951.4912 - val_mse: 142409296.0000 - val_mae: 10951.4912\n",
            "Epoch 84/200\n",
            "50/50 [==============================] - 0s 731us/step - loss: 11675.1010 - mse: 155120608.0000 - mae: 11675.1016 - val_loss: 10696.7793 - val_mse: 137777456.0000 - val_mae: 10696.7793\n",
            "Epoch 85/200\n",
            "50/50 [==============================] - 0s 781us/step - loss: 11017.7201 - mse: 145203264.0000 - mae: 11017.7197 - val_loss: 10425.5811 - val_mse: 133012712.0000 - val_mae: 10425.5811\n",
            "Epoch 86/200\n",
            "50/50 [==============================] - 0s 858us/step - loss: 11533.1264 - mse: 156899776.0000 - mae: 11533.1260 - val_loss: 10149.5889 - val_mse: 128348272.0000 - val_mae: 10149.5889\n",
            "Epoch 87/200\n",
            "50/50 [==============================] - 0s 755us/step - loss: 9958.2292 - mse: 122164480.0000 - mae: 9958.2285 - val_loss: 9870.6045 - val_mse: 123824240.0000 - val_mae: 9870.6045\n",
            "Epoch 88/200\n",
            "50/50 [==============================] - 0s 899us/step - loss: 10155.1432 - mse: 124473568.0000 - mae: 10155.1436 - val_loss: 9609.2314 - val_mse: 119687320.0000 - val_mae: 9609.2314\n",
            "Epoch 89/200\n",
            "50/50 [==============================] - 0s 694us/step - loss: 8773.9562 - mse: 95357776.0000 - mae: 8773.9561 - val_loss: 9298.8691 - val_mse: 115029016.0000 - val_mae: 9298.8691\n",
            "Epoch 90/200\n",
            "50/50 [==============================] - 0s 880us/step - loss: 9736.5533 - mse: 113822536.0000 - mae: 9736.5527 - val_loss: 9037.0215 - val_mse: 111243088.0000 - val_mae: 9037.0215\n",
            "Epoch 91/200\n",
            "50/50 [==============================] - 0s 747us/step - loss: 8936.6303 - mse: 102277776.0000 - mae: 8936.6299 - val_loss: 8795.4395 - val_mse: 107846448.0000 - val_mae: 8795.4395\n",
            "Epoch 92/200\n",
            "50/50 [==============================] - 0s 893us/step - loss: 9031.7037 - mse: 107883016.0000 - mae: 9031.7041 - val_loss: 8537.8652 - val_mse: 104342384.0000 - val_mae: 8537.8652\n",
            "Epoch 93/200\n",
            "50/50 [==============================] - 0s 716us/step - loss: 9696.3070 - mse: 117912896.0000 - mae: 9696.3066 - val_loss: 8280.2812 - val_mse: 101051752.0000 - val_mae: 8280.2812\n",
            "Epoch 94/200\n",
            "50/50 [==============================] - 0s 814us/step - loss: 8469.4989 - mse: 91094200.0000 - mae: 8469.4990 - val_loss: 8008.0718 - val_mse: 97702088.0000 - val_mae: 8008.0718\n",
            "Epoch 95/200\n",
            "50/50 [==============================] - 0s 832us/step - loss: 8690.3288 - mse: 99170448.0000 - mae: 8690.3291 - val_loss: 7735.0732 - val_mse: 94592536.0000 - val_mae: 7735.0732\n",
            "Epoch 96/200\n",
            "50/50 [==============================] - 0s 719us/step - loss: 8014.4487 - mse: 86356248.0000 - mae: 8014.4487 - val_loss: 7506.6914 - val_mse: 92224232.0000 - val_mae: 7506.6914\n",
            "Epoch 97/200\n",
            "50/50 [==============================] - 0s 682us/step - loss: 9697.8331 - mse: 110359856.0000 - mae: 9697.8320 - val_loss: 7265.9048 - val_mse: 89710800.0000 - val_mae: 7265.9048\n",
            "Epoch 98/200\n",
            "50/50 [==============================] - 0s 690us/step - loss: 7279.5202 - mse: 71705232.0000 - mae: 7279.5200 - val_loss: 7035.5093 - val_mse: 87372016.0000 - val_mae: 7035.5093\n",
            "Epoch 99/200\n",
            "50/50 [==============================] - 0s 747us/step - loss: 7675.6482 - mse: 77148744.0000 - mae: 7675.6489 - val_loss: 6838.3828 - val_mse: 85420880.0000 - val_mae: 6838.3828\n",
            "Epoch 100/200\n",
            "50/50 [==============================] - 0s 845us/step - loss: 7512.7426 - mse: 70596448.0000 - mae: 7512.7427 - val_loss: 6622.9858 - val_mse: 83582856.0000 - val_mae: 6622.9858\n",
            "Epoch 101/200\n",
            "50/50 [==============================] - 0s 805us/step - loss: 7134.7778 - mse: 72609512.0000 - mae: 7134.7773 - val_loss: 6416.2998 - val_mse: 81858904.0000 - val_mae: 6416.2998\n",
            "Epoch 102/200\n",
            "50/50 [==============================] - 0s 908us/step - loss: 6846.0799 - mse: 62159596.0000 - mae: 6846.0801 - val_loss: 6186.1021 - val_mse: 80097008.0000 - val_mae: 6186.1021\n",
            "Epoch 103/200\n",
            "50/50 [==============================] - 0s 805us/step - loss: 7069.8720 - mse: 66896080.0000 - mae: 7069.8721 - val_loss: 6022.6245 - val_mse: 78899048.0000 - val_mae: 6022.6245\n",
            "Epoch 104/200\n",
            "50/50 [==============================] - 0s 869us/step - loss: 6487.4964 - mse: 54320384.0000 - mae: 6487.4961 - val_loss: 5814.6577 - val_mse: 77145328.0000 - val_mae: 5814.6577\n",
            "Epoch 105/200\n",
            "50/50 [==============================] - 0s 861us/step - loss: 5393.4276 - mse: 44060344.0000 - mae: 5393.4277 - val_loss: 5682.5015 - val_mse: 75759216.0000 - val_mae: 5682.5015\n",
            "Epoch 106/200\n",
            "50/50 [==============================] - 0s 904us/step - loss: 6271.9143 - mse: 52481872.0000 - mae: 6271.9136 - val_loss: 5606.6899 - val_mse: 74720896.0000 - val_mae: 5606.6899\n",
            "Epoch 107/200\n",
            "50/50 [==============================] - 0s 813us/step - loss: 7271.4678 - mse: 65687760.0000 - mae: 7271.4673 - val_loss: 5555.7256 - val_mse: 73544904.0000 - val_mae: 5555.7256\n",
            "Epoch 108/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6598.6470 - mse: 63782216.0000 - mae: 6598.6475 - val_loss: 5538.8374 - val_mse: 73160160.0000 - val_mae: 5538.8374\n",
            "Epoch 109/200\n",
            "50/50 [==============================] - 0s 849us/step - loss: 6413.4190 - mse: 60057776.0000 - mae: 6413.4189 - val_loss: 5533.8584 - val_mse: 72921600.0000 - val_mae: 5533.8584\n",
            "Epoch 110/200\n",
            "50/50 [==============================] - 0s 805us/step - loss: 6966.4206 - mse: 67197152.0000 - mae: 6966.4204 - val_loss: 5535.2109 - val_mse: 72804240.0000 - val_mae: 5535.2109\n",
            "Epoch 111/200\n",
            "50/50 [==============================] - 0s 929us/step - loss: 6889.9916 - mse: 65268900.0000 - mae: 6889.9917 - val_loss: 5603.2500 - val_mse: 71718488.0000 - val_mae: 5603.2500\n",
            "Epoch 112/200\n",
            "50/50 [==============================] - 0s 998us/step - loss: 6702.8410 - mse: 63873752.0000 - mae: 6702.8413 - val_loss: 5699.0889 - val_mse: 71116008.0000 - val_mae: 5699.0889\n",
            "Epoch 113/200\n",
            "50/50 [==============================] - 0s 816us/step - loss: 6265.1572 - mse: 58867108.0000 - mae: 6265.1577 - val_loss: 5820.9829 - val_mse: 70220080.0000 - val_mae: 5820.9829\n",
            "Epoch 114/200\n",
            "50/50 [==============================] - 0s 928us/step - loss: 5314.6510 - mse: 46758912.0000 - mae: 5314.6514 - val_loss: 5842.2251 - val_mse: 69895608.0000 - val_mae: 5842.2251\n",
            "Epoch 115/200\n",
            "50/50 [==============================] - 0s 811us/step - loss: 5762.9298 - mse: 53441304.0000 - mae: 5762.9292 - val_loss: 5962.2881 - val_mse: 69083480.0000 - val_mae: 5962.2881\n",
            "Epoch 116/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5569.1984 - mse: 46345972.0000 - mae: 5569.1987 - val_loss: 6009.7549 - val_mse: 68621184.0000 - val_mae: 6009.7549\n",
            "Epoch 117/200\n",
            "50/50 [==============================] - 0s 811us/step - loss: 5553.3002 - mse: 48108156.0000 - mae: 5553.2998 - val_loss: 6082.8071 - val_mse: 68176872.0000 - val_mae: 6082.8071\n",
            "Epoch 118/200\n",
            "50/50 [==============================] - 0s 911us/step - loss: 6109.7339 - mse: 63922528.0000 - mae: 6109.7339 - val_loss: 6145.8604 - val_mse: 67897944.0000 - val_mae: 6145.8604\n",
            "Epoch 119/200\n",
            "50/50 [==============================] - 0s 939us/step - loss: 6583.5727 - mse: 74740744.0000 - mae: 6583.5723 - val_loss: 6195.2412 - val_mse: 67773480.0000 - val_mae: 6195.2412\n",
            "Epoch 120/200\n",
            "50/50 [==============================] - 0s 907us/step - loss: 5344.1741 - mse: 40052752.0000 - mae: 5344.1743 - val_loss: 6281.4287 - val_mse: 67509832.0000 - val_mae: 6281.4287\n",
            "Epoch 121/200\n",
            "50/50 [==============================] - 0s 869us/step - loss: 6531.9515 - mse: 60623116.0000 - mae: 6531.9512 - val_loss: 6350.6260 - val_mse: 67491120.0000 - val_mae: 6350.6260\n",
            "Epoch 122/200\n",
            "50/50 [==============================] - 0s 965us/step - loss: 6104.4244 - mse: 54220684.0000 - mae: 6104.4243 - val_loss: 6370.1069 - val_mse: 67462944.0000 - val_mae: 6370.1069\n",
            "Epoch 123/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6156.6855 - mse: 55806064.0000 - mae: 6156.6855 - val_loss: 6409.1846 - val_mse: 67464400.0000 - val_mae: 6409.1846\n",
            "Epoch 124/200\n",
            "50/50 [==============================] - 0s 698us/step - loss: 6531.7297 - mse: 58736260.0000 - mae: 6531.7300 - val_loss: 6472.4062 - val_mse: 67566024.0000 - val_mae: 6472.4062\n",
            "Epoch 125/200\n",
            "50/50 [==============================] - 0s 976us/step - loss: 5973.0536 - mse: 56165996.0000 - mae: 5973.0532 - val_loss: 6526.3408 - val_mse: 67501808.0000 - val_mae: 6526.3408\n",
            "Epoch 126/200\n",
            "50/50 [==============================] - 0s 714us/step - loss: 6082.0765 - mse: 58309456.0000 - mae: 6082.0762 - val_loss: 6531.7822 - val_mse: 67425640.0000 - val_mae: 6531.7822\n",
            "Epoch 127/200\n",
            "50/50 [==============================] - 0s 945us/step - loss: 5959.6408 - mse: 60161328.0000 - mae: 5959.6406 - val_loss: 6525.3613 - val_mse: 67326688.0000 - val_mae: 6525.3613\n",
            "Epoch 128/200\n",
            "50/50 [==============================] - 0s 678us/step - loss: 5712.2853 - mse: 52778416.0000 - mae: 5712.2852 - val_loss: 6563.8906 - val_mse: 67262960.0000 - val_mae: 6563.8906\n",
            "Epoch 129/200\n",
            "50/50 [==============================] - 0s 720us/step - loss: 5948.4491 - mse: 54045864.0000 - mae: 5948.4492 - val_loss: 6590.7749 - val_mse: 67146048.0000 - val_mae: 6590.7749\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "*******************Processing India************************\n",
            "~~~~~~~~~~~~~~~~~~~~*Pretraining Day: 30~~~~~~~~~~~~~~~~~~~~\n",
            "Train on 20 samples, validate on 10 samples\n",
            "Epoch 1/200\n",
            "20/20 [==============================] - 2s 117ms/step - loss: 467.9245 - mse: 295144.4375 - mae: 467.9245 - val_loss: 1253.0911 - val_mse: 1595480.0000 - val_mae: 1253.0911\n",
            "Epoch 2/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 467.8885 - mse: 295111.1875 - mae: 467.8886 - val_loss: 1253.0598 - val_mse: 1595402.0000 - val_mae: 1253.0598\n",
            "Epoch 3/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 467.8564 - mse: 295081.0938 - mae: 467.8564 - val_loss: 1253.0291 - val_mse: 1595324.6250 - val_mae: 1253.0291\n",
            "Epoch 4/200\n",
            "20/20 [==============================] - 0s 849us/step - loss: 467.8236 - mse: 295048.9062 - mae: 467.8235 - val_loss: 1252.9968 - val_mse: 1595243.2500 - val_mae: 1252.9968\n",
            "Epoch 5/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 467.7854 - mse: 295016.8438 - mae: 467.7853 - val_loss: 1252.9601 - val_mse: 1595150.3750 - val_mae: 1252.9601\n",
            "Epoch 6/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 467.7385 - mse: 294971.5000 - mae: 467.7385 - val_loss: 1252.9138 - val_mse: 1595033.2500 - val_mae: 1252.9138\n",
            "Epoch 7/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 467.6382 - mse: 294888.9062 - mae: 467.6382 - val_loss: 1252.8411 - val_mse: 1594846.6250 - val_mae: 1252.8411\n",
            "Epoch 8/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 467.5299 - mse: 294818.5625 - mae: 467.5299 - val_loss: 1252.7332 - val_mse: 1594568.7500 - val_mae: 1252.7332\n",
            "Epoch 9/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 466.7627 - mse: 294314.2188 - mae: 466.7627 - val_loss: 1252.4690 - val_mse: 1593882.2500 - val_mae: 1252.4690\n",
            "Epoch 10/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 464.8026 - mse: 292867.8438 - mae: 464.8026 - val_loss: 1251.9180 - val_mse: 1592445.0000 - val_mae: 1251.9180\n",
            "Epoch 11/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 462.3941 - mse: 291056.8750 - mae: 462.3941 - val_loss: 1251.0387 - val_mse: 1590153.5000 - val_mae: 1251.0387\n",
            "Epoch 12/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 457.5936 - mse: 287496.5000 - mae: 457.5936 - val_loss: 1249.4642 - val_mse: 1586030.2500 - val_mae: 1249.4642\n",
            "Epoch 13/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 450.8717 - mse: 283696.4062 - mae: 450.8717 - val_loss: 1247.0911 - val_mse: 1579830.2500 - val_mae: 1247.0911\n",
            "Epoch 14/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 442.8452 - mse: 277587.0000 - mae: 442.8452 - val_loss: 1244.1422 - val_mse: 1572197.7500 - val_mae: 1244.1422\n",
            "Epoch 15/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 431.3107 - mse: 270164.6875 - mae: 431.3107 - val_loss: 1240.3497 - val_mse: 1562390.6250 - val_mae: 1240.3497\n",
            "Epoch 16/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 425.6477 - mse: 268668.8438 - mae: 425.6477 - val_loss: 1236.2354 - val_mse: 1551784.7500 - val_mae: 1236.2354\n",
            "Epoch 17/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 419.6778 - mse: 261892.0000 - mae: 419.6778 - val_loss: 1231.9020 - val_mse: 1540714.2500 - val_mae: 1231.9020\n",
            "Epoch 18/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 410.1437 - mse: 256348.8750 - mae: 410.1437 - val_loss: 1226.9543 - val_mse: 1528133.5000 - val_mae: 1226.9543\n",
            "Epoch 19/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 391.5497 - mse: 251006.4062 - mae: 391.5497 - val_loss: 1221.8279 - val_mse: 1515319.0000 - val_mae: 1221.8279\n",
            "Epoch 20/200\n",
            "20/20 [==============================] - 0s 943us/step - loss: 381.4832 - mse: 232055.5000 - mae: 381.4832 - val_loss: 1216.7268 - val_mse: 1502853.5000 - val_mae: 1216.7268\n",
            "Epoch 21/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 383.1831 - mse: 238750.8438 - mae: 383.1830 - val_loss: 1212.5769 - val_mse: 1492818.6250 - val_mae: 1212.5769\n",
            "Epoch 22/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 371.8878 - mse: 225158.2969 - mae: 371.8878 - val_loss: 1207.1836 - val_mse: 1479737.7500 - val_mae: 1207.1836\n",
            "Epoch 23/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 364.0047 - mse: 219088.5781 - mae: 364.0047 - val_loss: 1202.1580 - val_mse: 1467739.2500 - val_mae: 1202.1580\n",
            "Epoch 24/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 341.7126 - mse: 200469.1562 - mae: 341.7126 - val_loss: 1195.8544 - val_mse: 1452764.3750 - val_mae: 1195.8544\n",
            "Epoch 25/200\n",
            "20/20 [==============================] - 0s 984us/step - loss: 348.5166 - mse: 211970.8281 - mae: 348.5165 - val_loss: 1190.9941 - val_mse: 1440966.2500 - val_mae: 1190.9941\n",
            "Epoch 26/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 340.7985 - mse: 199682.2500 - mae: 340.7985 - val_loss: 1187.4069 - val_mse: 1432638.5000 - val_mae: 1187.4069\n",
            "Epoch 27/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 342.9697 - mse: 198627.9688 - mae: 342.9697 - val_loss: 1181.7841 - val_mse: 1419654.3750 - val_mae: 1181.7841\n",
            "Epoch 28/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 327.6077 - mse: 198623.3281 - mae: 327.6077 - val_loss: 1178.0715 - val_mse: 1410998.5000 - val_mae: 1178.0715\n",
            "Epoch 29/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 298.8003 - mse: 183563.2656 - mae: 298.8003 - val_loss: 1174.4799 - val_mse: 1402945.6250 - val_mae: 1174.4799\n",
            "Epoch 30/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 328.5024 - mse: 192013.0000 - mae: 328.5024 - val_loss: 1173.2841 - val_mse: 1400528.5000 - val_mae: 1173.2841\n",
            "Epoch 31/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 302.8575 - mse: 172815.2031 - mae: 302.8575 - val_loss: 1166.2717 - val_mse: 1384414.6250 - val_mae: 1166.2717\n",
            "Epoch 32/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 305.7792 - mse: 177269.2344 - mae: 305.7792 - val_loss: 1160.9617 - val_mse: 1372500.7500 - val_mae: 1160.9617\n",
            "Epoch 33/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 294.9566 - mse: 174771.6406 - mae: 294.9565 - val_loss: 1160.6918 - val_mse: 1372261.7500 - val_mae: 1160.6918\n",
            "Epoch 34/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 290.4014 - mse: 170126.3281 - mae: 290.4014 - val_loss: 1157.7970 - val_mse: 1365853.3750 - val_mae: 1157.7970\n",
            "Epoch 35/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 280.9518 - mse: 164049.9844 - mae: 280.9518 - val_loss: 1153.9612 - val_mse: 1357308.1250 - val_mae: 1153.9612\n",
            "Epoch 36/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 285.4858 - mse: 168157.6719 - mae: 285.4858 - val_loss: 1148.0713 - val_mse: 1343967.8750 - val_mae: 1148.0713\n",
            "Epoch 37/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 278.0300 - mse: 158255.0312 - mae: 278.0300 - val_loss: 1146.0559 - val_mse: 1339685.6250 - val_mae: 1146.0559\n",
            "Epoch 38/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 260.4213 - mse: 155054.4688 - mae: 260.4213 - val_loss: 1138.2830 - val_mse: 1322165.6250 - val_mae: 1138.2830\n",
            "Epoch 39/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 264.3105 - mse: 139671.6562 - mae: 264.3105 - val_loss: 1135.0837 - val_mse: 1315061.2500 - val_mae: 1135.0837\n",
            "Epoch 40/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 268.3300 - mse: 142166.1250 - mae: 268.3300 - val_loss: 1132.1715 - val_mse: 1309058.2500 - val_mae: 1132.1715\n",
            "Epoch 41/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 245.2272 - mse: 129984.3984 - mae: 245.2272 - val_loss: 1130.8049 - val_mse: 1306308.2500 - val_mae: 1130.8049\n",
            "Epoch 42/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 252.5770 - mse: 142140.5469 - mae: 252.5770 - val_loss: 1121.8264 - val_mse: 1286853.7500 - val_mae: 1121.8264\n",
            "Epoch 43/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 252.0241 - mse: 135452.5000 - mae: 252.0241 - val_loss: 1119.4957 - val_mse: 1281749.2500 - val_mae: 1119.4957\n",
            "Epoch 44/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 264.3736 - mse: 145455.2188 - mae: 264.3735 - val_loss: 1115.0924 - val_mse: 1272248.7500 - val_mae: 1115.0924\n",
            "Epoch 45/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 251.6939 - mse: 147230.9688 - mae: 251.6939 - val_loss: 1113.5542 - val_mse: 1269103.0000 - val_mae: 1113.5542\n",
            "Epoch 46/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 224.9726 - mse: 122246.1094 - mae: 224.9726 - val_loss: 1105.6597 - val_mse: 1252428.2500 - val_mae: 1105.6597\n",
            "Epoch 47/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 243.7368 - mse: 138783.1562 - mae: 243.7368 - val_loss: 1107.0408 - val_mse: 1255601.2500 - val_mae: 1107.0408\n",
            "Epoch 48/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 217.0839 - mse: 111260.2734 - mae: 217.0839 - val_loss: 1108.8329 - val_mse: 1259619.8750 - val_mae: 1108.8329\n",
            "Epoch 49/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 228.5859 - mse: 122458.1719 - mae: 228.5859 - val_loss: 1103.8083 - val_mse: 1248683.6250 - val_mae: 1103.8083\n",
            "Epoch 50/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 241.5886 - mse: 124413.8750 - mae: 241.5886 - val_loss: 1096.2460 - val_mse: 1232481.0000 - val_mae: 1096.2460\n",
            "Epoch 51/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 214.2782 - mse: 107618.2734 - mae: 214.2782 - val_loss: 1094.4512 - val_mse: 1228968.6250 - val_mae: 1094.4512\n",
            "Epoch 52/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 242.2013 - mse: 128564.6719 - mae: 242.2013 - val_loss: 1090.3969 - val_mse: 1220953.1250 - val_mae: 1090.3969\n",
            "Epoch 53/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 187.0921 - mse: 103505.7969 - mae: 187.0921 - val_loss: 1083.2643 - val_mse: 1206129.2500 - val_mae: 1083.2643\n",
            "Epoch 54/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 254.9406 - mse: 126704.2969 - mae: 254.9406 - val_loss: 1085.5345 - val_mse: 1211458.7500 - val_mae: 1085.5345\n",
            "Epoch 55/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 245.0519 - mse: 128085.0391 - mae: 245.0518 - val_loss: 1078.5166 - val_mse: 1196985.6250 - val_mae: 1078.5166\n",
            "Epoch 56/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 202.5229 - mse: 98933.4141 - mae: 202.5229 - val_loss: 1072.0267 - val_mse: 1183655.2500 - val_mae: 1072.0267\n",
            "Epoch 57/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 204.3493 - mse: 103107.7266 - mae: 204.3493 - val_loss: 1070.5881 - val_mse: 1180533.6250 - val_mae: 1070.5881\n",
            "Epoch 58/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 241.7522 - mse: 117856.8281 - mae: 241.7522 - val_loss: 1076.7113 - val_mse: 1193697.7500 - val_mae: 1076.7113\n",
            "Epoch 59/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 207.9605 - mse: 103882.4844 - mae: 207.9605 - val_loss: 1071.6985 - val_mse: 1183350.3750 - val_mae: 1071.6985\n",
            "Epoch 60/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 225.8106 - mse: 113242.8516 - mae: 225.8106 - val_loss: 1071.3604 - val_mse: 1183072.2500 - val_mae: 1071.3604\n",
            "Epoch 61/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 225.6972 - mse: 114576.1406 - mae: 225.6972 - val_loss: 1073.5609 - val_mse: 1188070.0000 - val_mae: 1073.5609\n",
            "Epoch 62/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 231.9840 - mse: 104211.1094 - mae: 231.9840 - val_loss: 1072.7942 - val_mse: 1186573.2500 - val_mae: 1072.7942\n",
            "Epoch 63/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 232.3764 - mse: 109730.1016 - mae: 232.3765 - val_loss: 1072.1866 - val_mse: 1185286.1250 - val_mae: 1072.1866\n",
            "Epoch 64/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 204.9706 - mse: 98515.6406 - mae: 204.9706 - val_loss: 1073.7155 - val_mse: 1188531.2500 - val_mae: 1073.7155\n",
            "Epoch 65/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 202.2623 - mse: 96508.9062 - mae: 202.2623 - val_loss: 1073.4536 - val_mse: 1187893.7500 - val_mae: 1073.4536\n",
            "Epoch 66/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 215.7396 - mse: 101085.9922 - mae: 215.7396 - val_loss: 1073.9187 - val_mse: 1189294.2500 - val_mae: 1073.9187\n",
            "Epoch 67/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 190.7009 - mse: 86749.4141 - mae: 190.7009 - val_loss: 1070.3644 - val_mse: 1181961.0000 - val_mae: 1070.3644\n",
            "Epoch 68/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 215.5198 - mse: 111607.6484 - mae: 215.5198 - val_loss: 1062.5181 - val_mse: 1165848.8750 - val_mae: 1062.5181\n",
            "Epoch 69/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 174.4300 - mse: 81614.1406 - mae: 174.4300 - val_loss: 1065.8789 - val_mse: 1173099.7500 - val_mae: 1065.8789\n",
            "Epoch 70/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 212.5644 - mse: 89449.8906 - mae: 212.5644 - val_loss: 1062.0759 - val_mse: 1165576.1250 - val_mae: 1062.0759\n",
            "Epoch 71/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 194.1647 - mse: 109207.8516 - mae: 194.1647 - val_loss: 1056.4142 - val_mse: 1154213.2500 - val_mae: 1056.4142\n",
            "Epoch 72/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 201.7533 - mse: 86420.5391 - mae: 201.7533 - val_loss: 1052.3630 - val_mse: 1146633.0000 - val_mae: 1052.3630\n",
            "Epoch 73/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 183.8572 - mse: 82340.8672 - mae: 183.8572 - val_loss: 1058.7448 - val_mse: 1159458.0000 - val_mae: 1058.7448\n",
            "Epoch 74/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 175.2935 - mse: 85644.6875 - mae: 175.2935 - val_loss: 1060.8331 - val_mse: 1163706.7500 - val_mae: 1060.8331\n",
            "Epoch 75/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 188.4696 - mse: 97549.5859 - mae: 188.4696 - val_loss: 1059.0782 - val_mse: 1160141.6250 - val_mae: 1059.0782\n",
            "Epoch 76/200\n",
            "20/20 [==============================] - 0s 951us/step - loss: 184.9651 - mse: 82487.0391 - mae: 184.9651 - val_loss: 1057.7861 - val_mse: 1157788.2500 - val_mae: 1057.7861\n",
            "Epoch 77/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 176.7010 - mse: 77675.1016 - mae: 176.7010 - val_loss: 1057.2350 - val_mse: 1156796.5000 - val_mae: 1057.2350\n",
            "Epoch 78/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 192.7646 - mse: 93949.8438 - mae: 192.7646 - val_loss: 1057.7458 - val_mse: 1157929.6250 - val_mae: 1057.7458\n",
            "Epoch 79/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 178.1303 - mse: 77589.8750 - mae: 178.1303 - val_loss: 1051.3121 - val_mse: 1145395.3750 - val_mae: 1051.3121\n",
            "Epoch 80/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 202.4212 - mse: 82787.8906 - mae: 202.4212 - val_loss: 1051.9731 - val_mse: 1147058.0000 - val_mae: 1051.9731\n",
            "Epoch 81/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 168.5789 - mse: 70962.5078 - mae: 168.5789 - val_loss: 1052.9128 - val_mse: 1149096.6250 - val_mae: 1052.9128\n",
            "Epoch 82/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 199.0553 - mse: 83622.8359 - mae: 199.0553 - val_loss: 1049.3492 - val_mse: 1142177.7500 - val_mae: 1049.3492\n",
            "Epoch 83/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 188.9241 - mse: 81734.8516 - mae: 188.9241 - val_loss: 1048.3757 - val_mse: 1140371.3750 - val_mae: 1048.3757\n",
            "Epoch 84/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 210.3140 - mse: 98006.9531 - mae: 210.3140 - val_loss: 1044.1548 - val_mse: 1132267.5000 - val_mae: 1044.1548\n",
            "Epoch 85/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 184.8504 - mse: 74622.5156 - mae: 184.8504 - val_loss: 1046.1541 - val_mse: 1136500.7500 - val_mae: 1046.1541\n",
            "Epoch 86/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 193.8217 - mse: 78407.7109 - mae: 193.8217 - val_loss: 1036.7849 - val_mse: 1117927.5000 - val_mae: 1036.7849\n",
            "Epoch 87/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 172.9692 - mse: 76293.6719 - mae: 172.9692 - val_loss: 1038.0033 - val_mse: 1120969.2500 - val_mae: 1038.0033\n",
            "Epoch 88/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 141.6903 - mse: 62047.9805 - mae: 141.6903 - val_loss: 1036.9906 - val_mse: 1119080.5000 - val_mae: 1036.9906\n",
            "Epoch 89/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 155.0445 - mse: 59997.2578 - mae: 155.0446 - val_loss: 1040.4286 - val_mse: 1125787.5000 - val_mae: 1040.4286\n",
            "Epoch 90/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 191.2723 - mse: 92910.7109 - mae: 191.2723 - val_loss: 1037.6086 - val_mse: 1120179.6250 - val_mae: 1037.6086\n",
            "Epoch 91/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 196.6348 - mse: 92259.8750 - mae: 196.6348 - val_loss: 1035.6353 - val_mse: 1116531.3750 - val_mae: 1035.6353\n",
            "Epoch 92/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 192.4975 - mse: 77200.7188 - mae: 192.4975 - val_loss: 1037.0826 - val_mse: 1119637.7500 - val_mae: 1037.0826\n",
            "Epoch 93/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 181.2411 - mse: 82383.4844 - mae: 181.2411 - val_loss: 1038.2159 - val_mse: 1121800.8750 - val_mae: 1038.2159\n",
            "Epoch 94/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 176.2030 - mse: 79222.1875 - mae: 176.2030 - val_loss: 1036.5059 - val_mse: 1118164.3750 - val_mae: 1036.5059\n",
            "Epoch 95/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 138.5611 - mse: 56413.3242 - mae: 138.5611 - val_loss: 1034.8002 - val_mse: 1115166.0000 - val_mae: 1034.8002\n",
            "Epoch 96/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 156.6398 - mse: 71936.9844 - mae: 156.6398 - val_loss: 1037.4410 - val_mse: 1120723.5000 - val_mae: 1037.4410\n",
            "Epoch 97/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 194.1250 - mse: 88388.6406 - mae: 194.1250 - val_loss: 1031.0352 - val_mse: 1108096.2500 - val_mae: 1031.0352\n",
            "Epoch 98/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 180.7806 - mse: 81955.7344 - mae: 180.7806 - val_loss: 1031.9473 - val_mse: 1110128.1250 - val_mae: 1031.9473\n",
            "Epoch 99/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 164.4618 - mse: 64304.1445 - mae: 164.4618 - val_loss: 1026.2878 - val_mse: 1099438.6250 - val_mae: 1026.2878\n",
            "Epoch 100/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 183.4061 - mse: 75014.5625 - mae: 183.4061 - val_loss: 1022.0397 - val_mse: 1091181.3750 - val_mae: 1022.0397\n",
            "Epoch 101/200\n",
            "20/20 [==============================] - 0s 987us/step - loss: 217.0134 - mse: 94908.9141 - mae: 217.0134 - val_loss: 1023.3849 - val_mse: 1093816.8750 - val_mae: 1023.3849\n",
            "Epoch 102/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 170.9316 - mse: 71986.2500 - mae: 170.9317 - val_loss: 1021.7559 - val_mse: 1090906.7500 - val_mae: 1021.7559\n",
            "Epoch 103/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 157.7577 - mse: 63337.6016 - mae: 157.7577 - val_loss: 1027.8287 - val_mse: 1102905.7500 - val_mae: 1027.8287\n",
            "Epoch 104/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 183.9319 - mse: 79848.0000 - mae: 183.9319 - val_loss: 1019.9615 - val_mse: 1087702.6250 - val_mae: 1019.9615\n",
            "Epoch 105/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 148.9518 - mse: 59293.2383 - mae: 148.9518 - val_loss: 1014.5389 - val_mse: 1077429.6250 - val_mae: 1014.5389\n",
            "Epoch 106/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 177.9196 - mse: 67890.4531 - mae: 177.9196 - val_loss: 1017.5281 - val_mse: 1083234.0000 - val_mae: 1017.5281\n",
            "Epoch 107/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 169.0204 - mse: 66172.3672 - mae: 169.0204 - val_loss: 1016.9189 - val_mse: 1082474.2500 - val_mae: 1016.9189\n",
            "Epoch 108/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 179.1434 - mse: 80526.2812 - mae: 179.1434 - val_loss: 1020.7484 - val_mse: 1090038.2500 - val_mae: 1020.7484\n",
            "Epoch 109/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 175.6735 - mse: 76637.8359 - mae: 175.6735 - val_loss: 1018.0955 - val_mse: 1084575.2500 - val_mae: 1018.0955\n",
            "Epoch 110/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 156.0448 - mse: 58211.6875 - mae: 156.0448 - val_loss: 1014.6535 - val_mse: 1078221.1250 - val_mae: 1014.6535\n",
            "Epoch 111/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 158.9151 - mse: 70266.4609 - mae: 158.9151 - val_loss: 1016.3592 - val_mse: 1081731.2500 - val_mae: 1016.3592\n",
            "Epoch 112/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 149.8497 - mse: 59054.3945 - mae: 149.8497 - val_loss: 1014.3320 - val_mse: 1077933.5000 - val_mae: 1014.3320\n",
            "Epoch 113/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 159.8557 - mse: 61598.1133 - mae: 159.8557 - val_loss: 1013.9196 - val_mse: 1077280.2500 - val_mae: 1013.9196\n",
            "Epoch 114/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 169.9145 - mse: 63629.2812 - mae: 169.9145 - val_loss: 1011.6237 - val_mse: 1072804.6250 - val_mae: 1011.6237\n",
            "Epoch 115/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 168.1527 - mse: 54475.4688 - mae: 168.1527 - val_loss: 1014.7650 - val_mse: 1079411.6250 - val_mae: 1014.7650\n",
            "Epoch 116/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 145.7329 - mse: 57720.7500 - mae: 145.7329 - val_loss: 1017.5969 - val_mse: 1084569.1250 - val_mae: 1017.5969\n",
            "Epoch 117/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 148.9740 - mse: 58512.2109 - mae: 148.9740 - val_loss: 1011.9901 - val_mse: 1073579.2500 - val_mae: 1011.9901\n",
            "Epoch 118/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 115.5882 - mse: 40654.2305 - mae: 115.5882 - val_loss: 1007.4670 - val_mse: 1065043.0000 - val_mae: 1007.4670\n",
            "Epoch 119/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 165.6050 - mse: 57791.6016 - mae: 165.6050 - val_loss: 1006.0614 - val_mse: 1062859.8750 - val_mae: 1006.0614\n",
            "Epoch 120/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 156.4494 - mse: 59021.5078 - mae: 156.4494 - val_loss: 1004.9529 - val_mse: 1060648.2500 - val_mae: 1004.9529\n",
            "Epoch 121/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 167.0909 - mse: 65118.9258 - mae: 167.0909 - val_loss: 1006.3407 - val_mse: 1063775.1250 - val_mae: 1006.3407\n",
            "Epoch 122/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 154.5757 - mse: 59400.9883 - mae: 154.5757 - val_loss: 1004.5076 - val_mse: 1060326.6250 - val_mae: 1004.5076\n",
            "Epoch 123/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 155.1239 - mse: 55019.7891 - mae: 155.1239 - val_loss: 1004.5526 - val_mse: 1060769.3750 - val_mae: 1004.5526\n",
            "Epoch 124/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 131.0578 - mse: 43358.3320 - mae: 131.0578 - val_loss: 1002.7274 - val_mse: 1057841.7500 - val_mae: 1002.7274\n",
            "Epoch 125/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 154.5759 - mse: 55256.8203 - mae: 154.5759 - val_loss: 1000.5266 - val_mse: 1053522.7500 - val_mae: 1000.5266\n",
            "Epoch 126/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 144.9631 - mse: 52922.1055 - mae: 144.9631 - val_loss: 1000.0635 - val_mse: 1052486.2500 - val_mae: 1000.0635\n",
            "Epoch 127/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 123.6186 - mse: 42449.1797 - mae: 123.6186 - val_loss: 1003.2784 - val_mse: 1058533.0000 - val_mae: 1003.2784\n",
            "Epoch 128/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 136.5828 - mse: 45477.9609 - mae: 136.5827 - val_loss: 1004.7811 - val_mse: 1061683.0000 - val_mae: 1004.7811\n",
            "Epoch 129/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 164.1805 - mse: 58327.4922 - mae: 164.1805 - val_loss: 998.1210 - val_mse: 1049204.7500 - val_mae: 998.1210\n",
            "Epoch 130/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 124.5852 - mse: 43516.3047 - mae: 124.5852 - val_loss: 1000.6742 - val_mse: 1054131.2500 - val_mae: 1000.6742\n",
            "Epoch 131/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 157.5705 - mse: 63165.1172 - mae: 157.5705 - val_loss: 999.3372 - val_mse: 1051578.2500 - val_mae: 999.3372\n",
            "Epoch 132/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 131.7469 - mse: 47328.5938 - mae: 131.7469 - val_loss: 995.7702 - val_mse: 1044839.0000 - val_mae: 995.7702\n",
            "Epoch 133/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 169.6338 - mse: 61636.3750 - mae: 169.6338 - val_loss: 994.2266 - val_mse: 1042691.3125 - val_mae: 994.2266\n",
            "Epoch 134/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 131.9702 - mse: 45519.9453 - mae: 131.9702 - val_loss: 992.5892 - val_mse: 1039527.8125 - val_mae: 992.5892\n",
            "Epoch 135/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 142.9463 - mse: 47660.3750 - mae: 142.9463 - val_loss: 987.6561 - val_mse: 1030354.1875 - val_mae: 987.6561\n",
            "Epoch 136/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 143.9737 - mse: 44303.7930 - mae: 143.9737 - val_loss: 988.4244 - val_mse: 1031714.1250 - val_mae: 988.4244\n",
            "Epoch 137/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 166.4727 - mse: 50905.8008 - mae: 166.4727 - val_loss: 993.4598 - val_mse: 1041237.3750 - val_mae: 993.4598\n",
            "Epoch 138/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 152.9044 - mse: 46305.2148 - mae: 152.9044 - val_loss: 991.4655 - val_mse: 1037253.0000 - val_mae: 991.4655\n",
            "Epoch 139/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 143.9295 - mse: 51764.6055 - mae: 143.9294 - val_loss: 995.0148 - val_mse: 1043884.1875 - val_mae: 995.0148\n",
            "Epoch 140/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 150.8369 - mse: 51066.6602 - mae: 150.8369 - val_loss: 989.4927 - val_mse: 1033222.3125 - val_mae: 989.4927\n",
            "Epoch 141/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 162.1957 - mse: 54192.2734 - mae: 162.1957 - val_loss: 992.8343 - val_mse: 1039745.1875 - val_mae: 992.8343\n",
            "Epoch 142/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 140.0321 - mse: 52336.7109 - mae: 140.0321 - val_loss: 990.2928 - val_mse: 1034969.6250 - val_mae: 990.2928\n",
            "Epoch 143/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 167.6345 - mse: 53179.3203 - mae: 167.6345 - val_loss: 992.3773 - val_mse: 1039106.6250 - val_mae: 992.3773\n",
            "Epoch 144/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 122.6045 - mse: 40067.3984 - mae: 122.6045 - val_loss: 991.3793 - val_mse: 1037211.3750 - val_mae: 991.3793\n",
            "Epoch 145/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 146.8943 - mse: 46585.9180 - mae: 146.8943 - val_loss: 989.1453 - val_mse: 1032943.1875 - val_mae: 989.1453\n",
            "Epoch 146/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 177.3340 - mse: 58761.6797 - mae: 177.3340 - val_loss: 990.4484 - val_mse: 1035414.0000 - val_mae: 990.4484\n",
            "Epoch 147/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 132.7959 - mse: 38056.7734 - mae: 132.7959 - val_loss: 985.4656 - val_mse: 1026378.1875 - val_mae: 985.4656\n",
            "Epoch 148/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 167.1177 - mse: 54536.2422 - mae: 167.1176 - val_loss: 984.3060 - val_mse: 1024184.8750 - val_mae: 984.3060\n",
            "Epoch 149/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 154.1680 - mse: 49291.2109 - mae: 154.1680 - val_loss: 988.8493 - val_mse: 1032339.3750 - val_mae: 988.8493\n",
            "Epoch 150/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 128.5986 - mse: 34789.6484 - mae: 128.5986 - val_loss: 990.3763 - val_mse: 1035195.3125 - val_mae: 990.3763\n",
            "Epoch 151/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 157.6847 - mse: 51562.6602 - mae: 157.6847 - val_loss: 990.8532 - val_mse: 1035958.3125 - val_mae: 990.8532\n",
            "Epoch 152/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 146.3303 - mse: 51329.5195 - mae: 146.3303 - val_loss: 987.8125 - val_mse: 1030043.5000 - val_mae: 987.8125\n",
            "Epoch 153/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 134.8854 - mse: 37230.7461 - mae: 134.8854 - val_loss: 988.6597 - val_mse: 1031577.3750 - val_mae: 988.6597\n",
            "Epoch 154/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 154.0515 - mse: 50581.0430 - mae: 154.0515 - val_loss: 988.2369 - val_mse: 1030985.8125 - val_mae: 988.2369\n",
            "Epoch 155/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 149.1733 - mse: 46589.5547 - mae: 149.1733 - val_loss: 990.9367 - val_mse: 1036119.1875 - val_mae: 990.9367\n",
            "Epoch 156/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 154.7501 - mse: 44177.5820 - mae: 154.7501 - val_loss: 992.3165 - val_mse: 1038944.8750 - val_mae: 992.3165\n",
            "Epoch 157/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 168.3498 - mse: 58021.4492 - mae: 168.3498 - val_loss: 997.6460 - val_mse: 1048959.0000 - val_mae: 997.6460\n",
            "Epoch 158/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 170.0834 - mse: 59046.3672 - mae: 170.0834 - val_loss: 993.0092 - val_mse: 1040083.0000 - val_mae: 993.0092\n",
            "Epoch 159/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 135.8829 - mse: 43521.4023 - mae: 135.8829 - val_loss: 989.8703 - val_mse: 1034260.3125 - val_mae: 989.8703\n",
            "Epoch 160/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 139.6212 - mse: 54040.5625 - mae: 139.6212 - val_loss: 993.9053 - val_mse: 1041725.8125 - val_mae: 993.9053\n",
            "Epoch 161/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 143.7746 - mse: 42314.6133 - mae: 143.7746 - val_loss: 990.4213 - val_mse: 1035221.6250 - val_mae: 990.4213\n",
            "Epoch 162/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 147.8412 - mse: 51277.2617 - mae: 147.8412 - val_loss: 989.2560 - val_mse: 1033230.6250 - val_mae: 989.2560\n",
            "Epoch 163/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 172.5740 - mse: 64259.8438 - mae: 172.5740 - val_loss: 988.4000 - val_mse: 1031497.8750 - val_mae: 988.4000\n",
            "Epoch 164/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 151.6901 - mse: 63880.7695 - mae: 151.6901 - val_loss: 990.1297 - val_mse: 1034949.1875 - val_mae: 990.1297\n",
            "Epoch 165/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 149.9822 - mse: 44367.4414 - mae: 149.9822 - val_loss: 989.2301 - val_mse: 1033154.3750 - val_mae: 989.2301\n",
            "Epoch 166/200\n",
            "20/20 [==============================] - 0s 995us/step - loss: 147.5354 - mse: 48211.3203 - mae: 147.5354 - val_loss: 990.8307 - val_mse: 1036184.6875 - val_mae: 990.8307\n",
            "Epoch 167/200\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 144.2998 - mse: 45549.4609 - mae: 144.2998 - val_loss: 990.5646 - val_mse: 1035755.8125 - val_mae: 990.5646\n",
            "Epoch 168/200\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 126.0150 - mse: 32214.4902 - mae: 126.0150 - val_loss: 991.7455 - val_mse: 1037738.6250 - val_mae: 991.7455\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~*Pretraining Day: 60~~~~~~~~~~~~~~~~~~~~\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50 samples, validate on 10 samples\n",
            "Epoch 1/200\n",
            "50/50 [==============================] - 2s 49ms/step - loss: 1524.6419 - mse: 3647288.2500 - mae: 1524.6418 - val_loss: 5221.8062 - val_mse: 27774756.0000 - val_mae: 5221.8062\n",
            "Epoch 2/200\n",
            "50/50 [==============================] - 0s 737us/step - loss: 1524.4920 - mse: 3646960.0000 - mae: 1524.4921 - val_loss: 5221.3447 - val_mse: 27770550.0000 - val_mae: 5221.3447\n",
            "Epoch 3/200\n",
            "50/50 [==============================] - 0s 736us/step - loss: 1522.8752 - mse: 3644395.5000 - mae: 1522.8754 - val_loss: 5217.9863 - val_mse: 27740052.0000 - val_mae: 5217.9863\n",
            "Epoch 4/200\n",
            "50/50 [==============================] - 0s 750us/step - loss: 1518.3063 - mse: 3636349.5000 - mae: 1518.3064 - val_loss: 5212.7329 - val_mse: 27691940.0000 - val_mae: 5212.7329\n",
            "Epoch 5/200\n",
            "50/50 [==============================] - 0s 722us/step - loss: 1507.4977 - mse: 3618763.7500 - mae: 1507.4978 - val_loss: 5204.5928 - val_mse: 27617452.0000 - val_mae: 5204.5928\n",
            "Epoch 6/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1499.8744 - mse: 3604581.0000 - mae: 1499.8745 - val_loss: 5197.0303 - val_mse: 27548204.0000 - val_mae: 5197.0303\n",
            "Epoch 7/200\n",
            "50/50 [==============================] - 0s 885us/step - loss: 1484.6783 - mse: 3574098.0000 - mae: 1484.6782 - val_loss: 5188.6455 - val_mse: 27471450.0000 - val_mae: 5188.6455\n",
            "Epoch 8/200\n",
            "50/50 [==============================] - 0s 977us/step - loss: 1468.6642 - mse: 3539038.0000 - mae: 1468.6644 - val_loss: 5179.3931 - val_mse: 27386492.0000 - val_mae: 5179.3931\n",
            "Epoch 9/200\n",
            "50/50 [==============================] - 0s 847us/step - loss: 1455.5527 - mse: 3507983.2500 - mae: 1455.5526 - val_loss: 5170.7212 - val_mse: 27306548.0000 - val_mae: 5170.7212\n",
            "Epoch 10/200\n",
            "50/50 [==============================] - 0s 742us/step - loss: 1455.8395 - mse: 3503633.2500 - mae: 1455.8394 - val_loss: 5162.6162 - val_mse: 27231398.0000 - val_mae: 5162.6162\n",
            "Epoch 11/200\n",
            "50/50 [==============================] - 0s 785us/step - loss: 1426.1574 - mse: 3446151.2500 - mae: 1426.1575 - val_loss: 5151.6079 - val_mse: 27129798.0000 - val_mae: 5151.6079\n",
            "Epoch 12/200\n",
            "50/50 [==============================] - 0s 759us/step - loss: 1414.0920 - mse: 3414227.7500 - mae: 1414.0920 - val_loss: 5140.4141 - val_mse: 27025466.0000 - val_mae: 5140.4141\n",
            "Epoch 13/200\n",
            "50/50 [==============================] - 0s 708us/step - loss: 1406.4073 - mse: 3390156.7500 - mae: 1406.4072 - val_loss: 5128.3794 - val_mse: 26912970.0000 - val_mae: 5128.3794\n",
            "Epoch 14/200\n",
            "50/50 [==============================] - 0s 861us/step - loss: 1385.7843 - mse: 3325835.7500 - mae: 1385.7844 - val_loss: 5116.1040 - val_mse: 26798352.0000 - val_mae: 5116.1040\n",
            "Epoch 15/200\n",
            "50/50 [==============================] - 0s 824us/step - loss: 1384.1728 - mse: 3343391.2500 - mae: 1384.1729 - val_loss: 5103.2437 - val_mse: 26678102.0000 - val_mae: 5103.2437\n",
            "Epoch 16/200\n",
            "50/50 [==============================] - 0s 668us/step - loss: 1347.8981 - mse: 3240761.0000 - mae: 1347.8981 - val_loss: 5089.0884 - val_mse: 26545948.0000 - val_mae: 5089.0884\n",
            "Epoch 17/200\n",
            "50/50 [==============================] - 0s 738us/step - loss: 1357.3661 - mse: 3238001.0000 - mae: 1357.3661 - val_loss: 5076.4243 - val_mse: 26426934.0000 - val_mae: 5076.4243\n",
            "Epoch 18/200\n",
            "50/50 [==============================] - 0s 825us/step - loss: 1331.4692 - mse: 3185937.5000 - mae: 1331.4692 - val_loss: 5060.3262 - val_mse: 26276450.0000 - val_mae: 5060.3262\n",
            "Epoch 19/200\n",
            "50/50 [==============================] - 0s 870us/step - loss: 1334.0047 - mse: 3209575.7500 - mae: 1334.0046 - val_loss: 5045.2734 - val_mse: 26136064.0000 - val_mae: 5045.2734\n",
            "Epoch 20/200\n",
            "50/50 [==============================] - 0s 706us/step - loss: 1342.0403 - mse: 3147190.5000 - mae: 1342.0403 - val_loss: 5030.6455 - val_mse: 25998838.0000 - val_mae: 5030.6455\n",
            "Epoch 21/200\n",
            "50/50 [==============================] - 0s 711us/step - loss: 1294.9427 - mse: 3056912.7500 - mae: 1294.9427 - val_loss: 5015.3818 - val_mse: 25855882.0000 - val_mae: 5015.3818\n",
            "Epoch 22/200\n",
            "50/50 [==============================] - 0s 785us/step - loss: 1294.0327 - mse: 3029046.0000 - mae: 1294.0327 - val_loss: 5001.8540 - val_mse: 25727878.0000 - val_mae: 5001.8540\n",
            "Epoch 23/200\n",
            "50/50 [==============================] - 0s 709us/step - loss: 1258.2592 - mse: 2973611.0000 - mae: 1258.2593 - val_loss: 4982.2266 - val_mse: 25545888.0000 - val_mae: 4982.2266\n",
            "Epoch 24/200\n",
            "50/50 [==============================] - 0s 685us/step - loss: 1248.9757 - mse: 2960960.0000 - mae: 1248.9756 - val_loss: 4964.8301 - val_mse: 25383808.0000 - val_mae: 4964.8301\n",
            "Epoch 25/200\n",
            "50/50 [==============================] - 0s 802us/step - loss: 1233.1521 - mse: 2883814.5000 - mae: 1233.1521 - val_loss: 4944.2632 - val_mse: 25194236.0000 - val_mae: 4944.2632\n",
            "Epoch 26/200\n",
            "50/50 [==============================] - 0s 748us/step - loss: 1211.0933 - mse: 2801382.7500 - mae: 1211.0934 - val_loss: 4927.5054 - val_mse: 25036972.0000 - val_mae: 4927.5054\n",
            "Epoch 27/200\n",
            "50/50 [==============================] - 0s 912us/step - loss: 1225.2082 - mse: 2788086.7500 - mae: 1225.2083 - val_loss: 4909.1138 - val_mse: 24866308.0000 - val_mae: 4909.1138\n",
            "Epoch 28/200\n",
            "50/50 [==============================] - 0s 742us/step - loss: 1219.2433 - mse: 2819567.0000 - mae: 1219.2433 - val_loss: 4890.6660 - val_mse: 24695940.0000 - val_mae: 4890.6660\n",
            "Epoch 29/200\n",
            "50/50 [==============================] - 0s 710us/step - loss: 1191.5945 - mse: 2776733.0000 - mae: 1191.5944 - val_loss: 4872.4951 - val_mse: 24527800.0000 - val_mae: 4872.4951\n",
            "Epoch 30/200\n",
            "50/50 [==============================] - 0s 871us/step - loss: 1215.5707 - mse: 2805392.2500 - mae: 1215.5708 - val_loss: 4853.2207 - val_mse: 24351612.0000 - val_mae: 4853.2207\n",
            "Epoch 31/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1191.6364 - mse: 2737515.2500 - mae: 1191.6364 - val_loss: 4834.9092 - val_mse: 24185000.0000 - val_mae: 4834.9092\n",
            "Epoch 32/200\n",
            "50/50 [==============================] - 0s 706us/step - loss: 1163.6616 - mse: 2632431.2500 - mae: 1163.6616 - val_loss: 4813.7241 - val_mse: 23990926.0000 - val_mae: 4813.7241\n",
            "Epoch 33/200\n",
            "50/50 [==============================] - 0s 789us/step - loss: 1168.9139 - mse: 2577023.2500 - mae: 1168.9139 - val_loss: 4795.3271 - val_mse: 23822468.0000 - val_mae: 4795.3271\n",
            "Epoch 34/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1091.0892 - mse: 2437287.2500 - mae: 1091.0894 - val_loss: 4777.0952 - val_mse: 23656044.0000 - val_mae: 4777.0952\n",
            "Epoch 35/200\n",
            "50/50 [==============================] - 0s 752us/step - loss: 1103.3363 - mse: 2473982.5000 - mae: 1103.3363 - val_loss: 4759.0518 - val_mse: 23491716.0000 - val_mae: 4759.0518\n",
            "Epoch 36/200\n",
            "50/50 [==============================] - 0s 899us/step - loss: 1103.3289 - mse: 2429208.0000 - mae: 1103.3289 - val_loss: 4748.4585 - val_mse: 23391780.0000 - val_mae: 4748.4585\n",
            "Epoch 37/200\n",
            "50/50 [==============================] - 0s 975us/step - loss: 1132.3964 - mse: 2490301.2500 - mae: 1132.3964 - val_loss: 4731.3633 - val_mse: 23235308.0000 - val_mae: 4731.3633\n",
            "Epoch 38/200\n",
            "50/50 [==============================] - 0s 960us/step - loss: 1113.5863 - mse: 2442196.5000 - mae: 1113.5863 - val_loss: 4714.4297 - val_mse: 23080818.0000 - val_mae: 4714.4297\n",
            "Epoch 39/200\n",
            "50/50 [==============================] - 0s 763us/step - loss: 1062.8893 - mse: 2318913.5000 - mae: 1062.8892 - val_loss: 4696.3389 - val_mse: 22916768.0000 - val_mae: 4696.3389\n",
            "Epoch 40/200\n",
            "50/50 [==============================] - 0s 759us/step - loss: 1086.3428 - mse: 2249444.5000 - mae: 1086.3428 - val_loss: 4681.2334 - val_mse: 22777094.0000 - val_mae: 4681.2334\n",
            "Epoch 41/200\n",
            "50/50 [==============================] - 0s 716us/step - loss: 1106.8781 - mse: 2324468.7500 - mae: 1106.8779 - val_loss: 4672.2080 - val_mse: 22690382.0000 - val_mae: 4672.2080\n",
            "Epoch 42/200\n",
            "50/50 [==============================] - 0s 900us/step - loss: 1076.3524 - mse: 2242936.7500 - mae: 1076.3523 - val_loss: 4653.0420 - val_mse: 22517258.0000 - val_mae: 4653.0420\n",
            "Epoch 43/200\n",
            "50/50 [==============================] - 0s 881us/step - loss: 1104.3374 - mse: 2265286.7500 - mae: 1104.3374 - val_loss: 4642.4326 - val_mse: 22418546.0000 - val_mae: 4642.4326\n",
            "Epoch 44/200\n",
            "50/50 [==============================] - 0s 777us/step - loss: 1064.8883 - mse: 2283126.5000 - mae: 1064.8883 - val_loss: 4624.5381 - val_mse: 22258580.0000 - val_mae: 4624.5381\n",
            "Epoch 45/200\n",
            "50/50 [==============================] - 0s 815us/step - loss: 1068.2266 - mse: 2309279.0000 - mae: 1068.2267 - val_loss: 4614.3125 - val_mse: 22164636.0000 - val_mae: 4614.3125\n",
            "Epoch 46/200\n",
            "50/50 [==============================] - 0s 716us/step - loss: 1083.9086 - mse: 2195932.0000 - mae: 1083.9086 - val_loss: 4598.2217 - val_mse: 22019612.0000 - val_mae: 4598.2217\n",
            "Epoch 47/200\n",
            "50/50 [==============================] - 0s 661us/step - loss: 1077.6646 - mse: 2221346.5000 - mae: 1077.6646 - val_loss: 4596.1753 - val_mse: 21995344.0000 - val_mae: 4596.1753\n",
            "Epoch 48/200\n",
            "50/50 [==============================] - 0s 773us/step - loss: 1036.2979 - mse: 2095433.6250 - mae: 1036.2979 - val_loss: 4588.5757 - val_mse: 21923748.0000 - val_mae: 4588.5757\n",
            "Epoch 49/200\n",
            "50/50 [==============================] - 0s 828us/step - loss: 1041.4664 - mse: 2108050.2500 - mae: 1041.4664 - val_loss: 4573.7979 - val_mse: 21791406.0000 - val_mae: 4573.7979\n",
            "Epoch 50/200\n",
            "50/50 [==============================] - 0s 913us/step - loss: 1058.3914 - mse: 2059405.1250 - mae: 1058.3914 - val_loss: 4556.6392 - val_mse: 21638304.0000 - val_mae: 4556.6392\n",
            "Epoch 51/200\n",
            "50/50 [==============================] - 0s 868us/step - loss: 1005.5410 - mse: 1999131.8750 - mae: 1005.5410 - val_loss: 4544.1812 - val_mse: 21526652.0000 - val_mae: 4544.1812\n",
            "Epoch 52/200\n",
            "50/50 [==============================] - 0s 878us/step - loss: 1019.4635 - mse: 1986737.2500 - mae: 1019.4636 - val_loss: 4535.3750 - val_mse: 21444938.0000 - val_mae: 4535.3750\n",
            "Epoch 53/200\n",
            "50/50 [==============================] - 0s 857us/step - loss: 1047.0538 - mse: 2108120.7500 - mae: 1047.0537 - val_loss: 4532.8701 - val_mse: 21417826.0000 - val_mae: 4532.8701\n",
            "Epoch 54/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1036.7633 - mse: 2003526.5000 - mae: 1036.7633 - val_loss: 4524.8843 - val_mse: 21343622.0000 - val_mae: 4524.8843\n",
            "Epoch 55/200\n",
            "50/50 [==============================] - 0s 834us/step - loss: 1006.2272 - mse: 1910386.2500 - mae: 1006.2272 - val_loss: 4501.6743 - val_mse: 21141122.0000 - val_mae: 4501.6743\n",
            "Epoch 56/200\n",
            "50/50 [==============================] - 0s 895us/step - loss: 1019.1806 - mse: 1940529.1250 - mae: 1019.1805 - val_loss: 4487.8906 - val_mse: 21018394.0000 - val_mae: 4487.8906\n",
            "Epoch 57/200\n",
            "50/50 [==============================] - 0s 874us/step - loss: 1053.5875 - mse: 2043541.7500 - mae: 1053.5875 - val_loss: 4482.9053 - val_mse: 20970408.0000 - val_mae: 4482.9053\n",
            "Epoch 58/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1059.2385 - mse: 2018794.5000 - mae: 1059.2384 - val_loss: 4479.7222 - val_mse: 20937186.0000 - val_mae: 4479.7222\n",
            "Epoch 59/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1053.4918 - mse: 1976155.5000 - mae: 1053.4917 - val_loss: 4475.0322 - val_mse: 20891892.0000 - val_mae: 4475.0322\n",
            "Epoch 60/200\n",
            "50/50 [==============================] - 0s 775us/step - loss: 1075.7172 - mse: 2013084.5000 - mae: 1075.7172 - val_loss: 4469.5947 - val_mse: 20840700.0000 - val_mae: 4469.5947\n",
            "Epoch 61/200\n",
            "50/50 [==============================] - 0s 756us/step - loss: 1121.9402 - mse: 2154095.2500 - mae: 1121.9402 - val_loss: 4467.4580 - val_mse: 20817894.0000 - val_mae: 4467.4580\n",
            "Epoch 62/200\n",
            "50/50 [==============================] - 0s 857us/step - loss: 986.7162 - mse: 1884211.6250 - mae: 986.7162 - val_loss: 4455.2354 - val_mse: 20710432.0000 - val_mae: 4455.2354\n",
            "Epoch 63/200\n",
            "50/50 [==============================] - 0s 859us/step - loss: 1038.8874 - mse: 1978048.6250 - mae: 1038.8873 - val_loss: 4443.8120 - val_mse: 20609680.0000 - val_mae: 4443.8120\n",
            "Epoch 64/200\n",
            "50/50 [==============================] - 0s 807us/step - loss: 956.2529 - mse: 1792187.6250 - mae: 956.2530 - val_loss: 4434.8813 - val_mse: 20529296.0000 - val_mae: 4434.8813\n",
            "Epoch 65/200\n",
            "50/50 [==============================] - 0s 816us/step - loss: 1023.9208 - mse: 1926185.2500 - mae: 1023.9208 - val_loss: 4429.1958 - val_mse: 20476998.0000 - val_mae: 4429.1958\n",
            "Epoch 66/200\n",
            "50/50 [==============================] - 0s 753us/step - loss: 1093.2600 - mse: 2034765.0000 - mae: 1093.2600 - val_loss: 4432.5063 - val_mse: 20500286.0000 - val_mae: 4432.5063\n",
            "Epoch 67/200\n",
            "50/50 [==============================] - 0s 733us/step - loss: 1048.0065 - mse: 2010707.8750 - mae: 1048.0065 - val_loss: 4437.0620 - val_mse: 20533566.0000 - val_mae: 4437.0620\n",
            "Epoch 68/200\n",
            "50/50 [==============================] - 0s 824us/step - loss: 1008.0121 - mse: 1846461.7500 - mae: 1008.0120 - val_loss: 4432.9414 - val_mse: 20494484.0000 - val_mae: 4432.9414\n",
            "Epoch 69/200\n",
            "50/50 [==============================] - 0s 789us/step - loss: 1016.8775 - mse: 1875182.1250 - mae: 1016.8774 - val_loss: 4428.9619 - val_mse: 20457058.0000 - val_mae: 4428.9619\n",
            "Epoch 70/200\n",
            "50/50 [==============================] - 0s 735us/step - loss: 1075.5338 - mse: 1991517.5000 - mae: 1075.5339 - val_loss: 4424.5166 - val_mse: 20416268.0000 - val_mae: 4424.5166\n",
            "Epoch 71/200\n",
            "50/50 [==============================] - 0s 888us/step - loss: 992.1168 - mse: 1783478.3750 - mae: 992.1169 - val_loss: 4411.7900 - val_mse: 20305808.0000 - val_mae: 4411.7900\n",
            "Epoch 72/200\n",
            "50/50 [==============================] - 0s 826us/step - loss: 1008.8743 - mse: 1812722.5000 - mae: 1008.8743 - val_loss: 4407.9775 - val_mse: 20270228.0000 - val_mae: 4407.9775\n",
            "Epoch 73/200\n",
            "50/50 [==============================] - 0s 772us/step - loss: 1016.0179 - mse: 1826056.6250 - mae: 1016.0179 - val_loss: 4397.9067 - val_mse: 20182112.0000 - val_mae: 4397.9067\n",
            "Epoch 74/200\n",
            "50/50 [==============================] - 0s 813us/step - loss: 1047.1859 - mse: 1908741.1250 - mae: 1047.1859 - val_loss: 4389.0029 - val_mse: 20103868.0000 - val_mae: 4389.0029\n",
            "Epoch 75/200\n",
            "50/50 [==============================] - 0s 856us/step - loss: 1027.4130 - mse: 1814057.0000 - mae: 1027.4130 - val_loss: 4384.0469 - val_mse: 20058988.0000 - val_mae: 4384.0469\n",
            "Epoch 76/200\n",
            "50/50 [==============================] - 0s 779us/step - loss: 1030.7297 - mse: 1854653.5000 - mae: 1030.7297 - val_loss: 4377.3511 - val_mse: 19999268.0000 - val_mae: 4377.3511\n",
            "Epoch 77/200\n",
            "50/50 [==============================] - 0s 806us/step - loss: 916.5894 - mse: 1586903.0000 - mae: 916.5894 - val_loss: 4370.5146 - val_mse: 19938114.0000 - val_mae: 4370.5146\n",
            "Epoch 78/200\n",
            "50/50 [==============================] - 0s 775us/step - loss: 1017.5841 - mse: 1797313.0000 - mae: 1017.5842 - val_loss: 4365.2041 - val_mse: 19890724.0000 - val_mae: 4365.2041\n",
            "Epoch 79/200\n",
            "50/50 [==============================] - 0s 942us/step - loss: 1013.6453 - mse: 1800416.0000 - mae: 1013.6453 - val_loss: 4359.3130 - val_mse: 19838068.0000 - val_mae: 4359.3130\n",
            "Epoch 80/200\n",
            "50/50 [==============================] - 0s 757us/step - loss: 971.3574 - mse: 1835727.8750 - mae: 971.3574 - val_loss: 4351.8418 - val_mse: 19773408.0000 - val_mae: 4351.8418\n",
            "Epoch 81/200\n",
            "50/50 [==============================] - 0s 971us/step - loss: 990.7012 - mse: 1728156.6250 - mae: 990.7012 - val_loss: 4348.2773 - val_mse: 19740562.0000 - val_mae: 4348.2773\n",
            "Epoch 82/200\n",
            "50/50 [==============================] - 0s 729us/step - loss: 981.5821 - mse: 1634471.3750 - mae: 981.5821 - val_loss: 4336.9404 - val_mse: 19643302.0000 - val_mae: 4336.9404\n",
            "Epoch 83/200\n",
            "50/50 [==============================] - 0s 703us/step - loss: 1040.5804 - mse: 1832714.5000 - mae: 1040.5804 - val_loss: 4335.2993 - val_mse: 19626140.0000 - val_mae: 4335.2993\n",
            "Epoch 84/200\n",
            "50/50 [==============================] - 0s 720us/step - loss: 959.9271 - mse: 1689248.1250 - mae: 959.9271 - val_loss: 4326.4360 - val_mse: 19549302.0000 - val_mae: 4326.4360\n",
            "Epoch 85/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1021.4116 - mse: 1783509.5000 - mae: 1021.4116 - val_loss: 4323.3247 - val_mse: 19520326.0000 - val_mae: 4323.3247\n",
            "Epoch 86/200\n",
            "50/50 [==============================] - 0s 831us/step - loss: 1041.4488 - mse: 1889400.7500 - mae: 1041.4489 - val_loss: 4323.3901 - val_mse: 19518714.0000 - val_mae: 4323.3901\n",
            "Epoch 87/200\n",
            "50/50 [==============================] - 0s 927us/step - loss: 1042.9105 - mse: 1911690.1250 - mae: 1042.9105 - val_loss: 4321.6826 - val_mse: 19502502.0000 - val_mae: 4321.6826\n",
            "Epoch 88/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1024.2101 - mse: 1757462.1250 - mae: 1024.2102 - val_loss: 4319.6055 - val_mse: 19482804.0000 - val_mae: 4319.6055\n",
            "Epoch 89/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 958.0868 - mse: 1692167.5000 - mae: 958.0868 - val_loss: 4319.0830 - val_mse: 19475678.0000 - val_mae: 4319.0830\n",
            "Epoch 90/200\n",
            "50/50 [==============================] - 0s 884us/step - loss: 940.5961 - mse: 1651269.8750 - mae: 940.5960 - val_loss: 4314.1821 - val_mse: 19433100.0000 - val_mae: 4314.1821\n",
            "Epoch 91/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 910.2121 - mse: 1579140.7500 - mae: 910.2120 - val_loss: 4297.1650 - val_mse: 19291244.0000 - val_mae: 4297.1650\n",
            "Epoch 92/200\n",
            "50/50 [==============================] - 0s 936us/step - loss: 996.4383 - mse: 1608146.5000 - mae: 996.4383 - val_loss: 4289.6807 - val_mse: 19226782.0000 - val_mae: 4289.6807\n",
            "Epoch 93/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1019.1329 - mse: 1801689.6250 - mae: 1019.1328 - val_loss: 4288.6260 - val_mse: 19215100.0000 - val_mae: 4288.6260\n",
            "Epoch 94/200\n",
            "50/50 [==============================] - 0s 753us/step - loss: 956.4040 - mse: 1731592.1250 - mae: 956.4041 - val_loss: 4277.9204 - val_mse: 19125034.0000 - val_mae: 4277.9204\n",
            "Epoch 95/200\n",
            "50/50 [==============================] - 0s 849us/step - loss: 985.9725 - mse: 1680153.0000 - mae: 985.9725 - val_loss: 4278.2295 - val_mse: 19125204.0000 - val_mae: 4278.2295\n",
            "Epoch 96/200\n",
            "50/50 [==============================] - 0s 904us/step - loss: 1000.5291 - mse: 1738848.3750 - mae: 1000.5291 - val_loss: 4281.3047 - val_mse: 19147816.0000 - val_mae: 4281.3047\n",
            "Epoch 97/200\n",
            "50/50 [==============================] - 0s 846us/step - loss: 975.3512 - mse: 1814402.8750 - mae: 975.3513 - val_loss: 4281.6323 - val_mse: 19148438.0000 - val_mae: 4281.6323\n",
            "Epoch 98/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 977.2889 - mse: 1640520.6250 - mae: 977.2889 - val_loss: 4288.6045 - val_mse: 19203172.0000 - val_mae: 4288.6045\n",
            "Epoch 99/200\n",
            "50/50 [==============================] - 0s 971us/step - loss: 1041.6177 - mse: 1661271.0000 - mae: 1041.6177 - val_loss: 4291.1904 - val_mse: 19222348.0000 - val_mae: 4291.1904\n",
            "Epoch 100/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1007.9606 - mse: 1767872.6250 - mae: 1007.9606 - val_loss: 4287.9263 - val_mse: 19194206.0000 - val_mae: 4287.9263\n",
            "Epoch 101/200\n",
            "50/50 [==============================] - 0s 879us/step - loss: 975.4857 - mse: 1641928.5000 - mae: 975.4856 - val_loss: 4281.9927 - val_mse: 19143692.0000 - val_mae: 4281.9927\n",
            "Epoch 102/200\n",
            "50/50 [==============================] - 0s 738us/step - loss: 921.3607 - mse: 1494856.3750 - mae: 921.3608 - val_loss: 4272.8398 - val_mse: 19066570.0000 - val_mae: 4272.8398\n",
            "Epoch 103/200\n",
            "50/50 [==============================] - 0s 805us/step - loss: 1022.8625 - mse: 1654855.0000 - mae: 1022.8624 - val_loss: 4273.5869 - val_mse: 19071690.0000 - val_mae: 4273.5869\n",
            "Epoch 104/200\n",
            "50/50 [==============================] - 0s 787us/step - loss: 1007.0244 - mse: 1762972.7500 - mae: 1007.0245 - val_loss: 4270.1299 - val_mse: 19042250.0000 - val_mae: 4270.1299\n",
            "Epoch 105/200\n",
            "50/50 [==============================] - 0s 863us/step - loss: 1004.0763 - mse: 1700651.5000 - mae: 1004.0764 - val_loss: 4263.9761 - val_mse: 18990860.0000 - val_mae: 4263.9761\n",
            "Epoch 106/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1048.4830 - mse: 1888161.8750 - mae: 1048.4829 - val_loss: 4266.3472 - val_mse: 19008518.0000 - val_mae: 4266.3472\n",
            "Epoch 107/200\n",
            "50/50 [==============================] - 0s 949us/step - loss: 1000.9639 - mse: 1637457.0000 - mae: 1000.9640 - val_loss: 4262.4727 - val_mse: 18975436.0000 - val_mae: 4262.4727\n",
            "Epoch 108/200\n",
            "50/50 [==============================] - 0s 819us/step - loss: 1034.7404 - mse: 1680130.5000 - mae: 1034.7405 - val_loss: 4262.7124 - val_mse: 18975540.0000 - val_mae: 4262.7124\n",
            "Epoch 109/200\n",
            "50/50 [==============================] - 0s 946us/step - loss: 999.1135 - mse: 1666736.0000 - mae: 999.1135 - val_loss: 4258.5107 - val_mse: 18940522.0000 - val_mae: 4258.5107\n",
            "Epoch 110/200\n",
            "50/50 [==============================] - 0s 933us/step - loss: 978.9201 - mse: 1580402.1250 - mae: 978.9200 - val_loss: 4250.3696 - val_mse: 18872970.0000 - val_mae: 4250.3696\n",
            "Epoch 111/200\n",
            "50/50 [==============================] - 0s 816us/step - loss: 1006.0040 - mse: 1694466.7500 - mae: 1006.0041 - val_loss: 4252.5098 - val_mse: 18889242.0000 - val_mae: 4252.5098\n",
            "Epoch 112/200\n",
            "50/50 [==============================] - 0s 963us/step - loss: 1020.7427 - mse: 1777463.5000 - mae: 1020.7426 - val_loss: 4252.7959 - val_mse: 18890252.0000 - val_mae: 4252.7959\n",
            "Epoch 113/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 982.9533 - mse: 1724854.3750 - mae: 982.9533 - val_loss: 4251.8833 - val_mse: 18881720.0000 - val_mae: 4251.8833\n",
            "Epoch 114/200\n",
            "50/50 [==============================] - 0s 702us/step - loss: 994.6940 - mse: 1720732.1250 - mae: 994.6940 - val_loss: 4252.7573 - val_mse: 18887822.0000 - val_mae: 4252.7573\n",
            "Epoch 115/200\n",
            "50/50 [==============================] - 0s 817us/step - loss: 991.9588 - mse: 1559051.8750 - mae: 991.9588 - val_loss: 4253.9062 - val_mse: 18895840.0000 - val_mae: 4253.9062\n",
            "Epoch 116/200\n",
            "50/50 [==============================] - 0s 951us/step - loss: 997.3570 - mse: 1688345.2500 - mae: 997.3569 - val_loss: 4253.2524 - val_mse: 18889094.0000 - val_mae: 4253.2524\n",
            "Epoch 117/200\n",
            "50/50 [==============================] - 0s 974us/step - loss: 969.1085 - mse: 1714955.0000 - mae: 969.1085 - val_loss: 4252.9268 - val_mse: 18885182.0000 - val_mae: 4252.9268\n",
            "Epoch 118/200\n",
            "50/50 [==============================] - 0s 955us/step - loss: 1009.8553 - mse: 1764801.0000 - mae: 1009.8553 - val_loss: 4254.7344 - val_mse: 18898484.0000 - val_mae: 4254.7344\n",
            "Epoch 119/200\n",
            "50/50 [==============================] - 0s 796us/step - loss: 1064.6878 - mse: 1803630.1250 - mae: 1064.6877 - val_loss: 4255.1396 - val_mse: 18900816.0000 - val_mae: 4255.1396\n",
            "Epoch 120/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 948.5803 - mse: 1558060.6250 - mae: 948.5803 - val_loss: 4256.1514 - val_mse: 18908024.0000 - val_mae: 4256.1514\n",
            "Epoch 121/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1008.8878 - mse: 1835346.2500 - mae: 1008.8878 - val_loss: 4251.0874 - val_mse: 18865764.0000 - val_mae: 4251.0874\n",
            "Epoch 122/200\n",
            "50/50 [==============================] - 0s 894us/step - loss: 956.8470 - mse: 1577087.8750 - mae: 956.8470 - val_loss: 4249.9033 - val_mse: 18855042.0000 - val_mae: 4249.9033\n",
            "Epoch 123/200\n",
            "50/50 [==============================] - 0s 760us/step - loss: 972.8609 - mse: 1615448.3750 - mae: 972.8610 - val_loss: 4247.2471 - val_mse: 18832130.0000 - val_mae: 4247.2471\n",
            "Epoch 124/200\n",
            "50/50 [==============================] - 0s 983us/step - loss: 1000.9246 - mse: 1746100.3750 - mae: 1000.9246 - val_loss: 4245.4468 - val_mse: 18816478.0000 - val_mae: 4245.4468\n",
            "Epoch 125/200\n",
            "50/50 [==============================] - 0s 922us/step - loss: 965.0540 - mse: 1601289.1250 - mae: 965.0540 - val_loss: 4243.4614 - val_mse: 18799150.0000 - val_mae: 4243.4614\n",
            "Epoch 126/200\n",
            "50/50 [==============================] - 0s 977us/step - loss: 964.7769 - mse: 1671804.0000 - mae: 964.7770 - val_loss: 4245.1802 - val_mse: 18812260.0000 - val_mae: 4245.1802\n",
            "Epoch 127/200\n",
            "50/50 [==============================] - 0s 815us/step - loss: 924.9000 - mse: 1551821.7500 - mae: 924.9000 - val_loss: 4249.4561 - val_mse: 18846096.0000 - val_mae: 4249.4561\n",
            "Epoch 128/200\n",
            "50/50 [==============================] - 0s 809us/step - loss: 997.9790 - mse: 1633266.8750 - mae: 997.9791 - val_loss: 4253.6143 - val_mse: 18878668.0000 - val_mae: 4253.6143\n",
            "Epoch 129/200\n",
            "50/50 [==============================] - 0s 994us/step - loss: 931.0003 - mse: 1483799.5000 - mae: 931.0003 - val_loss: 4249.7739 - val_mse: 18846244.0000 - val_mae: 4249.7739\n",
            "Epoch 130/200\n",
            "50/50 [==============================] - 0s 979us/step - loss: 888.6221 - mse: 1607900.1250 - mae: 888.6220 - val_loss: 4240.7700 - val_mse: 18772028.0000 - val_mae: 4240.7700\n",
            "Epoch 131/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 902.9080 - mse: 1573027.5000 - mae: 902.9080 - val_loss: 4232.2461 - val_mse: 18701524.0000 - val_mae: 4232.2461\n",
            "Epoch 132/200\n",
            "50/50 [==============================] - 0s 749us/step - loss: 1014.3605 - mse: 1678607.3750 - mae: 1014.3605 - val_loss: 4233.3423 - val_mse: 18709350.0000 - val_mae: 4233.3423\n",
            "Epoch 133/200\n",
            "50/50 [==============================] - 0s 785us/step - loss: 921.1536 - mse: 1537943.5000 - mae: 921.1535 - val_loss: 4228.4912 - val_mse: 18668688.0000 - val_mae: 4228.4912\n",
            "Epoch 134/200\n",
            "50/50 [==============================] - 0s 770us/step - loss: 919.8559 - mse: 1595072.6250 - mae: 919.8560 - val_loss: 4220.5596 - val_mse: 18603478.0000 - val_mae: 4220.5596\n",
            "Epoch 135/200\n",
            "50/50 [==============================] - 0s 940us/step - loss: 931.0478 - mse: 1482269.5000 - mae: 931.0478 - val_loss: 4216.0020 - val_mse: 18565400.0000 - val_mae: 4216.0020\n",
            "Epoch 136/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 978.9846 - mse: 1648413.7500 - mae: 978.9846 - val_loss: 4214.6689 - val_mse: 18553032.0000 - val_mae: 4214.6689\n",
            "Epoch 137/200\n",
            "50/50 [==============================] - 0s 951us/step - loss: 1026.1012 - mse: 1636928.6250 - mae: 1026.1012 - val_loss: 4208.9385 - val_mse: 18505728.0000 - val_mae: 4208.9385\n",
            "Epoch 138/200\n",
            "50/50 [==============================] - 0s 968us/step - loss: 962.5446 - mse: 1567946.2500 - mae: 962.5447 - val_loss: 4210.0674 - val_mse: 18513564.0000 - val_mae: 4210.0674\n",
            "Epoch 139/200\n",
            "50/50 [==============================] - 0s 927us/step - loss: 1038.6627 - mse: 1666688.3750 - mae: 1038.6626 - val_loss: 4211.2515 - val_mse: 18521994.0000 - val_mae: 4211.2515\n",
            "Epoch 140/200\n",
            "50/50 [==============================] - 0s 851us/step - loss: 1006.3045 - mse: 1642385.8750 - mae: 1006.3044 - val_loss: 4205.0410 - val_mse: 18470700.0000 - val_mae: 4205.0410\n",
            "Epoch 141/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1020.7056 - mse: 1684165.1250 - mae: 1020.7056 - val_loss: 4210.3525 - val_mse: 18512420.0000 - val_mae: 4210.3525\n",
            "Epoch 142/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 964.4707 - mse: 1642380.1250 - mae: 964.4706 - val_loss: 4203.8750 - val_mse: 18458844.0000 - val_mae: 4203.8750\n",
            "Epoch 143/200\n",
            "50/50 [==============================] - 0s 720us/step - loss: 915.5482 - mse: 1560668.6250 - mae: 915.5481 - val_loss: 4198.3506 - val_mse: 18413238.0000 - val_mae: 4198.3506\n",
            "Epoch 144/200\n",
            "50/50 [==============================] - 0s 863us/step - loss: 969.0212 - mse: 1562286.5000 - mae: 969.0212 - val_loss: 4202.3726 - val_mse: 18444200.0000 - val_mae: 4202.3726\n",
            "Epoch 145/200\n",
            "50/50 [==============================] - 0s 818us/step - loss: 924.6032 - mse: 1390111.3750 - mae: 924.6033 - val_loss: 4197.3696 - val_mse: 18402540.0000 - val_mae: 4197.3696\n",
            "Epoch 146/200\n",
            "50/50 [==============================] - 0s 779us/step - loss: 958.4186 - mse: 1523499.5000 - mae: 958.4186 - val_loss: 4195.2153 - val_mse: 18383808.0000 - val_mae: 4195.2153\n",
            "Epoch 147/200\n",
            "50/50 [==============================] - 0s 804us/step - loss: 1101.9788 - mse: 1942310.3750 - mae: 1101.9789 - val_loss: 4198.0415 - val_mse: 18405434.0000 - val_mae: 4198.0415\n",
            "Epoch 148/200\n",
            "50/50 [==============================] - 0s 708us/step - loss: 957.2939 - mse: 1670357.1250 - mae: 957.2939 - val_loss: 4189.0239 - val_mse: 18331748.0000 - val_mae: 4189.0239\n",
            "Epoch 149/200\n",
            "50/50 [==============================] - 0s 766us/step - loss: 872.2662 - mse: 1305713.8750 - mae: 872.2662 - val_loss: 4182.3735 - val_mse: 18277264.0000 - val_mae: 4182.3735\n",
            "Epoch 150/200\n",
            "50/50 [==============================] - 0s 758us/step - loss: 958.2767 - mse: 1512824.6250 - mae: 958.2767 - val_loss: 4178.1279 - val_mse: 18241858.0000 - val_mae: 4178.1279\n",
            "Epoch 151/200\n",
            "50/50 [==============================] - 0s 873us/step - loss: 967.8975 - mse: 1593902.8750 - mae: 967.8975 - val_loss: 4182.1689 - val_mse: 18273138.0000 - val_mae: 4182.1689\n",
            "Epoch 152/200\n",
            "50/50 [==============================] - 0s 831us/step - loss: 1002.9337 - mse: 1625914.3750 - mae: 1002.9338 - val_loss: 4178.4189 - val_mse: 18241900.0000 - val_mae: 4178.4189\n",
            "Epoch 153/200\n",
            "50/50 [==============================] - 0s 753us/step - loss: 990.4399 - mse: 1644324.1250 - mae: 990.4399 - val_loss: 4184.8516 - val_mse: 18292488.0000 - val_mae: 4184.8516\n",
            "Epoch 154/200\n",
            "50/50 [==============================] - 0s 769us/step - loss: 956.7614 - mse: 1606311.6250 - mae: 956.7615 - val_loss: 4177.3101 - val_mse: 18231180.0000 - val_mae: 4177.3101\n",
            "Epoch 155/200\n",
            "50/50 [==============================] - 0s 873us/step - loss: 1018.4420 - mse: 1701426.7500 - mae: 1018.4420 - val_loss: 4179.7910 - val_mse: 18250158.0000 - val_mae: 4179.7910\n",
            "Epoch 156/200\n",
            "50/50 [==============================] - 0s 894us/step - loss: 945.1078 - mse: 1561765.8750 - mae: 945.1078 - val_loss: 4177.5474 - val_mse: 18231254.0000 - val_mae: 4177.5474\n",
            "Epoch 157/200\n",
            "50/50 [==============================] - 0s 817us/step - loss: 1026.4384 - mse: 1663739.8750 - mae: 1026.4384 - val_loss: 4176.9907 - val_mse: 18225766.0000 - val_mae: 4176.9907\n",
            "Epoch 158/200\n",
            "50/50 [==============================] - 0s 909us/step - loss: 973.9016 - mse: 1534567.0000 - mae: 973.9017 - val_loss: 4171.9521 - val_mse: 18184284.0000 - val_mae: 4171.9521\n",
            "Epoch 159/200\n",
            "50/50 [==============================] - 0s 966us/step - loss: 1002.6957 - mse: 1650446.5000 - mae: 1002.6958 - val_loss: 4169.6943 - val_mse: 18165260.0000 - val_mae: 4169.6943\n",
            "Epoch 160/200\n",
            "50/50 [==============================] - 0s 870us/step - loss: 1016.0939 - mse: 1711259.3750 - mae: 1016.0939 - val_loss: 4169.6396 - val_mse: 18163970.0000 - val_mae: 4169.6396\n",
            "Epoch 161/200\n",
            "50/50 [==============================] - 0s 947us/step - loss: 961.9605 - mse: 1587345.7500 - mae: 961.9604 - val_loss: 4165.1836 - val_mse: 18127140.0000 - val_mae: 4165.1836\n",
            "Epoch 162/200\n",
            "50/50 [==============================] - 0s 908us/step - loss: 952.4127 - mse: 1493784.1250 - mae: 952.4127 - val_loss: 4164.0425 - val_mse: 18116874.0000 - val_mae: 4164.0425\n",
            "Epoch 163/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 972.0307 - mse: 1568005.6250 - mae: 972.0306 - val_loss: 4166.2666 - val_mse: 18133776.0000 - val_mae: 4166.2666\n",
            "Epoch 164/200\n",
            "50/50 [==============================] - 0s 759us/step - loss: 994.6465 - mse: 1662123.5000 - mae: 994.6465 - val_loss: 4162.5547 - val_mse: 18102984.0000 - val_mae: 4162.5547\n",
            "Epoch 165/200\n",
            "50/50 [==============================] - 0s 774us/step - loss: 927.3016 - mse: 1455802.7500 - mae: 927.3015 - val_loss: 4154.4043 - val_mse: 18036676.0000 - val_mae: 4154.4043\n",
            "Epoch 166/200\n",
            "50/50 [==============================] - 0s 830us/step - loss: 1014.7055 - mse: 1676548.5000 - mae: 1014.7056 - val_loss: 4155.8228 - val_mse: 18046900.0000 - val_mae: 4155.8228\n",
            "Epoch 167/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 990.1961 - mse: 1703367.5000 - mae: 990.1962 - val_loss: 4154.4868 - val_mse: 18035382.0000 - val_mae: 4154.4868\n",
            "Epoch 168/200\n",
            "50/50 [==============================] - 0s 896us/step - loss: 939.6785 - mse: 1464281.8750 - mae: 939.6785 - val_loss: 4152.7915 - val_mse: 18021054.0000 - val_mae: 4152.7915\n",
            "Epoch 169/200\n",
            "50/50 [==============================] - 0s 912us/step - loss: 916.2185 - mse: 1521001.5000 - mae: 916.2185 - val_loss: 4148.1616 - val_mse: 17983300.0000 - val_mae: 4148.1616\n",
            "Epoch 170/200\n",
            "50/50 [==============================] - 0s 767us/step - loss: 923.5261 - mse: 1517208.0000 - mae: 923.5262 - val_loss: 4138.5024 - val_mse: 17905388.0000 - val_mae: 4138.5024\n",
            "Epoch 171/200\n",
            "50/50 [==============================] - 0s 991us/step - loss: 953.1868 - mse: 1373615.3750 - mae: 953.1868 - val_loss: 4131.8247 - val_mse: 17851254.0000 - val_mae: 4131.8247\n",
            "Epoch 172/200\n",
            "50/50 [==============================] - 0s 886us/step - loss: 943.6703 - mse: 1430550.7500 - mae: 943.6704 - val_loss: 4127.8218 - val_mse: 17818280.0000 - val_mae: 4127.8218\n",
            "Epoch 173/200\n",
            "50/50 [==============================] - 0s 770us/step - loss: 918.2519 - mse: 1468768.3750 - mae: 918.2518 - val_loss: 4130.5928 - val_mse: 17839306.0000 - val_mae: 4130.5928\n",
            "Epoch 174/200\n",
            "50/50 [==============================] - 0s 832us/step - loss: 898.2131 - mse: 1359324.3750 - mae: 898.2131 - val_loss: 4127.6445 - val_mse: 17814682.0000 - val_mae: 4127.6445\n",
            "Epoch 175/200\n",
            "50/50 [==============================] - 0s 941us/step - loss: 1020.3715 - mse: 1586250.8750 - mae: 1020.3715 - val_loss: 4127.3276 - val_mse: 17811094.0000 - val_mae: 4127.3276\n",
            "Epoch 176/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 953.3347 - mse: 1550424.0000 - mae: 953.3346 - val_loss: 4128.2822 - val_mse: 17817792.0000 - val_mae: 4128.2822\n",
            "Epoch 177/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 963.2911 - mse: 1472815.6250 - mae: 963.2912 - val_loss: 4124.3530 - val_mse: 17785810.0000 - val_mae: 4124.3530\n",
            "Epoch 178/200\n",
            "50/50 [==============================] - 0s 794us/step - loss: 967.1820 - mse: 1589365.7500 - mae: 967.1820 - val_loss: 4126.4780 - val_mse: 17801900.0000 - val_mae: 4126.4780\n",
            "Epoch 179/200\n",
            "50/50 [==============================] - 0s 937us/step - loss: 1033.7989 - mse: 1631321.8750 - mae: 1033.7990 - val_loss: 4131.0640 - val_mse: 17837554.0000 - val_mae: 4131.0640\n",
            "Epoch 180/200\n",
            "50/50 [==============================] - 0s 991us/step - loss: 1030.1484 - mse: 1645960.7500 - mae: 1030.1483 - val_loss: 4132.4478 - val_mse: 17847824.0000 - val_mae: 4132.4478\n",
            "Epoch 181/200\n",
            "50/50 [==============================] - 0s 862us/step - loss: 871.1209 - mse: 1241784.7500 - mae: 871.1209 - val_loss: 4129.4399 - val_mse: 17822876.0000 - val_mae: 4129.4399\n",
            "Epoch 182/200\n",
            "50/50 [==============================] - 0s 863us/step - loss: 1006.8227 - mse: 1596954.5000 - mae: 1006.8226 - val_loss: 4125.9321 - val_mse: 17794262.0000 - val_mae: 4125.9321\n",
            "Epoch 183/200\n",
            "50/50 [==============================] - 0s 780us/step - loss: 1086.8390 - mse: 1789076.5000 - mae: 1086.8390 - val_loss: 4128.1655 - val_mse: 17811334.0000 - val_mae: 4128.1655\n",
            "Epoch 184/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 891.2661 - mse: 1393857.8750 - mae: 891.2661 - val_loss: 4122.8389 - val_mse: 17768372.0000 - val_mae: 4122.8389\n",
            "Epoch 185/200\n",
            "50/50 [==============================] - 0s 919us/step - loss: 983.6407 - mse: 1721260.1250 - mae: 983.6406 - val_loss: 4122.6611 - val_mse: 17766116.0000 - val_mae: 4122.6611\n",
            "Epoch 186/200\n",
            "50/50 [==============================] - 0s 873us/step - loss: 922.0694 - mse: 1454478.8750 - mae: 922.0694 - val_loss: 4127.6299 - val_mse: 17804908.0000 - val_mae: 4127.6299\n",
            "Epoch 187/200\n",
            "50/50 [==============================] - 0s 939us/step - loss: 985.4631 - mse: 1526789.6250 - mae: 985.4631 - val_loss: 4133.0254 - val_mse: 17846980.0000 - val_mae: 4133.0254\n",
            "Epoch 188/200\n",
            "50/50 [==============================] - 0s 960us/step - loss: 993.5071 - mse: 1520469.7500 - mae: 993.5072 - val_loss: 4137.6162 - val_mse: 17883122.0000 - val_mae: 4137.6162\n",
            "Epoch 189/200\n",
            "50/50 [==============================] - 0s 911us/step - loss: 954.0071 - mse: 1530417.5000 - mae: 954.0072 - val_loss: 4129.2773 - val_mse: 17815722.0000 - val_mae: 4129.2773\n",
            "Epoch 190/200\n",
            "50/50 [==============================] - 0s 872us/step - loss: 923.1142 - mse: 1395958.2500 - mae: 923.1142 - val_loss: 4126.4019 - val_mse: 17792212.0000 - val_mae: 4126.4019\n",
            "Epoch 191/200\n",
            "50/50 [==============================] - 0s 885us/step - loss: 1025.4189 - mse: 1698837.1250 - mae: 1025.4189 - val_loss: 4127.4048 - val_mse: 17799560.0000 - val_mae: 4127.4048\n",
            "Epoch 192/200\n",
            "50/50 [==============================] - 0s 815us/step - loss: 903.3743 - mse: 1545776.5000 - mae: 903.3742 - val_loss: 4123.6992 - val_mse: 17769248.0000 - val_mae: 4123.6992\n",
            "Epoch 193/200\n",
            "50/50 [==============================] - 0s 966us/step - loss: 1008.7704 - mse: 1638255.0000 - mae: 1008.7704 - val_loss: 4128.1812 - val_mse: 17804038.0000 - val_mae: 4128.1812\n",
            "Epoch 194/200\n",
            "50/50 [==============================] - 0s 781us/step - loss: 914.9811 - mse: 1398203.3750 - mae: 914.9811 - val_loss: 4126.7944 - val_mse: 17791872.0000 - val_mae: 4126.7944\n",
            "Epoch 195/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 971.9256 - mse: 1590237.5000 - mae: 971.9256 - val_loss: 4124.5508 - val_mse: 17773038.0000 - val_mae: 4124.5508\n",
            "Epoch 196/200\n",
            "50/50 [==============================] - 0s 889us/step - loss: 923.6914 - mse: 1286797.0000 - mae: 923.6914 - val_loss: 4118.2241 - val_mse: 17721748.0000 - val_mae: 4118.2241\n",
            "Epoch 197/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 975.4991 - mse: 1607697.1250 - mae: 975.4991 - val_loss: 4115.1973 - val_mse: 17696896.0000 - val_mae: 4115.1973\n",
            "Epoch 198/200\n",
            "50/50 [==============================] - 0s 767us/step - loss: 985.2978 - mse: 1579411.0000 - mae: 985.2978 - val_loss: 4114.0239 - val_mse: 17686452.0000 - val_mae: 4114.0239\n",
            "Epoch 199/200\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1002.0325 - mse: 1559997.6250 - mae: 1002.0326 - val_loss: 4117.3218 - val_mse: 17711508.0000 - val_mae: 4117.3218\n",
            "Epoch 200/200\n",
            "50/50 [==============================] - 0s 848us/step - loss: 1035.8671 - mse: 1710264.6250 - mae: 1035.8671 - val_loss: 4119.5254 - val_mse: 17728332.0000 - val_mae: 4119.5254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im2-1jnV8T_W"
      },
      "source": [
        "# Save the running time for each country\n",
        "# for i in range(len(countries)):\n",
        "#   save_runtime(results_runtime_static[i], path=exp1_runtime_path, country = countries[i], static_learner=True)\n",
        "\n",
        "# display_runtime_per_country(results_runtime_static, countries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NlVmdHK37Bk"
      },
      "source": [
        "countrywise_error_scores_static = calc_save_err_metric_countrywise(countries, error_metrics, results_static, max_of_pretrain_per_country, max_cases_per_country, path=exp1_path, static_learner=True, transpose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU9yFzgj_IHX"
      },
      "source": [
        "summary_table_countrywise_static = get_summary_table_countrywise(countrywise_error_scores_static, ['MAPE'], static_learner=True)\n",
        "\n",
        "# Saving the transposed matrix\n",
        "save_summary_table(summary_table_countrywise_static, exp1_summary_path, country=True, static_learner=True, alternate_batch=False, transpose=True)\n",
        "\n",
        "summary_table_countrywise_static"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTMQFGlMfCkT"
      },
      "source": [
        "sum_static_countrywise_mean = get_sum_table_combined_mean(countrywise_error_scores_static,results_runtime_static,static_learner=True)\n",
        "save_combined_summary_table(sum_static_countrywise_mean, exp1_summary_path, static_learner=True, transpose=True) \n",
        "sum_static_countrywise_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIMr3U1QAc63"
      },
      "source": [
        "## Significance tests for Experiment 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlZxtco1AbsI"
      },
      "source": [
        "## EXP1\n",
        "# Significance results for Experiment 1\n",
        "err_metric_for_significance = 'MAPE'\n",
        "significance_thresh = 0.01\n",
        "\n",
        "# Concatenating a population of all results (as in boxplot) for experiment 1\n",
        "concated_df = pd.concat([summary_table_countrywise_incremental.transpose(), summary_table_countrywise_static.transpose()]).transpose().drop(columns=['EvaluationMeasurement'], axis=1)\n",
        "concated_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyz23f1PMrpY"
      },
      "source": [
        "# Selecting the best algorithm for statistical comparisons\n",
        "# We want to know if the best is statistically significantly better compared to the rest.\n",
        "best_algo = concated_df.mean().sort_values(ascending=True).index[0]\n",
        "best_algo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFR6AjtxMv-M"
      },
      "source": [
        "print('AVG results across countries')\n",
        "concated_df.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLYuRNC8M0yF"
      },
      "source": [
        "print('STDEV across countries')\n",
        "concated_df.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpiiYDApAbeX"
      },
      "source": [
        "# Iterate through all the other algorithms to see if the difference in results is significant\n",
        "competitors = list(concated_df.columns)\n",
        "competitors.remove(best_algo)\n",
        "\n",
        "for significance_thresh in [0.01,0.05]:\n",
        "  print(f'Running significane at: {significance_thresh}')\n",
        "  for competitor in competitors:\n",
        "    # print(competitor)\n",
        "    pval, significant = check_significance(concated_df[best_algo], concated_df[competitor], significance_at=significance_thresh)\n",
        "    print(f'Comparison of {best_algo} to {competitor} pvalue: {pval}   /  significant?: {significant}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0kt9cV_XWS5"
      },
      "source": [
        "# Iterate through all the other algorithms to see if the difference in results is significant\n",
        "best_algo2 = concated_df.mean().sort_values(ascending=True).index[1]\n",
        "competitors = list(concated_df.columns)\n",
        "competitors.remove(best_algo2)\n",
        "for significance_thresh in [0.01,0.05]:\n",
        "  print(f'Running significane at: {significance_thresh}')\n",
        "  for competitor in competitors:\n",
        "    # print(competitor)\n",
        "    pval, significant = check_significance(concated_df[best_algo2], concated_df[competitor], significance_at=significance_thresh)\n",
        "    print(f'Comparison of {best_algo2} to {competitor} pvalue: {pval}   /  significant?: {significant}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3abKxWJwqNU6"
      },
      "source": [
        "# Experiment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM8GB6LTRPxY"
      },
      "source": [
        "### Creating Combined dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7vb_yHqvwPX"
      },
      "source": [
        "# Reading file \n",
        "url = 'https://drive.google.com/file/d/1eYy56fHe1XsWgPkBGVc0i6d6A5EU3nxj/view?usp=sharing'\n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "df = pd.read_csv(path)\n",
        "num_selected_countries = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxDuX7-ansDh"
      },
      "source": [
        "\n",
        "# Pre-processing dataset\n",
        "df = preprocess_dataset(df)\n",
        "\n",
        "# Grouping records by country\n",
        "df_grouped = df.groupby('country')\n",
        "\n",
        "# Taking only those countries which have sufficient data records\n",
        "#valid_countries = get_countries_with_valid_size(df_grouped)\n",
        "\n",
        "# Sorting countries by number of cases\n",
        "df_countries_sortedbycases = get_countries_sortedby_cases(valid_countries, df_grouped)\n",
        "\n",
        "# Taking only top selected countries\n",
        "top_selected_countries = df_countries_sortedbycases.iloc[0:num_selected_countries].index\n",
        "\n",
        "# Calculating targets and lags for the above countries\n",
        "result = get_dataset_with_target(top_selected_countries,df_grouped)\n",
        "\n",
        "# Getting max of each subset in pretrain size\n",
        "max_of_pretrain_days = calc_max_of_pretrain_days(pretrain_days,result)\n",
        "\n",
        "# Mean of top selected countries\n",
        "max_selected_countries = result['cases'].max()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtxpRv3FvxdD"
      },
      "source": [
        "### Incremental Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcRN9CoVFigI"
      },
      "source": [
        "# Old Script\n",
        "\"\"\"\n",
        "def scikit_multiflow(df, pretrain_days):\n",
        "\n",
        "  model, model_names = instantiate_regressors()\n",
        "\n",
        "  len_countries = len(df['country'].unique())\n",
        "\n",
        "  frames , running_time_frames = [], []\n",
        "\n",
        "  # Setup the evaluator\n",
        "  for day in pretrain_days:\n",
        "\n",
        "      df_subset = create_subset(df,day)\n",
        "      \n",
        "      # Creating a stream from dataframe\n",
        "      stream = DataStream(np.array(df_subset.iloc[:,4:-1]), y=np.array(df_subset.iloc[:,-1])) #TODO: Drop columns with name \n",
        "\n",
        "      pretrain_size = day * len_countries\n",
        "      max_samples = (day + 30) * len_countries #Testing on set one month ahead only\n",
        "\n",
        "      evaluator = EvaluatePrequential(show_plot=False,\n",
        "                                    pretrain_size=pretrain_size,\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\n",
        "                                    max_samples=max_samples)\n",
        "      # Run evaluation\n",
        "      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
        "\n",
        "      # Dictionary to store each iteration error scores\n",
        "      mdl_evaluation_scores = {}\n",
        "\n",
        "      # Adding Evaluation Measurements and pretraining days\n",
        "      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\n",
        "      mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "\n",
        "      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\n",
        "\n",
        "      # Errors of each model on a specific pre-train days\n",
        "      frames.append(mdl_evaluation_df)\n",
        "\n",
        "      # Run time for each algorithm\n",
        "      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
        "\n",
        "   # Final Run Time DataFrame\n",
        "  running_time_df = pd.concat(running_time_frames,ignore_index=True)\n",
        "\n",
        "  # Final Evaluation Score Dataframe\n",
        "  evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
        "  return evaluation_scores_df, running_time_df\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6RpyYrC-HSB"
      },
      "source": [
        "def scikit_multiflow(df, pretrain_days):  # Updated Now\n",
        "    model, model_names = instantiate_regressors()\n",
        "\n",
        "    len_countries = len(df['country'].unique())\n",
        "\n",
        "    # Selecting only required countries\n",
        "    df = df[df['country'].isin(df['country'].unique()[0:len_countries])]  # Added Now\n",
        "\n",
        "    frames, running_time_frames = [], []\n",
        "\n",
        "    united_dataframe = []  # Added Now\n",
        "\n",
        "    # Setup the evaluator\n",
        "    for day in pretrain_days:\n",
        "        df_subset = create_subset(df, day)\n",
        "\n",
        "        # Creating a stream from dataframe\n",
        "        stream = DataStream(np.array(df_subset.iloc[:, 4:-1]), y=np.array(df_subset.iloc[:, -1]))  # Selecting features x=[t-89:t-39] and y=[target].\n",
        "\n",
        "        pretrain_size = day * len_countries\n",
        "        max_samples = pretrain_size + 1  # One Extra Sample\n",
        "        testing_samples_size = (day + 30) * len_countries\n",
        "\n",
        "        evaluator = EvaluatePrequential(show_plot=False,\n",
        "                                        pretrain_size=pretrain_size,\n",
        "                                        metrics=['mean_square_error', 'mean_absolute_error',\n",
        "                                                 'mean_absolute_percentage_error'],\n",
        "                                        max_samples=max_samples)\n",
        "        # Run evaluation\n",
        "        evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
        "\n",
        "        # Added Now\n",
        "        X = stream.X[pretrain_size: testing_samples_size]  # Updated Now\n",
        "        y = stream.y[pretrain_size: testing_samples_size]  # Updated Now\n",
        "        date_idx = list(df_subset.columns).index('date')  # Added Now\n",
        "        target_dates = df_subset.iloc[pretrain_size: testing_samples_size, date_idx]  # Added Now\n",
        "\n",
        "        prediction = evaluator.predict(X)\n",
        "\n",
        "        # Since we add one extra sample, reset the evaluator\n",
        "        evaluator = reset_evaluator(evaluator)\n",
        "        evaluator = update_incremental_metrics(evaluator, y, prediction)\n",
        "\n",
        "        country_idx = list(df_subset.columns).index('country')  # Added Now\n",
        "        subset_countries_names = df_subset.iloc[pretrain_size:testing_samples_size, country_idx]  # Added Now\n",
        "        united_dataframe.append(unit_incremental_df(subset_countries_names, evaluator, target_dates, day))  # Added now\n",
        "\n",
        "        # Dictionary to store each iteration error scores\n",
        "        mdl_evaluation_scores = {}\n",
        "\n",
        "        # Adding Evaluation Measurements and pretraining days\n",
        "        mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE', 'MAE', 'MAPE']  # ,'MSE']\n",
        "        mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "        mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\n",
        "\n",
        "        # Errors of each model on a specific pre-train days\n",
        "        frames.append(mdl_evaluation_df)\n",
        "\n",
        "        # Run time for each algorithm\n",
        "        running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
        "\n",
        "    # Final Run Time DataFrame\n",
        "    running_time_df = pd.concat(running_time_frames, ignore_index=True)\n",
        "\n",
        "    united_df = pd.concat(united_dataframe, ignore_index=True)\n",
        "\n",
        "    # Final Evaluation Score Dataframe\n",
        "    evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    return evaluation_scores_df, running_time_df, united_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32jMOvemFnHG"
      },
      "source": [
        "result_skmlflow, running_time_combined_incremental, united_df = scikit_multiflow(result, pretrain_days)  # Updated Now\n",
        "save_united_df(united_df, exp2_inc_united_df_path)  # Added Now"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xOniEJ11niO"
      },
      "source": [
        "df_skmlflow = calc_save_err_metric_combined(error_metrics, result_skmlflow, max_of_pretrain_days, max_selected_countries, path=exp2_path, static_learner=False, alternate_batch=False, transpose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq1ffodM5C4f"
      },
      "source": [
        "save_runtime(running_time_combined_incremental, path=exp2_runtime_path, static_learner=False)\n",
        "running_time_combined_incremental"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrGTNx3kTNDU"
      },
      "source": [
        "summary_table_incremental = get_summary_table(df_skmlflow, running_time_combined_incremental, error_metrics, static_learner=False)\n",
        "save_summary_table(summary_table_incremental, exp2_summary_path,static_learner=False,alternate_batch=False, transpose=True)\n",
        "summary_table_incremental"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m4-HvnEwA-l"
      },
      "source": [
        "### Static Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYtaCPtjjpv5"
      },
      "source": [
        "\"\"\"\n",
        "def scikit_learn(df, training_days):\n",
        "  len_countries = len(df['country'].unique())\n",
        "  frames = []\n",
        "  model_predictions = {\n",
        "      'RandomForest':[],\n",
        "      'GradientBoosting':[],\n",
        "      'LinearSVR':[],\n",
        "      'DecisionTree':[],\n",
        "      'BayesianRidge':[],\n",
        "      'LSTM': []\n",
        "      #'MLPRegressor': [],\n",
        "      #'LinearRegression': []\n",
        "    }\n",
        "\n",
        "  # LSTM (TODO: feel free to put the whole LSTM definition in a funtion)\n",
        "  # params (others like epoch and batch size are also hardcoded in train_test_lstm())\n",
        "  layers = [20, 10, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\n",
        "  activations = ['tanh', 'tanh', 'relu']\n",
        "  patience = 20\n",
        "  total_execution_time = []\n",
        "\n",
        "  for day in training_days:\n",
        "\n",
        "    df_subset = create_subset(df,day)\n",
        "    \n",
        "    train_end_day = day * len_countries\n",
        "    test_end_day = (day+30) * len_countries\n",
        "\n",
        "    train = df_subset.iloc[:train_end_day, :] \n",
        "    test = df_subset.iloc[train_end_day:test_end_day, :]  # Testing on set one month ahead only, hence day+30.\n",
        "    test_df, val_df = get_validation_set(test, batch_size=10)  # Getting validation set from test set\n",
        "\n",
        "    # training and test sets for all models except LSTM\n",
        "    X_train, y_train = train.iloc[:,4:-1], train.iloc[:,-1]\n",
        "    X_test, y_test = test.iloc[:,4:-1], test.iloc[:,-1]\n",
        "\n",
        "    # Validation and test set for LSTM\n",
        "    X_test_lstm, y_test_lstm = test_df.iloc[:, 4:-1], test_df.iloc[:, -1]\n",
        "    X_val, y_val = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\n",
        "\n",
        "    X_train_lstm, X_val, X_test_lstm = reshape_dataframe(X_train, X_val, X_test_lstm)  # reshaping the vector for lstm\n",
        "    cur_exec_time = [day]\n",
        "\n",
        "    rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "    model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train,y_train,X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    gb_reg = GradientBoostingRegressor(random_state=0)\n",
        "    model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    lsv_reg = LinearSVR(random_state=0, tol=1e-5)\n",
        "    model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    dt_reg = DecisionTreeRegressor(random_state=0)\n",
        "    model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    br_reg = BayesianRidge()\n",
        "    model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    '''\n",
        "    mlp_reg = MLPRegressor(random_state=1, max_iter=500)\n",
        "    model_predictions['MLPRegressor'], exec_time = train_test_model(mlp_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    lin_reg = LinearRegression()\n",
        "    model_predictions['LinearRegression'], exec_time = train_test_model(lin_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "    '''\n",
        "\n",
        "    lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\n",
        "    model_predictions['LSTM'], exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train, X_val, y_val, X_test_lstm, patience)         \n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "    mdl_evaluation_df = get_scores(y_test,y_test_lstm, model_predictions, day)\n",
        "    total_execution_time.append(cur_exec_time)\n",
        "    frames.append(mdl_evaluation_df)\n",
        "\n",
        "  evaluation_score_df = pd.concat(frames, ignore_index=True)\n",
        "  running_time_df = get_running_time_per_model_static_learner(model_predictions,total_execution_time)\n",
        "  return evaluation_score_df, running_time_df\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2oijsU0UJkD"
      },
      "source": [
        "def scikit_learn(df, training_days):\n",
        "\n",
        "    len_countries = len(df['country'].unique())\n",
        "\n",
        "    # Selecting only required countries\n",
        "    df = df[df['country'].isin(df['country'].unique()[0:len_countries])]  # Added Now\n",
        "\n",
        "    frames = []\n",
        "    model_predictions = {\n",
        "        'RandomForest': [],\n",
        "        'GradientBoosting': [],\n",
        "        'LinearSVR': [],\n",
        "        'DecisionTree': [],\n",
        "        'BayesianRidge': [],\n",
        "        'LSTM': []\n",
        "    }\n",
        "    total_execution_time = []\n",
        "\n",
        "    layers = [50, 30, 20, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\n",
        "    activations = ['tanh', 'tanh', 'relu']\n",
        "    epochs = 200 # Previously: 500\n",
        "    patience = 20 * num_selected_countries\n",
        "    batch_size_lstm = 10 * num_selected_countries\n",
        "\n",
        "    united_dataframe = []  # Added Now\n",
        "\n",
        "    for day in training_days:\n",
        "        df_subset = create_subset(df, day)\n",
        "\n",
        "        train_end_day = day * len_countries\n",
        "        test_end_day = (day + 30) * len_countries\n",
        "\n",
        "        date_idx = list(df_subset.columns).index('date')  # Added Now\n",
        "        target_dates = df_subset.iloc[train_end_day: test_end_day, date_idx]  # Added Now\n",
        "\n",
        "        train = df_subset.iloc[:train_end_day, :]\n",
        "        test = df_subset.iloc[train_end_day:test_end_day, :]  # Testing on set one month ahead only, hence day+30.\n",
        "        cur_exec_time = [day]\n",
        "\n",
        "        # training and test sets for all models except LSTM\n",
        "        X_train, y_train = train.iloc[:, 4:-1], train.iloc[:, -1]\n",
        "        X_test, y_test = test.iloc[:, 4:-1], test.iloc[:, -1]\n",
        "\n",
        "        # Seperating validation set from train set\n",
        "        train_df, val_df = get_validation_set(train, batch_size=10)\n",
        "\n",
        "        # Splitting test and validation into dependent and independent sets\n",
        "        X_train_batch, y_train_batch = train_df.iloc[:, 4:-1], train_df.iloc[:, -1]  # Consist only odd batches\n",
        "        X_val_batch, y_val_batch = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\n",
        "\n",
        "        # Normalizing dataset\n",
        "        X_train_lstm_norm, X_test_lstm_norm, X_val_lstm_norm = normalize_dataset(X_train_batch, X_test, X_val_batch)\n",
        "\n",
        "        # Reshaping the dataframes\n",
        "        X_train_lstm, X_val_lstm, X_test_lstm = reshape_dataframe(X_train_lstm_norm, X_val_lstm_norm, X_test_lstm_norm)\n",
        "\n",
        "        rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "        model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        gb_reg = GradientBoostingRegressor(random_state=0)\n",
        "        model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        lsv_reg = LinearSVR(random_state=0, tol=1e-5)\n",
        "        model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        dt_reg = DecisionTreeRegressor(random_state=0)\n",
        "        model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        br_reg = BayesianRidge()\n",
        "        model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\n",
        "        model_predictions['LSTM'], exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train_batch, X_val_lstm, y_val_batch, X_test_lstm, patience, epochs,batch_size_lstm)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        country_idx = list(df_subset.columns).index('country')  # Added Now\n",
        "        subset_countries_names = df_subset.iloc[train_end_day: test_end_day, country_idx]  # Added Now\n",
        "        united_dataframe.append(unit_static_df(subset_countries_names, target_dates, y_test, day, model_predictions))  # Added now\n",
        "\n",
        "        mdl_evaluation_df = get_scores(y_test, model_predictions, day)\n",
        "        total_execution_time.append(cur_exec_time)\n",
        "        frames.append(mdl_evaluation_df)\n",
        "\n",
        "    evaluation_score_df = pd.concat(frames, ignore_index=True)\n",
        "    united_df = pd.concat(united_dataframe, ignore_index=True)  # Added Now\n",
        "    running_time_df = get_running_time_per_model_static_learner(model_predictions, total_execution_time)\n",
        "    return evaluation_score_df, running_time_df, united_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inEIGOoYn0ng"
      },
      "source": [
        "result_sklearn, running_time_static, united_df = scikit_learn(result, pretrain_days)  # Updated Now\n",
        "save_united_df(united_df, exp2_static_united_df_path)  # Added Now"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZSPkuft7YMp"
      },
      "source": [
        "df_sklearn = calc_save_err_metric_combined(error_metrics, result_sklearn, max_of_pretrain_days, max_selected_countries, path=exp2_path, static_learner=True, alternate_batch=False, transpose=True)\n",
        "display_scores(df_sklearn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U44wYr8WAqq8"
      },
      "source": [
        "save_runtime(running_time_static, path=exp2_runtime_path, static_learner=True)\n",
        "running_time_static"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7NiYw19VchA"
      },
      "source": [
        "summary_table_static = get_summary_table(df_sklearn, running_time_static, error_metrics, static_learner=True)\n",
        "save_summary_table(summary_table_static, exp2_summary_path,static_learner=True,alternate_batch=False, transpose=True)\n",
        "summary_table_static"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj09CNlNQYYL"
      },
      "source": [
        "# Experiment 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAdncNlrKXD8"
      },
      "source": [
        "## Incremental Learner: Interleaved Batches\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGGSn84iKraj"
      },
      "source": [
        "def scikit_multiflow_alternate_batch(df, pretrain_days):\n",
        "\n",
        "    model, model_names = instantiate_regressors()\n",
        "\n",
        "    len_countries = len(df['country'].unique())\n",
        "\n",
        "    # Selecting only required countries\n",
        "    df = df[df['country'].isin(df['country'].unique()[0:len_countries])]  # Added Now\n",
        "\n",
        "    frames, running_time_frames = [], []\n",
        "\n",
        "    united_dataframe = []  # Added Now\n",
        "\n",
        "    # Setup the evaluator\n",
        "    for day in pretrain_days:\n",
        "\n",
        "        df_subset = create_alternate_batch_subset(df, day, batch_size=10)\n",
        "\n",
        "        # Creating a stream from dataframe\n",
        "        stream = DataStream(np.array(df_subset.iloc[:, 4:-1]), y=np.array(df_subset.iloc[:, -1])) \n",
        "\n",
        "        pretrain_size = (day//2) * len_countries\n",
        "        max_samples = (day//2 + 30) * len_countries  # Testing on set one month ahead only\n",
        "\n",
        "        evaluator = EvaluatePrequential(show_plot=False,\n",
        "                                    pretrain_size=pretrain_size,\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\n",
        "                                    max_samples=max_samples)\n",
        "        # Run evaluation\n",
        "        evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
        "\n",
        "        # # Added Now\n",
        "        # X = stream.X[pretrain_size: testing_samples_size]  # Updated Now\n",
        "        # y = stream.y[pretrain_size: testing_samples_size]  # Updated Now\n",
        "        date_idx = list(df_subset.columns).index('date')  # Added Now\n",
        "        target_dates = df_subset.iloc[pretrain_size: max_samples, date_idx]  # Added Now\n",
        "\n",
        "        # prediction = evaluator.predict(X)\n",
        "\n",
        "        # # Since we add one extra sample, reset the evaluator\n",
        "        # evaluator = reset_evaluator(evaluator)\n",
        "        # evaluator = update_incremental_metrics(evaluator, y, prediction)\n",
        "\n",
        "        country_idx = list(df_subset.columns).index('country')  # Added Now\n",
        "        subset_countries_names = df_subset.iloc[pretrain_size:max_samples, country_idx]  # Added Now\n",
        "        united_dataframe.append(unit_incremental_df(subset_countries_names, evaluator, target_dates, day))  # Added now\n",
        "\n",
        "        # Dictionary to store each iteration error scores\n",
        "        mdl_evaluation_scores = {}\n",
        "\n",
        "        # Adding Evaluation Measurements and pretraining days\n",
        "        mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\n",
        "        mdl_evaluation_scores['PretrainDays'] = [day//2] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "\n",
        "        mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores, inc_alt_batches=True)\n",
        "\n",
        "        # Errors of each model on a specific pre-train days\n",
        "        frames.append(mdl_evaluation_df)\n",
        "\n",
        "        # Run time for each algorithm\n",
        "        running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
        "\n",
        "    # Final Run Time DataFrame\n",
        "    running_time_df = pd.concat(running_time_frames,ignore_index=True)\n",
        "\n",
        "    united_df = pd.concat(united_dataframe, ignore_index=True)\n",
        "\n",
        "    # Final Evaluation Score Dataframe\n",
        "    evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
        "    return evaluation_scores_df, running_time_df, united_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv6KT2eyheiH"
      },
      "source": [
        "result_skmlflow_alternate_batch, running_time_incremental_alternate_batch, united_df = scikit_multiflow_alternate_batch(result, pretrain_days)\n",
        "save_united_df(united_df, exp3_inc_alt_united_df_path)  # Added Now"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfluozdk7zod"
      },
      "source": [
        "df_alternate_batch = calc_save_err_metric_combined(error_metrics, result_skmlflow_alternate_batch, max_of_pretrain_days, max_selected_countries, path=exp3_path, static_learner=False, alternate_batch=True, transpose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL_NOS6va8bo"
      },
      "source": [
        "display_scores(df_alternate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNBEQbIeA-7H"
      },
      "source": [
        "save_runtime(running_time_incremental_alternate_batch, path=exp3_runtime_path, static_learner=False, alternate_batch=True)\n",
        "running_time_incremental_alternate_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RuBt5n7Vqzf"
      },
      "source": [
        "summary_table_incremental_alternate = get_summary_table(df_alternate_batch, running_time_incremental_alternate_batch, error_metrics, static_learner=False)\n",
        "save_summary_table(summary_table_incremental_alternate, exp3_summary_path, static_learner=False, alternate_batch=True, transpose=True)\n",
        "summary_table_incremental_alternate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQYy8fGQ_EJ5"
      },
      "source": [
        "## Significance tests for Experiment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yH-IbaPBk2D"
      },
      "source": [
        "## EXP2\n",
        "# Significance results for Experiment 2\n",
        "err_metric_for_significance = 'MAPE'\n",
        "significance_thresh = 0.05\n",
        "plot_pop = False\n",
        "\n",
        "# Concatenating a population of all results (as in boxplot) for experiment 2\n",
        "# This is done for runs per batch for experiment 2. But for experiment 1 it's done for runs per country (their final averages, like the result sent to the boxplots).\n",
        "static = df_sklearn[df_sklearn['EvaluationMeasurement'] == err_metric_for_significance].drop(columns=['EvaluationMeasurement'], axis=1).transpose()\n",
        "incremental = df_alternate_batch[df_alternate_batch['EvaluationMeasurement'] == err_metric_for_significance].drop(columns=['EvaluationMeasurement', 'PretrainDays'], axis=1).transpose()\n",
        "concated_df = pd.concat([static, incremental]).transpose()\n",
        "concated_df.set_index('PretrainDays', inplace=True, drop=True)\n",
        "concated_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbQZ10ONBwJZ"
      },
      "source": [
        "# Selecting the best algorithm for statistical comparisons\n",
        "# We want to know if the best is statistically significantly better compared to the rest.\n",
        "best_algo = concated_df.mean().sort_values(ascending=True).index[0]\n",
        "best_algo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i0TvT9BNHwS"
      },
      "source": [
        "print('AVG results across countries')\n",
        "concated_df.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v6za-agNIrF"
      },
      "source": [
        "print('STEDEV across countries')\n",
        "concated_df.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG_TSWRXBwtM"
      },
      "source": [
        "# Iterate through all the other algorithms to see if the difference in results is significant\n",
        "competitors = list(concated_df.columns)\n",
        "competitors.remove(best_algo)\n",
        "for significance_thresh in [0.01,0.05]:\n",
        "  print(f'Running significane at: {significance_thresh}')\n",
        "  for competitor in competitors:\n",
        "    # print(competitor)\n",
        "    pval, significant = check_significance(concated_df[best_algo], concated_df[competitor], significance_at=significance_thresh)\n",
        "    print(f'Comparison of {best_algo} to {competitor} pvalue: {pval}   /  significant?: {significant}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsx2eNV0WqxC"
      },
      "source": [
        "# Iterate through all the other algorithms to see if the difference in results is significant\n",
        "best_algo2 = concated_df.mean().sort_values(ascending=True).index[1]\n",
        "competitors = list(concated_df.columns)\n",
        "competitors.remove(best_algo2)\n",
        "for significance_thresh in [0.01,0.05]:\n",
        "  print(f'Running significance at: {significance_thresh}')\n",
        "  for competitor in competitors:\n",
        "    # print(competitor)\n",
        "    pval, significant = check_significance(concated_df[best_algo2], concated_df[competitor], significance_at=significance_thresh)\n",
        "    print(f'Comparison of {best_algo2} to {competitor} pvalue: {pval}   /  significant?: {significant}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPwMeUDwn5Tk"
      },
      "source": [
        "# Download Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fVUOfpj78ES"
      },
      "source": [
        "!zip -r /content/Result.zip /content/Result\n",
        "from google.colab import files\n",
        "files.download(\"/content/Result.zip\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDICI28viZYg"
      },
      "source": [
        "# Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DR7Z02IGVCk"
      },
      "source": [
        "## Bar Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkebu9hm0GMy"
      },
      "source": [
        "\"\"\"\n",
        "def get_filenames(pattern):\n",
        "  filenames=[]\n",
        "  for name in glob.glob(f\"{exp1_path}/*{pattern}*.csv\"):\n",
        "    filenames.append(name)\n",
        "  return filenames\n",
        "\n",
        "def get_countryname(filename):\n",
        "  country_name = filename.split('/')[-1].split('.')[0].split('_')[:-2]\n",
        "  return '_'.join(country_name)\n",
        "\n",
        "def get_error_stat_name(pattern):\n",
        "  return pattern.split('_')\n",
        "\n",
        "def plot_graph(df, statistics, metric):\n",
        "  sns.set_theme(style=\"darkgrid\")\n",
        "  df.plot(kind='bar',figsize=(12,10))\n",
        "  plt.title(f'{statistics}')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yscale('log')\n",
        "  plt.ylabel(metric)\n",
        "  ax = plt.gca()\n",
        "  ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))  # Changing scientific notation to plain text\n",
        "  plt.show()\n",
        "\n",
        "def  get_list_of_error_metric_per_country(filenames):\n",
        "  metric_dataframes_list = []\n",
        "  for filename in filenames:\n",
        "    country_name = get_countryname(filename)\n",
        "    df = pd.read_csv(filename)\n",
        "    df = df.set_index('Unnamed: 0')\n",
        "    df['country'] = country_name\n",
        "    metric_dataframes_list.append(df)\n",
        "  return metric_dataframes_list\n",
        "\n",
        "\n",
        "def get_mean_error_dataframes(metric_dataframes_list, learner_type):\n",
        "  mean_error_dataframes=[]\n",
        "  start_row = 'mean'\n",
        "  if learner_type == 'static':\n",
        "    start_col = 'RandomForest'\n",
        "  elif learner_type=='incremental':\n",
        "    start_col='HT_Reg'\n",
        "\n",
        "  for i in range(len(metric_dataframes_list)):\n",
        "    row = pd.DataFrame([metric_dataframes_list[i].loc[start_row][start_col:]])\n",
        "    mean_error_dataframes.append(row) # Passing values as a list for grouped bar chart\n",
        "    \n",
        "  result_df = pd.concat(mean_error_dataframes, ignore_index=True)\n",
        "  result_df.set_index('country', inplace=True)\n",
        "\n",
        "  return result_df\n",
        "\n",
        "filename_patterns = ['MAE_static','MAPE_static','MAE_incremental','MAPE_incremental']\n",
        "error_metric_mapper = {\n",
        "    'MAE': 'Mean Absolute Error(MAE)', \n",
        "    'MAPE':'Mean Absolute Percentage Error(MAPE)',\n",
        "    'RMSE':'Root Mean Square Error(RMSE)'}\n",
        "\n",
        "for pattern in filename_patterns:\n",
        "  filenames = get_filenames(pattern)\n",
        "  stat,learner_type = get_error_stat_name(pattern)\n",
        "  metric_dataframes_list = get_list_of_error_metric_per_country(filenames)\n",
        "  mean_error_dataframes = get_mean_error_dataframes(metric_dataframes_list,learner_type)\n",
        "  plot_graph(mean_error_dataframes,pattern,error_metric_mapper[stat])\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otlR4XEz2Kmf"
      },
      "source": [
        "\"\"\"!zip -r /content/summary_1.zip /content/Result/exp1/summary\n",
        "!zip -r /content/summary_2.zip /content/Result/exp2/summary\n",
        "from google.colab import files\n",
        "files.download(\"/content/summary_1.zip\")\n",
        "files.download(\"/content/summary_2.zip\")\"\"\"\n",
        "\n",
        "\"\"\"!zip -r /content/Result.zip /content/Result\n",
        "from google.colab import files\n",
        "files.download(\"/content/Result.zip\")\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmQXDjmrXAiS"
      },
      "source": [
        "# Download csv files\n",
        "!zip -r /content/csv_files.zip /content/csv_files\n",
        "from google.colab import files\n",
        "files.download(\"/content/csv_files.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQRkdX0oGaec"
      },
      "source": [
        "## Box Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oL2TxDdHRwM"
      },
      "source": [
        "def preprocess_data(df, metric_type, learner_type, col_mapper):\n",
        "  # Only pretrain days records are required not the mean row\n",
        "  df.drop(['mean'], axis=1, inplace=True)\n",
        "\n",
        "  # Renaming the Algorithm Columns\n",
        "  df.rename(columns={'Unnamed: 0': 'Algorithms'}, inplace=True)  \n",
        "\n",
        "  # Dropping first two rows: \"EvaluationMeasurement\" & \"PretrainDays\"\n",
        "  df.drop([0,1], axis=0, inplace=True)  \n",
        "\n",
        "  # Renaming columns based on mapper\n",
        "  df['Algorithms'].replace(col_mapper, inplace=True)  \n",
        "\n",
        "  # Melting the dataframe based on 'Algorithms'\n",
        "  df_melt = df.melt(id_vars=['Algorithms'])  \n",
        "\n",
        "  # Dropping unwanted varibale column(created bcoz of index)\n",
        "  df_melt.drop('variable', axis=1, inplace=True)  \n",
        "\n",
        "  # Renaming the value column by metric type\n",
        "  df_melt.rename(columns={'value':metric_type}, inplace=True)  \n",
        "\n",
        "  # Converting to float value bcoz by default the values are of type object\n",
        "  df_melt[metric_type] = df_melt[metric_type].astype('float64')  \n",
        "  \n",
        "  df_melt['Learner Type']= learner_type  # Adding the learner type\n",
        "\n",
        "  return df_melt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKugGWSVBPH6"
      },
      "source": [
        "def order_by_median(df,reverse = False):\n",
        "    grouped_df = df.groupby('Algorithms')\n",
        "    algo_medians = {}\n",
        "    for cur_group in grouped_df.groups.keys():\n",
        "        df_cur_grp = grouped_df.get_group(cur_group)\n",
        "        algo_medians[cur_group] = df_cur_grp['MAPE'].median()\n",
        "    sorted_algo_medians = dict(sorted(algo_medians.items(), key=lambda kv: kv[1], reverse=reverse))\n",
        "    return list(sorted_algo_medians.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-GmwJSgmvw6"
      },
      "source": [
        "# If value is less than zero return float value otherwise an integer value\n",
        "def format_values(y_val,pos):\n",
        "    if y_val < 1:\n",
        "        return format(float(y_val))\n",
        "    else:\n",
        "        return format(int(y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1676SLhHSe_"
      },
      "source": [
        "def draw_save_boxplot(df, hue_order_learner, save_filename):\n",
        "    colors = ['#1f77b4','#2ca02c', '#ff7f0e', '#ca0020']\n",
        "\n",
        "    # Setting custom color palette\n",
        "    sns.set_palette(sns.color_palette(colors))\n",
        "\n",
        "    plt.figure(figsize=(10,6),dpi=90)\n",
        "    ordered_algo_list = order_by_median(df, reverse= False)\n",
        "\n",
        "    ax = sns.boxplot(x=\"Algorithms\", y=metric_type, hue='Learner Type', data=df, order=ordered_algo_list, dodge =False, width=0.5, hue_order= hue_order_learner)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
        "    ax.set(yscale = 'log')\n",
        "    ax.set_ylim(top =100)\n",
        "    ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(format_values))  # lambda x, p: format(int(x), ',')\n",
        "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
        "    ax.set_ylabel(metric_type,fontsize=18)\n",
        "    ax.legend(loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{box_plot_path}/{save_filename}.pdf')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKWFUip8HZd3"
      },
      "source": [
        "def read_preprocess_plot_graph(filenames, col_mapper, save_filename, metric_type='MAPE'):\n",
        "    metric_type = metric_type\n",
        "    frames = []\n",
        "    for filename in filenames:\n",
        "        if 'static' in filename:\n",
        "            learner_type = 'Static'\n",
        "        else:\n",
        "            learner_type = 'Incremental'\n",
        "\n",
        "        df = pd.read_csv(filename)\n",
        "        df_melt = preprocess_data(df, metric_type, learner_type, col_mapper)\n",
        "        frames.append(df_melt)\n",
        "\n",
        "    final_df = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    # Updating LSTM learner type as Sequential\n",
        "    final_df.loc[final_df['Algorithms'] == 'LSTM', 'Learner Type'] = 'Sequential'\n",
        "\n",
        "    # Sorting final dataframe\n",
        "    final_df = final_df.sort_values(by=['MAPE'])\n",
        "\n",
        "    hue_order_learner = sorted(final_df['Learner Type'].unique())\n",
        "\n",
        "    draw_save_boxplot(final_df, hue_order_learner, save_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeltxQL_JnUH"
      },
      "source": [
        "col_mapper = {'HT_Reg':'Hoeffding Trees',\n",
        "              'HAT_Reg':'Hoeffding Adapt Tr',\n",
        "              'ARF_Reg':'Adaptive RF',\n",
        "              'PA_Reg':'Pass Agg Regr',\n",
        "              'RandomForest':'Random Forest',\n",
        "              'GradientBoosting': 'Gradient Boosting',\n",
        "              'DecisionTree': 'Decision Trees',\n",
        "              'LinearSVR': 'Linear SVR',\n",
        "              'BayesianRidge':'Bayesian Ridge'\n",
        "              }\n",
        "\n",
        "metric_type = 'MAPE'\n",
        "exp1_filenames = glob.glob(f'{exp1_path}/*{metric_type}*.csv')\n",
        "exp2_filenames = []\n",
        "\n",
        "for filename in glob.glob(f'{exp2_path}/*{metric_type}*.csv'):\n",
        "  exp2_filenames.append(filename)\n",
        "  #if 'alternate' in filename or 'static' in filename:\n",
        "  #  exp2_filenames.append(filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzhBsA4ynwwO"
      },
      "source": [
        "# TODO: remove this later. Avoiding the alternate batch results\n",
        "exp2_filenames = exp2_filenames[0:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uXMLcXK_ZkQ"
      },
      "source": [
        "'''\n",
        "col_mapper = {'HT_Reg':'HT',\n",
        "              'HAT_Reg':'HAT',\n",
        "              'ARF_Reg':'ARF',\n",
        "              'PA_Reg':'PA',\n",
        "              'RandomForest':'Random Forest',\n",
        "              'GradientBoosting': 'Gradient Boosting',\n",
        "              'DecisionTree': 'Decision Tree',\n",
        "              'LinearSVR': 'Linear SVR',\n",
        "              'BayesianRidge':'Bayesian Ridge',\n",
        "              'LSTM': 'LSTM'\n",
        "              }\n",
        "\n",
        "metric_type = 'MAPE'\n",
        "exp1_filenames = glob.glob(f'{exp1_path}/*{metric_type}*.csv')\n",
        "exp2_filenames = []\n",
        "\n",
        "for filename in glob.glob(f'{exp2_path}/*{metric_type}*.csv'):\n",
        "  if 'alternate' in filename or 'static' in filename:\n",
        "    exp2_filenames.append(filename)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFw4A4RoXGhs"
      },
      "source": [
        "save_filename = 'fig1'\n",
        "read_preprocess_plot_graph(exp1_filenames, col_mapper, save_filename, metric_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9MYJNUYjvfZ"
      },
      "source": [
        "save_filename = 'fig2'\n",
        "read_preprocess_plot_graph(exp2_filenames, col_mapper, save_filename,  metric_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA9pc2xtzDUL"
      },
      "source": [
        "# Downloading plots\n",
        "!zip -r /content/Plots.zip /content/Plots\n",
        "from google.colab import files\n",
        "files.download(\"/content/Plots.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCfQEvF1oEzE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Incremental_learning_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riu9BVr_-nl2"
      },
      "source": [
        "# Covid Dataset\n",
        "\n",
        "**Required Dataset features and target**\n",
        "\n",
        "The dataset has 53 columns; 1 to represent the country, 1 to represent the day (it will be an integer), 50 floats to represent the positive cases of the 50 previous days, and 1 column to represent the output that is the average of a full week of cases.\n",
        "\n",
        "![required_features.jpg](https://drive.google.com/uc?id=1smUwSHRwMT8h-M8kjG3ymmxdhQbe1HvY)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "ZhjnZH15MMQA",
        "outputId": "b9abaf77-a52b-4863-9ce0-c90887c86542"
      },
      "source": [
        "# Installing Incremental learner: Scikit-Multiflow\n",
        "!pip install scikit-multiflow\n",
        "\n",
        "\n",
        "# Overdiding some files from scikit multiflow library\n",
        "!gdown https://drive.google.com/uc?id=1f5GgBqjAsTUFnubHODmfqn6qiNyDBqIw\n",
        "!unzip /content/src.zip -d /content/src\n",
        "!cp -r /content/src/src /content/\n",
        "!rm -r /content/src/src\n",
        "\n",
        "# Creating a seperate directory to store all csv's\n",
        "! mkdir -p /content/csv_files\n",
        "! mkdir -p /content/csv_files/processed_null\n",
        "! mkdir -p /content/csv_files/processed\n",
        "! mkdir -p /content/Result/exp1\n",
        "! mkdir -p /content/Result/exp2\n",
        "! mkdir -p /content/Result/exp1/runtime\n",
        "! mkdir -p /content/Result/exp2/runtime\n",
        "! mkdir -p /content/Result/exp1/summary\n",
        "! mkdir -p /content/Result/exp2/summary\n",
        "! mkdir -p /content/Plots\n",
        "! mkdir -p /content/Plots/barplot\n",
        "! mkdir -p /content/Plots/boxplots\n",
        "! mkdir -p /content/Result/exp1/united_dataframe\n",
        "! mkdir -p /content/Result/exp1/united_dataframe/incremental\n",
        "! mkdir -p /content/Result/exp1/united_dataframe/static\n",
        "! mkdir -p /content/Result/exp2/united_dataframe\n",
        "! mkdir -p /content/Result/exp2/united_dataframe/incremental\n",
        "! mkdir -p /content/Result/exp2/united_dataframe/static\n",
        "\n",
        "# Download the zip file\n",
        "\"\"\"\n",
        "!zip -r /content/file.zip /content/csv_files\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-multiflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/ac/5f4675aa1e9f4c2a1d139241b50288a17e4048f5bad3484b18efc6acc4b8/scikit_multiflow-0.5.3-cp36-cp36m-manylinux2010_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 14.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 20.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 16.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 11.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61kB 4.7MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 4.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 5.2MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92kB 5.3MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 5.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 112kB 5.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122kB 5.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 5.6MB/s eta 0:00:01\r\u001b[K     |████                            | 143kB 5.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153kB 5.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 225kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 245kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 256kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 286kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 337kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 368kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 399kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 430kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 450kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 460kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 481kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 512kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 542kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 563kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 573kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 604kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 624kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 655kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 675kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 686kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 706kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 716kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 737kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 747kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 757kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 788kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 798kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 819kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 829kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 849kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 860kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 870kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 880kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 890kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 901kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 911kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 921kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 931kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 962kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 972kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 983kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 993kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.0MB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0MB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.0MB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.0MB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0MB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1MB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1MB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.1MB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1MB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (1.1.5)\n",
            "Requirement already satisfied: sortedcontainers>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (2.3.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.3->scikit-multiflow) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.3->scikit-multiflow) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->scikit-multiflow) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.3->scikit-multiflow) (1.15.0)\n",
            "Installing collected packages: scikit-multiflow\n",
            "Successfully installed scikit-multiflow-0.5.3\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1f5GgBqjAsTUFnubHODmfqn6qiNyDBqIw\n",
            "To: /content/src.zip\n",
            "100% 26.1k/26.1k [00:00<00:00, 9.59MB/s]\n",
            "Archive:  /content/src.zip\n",
            "  inflating: /content/src/src/_classification_performance_evaluator.py  \n",
            "  inflating: /content/src/src/base_evaluator.py  \n",
            "  inflating: /content/src/src/constants.py  \n",
            "  inflating: /content/src/src/evaluate_prequential.py  \n",
            "  inflating: /content/src/src/evaluation_data_buffer.py  \n",
            "  inflating: /content/src/src/measure_collection.py  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!zip -r /content/file.zip /content/csv_files\\nfrom google.colab import files\\nfiles.download(\"/content/file.zip\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz3gWpvxYPLo",
        "outputId": "ef6ed0cb-e8b7-4ef1-f1fe-d0697cf40f3f"
      },
      "source": [
        "# For Box plot: Run this only if manually uploaded the results\n",
        "!unzip /content/Result.zip -d /content/Result\n",
        "!cp -r /content/Result/content/Result /content/\n",
        "!rm -r /content/Result/content/Result\n",
        "!rm -r /content/Result/content\n",
        "\n",
        "csv_processed_path = '/content/csv_files/processed'\n",
        "csv_processed_with_null_path = '/content/csv_files/processed_null'\n",
        "exp1_path = '/content/Result/exp1'\n",
        "exp2_path = '/content/Result/exp2'\n",
        "exp1_runtime_path = '/content/Result/exp1/runtime'\n",
        "exp2_runtime_path = '/content/Result/exp2/runtime'\n",
        "exp1_summary_path = '/content/Result/exp1/summary'\n",
        "exp2_summary_path = '/content/Result/exp2/summary'\n",
        "bar_plot_path = r'/content/Plots/barplot'\n",
        "box_plot_path = r'/content/Plots/boxplots'\n",
        "exp1_static_united_df_path = '/content/Result/exp1/united_dataframe/static'\n",
        "exp1_inc_united_df_path = '/content/Result/exp1/united_dataframe/incremental'\n",
        "exp2_static_united_df_path = '/content/Result/exp2/united_dataframe/static'\n",
        "exp2_inc_united_df_path = '/content/Result/exp2/united_dataframe/incremental'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open /content/Result.zip, /content/Result.zip.zip or /content/Result.zip.ZIP.\n",
            "cp: cannot stat '/content/Result/content/Result': No such file or directory\n",
            "rm: cannot remove '/content/Result/content/Result': No such file or directory\n",
            "rm: cannot remove '/content/Result/content': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqTR_N3fpMON",
        "outputId": "bfc499f4-ff04-4988-daed-a589c1d0cfa0"
      },
      "source": [
        "#!pip uninstall keras\n",
        "#!pip uninstall tensorflow\n",
        "\n",
        "!pip install keras==2.3.1\n",
        "!pip install tensorflow==2.1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.19.5)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Installing collected packages: keras-applications, keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.3.1 keras-applications-1.0.8\n",
            "Collecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 29kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.15.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 43.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.36.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.12.4)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 37.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.19.5)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.1.0) (53.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.3.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.4.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=b993d48134e430cfeb040c5fb229c7365fe290c476fa9dd08b30cd10e4baaf63\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, gast, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvDISgy5CUOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1a96e2-9097-4694-c828-74750e6cf306"
      },
      "source": [
        "# General Imports \n",
        "import pandas as pd\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "import warnings\n",
        "from pandas.core.common import SettingWithCopyWarning\n",
        "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme(style='darkgrid')\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "# Imports for incremental learner\n",
        "from skmultiflow.data import DataStream\n",
        "from skmultiflow.trees import HoeffdingTreeRegressor\n",
        "from src.evaluate_prequential import EvaluatePrequential\n",
        "from skmultiflow.meta import AdaptiveRandomForestRegressor\n",
        "from skmultiflow.trees import HoeffdingAdaptiveTreeRegressor\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.linear_model import PassiveAggressiveRegressor\n",
        "\n",
        "# Imports for static Learner\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from time import perf_counter as pc_timer\n",
        "from functools import wraps\n",
        "\n",
        "import keras\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "#from tensorflow.keras import Sequential\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# For significance tests\n",
        "from scipy.stats import normaltest\n",
        "from scipy import stats \n",
        "# pd.set_option('display.max_colwidth', 500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "wHwu0MJOCm68",
        "outputId": "0710dc77-fe6d-4df8-bab8-49c0e4d748a4"
      },
      "source": [
        "#url = 'https://drive.google.com/file/d/1e7NsptfEFLG2gGLykYlrzjNbDJLiRbGm/view?usp=sharing'\n",
        "url = 'https://drive.google.com/file/d/1VH-nkePskK3gT6U5qkoOP-0hFT4beszC/view?usp=sharing'\n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "df = pd.read_csv(path)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dateRep</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "      <th>cases</th>\n",
              "      <th>deaths</th>\n",
              "      <th>countriesAndTerritories</th>\n",
              "      <th>geoId</th>\n",
              "      <th>countryterritoryCode</th>\n",
              "      <th>popData2019</th>\n",
              "      <th>continentExp</th>\n",
              "      <th>Cumulative_number_for_14_days_of_COVID-19_cases_per_100000</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>02/11/2020</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>2020</td>\n",
              "      <td>132</td>\n",
              "      <td>5</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>3.767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01/11/2020</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>2020</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>3.575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31/10/2020</td>\n",
              "      <td>31</td>\n",
              "      <td>10</td>\n",
              "      <td>2020</td>\n",
              "      <td>157</td>\n",
              "      <td>4</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>3.554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30/10/2020</td>\n",
              "      <td>30</td>\n",
              "      <td>10</td>\n",
              "      <td>2020</td>\n",
              "      <td>123</td>\n",
              "      <td>3</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>3.265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>29/10/2020</td>\n",
              "      <td>29</td>\n",
              "      <td>10</td>\n",
              "      <td>2020</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>2.942</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      dateRep  ...  Cumulative_number_for_14_days_of_COVID-19_cases_per_100000\n",
              "0  02/11/2020  ...                                              3.767         \n",
              "1  01/11/2020  ...                                              3.575         \n",
              "2  31/10/2020  ...                                              3.554         \n",
              "3  30/10/2020  ...                                              3.265         \n",
              "4  29/10/2020  ...                                              2.942         \n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6NQxu_LuoTV"
      },
      "source": [
        "# Grouping countries together for analysis\n",
        "total_countries = df['countriesAndTerritories'].unique()\n",
        "df_grouped = df.groupby('countriesAndTerritories')\n",
        "pretrain_days = [30,60,90,120,150,180]  # List of pretrain days\n",
        "valid_countries = []\n",
        "decimal = 3  # Specify the scale of decimal places \n",
        "error_metrics = ['MAE','MAPE', 'RMSE']\n",
        "\n",
        "# Setting path variables for both experiments\n",
        "csv_processed_path = '/content/csv_files/processed'\n",
        "csv_processed_with_null_path = '/content/csv_files/processed_null'\n",
        "exp1_path = '/content/Result/exp1'\n",
        "exp2_path = '/content/Result/exp2'\n",
        "exp1_runtime_path = '/content/Result/exp1/runtime'\n",
        "exp2_runtime_path = '/content/Result/exp2/runtime'\n",
        "exp1_summary_path = '/content/Result/exp1/summary'\n",
        "exp2_summary_path = '/content/Result/exp2/summary'\n",
        "bar_plot_path = r'/content/Plots/barplot'\n",
        "box_plot_path = r'/content/Plots/boxplots'\n",
        "\n",
        "\n",
        "# Top countries to select for experiment 1\n",
        "Number_of_countries = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oubf_WqnATmM"
      },
      "source": [
        "## Feature Set with Individual Countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0vj8668gNrP"
      },
      "source": [
        "# Create lags\n",
        "def create_features_with_lags(df):\n",
        "  for i in range(89, 0, -1):  # Loop in reverse order for creating ordered lags eg: cases_t-10, cases_t-9... cases_t-1. t=current cases\n",
        "    df[f'cases_t-{i}'] = df['cases'].shift(i, axis=0)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G23w6M6HdyEb",
        "outputId": "2bede430-53e5-474e-9d9e-0ea57251e1ab"
      },
      "source": [
        "# Pre-Processing dataset and saving them into csv's.\n",
        "for country in total_countries:\n",
        "  df = df_grouped.get_group(country)\n",
        "\n",
        "  # Selecting required features\n",
        "  df= df[['dateRep','cases','countriesAndTerritories']]\n",
        "\n",
        "  # Rename features\n",
        "  df.rename(columns={'countriesAndTerritories':'country', 'dateRep':'date'}, inplace=True)\n",
        "\n",
        "  # Convert to date, sort and set index\n",
        "  df['date'] = pd.to_datetime(df['date'],format='%d/%m/%Y')\n",
        "  df.sort_values('date', inplace=True)\n",
        "  df.set_index('date', inplace=True)\n",
        "\n",
        "  # Adding feature\n",
        "  df['day_no']= pd.Series([i for i in range(1,len(df)+1)], index=df.index)\n",
        "\n",
        "  # Reordering features\n",
        "  df = df[['day_no', 'country','cases']]\n",
        "\n",
        "  # Adding features through lags\n",
        "  df = create_features_with_lags(df)\n",
        "\n",
        "  # Creating target with last 10 days cases\n",
        "  df['target'] = df.iloc[:,[2]+[i*-1 for i in range(1,10)]].mean(axis=1)\n",
        "\n",
        "  # Dropping mid columns\n",
        "  drop_columns = list(df.loc[:,'cases_t-39':'cases_t-1'].columns)  #list(df.loc[:,'cases_t-38':'cases_t-1'].columns)\n",
        "  df.drop(drop_columns, axis=1, inplace=True)\n",
        "\n",
        "  # Country name\n",
        "  filename = df['country'].unique()[0]\n",
        "\n",
        "  # Saving file\n",
        "  df.to_csv(f'{csv_processed_with_null_path}/{filename}.csv')\n",
        "\n",
        "  # Dropping null records\n",
        "  df.dropna(how='any', axis=0, inplace=True)\n",
        "\n",
        "  # Valid countries that have records more than max of pretrain\n",
        "  if len(df)>max(pretrain_days):\n",
        "    valid_countries.append(country)  \n",
        "    df.to_csv(f'{csv_processed_path}/{filename}.csv')\n",
        "  \n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMZCoXFwPsHA"
      },
      "source": [
        "## Total cases of top selected countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTQjGxITEeMD"
      },
      "source": [
        "# Added just for plots. Remove later\n",
        "Number_of_countries = 25\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDWlSV8OJUQP"
      },
      "source": [
        "# Replaces underscore from country names\n",
        "def format_names(list_countries):\n",
        "  updated_country_list = []\n",
        "  for country_name in list_countries:\n",
        "    updated_country_list.append(country_name.replace(\"_\",\" \"))\n",
        "  return updated_country_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQfWz6LlAyHf"
      },
      "source": [
        "# A dictionary of all countries\n",
        "dict_countries = Counter(valid_countries)\n",
        "\n",
        "for country in valid_countries:\n",
        "  dict_countries[country] = df_grouped.get_group(country)['cases'].sum()\n",
        "\n",
        "# Select top_countries and order(Ascending/Decending) \n",
        "top_countries = sorted(dict_countries.items(), key=lambda dict_countries: dict_countries[1], reverse=True) [0:Number_of_countries]\n",
        "\n",
        "# Creating dataframe of top selected countries\n",
        "df_top_countries = pd.DataFrame.from_dict(dict(top_countries), orient='index', columns=['Total Cases'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_U3MIv16lLfX",
        "outputId": "9aa5ec55-f6ab-43f2-aabf-7b1afa3e3b6f"
      },
      "source": [
        "# Plotting graph\n",
        "sns.set_theme(style='white')\n",
        "plt.figure(figsize=(12,16))\n",
        "top_countries_list = format_names(df_top_countries.index)\n",
        "plt.barh(top_countries_list[::-1], df_top_countries['Total Cases'].values[::-1]) # Reversing the order to have heighest values at the top of bar chart\n",
        "#plt.title(f'Top {len(top_countries)} Countries with Most Cases')\n",
        "plt.xscale('log')\n",
        "ax = plt.axes() # for updating axes values to plain text\n",
        "ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "plt.margins(y=0)\n",
        "ax.tick_params(axis='both', which='major', labelsize=18)\n",
        "plt.xlabel('Number of Cases', fontsize=20)\n",
        "plt.ylabel('Countries',fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{bar_plot_path}/top_selected_country_cases.pdf')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x1152 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "qbgnoDsBEsoQ",
        "outputId": "2a326a8d-8dce-470a-d617-181b40ce4038"
      },
      "source": [
        "\"\"\"\n",
        "# Plotting graph\n",
        "sns.set_theme(style='white')\n",
        "plt.figure(figsize=(12,16), dpi=90)\n",
        "\n",
        "#plt.bar(df_top_countries.index, df_top_countries['Total Cases'].values)\n",
        "top_countries_list = format_names(df_top_countries.index)\n",
        "plt.barh(top_countries_list, df_top_countries['Total Cases'].values)\n",
        "\n",
        "#plt.axvline(Number_of_countries-0.5, 0,1, ls='--', c='black')\n",
        "plt.axhline(y=(Number_of_countries-0.5), ls='--', c='black')\n",
        "\n",
        "plt.title(f'Top {len(top_countries)} Countries with Most Cases')\n",
        "\n",
        "plt.xscale('log')\n",
        "ax = plt.axes() # for updating \n",
        "ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "\n",
        "#plt.gcf().axes[0].yaxis.get_major_formatter().set_scientific(False)  # To get labels in plain text \n",
        "#plt.xticks(rotation=90)\n",
        "plt.margins(y=0)\n",
        "plt.xlabel('Number of Cases')\n",
        "plt.ylabel('Countries')\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Plotting graph\\nsns.set_theme(style='white')\\nplt.figure(figsize=(12,16), dpi=90)\\n\\n#plt.bar(df_top_countries.index, df_top_countries['Total Cases'].values)\\ntop_countries_list = format_names(df_top_countries.index)\\nplt.barh(top_countries_list, df_top_countries['Total Cases'].values)\\n\\n#plt.axvline(Number_of_countries-0.5, 0,1, ls='--', c='black')\\nplt.axhline(y=(Number_of_countries-0.5), ls='--', c='black')\\n\\nplt.title(f'Top {len(top_countries)} Countries with Most Cases')\\n\\nplt.xscale('log')\\nax = plt.axes() # for updating \\nax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\\n\\n#plt.gcf().axes[0].yaxis.get_major_formatter().set_scientific(False)  # To get labels in plain text \\n#plt.xticks(rotation=90)\\nplt.margins(y=0)\\nplt.xlabel('Number of Cases')\\nplt.ylabel('Countries')\\nplt.show()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0TQucg2Pwu1"
      },
      "source": [
        "## Average cases of top selected countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "id": "6kOjiJOSRCtG",
        "outputId": "0702f97b-f9f0-4d77-d3ff-2df28c60f47a"
      },
      "source": [
        "dict_countries_avg = Counter(total_countries)\n",
        "\n",
        "for country in dict_countries.keys():\n",
        "  dict_countries_avg[country] = df_grouped.get_group(country)['cases'].mean()\n",
        "\n",
        "# Average cases for all countries\n",
        "df_avg_cases_countries = pd.DataFrame.from_dict(dict_countries_avg, orient='index', columns=['Average'])\n",
        "\n",
        "# List of top selected countries\n",
        "top_countries = list(df_top_countries.index)\n",
        "\n",
        "# Average of selected top countries\n",
        "avg_df = df_avg_cases_countries[df_avg_cases_countries.index.isin(top_countries)]\n",
        "avg_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Belgium</th>\n",
              "      <td>1431.568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Brazil</th>\n",
              "      <td>18005.536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Canada</th>\n",
              "      <td>768.964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Czechia</th>\n",
              "      <td>1109.234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ecuador</th>\n",
              "      <td>558.396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>France</th>\n",
              "      <td>4590.633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Germany</th>\n",
              "      <td>1769.568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>India</th>\n",
              "      <td>26805.580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Indonesia</th>\n",
              "      <td>1371.375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iran</th>\n",
              "      <td>2014.581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iraq</th>\n",
              "      <td>1553.229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Israel</th>\n",
              "      <td>1033.216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Italy</th>\n",
              "      <td>2303.036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mexico</th>\n",
              "      <td>3097.973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nepal</th>\n",
              "      <td>588.363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Netherlands</th>\n",
              "      <td>1167.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pakistan</th>\n",
              "      <td>1105.917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Philippines</th>\n",
              "      <td>1260.240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Romania</th>\n",
              "      <td>806.088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Russia</th>\n",
              "      <td>5314.224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Spain</th>\n",
              "      <td>3862.143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Switzerland</th>\n",
              "      <td>499.117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>United_Arab_Emirates</th>\n",
              "      <td>443.401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>United_Kingdom</th>\n",
              "      <td>3360.110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>United_States_of_America</th>\n",
              "      <td>29894.032</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           Average\n",
              "Belgium                   1431.568\n",
              "Brazil                   18005.536\n",
              "Canada                     768.964\n",
              "Czechia                   1109.234\n",
              "Ecuador                    558.396\n",
              "France                    4590.633\n",
              "Germany                   1769.568\n",
              "India                    26805.580\n",
              "Indonesia                 1371.375\n",
              "Iran                      2014.581\n",
              "Iraq                      1553.229\n",
              "Israel                    1033.216\n",
              "Italy                     2303.036\n",
              "Mexico                    3097.973\n",
              "Nepal                      588.363\n",
              "Netherlands               1167.003\n",
              "Pakistan                  1105.917\n",
              "Philippines               1260.240\n",
              "Romania                    806.088\n",
              "Russia                    5314.224\n",
              "Spain                     3862.143\n",
              "Switzerland                499.117\n",
              "United_Arab_Emirates       443.401\n",
              "United_Kingdom            3360.110\n",
              "United_States_of_America 29894.032"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkabfg44B2iE"
      },
      "source": [
        "# Common Methods for All Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jkIXUcMCpwy"
      },
      "source": [
        "## Common Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONXUYHUmNJka"
      },
      "source": [
        "#### Updated Number of countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiLed1lngDEB"
      },
      "source": [
        "Number_of_countries = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2wFG6bg0-0W"
      },
      "source": [
        "# Global variables for countries\n",
        "countries = top_countries[0:Number_of_countries]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHYQO1XBALXe"
      },
      "source": [
        "# Return a combined dataframe for a each error statistics(MAE,RMSE,MAPE etc) along with the newly added mean row.\n",
        "def get_metric_with_mean(result: pd.DataFrame, error_metric: str)->pd.DataFrame:\n",
        "  df_grouped = result.groupby('EvaluationMeasurement')\n",
        "  df = df_grouped.get_group(error_metric).reset_index(drop=True)\n",
        "  df = df.append(df.describe().loc['mean'])\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AVGVaKLwhHu"
      },
      "source": [
        "def calc_mean_to_max_error(df, max_of_pretrain_days, max_of_df):\n",
        "  i=-1\n",
        "  for row_num in range(len(df)-1):  # Go before mean row\n",
        "    i += 1\n",
        "    for col_num in df.columns[2:]:\n",
        "      df.loc[row_num,col_num] = df.loc[row_num,col_num]/max_of_pretrain_days[i] \n",
        "  \n",
        "  for col in df.columns[2:]:\n",
        "      df.loc['mean',col] = df.loc['mean',col]/max_of_df\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgvjK5nJkUaG"
      },
      "source": [
        "# Note: Do not change the filenames, since they are later being used for visualizations \n",
        "def save_runtime(df,path,country=None,static_learner=True,alternate_batch=False, transpose=False):\n",
        "  df = df.apply(pd.to_numeric,errors='ignore') # Converting the dataframe to numeric\n",
        "  df = df.round(decimal) # Setting the precision\n",
        "  \n",
        "  # if transpose flag is set to true\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "\n",
        "  if country==None:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/combined25country_runtime_static.tex')\n",
        "      df.to_csv(f'{path}/combined25country_runtime_static.csv')\n",
        "    else:\n",
        "      if alternate_batch:\n",
        "         df.to_latex(f'{path}/combined25country_runtime_incremental_alternate_batch.tex')\n",
        "         df.to_csv(f'{path}/combined25country_runtime_incremental_alternate_batch.csv')\n",
        "      else:\n",
        "        df.to_latex(f'{path}/combined25country_runtime_incremental.tex')\n",
        "        df.to_csv(f'{path}/combined25country_runtime_incremental.csv')\n",
        "  else:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/{country}_runtime_static.tex')\n",
        "      df.to_csv(f'{path}/{country}_runtime_static.csv')\n",
        "    else:\n",
        "      df.to_latex(f'{path}/{country}_runtime_incremental.tex')\n",
        "      df.to_csv(f'{path}/{country}_runtime_incremental.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJCVpF7oT7r0"
      },
      "source": [
        "# Note: Do not change the filenames, since they are later being used for visualizations \n",
        "def save_summary_table(df,path,country=False,static_learner=True,alternate_batch=False, transpose=False):\n",
        "\n",
        "  df = df.apply(pd.to_numeric,errors='ignore') # Converting the dataframe to numeric\n",
        "  df = df.round(decimal) # Setting the precision\n",
        "  \n",
        "  # if transpose flag is set to true\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "\n",
        "  if country:\n",
        "    metric = df.loc['EvaluationMeasurement'].unique()[0]\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/top_countries_{metric}_summary_table_static.tex')\n",
        "      df.to_csv(f'{path}/top_countries_{metric}_summary_table_static.csv')\n",
        "    else:\n",
        "      df.to_latex(f'{path}/top_countries_{metric}_summary_table_incremental.tex')\n",
        "      df.to_csv(f'{path}/top_countries_{metric}_summary_table_incremental.csv')\n",
        "    \n",
        "  else:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/combined25country_summary_table_static.tex')\n",
        "      df.to_csv(f'{path}/combined25country_summary_table_static.csv')\n",
        "    else:\n",
        "      if alternate_batch:\n",
        "         df.to_latex(f'{path}/combined25country_summary_table_incremental_alternate_batch.tex')\n",
        "         df.to_csv(f'{path}/combined25country_summary_table_incremental_alternate_batch.csv')\n",
        "      else:\n",
        "        df.to_latex(f'{path}/combined25country_summary_table_incremental.tex')\n",
        "        df.to_csv(f'{path}/combined25country_summary_table_incremental.csv')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnS1UsFdKRbW"
      },
      "source": [
        "# Note: Do not change the filenames since they are later being used for visualizations\n",
        "def save_metrics(df, path, country=None, static_learner=True, alternate_batch=False, transpose=False): \n",
        "  df = df.apply(pd.to_numeric,errors='ignore') # Converting the dataframe to numeric\n",
        "  df = df.round(decimal) # Setting the precision\n",
        "  \n",
        "  # if transpose flag is set to true\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "\n",
        "  metric_type = df.loc['EvaluationMeasurement'].unique()[0]\n",
        "  if country==None:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/combined25country_{metric_type}_static.tex')\n",
        "      df.to_csv(f'{path}/combined25country_{metric_type}_static.csv')\n",
        "    else:\n",
        "      if alternate_batch:\n",
        "         df.to_latex(f'{path}/combined25country_{metric_type}_incremental_alternate_batch.tex')\n",
        "         df.to_csv(f'{path}/combined25country_{metric_type}_incremental_alternate_batch.csv')\n",
        "      else:\n",
        "        df.to_latex(f'{path}/combined25country_{metric_type}_incremental.tex')\n",
        "        df.to_csv(f'{path}/combined25country_{metric_type}_incremental.csv')\n",
        "  else:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/{country}_{metric_type}_static.tex')\n",
        "      df.to_csv(f'{path}/{country}_{metric_type}_static.csv')\n",
        "    else:\n",
        "      df.to_latex(f'{path}/{country}_{metric_type}_incremental.tex')\n",
        "      df.to_csv(f'{path}/{country}_{metric_type}_incremental.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQFR3NKzuSK-"
      },
      "source": [
        "def save_combined_summary_table(df, path, static_learner=False,transpose=False):\n",
        "  df = df.apply(pd.to_numeric,errors='ignore')\n",
        "  df = df.round(decimal)\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "  \n",
        "  if static_learner:\n",
        "    save_path = f'{path}/summary_table_combined_mean_static'\n",
        "  else:\n",
        "    save_path = f'{path}/summary_table_combined_mean_incremental'\n",
        "\n",
        "  df.to_csv(f'{save_path}.csv')\n",
        "  df.to_latex(f'{save_path}.tex')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUrUegDEPqof"
      },
      "source": [
        "def display_runtime_per_country(results_runtime,countries):\n",
        "  for i in range(len(countries)):\n",
        "    print(f'_____________Running Time for {countries[i]}________________')\n",
        "    print(results_runtime[i].to_string())\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI4IiENZtLFG"
      },
      "source": [
        "def calc_save_err_metric_countrywise(countries, error_metrics, results, max_of_pretrain_per_country, max_cases_per_country, path, static_learner, transpose):\n",
        "  countrywise_error_scores={}\n",
        "  for i in range(len(countries)):\n",
        "    country_error_score = []\n",
        "    for error_metric in error_metrics:\n",
        "      \n",
        "      df_error_metric = get_metric_with_mean(results[i], error_metric=error_metric)\n",
        "\n",
        "      #if error_metric != 'MAPE':\n",
        "      #  df_error_metric = calc_mean_to_max_error(df_error_metric, max_of_pretrain_per_country[i], max_cases_per_country[i])\n",
        "\n",
        "      country_error_score.append(df_error_metric)\n",
        "      display_countrywise_scores(countries[i],df_error_metric)\n",
        "\n",
        "      # Transposing the metrics while saving\n",
        "      save_metrics(df_error_metric, path=path, country=countries[i], static_learner=static_learner, transpose=transpose)\n",
        "      \n",
        "    countrywise_error_scores[countries[i]] = pd.concat(country_error_score,ignore_index=True)\n",
        "    \n",
        "  return countrywise_error_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e4uCM0f65tm"
      },
      "source": [
        "def calc_save_err_metric_combined(error_metrics, results, max_of_pretrain_days, max_selected_countries, path, static_learner, alternate_batch, transpose):\n",
        "  combined_err_metric = []\n",
        "  for error_metric in error_metrics:\n",
        "    df_error_metric = get_metric_with_mean(results, error_metric=error_metric)\n",
        "\n",
        "    #if error_metric != 'MAPE':\n",
        "    #  df_error_metric = calc_mean_to_max_error(df_error_metric, max_of_pretrain_days, max_selected_countries)\n",
        "\n",
        "    # Transposing the metrics while saving\n",
        "    save_metrics(df_error_metric, path=path, static_learner=static_learner, alternate_batch=alternate_batch, transpose=transpose)\n",
        "    \n",
        "    combined_err_metric.append(df_error_metric)\n",
        "  return (pd.concat(combined_err_metric, ignore_index=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7nnCdGu8QVD"
      },
      "source": [
        "def get_summary_table(df_result, df_runtime_result, error_metrics, static_learner=True):\n",
        "  sum_metric=[]\n",
        "  measure_col_name = 'Metric'\n",
        "  \n",
        "  # Setting start row and column for static and incremental learner\n",
        "  for metric in error_metrics:\n",
        "    start_row = 'mean'\n",
        "    if static_learner:\n",
        "      start_col='RandomForest'\n",
        "    else:\n",
        "      start_col='HT_Reg'\n",
        "\n",
        "    df_metric = get_metric_with_mean(df_result, metric)\n",
        "    df_row = pd.DataFrame([df_metric.loc[start_row][start_col:]])\n",
        "    \n",
        "    df_row[measure_col_name] = str(metric)    \n",
        "    sum_metric.append(df_row)\n",
        "\n",
        "  # Adding run time\n",
        "  df_runtime_row = pd.DataFrame([df_runtime_result.describe().loc[start_row][start_col:]])\n",
        "  df_runtime_row[measure_col_name]='Time(sec)'\n",
        "  sum_metric.append(df_runtime_row)\n",
        "\n",
        "  df_summary = pd.concat(sum_metric, ignore_index=True)\n",
        "  df_summary.set_index(measure_col_name, inplace=True)\n",
        "\n",
        "  return df_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLrQqgWXAPgi"
      },
      "source": [
        "def get_summary_table_countrywise(df_result_dict, error_metrics, static_learner=True):  #df_runtime_result,\n",
        "  summary_metric=[]\n",
        "  measure_col_name = f'Country({str(error_metrics[0])})'\n",
        "  eval_measure_col = 'EvaluationMeasurement'\n",
        "  start_row = 'mean'\n",
        "  if static_learner:\n",
        "    start_col='RandomForest'\n",
        "  else:\n",
        "    start_col='HT_Reg'\n",
        "\n",
        "  for country in df_result_dict.keys():\n",
        "    df_result = df_result_dict[country]\n",
        "\n",
        "    # Setting start row and column for static and incremental learner\n",
        "    for metric in error_metrics:      \n",
        "      df_metric = get_metric_with_mean(df_result, metric)\n",
        "      df_row = pd.DataFrame([df_metric.loc[start_row][start_col:]])\n",
        "      df_row[eval_measure_col] = metric\n",
        "      df_row[measure_col_name] = country\n",
        "      summary_metric.append(df_row)\n",
        "\n",
        "  df_summary = pd.concat(summary_metric, ignore_index=True)\n",
        "  df_summary.set_index(measure_col_name, inplace=True)\n",
        "\n",
        "  return df_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSX0nYK3SW2Q"
      },
      "source": [
        "def get_sum_table_combined_mean(countrywise_error_score_incremental,results_runtime, static_learner=False):\n",
        "  sum_table_combined_mean=[]\n",
        "  measure_col_name = 'Metric'\n",
        "  start_row = 'mean'\n",
        "  if static_learner:\n",
        "    start_col = 'RandomForest'\n",
        "  else:\n",
        "    start_col= 'HT_Reg'\n",
        "\n",
        "  for metric in error_metrics:\n",
        "    df_sum_cur_metric = get_summary_table_countrywise(countrywise_error_score_incremental, [metric], static_learner=static_learner)\n",
        "    df_row = pd.DataFrame([df_sum_cur_metric.describe().loc[start_row]])\n",
        "\n",
        "    df_row[measure_col_name] = metric\n",
        "    sum_table_combined_mean.append(df_row)\n",
        "\n",
        "  # Adding run time\n",
        "  df_runtime = pd.concat(results_runtime, ignore_index=True).describe().loc[start_row][start_col:]\n",
        "  df_runtime_row = pd.DataFrame([df_runtime])\n",
        "  df_runtime_row[measure_col_name]='Time(sec)'\n",
        "  sum_table_combined_mean.append(df_runtime_row)\n",
        "\n",
        "  # Concating results to one dataframe\n",
        "  sum_table_combined_mean = pd.concat(sum_table_combined_mean, ignore_index=True)\n",
        "  sum_table_combined_mean.set_index(measure_col_name, inplace=True)\n",
        "  return sum_table_combined_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AKhvSFwAzUj"
      },
      "source": [
        "def check_significance(target_pop, competitor_pop, significance_at: float):\n",
        "    \"\"\"\n",
        "    Comparing algorithms per batch or per country pairs (exp 2 or 1 respectively), \n",
        "      so for each pair, we compare the significance of the best algo to all of the the other algos.\n",
        "    Ttest performed if the distribution is normal, otherwise we perform a non-parametric test.\n",
        "    \"\"\"\n",
        "    model_pop, population = target_pop, competitor_pop  \n",
        "    \n",
        "    # Normality tests\n",
        "    if len(model_pop) >= 8:  # skew test not valid for smaller populations\n",
        "      value_mdl, p_mdl = normaltest(model_pop.values)\n",
        "      value_pop, p_pop = normaltest(population.values)\n",
        "      if (p_mdl >= 0.05) & (p_pop >= 0.05):\n",
        "          # print('It is likely that both populations are normal. Thus, running T-Test...')\n",
        "          tset, pval = stats.ttest_ind(model_pop, population)\n",
        "          if pval < significance_at:    # alpha value is 0.05 or 5%\n",
        "              significant = 'Significant (Ttest)'\n",
        "          else:\n",
        "              significant = 'Not Significant (Ttest)'\n",
        "      else:\n",
        "          # print('It is unlikely that the result is normal. Thus, running Wilcoxon test...')\n",
        "          if np.sum(np.subtract(list(model_pop), list(competitor_pop))) != 0.0:  # if values are identical the test will crash, but we now it's not significant\n",
        "              tset, pval = stats.wilcoxon(model_pop, population)\n",
        "              if pval < significance_at:    # alpha value is 0.05 or 5%\n",
        "                  significant = 'Significant (Wilcox Test)'\n",
        "              else:\n",
        "                  significant = 'Not Significant (Wilcox Test)'\n",
        "          else:\n",
        "              # print('Warning: results are identical')\n",
        "              tset, pval = stats.ttest_ind(model_pop, population)\n",
        "              significant = 'Not Significant (Wilcox Test)'\n",
        "    else:\n",
        "      print('Population too small.')\n",
        "      if np.sum(np.subtract(list(model_pop), list(competitor_pop))) != 0.0:  # if values are identical the test will crash, but we now it's not significant\n",
        "          tset, pval = stats.wilcoxon(model_pop, population)\n",
        "          if pval < significance_at:    # alpha value is 0.05 or 5%\n",
        "              significant = 'Significant (Wilcox Test)'\n",
        "          else:\n",
        "              significant = 'Not Significant (Wilcox Test)'\n",
        "    return pval, significant "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZYx2PYrRBMV"
      },
      "source": [
        "## Combining Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g11ZDONMQvHT"
      },
      "source": [
        "def sortby_date_and_set_index(df):\n",
        "  df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y')\n",
        "  df.sort_values('date', inplace=True)\n",
        "  df.set_index('date', inplace=True)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZr-9CCDwIVo"
      },
      "source": [
        "def get_dataset_with_target(countries, df_grouped):\n",
        "  \n",
        "  # Empty list to store Dataframes of each country\n",
        "  frames = []\n",
        "  \n",
        "  for country in countries:\n",
        "    \n",
        "    df = df_grouped.get_group(country)\n",
        "\n",
        "    # Creating feature 'day_no'\n",
        "    df['day_no']= pd.Series([i for i in range(1,len(df)+1)], index=df.index)\n",
        "\n",
        "    # Reordering features\n",
        "    df = df[['day_no', 'country','cases']]\n",
        "\n",
        "    # Adding features through lags\n",
        "    df = create_features_with_lags(df)\n",
        "\n",
        "    # Creating target with last 10 days cases\n",
        "    df['target'] = df.iloc[:,[2]+[i*-1 for i in range(1,10)]].mean(axis=1)\n",
        "\n",
        "    # Dropping null columns\n",
        "    df.dropna(how='any', axis=0, inplace=True)\n",
        "\n",
        "    # Dropping mid columns\n",
        "    drop_columns = list(df.loc[:,'cases_t-38':'cases_t-1'].columns)\n",
        "    df.drop(drop_columns, axis=1, inplace=True)\n",
        "\n",
        "    frames.append(df)\n",
        "\n",
        "  return (pd.concat(frames, ignore_index=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sit5iHT_0UWl"
      },
      "source": [
        "def reshape_dataframe(*data: np.ndarray):\n",
        "    # This function adds an extra dimension which is necessary in the LSTM\n",
        "    arr = []\n",
        "    for d in data:\n",
        "        arr.append(np.reshape(np.array(d), (d.shape[0], 1, d.shape[1])))\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1buCiuYoliN"
      },
      "source": [
        "def get_countries_sortedby_cases(valid_countries, df_grouped):\n",
        "  # A dictionary of all countries\n",
        "  dict_countries = Counter(valid_countries)\n",
        "\n",
        "  for country in dict_countries.keys():\n",
        "    dict_countries[country] = df_grouped.get_group(country)['cases'].sum()\n",
        "\n",
        "  # Sorting countries based on number of cases\n",
        "  countries_sortedby_cases = sorted(dict_countries.items(), key=lambda dict_countries: dict_countries[1], reverse=True)\n",
        "\n",
        "  # Creating dataframe \n",
        "  df_countries_sortedbycases = pd.DataFrame.from_dict(dict(countries_sortedby_cases), orient='index', columns=['Total Cases'])\n",
        "  \n",
        "  return df_countries_sortedbycases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvuRHEVArNJ0"
      },
      "source": [
        "# Getting a list of valid countries\n",
        "def get_countries_with_valid_size(df):\n",
        "  total_countries = list(df_grouped.groups.keys())\n",
        "\n",
        "  # A list for countries with required datasize\n",
        "  valid_countries = []\n",
        "\n",
        "  # List of countries with more than 230 records. Because, max training size = 150, lags removed = 50, prediction = 30.\n",
        "  for country in total_countries:\n",
        "    if len(df_grouped.get_group(country)) >= 230:\n",
        "      valid_countries.append(country)\n",
        "\n",
        "  return valid_countries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tqHxnYhwsbk"
      },
      "source": [
        "def preprocess_dataset(df):  \n",
        "  # Selecting required features\n",
        "  df= df[['dateRep','cases','countriesAndTerritories']]\n",
        "\n",
        "  # Rename features\n",
        "  df.rename(columns={'countriesAndTerritories':'country', 'dateRep':'date'}, inplace=True)\n",
        "\n",
        "  # Convert to date, sort and set index\n",
        "  df = sortby_date_and_set_index(df)\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6nW5zkHuFLn"
      },
      "source": [
        "# Calculating maximum of dataframe for every pretrain size\n",
        "def calc_max_of_pretrain_days(pretrain_days,df)->list:\n",
        "  max_of_pretrain_days = []\n",
        "  \n",
        "  for day in pretrain_days:\n",
        "    df_subset = create_subset(df,day)\n",
        "    max_of_pretrain_days.append(df_subset['cases'].max())\n",
        "  \n",
        "  return max_of_pretrain_days"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze3Ju2mOFltA"
      },
      "source": [
        "def display_scores(results):\n",
        "  #print(f'_________________________________{country}____________________________________________')\n",
        "  df_MAE = get_metric_with_mean(results,'MAE' )\n",
        "  df_RMSE = get_metric_with_mean(results,'RMSE')\n",
        "  df_MAPE = get_metric_with_mean(results,'MAPE')\n",
        "  print('MAE Score')\n",
        "  print(df_MAE.to_string())\n",
        "  print('-----------------------------------------------------------------------------------')\n",
        "  print('RMSE Score')\n",
        "  print(df_RMSE.to_string())\n",
        "  print('-----------------------------------------------------------------------------------')\n",
        "  print('MAPE Score')\n",
        "  print(df_MAPE.to_string())\n",
        "  print('\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZbb27ZOpQmT"
      },
      "source": [
        "## Alternate Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SKzw1NdWenN"
      },
      "source": [
        "def get_alternate_batch_records_idx(batch_size,total_records): \n",
        "  total_batches = total_records//batch_size\n",
        "  current_batch=1\n",
        "  start_idx = 0\n",
        "  end_idx = batch_size\n",
        "  idx_list = []\n",
        "  \n",
        "  while current_batch <= total_batches:\n",
        "    if current_batch%2!=0:\n",
        "      idx_list.extend([x for x in range(start_idx,end_idx)])\n",
        "      start_idx = idx_list[-1]+(batch_size+1)\n",
        "      end_idx = start_idx + batch_size\n",
        "    current_batch += 1\n",
        "\n",
        "  return idx_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1Sdy4o7oufO"
      },
      "source": [
        "def create_alternate_batch_subset(df,days,batch_size):\n",
        "  df_grouped = df.groupby('country')\n",
        "  countries = df['country'].unique()\n",
        "  frame1,frame2 = [],[]\n",
        "\n",
        "  for country in countries:\n",
        "    df_cur_country = df_grouped.get_group(country)\n",
        "\n",
        "    df1 = df_cur_country.iloc[0:days//2]\n",
        "    df2 = df_cur_country.iloc[days:days+30]  # Adding 30 for a testing batch that is one month ahead\n",
        "    \n",
        "    # Selecting alternate batches\n",
        "    idx = get_alternate_batch_records_idx(batch_size,total_records=len(df2))\n",
        "    df2 = df2.iloc[idx]\n",
        "\n",
        "    # Appending dataframes\n",
        "    frame1.append(df1)\n",
        "    frame2.append(df2)\n",
        "\n",
        "  r1 = pd.concat(frame1, ignore_index=True)\n",
        "  r2 = pd.concat(frame2, ignore_index=True)\n",
        "  r = r1.append(r2, ignore_index=True)\n",
        "  \n",
        "  return (r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPViJY9uDZ0q"
      },
      "source": [
        "## Incremental Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9DZn8PjUter"
      },
      "source": [
        "def instantiate_regressors():\n",
        "  ht_reg = HoeffdingTreeRegressor()\n",
        "  hat_reg = HoeffdingAdaptiveTreeRegressor()\n",
        "  arf_reg = AdaptiveRandomForestRegressor()\n",
        "  pa_reg = PassiveAggressiveRegressor(max_iter=1, random_state=0, tol=1e-3)\n",
        "\n",
        "  model = [ht_reg, hat_reg, arf_reg, pa_reg]\n",
        "  model_names = ['HT_Reg', 'HAT_Reg', 'ARF_Reg', 'PA_Reg']\n",
        "\n",
        "  return model, model_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NwhtdOBXBUy"
      },
      "source": [
        "def get_error_scores_per_model(evaluator, mdl_evaluation_scores)-> pd.DataFrame:\n",
        "  \n",
        "  for i in range(len(evaluator.model_names)):\n",
        "    # Desired error metrics\n",
        "    mse = evaluator.mean_eval_measurements[i].get_mean_square_error()\n",
        "    mae = evaluator.mean_eval_measurements[i].get_average_error()\n",
        "    mape = evaluator.mean_eval_measurements[i].get_mean_absolute_percentage_error()\n",
        "    rmse = sqrt(mse)\n",
        "\n",
        "    # Dictionary of errors per model\n",
        "    mdl_evaluation_scores[str(evaluator.model_names[i])] = [rmse, mae, mape]\n",
        "\n",
        "  return(pd.DataFrame(mdl_evaluation_scores))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSq_iuypN2BE"
      },
      "source": [
        "def get_running_time_per_model_incremental_learner(evaluator,day):\n",
        "    cols = ['PretrainDays']  # Adding pretrain as first column\n",
        "    cols += evaluator.model_names  # Adding remaining columns of different algorithm\n",
        "    running_time = []\n",
        "    running_time.append(day)\n",
        "    for i in range(len(evaluator.model_names)):\n",
        "        running_time.append(evaluator.running_time_measurements[i]._total_time)\n",
        "\n",
        "    return (pd.DataFrame([running_time],columns=cols))  # Passing running_time as a list of list to insert it as a row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VbOOY4WfUzT"
      },
      "source": [
        "def display_countrywise_scores(country,df_error_metric):\n",
        "  print(f'_________________________________{country}____________________________________________')\n",
        "  print(df_error_metric.to_string())\n",
        "  print('\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC8vD-Dl3uNs"
      },
      "source": [
        "# Create a dataframe of all countries with pre-train size = pretrain days and test&train size = pretrain days\n",
        "def create_subset(result,days):\n",
        "  result_grouped = result.groupby('country')\n",
        "  countries = result['country'].unique()\n",
        "  frame1,frame2 = [],[]\n",
        "  for country in countries:\n",
        "    df = result_grouped.get_group(country) \n",
        "    df1 = df.iloc[0:days]\n",
        "    df2 = df.iloc[days:days+30]\n",
        "    frame1.append(df1)\n",
        "    frame2.append(df2)\n",
        "\n",
        "  r1 = pd.concat(frame1, ignore_index=True)\n",
        "  r2 = pd.concat(frame2, ignore_index=True)\n",
        "  r = r1.append(r2, ignore_index=True)\n",
        "  \n",
        "  return (r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGzbZRfVDc-c"
      },
      "source": [
        "## Static Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKVRq7O9onX7"
      },
      "source": [
        "def mean_absolute_percentage_error(actual, predicted):\n",
        "    \"\"\"\n",
        "    Mean absolute percentage error (MAPE).\n",
        "    :return error\n",
        "    \"\"\"\n",
        "    actual =  np.array(actual) \n",
        "    predicted = np.array(predicted) \n",
        "\n",
        "    mask = actual != 0\n",
        "    return (np.fabs(actual - predicted) / np.fabs(actual))[mask].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olpCsifni4p-"
      },
      "source": [
        "def get_scores(y_true, model_predictions, days):\n",
        "    mdl_evaluation_scores = {}\n",
        "    mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE', 'MAE', 'MAPE']\n",
        "    mdl_evaluation_scores['PretrainDays'] = [days] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "\n",
        "    for model in model_predictions:\n",
        "        y_pred = model_predictions[model]\n",
        "        if model == 'LSTM':\n",
        "            rmse = mean_squared_error(y_true[:, np.newaxis], y_pred, squared=False)\n",
        "            mae = mean_absolute_error(y_true[:, np.newaxis], y_pred)\n",
        "            mape = mean_absolute_percentage_error(y_true[:, np.newaxis], y_pred)\n",
        "        else:\n",
        "            rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "            mae = mean_absolute_error(y_true, y_pred)\n",
        "            mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "\n",
        "        mdl_evaluation_scores[model] = [rmse, mae, mape]\n",
        "    return pd.DataFrame(mdl_evaluation_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBKHhy5I0sWy"
      },
      "source": [
        "def get_running_time_per_model_static_learner(model_predictions,total_execution_time):\n",
        "    cols = ['PretrainDays']\n",
        "    cols += model_predictions.keys()\n",
        "    return pd.DataFrame(total_execution_time, columns=cols)\n",
        "\n",
        "\n",
        "def measure(wrapped_func):\n",
        "    @wraps(wrapped_func)\n",
        "    def _time_it(*args, **kwargs):\n",
        "        start = pc_timer()\n",
        "        try:\n",
        "            model_predictions = wrapped_func(*args, **kwargs)\n",
        "        finally:\n",
        "            end_ = pc_timer() - start\n",
        "            return model_predictions, end_\n",
        "    return _time_it\n",
        "\n",
        "\n",
        "@measure\n",
        "def train_test_model(regressor, X_train, y_train, X_test):\n",
        "    regressor.fit(X_train, y_train)\n",
        "    return regressor.predict(X_test)\n",
        "\n",
        "\n",
        "@measure\n",
        "def train_test_lstm(regressor, X_train_lstm, y_train, X_val_lstm, y_val, X_test_lstm, patience, epochs, batch_size_lstm):\n",
        "    regressor.compile(loss='mae', optimizer='adagrad', metrics=['mse', 'mae'])\n",
        "\n",
        "    history = regressor.fit(\n",
        "        X_train_lstm,\n",
        "        y_train,\n",
        "        validation_data=(X_val_lstm, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size_lstm,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss',\n",
        "                                 mode='min',\n",
        "                                 patience=patience)])\n",
        "\n",
        "    return regressor.predict(X_test_lstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB-GhumBoRRH"
      },
      "source": [
        "def define_lstm_model(x_train_lstm, layers, activations, patience):\n",
        "    # Start defining the model\n",
        "    input_shape = x_train_lstm.shape\n",
        "\n",
        "    # Definining model first with LSTM n layers\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(layers[0], input_shape=input_shape[1:], activation=activations[0], return_sequences=True))\n",
        "\n",
        "    # Adding middle layers\n",
        "    for l in range(1, len(layers)-1):\n",
        "      model.add(LSTM(layers[l], activation=activations[l], return_sequences=True))\n",
        "      model.add(Dropout(0.2))\n",
        "\n",
        "    # Add last Dense and LSTMs layers\n",
        "    if len(layers) > 1:\n",
        "      model.add(Dense(layers[-1], activation=activations[-1]))\n",
        "      model.add(Dropout(0.2))\n",
        "      model.add(LSTM(layers[-1], activation=activations[-1]))\n",
        "\n",
        "    model.add(Dense(1))  # output layer. Since we have only 1 output value\n",
        "    # End defining model\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjnk6w9_cqG6"
      },
      "source": [
        "def normalize_dataset(*dataframes):\n",
        "    arr = []\n",
        "    for df in dataframes:\n",
        "        arr.append(StandardScaler().fit_transform(df))\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GQGxLLNw1z5"
      },
      "source": [
        "def get_validation_set(df_train, batch_size=10):\n",
        "    lst_idx = -1\n",
        "    total_batches = len(df_train) // batch_size\n",
        "    train_set, val_set = [], []\n",
        "\n",
        "    for cur_batch in range(total_batches):\n",
        "        start = lst_idx + 1\n",
        "        end = start + batch_size\n",
        "        if cur_batch % 2 == 0:\n",
        "            train_set.append(df_train.iloc[start:end])\n",
        "        else:\n",
        "            val_set.append(df_train.iloc[start:end])\n",
        "\n",
        "        lst_idx = end - 1  # adjusting last index because we add 1 in starting\n",
        "\n",
        "    return pd.concat(train_set, ignore_index=True), pd.concat(val_set, ignore_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMyZ5jcy_m6j"
      },
      "source": [
        "# Experiment 1\n",
        "Training and testing with five countries individually. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDnpy-TycyY9"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "* cases(t): Number of cases on current day(Column='cases') \n",
        "\n",
        "* cases(t-n): Number of cases 'n' days before current day 't'\n",
        "\n",
        "* 30 day gap: Training from day number t-89 to t-39(50 days). Then a gap of 30 days and then creating target by averaging t to t-9(10 Days).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "SeJJB0sdNnIq",
        "outputId": "2f2962b9-3031-4401-bb07-968d3672bb7b"
      },
      "source": [
        "# Sample set for understanding dataset\n",
        "sample_df = pd.read_csv(f'{csv_processed_path}/United_States_of_America.csv')\n",
        "sample_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>day_no</th>\n",
              "      <th>country</th>\n",
              "      <th>cases</th>\n",
              "      <th>cases_t-89</th>\n",
              "      <th>cases_t-88</th>\n",
              "      <th>cases_t-87</th>\n",
              "      <th>cases_t-86</th>\n",
              "      <th>cases_t-85</th>\n",
              "      <th>cases_t-84</th>\n",
              "      <th>cases_t-83</th>\n",
              "      <th>cases_t-82</th>\n",
              "      <th>cases_t-81</th>\n",
              "      <th>cases_t-80</th>\n",
              "      <th>cases_t-79</th>\n",
              "      <th>cases_t-78</th>\n",
              "      <th>cases_t-77</th>\n",
              "      <th>cases_t-76</th>\n",
              "      <th>cases_t-75</th>\n",
              "      <th>cases_t-74</th>\n",
              "      <th>cases_t-73</th>\n",
              "      <th>cases_t-72</th>\n",
              "      <th>cases_t-71</th>\n",
              "      <th>cases_t-70</th>\n",
              "      <th>cases_t-69</th>\n",
              "      <th>cases_t-68</th>\n",
              "      <th>cases_t-67</th>\n",
              "      <th>cases_t-66</th>\n",
              "      <th>cases_t-65</th>\n",
              "      <th>cases_t-64</th>\n",
              "      <th>cases_t-63</th>\n",
              "      <th>cases_t-62</th>\n",
              "      <th>cases_t-61</th>\n",
              "      <th>cases_t-60</th>\n",
              "      <th>cases_t-59</th>\n",
              "      <th>cases_t-58</th>\n",
              "      <th>cases_t-57</th>\n",
              "      <th>cases_t-56</th>\n",
              "      <th>cases_t-55</th>\n",
              "      <th>cases_t-54</th>\n",
              "      <th>cases_t-53</th>\n",
              "      <th>cases_t-52</th>\n",
              "      <th>cases_t-51</th>\n",
              "      <th>cases_t-50</th>\n",
              "      <th>cases_t-49</th>\n",
              "      <th>cases_t-48</th>\n",
              "      <th>cases_t-47</th>\n",
              "      <th>cases_t-46</th>\n",
              "      <th>cases_t-45</th>\n",
              "      <th>cases_t-44</th>\n",
              "      <th>cases_t-43</th>\n",
              "      <th>cases_t-42</th>\n",
              "      <th>cases_t-41</th>\n",
              "      <th>cases_t-40</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>90</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>19979</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>11525.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-30</td>\n",
              "      <td>91</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>18360</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>12877.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-03-31</td>\n",
              "      <td>92</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>21595</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>14499.600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-04-01</td>\n",
              "      <td>93</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>24998</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>16287.100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-04-02</td>\n",
              "      <td>94</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>27103</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>19.000</td>\n",
              "      <td>18151.500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         date  day_no  ... cases_t-40    target\n",
              "0  2020-03-29      90  ...      0.000 11525.000\n",
              "1  2020-03-30      91  ...      0.000 12877.500\n",
              "2  2020-03-31      92  ...      0.000 14499.600\n",
              "3  2020-04-01      93  ...      1.000 16287.100\n",
              "4  2020-04-02      94  ...     19.000 18151.500\n",
              "\n",
              "[5 rows x 55 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJiGVxO_hsNv"
      },
      "source": [
        "## Incremental Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZrg_v5CLReb"
      },
      "source": [
        "def reset_evaluator(evaluator):  # Added Now\r\n",
        "  for j in range(evaluator.n_models):\r\n",
        "      evaluator.mean_eval_measurements[j].reset()\r\n",
        "      evaluator.current_eval_measurements[j].reset()\r\n",
        "  return evaluator\r\n",
        "\r\n",
        "\r\n",
        "def update_incremental_metrics(evaluator, y, prediction):  # Added Now\r\n",
        "  for j in range(evaluator.n_models):\r\n",
        "    for i in range(len(prediction[0])):\r\n",
        "      evaluator.mean_eval_measurements[j].add_result(y[i], prediction[j][i])\r\n",
        "      evaluator.current_eval_measurements[j].add_result(y[i], prediction[j][i])\r\n",
        "\r\n",
        "    # Adding result manually causes y_true_vector to have a objects inserted like array([123.45]) in a list.\r\n",
        "    # For calculating metrics we have to convert them into flat list.\r\n",
        "    evaluator.mean_eval_measurements[j].y_true_vector = np.array(evaluator.mean_eval_measurements[j].y_true_vector).flatten().tolist()\r\n",
        "    evaluator.current_eval_measurements[j].y_true_vector = np.array(evaluator.current_eval_measurements[j].y_true_vector).flatten().tolist()\r\n",
        "  return evaluator\r\n",
        "\r\n",
        "\r\n",
        "def get_error_scores_per_model(evaluator, mdl_evaluation_scores) -> pd.DataFrame:\r\n",
        "  for i in range(len(evaluator.model_names)):\r\n",
        "    # Desired error metrics\r\n",
        "    mse = evaluator.mean_eval_measurements[i].get_mean_square_error()\r\n",
        "    mae = evaluator.mean_eval_measurements[i].get_average_error()\r\n",
        "    mae = mae[0]  # get_average_error() is returning a List instead of single value.  # Updated Now\r\n",
        "    mape = evaluator.mean_eval_measurements[i].get_mean_absolute_percentage_error()\r\n",
        "    rmse = sqrt(mse)\r\n",
        "\r\n",
        "    # Dictionary of errors per model\r\n",
        "    mdl_evaluation_scores[str(evaluator.model_names[i])] = [rmse, mae, mape]\r\n",
        "  return (pd.DataFrame(mdl_evaluation_scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw3R1IKXR05k"
      },
      "source": [
        "\"\"\"\r\n",
        "# Old\r\n",
        "\r\n",
        "def scikit_multiflow(df, pretrain_days):\r\n",
        "\r\n",
        "  # Creating a stream from dataframe\r\n",
        "  stream = DataStream(np.array(df.iloc[:,4:-1]), y=np.array(df.iloc[:,-1])) # Selecting features x=[t-89:t-39] and y=[target]. TODO: Drop columns with name \r\n",
        "\r\n",
        "  model, model_names = instantiate_regressors()\r\n",
        "\r\n",
        "  frames, running_time_frames = [], []\r\n",
        "\r\n",
        "  # Setup the evaluator\r\n",
        "  for day in pretrain_days:\r\n",
        "\r\n",
        "      pretrain_days = day\r\n",
        "      max_samples = pretrain_days + 30  #Testing on set one month ahead only\r\n",
        "\r\n",
        "      '''evaluator = EvaluatePrequential(show_plot=False,\r\n",
        "                                    pretrain_size=day,\r\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'], \r\n",
        "                                    max_samples=max_samples)'''\r\n",
        "      \r\n",
        "      evaluator = EvaluatePrequential(show_plot=False,\r\n",
        "                                    pretrain_size=day-1,\r\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'], \r\n",
        "                                    max_samples=day)\r\n",
        "\r\n",
        "\r\n",
        "      # Run evaluation\r\n",
        "      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\r\n",
        "\r\n",
        "      # Dictionary to store each iteration error scores\r\n",
        "      mdl_evaluation_scores = {}\r\n",
        "\r\n",
        "      # Adding Evaluation Measurements and pretraining days\r\n",
        "      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE']\r\n",
        "      mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\r\n",
        "      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\r\n",
        "\r\n",
        "      # Errors of each model on a specific pre-train days\r\n",
        "      frames.append(mdl_evaluation_df)\r\n",
        "\r\n",
        "      # Run time for each algorithm\r\n",
        "      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\r\n",
        "\r\n",
        "  # Final Run Time DataFrame\r\n",
        "  running_time_df = pd.concat(running_time_frames,ignore_index=True)\r\n",
        "\r\n",
        "  # Final Evaluation Score Dataframe\r\n",
        "  evaluation_scores_df = pd.concat(frames, ignore_index=True)\r\n",
        "  return evaluation_scores_df, running_time_df\r\n",
        "\r\n",
        "for country in countries:\r\n",
        "  # Read each country  \r\n",
        "  df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\r\n",
        "\r\n",
        "  # Get evaluation scores and running time for country\r\n",
        "  evaluation_scores_df, running_time_df = scikit_multiflow(df_country,pretrain_days)\r\n",
        "\r\n",
        "  # Appending evaluation scores and runtime for each country\r\n",
        "  results_incremental.append(evaluation_scores_df)\r\n",
        "  results_runtime_incremental.append(running_time_df)\r\n",
        "\r\n",
        "  # Get max of each pretrain subset and for each country dataset\r\n",
        "  max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days,df_country))\r\n",
        "  max_cases_per_country.append(df_country['cases'].max())\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ3htjYaL-1o"
      },
      "source": [
        "def scikit_multiflow(df, pretrain_days):  \r\n",
        "  # Creating a stream from dataframe\r\n",
        "  stream = DataStream(np.array(df.iloc[:, 4:-1]), y=np.array(\r\n",
        "      df.iloc[:, -1]))  # Selecting features x=[t-89:t-39] and y=[target]. TODO: Drop columns with name\r\n",
        "\r\n",
        "  model, model_names = instantiate_regressors()\r\n",
        "\r\n",
        "  frames, running_time_frames = [], []\r\n",
        "\r\n",
        "  # Setup the evaluator\r\n",
        "  for day in pretrain_days:\r\n",
        "    pretrain_days = day\r\n",
        "    # max_samples = pretrain_days + 30  # Training and then testing on set one month ahead only\r\n",
        "    max_samples = pretrain_days + 1  # Updated Now\r\n",
        "    testing_samples_size = 30  # Updated Now\r\n",
        "\r\n",
        "    evaluator = EvaluatePrequential(show_plot=False,\r\n",
        "                                    pretrain_size=pretrain_days,\r\n",
        "                                    metrics=['mean_square_error', 'mean_absolute_error',\r\n",
        "                                              'mean_absolute_percentage_error'],\r\n",
        "                                    max_samples=max_samples)\r\n",
        "\r\n",
        "    # Run evaluation\r\n",
        "    evaluator.evaluate(stream=stream, model=model, model_names=model_names)\r\n",
        "\r\n",
        "    X = stream.X[pretrain_days: pretrain_days + testing_samples_size]  # Added Now\r\n",
        "    y = stream.y[pretrain_days: pretrain_days + testing_samples_size]  # Added Now\r\n",
        "\r\n",
        "    prediction = evaluator.predict(X)  # Added Now\r\n",
        "\r\n",
        "    # Since we add one extra sample, reset the evaluator, to test on complete new samples\r\n",
        "    evaluator = reset_evaluator(evaluator)  # Added Now\r\n",
        "\r\n",
        "    # Updating metrics\r\n",
        "    evaluator = update_incremental_metrics(evaluator, y, prediction)  # Added Now\r\n",
        "\r\n",
        "    # Dictionary to store each iteration error scores\r\n",
        "    mdl_evaluation_scores = {}\r\n",
        "\r\n",
        "    # Adding Evaluation Measurements and pretraining days\r\n",
        "    mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE', 'MAE', 'MAPE']\r\n",
        "    mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\r\n",
        "    mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\r\n",
        "\r\n",
        "    # Errors of each model on a specific pre-train days\r\n",
        "    frames.append(mdl_evaluation_df)\r\n",
        "\r\n",
        "    # Run time for each algorithm\r\n",
        "    running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\r\n",
        "\r\n",
        "  # Final Run Time DataFrame\r\n",
        "  running_time_df = pd.concat(running_time_frames, ignore_index=True)\r\n",
        "\r\n",
        "  # Final Evaluation Score Dataframe\r\n",
        "  evaluation_scores_df = pd.concat(frames, ignore_index=True)\r\n",
        "  return evaluation_scores_df, running_time_df\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUw_30g9T4fS",
        "outputId": "ac37d0da-8433-4157-92c5-aa1d446467bc"
      },
      "source": [
        "# Training all countries\n",
        "results_incremental = []\n",
        "results_runtime_incremental = []\n",
        "max_of_pretrain_per_country = []\n",
        "max_cases_per_country = []\n",
        "\n",
        "for country in countries:\n",
        "    # Read each country\n",
        "    df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\n",
        "\n",
        "    # Get evaluation scores and running time for country\n",
        "    evaluation_scores_df, running_time_df, united_dataframe = scikit_multiflow(df_country,pretrain_days, country)\n",
        "\n",
        "    save_united_df(united_dataframe, exp1_inc_united_df_path, country=country)\n",
        "\n",
        "    # Appending evaluation scores and runtime for each country\n",
        "    results_incremental.append(evaluation_scores_df)\n",
        "\n",
        "    results_runtime_incremental.append(running_time_df)\n",
        "\n",
        "    # Get max of each pretrain subset and for each country dataset\n",
        "    max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days,df_country))\n",
        "    max_cases_per_country.append(df_country['cases'].max())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.52s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 27469156.2458\n",
            "HT_Reg - MAPE          : 0.1831\n",
            "HT_Reg - MAE          : 5241.102579\n",
            "HAT_Reg - MSE          : 518033.4060\n",
            "HAT_Reg - MAPE          : 0.0251\n",
            "HAT_Reg - MAE          : 719.745376\n",
            "ARF_Reg - MSE          : 19028.5069\n",
            "ARF_Reg - MAPE          : 0.0048\n",
            "ARF_Reg - MAE          : 137.943854\n",
            "PA_Reg - MSE          : 116920245843.2590\n",
            "PA_Reg - MAPE          : 11.9438\n",
            "PA_Reg - MAE          : 341936.025951\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.21s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1232322757.5353\n",
            "HT_Reg - MAPE          : 1.6467\n",
            "HT_Reg - MAE          : 35104.454953\n",
            "HAT_Reg - MSE          : 625852723.9667\n",
            "HAT_Reg - MAPE          : 1.1735\n",
            "HAT_Reg - MAE          : 25017.048666\n",
            "ARF_Reg - MSE          : 6537951.8358\n",
            "ARF_Reg - MAPE          : 0.1199\n",
            "ARF_Reg - MAE          : 2556.941891\n",
            "PA_Reg - MSE          : 10016903936937.6016\n",
            "PA_Reg - MAPE          : 148.4665\n",
            "PA_Reg - MAE          : 3164949.278731\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.88s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 4467646.2960\n",
            "HT_Reg - MAPE          : 0.0640\n",
            "HT_Reg - MAE          : 2113.680746\n",
            "HAT_Reg - MSE          : 2527963.3441\n",
            "HAT_Reg - MAPE          : 0.0482\n",
            "HAT_Reg - MAE          : 1589.957026\n",
            "ARF_Reg - MSE          : 84212632.5433\n",
            "ARF_Reg - MAPE          : 0.2780\n",
            "ARF_Reg - MAE          : 9176.744115\n",
            "PA_Reg - MSE          : 5823844781569.0518\n",
            "PA_Reg - MAPE          : 73.1057\n",
            "PA_Reg - MAE          : 2413264.341420\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.03s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 466570325.4195\n",
            "HT_Reg - MAPE          : 0.3284\n",
            "HT_Reg - MAE          : 21600.239013\n",
            "HAT_Reg - MSE          : 466572413.7923\n",
            "HAT_Reg - MAPE          : 0.3284\n",
            "HAT_Reg - MAE          : 21600.287354\n",
            "ARF_Reg - MSE          : 480618556.7197\n",
            "ARF_Reg - MAPE          : 0.3333\n",
            "ARF_Reg - MAE          : 21923.014316\n",
            "PA_Reg - MSE          : 23272453574946.4727\n",
            "PA_Reg - MAPE          : 73.3378\n",
            "PA_Reg - MAE          : 4824153.145884\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.08s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 110984742.2613\n",
            "HT_Reg - MAPE          : 0.2521\n",
            "HT_Reg - MAE          : 10534.929628\n",
            "HAT_Reg - MSE          : 110984711.4356\n",
            "HAT_Reg - MAPE          : 0.2521\n",
            "HAT_Reg - MAE          : 10534.928165\n",
            "ARF_Reg - MSE          : 60962619.0807\n",
            "ARF_Reg - MAPE          : 0.1869\n",
            "ARF_Reg - MAE          : 7807.856241\n",
            "PA_Reg - MSE          : 138922480273135.6875\n",
            "PA_Reg - MAPE          : 282.0670\n",
            "PA_Reg - MAE          : 11786538.095350\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [5.10s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 7973.3554\n",
            "HT_Reg - MAPE          : 0.0021\n",
            "HT_Reg - MAE          : 89.293647\n",
            "HAT_Reg - MSE          : 7973.3658\n",
            "HAT_Reg - MAPE          : 0.0021\n",
            "HAT_Reg - MAE          : 89.293705\n",
            "ARF_Reg - MSE          : 1208196523.2502\n",
            "ARF_Reg - MAPE          : 0.8206\n",
            "ARF_Reg - MAE          : 34759.121440\n",
            "PA_Reg - MSE          : 407133534203.6523\n",
            "PA_Reg - MAPE          : 15.0631\n",
            "PA_Reg - MAE          : 638070.164013\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.82s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 167.4348\n",
            "HT_Reg - MAPE          : 0.0083\n",
            "HT_Reg - MAE          : 12.939661\n",
            "HAT_Reg - MSE          : 207467.2504\n",
            "HAT_Reg - MAPE          : 0.2916\n",
            "HAT_Reg - MAE          : 455.485730\n",
            "ARF_Reg - MSE          : 116596.4550\n",
            "ARF_Reg - MAPE          : 0.2186\n",
            "ARF_Reg - MAE          : 341.462231\n",
            "PA_Reg - MSE          : 1551618.2210\n",
            "PA_Reg - MAPE          : 0.7975\n",
            "PA_Reg - MAE          : 1245.639683\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.37s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 5124839.7408\n",
            "HT_Reg - MAPE          : 0.3501\n",
            "HT_Reg - MAE          : 2263.810889\n",
            "HAT_Reg - MSE          : 1117933.4186\n",
            "HAT_Reg - MAPE          : 0.1635\n",
            "HAT_Reg - MAE          : 1057.323706\n",
            "ARF_Reg - MSE          : 168576739.8223\n",
            "ARF_Reg - MAPE          : 2.0080\n",
            "ARF_Reg - MAE          : 12983.710557\n",
            "PA_Reg - MSE          : 1090244779.9753\n",
            "PA_Reg - MAPE          : 5.1065\n",
            "PA_Reg - MAE          : 33018.854916\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.75s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 7910047.9848\n",
            "HT_Reg - MAPE          : 0.1737\n",
            "HT_Reg - MAE          : 2812.480753\n",
            "HAT_Reg - MSE          : 5522954.8854\n",
            "HAT_Reg - MAPE          : 0.1451\n",
            "HAT_Reg - MAE          : 2350.096782\n",
            "ARF_Reg - MSE          : 161880413.3679\n",
            "ARF_Reg - MAPE          : 0.7858\n",
            "ARF_Reg - MAE          : 12723.223387\n",
            "PA_Reg - MSE          : 14335884411.2420\n",
            "PA_Reg - MAPE          : 7.3949\n",
            "PA_Reg - MAE          : 119732.553682\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.70s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 459185.6879\n",
            "HT_Reg - MAPE          : 0.0152\n",
            "HT_Reg - MAE          : 677.632414\n",
            "HAT_Reg - MSE          : 767691.2882\n",
            "HAT_Reg - MAPE          : 0.0197\n",
            "HAT_Reg - MAE          : 876.179941\n",
            "ARF_Reg - MSE          : 152024.4858\n",
            "ARF_Reg - MAPE          : 0.0088\n",
            "ARF_Reg - MAE          : 389.903175\n",
            "PA_Reg - MSE          : 10315609496.2229\n",
            "PA_Reg - MAPE          : 2.2853\n",
            "PA_Reg - MAE          : 101565.789005\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.23s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 11902.3232\n",
            "HT_Reg - MAPE          : 0.0016\n",
            "HT_Reg - MAE          : 109.097769\n",
            "HAT_Reg - MSE          : 41002.5431\n",
            "HAT_Reg - MAPE          : 0.0031\n",
            "HAT_Reg - MAE          : 202.490847\n",
            "ARF_Reg - MSE          : 3371067211.1326\n",
            "ARF_Reg - MAPE          : 0.8763\n",
            "ARF_Reg - MAE          : 58060.892270\n",
            "PA_Reg - MSE          : 5746350386.9501\n",
            "PA_Reg - MAPE          : 1.1441\n",
            "PA_Reg - MAE          : 75804.685785\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.07s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3732157387.1134\n",
            "HT_Reg - MAPE          : 0.6914\n",
            "HT_Reg - MAE          : 61091.385539\n",
            "HAT_Reg - MSE          : 3734596706.4590\n",
            "HAT_Reg - MAPE          : 0.6916\n",
            "HAT_Reg - MAE          : 61111.346790\n",
            "ARF_Reg - MSE          : 8489113494.2533\n",
            "ARF_Reg - MAPE          : 1.0428\n",
            "ARF_Reg - MAE          : 92136.385290\n",
            "PA_Reg - MSE          : 4947979233318.7197\n",
            "PA_Reg - MAPE          : 25.1751\n",
            "PA_Reg - MAE          : 2224405.366231\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.46s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 37565.6445\n",
            "HT_Reg - MAPE          : 0.0591\n",
            "HT_Reg - MAE          : 193.818586\n",
            "HAT_Reg - MSE          : 91.5391\n",
            "HAT_Reg - MAPE          : 0.0029\n",
            "HAT_Reg - MAE          : 9.567605\n",
            "ARF_Reg - MSE          : 207054.6931\n",
            "ARF_Reg - MAPE          : 0.1386\n",
            "ARF_Reg - MAE          : 455.032629\n",
            "PA_Reg - MSE          : 6131073.4676\n",
            "PA_Reg - MAPE          : 0.7545\n",
            "PA_Reg - MAE          : 2476.100456\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.01s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3322133.2001\n",
            "HT_Reg - MAPE          : 0.1068\n",
            "HT_Reg - MAE          : 1822.671995\n",
            "HAT_Reg - MSE          : 10218597.9641\n",
            "HAT_Reg - MAPE          : 0.1872\n",
            "HAT_Reg - MAE          : 3196.654183\n",
            "ARF_Reg - MSE          : 160171852.1256\n",
            "ARF_Reg - MAPE          : 0.7412\n",
            "ARF_Reg - MAE          : 12655.901869\n",
            "PA_Reg - MSE          : 908456228.3308\n",
            "PA_Reg - MAPE          : 1.7653\n",
            "PA_Reg - MAE          : 30140.607630\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.73s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1176042654.8701\n",
            "HT_Reg - MAPE          : 0.9748\n",
            "HT_Reg - MAE          : 34293.478314\n",
            "HAT_Reg - MSE          : 1182320415.2017\n",
            "HAT_Reg - MAPE          : 0.9774\n",
            "HAT_Reg - MAE          : 34384.886436\n",
            "ARF_Reg - MSE          : 5371657311.6336\n",
            "ARF_Reg - MAPE          : 2.0834\n",
            "ARF_Reg - MAE          : 73291.591002\n",
            "PA_Reg - MSE          : 25154012961.7168\n",
            "PA_Reg - MAPE          : 4.5084\n",
            "PA_Reg - MAE          : 158600.166966\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.24s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 8912238908.3391\n",
            "HT_Reg - MAPE          : 2.3199\n",
            "HT_Reg - MAE          : 94404.655120\n",
            "HAT_Reg - MSE          : 8912467119.2041\n",
            "HAT_Reg - MAPE          : 2.3199\n",
            "HAT_Reg - MAE          : 94405.863797\n",
            "ARF_Reg - MSE          : 835900331.1455\n",
            "ARF_Reg - MAPE          : 0.7105\n",
            "ARF_Reg - MAE          : 28911.940979\n",
            "PA_Reg - MSE          : 119026286088.2047\n",
            "PA_Reg - MAPE          : 8.4780\n",
            "PA_Reg - MAE          : 345001.863891\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.18s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1006513111.9963\n",
            "HT_Reg - MAPE          : 0.8990\n",
            "HT_Reg - MAE          : 31725.590806\n",
            "HAT_Reg - MSE          : 1006420422.6122\n",
            "HAT_Reg - MAPE          : 0.8990\n",
            "HAT_Reg - MAE          : 31724.129974\n",
            "ARF_Reg - MSE          : 2223574059.9731\n",
            "ARF_Reg - MAPE          : 1.3362\n",
            "ARF_Reg - MAE          : 47154.788304\n",
            "PA_Reg - MSE          : 2392688382669.3599\n",
            "PA_Reg - MAPE          : 43.8321\n",
            "PA_Reg - MAE          : 1546831.724096\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.67s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 10065323.4748\n",
            "HT_Reg - MAPE          : 0.1017\n",
            "HT_Reg - MAE          : 3172.589396\n",
            "HAT_Reg - MSE          : 10065436.9598\n",
            "HAT_Reg - MAPE          : 0.1017\n",
            "HAT_Reg - MAE          : 3172.607281\n",
            "ARF_Reg - MSE          : 458471.0358\n",
            "ARF_Reg - MAPE          : 0.0217\n",
            "ARF_Reg - MAE          : 677.104893\n",
            "PA_Reg - MSE          : 5881873219.8205\n",
            "PA_Reg - MAPE          : 2.4574\n",
            "PA_Reg - MAE          : 76693.371420\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " ###################- [97%] [0.46s]\n",
            "Processed samples: 31\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 344535.0149\n",
            "HT_Reg - MAPE          : 0.1065\n",
            "HT_Reg - MAE          : 586.971051\n",
            "HAT_Reg - MSE          : 144641.9443\n",
            "HAT_Reg - MAPE          : 0.0690\n",
            "HAT_Reg - MAE          : 380.318215\n",
            "ARF_Reg - MSE          : 359688.1853\n",
            "ARF_Reg - MAPE          : 0.1088\n",
            "ARF_Reg - MAE          : 599.740098\n",
            "PA_Reg - MSE          : 4742279.9691\n",
            "PA_Reg - MAPE          : 0.3949\n",
            "PA_Reg - MAE          : 2177.677655\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.13s]\n",
            "Processed samples: 61\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 4150905.8017\n",
            "HT_Reg - MAPE          : 0.2291\n",
            "HT_Reg - MAE          : 2037.377187\n",
            "HAT_Reg - MSE          : 281314.3385\n",
            "HAT_Reg - MAPE          : 0.0596\n",
            "HAT_Reg - MAE          : 530.390741\n",
            "ARF_Reg - MSE          : 519172046.1882\n",
            "ARF_Reg - MAPE          : 2.5622\n",
            "ARF_Reg - MAE          : 22785.347182\n",
            "PA_Reg - MSE          : 224168514.8517\n",
            "PA_Reg - MAPE          : 1.6836\n",
            "PA_Reg - MAE          : 14972.258175\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.86s]\n",
            "Processed samples: 91\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 482978515.5123\n",
            "HT_Reg - MAPE          : 2.9172\n",
            "HT_Reg - MAE          : 21976.772181\n",
            "HAT_Reg - MSE          : 482978515.5361\n",
            "HAT_Reg - MAPE          : 2.9172\n",
            "HAT_Reg - MAE          : 21976.772182\n",
            "ARF_Reg - MSE          : 1985238.0019\n",
            "ARF_Reg - MAPE          : 0.1870\n",
            "ARF_Reg - MAE          : 1408.984742\n",
            "PA_Reg - MSE          : 1401103626561.1465\n",
            "PA_Reg - MAPE          : 157.1204\n",
            "PA_Reg - MAE          : 1183682.232088\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.32s]\n",
            "Processed samples: 121\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 17558351.3849\n",
            "HT_Reg - MAPE          : 0.7020\n",
            "HT_Reg - MAE          : 4190.268653\n",
            "HAT_Reg - MSE          : 17558351.3849\n",
            "HAT_Reg - MAPE          : 0.7020\n",
            "HAT_Reg - MAE          : 4190.268653\n",
            "ARF_Reg - MSE          : 1600.5468\n",
            "ARF_Reg - MAPE          : 0.0067\n",
            "ARF_Reg - MAE          : 40.006834\n",
            "PA_Reg - MSE          : 53194449517.3266\n",
            "PA_Reg - MAPE          : 38.6408\n",
            "PA_Reg - MAE          : 230639.219382\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.11s]\n",
            "Processed samples: 151\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 21074.1018\n",
            "HT_Reg - MAPE          : 0.0301\n",
            "HT_Reg - MAE          : 145.169218\n",
            "HAT_Reg - MSE          : 21074.1018\n",
            "HAT_Reg - MAPE          : 0.0301\n",
            "HAT_Reg - MAE          : 145.169218\n",
            "ARF_Reg - MSE          : 2023.2719\n",
            "ARF_Reg - MAPE          : 0.0093\n",
            "ARF_Reg - MAE          : 44.980795\n",
            "PA_Reg - MSE          : 77889069315.2173\n",
            "PA_Reg - MAPE          : 57.7758\n",
            "PA_Reg - MAE          : 279086.132431\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.53s]\n",
            "Processed samples: 181\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 97763.9415\n",
            "HT_Reg - MAPE          : 0.0517\n",
            "HT_Reg - MAE          : 312.672259\n",
            "HAT_Reg - MSE          : 97763.9415\n",
            "HAT_Reg - MAPE          : 0.0517\n",
            "HAT_Reg - MAE          : 312.672259\n",
            "ARF_Reg - MSE          : 15956.8954\n",
            "ARF_Reg - MAPE          : 0.0209\n",
            "ARF_Reg - MAE          : 126.320606\n",
            "PA_Reg - MSE          : 268813038958.8328\n",
            "PA_Reg - MAPE          : 85.6752\n",
            "PA_Reg - MAE          : 518471.830439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HQdWByW7GTn",
        "outputId": "e969645d-4275-487b-af07-1baef826391b"
      },
      "source": [
        "# Save the running time for each country\n",
        "for i in range(len(countries)):\n",
        "  save_runtime(results_runtime_incremental[i], path=exp1_runtime_path, country = countries[i], static_learner=False)\n",
        "\n",
        "# Display countrywise running time complexity\n",
        "display_runtime_per_country(results_runtime_incremental, countries)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_____________Running Time for United_States_of_America________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.013    0.019    0.513   0.002\n",
            "1            60   0.032    0.045    1.147   0.002\n",
            "2            90   0.050    0.206    1.647   0.002\n",
            "3           120   0.124    0.164    2.771   0.002\n",
            "4           150   0.176    0.207    4.731   0.002\n",
            "5           180   0.254    0.267    4.600   0.004\n",
            "\n",
            "\n",
            "_____________Running Time for India________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.016    0.025    0.794   0.002\n",
            "1            60   0.047    0.060    1.285   0.002\n",
            "2            90   0.037    0.063    1.679   0.002\n",
            "3           120   0.082    0.291    2.356   0.002\n",
            "4           150   0.128    0.169    2.975   0.004\n",
            "5           180   0.190    0.216    3.686   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Brazil________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.010    0.018    0.446   0.002\n",
            "1            60   0.021    0.034    0.973   0.002\n",
            "2            90   0.040    0.060    1.657   0.002\n",
            "3           120   0.085    0.112    2.070   0.002\n",
            "4           150   0.130    0.164    2.921   0.002\n",
            "5           180   0.189    0.227    4.280   0.002\n",
            "\n",
            "\n",
            "_____________Running Time for Russia________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.009    0.013    0.455   0.002\n",
            "1            60   0.022    0.036    1.079   0.002\n",
            "2            90   0.040    0.197    1.651   0.003\n",
            "3           120   0.092    0.134    2.108   0.002\n",
            "4           150   0.152    0.204    2.771   0.002\n",
            "5           180   0.222    0.259    3.082   0.002\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMH2CG_nuloA",
        "outputId": "09164de5-5ba1-4816-e045-9dfe730d5cc0"
      },
      "source": [
        "countrywise_error_score_incremental = calc_save_err_metric_countrywise(countries, error_metrics, results_incremental, max_of_pretrain_per_country, max_cases_per_country, path=exp1_path, static_learner=False, transpose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________United_States_of_America____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays      HT_Reg     HAT_Reg     ARF_Reg      PA_Reg\n",
            "0                      MAE        30.000  303721.172  543820.366 1346053.727 1819162.372\n",
            "1                      MAE        60.000 1448008.929 1794044.130 1579798.627 5818965.043\n",
            "2                      MAE        90.000   20005.990   20705.389   43699.996  432785.063\n",
            "3                      MAE       120.000   11559.992   11560.028   11887.098 1818992.710\n",
            "4                      MAE       150.000   20166.300   20166.298   16844.438 7566808.707\n",
            "5                      MAE       180.000    4860.379    4860.379   42974.034  992635.876\n",
            "mean                   NaN       105.000  301387.127  399192.765  506876.320 3074891.628\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_States_of_America____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  13.687   24.001   59.129  79.974\n",
            "1                     MAPE        60.000  58.193   72.107   63.648 249.268\n",
            "2                     MAPE        90.000   0.341    0.354    0.812   8.037\n",
            "3                     MAPE       120.000   0.194    0.194    0.200  35.335\n",
            "4                     MAPE       150.000   0.519    0.519    0.428 194.068\n",
            "5                     MAPE       180.000   0.103    0.103    0.913  20.012\n",
            "mean                   NaN       105.000  12.173   16.213   20.855  97.783\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_States_of_America____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays      HT_Reg     HAT_Reg     ARF_Reg      PA_Reg\n",
            "0                     RMSE        30.000  704569.189  858867.826 2005681.777 2682115.944\n",
            "1                     RMSE        60.000 2109448.624 2612992.123 2279506.193 6646783.857\n",
            "2                     RMSE        90.000   23125.696   23792.641   44965.196  528386.433\n",
            "3                     RMSE       120.000   13663.692   13663.738   13949.439 2307348.646\n",
            "4                     RMSE       150.000   22518.172   22518.170   18077.332 8524420.726\n",
            "5                     RMSE       180.000    5778.620    5778.620   44161.349 1228151.989\n",
            "mean                   NaN       105.000  479850.665  589602.186  734390.214 3652867.932\n",
            "\n",
            "\n",
            "\n",
            "_________________________________India____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays     HT_Reg    HAT_Reg    ARF_Reg      PA_Reg\n",
            "0                      MAE        30.000   5071.061   1274.907   5711.347   36026.852\n",
            "1                      MAE        60.000  34275.411  31077.955  63321.861   29684.748\n",
            "2                      MAE        90.000   3298.225   3492.183  22774.136   19643.547\n",
            "3                      MAE       120.000  14407.237  14360.928  18823.388  221941.660\n",
            "4                      MAE       150.000  22995.251  22539.287 106160.259  410557.501\n",
            "5                      MAE       180.000 249576.299 249378.453 101936.527 1319724.742\n",
            "mean                   NaN       105.000  54937.247  53687.285  53121.253  339596.508\n",
            "\n",
            "\n",
            "\n",
            "_________________________________India____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   1.055    0.358    1.246   8.123\n",
            "1                     MAPE        60.000   2.715    2.467    5.208   2.608\n",
            "2                     MAPE        90.000   0.122    0.130    0.813   0.719\n",
            "3                     MAPE       120.000   0.233    0.232    0.307   3.773\n",
            "4                     MAPE       150.000   0.262    0.257    1.263   4.680\n",
            "5                     MAPE       180.000   3.527    3.524    1.491  19.889\n",
            "mean                   NaN       105.000   1.319    1.161    1.721   6.632\n",
            "\n",
            "\n",
            "\n",
            "_________________________________India____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays     HT_Reg    HAT_Reg    ARF_Reg      PA_Reg\n",
            "0                     RMSE        30.000   7742.883   1616.380   9172.181   47997.804\n",
            "1                     RMSE        60.000  54554.459  49231.409  90528.638   41134.832\n",
            "2                     RMSE        90.000   3803.759   3944.783  25266.589   25765.992\n",
            "3                     RMSE       120.000  19348.672  19334.726  24014.823  243010.217\n",
            "4                     RMSE       150.000  27670.931  27125.620 108746.043  535538.088\n",
            "5                     RMSE       180.000 251067.222 250857.246 108680.969 1603274.278\n",
            "mean                   NaN       105.000  60697.988  58685.027  61068.207  416120.202\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Brazil____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg    HAT_Reg   ARF_Reg     PA_Reg\n",
            "0                      MAE        30.000 80348.755  87965.892  4613.238  19764.431\n",
            "1                      MAE        60.000 74052.950 121976.997 16008.713  63852.384\n",
            "2                      MAE        90.000 82248.464  82880.514 29967.730 225313.012\n",
            "3                      MAE       120.000 76685.030  76683.716 76180.379 352417.677\n",
            "4                      MAE       150.000 41902.073  41899.610 32629.551 690891.821\n",
            "5                      MAE       180.000  2893.681   2893.682 27875.734 585389.598\n",
            "mean                   NaN       105.000 59688.492  69050.068 31212.558 322938.154\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Brazil____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   6.623    7.117    0.394   1.876\n",
            "1                     MAPE        60.000   2.549    4.329    0.637   2.433\n",
            "2                     MAPE        90.000   2.232    2.249    0.813   6.058\n",
            "3                     MAPE       120.000   1.819    1.819    1.807   8.418\n",
            "4                     MAPE       150.000   1.254    1.254    0.932  19.766\n",
            "5                     MAPE       180.000   0.125    0.125    1.112  24.702\n",
            "mean                   NaN       105.000   2.434    2.816    0.949  10.542\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Brazil____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays     HT_Reg    HAT_Reg   ARF_Reg     PA_Reg\n",
            "0                     RMSE        30.000 114919.648 131381.850  8352.977  26928.168\n",
            "1                     RMSE        60.000 138390.999 182570.691 18434.092  92941.595\n",
            "2                     RMSE        90.000  96809.866  97586.413 30093.541 282261.207\n",
            "3                     RMSE       120.000  82398.148  82396.673 81945.860 457331.020\n",
            "4                     RMSE       150.000  42020.297  42017.840 36968.016 930866.369\n",
            "5                     RMSE       180.000   3630.361   3630.393 28217.249 747161.566\n",
            "mean                   NaN       105.000  79694.887  89930.643 34001.956 422914.988\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Russia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays     HT_Reg    HAT_Reg    ARF_Reg      PA_Reg\n",
            "0                      MAE        30.000 150454.410 130994.788  23837.942  196166.536\n",
            "1                      MAE        60.000 128085.332 128291.879 299506.720  883443.165\n",
            "2                      MAE        90.000  83519.413  83519.413  74744.418 1400600.915\n",
            "3                      MAE       120.000   2196.961   2196.961    753.971  102094.475\n",
            "4                      MAE       150.000    144.777    144.777    122.101   57119.191\n",
            "5                      MAE       180.000   4431.450   4431.450   4240.652   34966.571\n",
            "mean                   NaN       105.000  61472.057  58263.211  67200.967  445731.809\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Russia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000  15.771   13.720    2.566  20.509\n",
            "1                     MAPE        60.000  15.803   15.774   36.306 105.468\n",
            "2                     MAPE        90.000  12.894   12.894   11.536 216.406\n",
            "3                     MAPE       120.000   0.415    0.415    0.147  19.613\n",
            "4                     MAPE       150.000   0.028    0.028    0.024  11.585\n",
            "5                     MAPE       180.000   0.375    0.375    0.354   3.794\n",
            "mean                   NaN       105.000   7.548    7.201    8.489  62.896\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Russia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays     HT_Reg    HAT_Reg    ARF_Reg      PA_Reg\n",
            "0                     RMSE        30.000 242985.273 209838.471  33562.316  301024.766\n",
            "1                     RMSE        60.000 227625.201 218997.500 433455.264 1080081.736\n",
            "2                     RMSE        90.000  93169.738  93169.738  83166.821 1576512.428\n",
            "3                     RMSE       120.000   2209.424   2209.424    889.490  116001.716\n",
            "4                     RMSE       150.000    171.533    171.533    135.713   75001.304\n",
            "5                     RMSE       180.000   5264.007   5264.007   5118.318   39017.680\n",
            "mean                   NaN       105.000  95237.529  88275.112  92721.320  531273.271\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "T6hi5rPZF4Uf",
        "outputId": "372ea48c-edff-49cd-e6a5-be507c76ab1d"
      },
      "source": [
        "# Get summary table for each country for specified metric\n",
        "summary_table_countrywise_incremental = get_summary_table_countrywise(countrywise_error_score_incremental, ['MAPE'], static_learner=False)\n",
        "\n",
        "# Saving the summary table\n",
        "save_summary_table(summary_table_countrywise_incremental, exp1_summary_path,country=True, static_learner=False,alternate_batch=False,transpose=True)\n",
        "\n",
        "summary_table_countrywise_incremental"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "      <th>EvaluationMeasurement</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Country(MAPE)</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>United_States_of_America</th>\n",
              "      <td>12.173</td>\n",
              "      <td>16.213</td>\n",
              "      <td>20.855</td>\n",
              "      <td>97.783</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>India</th>\n",
              "      <td>1.319</td>\n",
              "      <td>1.161</td>\n",
              "      <td>1.721</td>\n",
              "      <td>6.632</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Brazil</th>\n",
              "      <td>2.434</td>\n",
              "      <td>2.816</td>\n",
              "      <td>0.949</td>\n",
              "      <td>10.542</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Russia</th>\n",
              "      <td>7.548</td>\n",
              "      <td>7.201</td>\n",
              "      <td>8.489</td>\n",
              "      <td>62.896</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          HT_Reg  HAT_Reg  ...  PA_Reg  EvaluationMeasurement\n",
              "Country(MAPE)                              ...                               \n",
              "United_States_of_America  12.173   16.213  ...  97.783                   MAPE\n",
              "India                      1.319    1.161  ...   6.632                   MAPE\n",
              "Brazil                     2.434    2.816  ...  10.542                   MAPE\n",
              "Russia                     7.548    7.201  ...  62.896                   MAPE\n",
              "\n",
              "[4 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "-jEmM1GgdAqq",
        "outputId": "1a73e815-1a7a-4ec4-9d02-584435669c3d"
      },
      "source": [
        "sum_inc_countrywise_mean = get_sum_table_combined_mean(countrywise_error_score_incremental,results_runtime_incremental)\n",
        "save_combined_summary_table(sum_inc_countrywise_mean, exp1_summary_path, static_learner=False, transpose=True) \n",
        "sum_inc_countrywise_mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Metric</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MAE</th>\n",
              "      <td>119371.231</td>\n",
              "      <td>145048.333</td>\n",
              "      <td>164602.774</td>\n",
              "      <td>1045789.525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MAPE</th>\n",
              "      <td>5.868</td>\n",
              "      <td>6.848</td>\n",
              "      <td>8.004</td>\n",
              "      <td>44.463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RMSE</th>\n",
              "      <td>178870.267</td>\n",
              "      <td>206623.242</td>\n",
              "      <td>230545.424</td>\n",
              "      <td>1255794.098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time(sec)</th>\n",
              "      <td>0.090</td>\n",
              "      <td>0.133</td>\n",
              "      <td>2.153</td>\n",
              "      <td>0.002</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              HT_Reg    HAT_Reg    ARF_Reg      PA_Reg\n",
              "Metric                                                \n",
              "MAE       119371.231 145048.333 164602.774 1045789.525\n",
              "MAPE           5.868      6.848      8.004      44.463\n",
              "RMSE      178870.267 206623.242 230545.424 1255794.098\n",
              "Time(sec)      0.090      0.133      2.153       0.002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEBzePUFh8FO"
      },
      "source": [
        "## Static Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdlwyL_f04aH"
      },
      "source": [
        "def scikit_learn(df, training_days):\n",
        "    frames = []\n",
        "    model_predictions = {\n",
        "        'RandomForest': [],\n",
        "        'GradientBoosting': [],\n",
        "        'LinearSVR': [],\n",
        "        'DecisionTree': [],\n",
        "        'BayesianRidge': [],\n",
        "        'LSTM': []\n",
        "        #'MLPRegressor': [],\n",
        "        #'LinearRegression': []\n",
        "    }\n",
        "    total_execution_time = []\n",
        "            \n",
        "    # LSTM (TODO: feel free to put the whole LSTM definition in a funtion)\n",
        "    # params (others like epoch and batch size are also hardcoded in train_test_lstm())\n",
        "    layers = [50, 30, 20, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\n",
        "    activations = ['tanh', 'tanh', 'relu']\n",
        "    epochs = 500\n",
        "    patience = 20\n",
        "    batch_size_lstm = 10\n",
        "\n",
        "    for day in training_days:\n",
        "        \n",
        "        cur_exec_time = [day]  # Keeping runing time for each pre-train set\n",
        "\n",
        "        train = df.iloc[:day, :]\n",
        "        test = df.iloc[day:day + 30, :]  # Testing on set one month ahead only, hence day+30.\n",
        "        \n",
        "        # training and test sets for all models except LSTM\n",
        "        X_train, y_train = train.iloc[:, 4:-1], train.iloc[:, -1]\n",
        "        X_test, y_test = test.iloc[:, 4:-1], test.iloc[:, -1]\n",
        "\n",
        "        # Seperating validation set from train set\n",
        "        train_df, val_df = get_validation_set(train, batch_size=10)\n",
        "\n",
        "        # Splitting test and validation into dependent and independent sets\n",
        "        X_train_batch, y_train_batch = train_df.iloc[:, 4:-1], train_df.iloc[:, -1]  # Consist only odd batches\n",
        "        X_val_batch, y_val_batch = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\n",
        "\n",
        "        # Normalizing dataset\n",
        "        X_train_lstm_norm, X_test_lstm_norm, X_val_lstm_norm = normalize_dataset(X_train_batch, X_test, X_val_batch)\n",
        "\n",
        "        # Reshaping the dataframes\n",
        "        X_train_lstm, X_val_lstm, X_test_lstm = reshape_dataframe(X_train_lstm_norm, X_val_lstm_norm, X_test_lstm_norm)\n",
        "       \n",
        "\n",
        "        rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "        model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train,y_train,X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "        gb_reg = GradientBoostingRegressor(random_state=0)\n",
        "        model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "        lsv_reg = LinearSVR(random_state=0, tol=1e-5)\n",
        "        model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "        dt_reg = DecisionTreeRegressor(random_state=0)\n",
        "        model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "        br_reg = BayesianRidge()\n",
        "        model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        \n",
        "        lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\n",
        "        model_predictions['LSTM'],exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train_batch, X_val_lstm, y_val_batch, X_test_lstm, patience, epochs, batch_size_lstm)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        \n",
        "        mdl_evaluation_df = get_scores(y_test, model_predictions, day)\n",
        "        total_execution_time.append(cur_exec_time)\n",
        "        frames.append(mdl_evaluation_df)\n",
        "\n",
        "    evaluation_score_df = pd.concat(frames, ignore_index=True)\n",
        "    running_time_df = get_running_time_per_model_static_learner(model_predictions,total_execution_time)\n",
        "    return evaluation_score_df, running_time_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIMqSQZDjq1C",
        "outputId": "8453b419-9820-424f-f0be-e26b40573ce7"
      },
      "source": [
        "# Training all countries\n",
        "results_static = []\n",
        "results_runtime_static = []\n",
        "max_of_pretrain_per_country = []\n",
        "max_cases_per_country = []\n",
        "\n",
        "for country in countries:\n",
        "  # Read country wise csv file\n",
        "  df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\n",
        "\n",
        "  # Evaluation scores and running time of each algorithm over different pre-training days\n",
        "  evaluation_scores_df, running_time_df = scikit_learn(df_country, pretrain_days)\n",
        "\n",
        "  # Append result of each pretrain size in results\n",
        "  results_static.append(evaluation_scores_df)\n",
        "\n",
        "  # Appending every country runtime \n",
        "  results_runtime_static.append(running_time_df)\n",
        "\n",
        "  # Calculating max cases per country based on pre-train size\n",
        "  max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days,df_country))\n",
        "\n",
        "  # Maximum case of each country\n",
        "  max_cases_per_country.append(df_country['cases'].max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20 samples, validate on 10 samples\n",
            "Epoch 1/500\n",
            "20/20 [==============================] - 2s 96ms/step - loss: 24206.4551 - mse: 624614400.0000 - mae: 24206.4551 - val_loss: 30089.3496 - val_mse: 906655424.0000 - val_mae: 30089.3496\n",
            "Epoch 2/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24206.4189 - mse: 624612672.0000 - mae: 24206.4180 - val_loss: 30089.3184 - val_mse: 906653376.0000 - val_mae: 30089.3184\n",
            "Epoch 3/500\n",
            "20/20 [==============================] - 0s 843us/step - loss: 24206.3809 - mse: 624610880.0000 - mae: 24206.3809 - val_loss: 30089.2812 - val_mse: 906651264.0000 - val_mae: 30089.2812\n",
            "Epoch 4/500\n",
            "20/20 [==============================] - 0s 850us/step - loss: 24206.3174 - mse: 624608000.0000 - mae: 24206.3184 - val_loss: 30089.2227 - val_mse: 906647744.0000 - val_mae: 30089.2227\n",
            "Epoch 5/500\n",
            "20/20 [==============================] - 0s 941us/step - loss: 24206.1846 - mse: 624602240.0000 - mae: 24206.1836 - val_loss: 30089.1133 - val_mse: 906641088.0000 - val_mae: 30089.1133\n",
            "Epoch 6/500\n",
            "20/20 [==============================] - 0s 868us/step - loss: 24205.9014 - mse: 624590848.0000 - mae: 24205.9023 - val_loss: 30088.8184 - val_mse: 906623808.0000 - val_mae: 30088.8184\n",
            "Epoch 7/500\n",
            "20/20 [==============================] - 0s 930us/step - loss: 24204.7783 - mse: 624545664.0000 - mae: 24204.7773 - val_loss: 30087.8594 - val_mse: 906567168.0000 - val_mae: 30087.8594\n",
            "Epoch 8/500\n",
            "20/20 [==============================] - 0s 944us/step - loss: 24202.1992 - mse: 624448640.0000 - mae: 24202.1992 - val_loss: 30085.9023 - val_mse: 906451328.0000 - val_mae: 30085.9023\n",
            "Epoch 9/500\n",
            "20/20 [==============================] - 0s 966us/step - loss: 24199.3613 - mse: 624322496.0000 - mae: 24199.3613 - val_loss: 30082.8594 - val_mse: 906270912.0000 - val_mae: 30082.8594\n",
            "Epoch 10/500\n",
            "20/20 [==============================] - 0s 847us/step - loss: 24193.8916 - mse: 624112064.0000 - mae: 24193.8926 - val_loss: 30078.8535 - val_mse: 906032960.0000 - val_mae: 30078.8535\n",
            "Epoch 11/500\n",
            "20/20 [==============================] - 0s 909us/step - loss: 24193.5498 - mse: 624086848.0000 - mae: 24193.5508 - val_loss: 30075.0820 - val_mse: 905808704.0000 - val_mae: 30075.0820\n",
            "Epoch 12/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24180.9365 - mse: 623554240.0000 - mae: 24180.9375 - val_loss: 30069.7441 - val_mse: 905491072.0000 - val_mae: 30069.7441\n",
            "Epoch 13/500\n",
            "20/20 [==============================] - 0s 957us/step - loss: 24175.9873 - mse: 623336064.0000 - mae: 24175.9883 - val_loss: 30064.4805 - val_mse: 905177728.0000 - val_mae: 30064.4805\n",
            "Epoch 14/500\n",
            "20/20 [==============================] - 0s 918us/step - loss: 24173.3975 - mse: 623114880.0000 - mae: 24173.3965 - val_loss: 30059.5879 - val_mse: 904886400.0000 - val_mae: 30059.5879\n",
            "Epoch 15/500\n",
            "20/20 [==============================] - 0s 941us/step - loss: 24158.3750 - mse: 622563200.0000 - mae: 24158.3750 - val_loss: 30053.6523 - val_mse: 904532608.0000 - val_mae: 30053.6523\n",
            "Epoch 16/500\n",
            "20/20 [==============================] - 0s 874us/step - loss: 24152.6641 - mse: 622326912.0000 - mae: 24152.6641 - val_loss: 30047.7852 - val_mse: 904182656.0000 - val_mae: 30047.7852\n",
            "Epoch 17/500\n",
            "20/20 [==============================] - 0s 919us/step - loss: 24148.8574 - mse: 622197632.0000 - mae: 24148.8574 - val_loss: 30041.9629 - val_mse: 903835648.0000 - val_mae: 30041.9629\n",
            "Epoch 18/500\n",
            "20/20 [==============================] - 0s 843us/step - loss: 24138.5625 - mse: 621608960.0000 - mae: 24138.5625 - val_loss: 30035.6309 - val_mse: 903458176.0000 - val_mae: 30035.6309\n",
            "Epoch 19/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24137.5020 - mse: 621541760.0000 - mae: 24137.5020 - val_loss: 30029.6055 - val_mse: 903099072.0000 - val_mae: 30029.6055\n",
            "Epoch 20/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24124.7988 - mse: 620881216.0000 - mae: 24124.8008 - val_loss: 30022.7930 - val_mse: 902693184.0000 - val_mae: 30022.7930\n",
            "Epoch 21/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24127.3428 - mse: 621038272.0000 - mae: 24127.3418 - val_loss: 30016.5371 - val_mse: 902319936.0000 - val_mae: 30016.5371\n",
            "Epoch 22/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24104.1055 - mse: 620179456.0000 - mae: 24104.1055 - val_loss: 30009.1660 - val_mse: 901880960.0000 - val_mae: 30009.1660\n",
            "Epoch 23/500\n",
            "20/20 [==============================] - 0s 951us/step - loss: 24091.8564 - mse: 619671872.0000 - mae: 24091.8555 - val_loss: 30001.4688 - val_mse: 901422272.0000 - val_mae: 30001.4688\n",
            "Epoch 24/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24081.3906 - mse: 619058496.0000 - mae: 24081.3906 - val_loss: 29993.1777 - val_mse: 900928512.0000 - val_mae: 29993.1777\n",
            "Epoch 25/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24066.6914 - mse: 618339584.0000 - mae: 24066.6914 - val_loss: 29984.4531 - val_mse: 900408960.0000 - val_mae: 29984.4531\n",
            "Epoch 26/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24076.1797 - mse: 618890048.0000 - mae: 24076.1797 - val_loss: 29976.7461 - val_mse: 899950208.0000 - val_mae: 29976.7461\n",
            "Epoch 27/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24070.4551 - mse: 618672960.0000 - mae: 24070.4570 - val_loss: 29968.8164 - val_mse: 899477824.0000 - val_mae: 29968.8164\n",
            "Epoch 28/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24039.2842 - mse: 617172480.0000 - mae: 24039.2852 - val_loss: 29959.4414 - val_mse: 898919552.0000 - val_mae: 29959.4414\n",
            "Epoch 29/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24022.5049 - mse: 616364160.0000 - mae: 24022.5039 - val_loss: 29949.8477 - val_mse: 898348864.0000 - val_mae: 29949.8477\n",
            "Epoch 30/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24021.1904 - mse: 616395072.0000 - mae: 24021.1914 - val_loss: 29940.3789 - val_mse: 897785536.0000 - val_mae: 29940.3789\n",
            "Epoch 31/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23998.0850 - mse: 615250816.0000 - mae: 23998.0840 - val_loss: 29929.9570 - val_mse: 897165632.0000 - val_mae: 29929.9570\n",
            "Epoch 32/500\n",
            "20/20 [==============================] - 0s 965us/step - loss: 23997.4092 - mse: 615147392.0000 - mae: 23997.4102 - val_loss: 29919.7305 - val_mse: 896557440.0000 - val_mae: 29919.7305\n",
            "Epoch 33/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23952.8438 - mse: 613120896.0000 - mae: 23952.8438 - val_loss: 29907.9102 - val_mse: 895854400.0000 - val_mae: 29907.9102\n",
            "Epoch 34/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23915.9902 - mse: 611116160.0000 - mae: 23915.9902 - val_loss: 29894.9492 - val_mse: 895084544.0000 - val_mae: 29894.9492\n",
            "Epoch 35/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23934.3525 - mse: 612356096.0000 - mae: 23934.3535 - val_loss: 29883.2930 - val_mse: 894392128.0000 - val_mae: 29883.2930\n",
            "Epoch 36/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23948.8330 - mse: 612800320.0000 - mae: 23948.8320 - val_loss: 29872.3691 - val_mse: 893743296.0000 - val_mae: 29872.3691\n",
            "Epoch 37/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 23863.9766 - mse: 609244928.0000 - mae: 23863.9766 - val_loss: 29858.3477 - val_mse: 892910976.0000 - val_mae: 29858.3477\n",
            "Epoch 38/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23891.9043 - mse: 610354304.0000 - mae: 23891.9023 - val_loss: 29845.5781 - val_mse: 892152960.0000 - val_mae: 29845.5781\n",
            "Epoch 39/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23846.1934 - mse: 608033536.0000 - mae: 23846.1934 - val_loss: 29831.7090 - val_mse: 891330432.0000 - val_mae: 29831.7090\n",
            "Epoch 40/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23892.8477 - mse: 610175616.0000 - mae: 23892.8477 - val_loss: 29819.4180 - val_mse: 890601088.0000 - val_mae: 29819.4180\n",
            "Epoch 41/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23852.3350 - mse: 608296640.0000 - mae: 23852.3340 - val_loss: 29806.0625 - val_mse: 889809600.0000 - val_mae: 29806.0625\n",
            "Epoch 42/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23860.1455 - mse: 609142912.0000 - mae: 23860.1445 - val_loss: 29793.2559 - val_mse: 889051008.0000 - val_mae: 29793.2559\n",
            "Epoch 43/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23799.3047 - mse: 606463040.0000 - mae: 23799.3047 - val_loss: 29778.6133 - val_mse: 888183616.0000 - val_mae: 29778.6133\n",
            "Epoch 44/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23756.1768 - mse: 604209024.0000 - mae: 23756.1777 - val_loss: 29762.6602 - val_mse: 887239488.0000 - val_mae: 29762.6602\n",
            "Epoch 45/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23763.1875 - mse: 603890560.0000 - mae: 23763.1875 - val_loss: 29747.5059 - val_mse: 886343168.0000 - val_mae: 29747.5059\n",
            "Epoch 46/500\n",
            "20/20 [==============================] - 0s 897us/step - loss: 23726.0420 - mse: 602961664.0000 - mae: 23726.0430 - val_loss: 29731.5664 - val_mse: 885401216.0000 - val_mae: 29731.5664\n",
            "Epoch 47/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 23766.1807 - mse: 604579584.0000 - mae: 23766.1816 - val_loss: 29717.2090 - val_mse: 884553024.0000 - val_mae: 29717.2090\n",
            "Epoch 48/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23694.4141 - mse: 600680704.0000 - mae: 23694.4141 - val_loss: 29700.5312 - val_mse: 883567936.0000 - val_mae: 29700.5312\n",
            "Epoch 49/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23692.0049 - mse: 601211264.0000 - mae: 23692.0039 - val_loss: 29684.2402 - val_mse: 882606400.0000 - val_mae: 29684.2402\n",
            "Epoch 50/500\n",
            "20/20 [==============================] - 0s 971us/step - loss: 23631.4600 - mse: 597739328.0000 - mae: 23631.4590 - val_loss: 29666.6621 - val_mse: 881569792.0000 - val_mae: 29666.6621\n",
            "Epoch 51/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23612.8916 - mse: 597965632.0000 - mae: 23612.8926 - val_loss: 29648.6250 - val_mse: 880506368.0000 - val_mae: 29648.6250\n",
            "Epoch 52/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23635.4014 - mse: 598872384.0000 - mae: 23635.4004 - val_loss: 29631.6992 - val_mse: 879509696.0000 - val_mae: 29631.6992\n",
            "Epoch 53/500\n",
            "20/20 [==============================] - 0s 988us/step - loss: 23599.5557 - mse: 596980864.0000 - mae: 23599.5566 - val_loss: 29614.0117 - val_mse: 878468416.0000 - val_mae: 29614.0117\n",
            "Epoch 54/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23548.0537 - mse: 593792832.0000 - mae: 23548.0527 - val_loss: 29595.1133 - val_mse: 877356736.0000 - val_mae: 29595.1133\n",
            "Epoch 55/500\n",
            "20/20 [==============================] - 0s 937us/step - loss: 23586.1572 - mse: 595892544.0000 - mae: 23586.1562 - val_loss: 29577.0410 - val_mse: 876294144.0000 - val_mae: 29577.0410\n",
            "Epoch 56/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23519.5684 - mse: 593372864.0000 - mae: 23519.5684 - val_loss: 29557.8809 - val_mse: 875168768.0000 - val_mae: 29557.8809\n",
            "Epoch 57/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23486.0801 - mse: 591395840.0000 - mae: 23486.0820 - val_loss: 29538.1250 - val_mse: 874009472.0000 - val_mae: 29538.1250\n",
            "Epoch 58/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23464.8682 - mse: 589365696.0000 - mae: 23464.8691 - val_loss: 29517.9414 - val_mse: 872826176.0000 - val_mae: 29517.9414\n",
            "Epoch 59/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23436.9150 - mse: 589405632.0000 - mae: 23436.9160 - val_loss: 29496.8945 - val_mse: 871592960.0000 - val_mae: 29496.8945\n",
            "Epoch 60/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23402.3496 - mse: 587620992.0000 - mae: 23402.3496 - val_loss: 29475.9434 - val_mse: 870366848.0000 - val_mae: 29475.9434\n",
            "Epoch 61/500\n",
            "20/20 [==============================] - 0s 958us/step - loss: 23473.3262 - mse: 590897024.0000 - mae: 23473.3262 - val_loss: 29456.1934 - val_mse: 869211136.0000 - val_mae: 29456.1934\n",
            "Epoch 62/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23384.1250 - mse: 586587264.0000 - mae: 23384.1250 - val_loss: 29434.3789 - val_mse: 867935552.0000 - val_mae: 29434.3789\n",
            "Epoch 63/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23253.9883 - mse: 580764672.0000 - mae: 23253.9883 - val_loss: 29409.8008 - val_mse: 866499968.0000 - val_mae: 29409.8008\n",
            "Epoch 64/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23329.1895 - mse: 584644288.0000 - mae: 23329.1895 - val_loss: 29386.9941 - val_mse: 865169024.0000 - val_mae: 29386.9941\n",
            "Epoch 65/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23347.7139 - mse: 583809920.0000 - mae: 23347.7148 - val_loss: 29364.9941 - val_mse: 863886144.0000 - val_mae: 29364.9941\n",
            "Epoch 66/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23179.1904 - mse: 576994432.0000 - mae: 23179.1914 - val_loss: 29339.4961 - val_mse: 862401024.0000 - val_mae: 29339.4961\n",
            "Epoch 67/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23218.0322 - mse: 578963456.0000 - mae: 23218.0312 - val_loss: 29315.2500 - val_mse: 860989312.0000 - val_mae: 29315.2500\n",
            "Epoch 68/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23111.1807 - mse: 573891712.0000 - mae: 23111.1816 - val_loss: 29288.9316 - val_mse: 859458944.0000 - val_mae: 29288.9316\n",
            "Epoch 69/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 23071.3945 - mse: 571782528.0000 - mae: 23071.3945 - val_loss: 29262.4121 - val_mse: 857918720.0000 - val_mae: 29262.4121\n",
            "Epoch 70/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23185.3350 - mse: 577051968.0000 - mae: 23185.3340 - val_loss: 29238.3066 - val_mse: 856519808.0000 - val_mae: 29238.3066\n",
            "Epoch 71/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23115.6846 - mse: 574401920.0000 - mae: 23115.6836 - val_loss: 29212.8184 - val_mse: 855041664.0000 - val_mae: 29212.8184\n",
            "Epoch 72/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23009.9580 - mse: 570111040.0000 - mae: 23009.9590 - val_loss: 29185.8320 - val_mse: 853479040.0000 - val_mae: 29185.8320\n",
            "Epoch 73/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23031.4688 - mse: 570605376.0000 - mae: 23031.4688 - val_loss: 29159.1621 - val_mse: 851936064.0000 - val_mae: 29159.1621\n",
            "Epoch 74/500\n",
            "20/20 [==============================] - 0s 871us/step - loss: 22979.1279 - mse: 568990656.0000 - mae: 22979.1289 - val_loss: 29132.0098 - val_mse: 850367104.0000 - val_mae: 29132.0098\n",
            "Epoch 75/500\n",
            "20/20 [==============================] - 0s 925us/step - loss: 22752.6631 - mse: 558035456.0000 - mae: 22752.6621 - val_loss: 29100.8164 - val_mse: 848566080.0000 - val_mae: 29100.8164\n",
            "Epoch 76/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 22992.7441 - mse: 567097664.0000 - mae: 22992.7441 - val_loss: 29074.5254 - val_mse: 847049536.0000 - val_mae: 29074.5254\n",
            "Epoch 77/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22847.7568 - mse: 561701504.0000 - mae: 22847.7559 - val_loss: 29045.6621 - val_mse: 845386560.0000 - val_mae: 29045.6621\n",
            "Epoch 78/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22822.8887 - mse: 561246016.0000 - mae: 22822.8867 - val_loss: 29016.6680 - val_mse: 843718336.0000 - val_mae: 29016.6680\n",
            "Epoch 79/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22842.9766 - mse: 563240576.0000 - mae: 22842.9766 - val_loss: 28988.5430 - val_mse: 842101824.0000 - val_mae: 28988.5430\n",
            "Epoch 80/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22842.0811 - mse: 562079616.0000 - mae: 22842.0820 - val_loss: 28960.8789 - val_mse: 840513536.0000 - val_mae: 28960.8789\n",
            "Epoch 81/500\n",
            "20/20 [==============================] - 0s 979us/step - loss: 22662.2725 - mse: 553422016.0000 - mae: 22662.2715 - val_loss: 28929.4961 - val_mse: 838713600.0000 - val_mae: 28929.4961\n",
            "Epoch 82/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22756.5996 - mse: 558591808.0000 - mae: 22756.5996 - val_loss: 28899.9316 - val_mse: 837019456.0000 - val_mae: 28899.9316\n",
            "Epoch 83/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22676.1895 - mse: 553652992.0000 - mae: 22676.1914 - val_loss: 28869.6309 - val_mse: 835285504.0000 - val_mae: 28869.6309\n",
            "Epoch 84/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22647.0654 - mse: 551105216.0000 - mae: 22647.0664 - val_loss: 28838.8184 - val_mse: 833524096.0000 - val_mae: 28838.8184\n",
            "Epoch 85/500\n",
            "20/20 [==============================] - 0s 940us/step - loss: 22476.4531 - mse: 546391680.0000 - mae: 22476.4531 - val_loss: 28805.2598 - val_mse: 831607744.0000 - val_mae: 28805.2598\n",
            "Epoch 86/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22565.7744 - mse: 552078656.0000 - mae: 22565.7754 - val_loss: 28774.2910 - val_mse: 829842432.0000 - val_mae: 28774.2910\n",
            "Epoch 87/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22430.9971 - mse: 544541824.0000 - mae: 22430.9961 - val_loss: 28740.8242 - val_mse: 827936448.0000 - val_mae: 28740.8242\n",
            "Epoch 88/500\n",
            "20/20 [==============================] - 0s 974us/step - loss: 22442.0049 - mse: 543072512.0000 - mae: 22442.0039 - val_loss: 28708.0469 - val_mse: 826072768.0000 - val_mae: 28708.0469\n",
            "Epoch 89/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22623.4688 - mse: 554347840.0000 - mae: 22623.4688 - val_loss: 28678.6602 - val_mse: 824403520.0000 - val_mae: 28678.6602\n",
            "Epoch 90/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22422.2695 - mse: 542594176.0000 - mae: 22422.2695 - val_loss: 28646.2461 - val_mse: 822565184.0000 - val_mae: 28646.2461\n",
            "Epoch 91/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22432.4258 - mse: 546062912.0000 - mae: 22432.4258 - val_loss: 28613.4688 - val_mse: 820706944.0000 - val_mae: 28613.4688\n",
            "Epoch 92/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22236.7285 - mse: 535657888.0000 - mae: 22236.7285 - val_loss: 28577.9785 - val_mse: 818697792.0000 - val_mae: 28577.9785\n",
            "Epoch 93/500\n",
            "20/20 [==============================] - 0s 861us/step - loss: 22323.7852 - mse: 537838528.0000 - mae: 22323.7852 - val_loss: 28544.1562 - val_mse: 816786368.0000 - val_mae: 28544.1562\n",
            "Epoch 94/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22114.2754 - mse: 528835392.0000 - mae: 22114.2754 - val_loss: 28507.4004 - val_mse: 814711808.0000 - val_mae: 28507.4004\n",
            "Epoch 95/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22145.5713 - mse: 530854720.0000 - mae: 22145.5723 - val_loss: 28471.6934 - val_mse: 812699264.0000 - val_mae: 28471.6934\n",
            "Epoch 96/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22214.3564 - mse: 533636512.0000 - mae: 22214.3555 - val_loss: 28437.5625 - val_mse: 810778880.0000 - val_mae: 28437.5625\n",
            "Epoch 97/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22128.5420 - mse: 529849152.0000 - mae: 22128.5430 - val_loss: 28402.3008 - val_mse: 808797184.0000 - val_mae: 28402.3008\n",
            "Epoch 98/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21990.1240 - mse: 526351008.0000 - mae: 21990.1250 - val_loss: 28364.9180 - val_mse: 806699392.0000 - val_mae: 28364.9180\n",
            "Epoch 99/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21941.7646 - mse: 518977376.0000 - mae: 21941.7637 - val_loss: 28326.5938 - val_mse: 804550528.0000 - val_mae: 28326.5938\n",
            "Epoch 100/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21903.0625 - mse: 522565952.0000 - mae: 21903.0625 - val_loss: 28288.5527 - val_mse: 802421632.0000 - val_mae: 28288.5527\n",
            "Epoch 101/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21792.0596 - mse: 514986496.0000 - mae: 21792.0586 - val_loss: 28248.8184 - val_mse: 800201792.0000 - val_mae: 28248.8184\n",
            "Epoch 102/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21809.8867 - mse: 517845088.0000 - mae: 21809.8867 - val_loss: 28209.9883 - val_mse: 798035456.0000 - val_mae: 28209.9883\n",
            "Epoch 103/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 21845.1738 - mse: 516410880.0000 - mae: 21845.1738 - val_loss: 28171.5781 - val_mse: 795894976.0000 - val_mae: 28171.5781\n",
            "Epoch 104/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21912.9170 - mse: 522240608.0000 - mae: 21912.9180 - val_loss: 28135.0371 - val_mse: 793862144.0000 - val_mae: 28135.0371\n",
            "Epoch 105/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21693.5146 - mse: 513050208.0000 - mae: 21693.5156 - val_loss: 28094.9883 - val_mse: 791637120.0000 - val_mae: 28094.9883\n",
            "Epoch 106/500\n",
            "20/20 [==============================] - 0s 976us/step - loss: 21549.3555 - mse: 507522464.0000 - mae: 21549.3555 - val_loss: 28053.0254 - val_mse: 789309504.0000 - val_mae: 28053.0254\n",
            "Epoch 107/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21795.3271 - mse: 517768896.0000 - mae: 21795.3262 - val_loss: 28015.7441 - val_mse: 787245568.0000 - val_mae: 28015.7441\n",
            "Epoch 108/500\n",
            "20/20 [==============================] - 0s 920us/step - loss: 21641.9229 - mse: 509120704.0000 - mae: 21641.9238 - val_loss: 27976.1250 - val_mse: 785055232.0000 - val_mae: 27976.1250\n",
            "Epoch 109/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21696.0215 - mse: 508974496.0000 - mae: 21696.0215 - val_loss: 27937.8496 - val_mse: 782942464.0000 - val_mae: 27937.8496\n",
            "Epoch 110/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21767.5107 - mse: 515670624.0000 - mae: 21767.5117 - val_loss: 27900.4434 - val_mse: 780879552.0000 - val_mae: 27900.4434\n",
            "Epoch 111/500\n",
            "20/20 [==============================] - 0s 886us/step - loss: 21287.2451 - mse: 496086432.0000 - mae: 21287.2461 - val_loss: 27856.4219 - val_mse: 778456896.0000 - val_mae: 27856.4219\n",
            "Epoch 112/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21601.1689 - mse: 510310976.0000 - mae: 21601.1680 - val_loss: 27817.9336 - val_mse: 776342656.0000 - val_mae: 27817.9336\n",
            "Epoch 113/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 21650.0684 - mse: 512233792.0000 - mae: 21650.0684 - val_loss: 27780.3184 - val_mse: 774280320.0000 - val_mae: 27780.3184\n",
            "Epoch 114/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21360.4805 - mse: 494045888.0000 - mae: 21360.4805 - val_loss: 27738.2500 - val_mse: 771976512.0000 - val_mae: 27738.2500\n",
            "Epoch 115/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21251.4932 - mse: 493467136.0000 - mae: 21251.4941 - val_loss: 27694.6914 - val_mse: 769593856.0000 - val_mae: 27694.6914\n",
            "Epoch 116/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21132.2383 - mse: 486366016.0000 - mae: 21132.2383 - val_loss: 27649.7910 - val_mse: 767141760.0000 - val_mae: 27649.7910\n",
            "Epoch 117/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21062.6973 - mse: 483516160.0000 - mae: 21062.6973 - val_loss: 27604.2461 - val_mse: 764659264.0000 - val_mae: 27604.2461\n",
            "Epoch 118/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20921.2842 - mse: 479223040.0000 - mae: 20921.2852 - val_loss: 27558.3242 - val_mse: 762163520.0000 - val_mae: 27558.3242\n",
            "Epoch 119/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21193.1738 - mse: 491883520.0000 - mae: 21193.1738 - val_loss: 27515.7305 - val_mse: 759851072.0000 - val_mae: 27515.7305\n",
            "Epoch 120/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20933.5518 - mse: 481686016.0000 - mae: 20933.5508 - val_loss: 27469.8281 - val_mse: 757363392.0000 - val_mae: 27469.8281\n",
            "Epoch 121/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20910.7139 - mse: 479823872.0000 - mae: 20910.7148 - val_loss: 27423.9844 - val_mse: 754883584.0000 - val_mae: 27423.9844\n",
            "Epoch 122/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20641.8994 - mse: 468563968.0000 - mae: 20641.9004 - val_loss: 27375.2441 - val_mse: 752252096.0000 - val_mae: 27375.2441\n",
            "Epoch 123/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21194.4434 - mse: 489587520.0000 - mae: 21194.4434 - val_loss: 27334.0352 - val_mse: 750031104.0000 - val_mae: 27334.0352\n",
            "Epoch 124/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20668.2744 - mse: 467673920.0000 - mae: 20668.2754 - val_loss: 27286.3320 - val_mse: 747465344.0000 - val_mae: 27286.3320\n",
            "Epoch 125/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20551.6260 - mse: 462514688.0000 - mae: 20551.6270 - val_loss: 27236.8945 - val_mse: 744811008.0000 - val_mae: 27236.8945\n",
            "Epoch 126/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20695.8516 - mse: 472341664.0000 - mae: 20695.8496 - val_loss: 27191.0781 - val_mse: 742359360.0000 - val_mae: 27191.0781\n",
            "Epoch 127/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20683.2715 - mse: 467851680.0000 - mae: 20683.2715 - val_loss: 27144.2031 - val_mse: 739851584.0000 - val_mae: 27144.2031\n",
            "Epoch 128/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20405.9727 - mse: 459856832.0000 - mae: 20405.9727 - val_loss: 27094.2070 - val_mse: 737182528.0000 - val_mae: 27094.2070\n",
            "Epoch 129/500\n",
            "20/20 [==============================] - 0s 960us/step - loss: 20294.7021 - mse: 454937920.0000 - mae: 20294.7012 - val_loss: 27043.7383 - val_mse: 734494144.0000 - val_mae: 27043.7383\n",
            "Epoch 130/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 20592.3740 - mse: 464656064.0000 - mae: 20592.3750 - val_loss: 26995.8594 - val_mse: 731946624.0000 - val_mae: 26995.8594\n",
            "Epoch 131/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 20467.4082 - mse: 459431104.0000 - mae: 20467.4082 - val_loss: 26947.9961 - val_mse: 729407296.0000 - val_mae: 26947.9961\n",
            "Epoch 132/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19905.7988 - mse: 434663936.0000 - mae: 19905.7988 - val_loss: 26892.6836 - val_mse: 726475904.0000 - val_mae: 26892.6836\n",
            "Epoch 133/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20054.7607 - mse: 445263936.0000 - mae: 20054.7598 - val_loss: 26840.5312 - val_mse: 723720576.0000 - val_mae: 26840.5312\n",
            "Epoch 134/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20343.9414 - mse: 459249664.0000 - mae: 20343.9414 - val_loss: 26791.7559 - val_mse: 721149568.0000 - val_mae: 26791.7559\n",
            "Epoch 135/500\n",
            "20/20 [==============================] - 0s 938us/step - loss: 20017.3486 - mse: 444436640.0000 - mae: 20017.3477 - val_loss: 26739.6680 - val_mse: 718409984.0000 - val_mae: 26739.6680\n",
            "Epoch 136/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20033.9307 - mse: 441864032.0000 - mae: 20033.9316 - val_loss: 26687.9902 - val_mse: 715696320.0000 - val_mae: 26687.9902\n",
            "Epoch 137/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19813.8066 - mse: 431608928.0000 - mae: 19813.8066 - val_loss: 26634.3320 - val_mse: 712884032.0000 - val_mae: 26634.3320\n",
            "Epoch 138/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20163.2725 - mse: 441978688.0000 - mae: 20163.2715 - val_loss: 26584.3242 - val_mse: 710266560.0000 - val_mae: 26584.3242\n",
            "Epoch 139/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20462.8779 - mse: 455990624.0000 - mae: 20462.8789 - val_loss: 26537.9473 - val_mse: 707844224.0000 - val_mae: 26537.9473\n",
            "Epoch 140/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19730.2148 - mse: 427934400.0000 - mae: 19730.2148 - val_loss: 26484.4004 - val_mse: 705055936.0000 - val_mae: 26484.4004\n",
            "Epoch 141/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19943.2109 - mse: 437255360.0000 - mae: 19943.2090 - val_loss: 26432.8477 - val_mse: 702374656.0000 - val_mae: 26432.8477\n",
            "Epoch 142/500\n",
            "20/20 [==============================] - 0s 973us/step - loss: 19752.7051 - mse: 437126080.0000 - mae: 19752.7051 - val_loss: 26379.2812 - val_mse: 699593920.0000 - val_mae: 26379.2812\n",
            "Epoch 143/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19712.6914 - mse: 430520064.0000 - mae: 19712.6914 - val_loss: 26325.9746 - val_mse: 696834560.0000 - val_mae: 26325.9746\n",
            "Epoch 144/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20032.8955 - mse: 440653376.0000 - mae: 20032.8945 - val_loss: 26276.2441 - val_mse: 694264704.0000 - val_mae: 26276.2441\n",
            "Epoch 145/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19244.6748 - mse: 409161728.0000 - mae: 19244.6758 - val_loss: 26218.8691 - val_mse: 691310272.0000 - val_mae: 26218.8691\n",
            "Epoch 146/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19731.2578 - mse: 440107360.0000 - mae: 19731.2578 - val_loss: 26167.0645 - val_mse: 688646464.0000 - val_mae: 26167.0645\n",
            "Epoch 147/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19049.4004 - mse: 405698976.0000 - mae: 19049.4004 - val_loss: 26107.9961 - val_mse: 685618368.0000 - val_mae: 26107.9961\n",
            "Epoch 148/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19499.1660 - mse: 426346560.0000 - mae: 19499.1660 - val_loss: 26054.1934 - val_mse: 682865088.0000 - val_mae: 26054.1934\n",
            "Epoch 149/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19943.4746 - mse: 438831008.0000 - mae: 19943.4746 - val_loss: 26006.3789 - val_mse: 680426176.0000 - val_mae: 26006.3789\n",
            "Epoch 150/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18711.5283 - mse: 393257504.0000 - mae: 18711.5273 - val_loss: 25944.5586 - val_mse: 677278400.0000 - val_mae: 25944.5586\n",
            "Epoch 151/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19294.2393 - mse: 419989696.0000 - mae: 19294.2402 - val_loss: 25889.9121 - val_mse: 674502848.0000 - val_mae: 25889.9121\n",
            "Epoch 152/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18680.3506 - mse: 387614112.0000 - mae: 18680.3496 - val_loss: 25828.8223 - val_mse: 671407616.0000 - val_mae: 25828.8223\n",
            "Epoch 153/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19229.5566 - mse: 411104928.0000 - mae: 19229.5566 - val_loss: 25772.2227 - val_mse: 668544064.0000 - val_mae: 25772.2227\n",
            "Epoch 154/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 18873.0117 - mse: 399017120.0000 - mae: 18873.0117 - val_loss: 25712.7891 - val_mse: 665547584.0000 - val_mae: 25712.7891\n",
            "Epoch 155/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18931.6514 - mse: 403615168.0000 - mae: 18931.6523 - val_loss: 25653.2812 - val_mse: 662551872.0000 - val_mae: 25653.2812\n",
            "Epoch 156/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18469.9141 - mse: 390090144.0000 - mae: 18469.9141 - val_loss: 25589.9316 - val_mse: 659374464.0000 - val_mae: 25589.9316\n",
            "Epoch 157/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18891.6406 - mse: 396723040.0000 - mae: 18891.6406 - val_loss: 25530.9219 - val_mse: 656424064.0000 - val_mae: 25530.9219\n",
            "Epoch 158/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18824.8496 - mse: 399906496.0000 - mae: 18824.8496 - val_loss: 25471.7871 - val_mse: 653475968.0000 - val_mae: 25471.7871\n",
            "Epoch 159/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18907.5020 - mse: 406087680.0000 - mae: 18907.5039 - val_loss: 25414.6270 - val_mse: 650633408.0000 - val_mae: 25414.6270\n",
            "Epoch 160/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19073.0977 - mse: 402822496.0000 - mae: 19073.0977 - val_loss: 25357.7988 - val_mse: 647811072.0000 - val_mae: 25357.7988\n",
            "Epoch 161/500\n",
            "20/20 [==============================] - 0s 991us/step - loss: 18402.6719 - mse: 387013568.0000 - mae: 18402.6719 - val_loss: 25294.4414 - val_mse: 644673728.0000 - val_mae: 25294.4414\n",
            "Epoch 162/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18517.8486 - mse: 382187904.0000 - mae: 18517.8477 - val_loss: 25232.4766 - val_mse: 641610624.0000 - val_mae: 25232.4766\n",
            "Epoch 163/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18567.0566 - mse: 392162752.0000 - mae: 18567.0566 - val_loss: 25171.9570 - val_mse: 638631488.0000 - val_mae: 25171.9570\n",
            "Epoch 164/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 18199.2061 - mse: 377263776.0000 - mae: 18199.2070 - val_loss: 25108.4746 - val_mse: 635514624.0000 - val_mae: 25108.4746\n",
            "Epoch 165/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18698.5723 - mse: 391799744.0000 - mae: 18698.5723 - val_loss: 25049.3105 - val_mse: 632614016.0000 - val_mae: 25049.3105\n",
            "Epoch 166/500\n",
            "20/20 [==============================] - 0s 973us/step - loss: 18659.2393 - mse: 389003904.0000 - mae: 18659.2402 - val_loss: 24989.8027 - val_mse: 629702080.0000 - val_mae: 24989.8027\n",
            "Epoch 167/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18611.7490 - mse: 386603872.0000 - mae: 18611.7500 - val_loss: 24930.2246 - val_mse: 626796672.0000 - val_mae: 24930.2246\n",
            "Epoch 168/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18357.3125 - mse: 388185280.0000 - mae: 18357.3125 - val_loss: 24869.1289 - val_mse: 623826560.0000 - val_mae: 24869.1289\n",
            "Epoch 169/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18400.1523 - mse: 380869184.0000 - mae: 18400.1523 - val_loss: 24808.5781 - val_mse: 620892544.0000 - val_mae: 24808.5781\n",
            "Epoch 170/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 17356.0498 - mse: 347064064.0000 - mae: 17356.0508 - val_loss: 24737.9219 - val_mse: 617476416.0000 - val_mae: 24737.9219\n",
            "Epoch 171/500\n",
            "20/20 [==============================] - 0s 960us/step - loss: 17751.1611 - mse: 353159264.0000 - mae: 17751.1621 - val_loss: 24670.2852 - val_mse: 614215296.0000 - val_mae: 24670.2852\n",
            "Epoch 172/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17973.5840 - mse: 369825472.0000 - mae: 17973.5840 - val_loss: 24606.5430 - val_mse: 611151168.0000 - val_mae: 24606.5430\n",
            "Epoch 173/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17835.5381 - mse: 356547424.0000 - mae: 17835.5371 - val_loss: 24541.3906 - val_mse: 608027840.0000 - val_mae: 24541.3906\n",
            "Epoch 174/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17342.6562 - mse: 346833344.0000 - mae: 17342.6562 - val_loss: 24471.9258 - val_mse: 604709184.0000 - val_mae: 24471.9258\n",
            "Epoch 175/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17391.7188 - mse: 358117792.0000 - mae: 17391.7188 - val_loss: 24404.7539 - val_mse: 601515712.0000 - val_mae: 24404.7539\n",
            "Epoch 176/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 18074.1748 - mse: 365349184.0000 - mae: 18074.1758 - val_loss: 24342.7812 - val_mse: 598568896.0000 - val_mae: 24342.7812\n",
            "Epoch 177/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17626.7637 - mse: 359296928.0000 - mae: 17626.7637 - val_loss: 24277.8125 - val_mse: 595496640.0000 - val_mae: 24277.8125\n",
            "Epoch 178/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17143.3892 - mse: 334058432.0000 - mae: 17143.3887 - val_loss: 24208.6992 - val_mse: 592237184.0000 - val_mae: 24208.6992\n",
            "Epoch 179/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17559.7183 - mse: 346261888.0000 - mae: 17559.7188 - val_loss: 24143.3359 - val_mse: 589161600.0000 - val_mae: 24143.3359\n",
            "Epoch 180/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16921.5381 - mse: 325376672.0000 - mae: 16921.5371 - val_loss: 24072.1348 - val_mse: 585820608.0000 - val_mae: 24072.1348\n",
            "Epoch 181/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16976.2451 - mse: 327989664.0000 - mae: 16976.2461 - val_loss: 24001.5938 - val_mse: 582520832.0000 - val_mae: 24001.5938\n",
            "Epoch 182/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16945.8672 - mse: 331069760.0000 - mae: 16945.8672 - val_loss: 23931.4883 - val_mse: 579251264.0000 - val_mae: 23931.4883\n",
            "Epoch 183/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17121.2021 - mse: 344458080.0000 - mae: 17121.2031 - val_loss: 23862.7773 - val_mse: 576056448.0000 - val_mae: 23862.7773\n",
            "Epoch 184/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16144.3599 - mse: 305935584.0000 - mae: 16144.3594 - val_loss: 23786.0352 - val_mse: 572504000.0000 - val_mae: 23786.0352\n",
            "Epoch 185/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16895.6592 - mse: 323370784.0000 - mae: 16895.6602 - val_loss: 23716.2754 - val_mse: 569287552.0000 - val_mae: 23716.2754\n",
            "Epoch 186/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16378.8716 - mse: 319153760.0000 - mae: 16378.8721 - val_loss: 23642.1250 - val_mse: 565877568.0000 - val_mae: 23642.1250\n",
            "Epoch 187/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16706.8301 - mse: 324853248.0000 - mae: 16706.8301 - val_loss: 23572.1133 - val_mse: 562669440.0000 - val_mae: 23572.1133\n",
            "Epoch 188/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16118.1465 - mse: 317753792.0000 - mae: 16118.1465 - val_loss: 23497.0000 - val_mse: 559239168.0000 - val_mae: 23497.0000\n",
            "Epoch 189/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16563.9741 - mse: 322564736.0000 - mae: 16563.9746 - val_loss: 23424.6914 - val_mse: 555938944.0000 - val_mae: 23424.6914\n",
            "Epoch 190/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15599.9883 - mse: 288618752.0000 - mae: 15599.9873 - val_loss: 23345.3086 - val_mse: 552340800.0000 - val_mae: 23345.3086\n",
            "Epoch 191/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15469.0161 - mse: 282150208.0000 - mae: 15469.0156 - val_loss: 23265.5664 - val_mse: 548742016.0000 - val_mae: 23265.5664\n",
            "Epoch 192/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15842.1128 - mse: 297539936.0000 - mae: 15842.1123 - val_loss: 23188.8438 - val_mse: 545288704.0000 - val_mae: 23188.8438\n",
            "Epoch 193/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15489.5635 - mse: 283708416.0000 - mae: 15489.5625 - val_loss: 23109.7441 - val_mse: 541739200.0000 - val_mae: 23109.7441\n",
            "Epoch 194/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15996.6250 - mse: 297538080.0000 - mae: 15996.6250 - val_loss: 23033.8945 - val_mse: 538342272.0000 - val_mae: 23033.8945\n",
            "Epoch 195/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15970.2939 - mse: 302841600.0000 - mae: 15970.2939 - val_loss: 22959.0781 - val_mse: 535007808.0000 - val_mae: 22959.0781\n",
            "Epoch 196/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16106.6992 - mse: 312071072.0000 - mae: 16106.7002 - val_loss: 22885.8008 - val_mse: 531752288.0000 - val_mae: 22885.8008\n",
            "Epoch 197/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15557.6948 - mse: 270565696.0000 - mae: 15557.6953 - val_loss: 22808.5254 - val_mse: 528334272.0000 - val_mae: 22808.5254\n",
            "Epoch 198/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15088.6758 - mse: 274675904.0000 - mae: 15088.6748 - val_loss: 22726.3652 - val_mse: 524708448.0000 - val_mae: 22726.3652\n",
            "Epoch 199/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16237.1992 - mse: 310858400.0000 - mae: 16237.2002 - val_loss: 22664.3848 - val_mse: 521982720.0000 - val_mae: 22664.3848\n",
            "Epoch 200/500\n",
            "20/20 [==============================] - 0s 960us/step - loss: 15350.3960 - mse: 274053152.0000 - mae: 15350.3965 - val_loss: 22586.1562 - val_mse: 518561696.0000 - val_mae: 22586.1562\n",
            "Epoch 201/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15337.7773 - mse: 277949888.0000 - mae: 15337.7783 - val_loss: 22517.9727 - val_mse: 515586368.0000 - val_mae: 22517.9727\n",
            "Epoch 202/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 14656.3208 - mse: 263928352.0000 - mae: 14656.3223 - val_loss: 22434.6055 - val_mse: 511966272.0000 - val_mae: 22434.6055\n",
            "Epoch 203/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 15559.1260 - mse: 289391808.0000 - mae: 15559.1250 - val_loss: 22369.4316 - val_mse: 509145696.0000 - val_mae: 22369.4316\n",
            "Epoch 204/500\n",
            "20/20 [==============================] - 0s 957us/step - loss: 14908.3120 - mse: 268711968.0000 - mae: 14908.3125 - val_loss: 22288.6992 - val_mse: 505662016.0000 - val_mae: 22288.6992\n",
            "Epoch 205/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13990.9741 - mse: 241472640.0000 - mae: 13990.9746 - val_loss: 22200.1777 - val_mse: 501859648.0000 - val_mae: 22200.1777\n",
            "Epoch 206/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 14057.0483 - mse: 234842448.0000 - mae: 14057.0488 - val_loss: 22112.8613 - val_mse: 498129664.0000 - val_mae: 22112.8613\n",
            "Epoch 207/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 15000.6377 - mse: 271281248.0000 - mae: 15000.6377 - val_loss: 22033.9707 - val_mse: 494768800.0000 - val_mae: 22033.9707\n",
            "Epoch 208/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 14864.4463 - mse: 261109552.0000 - mae: 14864.4473 - val_loss: 21953.5938 - val_mse: 491358144.0000 - val_mae: 21953.5938\n",
            "Epoch 209/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15147.3843 - mse: 281182624.0000 - mae: 15147.3848 - val_loss: 21895.9746 - val_mse: 488913056.0000 - val_mae: 21895.9746\n",
            "Epoch 210/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 14945.0566 - mse: 266442800.0000 - mae: 14945.0566 - val_loss: 21829.0059 - val_mse: 486097216.0000 - val_mae: 21829.0059\n",
            "Epoch 211/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 14422.4580 - mse: 252553344.0000 - mae: 14422.4580 - val_loss: 21745.2344 - val_mse: 482569120.0000 - val_mae: 21745.2344\n",
            "Epoch 212/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13902.4321 - mse: 239656704.0000 - mae: 13902.4316 - val_loss: 21668.4414 - val_mse: 479351712.0000 - val_mae: 21668.4414\n",
            "Epoch 213/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 14396.8247 - mse: 254346240.0000 - mae: 14396.8252 - val_loss: 21596.0820 - val_mse: 476333056.0000 - val_mae: 21596.0820\n",
            "Epoch 214/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 14068.2339 - mse: 249335504.0000 - mae: 14068.2344 - val_loss: 21521.0723 - val_mse: 473217376.0000 - val_mae: 21521.0723\n",
            "Epoch 215/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 14495.3481 - mse: 261188864.0000 - mae: 14495.3486 - val_loss: 21439.9727 - val_mse: 469865408.0000 - val_mae: 21439.9727\n",
            "Epoch 216/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 14681.0625 - mse: 264274992.0000 - mae: 14681.0625 - val_loss: 21370.2246 - val_mse: 466990144.0000 - val_mae: 21370.2246\n",
            "Epoch 217/500\n",
            "20/20 [==============================] - 0s 969us/step - loss: 14468.0225 - mse: 245038624.0000 - mae: 14468.0215 - val_loss: 21289.4902 - val_mse: 463673344.0000 - val_mae: 21289.4902\n",
            "Epoch 218/500\n",
            "20/20 [==============================] - 0s 942us/step - loss: 14344.1875 - mse: 258384592.0000 - mae: 14344.1875 - val_loss: 21227.5391 - val_mse: 461129728.0000 - val_mae: 21227.5391\n",
            "Epoch 219/500\n",
            "20/20 [==============================] - 0s 892us/step - loss: 12917.0898 - mse: 206943136.0000 - mae: 12917.0898 - val_loss: 21143.7754 - val_mse: 457711520.0000 - val_mae: 21143.7754\n",
            "Epoch 220/500\n",
            "20/20 [==============================] - 0s 997us/step - loss: 13442.1733 - mse: 222796880.0000 - mae: 13442.1738 - val_loss: 21055.6680 - val_mse: 454142784.0000 - val_mae: 21055.6680\n",
            "Epoch 221/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13073.9473 - mse: 215508576.0000 - mae: 13073.9473 - val_loss: 20982.2852 - val_mse: 451160512.0000 - val_mae: 20982.2852\n",
            "Epoch 222/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 13814.3364 - mse: 230967552.0000 - mae: 13814.3359 - val_loss: 20905.9688 - val_mse: 448081312.0000 - val_mae: 20905.9688\n",
            "Epoch 223/500\n",
            "20/20 [==============================] - 0s 970us/step - loss: 13458.1772 - mse: 228052736.0000 - mae: 13458.1777 - val_loss: 20818.5449 - val_mse: 444578560.0000 - val_mae: 20818.5449\n",
            "Epoch 224/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13351.8872 - mse: 218633008.0000 - mae: 13351.8877 - val_loss: 20730.4824 - val_mse: 441064864.0000 - val_mae: 20730.4824\n",
            "Epoch 225/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 14490.4043 - mse: 260624736.0000 - mae: 14490.4043 - val_loss: 20661.5586 - val_mse: 438320736.0000 - val_mae: 20661.5586\n",
            "Epoch 226/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 13763.9204 - mse: 240732160.0000 - mae: 13763.9199 - val_loss: 20595.6309 - val_mse: 435705344.0000 - val_mae: 20595.6309\n",
            "Epoch 227/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 14339.8374 - mse: 245622448.0000 - mae: 14339.8379 - val_loss: 20516.2402 - val_mse: 432575936.0000 - val_mae: 20516.2402\n",
            "Epoch 228/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12902.8901 - mse: 213943856.0000 - mae: 12902.8906 - val_loss: 20433.1641 - val_mse: 429305760.0000 - val_mae: 20433.1641\n",
            "Epoch 229/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13046.4443 - mse: 207993312.0000 - mae: 13046.4434 - val_loss: 20354.4062 - val_mse: 426218336.0000 - val_mae: 20354.4062\n",
            "Epoch 230/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13229.1768 - mse: 221177856.0000 - mae: 13229.1768 - val_loss: 20266.3086 - val_mse: 422783680.0000 - val_mae: 20266.3086\n",
            "Epoch 231/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12709.8672 - mse: 201217808.0000 - mae: 12709.8672 - val_loss: 20183.7891 - val_mse: 419581632.0000 - val_mae: 20183.7891\n",
            "Epoch 232/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12313.6733 - mse: 200589904.0000 - mae: 12313.6738 - val_loss: 20110.3809 - val_mse: 416742400.0000 - val_mae: 20110.3809\n",
            "Epoch 233/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11893.1787 - mse: 172010720.0000 - mae: 11893.1777 - val_loss: 20021.9648 - val_mse: 413338816.0000 - val_mae: 20021.9648\n",
            "Epoch 234/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13923.0654 - mse: 233007200.0000 - mae: 13923.0654 - val_loss: 19948.9316 - val_mse: 410537920.0000 - val_mae: 19948.9316\n",
            "Epoch 235/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12890.2632 - mse: 201238880.0000 - mae: 12890.2637 - val_loss: 19879.0977 - val_mse: 407880256.0000 - val_mae: 19879.0977\n",
            "Epoch 236/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12817.2256 - mse: 208939952.0000 - mae: 12817.2256 - val_loss: 19809.3184 - val_mse: 405228224.0000 - val_mae: 19809.3184\n",
            "Epoch 237/500\n",
            "20/20 [==============================] - 0s 952us/step - loss: 11930.7246 - mse: 177185440.0000 - mae: 11930.7246 - val_loss: 19722.9883 - val_mse: 401955904.0000 - val_mae: 19722.9883\n",
            "Epoch 238/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12238.8936 - mse: 201300144.0000 - mae: 12238.8936 - val_loss: 19640.4258 - val_mse: 398854720.0000 - val_mae: 19640.4258\n",
            "Epoch 239/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12511.2515 - mse: 205426336.0000 - mae: 12511.2520 - val_loss: 19568.9141 - val_mse: 396169344.0000 - val_mae: 19568.9141\n",
            "Epoch 240/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12502.4062 - mse: 206370080.0000 - mae: 12502.4062 - val_loss: 19495.9512 - val_mse: 393439904.0000 - val_mae: 19495.9512\n",
            "Epoch 241/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11227.9189 - mse: 153737424.0000 - mae: 11227.9189 - val_loss: 19406.5234 - val_mse: 390117376.0000 - val_mae: 19406.5234\n",
            "Epoch 242/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12675.7725 - mse: 206539392.0000 - mae: 12675.7715 - val_loss: 19317.5918 - val_mse: 386835808.0000 - val_mae: 19317.5918\n",
            "Epoch 243/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12404.1782 - mse: 187435552.0000 - mae: 12404.1777 - val_loss: 19235.2285 - val_mse: 383798016.0000 - val_mae: 19235.2285\n",
            "Epoch 244/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13453.1113 - mse: 224213856.0000 - mae: 13453.1113 - val_loss: 19169.7285 - val_mse: 381389696.0000 - val_mae: 19169.7285\n",
            "Epoch 245/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 13106.6177 - mse: 214902624.0000 - mae: 13106.6172 - val_loss: 19111.9746 - val_mse: 379260416.0000 - val_mae: 19111.9746\n",
            "Epoch 246/500\n",
            "20/20 [==============================] - 0s 990us/step - loss: 11692.9297 - mse: 173026768.0000 - mae: 11692.9297 - val_loss: 19033.9922 - val_mse: 376424512.0000 - val_mae: 19033.9922\n",
            "Epoch 247/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12149.6528 - mse: 180673440.0000 - mae: 12149.6533 - val_loss: 18959.4844 - val_mse: 373718208.0000 - val_mae: 18959.4844\n",
            "Epoch 248/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12569.9155 - mse: 198544928.0000 - mae: 12569.9160 - val_loss: 18899.2695 - val_mse: 371529792.0000 - val_mae: 18899.2695\n",
            "Epoch 249/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11729.2314 - mse: 189512320.0000 - mae: 11729.2314 - val_loss: 18823.1270 - val_mse: 368797888.0000 - val_mae: 18823.1270\n",
            "Epoch 250/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11994.1938 - mse: 186939696.0000 - mae: 11994.1934 - val_loss: 18741.0859 - val_mse: 365865024.0000 - val_mae: 18741.0859\n",
            "Epoch 251/500\n",
            "20/20 [==============================] - 0s 969us/step - loss: 11130.9268 - mse: 171201280.0000 - mae: 11130.9268 - val_loss: 18670.6582 - val_mse: 363350720.0000 - val_mae: 18670.6582\n",
            "Epoch 252/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11685.9976 - mse: 177609856.0000 - mae: 11685.9971 - val_loss: 18593.5000 - val_mse: 360612544.0000 - val_mae: 18593.5000\n",
            "Epoch 253/500\n",
            "20/20 [==============================] - 0s 885us/step - loss: 11945.7109 - mse: 173475616.0000 - mae: 11945.7109 - val_loss: 18540.5840 - val_mse: 358734528.0000 - val_mae: 18540.5840\n",
            "Epoch 254/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11301.8315 - mse: 161973328.0000 - mae: 11301.8311 - val_loss: 18470.2227 - val_mse: 356244576.0000 - val_mae: 18470.2227\n",
            "Epoch 255/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 12264.0474 - mse: 188338560.0000 - mae: 12264.0469 - val_loss: 18388.1055 - val_mse: 353365952.0000 - val_mae: 18388.1055\n",
            "Epoch 256/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12239.1963 - mse: 199030112.0000 - mae: 12239.1973 - val_loss: 18314.6914 - val_mse: 350794304.0000 - val_mae: 18314.6914\n",
            "Epoch 257/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12714.3047 - mse: 206016928.0000 - mae: 12714.3047 - val_loss: 18246.0625 - val_mse: 348395872.0000 - val_mae: 18246.0625\n",
            "Epoch 258/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 10746.4697 - mse: 147239152.0000 - mae: 10746.4707 - val_loss: 18162.3789 - val_mse: 345496352.0000 - val_mae: 18162.3789\n",
            "Epoch 259/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11144.5806 - mse: 169536304.0000 - mae: 11144.5811 - val_loss: 18081.9648 - val_mse: 342723904.0000 - val_mae: 18081.9648\n",
            "Epoch 260/500\n",
            "20/20 [==============================] - 0s 929us/step - loss: 12023.7471 - mse: 179590160.0000 - mae: 12023.7471 - val_loss: 17990.0820 - val_mse: 339578368.0000 - val_mae: 17990.0820\n",
            "Epoch 261/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11735.4473 - mse: 176389664.0000 - mae: 11735.4473 - val_loss: 17917.0137 - val_mse: 337088096.0000 - val_mae: 17917.0137\n",
            "Epoch 262/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9857.0176 - mse: 132941264.0000 - mae: 9857.0176 - val_loss: 17838.4102 - val_mse: 334412128.0000 - val_mae: 17838.4102\n",
            "Epoch 263/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10229.6860 - mse: 145027248.0000 - mae: 10229.6855 - val_loss: 17757.6992 - val_mse: 331675584.0000 - val_mae: 17757.6992\n",
            "Epoch 264/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11224.2124 - mse: 153305760.0000 - mae: 11224.2129 - val_loss: 17681.6855 - val_mse: 329116480.0000 - val_mae: 17681.6855\n",
            "Epoch 265/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9824.0762 - mse: 127087208.0000 - mae: 9824.0762 - val_loss: 17594.7148 - val_mse: 326207744.0000 - val_mae: 17594.7148\n",
            "Epoch 266/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10744.9165 - mse: 162140192.0000 - mae: 10744.9160 - val_loss: 17546.4551 - val_mse: 324576000.0000 - val_mae: 17546.4551\n",
            "Epoch 267/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10753.2153 - mse: 143038624.0000 - mae: 10753.2158 - val_loss: 17466.1055 - val_mse: 321917024.0000 - val_mae: 17466.1055\n",
            "Epoch 268/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9625.3125 - mse: 124742248.0000 - mae: 9625.3125 - val_loss: 17387.5820 - val_mse: 319330400.0000 - val_mae: 17387.5820\n",
            "Epoch 269/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11589.6060 - mse: 161030384.0000 - mae: 11589.6064 - val_loss: 17324.5254 - val_mse: 317258656.0000 - val_mae: 17324.5254\n",
            "Epoch 270/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10553.8188 - mse: 151382544.0000 - mae: 10553.8184 - val_loss: 17242.0820 - val_mse: 314563424.0000 - val_mae: 17242.0820\n",
            "Epoch 271/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10637.5996 - mse: 146034208.0000 - mae: 10637.5996 - val_loss: 17171.8477 - val_mse: 312272096.0000 - val_mae: 17171.8477\n",
            "Epoch 272/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10819.6294 - mse: 153243488.0000 - mae: 10819.6299 - val_loss: 17093.3398 - val_mse: 309729088.0000 - val_mae: 17093.3398\n",
            "Epoch 273/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10497.2764 - mse: 146308992.0000 - mae: 10497.2764 - val_loss: 16992.3320 - val_mse: 306476192.0000 - val_mae: 16992.3320\n",
            "Epoch 274/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10364.0181 - mse: 146647264.0000 - mae: 10364.0186 - val_loss: 16901.8164 - val_mse: 303584512.0000 - val_mae: 16901.8164\n",
            "Epoch 275/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8173.9370 - mse: 89155736.0000 - mae: 8173.9375 - val_loss: 16813.5215 - val_mse: 300764928.0000 - val_mae: 16813.5215\n",
            "Epoch 276/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10654.0186 - mse: 145219808.0000 - mae: 10654.0186 - val_loss: 16753.6230 - val_mse: 298854144.0000 - val_mae: 16753.6230\n",
            "Epoch 277/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9403.8936 - mse: 114891496.0000 - mae: 9403.8936 - val_loss: 16661.9062 - val_mse: 295935648.0000 - val_mae: 16661.9062\n",
            "Epoch 278/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 9964.1182 - mse: 130727536.0000 - mae: 9964.1191 - val_loss: 16599.1992 - val_mse: 293947616.0000 - val_mae: 16599.1992\n",
            "Epoch 279/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10364.0239 - mse: 132177520.0000 - mae: 10364.0234 - val_loss: 16525.0938 - val_mse: 291613440.0000 - val_mae: 16525.0938\n",
            "Epoch 280/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9969.2715 - mse: 123881048.0000 - mae: 9969.2715 - val_loss: 16461.3027 - val_mse: 289611840.0000 - val_mae: 16461.3027\n",
            "Epoch 281/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9445.8867 - mse: 111788456.0000 - mae: 9445.8867 - val_loss: 16404.9570 - val_mse: 287838176.0000 - val_mae: 16404.9570\n",
            "Epoch 282/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8939.0657 - mse: 113982616.0000 - mae: 8939.0654 - val_loss: 16338.7656 - val_mse: 285784352.0000 - val_mae: 16338.7656\n",
            "Epoch 283/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8807.3909 - mse: 112879208.0000 - mae: 8807.3906 - val_loss: 16273.2686 - val_mse: 283760864.0000 - val_mae: 16273.2686\n",
            "Epoch 284/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10857.6660 - mse: 143426560.0000 - mae: 10857.6660 - val_loss: 16203.9893 - val_mse: 281643264.0000 - val_mae: 16203.9893\n",
            "Epoch 285/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9846.0845 - mse: 120695832.0000 - mae: 9846.0840 - val_loss: 16126.0547 - val_mse: 279257792.0000 - val_mae: 16126.0547\n",
            "Epoch 286/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 9408.1113 - mse: 114462848.0000 - mae: 9408.1113 - val_loss: 16058.9785 - val_mse: 277216672.0000 - val_mae: 16058.9785\n",
            "Epoch 287/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10929.3462 - mse: 160621296.0000 - mae: 10929.3457 - val_loss: 15981.5889 - val_mse: 274904384.0000 - val_mae: 15981.5889\n",
            "Epoch 288/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8606.0635 - mse: 92729360.0000 - mae: 8606.0645 - val_loss: 15899.2988 - val_mse: 272424896.0000 - val_mae: 15899.2988\n",
            "Epoch 289/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9338.0220 - mse: 110510312.0000 - mae: 9338.0215 - val_loss: 15833.7363 - val_mse: 270440896.0000 - val_mae: 15833.7363\n",
            "Epoch 290/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8754.3555 - mse: 110748520.0000 - mae: 8754.3555 - val_loss: 15773.9043 - val_mse: 268636704.0000 - val_mae: 15773.9043\n",
            "Epoch 291/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 9678.7158 - mse: 116747376.0000 - mae: 9678.7158 - val_loss: 15698.7451 - val_mse: 266402016.0000 - val_mae: 15698.7451\n",
            "Epoch 292/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10513.7446 - mse: 139615136.0000 - mae: 10513.7441 - val_loss: 15642.6982 - val_mse: 264737664.0000 - val_mae: 15642.6982\n",
            "Epoch 293/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9179.8560 - mse: 113868880.0000 - mae: 9179.8564 - val_loss: 15592.4795 - val_mse: 263255840.0000 - val_mae: 15592.4795\n",
            "Epoch 294/500\n",
            "20/20 [==============================] - 0s 938us/step - loss: 9012.3777 - mse: 105192168.0000 - mae: 9012.3779 - val_loss: 15508.6768 - val_mse: 260784848.0000 - val_mae: 15508.6768\n",
            "Epoch 295/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9292.2349 - mse: 115996136.0000 - mae: 9292.2344 - val_loss: 15442.8906 - val_mse: 258858704.0000 - val_mae: 15442.8906\n",
            "Epoch 296/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9287.9470 - mse: 114861848.0000 - mae: 9287.9473 - val_loss: 15382.7734 - val_mse: 257102560.0000 - val_mae: 15382.7734\n",
            "Epoch 297/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8685.0107 - mse: 105606400.0000 - mae: 8685.0107 - val_loss: 15328.6982 - val_mse: 255516448.0000 - val_mae: 15328.6982\n",
            "Epoch 298/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9056.7432 - mse: 96206208.0000 - mae: 9056.7441 - val_loss: 15272.5879 - val_mse: 253874400.0000 - val_mae: 15272.5879\n",
            "Epoch 299/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8619.1743 - mse: 99065920.0000 - mae: 8619.1738 - val_loss: 15198.8047 - val_mse: 251747968.0000 - val_mae: 15198.8047\n",
            "Epoch 300/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8799.7031 - mse: 104232488.0000 - mae: 8799.7031 - val_loss: 15119.0781 - val_mse: 249477296.0000 - val_mae: 15119.0781\n",
            "Epoch 301/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7598.5420 - mse: 93618800.0000 - mae: 7598.5420 - val_loss: 15072.5703 - val_mse: 248125488.0000 - val_mae: 15072.5703\n",
            "Epoch 302/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9486.4209 - mse: 116060848.0000 - mae: 9486.4209 - val_loss: 14993.3691 - val_mse: 245878016.0000 - val_mae: 14993.3691\n",
            "Epoch 303/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7571.7351 - mse: 83500952.0000 - mae: 7571.7354 - val_loss: 14903.2012 - val_mse: 243364096.0000 - val_mae: 14903.2012\n",
            "Epoch 304/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9676.4534 - mse: 127469336.0000 - mae: 9676.4531 - val_loss: 14849.3877 - val_mse: 241842816.0000 - val_mae: 14849.3877\n",
            "Epoch 305/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10691.7231 - mse: 142571936.0000 - mae: 10691.7236 - val_loss: 14802.9346 - val_mse: 240535200.0000 - val_mae: 14802.9346\n",
            "Epoch 306/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7426.3315 - mse: 75872232.0000 - mae: 7426.3311 - val_loss: 14726.3203 - val_mse: 238402944.0000 - val_mae: 14726.3203\n",
            "Epoch 307/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8369.2178 - mse: 89530440.0000 - mae: 8369.2168 - val_loss: 14654.0645 - val_mse: 236403168.0000 - val_mae: 14654.0645\n",
            "Epoch 308/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8258.7893 - mse: 88292504.0000 - mae: 8258.7891 - val_loss: 14573.8809 - val_mse: 234215648.0000 - val_mae: 14573.8809\n",
            "Epoch 309/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9406.1846 - mse: 113431552.0000 - mae: 9406.1846 - val_loss: 14510.3027 - val_mse: 232483744.0000 - val_mae: 14510.3027\n",
            "Epoch 310/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8397.8298 - mse: 100402624.0000 - mae: 8397.8301 - val_loss: 14461.1748 - val_mse: 231121104.0000 - val_mae: 14461.1748\n",
            "Epoch 311/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7224.0205 - mse: 74290000.0000 - mae: 7224.0205 - val_loss: 14367.8750 - val_mse: 228606304.0000 - val_mae: 14367.8750\n",
            "Epoch 312/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9763.6123 - mse: 108034896.0000 - mae: 9763.6123 - val_loss: 14329.4316 - val_mse: 227548000.0000 - val_mae: 14329.4316\n",
            "Epoch 313/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7675.3618 - mse: 82042256.0000 - mae: 7675.3618 - val_loss: 14264.5957 - val_mse: 225802576.0000 - val_mae: 14264.5957\n",
            "Epoch 314/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10051.2490 - mse: 130737232.0000 - mae: 10051.2480 - val_loss: 14227.4512 - val_mse: 224795904.0000 - val_mae: 14227.4512\n",
            "Epoch 315/500\n",
            "20/20 [==============================] - 0s 986us/step - loss: 8681.8608 - mse: 104074560.0000 - mae: 8681.8613 - val_loss: 14144.7324 - val_mse: 222597280.0000 - val_mae: 14144.7324\n",
            "Epoch 316/500\n",
            "20/20 [==============================] - 0s 962us/step - loss: 8749.8047 - mse: 91670056.0000 - mae: 8749.8047 - val_loss: 14074.1582 - val_mse: 220724400.0000 - val_mae: 14074.1582\n",
            "Epoch 317/500\n",
            "20/20 [==============================] - 0s 998us/step - loss: 7550.7937 - mse: 75757696.0000 - mae: 7550.7939 - val_loss: 13995.3418 - val_mse: 218656352.0000 - val_mae: 13995.3418\n",
            "Epoch 318/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9380.9707 - mse: 121258856.0000 - mae: 9380.9707 - val_loss: 13934.0176 - val_mse: 217060144.0000 - val_mae: 13934.0176\n",
            "Epoch 319/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8878.6689 - mse: 112054320.0000 - mae: 8878.6689 - val_loss: 13866.5762 - val_mse: 215314144.0000 - val_mae: 13866.5762\n",
            "Epoch 320/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 9797.3721 - mse: 120494184.0000 - mae: 9797.3721 - val_loss: 13831.6357 - val_mse: 214385376.0000 - val_mae: 13831.6357\n",
            "Epoch 321/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7558.5371 - mse: 87225072.0000 - mae: 7558.5376 - val_loss: 13777.7295 - val_mse: 212967872.0000 - val_mae: 13777.7295\n",
            "Epoch 322/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8107.3992 - mse: 85330408.0000 - mae: 8107.3994 - val_loss: 13703.9922 - val_mse: 211072928.0000 - val_mae: 13703.9922\n",
            "Epoch 323/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8212.6487 - mse: 94195968.0000 - mae: 8212.6484 - val_loss: 13664.5371 - val_mse: 210045776.0000 - val_mae: 13664.5371\n",
            "Epoch 324/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7288.2180 - mse: 66236200.0000 - mae: 7288.2178 - val_loss: 13611.1230 - val_mse: 208672688.0000 - val_mae: 13611.1230\n",
            "Epoch 325/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7050.3005 - mse: 70633240.0000 - mae: 7050.2998 - val_loss: 13519.2998 - val_mse: 206355408.0000 - val_mae: 13519.2998\n",
            "Epoch 326/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 9113.8372 - mse: 113465712.0000 - mae: 9113.8379 - val_loss: 13454.4199 - val_mse: 204720208.0000 - val_mae: 13454.4199\n",
            "Epoch 327/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6469.5042 - mse: 63120616.0000 - mae: 6469.5039 - val_loss: 13425.1738 - val_mse: 203942016.0000 - val_mae: 13425.1738\n",
            "Epoch 328/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8520.4670 - mse: 92525976.0000 - mae: 8520.4668 - val_loss: 13354.0137 - val_mse: 202144928.0000 - val_mae: 13354.0137\n",
            "Epoch 329/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8218.5781 - mse: 93337920.0000 - mae: 8218.5781 - val_loss: 13312.7949 - val_mse: 201065440.0000 - val_mae: 13312.7949\n",
            "Epoch 330/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7907.3442 - mse: 90366640.0000 - mae: 7907.3438 - val_loss: 13256.0410 - val_mse: 199646304.0000 - val_mae: 13256.0410\n",
            "Epoch 331/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9202.1309 - mse: 103444992.0000 - mae: 9202.1309 - val_loss: 13216.4688 - val_mse: 198642576.0000 - val_mae: 13216.4688\n",
            "Epoch 332/500\n",
            "20/20 [==============================] - 0s 998us/step - loss: 6639.4490 - mse: 60360564.0000 - mae: 6639.4492 - val_loss: 13157.3652 - val_mse: 197182160.0000 - val_mae: 13157.3652\n",
            "Epoch 333/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7926.8755 - mse: 84884024.0000 - mae: 7926.8750 - val_loss: 13090.6387 - val_mse: 195561280.0000 - val_mae: 13090.6387\n",
            "Epoch 334/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7665.0308 - mse: 84885072.0000 - mae: 7665.0303 - val_loss: 13038.2314 - val_mse: 194271968.0000 - val_mae: 13038.2314\n",
            "Epoch 335/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8024.1902 - mse: 83561608.0000 - mae: 8024.1904 - val_loss: 13003.9561 - val_mse: 193387488.0000 - val_mae: 13003.9561\n",
            "Epoch 336/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8822.0564 - mse: 105882088.0000 - mae: 8822.0566 - val_loss: 12936.1270 - val_mse: 191741664.0000 - val_mae: 12936.1270\n",
            "Epoch 337/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7442.7266 - mse: 77562008.0000 - mae: 7442.7266 - val_loss: 12901.8535 - val_mse: 190904048.0000 - val_mae: 12901.8535\n",
            "Epoch 338/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8234.7979 - mse: 93122672.0000 - mae: 8234.7979 - val_loss: 12840.9736 - val_mse: 189427120.0000 - val_mae: 12840.9736\n",
            "Epoch 339/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8090.4094 - mse: 86163480.0000 - mae: 8090.4092 - val_loss: 12770.4990 - val_mse: 187754080.0000 - val_mae: 12770.4990\n",
            "Epoch 340/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8083.3503 - mse: 100420376.0000 - mae: 8083.3501 - val_loss: 12723.4980 - val_mse: 186624800.0000 - val_mae: 12723.4980\n",
            "Epoch 341/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7700.6157 - mse: 87395088.0000 - mae: 7700.6157 - val_loss: 12626.2568 - val_mse: 184357024.0000 - val_mae: 12626.2568\n",
            "Epoch 342/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6140.7627 - mse: 51969320.0000 - mae: 6140.7627 - val_loss: 12575.1855 - val_mse: 183197360.0000 - val_mae: 12575.1855\n",
            "Epoch 343/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7761.7500 - mse: 87498384.0000 - mae: 7761.7500 - val_loss: 12511.4404 - val_mse: 181709808.0000 - val_mae: 12511.4404\n",
            "Epoch 344/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7865.3831 - mse: 86695248.0000 - mae: 7865.3828 - val_loss: 12464.3945 - val_mse: 180601456.0000 - val_mae: 12464.3945\n",
            "Epoch 345/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8973.0332 - mse: 97457472.0000 - mae: 8973.0332 - val_loss: 12419.6055 - val_mse: 179526832.0000 - val_mae: 12419.6055\n",
            "Epoch 346/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6323.9985 - mse: 51062280.0000 - mae: 6323.9985 - val_loss: 12345.8701 - val_mse: 177834096.0000 - val_mae: 12345.8701\n",
            "Epoch 347/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8300.8037 - mse: 93165104.0000 - mae: 8300.8037 - val_loss: 12302.2236 - val_mse: 176805008.0000 - val_mae: 12302.2236\n",
            "Epoch 348/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8804.7520 - mse: 108279616.0000 - mae: 8804.7520 - val_loss: 12249.1309 - val_mse: 175577568.0000 - val_mae: 12249.1309\n",
            "Epoch 349/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7694.9604 - mse: 85977768.0000 - mae: 7694.9609 - val_loss: 12184.1797 - val_mse: 174112432.0000 - val_mae: 12184.1797\n",
            "Epoch 350/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9069.0068 - mse: 93769360.0000 - mae: 9069.0059 - val_loss: 12141.6279 - val_mse: 173130256.0000 - val_mae: 12141.6279\n",
            "Epoch 351/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6992.2529 - mse: 70004112.0000 - mae: 6992.2529 - val_loss: 12103.9531 - val_mse: 172264608.0000 - val_mae: 12103.9531\n",
            "Epoch 352/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8265.3459 - mse: 94652896.0000 - mae: 8265.3457 - val_loss: 12073.8477 - val_mse: 171567632.0000 - val_mae: 12073.8477\n",
            "Epoch 353/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6095.8047 - mse: 55447928.0000 - mae: 6095.8047 - val_loss: 11993.3672 - val_mse: 169776992.0000 - val_mae: 11993.3672\n",
            "Epoch 354/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7039.5977 - mse: 68522264.0000 - mae: 7039.5977 - val_loss: 11931.2676 - val_mse: 168357072.0000 - val_mae: 11931.2676\n",
            "Epoch 355/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7876.9182 - mse: 80647024.0000 - mae: 7876.9189 - val_loss: 11904.2373 - val_mse: 167722400.0000 - val_mae: 11904.2373\n",
            "Epoch 356/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7723.8904 - mse: 76912496.0000 - mae: 7723.8906 - val_loss: 11830.3926 - val_mse: 166100144.0000 - val_mae: 11830.3926\n",
            "Epoch 357/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6901.1260 - mse: 67781608.0000 - mae: 6901.1260 - val_loss: 11752.2012 - val_mse: 164394384.0000 - val_mae: 11752.2012\n",
            "Epoch 358/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7284.9062 - mse: 66771992.0000 - mae: 7284.9062 - val_loss: 11714.2559 - val_mse: 163539280.0000 - val_mae: 11714.2559\n",
            "Epoch 359/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7668.1848 - mse: 88298112.0000 - mae: 7668.1846 - val_loss: 11667.8613 - val_mse: 162528192.0000 - val_mae: 11667.8613\n",
            "Epoch 360/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6653.5024 - mse: 59847576.0000 - mae: 6653.5024 - val_loss: 11653.5361 - val_mse: 162196064.0000 - val_mae: 11653.5361\n",
            "Epoch 361/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7701.3008 - mse: 84779088.0000 - mae: 7701.3008 - val_loss: 11577.0703 - val_mse: 160563296.0000 - val_mae: 11577.0703\n",
            "Epoch 362/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7100.5156 - mse: 65454412.0000 - mae: 7100.5156 - val_loss: 11533.9199 - val_mse: 159704288.0000 - val_mae: 11533.9199\n",
            "Epoch 363/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6608.1658 - mse: 59063468.0000 - mae: 6608.1655 - val_loss: 11477.5557 - val_mse: 158490672.0000 - val_mae: 11477.5557\n",
            "Epoch 364/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7245.0972 - mse: 67567600.0000 - mae: 7245.0967 - val_loss: 11424.8486 - val_mse: 157372832.0000 - val_mae: 11424.8486\n",
            "Epoch 365/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8233.8071 - mse: 91979520.0000 - mae: 8233.8066 - val_loss: 11390.4043 - val_mse: 156605616.0000 - val_mae: 11390.4043\n",
            "Epoch 366/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7584.5969 - mse: 87460560.0000 - mae: 7584.5967 - val_loss: 11369.6611 - val_mse: 156117920.0000 - val_mae: 11369.6611\n",
            "Epoch 367/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7167.3625 - mse: 77224088.0000 - mae: 7167.3623 - val_loss: 11322.4668 - val_mse: 155109856.0000 - val_mae: 11322.4668\n",
            "Epoch 368/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7608.6753 - mse: 76050096.0000 - mae: 7608.6748 - val_loss: 11271.9980 - val_mse: 154046208.0000 - val_mae: 11271.9980\n",
            "Epoch 369/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8246.4058 - mse: 98424240.0000 - mae: 8246.4053 - val_loss: 11206.8496 - val_mse: 152711216.0000 - val_mae: 11206.8496\n",
            "Epoch 370/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7852.8835 - mse: 72839280.0000 - mae: 7852.8838 - val_loss: 11186.7061 - val_mse: 152242832.0000 - val_mae: 11186.7061\n",
            "Epoch 371/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7477.2480 - mse: 74343752.0000 - mae: 7477.2485 - val_loss: 11119.6230 - val_mse: 150863728.0000 - val_mae: 11119.6230\n",
            "Epoch 372/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6777.8921 - mse: 64461976.0000 - mae: 6777.8921 - val_loss: 11120.8105 - val_mse: 150910464.0000 - val_mae: 11120.8105\n",
            "Epoch 373/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8222.0557 - mse: 88067368.0000 - mae: 8222.0566 - val_loss: 11123.8574 - val_mse: 150916192.0000 - val_mae: 11123.8574\n",
            "Epoch 374/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8597.5444 - mse: 110291024.0000 - mae: 8597.5449 - val_loss: 11108.0205 - val_mse: 150558592.0000 - val_mae: 11108.0205\n",
            "Epoch 375/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6691.1602 - mse: 59230816.0000 - mae: 6691.1602 - val_loss: 11070.1152 - val_mse: 149770848.0000 - val_mae: 11070.1152\n",
            "Epoch 376/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7933.7139 - mse: 85737640.0000 - mae: 7933.7139 - val_loss: 11007.9893 - val_mse: 148493328.0000 - val_mae: 11007.9893\n",
            "Epoch 377/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8416.9187 - mse: 94751672.0000 - mae: 8416.9189 - val_loss: 10972.3936 - val_mse: 147770736.0000 - val_mae: 10972.3936\n",
            "Epoch 378/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7873.9614 - mse: 80694256.0000 - mae: 7873.9609 - val_loss: 10928.6426 - val_mse: 146881632.0000 - val_mae: 10928.6426\n",
            "Epoch 379/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8504.9541 - mse: 97890256.0000 - mae: 8504.9541 - val_loss: 10851.9092 - val_mse: 145366800.0000 - val_mae: 10851.9092\n",
            "Epoch 380/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6159.0525 - mse: 52775508.0000 - mae: 6159.0522 - val_loss: 10846.0479 - val_mse: 145162784.0000 - val_mae: 10846.0479\n",
            "Epoch 381/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7783.3921 - mse: 87943488.0000 - mae: 7783.3921 - val_loss: 10816.5518 - val_mse: 144563680.0000 - val_mae: 10816.5518\n",
            "Epoch 382/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7807.8281 - mse: 84923080.0000 - mae: 7807.8281 - val_loss: 10800.1406 - val_mse: 144174080.0000 - val_mae: 10800.1406\n",
            "Epoch 383/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7528.0459 - mse: 90723760.0000 - mae: 7528.0454 - val_loss: 10774.7920 - val_mse: 143652336.0000 - val_mae: 10774.7920\n",
            "Epoch 384/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6311.2876 - mse: 58614408.0000 - mae: 6311.2876 - val_loss: 10770.4199 - val_mse: 143559328.0000 - val_mae: 10770.4199\n",
            "Epoch 385/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5363.2627 - mse: 50292388.0000 - mae: 5363.2627 - val_loss: 10738.8818 - val_mse: 142883616.0000 - val_mae: 10738.8818\n",
            "Epoch 386/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6656.0679 - mse: 67358672.0000 - mae: 6656.0679 - val_loss: 10691.5566 - val_mse: 141946320.0000 - val_mae: 10691.5566\n",
            "Epoch 387/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6697.2844 - mse: 65584200.0000 - mae: 6697.2842 - val_loss: 10646.6191 - val_mse: 141099872.0000 - val_mae: 10646.6191\n",
            "Epoch 388/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5582.3406 - mse: 50357288.0000 - mae: 5582.3408 - val_loss: 10572.8965 - val_mse: 139663392.0000 - val_mae: 10572.8965\n",
            "Epoch 389/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7107.5393 - mse: 64364596.0000 - mae: 7107.5391 - val_loss: 10536.1006 - val_mse: 138913552.0000 - val_mae: 10536.1006\n",
            "Epoch 390/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6347.0168 - mse: 63132584.0000 - mae: 6347.0166 - val_loss: 10504.3896 - val_mse: 138285328.0000 - val_mae: 10504.3896\n",
            "Epoch 391/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5674.9651 - mse: 59134348.0000 - mae: 5674.9648 - val_loss: 10488.0996 - val_mse: 137927648.0000 - val_mae: 10488.0996\n",
            "Epoch 392/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7481.2896 - mse: 75148560.0000 - mae: 7481.2891 - val_loss: 10441.9600 - val_mse: 137013760.0000 - val_mae: 10441.9600\n",
            "Epoch 393/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5150.0444 - mse: 37639144.0000 - mae: 5150.0444 - val_loss: 10379.1309 - val_mse: 135795424.0000 - val_mae: 10379.1309\n",
            "Epoch 394/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6698.4226 - mse: 59446944.0000 - mae: 6698.4229 - val_loss: 10343.7637 - val_mse: 135109168.0000 - val_mae: 10343.7637\n",
            "Epoch 395/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6122.8215 - mse: 61782720.0000 - mae: 6122.8213 - val_loss: 10338.1504 - val_mse: 134933936.0000 - val_mae: 10338.1504\n",
            "Epoch 396/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6040.4553 - mse: 57433100.0000 - mae: 6040.4556 - val_loss: 10270.7520 - val_mse: 133654360.0000 - val_mae: 10270.7520\n",
            "Epoch 397/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7707.1211 - mse: 68191960.0000 - mae: 7707.1211 - val_loss: 10236.3018 - val_mse: 132975920.0000 - val_mae: 10236.3018\n",
            "Epoch 398/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6958.7776 - mse: 65124116.0000 - mae: 6958.7783 - val_loss: 10234.0986 - val_mse: 132964440.0000 - val_mae: 10234.0986\n",
            "Epoch 399/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7066.8589 - mse: 66328872.0000 - mae: 7066.8594 - val_loss: 10227.2109 - val_mse: 132754816.0000 - val_mae: 10227.2109\n",
            "Epoch 400/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7060.4226 - mse: 65703852.0000 - mae: 7060.4229 - val_loss: 10172.0781 - val_mse: 131715608.0000 - val_mae: 10172.0781\n",
            "Epoch 401/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5421.4476 - mse: 49650804.0000 - mae: 5421.4478 - val_loss: 10114.8867 - val_mse: 130654848.0000 - val_mae: 10114.8867\n",
            "Epoch 402/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7033.0735 - mse: 63229524.0000 - mae: 7033.0732 - val_loss: 10112.1553 - val_mse: 130518992.0000 - val_mae: 10112.1553\n",
            "Epoch 403/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6578.3076 - mse: 57446516.0000 - mae: 6578.3076 - val_loss: 10086.0059 - val_mse: 129991472.0000 - val_mae: 10086.0059\n",
            "Epoch 404/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8344.2844 - mse: 98925184.0000 - mae: 8344.2842 - val_loss: 10068.0127 - val_mse: 129624400.0000 - val_mae: 10068.0127\n",
            "Epoch 405/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5225.2871 - mse: 40785676.0000 - mae: 5225.2871 - val_loss: 10049.8027 - val_mse: 129339344.0000 - val_mae: 10049.8027\n",
            "Epoch 406/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8137.5232 - mse: 85864576.0000 - mae: 8137.5234 - val_loss: 10001.2539 - val_mse: 128447664.0000 - val_mae: 10001.2539\n",
            "Epoch 407/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 4765.1555 - mse: 29359418.0000 - mae: 4765.1553 - val_loss: 10002.6221 - val_mse: 128415656.0000 - val_mae: 10002.6221\n",
            "Epoch 408/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7122.7307 - mse: 71554536.0000 - mae: 7122.7314 - val_loss: 9927.5908 - val_mse: 127054232.0000 - val_mae: 9927.5908\n",
            "Epoch 409/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7858.7900 - mse: 80956160.0000 - mae: 7858.7900 - val_loss: 9888.5449 - val_mse: 126311384.0000 - val_mae: 9888.5449\n",
            "Epoch 410/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 5288.3054 - mse: 44314376.0000 - mae: 5288.3057 - val_loss: 9847.7480 - val_mse: 125519272.0000 - val_mae: 9847.7480\n",
            "Epoch 411/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7919.2100 - mse: 78908736.0000 - mae: 7919.2095 - val_loss: 9842.0127 - val_mse: 125376600.0000 - val_mae: 9842.0127\n",
            "Epoch 412/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6125.4375 - mse: 65094572.0000 - mae: 6125.4375 - val_loss: 9831.8193 - val_mse: 125149712.0000 - val_mae: 9831.8193\n",
            "Epoch 413/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7437.0496 - mse: 77613016.0000 - mae: 7437.0498 - val_loss: 9833.0576 - val_mse: 125160592.0000 - val_mae: 9833.0576\n",
            "Epoch 414/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8066.8928 - mse: 82522648.0000 - mae: 8066.8931 - val_loss: 9799.3496 - val_mse: 124515368.0000 - val_mae: 9799.3496\n",
            "Epoch 415/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10490.1685 - mse: 139153744.0000 - mae: 10490.1689 - val_loss: 9834.3564 - val_mse: 125094544.0000 - val_mae: 9834.3564\n",
            "Epoch 416/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6289.3079 - mse: 56416416.0000 - mae: 6289.3076 - val_loss: 9810.2422 - val_mse: 124627312.0000 - val_mae: 9810.2422\n",
            "Epoch 417/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5499.2126 - mse: 42295388.0000 - mae: 5499.2124 - val_loss: 9819.3340 - val_mse: 124696272.0000 - val_mae: 9819.3340\n",
            "Epoch 418/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 5845.8196 - mse: 56396972.0000 - mae: 5845.8193 - val_loss: 9850.2949 - val_mse: 125213376.0000 - val_mae: 9850.2949\n",
            "Epoch 419/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6621.8723 - mse: 66845880.0000 - mae: 6621.8721 - val_loss: 9838.4883 - val_mse: 124941656.0000 - val_mae: 9838.4883\n",
            "Epoch 420/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7075.4045 - mse: 67999248.0000 - mae: 7075.4048 - val_loss: 9800.6504 - val_mse: 124240592.0000 - val_mae: 9800.6504\n",
            "Epoch 421/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6277.7019 - mse: 61748200.0000 - mae: 6277.7021 - val_loss: 9737.4248 - val_mse: 123092440.0000 - val_mae: 9737.4248\n",
            "Epoch 422/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7276.3694 - mse: 75681456.0000 - mae: 7276.3696 - val_loss: 9702.8828 - val_mse: 122474288.0000 - val_mae: 9702.8828\n",
            "Epoch 423/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7069.2710 - mse: 66362600.0000 - mae: 7069.2710 - val_loss: 9751.2568 - val_mse: 123265768.0000 - val_mae: 9751.2568\n",
            "Epoch 424/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6912.2334 - mse: 65698836.0000 - mae: 6912.2334 - val_loss: 9736.3330 - val_mse: 122930880.0000 - val_mae: 9736.3330\n",
            "Epoch 425/500\n",
            "20/20 [==============================] - 0s 940us/step - loss: 6280.0972 - mse: 62958208.0000 - mae: 6280.0972 - val_loss: 9733.8672 - val_mse: 122852328.0000 - val_mae: 9733.8672\n",
            "Epoch 426/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6850.3916 - mse: 73891160.0000 - mae: 6850.3921 - val_loss: 9710.9424 - val_mse: 122417168.0000 - val_mae: 9710.9424\n",
            "Epoch 427/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7971.8259 - mse: 83643496.0000 - mae: 7971.8257 - val_loss: 9736.6357 - val_mse: 122875856.0000 - val_mae: 9736.6357\n",
            "Epoch 428/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7521.7537 - mse: 78108496.0000 - mae: 7521.7539 - val_loss: 9739.3770 - val_mse: 122830208.0000 - val_mae: 9739.3770\n",
            "Epoch 429/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6569.0410 - mse: 62916660.0000 - mae: 6569.0405 - val_loss: 9731.6865 - val_mse: 122637568.0000 - val_mae: 9731.6865\n",
            "Epoch 430/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8016.0593 - mse: 84342904.0000 - mae: 8016.0596 - val_loss: 9688.3066 - val_mse: 121865424.0000 - val_mae: 9688.3066\n",
            "Epoch 431/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5057.5142 - mse: 43014568.0000 - mae: 5057.5142 - val_loss: 9690.7939 - val_mse: 121871824.0000 - val_mae: 9690.7939\n",
            "Epoch 432/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6926.9229 - mse: 65526804.0000 - mae: 6926.9233 - val_loss: 9666.0342 - val_mse: 121389800.0000 - val_mae: 9666.0342\n",
            "Epoch 433/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6726.7327 - mse: 67893520.0000 - mae: 6726.7329 - val_loss: 9661.4043 - val_mse: 121249304.0000 - val_mae: 9661.4043\n",
            "Epoch 434/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7326.2788 - mse: 75697512.0000 - mae: 7326.2788 - val_loss: 9642.4814 - val_mse: 120888640.0000 - val_mae: 9642.4814\n",
            "Epoch 435/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6369.5549 - mse: 62902680.0000 - mae: 6369.5547 - val_loss: 9601.7441 - val_mse: 120221352.0000 - val_mae: 9601.7441\n",
            "Epoch 436/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8107.6580 - mse: 90885040.0000 - mae: 8107.6577 - val_loss: 9597.7217 - val_mse: 120103120.0000 - val_mae: 9597.7217\n",
            "Epoch 437/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8791.4175 - mse: 112167888.0000 - mae: 8791.4170 - val_loss: 9559.0703 - val_mse: 119406960.0000 - val_mae: 9559.0703\n",
            "Epoch 438/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8495.9504 - mse: 104299416.0000 - mae: 8495.9502 - val_loss: 9564.9463 - val_mse: 119471384.0000 - val_mae: 9564.9463\n",
            "Epoch 439/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7997.9653 - mse: 96516872.0000 - mae: 7997.9658 - val_loss: 9585.5918 - val_mse: 119755248.0000 - val_mae: 9585.5918\n",
            "Epoch 440/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7252.8770 - mse: 73636168.0000 - mae: 7252.8765 - val_loss: 9606.2656 - val_mse: 120038000.0000 - val_mae: 9606.2656\n",
            "Epoch 441/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7202.9133 - mse: 70231608.0000 - mae: 7202.9131 - val_loss: 9582.7451 - val_mse: 119577496.0000 - val_mae: 9582.7451\n",
            "Epoch 442/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7887.2432 - mse: 90446768.0000 - mae: 7887.2432 - val_loss: 9570.2354 - val_mse: 119323392.0000 - val_mae: 9570.2354\n",
            "Epoch 443/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7839.6201 - mse: 100683616.0000 - mae: 7839.6201 - val_loss: 9573.7910 - val_mse: 119365808.0000 - val_mae: 9573.7910\n",
            "Epoch 444/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7674.3362 - mse: 76394736.0000 - mae: 7674.3359 - val_loss: 9568.3145 - val_mse: 119293416.0000 - val_mae: 9568.3145\n",
            "Epoch 445/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8076.0256 - mse: 90752600.0000 - mae: 8076.0249 - val_loss: 9546.9355 - val_mse: 118879104.0000 - val_mae: 9546.9355\n",
            "Epoch 446/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6162.8892 - mse: 63377656.0000 - mae: 6162.8892 - val_loss: 9517.0186 - val_mse: 118317008.0000 - val_mae: 9517.0186\n",
            "Epoch 447/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5495.4243 - mse: 48480916.0000 - mae: 5495.4243 - val_loss: 9441.0977 - val_mse: 117017112.0000 - val_mae: 9441.0977\n",
            "Epoch 448/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6524.7004 - mse: 57686048.0000 - mae: 6524.7002 - val_loss: 9433.7139 - val_mse: 116860376.0000 - val_mae: 9433.7139\n",
            "Epoch 449/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5584.0183 - mse: 43889172.0000 - mae: 5584.0186 - val_loss: 9408.6699 - val_mse: 116400976.0000 - val_mae: 9408.6699\n",
            "Epoch 450/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6779.8662 - mse: 79684488.0000 - mae: 6779.8662 - val_loss: 9372.9316 - val_mse: 115777520.0000 - val_mae: 9372.9316\n",
            "Epoch 451/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7282.2144 - mse: 76146880.0000 - mae: 7282.2139 - val_loss: 9368.4004 - val_mse: 115625296.0000 - val_mae: 9368.4004\n",
            "Epoch 452/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8156.0347 - mse: 88162432.0000 - mae: 8156.0342 - val_loss: 9368.1562 - val_mse: 115581712.0000 - val_mae: 9368.1562\n",
            "Epoch 453/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5925.6621 - mse: 56050056.0000 - mae: 5925.6626 - val_loss: 9355.3721 - val_mse: 115297680.0000 - val_mae: 9355.3721\n",
            "Epoch 454/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5909.7405 - mse: 48116052.0000 - mae: 5909.7407 - val_loss: 9359.3232 - val_mse: 115332520.0000 - val_mae: 9359.3232\n",
            "Epoch 455/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6042.9995 - mse: 52620152.0000 - mae: 6042.9990 - val_loss: 9362.9863 - val_mse: 115317864.0000 - val_mae: 9362.9863\n",
            "Epoch 456/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6909.9951 - mse: 70383448.0000 - mae: 6909.9951 - val_loss: 9308.7686 - val_mse: 114394432.0000 - val_mae: 9308.7686\n",
            "Epoch 457/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7946.7388 - mse: 88602944.0000 - mae: 7946.7393 - val_loss: 9339.6826 - val_mse: 114889048.0000 - val_mae: 9339.6826\n",
            "Epoch 458/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7223.9402 - mse: 66680168.0000 - mae: 7223.9404 - val_loss: 9340.4062 - val_mse: 114857960.0000 - val_mae: 9340.4062\n",
            "Epoch 459/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10057.9399 - mse: 122257744.0000 - mae: 10057.9404 - val_loss: 9337.0566 - val_mse: 114768936.0000 - val_mae: 9337.0566\n",
            "Epoch 460/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6163.7649 - mse: 55904328.0000 - mae: 6163.7646 - val_loss: 9325.9932 - val_mse: 114505920.0000 - val_mae: 9325.9932\n",
            "Epoch 461/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7510.2786 - mse: 72169960.0000 - mae: 7510.2783 - val_loss: 9282.1982 - val_mse: 113764648.0000 - val_mae: 9282.1982\n",
            "Epoch 462/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6484.9607 - mse: 73770640.0000 - mae: 6484.9604 - val_loss: 9293.0117 - val_mse: 113854312.0000 - val_mae: 9293.0117\n",
            "Epoch 463/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6283.1494 - mse: 60083048.0000 - mae: 6283.1494 - val_loss: 9259.5215 - val_mse: 113257064.0000 - val_mae: 9259.5215\n",
            "Epoch 464/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6523.7434 - mse: 58313336.0000 - mae: 6523.7432 - val_loss: 9304.0645 - val_mse: 113904816.0000 - val_mae: 9304.0645\n",
            "Epoch 465/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7064.9495 - mse: 75780264.0000 - mae: 7064.9492 - val_loss: 9300.1719 - val_mse: 113801424.0000 - val_mae: 9300.1719\n",
            "Epoch 466/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7613.9255 - mse: 84004248.0000 - mae: 7613.9248 - val_loss: 9270.2578 - val_mse: 113273040.0000 - val_mae: 9270.2578\n",
            "Epoch 467/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6332.0662 - mse: 62338496.0000 - mae: 6332.0664 - val_loss: 9258.1123 - val_mse: 113014800.0000 - val_mae: 9258.1123\n",
            "Epoch 468/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7632.9307 - mse: 78179072.0000 - mae: 7632.9312 - val_loss: 9226.9688 - val_mse: 112438352.0000 - val_mae: 9226.9688\n",
            "Epoch 469/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8893.1108 - mse: 106707624.0000 - mae: 8893.1113 - val_loss: 9235.6348 - val_mse: 112590800.0000 - val_mae: 9235.6348\n",
            "Epoch 470/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6685.1106 - mse: 66133400.0000 - mae: 6685.1108 - val_loss: 9184.0039 - val_mse: 111699568.0000 - val_mae: 9184.0039\n",
            "Epoch 471/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8301.9814 - mse: 99919728.0000 - mae: 8301.9814 - val_loss: 9191.9922 - val_mse: 111825896.0000 - val_mae: 9191.9922\n",
            "Epoch 472/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8146.9829 - mse: 102838152.0000 - mae: 8146.9829 - val_loss: 9182.6855 - val_mse: 111647216.0000 - val_mae: 9182.6855\n",
            "Epoch 473/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7056.9426 - mse: 73034792.0000 - mae: 7056.9424 - val_loss: 9130.8877 - val_mse: 110772328.0000 - val_mae: 9130.8877\n",
            "Epoch 474/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7664.3464 - mse: 88944616.0000 - mae: 7664.3467 - val_loss: 9108.3691 - val_mse: 110365416.0000 - val_mae: 9108.3691\n",
            "Epoch 475/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8999.6245 - mse: 104067208.0000 - mae: 8999.6250 - val_loss: 9090.0762 - val_mse: 110099728.0000 - val_mae: 9090.0762\n",
            "Epoch 476/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 4909.3855 - mse: 35061736.0000 - mae: 4909.3857 - val_loss: 9078.5566 - val_mse: 109892112.0000 - val_mae: 9078.5566\n",
            "Epoch 477/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6603.0857 - mse: 65155176.0000 - mae: 6603.0859 - val_loss: 9031.4746 - val_mse: 109112944.0000 - val_mae: 9031.4746\n",
            "Epoch 478/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5673.4360 - mse: 42549796.0000 - mae: 5673.4360 - val_loss: 9023.4170 - val_mse: 108928640.0000 - val_mae: 9023.4170\n",
            "Epoch 479/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8246.0493 - mse: 90652056.0000 - mae: 8246.0488 - val_loss: 9008.7266 - val_mse: 108655696.0000 - val_mae: 9008.7266\n",
            "Epoch 480/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6883.0164 - mse: 69388992.0000 - mae: 6883.0166 - val_loss: 9013.1748 - val_mse: 108712400.0000 - val_mae: 9013.1748\n",
            "Epoch 481/500\n",
            "20/20 [==============================] - 0s 954us/step - loss: 6228.5581 - mse: 60520296.0000 - mae: 6228.5581 - val_loss: 9010.8955 - val_mse: 108652480.0000 - val_mae: 9010.8955\n",
            "Epoch 482/500\n",
            "20/20 [==============================] - 0s 966us/step - loss: 6912.0964 - mse: 65679384.0000 - mae: 6912.0967 - val_loss: 8965.0127 - val_mse: 107887872.0000 - val_mae: 8965.0127\n",
            "Epoch 483/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8479.2263 - mse: 92267544.0000 - mae: 8479.2266 - val_loss: 8993.7959 - val_mse: 108296976.0000 - val_mae: 8993.7959\n",
            "Epoch 484/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8414.8696 - mse: 87117928.0000 - mae: 8414.8701 - val_loss: 8998.3643 - val_mse: 108334960.0000 - val_mae: 8998.3643\n",
            "Epoch 485/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7201.0430 - mse: 70801936.0000 - mae: 7201.0430 - val_loss: 8982.8730 - val_mse: 108018176.0000 - val_mae: 8982.8730\n",
            "Epoch 486/500\n",
            "20/20 [==============================] - 0s 995us/step - loss: 5136.0920 - mse: 38300344.0000 - mae: 5136.0923 - val_loss: 8969.3057 - val_mse: 107746984.0000 - val_mae: 8969.3057\n",
            "Epoch 487/500\n",
            "20/20 [==============================] - 0s 957us/step - loss: 6617.1707 - mse: 64927256.0000 - mae: 6617.1704 - val_loss: 8985.6592 - val_mse: 107937000.0000 - val_mae: 8985.6592\n",
            "Epoch 488/500\n",
            "20/20 [==============================] - 0s 912us/step - loss: 7874.0369 - mse: 88337896.0000 - mae: 7874.0366 - val_loss: 8971.2686 - val_mse: 107676864.0000 - val_mae: 8971.2686\n",
            "Epoch 489/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6519.0264 - mse: 54558872.0000 - mae: 6519.0264 - val_loss: 9018.7754 - val_mse: 108338576.0000 - val_mae: 9018.7754\n",
            "Epoch 490/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6240.4934 - mse: 63426636.0000 - mae: 6240.4932 - val_loss: 9027.6152 - val_mse: 108389848.0000 - val_mae: 9027.6152\n",
            "Epoch 491/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6093.8843 - mse: 47989032.0000 - mae: 6093.8843 - val_loss: 9049.8721 - val_mse: 108754584.0000 - val_mae: 9049.8721\n",
            "Epoch 492/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7211.8411 - mse: 81708264.0000 - mae: 7211.8408 - val_loss: 8988.2246 - val_mse: 107745304.0000 - val_mae: 8988.2246\n",
            "Epoch 493/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6296.4854 - mse: 64386804.0000 - mae: 6296.4854 - val_loss: 9048.6934 - val_mse: 108714520.0000 - val_mae: 9048.6934\n",
            "Epoch 494/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6815.1731 - mse: 62169960.0000 - mae: 6815.1733 - val_loss: 9049.2295 - val_mse: 108675664.0000 - val_mae: 9049.2295\n",
            "Epoch 495/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8902.1484 - mse: 101363832.0000 - mae: 8902.1484 - val_loss: 9025.1074 - val_mse: 108258352.0000 - val_mae: 9025.1074\n",
            "Epoch 496/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6886.6753 - mse: 65369688.0000 - mae: 6886.6748 - val_loss: 9002.1504 - val_mse: 107889072.0000 - val_mae: 9002.1504\n",
            "Epoch 497/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5397.2406 - mse: 46734024.0000 - mae: 5397.2407 - val_loss: 8949.8438 - val_mse: 107088680.0000 - val_mae: 8949.8438\n",
            "Epoch 498/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6964.5557 - mse: 81095440.0000 - mae: 6964.5557 - val_loss: 8955.4678 - val_mse: 107137432.0000 - val_mae: 8955.4678\n",
            "Epoch 499/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7399.3975 - mse: 67384552.0000 - mae: 7399.3975 - val_loss: 8940.8613 - val_mse: 106859936.0000 - val_mae: 8940.8613\n",
            "Epoch 500/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5432.1914 - mse: 48946456.0000 - mae: 5432.1914 - val_loss: 8911.8066 - val_mse: 106336024.0000 - val_mae: 8911.8066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 30 samples, validate on 30 samples\n",
            "Epoch 1/500\n",
            "30/30 [==============================] - 2s 62ms/step - loss: 24466.5130 - mse: 625249600.0000 - mae: 24466.5117 - val_loss: 26857.2070 - val_mse: 732523200.0000 - val_mae: 26857.2090\n",
            "Epoch 2/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24466.4688 - mse: 625247552.0000 - mae: 24466.4688 - val_loss: 26857.1504 - val_mse: 732520000.0000 - val_mae: 26857.1504\n",
            "Epoch 3/500\n",
            "30/30 [==============================] - 0s 935us/step - loss: 24466.3880 - mse: 625243776.0000 - mae: 24466.3867 - val_loss: 26856.8809 - val_mse: 732504448.0000 - val_mae: 26856.8809\n",
            "Epoch 4/500\n",
            "30/30 [==============================] - 0s 888us/step - loss: 24465.8574 - mse: 625217920.0000 - mae: 24465.8574 - val_loss: 26855.2754 - val_mse: 732411328.0000 - val_mae: 26855.2754\n",
            "Epoch 5/500\n",
            "30/30 [==============================] - 0s 942us/step - loss: 24463.7428 - mse: 625116928.0000 - mae: 24463.7422 - val_loss: 26851.7734 - val_mse: 732208384.0000 - val_mae: 26851.7754\n",
            "Epoch 6/500\n",
            "30/30 [==============================] - 0s 937us/step - loss: 24461.0469 - mse: 624988992.0000 - mae: 24461.0449 - val_loss: 26848.4668 - val_mse: 732017024.0000 - val_mae: 26848.4688\n",
            "Epoch 7/500\n",
            "30/30 [==============================] - 0s 939us/step - loss: 24458.0553 - mse: 624838656.0000 - mae: 24458.0547 - val_loss: 26844.7852 - val_mse: 731804096.0000 - val_mae: 26844.7852\n",
            "Epoch 8/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24452.3379 - mse: 624562752.0000 - mae: 24452.3379 - val_loss: 26840.3490 - val_mse: 731548480.0000 - val_mae: 26840.3496\n",
            "Epoch 9/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24448.8587 - mse: 624382528.0000 - mae: 24448.8574 - val_loss: 26835.7760 - val_mse: 731285952.0000 - val_mae: 26835.7754\n",
            "Epoch 10/500\n",
            "30/30 [==============================] - 0s 877us/step - loss: 24444.5983 - mse: 624174272.0000 - mae: 24444.5977 - val_loss: 26830.7650 - val_mse: 730999424.0000 - val_mae: 26830.7637\n",
            "Epoch 11/500\n",
            "30/30 [==============================] - 0s 941us/step - loss: 24435.2148 - mse: 623706944.0000 - mae: 24435.2148 - val_loss: 26824.5924 - val_mse: 730647936.0000 - val_mae: 26824.5918\n",
            "Epoch 12/500\n",
            "30/30 [==============================] - 0s 927us/step - loss: 24431.4831 - mse: 623552768.0000 - mae: 24431.4824 - val_loss: 26818.2409 - val_mse: 730288576.0000 - val_mae: 26818.2422\n",
            "Epoch 13/500\n",
            "30/30 [==============================] - 0s 884us/step - loss: 24421.0215 - mse: 623062656.0000 - mae: 24421.0215 - val_loss: 26810.9661 - val_mse: 729877888.0000 - val_mae: 26810.9668\n",
            "Epoch 14/500\n",
            "30/30 [==============================] - 0s 877us/step - loss: 24419.8913 - mse: 623029824.0000 - mae: 24419.8926 - val_loss: 26803.9043 - val_mse: 729482176.0000 - val_mae: 26803.9043\n",
            "Epoch 15/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 24406.1523 - mse: 622351104.0000 - mae: 24406.1523 - val_loss: 26795.5417 - val_mse: 729013824.0000 - val_mae: 26795.5391\n",
            "Epoch 16/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24398.1973 - mse: 621902912.0000 - mae: 24398.1953 - val_loss: 26786.7812 - val_mse: 728523968.0000 - val_mae: 26786.7832\n",
            "Epoch 17/500\n",
            "30/30 [==============================] - 0s 905us/step - loss: 24390.8151 - mse: 621538560.0000 - mae: 24390.8145 - val_loss: 26777.9108 - val_mse: 728028352.0000 - val_mae: 26777.9121\n",
            "Epoch 18/500\n",
            "30/30 [==============================] - 0s 911us/step - loss: 24387.0371 - mse: 621355200.0000 - mae: 24387.0371 - val_loss: 26768.8997 - val_mse: 727528640.0000 - val_mae: 26768.9004\n",
            "Epoch 19/500\n",
            "30/30 [==============================] - 0s 928us/step - loss: 24374.0430 - mse: 620664768.0000 - mae: 24374.0410 - val_loss: 26759.0983 - val_mse: 726984768.0000 - val_mae: 26759.0996\n",
            "Epoch 20/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24360.6803 - mse: 620023744.0000 - mae: 24360.6816 - val_loss: 26748.4746 - val_mse: 726397824.0000 - val_mae: 26748.4746\n",
            "Epoch 21/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24349.7324 - mse: 619507136.0000 - mae: 24349.7324 - val_loss: 26737.4746 - val_mse: 725790080.0000 - val_mae: 26737.4746\n",
            "Epoch 22/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24324.2643 - mse: 618293248.0000 - mae: 24324.2617 - val_loss: 26724.7780 - val_mse: 725089856.0000 - val_mae: 26724.7793\n",
            "Epoch 23/500\n",
            "30/30 [==============================] - 0s 990us/step - loss: 24343.4414 - mse: 619136448.0000 - mae: 24343.4414 - val_loss: 26714.1621 - val_mse: 724506944.0000 - val_mae: 26714.1621\n",
            "Epoch 24/500\n",
            "30/30 [==============================] - 0s 932us/step - loss: 24311.2780 - mse: 617888192.0000 - mae: 24311.2793 - val_loss: 26701.2533 - val_mse: 723799936.0000 - val_mae: 26701.2520\n",
            "Epoch 25/500\n",
            "30/30 [==============================] - 0s 990us/step - loss: 24301.6081 - mse: 617112704.0000 - mae: 24301.6074 - val_loss: 26687.9245 - val_mse: 723072128.0000 - val_mae: 26687.9238\n",
            "Epoch 26/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24267.6152 - mse: 615476672.0000 - mae: 24267.6172 - val_loss: 26672.6751 - val_mse: 722241984.0000 - val_mae: 26672.6758\n",
            "Epoch 27/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24270.9225 - mse: 615648192.0000 - mae: 24270.9238 - val_loss: 26658.3529 - val_mse: 721463232.0000 - val_mae: 26658.3535\n",
            "Epoch 28/500\n",
            "30/30 [==============================] - 0s 951us/step - loss: 24238.2428 - mse: 614083776.0000 - mae: 24238.2441 - val_loss: 26642.3939 - val_mse: 720595776.0000 - val_mae: 26642.3945\n",
            "Epoch 29/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24229.3809 - mse: 613677312.0000 - mae: 24229.3809 - val_loss: 26626.3294 - val_mse: 719724288.0000 - val_mae: 26626.3301\n",
            "Epoch 30/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24210.9557 - mse: 612981632.0000 - mae: 24210.9570 - val_loss: 26610.0996 - val_mse: 718843136.0000 - val_mae: 26610.0977\n",
            "Epoch 31/500\n",
            "30/30 [==============================] - 0s 991us/step - loss: 24191.2923 - mse: 611776640.0000 - mae: 24191.2930 - val_loss: 26593.1074 - val_mse: 717923264.0000 - val_mae: 26593.1074\n",
            "Epoch 32/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24206.4349 - mse: 612846272.0000 - mae: 24206.4336 - val_loss: 26577.2396 - val_mse: 717067456.0000 - val_mae: 26577.2383\n",
            "Epoch 33/500\n",
            "30/30 [==============================] - 0s 839us/step - loss: 24158.6165 - mse: 609944000.0000 - mae: 24158.6172 - val_loss: 26559.2025 - val_mse: 716094272.0000 - val_mae: 26559.2012\n",
            "Epoch 34/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24132.7201 - mse: 609314944.0000 - mae: 24132.7207 - val_loss: 26540.2025 - val_mse: 715071808.0000 - val_mae: 26540.2012\n",
            "Epoch 35/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24119.8991 - mse: 608509248.0000 - mae: 24119.9004 - val_loss: 26521.3490 - val_mse: 714057472.0000 - val_mae: 26521.3496\n",
            "Epoch 36/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24108.7780 - mse: 607870784.0000 - mae: 24108.7793 - val_loss: 26502.1810 - val_mse: 713028288.0000 - val_mae: 26502.1816\n",
            "Epoch 37/500\n",
            "30/30 [==============================] - 0s 991us/step - loss: 24080.0052 - mse: 606514432.0000 - mae: 24080.0059 - val_loss: 26481.9779 - val_mse: 711942848.0000 - val_mae: 26481.9766\n",
            "Epoch 38/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24073.8906 - mse: 606369088.0000 - mae: 24073.8887 - val_loss: 26461.7031 - val_mse: 710857600.0000 - val_mae: 26461.7012\n",
            "Epoch 39/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24062.8919 - mse: 605475200.0000 - mae: 24062.8926 - val_loss: 26441.2812 - val_mse: 709763712.0000 - val_mae: 26441.2812\n",
            "Epoch 40/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23965.3307 - mse: 600790656.0000 - mae: 23965.3301 - val_loss: 26416.9004 - val_mse: 708459840.0000 - val_mae: 26416.9004\n",
            "Epoch 41/500\n",
            "30/30 [==============================] - 0s 930us/step - loss: 24014.6237 - mse: 603548224.0000 - mae: 24014.6230 - val_loss: 26395.2663 - val_mse: 707305792.0000 - val_mae: 26395.2676\n",
            "Epoch 42/500\n",
            "30/30 [==============================] - 0s 960us/step - loss: 23953.1654 - mse: 600372608.0000 - mae: 23953.1641 - val_loss: 26371.6589 - val_mse: 706047040.0000 - val_mae: 26371.6582\n",
            "Epoch 43/500\n",
            "30/30 [==============================] - 0s 970us/step - loss: 23980.1797 - mse: 601283520.0000 - mae: 23980.1797 - val_loss: 26349.1413 - val_mse: 704851264.0000 - val_mae: 26349.1426\n",
            "Epoch 44/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23948.4980 - mse: 600396992.0000 - mae: 23948.4980 - val_loss: 26326.1120 - val_mse: 703627648.0000 - val_mae: 26326.1133\n",
            "Epoch 45/500\n",
            "30/30 [==============================] - 0s 916us/step - loss: 23886.0645 - mse: 597377408.0000 - mae: 23886.0645 - val_loss: 26300.8053 - val_mse: 702285952.0000 - val_mae: 26300.8066\n",
            "Epoch 46/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23868.8210 - mse: 596057472.0000 - mae: 23868.8203 - val_loss: 26275.3626 - val_mse: 700937472.0000 - val_mae: 26275.3633\n",
            "Epoch 47/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23893.3965 - mse: 597029888.0000 - mae: 23893.3965 - val_loss: 26251.6999 - val_mse: 699683008.0000 - val_mae: 26251.6973\n",
            "Epoch 48/500\n",
            "30/30 [==============================] - 0s 997us/step - loss: 23792.1875 - mse: 591888576.0000 - mae: 23792.1875 - val_loss: 26223.9212 - val_mse: 698218880.0000 - val_mae: 26223.9199\n",
            "Epoch 49/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23849.2617 - mse: 595549376.0000 - mae: 23849.2617 - val_loss: 26198.8164 - val_mse: 696897472.0000 - val_mae: 26198.8164\n",
            "Epoch 50/500\n",
            "30/30 [==============================] - 0s 894us/step - loss: 23774.6120 - mse: 591732736.0000 - mae: 23774.6133 - val_loss: 26171.4727 - val_mse: 695459584.0000 - val_mae: 26171.4707\n",
            "Epoch 51/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23769.6367 - mse: 591919360.0000 - mae: 23769.6367 - val_loss: 26144.0638 - val_mse: 694024256.0000 - val_mae: 26144.0625\n",
            "Epoch 52/500\n",
            "30/30 [==============================] - 0s 954us/step - loss: 23700.2552 - mse: 587947008.0000 - mae: 23700.2539 - val_loss: 26114.8535 - val_mse: 692493504.0000 - val_mae: 26114.8516\n",
            "Epoch 53/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23670.0339 - mse: 587304192.0000 - mae: 23670.0332 - val_loss: 26085.2533 - val_mse: 690942144.0000 - val_mae: 26085.2539\n",
            "Epoch 54/500\n",
            "30/30 [==============================] - 0s 991us/step - loss: 23707.3698 - mse: 588933696.0000 - mae: 23707.3711 - val_loss: 26057.1927 - val_mse: 689476736.0000 - val_mae: 26057.1914\n",
            "Epoch 55/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23606.6146 - mse: 583343552.0000 - mae: 23606.6152 - val_loss: 26025.8730 - val_mse: 687845568.0000 - val_mae: 26025.8730\n",
            "Epoch 56/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23620.8470 - mse: 584600768.0000 - mae: 23620.8457 - val_loss: 25996.1361 - val_mse: 686294336.0000 - val_mae: 25996.1367\n",
            "Epoch 57/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23515.2168 - mse: 579622464.0000 - mae: 23515.2168 - val_loss: 25962.9167 - val_mse: 684567232.0000 - val_mae: 25962.9160\n",
            "Epoch 58/500\n",
            "30/30 [==============================] - 0s 961us/step - loss: 23614.6361 - mse: 584117440.0000 - mae: 23614.6348 - val_loss: 25932.9883 - val_mse: 683017600.0000 - val_mae: 25932.9883\n",
            "Epoch 59/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23431.1973 - mse: 575674496.0000 - mae: 23431.1953 - val_loss: 25897.3626 - val_mse: 681176064.0000 - val_mae: 25897.3633\n",
            "Epoch 60/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23404.8724 - mse: 573711616.0000 - mae: 23404.8711 - val_loss: 25861.7793 - val_mse: 679340288.0000 - val_mae: 25861.7793\n",
            "Epoch 61/500\n",
            "30/30 [==============================] - 0s 976us/step - loss: 23435.0625 - mse: 576487104.0000 - mae: 23435.0625 - val_loss: 25826.5592 - val_mse: 677534656.0000 - val_mae: 25826.5586\n",
            "Epoch 62/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23444.9961 - mse: 576671488.0000 - mae: 23444.9961 - val_loss: 25792.9199 - val_mse: 675807424.0000 - val_mae: 25792.9199\n",
            "Epoch 63/500\n",
            "30/30 [==============================] - 0s 942us/step - loss: 23405.9707 - mse: 574348032.0000 - mae: 23405.9707 - val_loss: 25757.6673 - val_mse: 674008128.0000 - val_mae: 25757.6660\n",
            "Epoch 64/500\n",
            "30/30 [==============================] - 0s 959us/step - loss: 23424.1510 - mse: 574535808.0000 - mae: 23424.1504 - val_loss: 25724.7500 - val_mse: 672322240.0000 - val_mae: 25724.7500\n",
            "Epoch 65/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23388.0404 - mse: 574240704.0000 - mae: 23388.0391 - val_loss: 25690.9850 - val_mse: 670593408.0000 - val_mae: 25690.9883\n",
            "Epoch 66/500\n",
            "30/30 [==============================] - 0s 912us/step - loss: 23296.5202 - mse: 569236864.0000 - mae: 23296.5215 - val_loss: 25655.3197 - val_mse: 668766976.0000 - val_mae: 25655.3203\n",
            "Epoch 67/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23199.9056 - mse: 564888128.0000 - mae: 23199.9062 - val_loss: 25616.9779 - val_mse: 666809792.0000 - val_mae: 25616.9766\n",
            "Epoch 68/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23179.3333 - mse: 564689664.0000 - mae: 23179.3340 - val_loss: 25578.5853 - val_mse: 664853504.0000 - val_mae: 25578.5859\n",
            "Epoch 69/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23079.3112 - mse: 560159680.0000 - mae: 23079.3105 - val_loss: 25537.9961 - val_mse: 662790848.0000 - val_mae: 25537.9961\n",
            "Epoch 70/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23155.2806 - mse: 562604928.0000 - mae: 23155.2793 - val_loss: 25500.9427 - val_mse: 660904256.0000 - val_mae: 25500.9434\n",
            "Epoch 71/500\n",
            "30/30 [==============================] - 0s 970us/step - loss: 23096.2272 - mse: 560191872.0000 - mae: 23096.2266 - val_loss: 25462.0840 - val_mse: 658933888.0000 - val_mae: 25462.0859\n",
            "Epoch 72/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23086.9603 - mse: 561458240.0000 - mae: 23086.9590 - val_loss: 25422.8340 - val_mse: 656952000.0000 - val_mae: 25422.8340\n",
            "Epoch 73/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23004.7689 - mse: 555840448.0000 - mae: 23004.7695 - val_loss: 25381.9525 - val_mse: 654888704.0000 - val_mae: 25381.9512\n",
            "Epoch 74/500\n",
            "30/30 [==============================] - 0s 919us/step - loss: 22865.0091 - mse: 550184704.0000 - mae: 22865.0078 - val_loss: 25338.6882 - val_mse: 652703296.0000 - val_mae: 25338.6875\n",
            "Epoch 75/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22858.8633 - mse: 550029696.0000 - mae: 22858.8652 - val_loss: 25294.7044 - val_mse: 650485376.0000 - val_mae: 25294.7051\n",
            "Epoch 76/500\n",
            "30/30 [==============================] - 0s 910us/step - loss: 22937.1680 - mse: 554395328.0000 - mae: 22937.1660 - val_loss: 25254.1719 - val_mse: 648439680.0000 - val_mae: 25254.1699\n",
            "Epoch 77/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23127.4134 - mse: 563942784.0000 - mae: 23127.4121 - val_loss: 25218.6348 - val_mse: 646652288.0000 - val_mae: 25218.6348\n",
            "Epoch 78/500\n",
            "30/30 [==============================] - 0s 949us/step - loss: 22777.9857 - mse: 546031360.0000 - mae: 22777.9863 - val_loss: 25174.8092 - val_mse: 644445888.0000 - val_mae: 25174.8086\n",
            "Epoch 79/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22823.0898 - mse: 545633024.0000 - mae: 22823.0898 - val_loss: 25132.5872 - val_mse: 642324096.0000 - val_mae: 25132.5879\n",
            "Epoch 80/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22724.3529 - mse: 543664576.0000 - mae: 22724.3516 - val_loss: 25087.9336 - val_mse: 640088064.0000 - val_mae: 25087.9336\n",
            "Epoch 81/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22798.1673 - mse: 546803136.0000 - mae: 22798.1660 - val_loss: 25046.5215 - val_mse: 638015680.0000 - val_mae: 25046.5215\n",
            "Epoch 82/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22520.6146 - mse: 535098848.0000 - mae: 22520.6152 - val_loss: 24998.5996 - val_mse: 635620672.0000 - val_mae: 24998.5977\n",
            "Epoch 83/500\n",
            "30/30 [==============================] - 0s 949us/step - loss: 22460.5456 - mse: 531033280.0000 - mae: 22460.5449 - val_loss: 24948.5807 - val_mse: 633131392.0000 - val_mae: 24948.5820\n",
            "Epoch 84/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22499.1185 - mse: 532257728.0000 - mae: 22499.1172 - val_loss: 24901.7852 - val_mse: 630804736.0000 - val_mae: 24901.7832\n",
            "Epoch 85/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22504.1484 - mse: 532535904.0000 - mae: 22504.1484 - val_loss: 24855.4753 - val_mse: 628506432.0000 - val_mae: 24855.4746\n",
            "Epoch 86/500\n",
            "30/30 [==============================] - 0s 897us/step - loss: 22570.6328 - mse: 535020192.0000 - mae: 22570.6328 - val_loss: 24811.4961 - val_mse: 626326912.0000 - val_mae: 24811.4961\n",
            "Epoch 87/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22313.4466 - mse: 525499200.0000 - mae: 22313.4453 - val_loss: 24762.0391 - val_mse: 623881152.0000 - val_mae: 24762.0391\n",
            "Epoch 88/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22291.5150 - mse: 524528448.0000 - mae: 22291.5137 - val_loss: 24713.4616 - val_mse: 621480512.0000 - val_mae: 24713.4609\n",
            "Epoch 89/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22295.3385 - mse: 523379520.0000 - mae: 22295.3379 - val_loss: 24665.4206 - val_mse: 619111424.0000 - val_mae: 24665.4199\n",
            "Epoch 90/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22357.4896 - mse: 526621888.0000 - mae: 22357.4902 - val_loss: 24618.8750 - val_mse: 616821184.0000 - val_mae: 24618.8750\n",
            "Epoch 91/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22246.3561 - mse: 523665888.0000 - mae: 22246.3555 - val_loss: 24570.0540 - val_mse: 614424832.0000 - val_mae: 24570.0547\n",
            "Epoch 92/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22195.1602 - mse: 520146464.0000 - mae: 22195.1582 - val_loss: 24521.5788 - val_mse: 612046848.0000 - val_mae: 24521.5801\n",
            "Epoch 93/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22071.0059 - mse: 515241696.0000 - mae: 22071.0059 - val_loss: 24470.4681 - val_mse: 609547264.0000 - val_mae: 24470.4668\n",
            "Epoch 94/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22148.1204 - mse: 516299776.0000 - mae: 22148.1211 - val_loss: 24421.7747 - val_mse: 607168256.0000 - val_mae: 24421.7754\n",
            "Epoch 95/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21954.3236 - mse: 507061408.0000 - mae: 21954.3242 - val_loss: 24369.3763 - val_mse: 604616448.0000 - val_mae: 24369.3770\n",
            "Epoch 96/500\n",
            "30/30 [==============================] - 0s 957us/step - loss: 21942.2943 - mse: 507534176.0000 - mae: 21942.2930 - val_loss: 24316.5827 - val_mse: 602051648.0000 - val_mae: 24316.5820\n",
            "Epoch 97/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21827.4798 - mse: 504293920.0000 - mae: 21827.4785 - val_loss: 24262.1777 - val_mse: 599412032.0000 - val_mae: 24262.1797\n",
            "Epoch 98/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22057.5286 - mse: 514516384.0000 - mae: 22057.5293 - val_loss: 24212.4186 - val_mse: 597002112.0000 - val_mae: 24212.4180\n",
            "Epoch 99/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21721.2246 - mse: 499978624.0000 - mae: 21721.2246 - val_loss: 24156.1230 - val_mse: 594282752.0000 - val_mae: 24156.1230\n",
            "Epoch 100/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21865.7038 - mse: 506653568.0000 - mae: 21865.7051 - val_loss: 24103.5801 - val_mse: 591748544.0000 - val_mae: 24103.5820\n",
            "Epoch 101/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21440.9505 - mse: 486606688.0000 - mae: 21440.9492 - val_loss: 24043.3607 - val_mse: 588851072.0000 - val_mae: 24043.3633\n",
            "Epoch 102/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21513.7005 - mse: 491014752.0000 - mae: 21513.6992 - val_loss: 23985.3405 - val_mse: 586066624.0000 - val_mae: 23985.3418\n",
            "Epoch 103/500\n",
            "30/30 [==============================] - 0s 976us/step - loss: 21581.5176 - mse: 495860736.0000 - mae: 21581.5195 - val_loss: 23929.3118 - val_mse: 583383872.0000 - val_mae: 23929.3105\n",
            "Epoch 104/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21398.4701 - mse: 484746944.0000 - mae: 21398.4688 - val_loss: 23870.0365 - val_mse: 580550848.0000 - val_mae: 23870.0371\n",
            "Epoch 105/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21307.8223 - mse: 481787712.0000 - mae: 21307.8223 - val_loss: 23809.6458 - val_mse: 577675520.0000 - val_mae: 23809.6465\n",
            "Epoch 106/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21481.9870 - mse: 487578080.0000 - mae: 21481.9883 - val_loss: 23753.2715 - val_mse: 574997120.0000 - val_mae: 23753.2715\n",
            "Epoch 107/500\n",
            "30/30 [==============================] - 0s 949us/step - loss: 21514.7832 - mse: 491957760.0000 - mae: 21514.7832 - val_loss: 23698.2344 - val_mse: 572385920.0000 - val_mae: 23698.2324\n",
            "Epoch 108/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21235.8288 - mse: 478084000.0000 - mae: 21235.8301 - val_loss: 23637.9154 - val_mse: 569533248.0000 - val_mae: 23637.9141\n",
            "Epoch 109/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21065.7624 - mse: 472315328.0000 - mae: 21065.7617 - val_loss: 23575.5124 - val_mse: 566589248.0000 - val_mae: 23575.5117\n",
            "Epoch 110/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21014.0436 - mse: 467714976.0000 - mae: 21014.0410 - val_loss: 23513.2904 - val_mse: 563662976.0000 - val_mae: 23513.2891\n",
            "Epoch 111/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21010.3965 - mse: 469009728.0000 - mae: 21010.3965 - val_loss: 23451.2018 - val_mse: 560750784.0000 - val_mae: 23451.2012\n",
            "Epoch 112/500\n",
            "30/30 [==============================] - 0s 939us/step - loss: 21179.8366 - mse: 477242848.0000 - mae: 21179.8379 - val_loss: 23392.2871 - val_mse: 557994560.0000 - val_mae: 23392.2871\n",
            "Epoch 113/500\n",
            "30/30 [==============================] - 0s 997us/step - loss: 21015.2142 - mse: 470910016.0000 - mae: 21015.2148 - val_loss: 23331.8190 - val_mse: 555170048.0000 - val_mae: 23331.8203\n",
            "Epoch 114/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 20739.6875 - mse: 457795040.0000 - mae: 20739.6875 - val_loss: 23267.3633 - val_mse: 552167872.0000 - val_mae: 23267.3633\n",
            "Epoch 115/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 20845.7493 - mse: 463954784.0000 - mae: 20845.7480 - val_loss: 23204.4590 - val_mse: 549245504.0000 - val_mae: 23204.4590\n",
            "Epoch 116/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21121.4954 - mse: 473084128.0000 - mae: 21121.4961 - val_loss: 23147.1582 - val_mse: 546591296.0000 - val_mae: 23147.1582\n",
            "Epoch 117/500\n",
            "30/30 [==============================] - 0s 917us/step - loss: 20912.7539 - mse: 465086752.0000 - mae: 20912.7539 - val_loss: 23086.7487 - val_mse: 543798272.0000 - val_mae: 23086.7500\n",
            "Epoch 118/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 20490.8490 - mse: 447533952.0000 - mae: 20490.8496 - val_loss: 23020.2194 - val_mse: 540732544.0000 - val_mae: 23020.2188\n",
            "Epoch 119/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 20491.0560 - mse: 448189984.0000 - mae: 20491.0566 - val_loss: 22954.5332 - val_mse: 537713152.0000 - val_mae: 22954.5332\n",
            "Epoch 120/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 20703.2181 - mse: 457574624.0000 - mae: 20703.2188 - val_loss: 22892.1159 - val_mse: 534852480.0000 - val_mae: 22892.1172\n",
            "Epoch 121/500\n",
            "30/30 [==============================] - 0s 854us/step - loss: 20635.3626 - mse: 455641952.0000 - mae: 20635.3633 - val_loss: 22829.0495 - val_mse: 531970432.0000 - val_mae: 22829.0508\n",
            "Epoch 122/500\n",
            "30/30 [==============================] - 0s 894us/step - loss: 20151.5443 - mse: 435263520.0000 - mae: 20151.5430 - val_loss: 22759.1491 - val_mse: 528783488.0000 - val_mae: 22759.1504\n",
            "Epoch 123/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 20178.8971 - mse: 434136160.0000 - mae: 20178.8965 - val_loss: 22690.2676 - val_mse: 525652256.0000 - val_mae: 22690.2676\n",
            "Epoch 124/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 20364.9427 - mse: 441380512.0000 - mae: 20364.9414 - val_loss: 22624.5977 - val_mse: 522675680.0000 - val_mae: 22624.5977\n",
            "Epoch 125/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 20165.4154 - mse: 436607136.0000 - mae: 20165.4141 - val_loss: 22555.6491 - val_mse: 519559104.0000 - val_mae: 22555.6484\n",
            "Epoch 126/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 20115.0202 - mse: 433195424.0000 - mae: 20115.0215 - val_loss: 22486.6035 - val_mse: 516448064.0000 - val_mae: 22486.6035\n",
            "Epoch 127/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 20221.8197 - mse: 439787232.0000 - mae: 20221.8203 - val_loss: 22419.4564 - val_mse: 513430528.0000 - val_mae: 22419.4570\n",
            "Epoch 128/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 20553.2812 - mse: 448597952.0000 - mae: 20553.2812 - val_loss: 22357.3581 - val_mse: 510648928.0000 - val_mae: 22357.3574\n",
            "Epoch 129/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 19617.3340 - mse: 409218144.0000 - mae: 19617.3340 - val_loss: 22281.1901 - val_mse: 507246528.0000 - val_mae: 22281.1895\n",
            "Epoch 130/500\n",
            "30/30 [==============================] - 0s 997us/step - loss: 19918.5755 - mse: 423247424.0000 - mae: 19918.5742 - val_loss: 22210.1960 - val_mse: 504085696.0000 - val_mae: 22210.1953\n",
            "Epoch 131/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 19661.1074 - mse: 414828576.0000 - mae: 19661.1074 - val_loss: 22135.9954 - val_mse: 500793344.0000 - val_mae: 22135.9961\n",
            "Epoch 132/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 19908.5892 - mse: 423704160.0000 - mae: 19908.5879 - val_loss: 22065.9382 - val_mse: 497695168.0000 - val_mae: 22065.9375\n",
            "Epoch 133/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 19681.3040 - mse: 413620864.0000 - mae: 19681.3047 - val_loss: 21993.0456 - val_mse: 494481824.0000 - val_mae: 21993.0449\n",
            "Epoch 134/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 19369.9056 - mse: 404657440.0000 - mae: 19369.9062 - val_loss: 21916.1699 - val_mse: 491102848.0000 - val_mae: 21916.1699\n",
            "Epoch 135/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 19331.4707 - mse: 397760704.0000 - mae: 19331.4707 - val_loss: 21840.6771 - val_mse: 487796768.0000 - val_mae: 21840.6777\n",
            "Epoch 136/500\n",
            "30/30 [==============================] - 0s 958us/step - loss: 19492.3255 - mse: 406808768.0000 - mae: 19492.3242 - val_loss: 21767.1426 - val_mse: 484588480.0000 - val_mae: 21767.1426\n",
            "Epoch 137/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 19166.3014 - mse: 391890208.0000 - mae: 19166.3027 - val_loss: 21689.3945 - val_mse: 481207648.0000 - val_mae: 21689.3965\n",
            "Epoch 138/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 19006.0208 - mse: 387813856.0000 - mae: 19006.0215 - val_loss: 21610.1888 - val_mse: 477776064.0000 - val_mae: 21610.1875\n",
            "Epoch 139/500\n",
            "30/30 [==============================] - 0s 903us/step - loss: 19094.1836 - mse: 392893440.0000 - mae: 19094.1836 - val_loss: 21532.6699 - val_mse: 474430048.0000 - val_mae: 21532.6699\n",
            "Epoch 140/500\n",
            "30/30 [==============================] - 0s 953us/step - loss: 19171.6732 - mse: 395474400.0000 - mae: 19171.6758 - val_loss: 21456.9225 - val_mse: 471171904.0000 - val_mae: 21456.9199\n",
            "Epoch 141/500\n",
            "30/30 [==============================] - 0s 936us/step - loss: 19125.7467 - mse: 392230496.0000 - mae: 19125.7461 - val_loss: 21381.2884 - val_mse: 467929600.0000 - val_mae: 21381.2871\n",
            "Epoch 142/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 18777.7572 - mse: 386751616.0000 - mae: 18777.7578 - val_loss: 21300.6055 - val_mse: 464483616.0000 - val_mae: 21300.6035\n",
            "Epoch 143/500\n",
            "30/30 [==============================] - 0s 945us/step - loss: 18914.2324 - mse: 386030560.0000 - mae: 18914.2324 - val_loss: 21222.7559 - val_mse: 461171296.0000 - val_mae: 21222.7559\n",
            "Epoch 144/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 19052.5065 - mse: 391805760.0000 - mae: 19052.5078 - val_loss: 21147.2233 - val_mse: 457968768.0000 - val_mae: 21147.2227\n",
            "Epoch 145/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 18846.2129 - mse: 384153984.0000 - mae: 18846.2129 - val_loss: 21069.0840 - val_mse: 454667808.0000 - val_mae: 21069.0840\n",
            "Epoch 146/500\n",
            "30/30 [==============================] - 0s 969us/step - loss: 18432.5514 - mse: 371366016.0000 - mae: 18432.5508 - val_loss: 20986.4876 - val_mse: 451192736.0000 - val_mae: 20986.4883\n",
            "Epoch 147/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 18299.0488 - mse: 363149920.0000 - mae: 18299.0508 - val_loss: 20902.9512 - val_mse: 447691360.0000 - val_mae: 20902.9512\n",
            "Epoch 148/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 18673.0866 - mse: 376740192.0000 - mae: 18673.0879 - val_loss: 20824.3366 - val_mse: 444409152.0000 - val_mae: 20824.3379\n",
            "Epoch 149/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 18163.3926 - mse: 358786496.0000 - mae: 18163.3926 - val_loss: 20739.8327 - val_mse: 440894176.0000 - val_mae: 20739.8340\n",
            "Epoch 150/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 17867.0661 - mse: 350742816.0000 - mae: 17867.0664 - val_loss: 20652.3275 - val_mse: 437270336.0000 - val_mae: 20652.3262\n",
            "Epoch 151/500\n",
            "30/30 [==============================] - 0s 939us/step - loss: 18490.3327 - mse: 369006528.0000 - mae: 18490.3340 - val_loss: 20573.7282 - val_mse: 434028608.0000 - val_mae: 20573.7285\n",
            "Epoch 152/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 18242.6562 - mse: 362560704.0000 - mae: 18242.6562 - val_loss: 20491.2799 - val_mse: 430642304.0000 - val_mae: 20491.2793\n",
            "Epoch 153/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 18250.7868 - mse: 359072256.0000 - mae: 18250.7852 - val_loss: 20410.2663 - val_mse: 427327552.0000 - val_mae: 20410.2676\n",
            "Epoch 154/500\n",
            "30/30 [==============================] - 0s 993us/step - loss: 17816.0156 - mse: 349706592.0000 - mae: 17816.0176 - val_loss: 20324.1514 - val_mse: 423817408.0000 - val_mae: 20324.1504\n",
            "Epoch 155/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 18068.2904 - mse: 353603872.0000 - mae: 18068.2910 - val_loss: 20241.6390 - val_mse: 420468256.0000 - val_mae: 20241.6387\n",
            "Epoch 156/500\n",
            "30/30 [==============================] - 0s 971us/step - loss: 18445.1810 - mse: 376832896.0000 - mae: 18445.1816 - val_loss: 20164.3730 - val_mse: 417344480.0000 - val_mae: 20164.3730\n",
            "Epoch 157/500\n",
            "30/30 [==============================] - 0s 963us/step - loss: 17687.6436 - mse: 347923168.0000 - mae: 17687.6445 - val_loss: 20078.6048 - val_mse: 413890912.0000 - val_mae: 20078.6035\n",
            "Epoch 158/500\n",
            "30/30 [==============================] - 0s 964us/step - loss: 17904.2331 - mse: 350215104.0000 - mae: 17904.2324 - val_loss: 19995.9368 - val_mse: 410576032.0000 - val_mae: 19995.9375\n",
            "Epoch 159/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 17407.0944 - mse: 334532928.0000 - mae: 17407.0938 - val_loss: 19906.8678 - val_mse: 407020384.0000 - val_mae: 19906.8672\n",
            "Epoch 160/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 17780.8073 - mse: 343850048.0000 - mae: 17780.8086 - val_loss: 19823.5680 - val_mse: 403708640.0000 - val_mae: 19823.5684\n",
            "Epoch 161/500\n",
            "30/30 [==============================] - 0s 938us/step - loss: 16986.0098 - mse: 318563040.0000 - mae: 16986.0098 - val_loss: 19731.3506 - val_mse: 400059552.0000 - val_mae: 19731.3496\n",
            "Epoch 162/500\n",
            "30/30 [==============================] - 0s 976us/step - loss: 17266.7969 - mse: 328277664.0000 - mae: 17266.7969 - val_loss: 19641.8802 - val_mse: 396535360.0000 - val_mae: 19641.8789\n",
            "Epoch 163/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 17438.8874 - mse: 335843680.0000 - mae: 17438.8867 - val_loss: 19556.1699 - val_mse: 393174080.0000 - val_mae: 19556.1699\n",
            "Epoch 164/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 17087.7002 - mse: 319720352.0000 - mae: 17087.7012 - val_loss: 19466.0768 - val_mse: 389658048.0000 - val_mae: 19466.0762\n",
            "Epoch 165/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 17352.4473 - mse: 328807072.0000 - mae: 17352.4473 - val_loss: 19380.1048 - val_mse: 386316896.0000 - val_mae: 19380.1035\n",
            "Epoch 166/500\n",
            "30/30 [==============================] - 0s 980us/step - loss: 16732.3359 - mse: 309080384.0000 - mae: 16732.3359 - val_loss: 19287.1426 - val_mse: 382721856.0000 - val_mae: 19287.1426\n",
            "Epoch 167/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 16795.8307 - mse: 312369984.0000 - mae: 16795.8301 - val_loss: 19195.4297 - val_mse: 379191104.0000 - val_mae: 19195.4297\n",
            "Epoch 168/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 16615.4417 - mse: 303457152.0000 - mae: 16615.4414 - val_loss: 19101.9827 - val_mse: 375611872.0000 - val_mae: 19101.9824\n",
            "Epoch 169/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 16671.8639 - mse: 308671904.0000 - mae: 16671.8652 - val_loss: 19010.4740 - val_mse: 372123296.0000 - val_mae: 19010.4746\n",
            "Epoch 170/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 16812.9417 - mse: 311185760.0000 - mae: 16812.9414 - val_loss: 18920.6120 - val_mse: 368713440.0000 - val_mae: 18920.6113\n",
            "Epoch 171/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 16165.5098 - mse: 289352832.0000 - mae: 16165.5107 - val_loss: 18824.0729 - val_mse: 365069120.0000 - val_mae: 18824.0723\n",
            "Epoch 172/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 16246.7432 - mse: 294203200.0000 - mae: 16246.7441 - val_loss: 18729.0869 - val_mse: 361501600.0000 - val_mae: 18729.0879\n",
            "Epoch 173/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 16157.1406 - mse: 289848800.0000 - mae: 16157.1406 - val_loss: 18633.8187 - val_mse: 357941120.0000 - val_mae: 18633.8164\n",
            "Epoch 174/500\n",
            "30/30 [==============================] - 0s 983us/step - loss: 16236.7529 - mse: 299232928.0000 - mae: 16236.7529 - val_loss: 18550.0983 - val_mse: 354827168.0000 - val_mae: 18550.0977\n",
            "Epoch 175/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 16134.7643 - mse: 294548992.0000 - mae: 16134.7646 - val_loss: 18454.8477 - val_mse: 351301824.0000 - val_mae: 18454.8477\n",
            "Epoch 176/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 16460.0749 - mse: 304321824.0000 - mae: 16460.0742 - val_loss: 18363.6966 - val_mse: 347945984.0000 - val_mae: 18363.6973\n",
            "Epoch 177/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 15362.9850 - mse: 269268064.0000 - mae: 15362.9854 - val_loss: 18261.2458 - val_mse: 344193920.0000 - val_mae: 18261.2461\n",
            "Epoch 178/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 16199.2194 - mse: 290447232.0000 - mae: 16199.2197 - val_loss: 18167.8848 - val_mse: 340793184.0000 - val_mae: 18167.8848\n",
            "Epoch 179/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 16007.7191 - mse: 282001568.0000 - mae: 16007.7188 - val_loss: 18073.6660 - val_mse: 337378176.0000 - val_mae: 18073.6660\n",
            "Epoch 180/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 15030.8271 - mse: 256212560.0000 - mae: 15030.8271 - val_loss: 17969.0020 - val_mse: 333605504.0000 - val_mae: 17969.0020\n",
            "Epoch 181/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 15190.6156 - mse: 268039568.0000 - mae: 15190.6152 - val_loss: 17866.9128 - val_mse: 329948032.0000 - val_mae: 17866.9121\n",
            "Epoch 182/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 14917.9255 - mse: 254364144.0000 - mae: 14917.9258 - val_loss: 17771.8984 - val_mse: 326562784.0000 - val_mae: 17771.8984\n",
            "Epoch 183/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 15493.1657 - mse: 270989984.0000 - mae: 15493.1660 - val_loss: 17683.7402 - val_mse: 323438304.0000 - val_mae: 17683.7402\n",
            "Epoch 184/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 15165.7096 - mse: 266102016.0000 - mae: 15165.7100 - val_loss: 17582.9486 - val_mse: 319883168.0000 - val_mae: 17582.9473\n",
            "Epoch 185/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 15455.9922 - mse: 274023552.0000 - mae: 15455.9912 - val_loss: 17485.6810 - val_mse: 316473152.0000 - val_mae: 17485.6816\n",
            "Epoch 186/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 15053.5267 - mse: 259171040.0000 - mae: 15053.5273 - val_loss: 17393.0413 - val_mse: 313242752.0000 - val_mae: 17393.0410\n",
            "Epoch 187/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 15368.9030 - mse: 267203744.0000 - mae: 15368.9023 - val_loss: 17296.3105 - val_mse: 309886016.0000 - val_mae: 17296.3105\n",
            "Epoch 188/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 15187.3118 - mse: 256527664.0000 - mae: 15187.3125 - val_loss: 17197.2148 - val_mse: 306468000.0000 - val_mae: 17197.2148\n",
            "Epoch 189/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 15232.4300 - mse: 261992784.0000 - mae: 15232.4287 - val_loss: 17116.0169 - val_mse: 303684320.0000 - val_mae: 17116.0176\n",
            "Epoch 190/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 15303.8763 - mse: 258232352.0000 - mae: 15303.8770 - val_loss: 17019.2809 - val_mse: 300381920.0000 - val_mae: 17019.2812\n",
            "Epoch 191/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 14661.9759 - mse: 244107840.0000 - mae: 14661.9756 - val_loss: 16924.1696 - val_mse: 297154240.0000 - val_mae: 16924.1699\n",
            "Epoch 192/500\n",
            "30/30 [==============================] - 0s 971us/step - loss: 13695.6344 - mse: 220142496.0000 - mae: 13695.6348 - val_loss: 16819.9834 - val_mse: 293638656.0000 - val_mae: 16819.9824\n",
            "Epoch 193/500\n",
            "30/30 [==============================] - 0s 972us/step - loss: 14183.4320 - mse: 235529632.0000 - mae: 14183.4326 - val_loss: 16713.8379 - val_mse: 290079520.0000 - val_mae: 16713.8379\n",
            "Epoch 194/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 14047.3623 - mse: 221757120.0000 - mae: 14047.3623 - val_loss: 16605.9320 - val_mse: 286485408.0000 - val_mae: 16605.9316\n",
            "Epoch 195/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 14243.4424 - mse: 231513568.0000 - mae: 14243.4424 - val_loss: 16509.5345 - val_mse: 283293920.0000 - val_mae: 16509.5352\n",
            "Epoch 196/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 13879.2699 - mse: 219583920.0000 - mae: 13879.2695 - val_loss: 16400.9876 - val_mse: 279722240.0000 - val_mae: 16400.9883\n",
            "Epoch 197/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 14994.4020 - mse: 263748432.0000 - mae: 14994.4023 - val_loss: 16312.3874 - val_mse: 276825280.0000 - val_mae: 16312.3887\n",
            "Epoch 198/500\n",
            "30/30 [==============================] - 0s 978us/step - loss: 13724.5814 - mse: 221591648.0000 - mae: 13724.5811 - val_loss: 16210.7360 - val_mse: 273520224.0000 - val_mae: 16210.7354\n",
            "Epoch 199/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 14102.1530 - mse: 222664256.0000 - mae: 14102.1523 - val_loss: 16114.1331 - val_mse: 270398112.0000 - val_mae: 16114.1338\n",
            "Epoch 200/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 13611.1152 - mse: 220820224.0000 - mae: 13611.1143 - val_loss: 16005.4668 - val_mse: 266907712.0000 - val_mae: 16005.4668\n",
            "Epoch 201/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 13683.3652 - mse: 217312624.0000 - mae: 13683.3643 - val_loss: 15914.0540 - val_mse: 263990592.0000 - val_mae: 15914.0537\n",
            "Epoch 202/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 13108.2741 - mse: 208668256.0000 - mae: 13108.2744 - val_loss: 15816.6729 - val_mse: 260901792.0000 - val_mae: 15816.6729\n",
            "Epoch 203/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 13127.9476 - mse: 200960352.0000 - mae: 13127.9482 - val_loss: 15731.5788 - val_mse: 258218320.0000 - val_mae: 15731.5791\n",
            "Epoch 204/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 13663.4388 - mse: 213705872.0000 - mae: 13663.4385 - val_loss: 15631.9590 - val_mse: 255094640.0000 - val_mae: 15631.9590\n",
            "Epoch 205/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 13041.8753 - mse: 207715088.0000 - mae: 13041.8750 - val_loss: 15535.1878 - val_mse: 252079664.0000 - val_mae: 15535.1875\n",
            "Epoch 206/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 12480.8493 - mse: 188894416.0000 - mae: 12480.8486 - val_loss: 15428.0202 - val_mse: 248761904.0000 - val_mae: 15428.0205\n",
            "Epoch 207/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 12708.3590 - mse: 193076080.0000 - mae: 12708.3594 - val_loss: 15320.7259 - val_mse: 245463920.0000 - val_mae: 15320.7246\n",
            "Epoch 208/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 12718.4665 - mse: 194073264.0000 - mae: 12718.4668 - val_loss: 15215.2035 - val_mse: 242241936.0000 - val_mae: 15215.2041\n",
            "Epoch 209/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 12821.1266 - mse: 186575552.0000 - mae: 12821.1270 - val_loss: 15101.7646 - val_mse: 238802880.0000 - val_mae: 15101.7646\n",
            "Epoch 210/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 12817.4655 - mse: 201148384.0000 - mae: 12817.4658 - val_loss: 14996.8503 - val_mse: 235646976.0000 - val_mae: 14996.8496\n",
            "Epoch 211/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 12259.7152 - mse: 180952560.0000 - mae: 12259.7148 - val_loss: 14896.5195 - val_mse: 232648960.0000 - val_mae: 14896.5186\n",
            "Epoch 212/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 13076.1185 - mse: 204741152.0000 - mae: 13076.1191 - val_loss: 14804.1794 - val_mse: 229907520.0000 - val_mae: 14804.1787\n",
            "Epoch 213/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11750.1523 - mse: 166718032.0000 - mae: 11750.1523 - val_loss: 14682.5231 - val_mse: 226320160.0000 - val_mae: 14682.5225\n",
            "Epoch 214/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 12303.9860 - mse: 177590320.0000 - mae: 12303.9854 - val_loss: 14583.4788 - val_mse: 223423120.0000 - val_mae: 14583.4795\n",
            "Epoch 215/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11629.5547 - mse: 160000432.0000 - mae: 11629.5537 - val_loss: 14477.1152 - val_mse: 220333584.0000 - val_mae: 14477.1152\n",
            "Epoch 216/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 12491.3353 - mse: 193388480.0000 - mae: 12491.3350 - val_loss: 14371.7617 - val_mse: 217295472.0000 - val_mae: 14371.7627\n",
            "Epoch 217/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11953.4277 - mse: 173607520.0000 - mae: 11953.4277 - val_loss: 14261.3610 - val_mse: 214136144.0000 - val_mae: 14261.3604\n",
            "Epoch 218/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 12482.6286 - mse: 184142512.0000 - mae: 12482.6289 - val_loss: 14156.7038 - val_mse: 211162720.0000 - val_mae: 14156.7041\n",
            "Epoch 219/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11996.0762 - mse: 172065216.0000 - mae: 11996.0771 - val_loss: 14048.3480 - val_mse: 208108384.0000 - val_mae: 14048.3486\n",
            "Epoch 220/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 12670.3818 - mse: 186071696.0000 - mae: 12670.3818 - val_loss: 13953.0957 - val_mse: 205442848.0000 - val_mae: 13953.0957\n",
            "Epoch 221/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11012.9902 - mse: 147002512.0000 - mae: 11012.9893 - val_loss: 13835.9069 - val_mse: 202187472.0000 - val_mae: 13835.9072\n",
            "Epoch 222/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11978.3050 - mse: 174377712.0000 - mae: 11978.3057 - val_loss: 13744.5804 - val_mse: 199670128.0000 - val_mae: 13744.5811\n",
            "Epoch 223/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 12591.0612 - mse: 188387776.0000 - mae: 12591.0615 - val_loss: 13641.6481 - val_mse: 196851952.0000 - val_mae: 13641.6475\n",
            "Epoch 224/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11129.6484 - mse: 146721824.0000 - mae: 11129.6475 - val_loss: 13543.0667 - val_mse: 194173984.0000 - val_mae: 13543.0664\n",
            "Epoch 225/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 12809.7559 - mse: 199391168.0000 - mae: 12809.7559 - val_loss: 13450.8532 - val_mse: 191686128.0000 - val_mae: 13450.8535\n",
            "Epoch 226/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11793.2640 - mse: 169816496.0000 - mae: 11793.2646 - val_loss: 13359.7816 - val_mse: 189246800.0000 - val_mae: 13359.7812\n",
            "Epoch 227/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11369.1315 - mse: 159790400.0000 - mae: 11369.1309 - val_loss: 13247.6507 - val_mse: 186263584.0000 - val_mae: 13247.6504\n",
            "Epoch 228/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10943.6169 - mse: 139218720.0000 - mae: 10943.6162 - val_loss: 13140.1921 - val_mse: 183428720.0000 - val_mae: 13140.1914\n",
            "Epoch 229/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10897.3092 - mse: 142464288.0000 - mae: 10897.3086 - val_loss: 13034.1608 - val_mse: 180653840.0000 - val_mae: 13034.1602\n",
            "Epoch 230/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10220.9691 - mse: 130539440.0000 - mae: 10220.9688 - val_loss: 12931.9082 - val_mse: 178000064.0000 - val_mae: 12931.9082\n",
            "Epoch 231/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11165.9346 - mse: 150962752.0000 - mae: 11165.9346 - val_loss: 12839.3402 - val_mse: 175615456.0000 - val_mae: 12839.3398\n",
            "Epoch 232/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10669.2132 - mse: 143044720.0000 - mae: 10669.2139 - val_loss: 12740.2637 - val_mse: 173081952.0000 - val_mae: 12740.2646\n",
            "Epoch 233/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10506.7412 - mse: 138644768.0000 - mae: 10506.7412 - val_loss: 12632.0430 - val_mse: 170337216.0000 - val_mae: 12632.0439\n",
            "Epoch 234/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10798.1755 - mse: 147448544.0000 - mae: 10798.1758 - val_loss: 12525.5641 - val_mse: 167658944.0000 - val_mae: 12525.5635\n",
            "Epoch 235/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10914.3867 - mse: 148134240.0000 - mae: 10914.3877 - val_loss: 12427.7534 - val_mse: 165219424.0000 - val_mae: 12427.7529\n",
            "Epoch 236/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10003.2510 - mse: 124723712.0000 - mae: 10003.2510 - val_loss: 12308.8861 - val_mse: 162280144.0000 - val_mae: 12308.8857\n",
            "Epoch 237/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10111.3626 - mse: 125579904.0000 - mae: 10111.3623 - val_loss: 12216.3571 - val_mse: 160012224.0000 - val_mae: 12216.3584\n",
            "Epoch 238/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10066.6696 - mse: 137790528.0000 - mae: 10066.6699 - val_loss: 12124.3926 - val_mse: 157775632.0000 - val_mae: 12124.3936\n",
            "Epoch 239/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9411.4036 - mse: 112713856.0000 - mae: 9411.4043 - val_loss: 12019.8706 - val_mse: 155252704.0000 - val_mae: 12019.8711\n",
            "Epoch 240/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10181.5576 - mse: 130428104.0000 - mae: 10181.5586 - val_loss: 11920.0260 - val_mse: 152863952.0000 - val_mae: 11920.0264\n",
            "Epoch 241/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9516.9924 - mse: 113715112.0000 - mae: 9516.9932 - val_loss: 11805.3898 - val_mse: 150145696.0000 - val_mae: 11805.3896\n",
            "Epoch 242/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9557.3714 - mse: 110984792.0000 - mae: 9557.3711 - val_loss: 11689.9030 - val_mse: 147433008.0000 - val_mae: 11689.9043\n",
            "Epoch 243/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10589.9359 - mse: 127649872.0000 - mae: 10589.9365 - val_loss: 11584.7041 - val_mse: 144985504.0000 - val_mae: 11584.7041\n",
            "Epoch 244/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8885.1416 - mse: 104974256.0000 - mae: 8885.1416 - val_loss: 11481.7425 - val_mse: 142611520.0000 - val_mae: 11481.7432\n",
            "Epoch 245/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9470.5641 - mse: 123016776.0000 - mae: 9470.5645 - val_loss: 11366.6839 - val_mse: 139984352.0000 - val_mae: 11366.6846\n",
            "Epoch 246/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10388.7651 - mse: 130523816.0000 - mae: 10388.7656 - val_loss: 11270.7507 - val_mse: 137813536.0000 - val_mae: 11270.7500\n",
            "Epoch 247/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9299.3971 - mse: 108925936.0000 - mae: 9299.3975 - val_loss: 11164.5444 - val_mse: 135431136.0000 - val_mae: 11164.5449\n",
            "Epoch 248/500\n",
            "30/30 [==============================] - 0s 957us/step - loss: 8903.9840 - mse: 104859560.0000 - mae: 8903.9844 - val_loss: 11078.9832 - val_mse: 133526736.0000 - val_mae: 11078.9834\n",
            "Epoch 249/500\n",
            "30/30 [==============================] - 0s 960us/step - loss: 10670.4648 - mse: 137612464.0000 - mae: 10670.4648 - val_loss: 10966.9440 - val_mse: 131056352.0000 - val_mae: 10966.9434\n",
            "Epoch 250/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9348.3158 - mse: 108810392.0000 - mae: 9348.3154 - val_loss: 10851.1711 - val_mse: 128530928.0000 - val_mae: 10851.1709\n",
            "Epoch 251/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9597.2497 - mse: 119270488.0000 - mae: 9597.2500 - val_loss: 10746.5272 - val_mse: 126271192.0000 - val_mae: 10746.5273\n",
            "Epoch 252/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9215.3164 - mse: 102123480.0000 - mae: 9215.3164 - val_loss: 10631.7762 - val_mse: 123818512.0000 - val_mae: 10631.7773\n",
            "Epoch 253/500\n",
            "30/30 [==============================] - 0s 983us/step - loss: 9286.2139 - mse: 109570096.0000 - mae: 9286.2148 - val_loss: 10536.6405 - val_mse: 121804888.0000 - val_mae: 10536.6406\n",
            "Epoch 254/500\n",
            "30/30 [==============================] - 0s 960us/step - loss: 9711.0632 - mse: 121849400.0000 - mae: 9711.0625 - val_loss: 10455.9805 - val_mse: 120113080.0000 - val_mae: 10455.9814\n",
            "Epoch 255/500\n",
            "30/30 [==============================] - 0s 942us/step - loss: 9033.4523 - mse: 105683936.0000 - mae: 9033.4521 - val_loss: 10353.4583 - val_mse: 117980552.0000 - val_mae: 10353.4580\n",
            "Epoch 256/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9739.4072 - mse: 123261984.0000 - mae: 9739.4072 - val_loss: 10270.0807 - val_mse: 116262344.0000 - val_mae: 10270.0801\n",
            "Epoch 257/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10094.7376 - mse: 129898120.0000 - mae: 10094.7373 - val_loss: 10173.8532 - val_mse: 114295264.0000 - val_mae: 10173.8535\n",
            "Epoch 258/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8655.2484 - mse: 101581440.0000 - mae: 8655.2490 - val_loss: 10068.5549 - val_mse: 112164504.0000 - val_mae: 10068.5557\n",
            "Epoch 259/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9501.4095 - mse: 110459952.0000 - mae: 9501.4092 - val_loss: 9967.0785 - val_mse: 110131240.0000 - val_mae: 9967.0791\n",
            "Epoch 260/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10004.8128 - mse: 117666832.0000 - mae: 10004.8125 - val_loss: 9876.9749 - val_mse: 108343944.0000 - val_mae: 9876.9746\n",
            "Epoch 261/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9365.4644 - mse: 111578472.0000 - mae: 9365.4648 - val_loss: 9772.8944 - val_mse: 106299152.0000 - val_mae: 9772.8945\n",
            "Epoch 262/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8174.1753 - mse: 98651600.0000 - mae: 8174.1748 - val_loss: 9653.5120 - val_mse: 103980048.0000 - val_mae: 9653.5127\n",
            "Epoch 263/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8097.0456 - mse: 85316952.0000 - mae: 8097.0459 - val_loss: 9548.5373 - val_mse: 101965456.0000 - val_mae: 9548.5371\n",
            "Epoch 264/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7847.0467 - mse: 84975960.0000 - mae: 7847.0469 - val_loss: 9444.7679 - val_mse: 99995168.0000 - val_mae: 9444.7676\n",
            "Epoch 265/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8947.0973 - mse: 105434392.0000 - mae: 8947.0967 - val_loss: 9361.3049 - val_mse: 98426008.0000 - val_mae: 9361.3037\n",
            "Epoch 266/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7365.5654 - mse: 81051992.0000 - mae: 7365.5654 - val_loss: 9263.5441 - val_mse: 96605216.0000 - val_mae: 9263.5439\n",
            "Epoch 267/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8155.2871 - mse: 88473136.0000 - mae: 8155.2876 - val_loss: 9183.2054 - val_mse: 95122024.0000 - val_mae: 9183.2051\n",
            "Epoch 268/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8751.8337 - mse: 97598264.0000 - mae: 8751.8330 - val_loss: 9086.2152 - val_mse: 93350568.0000 - val_mae: 9086.2148\n",
            "Epoch 269/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8336.0592 - mse: 88203496.0000 - mae: 8336.0596 - val_loss: 8983.1672 - val_mse: 91488800.0000 - val_mae: 8983.1670\n",
            "Epoch 270/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7413.6361 - mse: 70029976.0000 - mae: 7413.6357 - val_loss: 8920.1686 - val_mse: 90359928.0000 - val_mae: 8920.1689\n",
            "Epoch 271/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7716.3846 - mse: 86082104.0000 - mae: 7716.3843 - val_loss: 8837.6540 - val_mse: 88895696.0000 - val_mae: 8837.6543\n",
            "Epoch 272/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7370.2443 - mse: 83651528.0000 - mae: 7370.2441 - val_loss: 8761.5570 - val_mse: 87556904.0000 - val_mae: 8761.5566\n",
            "Epoch 273/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6475.1226 - mse: 56624948.0000 - mae: 6475.1230 - val_loss: 8672.4010 - val_mse: 86003400.0000 - val_mae: 8672.4014\n",
            "Epoch 274/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7545.2967 - mse: 76023688.0000 - mae: 7545.2969 - val_loss: 8577.7791 - val_mse: 84371600.0000 - val_mae: 8577.7793\n",
            "Epoch 275/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8616.7220 - mse: 91043728.0000 - mae: 8616.7217 - val_loss: 8515.7979 - val_mse: 83313712.0000 - val_mae: 8515.7979\n",
            "Epoch 276/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7378.3698 - mse: 83015440.0000 - mae: 7378.3696 - val_loss: 8443.2643 - val_mse: 82083688.0000 - val_mae: 8443.2646\n",
            "Epoch 277/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7800.7669 - mse: 85083800.0000 - mae: 7800.7666 - val_loss: 8365.2814 - val_mse: 80771168.0000 - val_mae: 8365.2812\n",
            "Epoch 278/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8129.8605 - mse: 95134136.0000 - mae: 8129.8604 - val_loss: 8288.6899 - val_mse: 79494472.0000 - val_mae: 8288.6904\n",
            "Epoch 279/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7531.9966 - mse: 73732976.0000 - mae: 7531.9966 - val_loss: 8241.3948 - val_mse: 78710696.0000 - val_mae: 8241.3945\n",
            "Epoch 280/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7728.1706 - mse: 92780328.0000 - mae: 7728.1709 - val_loss: 8168.6085 - val_mse: 77515352.0000 - val_mae: 8168.6084\n",
            "Epoch 281/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6824.5024 - mse: 64169572.0000 - mae: 6824.5020 - val_loss: 8065.3022 - val_mse: 75839464.0000 - val_mae: 8065.3022\n",
            "Epoch 282/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7283.3271 - mse: 74133744.0000 - mae: 7283.3271 - val_loss: 7994.2713 - val_mse: 74697688.0000 - val_mae: 7994.2715\n",
            "Epoch 283/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7459.5073 - mse: 86635088.0000 - mae: 7459.5073 - val_loss: 7891.5794 - val_mse: 73065800.0000 - val_mae: 7891.5801\n",
            "Epoch 284/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6593.0764 - mse: 61597072.0000 - mae: 6593.0762 - val_loss: 7809.4891 - val_mse: 71778288.0000 - val_mae: 7809.4897\n",
            "Epoch 285/500\n",
            "30/30 [==============================] - 0s 994us/step - loss: 7885.0744 - mse: 83800744.0000 - mae: 7885.0737 - val_loss: 7731.6461 - val_mse: 70567816.0000 - val_mae: 7731.6460\n",
            "Epoch 286/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7358.8605 - mse: 71470000.0000 - mae: 7358.8604 - val_loss: 7672.8651 - val_mse: 69664072.0000 - val_mae: 7672.8652\n",
            "Epoch 287/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7888.5340 - mse: 80585632.0000 - mae: 7888.5337 - val_loss: 7607.7345 - val_mse: 68666328.0000 - val_mae: 7607.7344\n",
            "Epoch 288/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7931.1341 - mse: 79559560.0000 - mae: 7931.1343 - val_loss: 7520.3809 - val_mse: 67345440.0000 - val_mae: 7520.3809\n",
            "Epoch 289/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6872.5365 - mse: 64342128.0000 - mae: 6872.5366 - val_loss: 7431.9399 - val_mse: 66023780.0000 - val_mae: 7431.9399\n",
            "Epoch 290/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7505.6820 - mse: 82996344.0000 - mae: 7505.6816 - val_loss: 7350.4670 - val_mse: 64819776.0000 - val_mae: 7350.4668\n",
            "Epoch 291/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7458.8797 - mse: 77966944.0000 - mae: 7458.8799 - val_loss: 7262.2483 - val_mse: 63530252.0000 - val_mae: 7262.2485\n",
            "Epoch 292/500\n",
            "30/30 [==============================] - 0s 972us/step - loss: 7153.5215 - mse: 70709352.0000 - mae: 7153.5220 - val_loss: 7190.6852 - val_mse: 62493980.0000 - val_mae: 7190.6855\n",
            "Epoch 293/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8065.1963 - mse: 85412944.0000 - mae: 8065.1963 - val_loss: 7101.1606 - val_mse: 61213848.0000 - val_mae: 7101.1606\n",
            "Epoch 294/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6886.8991 - mse: 60790348.0000 - mae: 6886.8989 - val_loss: 6991.4060 - val_mse: 59667704.0000 - val_mae: 6991.4062\n",
            "Epoch 295/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7000.3525 - mse: 63162804.0000 - mae: 7000.3525 - val_loss: 6916.8854 - val_mse: 58632392.0000 - val_mae: 6916.8853\n",
            "Epoch 296/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 7613.6048 - mse: 81938032.0000 - mae: 7613.6050 - val_loss: 6883.8378 - val_mse: 58175260.0000 - val_mae: 6883.8379\n",
            "Epoch 297/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7843.1064 - mse: 77008304.0000 - mae: 7843.1064 - val_loss: 6828.8817 - val_mse: 57422980.0000 - val_mae: 6828.8818\n",
            "Epoch 298/500\n",
            "30/30 [==============================] - 0s 965us/step - loss: 7005.7284 - mse: 65686612.0000 - mae: 7005.7280 - val_loss: 6734.9657 - val_mse: 56149492.0000 - val_mae: 6734.9658\n",
            "Epoch 299/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7606.3893 - mse: 77105640.0000 - mae: 7606.3896 - val_loss: 6686.5609 - val_mse: 55499624.0000 - val_mae: 6686.5615\n",
            "Epoch 300/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6405.7985 - mse: 56981392.0000 - mae: 6405.7983 - val_loss: 6611.2113 - val_mse: 54497196.0000 - val_mae: 6611.2109\n",
            "Epoch 301/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7161.6937 - mse: 71256440.0000 - mae: 7161.6938 - val_loss: 6513.0589 - val_mse: 53209712.0000 - val_mae: 6513.0591\n",
            "Epoch 302/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 7930.5251 - mse: 86610552.0000 - mae: 7930.5249 - val_loss: 6448.0583 - val_mse: 52367528.0000 - val_mae: 6448.0581\n",
            "Epoch 303/500\n",
            "30/30 [==============================] - 0s 990us/step - loss: 6187.3459 - mse: 50756608.0000 - mae: 6187.3457 - val_loss: 6392.0196 - val_mse: 51647620.0000 - val_mae: 6392.0190\n",
            "Epoch 304/500\n",
            "30/30 [==============================] - 0s 994us/step - loss: 8281.9725 - mse: 101437624.0000 - mae: 8281.9727 - val_loss: 6341.8679 - val_mse: 51008164.0000 - val_mae: 6341.8677\n",
            "Epoch 305/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6113.7218 - mse: 56146492.0000 - mae: 6113.7217 - val_loss: 6245.2436 - val_mse: 49792436.0000 - val_mae: 6245.2437\n",
            "Epoch 306/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 6636.4473 - mse: 54013204.0000 - mae: 6636.4473 - val_loss: 6161.0595 - val_mse: 48747268.0000 - val_mae: 6161.0596\n",
            "Epoch 307/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7419.9914 - mse: 82755496.0000 - mae: 7419.9917 - val_loss: 6127.9349 - val_mse: 48337456.0000 - val_mae: 6127.9351\n",
            "Epoch 308/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8076.0809 - mse: 86497896.0000 - mae: 8076.0811 - val_loss: 6104.3962 - val_mse: 48049380.0000 - val_mae: 6104.3965\n",
            "Epoch 309/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6346.4632 - mse: 56131592.0000 - mae: 6346.4634 - val_loss: 6076.0953 - val_mse: 47703988.0000 - val_mae: 6076.0947\n",
            "Epoch 310/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6958.4539 - mse: 75320696.0000 - mae: 6958.4541 - val_loss: 5991.5690 - val_mse: 46683632.0000 - val_mae: 5991.5688\n",
            "Epoch 311/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6731.1382 - mse: 61511660.0000 - mae: 6731.1387 - val_loss: 5969.9548 - val_mse: 46424556.0000 - val_mae: 5969.9546\n",
            "Epoch 312/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6837.4766 - mse: 68280128.0000 - mae: 6837.4766 - val_loss: 5883.8881 - val_mse: 45404816.0000 - val_mae: 5883.8882\n",
            "Epoch 313/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6335.1649 - mse: 69172368.0000 - mae: 6335.1646 - val_loss: 5844.8197 - val_mse: 44947220.0000 - val_mae: 5844.8198\n",
            "Epoch 314/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7213.0213 - mse: 66994604.0000 - mae: 7213.0215 - val_loss: 5751.4535 - val_mse: 43865024.0000 - val_mae: 5751.4531\n",
            "Epoch 315/500\n",
            "30/30 [==============================] - 0s 956us/step - loss: 6203.9873 - mse: 55914768.0000 - mae: 6203.9873 - val_loss: 5650.2743 - val_mse: 42710456.0000 - val_mae: 5650.2739\n",
            "Epoch 316/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6991.8564 - mse: 68782408.0000 - mae: 6991.8564 - val_loss: 5603.9060 - val_mse: 42189340.0000 - val_mae: 5603.9062\n",
            "Epoch 317/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6498.8849 - mse: 67200296.0000 - mae: 6498.8853 - val_loss: 5508.1023 - val_mse: 41122808.0000 - val_mae: 5508.1021\n",
            "Epoch 318/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6848.5516 - mse: 71395864.0000 - mae: 6848.5518 - val_loss: 5450.5678 - val_mse: 40493804.0000 - val_mae: 5450.5679\n",
            "Epoch 319/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6216.8231 - mse: 54692992.0000 - mae: 6216.8228 - val_loss: 5402.7718 - val_mse: 39976740.0000 - val_mae: 5402.7720\n",
            "Epoch 320/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6481.2264 - mse: 58641036.0000 - mae: 6481.2261 - val_loss: 5328.2968 - val_mse: 39177692.0000 - val_mae: 5328.2969\n",
            "Epoch 321/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5620.1785 - mse: 44958484.0000 - mae: 5620.1787 - val_loss: 5249.7877 - val_mse: 38348140.0000 - val_mae: 5249.7881\n",
            "Epoch 322/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6391.5369 - mse: 65361560.0000 - mae: 6391.5366 - val_loss: 5221.6131 - val_mse: 38050800.0000 - val_mae: 5221.6128\n",
            "Epoch 323/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6450.2414 - mse: 73042608.0000 - mae: 6450.2412 - val_loss: 5186.0157 - val_mse: 37680788.0000 - val_mae: 5186.0161\n",
            "Epoch 324/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5171.2912 - mse: 42604928.0000 - mae: 5171.2910 - val_loss: 5125.5403 - val_mse: 37054504.0000 - val_mae: 5125.5400\n",
            "Epoch 325/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5279.5479 - mse: 48340608.0000 - mae: 5279.5479 - val_loss: 5096.5678 - val_mse: 36759760.0000 - val_mae: 5096.5679\n",
            "Epoch 326/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6744.9754 - mse: 62115984.0000 - mae: 6744.9756 - val_loss: 5070.8115 - val_mse: 36497080.0000 - val_mae: 5070.8120\n",
            "Epoch 327/500\n",
            "30/30 [==============================] - 0s 960us/step - loss: 6672.4463 - mse: 71838136.0000 - mae: 6672.4463 - val_loss: 5049.9385 - val_mse: 36285708.0000 - val_mae: 5049.9385\n",
            "Epoch 328/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6617.0928 - mse: 63432408.0000 - mae: 6617.0928 - val_loss: 4999.7972 - val_mse: 35783220.0000 - val_mae: 4999.7969\n",
            "Epoch 329/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6421.4818 - mse: 61432980.0000 - mae: 6421.4824 - val_loss: 4962.3261 - val_mse: 35396980.0000 - val_mae: 4962.3262\n",
            "Epoch 330/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 6740.3470 - mse: 69313080.0000 - mae: 6740.3472 - val_loss: 4940.3081 - val_mse: 35163236.0000 - val_mae: 4940.3076\n",
            "Epoch 331/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6558.4904 - mse: 56781996.0000 - mae: 6558.4907 - val_loss: 4933.2806 - val_mse: 35087564.0000 - val_mae: 4933.2808\n",
            "Epoch 332/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5764.9142 - mse: 56482568.0000 - mae: 5764.9146 - val_loss: 4929.2260 - val_mse: 35044204.0000 - val_mae: 4929.2261\n",
            "Epoch 333/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6553.9779 - mse: 60314112.0000 - mae: 6553.9780 - val_loss: 4872.9022 - val_mse: 34413040.0000 - val_mae: 4872.9019\n",
            "Epoch 334/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6814.3361 - mse: 66131252.0000 - mae: 6814.3364 - val_loss: 4839.5150 - val_mse: 34041156.0000 - val_mae: 4839.5151\n",
            "Epoch 335/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6524.7072 - mse: 57130032.0000 - mae: 6524.7065 - val_loss: 4794.5767 - val_mse: 33540696.0000 - val_mae: 4794.5767\n",
            "Epoch 336/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 6160.6720 - mse: 54577748.0000 - mae: 6160.6719 - val_loss: 4791.7455 - val_mse: 33507026.0000 - val_mae: 4791.7451\n",
            "Epoch 337/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6063.6063 - mse: 56241188.0000 - mae: 6063.6064 - val_loss: 4785.8969 - val_mse: 33442380.0000 - val_mae: 4785.8970\n",
            "Epoch 338/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5542.8985 - mse: 53314008.0000 - mae: 5542.8984 - val_loss: 4808.4657 - val_mse: 33689360.0000 - val_mae: 4808.4658\n",
            "Epoch 339/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6425.5965 - mse: 63981040.0000 - mae: 6425.5957 - val_loss: 4766.0471 - val_mse: 33224074.0000 - val_mae: 4766.0469\n",
            "Epoch 340/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8359.3294 - mse: 110828616.0000 - mae: 8359.3301 - val_loss: 4731.1634 - val_mse: 32845950.0000 - val_mae: 4731.1636\n",
            "Epoch 341/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7077.7282 - mse: 74708088.0000 - mae: 7077.7280 - val_loss: 4713.8312 - val_mse: 32654642.0000 - val_mae: 4713.8311\n",
            "Epoch 342/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7084.0622 - mse: 68762736.0000 - mae: 7084.0620 - val_loss: 4699.6938 - val_mse: 32489848.0000 - val_mae: 4699.6943\n",
            "Epoch 343/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6823.7710 - mse: 58394148.0000 - mae: 6823.7710 - val_loss: 4657.4208 - val_mse: 32000226.0000 - val_mae: 4657.4204\n",
            "Epoch 344/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6604.1369 - mse: 65171484.0000 - mae: 6604.1372 - val_loss: 4616.0477 - val_mse: 31526130.0000 - val_mae: 4616.0474\n",
            "Epoch 345/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7977.3545 - mse: 86793816.0000 - mae: 7977.3550 - val_loss: 4614.7828 - val_mse: 31511292.0000 - val_mae: 4614.7827\n",
            "Epoch 346/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6002.5386 - mse: 52414528.0000 - mae: 6002.5386 - val_loss: 4579.2691 - val_mse: 31073708.0000 - val_mae: 4579.2686\n",
            "Epoch 347/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6110.1742 - mse: 58333160.0000 - mae: 6110.1738 - val_loss: 4598.8729 - val_mse: 31315314.0000 - val_mae: 4598.8730\n",
            "Epoch 348/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 5188.7134 - mse: 45438004.0000 - mae: 5188.7134 - val_loss: 4560.0576 - val_mse: 30840122.0000 - val_mae: 4560.0571\n",
            "Epoch 349/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6793.6610 - mse: 70836352.0000 - mae: 6793.6606 - val_loss: 4526.9583 - val_mse: 30439204.0000 - val_mae: 4526.9585\n",
            "Epoch 350/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5838.0137 - mse: 50186768.0000 - mae: 5838.0137 - val_loss: 4491.6381 - val_mse: 29973004.0000 - val_mae: 4491.6382\n",
            "Epoch 351/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6105.0158 - mse: 54761344.0000 - mae: 6105.0156 - val_loss: 4479.5380 - val_mse: 29815172.0000 - val_mae: 4479.5381\n",
            "Epoch 352/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7174.3815 - mse: 71635448.0000 - mae: 7174.3813 - val_loss: 4481.8852 - val_mse: 29846048.0000 - val_mae: 4481.8848\n",
            "Epoch 353/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5609.0462 - mse: 42202300.0000 - mae: 5609.0464 - val_loss: 4491.2563 - val_mse: 29968690.0000 - val_mae: 4491.2563\n",
            "Epoch 354/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5449.8254 - mse: 50366440.0000 - mae: 5449.8257 - val_loss: 4515.5790 - val_mse: 30291438.0000 - val_mae: 4515.5791\n",
            "Epoch 355/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6705.1603 - mse: 72217392.0000 - mae: 6705.1606 - val_loss: 4532.5893 - val_mse: 30511706.0000 - val_mae: 4532.5889\n",
            "Epoch 356/500\n",
            "30/30 [==============================] - 0s 933us/step - loss: 6653.9041 - mse: 62870600.0000 - mae: 6653.9043 - val_loss: 4501.1853 - val_mse: 30103214.0000 - val_mae: 4501.1855\n",
            "Epoch 357/500\n",
            "30/30 [==============================] - 0s 953us/step - loss: 6311.4476 - mse: 57789956.0000 - mae: 6311.4478 - val_loss: 4460.9661 - val_mse: 29575346.0000 - val_mae: 4460.9663\n",
            "Epoch 358/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6639.0967 - mse: 59596244.0000 - mae: 6639.0967 - val_loss: 4429.0215 - val_mse: 29138172.0000 - val_mae: 4429.0215\n",
            "Epoch 359/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6231.6698 - mse: 56945756.0000 - mae: 6231.6699 - val_loss: 4412.8041 - val_mse: 28907150.0000 - val_mae: 4412.8042\n",
            "Epoch 360/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5825.8174 - mse: 54225960.0000 - mae: 5825.8179 - val_loss: 4409.5872 - val_mse: 28861778.0000 - val_mae: 4409.5874\n",
            "Epoch 361/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6514.3153 - mse: 61370972.0000 - mae: 6514.3154 - val_loss: 4364.4258 - val_mse: 28225514.0000 - val_mae: 4364.4258\n",
            "Epoch 362/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6240.7230 - mse: 61532032.0000 - mae: 6240.7231 - val_loss: 4338.8994 - val_mse: 27873252.0000 - val_mae: 4338.8994\n",
            "Epoch 363/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5580.5957 - mse: 46653644.0000 - mae: 5580.5957 - val_loss: 4341.2172 - val_mse: 27906934.0000 - val_mae: 4341.2173\n",
            "Epoch 364/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6147.3477 - mse: 58447708.0000 - mae: 6147.3481 - val_loss: 4344.3878 - val_mse: 27949846.0000 - val_mae: 4344.3877\n",
            "Epoch 365/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6172.4987 - mse: 58466004.0000 - mae: 6172.4990 - val_loss: 4358.9563 - val_mse: 28151674.0000 - val_mae: 4358.9561\n",
            "Epoch 366/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7000.2030 - mse: 75232152.0000 - mae: 7000.2031 - val_loss: 4351.3107 - val_mse: 28048806.0000 - val_mae: 4351.3105\n",
            "Epoch 367/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5413.2023 - mse: 39955516.0000 - mae: 5413.2021 - val_loss: 4330.3137 - val_mse: 27757684.0000 - val_mae: 4330.3140\n",
            "Epoch 368/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5365.0978 - mse: 43920188.0000 - mae: 5365.0981 - val_loss: 4335.1027 - val_mse: 27825012.0000 - val_mae: 4335.1025\n",
            "Epoch 369/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6322.1159 - mse: 54305588.0000 - mae: 6322.1157 - val_loss: 4327.5465 - val_mse: 27722712.0000 - val_mae: 4327.5464\n",
            "Epoch 370/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6173.8322 - mse: 53665964.0000 - mae: 6173.8325 - val_loss: 4350.1199 - val_mse: 28034062.0000 - val_mae: 4350.1201\n",
            "Epoch 371/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5617.8234 - mse: 51466028.0000 - mae: 5617.8237 - val_loss: 4318.6301 - val_mse: 27605368.0000 - val_mae: 4318.6304\n",
            "Epoch 372/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5715.9326 - mse: 45002916.0000 - mae: 5715.9321 - val_loss: 4287.0865 - val_mse: 27178196.0000 - val_mae: 4287.0864\n",
            "Epoch 373/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6451.8748 - mse: 60745208.0000 - mae: 6451.8750 - val_loss: 4273.4629 - val_mse: 26994302.0000 - val_mae: 4273.4629\n",
            "Epoch 374/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7008.6812 - mse: 70979352.0000 - mae: 7008.6812 - val_loss: 4290.9663 - val_mse: 27233560.0000 - val_mae: 4290.9663\n",
            "Epoch 375/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7422.5965 - mse: 90224872.0000 - mae: 7422.5967 - val_loss: 4290.2406 - val_mse: 27223702.0000 - val_mae: 4290.2407\n",
            "Epoch 376/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5875.3965 - mse: 54401856.0000 - mae: 5875.3965 - val_loss: 4288.4947 - val_mse: 27199842.0000 - val_mae: 4288.4946\n",
            "Epoch 377/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7799.9608 - mse: 80798304.0000 - mae: 7799.9614 - val_loss: 4276.6330 - val_mse: 27037672.0000 - val_mae: 4276.6333\n",
            "Epoch 378/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 6137.0942 - mse: 50097476.0000 - mae: 6137.0942 - val_loss: 4239.4299 - val_mse: 26539366.0000 - val_mae: 4239.4297\n",
            "Epoch 379/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5669.2163 - mse: 46314012.0000 - mae: 5669.2163 - val_loss: 4231.5451 - val_mse: 26435382.0000 - val_mae: 4231.5449\n",
            "Epoch 380/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 5689.4476 - mse: 50614288.0000 - mae: 5689.4473 - val_loss: 4222.6166 - val_mse: 26315592.0000 - val_mae: 4222.6167\n",
            "Epoch 381/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6324.5369 - mse: 60309568.0000 - mae: 6324.5371 - val_loss: 4213.3471 - val_mse: 26184116.0000 - val_mae: 4213.3472\n",
            "Epoch 382/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5565.3742 - mse: 45270512.0000 - mae: 5565.3740 - val_loss: 4193.3848 - val_mse: 25853758.0000 - val_mae: 4193.3848\n",
            "Epoch 383/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6921.5544 - mse: 63455100.0000 - mae: 6921.5542 - val_loss: 4194.7746 - val_mse: 25877934.0000 - val_mae: 4194.7744\n",
            "Epoch 384/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6856.2637 - mse: 63132444.0000 - mae: 6856.2637 - val_loss: 4178.1266 - val_mse: 25600220.0000 - val_mae: 4178.1265\n",
            "Epoch 385/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4659.0072 - mse: 42209808.0000 - mae: 4659.0073 - val_loss: 4167.3271 - val_mse: 25423606.0000 - val_mae: 4167.3271\n",
            "Epoch 386/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6568.4535 - mse: 63600292.0000 - mae: 6568.4531 - val_loss: 4139.6093 - val_mse: 24970220.0000 - val_mae: 4139.6094\n",
            "Epoch 387/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6214.6885 - mse: 51222500.0000 - mae: 6214.6885 - val_loss: 4118.4976 - val_mse: 24630164.0000 - val_mae: 4118.4976\n",
            "Epoch 388/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6042.6172 - mse: 70877128.0000 - mae: 6042.6172 - val_loss: 4109.6576 - val_mse: 24491916.0000 - val_mae: 4109.6577\n",
            "Epoch 389/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5947.3200 - mse: 57029156.0000 - mae: 5947.3203 - val_loss: 4086.1927 - val_mse: 24060930.0000 - val_mae: 4086.1926\n",
            "Epoch 390/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4650.1154 - mse: 38294044.0000 - mae: 4650.1157 - val_loss: 4073.2046 - val_mse: 23777338.0000 - val_mae: 4073.2046\n",
            "Epoch 391/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5513.0112 - mse: 42770268.0000 - mae: 5513.0112 - val_loss: 4063.8542 - val_mse: 23578332.0000 - val_mae: 4063.8542\n",
            "Epoch 392/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6031.6554 - mse: 60743228.0000 - mae: 6031.6553 - val_loss: 4054.8372 - val_mse: 23385038.0000 - val_mae: 4054.8369\n",
            "Epoch 393/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 6944.0885 - mse: 70687480.0000 - mae: 6944.0884 - val_loss: 4050.2587 - val_mse: 23286364.0000 - val_mae: 4050.2585\n",
            "Epoch 394/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6295.5355 - mse: 60756148.0000 - mae: 6295.5356 - val_loss: 4051.5069 - val_mse: 23315896.0000 - val_mae: 4051.5068\n",
            "Epoch 395/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5125.6035 - mse: 44313012.0000 - mae: 5125.6035 - val_loss: 4041.4330 - val_mse: 23101340.0000 - val_mae: 4041.4331\n",
            "Epoch 396/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6049.0085 - mse: 62160904.0000 - mae: 6049.0083 - val_loss: 4049.5297 - val_mse: 23279614.0000 - val_mae: 4049.5298\n",
            "Epoch 397/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6469.6410 - mse: 60508228.0000 - mae: 6469.6406 - val_loss: 4051.4797 - val_mse: 23321350.0000 - val_mae: 4051.4795\n",
            "Epoch 398/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5062.1804 - mse: 41820888.0000 - mae: 5062.1802 - val_loss: 4074.4467 - val_mse: 23817772.0000 - val_mae: 4074.4468\n",
            "Epoch 399/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5103.1506 - mse: 45408000.0000 - mae: 5103.1504 - val_loss: 4085.6845 - val_mse: 24063044.0000 - val_mae: 4085.6843\n",
            "Epoch 400/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6620.1574 - mse: 63655356.0000 - mae: 6620.1572 - val_loss: 4078.4060 - val_mse: 23902994.0000 - val_mae: 4078.4058\n",
            "Epoch 401/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6078.6525 - mse: 65038524.0000 - mae: 6078.6519 - val_loss: 4076.0476 - val_mse: 23858968.0000 - val_mae: 4076.0476\n",
            "Epoch 402/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6846.5688 - mse: 69652808.0000 - mae: 6846.5688 - val_loss: 4080.8162 - val_mse: 23961640.0000 - val_mae: 4080.8162\n",
            "Epoch 403/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6000.1413 - mse: 59430168.0000 - mae: 6000.1416 - val_loss: 4101.9001 - val_mse: 24376384.0000 - val_mae: 4101.8999\n",
            "Epoch 404/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5854.1947 - mse: 53085568.0000 - mae: 5854.1943 - val_loss: 4092.2203 - val_mse: 24199872.0000 - val_mae: 4092.2202\n",
            "Epoch 405/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7603.6418 - mse: 80418888.0000 - mae: 7603.6416 - val_loss: 4091.4997 - val_mse: 24185476.0000 - val_mae: 4091.4998\n",
            "Epoch 406/500\n",
            "30/30 [==============================] - 0s 937us/step - loss: 6528.1842 - mse: 64765552.0000 - mae: 6528.1846 - val_loss: 4069.9889 - val_mse: 23724280.0000 - val_mae: 4069.9888\n",
            "Epoch 407/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5865.6110 - mse: 56043928.0000 - mae: 5865.6108 - val_loss: 4055.9903 - val_mse: 23425012.0000 - val_mae: 4055.9905\n",
            "Epoch 408/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5689.6172 - mse: 43972744.0000 - mae: 5689.6172 - val_loss: 4054.8015 - val_mse: 23399516.0000 - val_mae: 4054.8015\n",
            "Epoch 409/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5569.6333 - mse: 44553456.0000 - mae: 5569.6333 - val_loss: 4031.8468 - val_mse: 22913564.0000 - val_mae: 4031.8469\n",
            "Epoch 410/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4964.7990 - mse: 35230956.0000 - mae: 4964.7988 - val_loss: 4029.8489 - val_mse: 22871954.0000 - val_mae: 4029.8489\n",
            "Epoch 411/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5163.3800 - mse: 46513004.0000 - mae: 5163.3799 - val_loss: 4044.5477 - val_mse: 23180902.0000 - val_mae: 4044.5476\n",
            "Epoch 412/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5780.7920 - mse: 58757336.0000 - mae: 5780.7915 - val_loss: 4051.5942 - val_mse: 23327218.0000 - val_mae: 4051.5942\n",
            "Epoch 413/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 7124.8529 - mse: 71093728.0000 - mae: 7124.8530 - val_loss: 4049.0854 - val_mse: 23271212.0000 - val_mae: 4049.0854\n",
            "Epoch 414/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6215.6302 - mse: 51621284.0000 - mae: 6215.6304 - val_loss: 4041.0262 - val_mse: 23106654.0000 - val_mae: 4041.0264\n",
            "Epoch 415/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7417.3911 - mse: 84289304.0000 - mae: 7417.3916 - val_loss: 4040.4900 - val_mse: 23094436.0000 - val_mae: 4040.4900\n",
            "Epoch 416/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6290.6131 - mse: 66323404.0000 - mae: 6290.6138 - val_loss: 4034.0647 - val_mse: 22956694.0000 - val_mae: 4034.0647\n",
            "Epoch 417/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6106.4785 - mse: 60524208.0000 - mae: 6106.4785 - val_loss: 4027.2891 - val_mse: 22815896.0000 - val_mae: 4027.2893\n",
            "Epoch 418/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4838.9163 - mse: 35421304.0000 - mae: 4838.9160 - val_loss: 4023.9416 - val_mse: 22747548.0000 - val_mae: 4023.9414\n",
            "Epoch 419/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5797.6548 - mse: 50686372.0000 - mae: 5797.6548 - val_loss: 4012.4780 - val_mse: 22506578.0000 - val_mae: 4012.4778\n",
            "Epoch 420/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8091.1989 - mse: 90067584.0000 - mae: 8091.1987 - val_loss: 4010.7411 - val_mse: 22472730.0000 - val_mae: 4010.7412\n",
            "Epoch 421/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6244.9508 - mse: 55884224.0000 - mae: 6244.9512 - val_loss: 4003.1610 - val_mse: 22321408.0000 - val_mae: 4003.1609\n",
            "Epoch 422/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6687.9523 - mse: 61524348.0000 - mae: 6687.9521 - val_loss: 4004.5798 - val_mse: 22350076.0000 - val_mae: 4004.5796\n",
            "Epoch 423/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5833.7617 - mse: 46565952.0000 - mae: 5833.7612 - val_loss: 3997.9634 - val_mse: 22217244.0000 - val_mae: 3997.9636\n",
            "Epoch 424/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 6550.1530 - mse: 70018856.0000 - mae: 6550.1533 - val_loss: 3989.9881 - val_mse: 22063046.0000 - val_mae: 3989.9880\n",
            "Epoch 425/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5615.6919 - mse: 45045632.0000 - mae: 5615.6919 - val_loss: 4000.1478 - val_mse: 22269622.0000 - val_mae: 4000.1479\n",
            "Epoch 426/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5505.7585 - mse: 41862648.0000 - mae: 5505.7583 - val_loss: 4010.3733 - val_mse: 22481740.0000 - val_mae: 4010.3735\n",
            "Epoch 427/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5486.5109 - mse: 48553732.0000 - mae: 5486.5107 - val_loss: 4001.2408 - val_mse: 22294570.0000 - val_mae: 4001.2407\n",
            "Epoch 428/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5155.8521 - mse: 44937080.0000 - mae: 5155.8521 - val_loss: 4004.5527 - val_mse: 22362378.0000 - val_mae: 4004.5525\n",
            "Epoch 429/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5527.0999 - mse: 59499324.0000 - mae: 5527.1001 - val_loss: 3997.7450 - val_mse: 22223656.0000 - val_mae: 3997.7451\n",
            "Epoch 430/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6355.8397 - mse: 59446844.0000 - mae: 6355.8394 - val_loss: 4005.7183 - val_mse: 22387444.0000 - val_mae: 4005.7185\n",
            "Epoch 431/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5944.4032 - mse: 52777652.0000 - mae: 5944.4033 - val_loss: 4001.1779 - val_mse: 22297728.0000 - val_mae: 4001.1780\n",
            "Epoch 432/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5544.3549 - mse: 51646904.0000 - mae: 5544.3550 - val_loss: 4002.8455 - val_mse: 22333526.0000 - val_mae: 4002.8452\n",
            "Epoch 433/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6165.7227 - mse: 62215228.0000 - mae: 6165.7231 - val_loss: 3995.6260 - val_mse: 22186260.0000 - val_mae: 3995.6260\n",
            "Epoch 434/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4949.3637 - mse: 35210056.0000 - mae: 4949.3638 - val_loss: 3992.3194 - val_mse: 22121948.0000 - val_mae: 3992.3193\n",
            "Epoch 435/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6358.7469 - mse: 53960472.0000 - mae: 6358.7471 - val_loss: 3976.4586 - val_mse: 21803660.0000 - val_mae: 3976.4587\n",
            "Epoch 436/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6332.3240 - mse: 65819176.0000 - mae: 6332.3237 - val_loss: 3960.3953 - val_mse: 21487708.0000 - val_mae: 3960.3953\n",
            "Epoch 437/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5684.4176 - mse: 51805852.0000 - mae: 5684.4175 - val_loss: 3964.2490 - val_mse: 21564650.0000 - val_mae: 3964.2490\n",
            "Epoch 438/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6145.1875 - mse: 48108428.0000 - mae: 6145.1875 - val_loss: 3964.1738 - val_mse: 21562410.0000 - val_mae: 3964.1741\n",
            "Epoch 439/500\n",
            "30/30 [==============================] - 0s 955us/step - loss: 5162.8615 - mse: 36216976.0000 - mae: 5162.8613 - val_loss: 3963.8347 - val_mse: 21562120.0000 - val_mae: 3963.8347\n",
            "Epoch 440/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6910.3411 - mse: 72732936.0000 - mae: 6910.3408 - val_loss: 3961.2464 - val_mse: 21515290.0000 - val_mae: 3961.2463\n",
            "Epoch 441/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6488.2515 - mse: 53983344.0000 - mae: 6488.2515 - val_loss: 3969.8108 - val_mse: 21688632.0000 - val_mae: 3969.8110\n",
            "Epoch 442/500\n",
            "30/30 [==============================] - 0s 988us/step - loss: 5535.1755 - mse: 50891672.0000 - mae: 5535.1753 - val_loss: 3961.0187 - val_mse: 21510372.0000 - val_mae: 3961.0186\n",
            "Epoch 443/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6028.3377 - mse: 53856180.0000 - mae: 6028.3374 - val_loss: 3956.6586 - val_mse: 21424578.0000 - val_mae: 3956.6587\n",
            "Epoch 444/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5618.8888 - mse: 51858772.0000 - mae: 5618.8887 - val_loss: 3956.7080 - val_mse: 21426470.0000 - val_mae: 3956.7078\n",
            "Epoch 445/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5096.8626 - mse: 37348172.0000 - mae: 5096.8623 - val_loss: 3958.2939 - val_mse: 21456494.0000 - val_mae: 3958.2937\n",
            "Epoch 446/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6886.3021 - mse: 82092712.0000 - mae: 6886.3022 - val_loss: 3948.2099 - val_mse: 21258376.0000 - val_mae: 3948.2100\n",
            "Epoch 447/500\n",
            "30/30 [==============================] - 0s 979us/step - loss: 5069.1055 - mse: 40154692.0000 - mae: 5069.1050 - val_loss: 3934.0759 - val_mse: 20987578.0000 - val_mae: 3934.0759\n",
            "Epoch 448/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5604.1354 - mse: 57131868.0000 - mae: 5604.1353 - val_loss: 3927.5727 - val_mse: 20863978.0000 - val_mae: 3927.5728\n",
            "Epoch 449/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7133.1458 - mse: 81433976.0000 - mae: 7133.1460 - val_loss: 3937.7497 - val_mse: 21063452.0000 - val_mae: 3937.7498\n",
            "Epoch 450/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5620.4505 - mse: 43978844.0000 - mae: 5620.4507 - val_loss: 3946.6777 - val_mse: 21232348.0000 - val_mae: 3946.6775\n",
            "Epoch 451/500\n",
            "30/30 [==============================] - 0s 993us/step - loss: 5720.1007 - mse: 55610760.0000 - mae: 5720.1011 - val_loss: 3942.6360 - val_mse: 21157892.0000 - val_mae: 3942.6362\n",
            "Epoch 452/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6046.1393 - mse: 58949596.0000 - mae: 6046.1392 - val_loss: 3937.8717 - val_mse: 21070220.0000 - val_mae: 3937.8716\n",
            "Epoch 453/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6048.3009 - mse: 55344528.0000 - mae: 6048.3013 - val_loss: 3933.6301 - val_mse: 20994184.0000 - val_mae: 3933.6301\n",
            "Epoch 454/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6513.5788 - mse: 68101768.0000 - mae: 6513.5781 - val_loss: 3933.9431 - val_mse: 21004096.0000 - val_mae: 3933.9431\n",
            "Epoch 455/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5682.2001 - mse: 53460164.0000 - mae: 5682.2002 - val_loss: 3925.0424 - val_mse: 20831366.0000 - val_mae: 3925.0425\n",
            "Epoch 456/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5042.4759 - mse: 40439748.0000 - mae: 5042.4761 - val_loss: 3919.1392 - val_mse: 20719932.0000 - val_mae: 3919.1394\n",
            "Epoch 457/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5639.6012 - mse: 46148716.0000 - mae: 5639.6011 - val_loss: 3911.1969 - val_mse: 20571134.0000 - val_mae: 3911.1968\n",
            "Epoch 458/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5797.2144 - mse: 59250356.0000 - mae: 5797.2144 - val_loss: 3919.9563 - val_mse: 20733062.0000 - val_mae: 3919.9563\n",
            "Epoch 459/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6847.1745 - mse: 68524736.0000 - mae: 6847.1743 - val_loss: 3917.2314 - val_mse: 20680110.0000 - val_mae: 3917.2314\n",
            "Epoch 460/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5590.0697 - mse: 47749960.0000 - mae: 5590.0698 - val_loss: 3912.8462 - val_mse: 20594780.0000 - val_mae: 3912.8464\n",
            "Epoch 461/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5299.4355 - mse: 41097348.0000 - mae: 5299.4355 - val_loss: 3915.5016 - val_mse: 20643934.0000 - val_mae: 3915.5015\n",
            "Epoch 462/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5737.5329 - mse: 46926556.0000 - mae: 5737.5327 - val_loss: 3917.9798 - val_mse: 20691388.0000 - val_mae: 3917.9797\n",
            "Epoch 463/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6337.0251 - mse: 60406092.0000 - mae: 6337.0249 - val_loss: 3926.6038 - val_mse: 20856450.0000 - val_mae: 3926.6038\n",
            "Epoch 464/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4747.9653 - mse: 29365280.0000 - mae: 4747.9653 - val_loss: 3911.6905 - val_mse: 20573138.0000 - val_mae: 3911.6904\n",
            "Epoch 465/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5581.3226 - mse: 48462796.0000 - mae: 5581.3228 - val_loss: 3923.1261 - val_mse: 20794200.0000 - val_mae: 3923.1260\n",
            "Epoch 466/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6993.9404 - mse: 71306648.0000 - mae: 6993.9404 - val_loss: 3927.4648 - val_mse: 20878578.0000 - val_mae: 3927.4648\n",
            "Epoch 467/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5962.5475 - mse: 48632076.0000 - mae: 5962.5479 - val_loss: 3916.1679 - val_mse: 20661566.0000 - val_mae: 3916.1677\n",
            "Epoch 468/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5977.6011 - mse: 57290572.0000 - mae: 5977.6011 - val_loss: 3907.5813 - val_mse: 20498474.0000 - val_mae: 3907.5813\n",
            "Epoch 469/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5808.2035 - mse: 54292888.0000 - mae: 5808.2031 - val_loss: 3918.7275 - val_mse: 20712070.0000 - val_mae: 3918.7273\n",
            "Epoch 470/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6746.7489 - mse: 71464912.0000 - mae: 6746.7490 - val_loss: 3913.0018 - val_mse: 20606556.0000 - val_mae: 3913.0017\n",
            "Epoch 471/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5908.9160 - mse: 58660464.0000 - mae: 5908.9160 - val_loss: 3912.9762 - val_mse: 20600628.0000 - val_mae: 3912.9763\n",
            "Epoch 472/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5546.8200 - mse: 44431084.0000 - mae: 5546.8198 - val_loss: 3906.4823 - val_mse: 20479302.0000 - val_mae: 3906.4822\n",
            "Epoch 473/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6392.0942 - mse: 60550416.0000 - mae: 6392.0942 - val_loss: 3912.1886 - val_mse: 20583666.0000 - val_mae: 3912.1885\n",
            "Epoch 474/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5986.7174 - mse: 59276980.0000 - mae: 5986.7178 - val_loss: 3901.6531 - val_mse: 20381582.0000 - val_mae: 3901.6531\n",
            "Epoch 475/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6188.9863 - mse: 65728912.0000 - mae: 6188.9863 - val_loss: 3908.8121 - val_mse: 20519062.0000 - val_mae: 3908.8123\n",
            "Epoch 476/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6805.9053 - mse: 77923056.0000 - mae: 6805.9053 - val_loss: 3904.3696 - val_mse: 20440346.0000 - val_mae: 3904.3696\n",
            "Epoch 477/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6221.4910 - mse: 50708840.0000 - mae: 6221.4912 - val_loss: 3901.5792 - val_mse: 20383950.0000 - val_mae: 3901.5791\n",
            "Epoch 478/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 5925.0174 - mse: 59623876.0000 - mae: 5925.0176 - val_loss: 3896.4816 - val_mse: 20290954.0000 - val_mae: 3896.4814\n",
            "Epoch 479/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5712.6841 - mse: 41087580.0000 - mae: 5712.6846 - val_loss: 3884.8896 - val_mse: 20080478.0000 - val_mae: 3884.8896\n",
            "Epoch 480/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6220.9787 - mse: 57724356.0000 - mae: 6220.9785 - val_loss: 3881.6520 - val_mse: 20024298.0000 - val_mae: 3881.6521\n",
            "Epoch 481/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5746.6642 - mse: 54065928.0000 - mae: 5746.6646 - val_loss: 3880.4858 - val_mse: 20004856.0000 - val_mae: 3880.4858\n",
            "Epoch 482/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5674.0339 - mse: 42983216.0000 - mae: 5674.0337 - val_loss: 3867.1638 - val_mse: 19761224.0000 - val_mae: 3867.1638\n",
            "Epoch 483/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5656.7816 - mse: 49679296.0000 - mae: 5656.7812 - val_loss: 3880.4159 - val_mse: 20007502.0000 - val_mae: 3880.4163\n",
            "Epoch 484/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4550.1145 - mse: 37360724.0000 - mae: 4550.1147 - val_loss: 3880.3409 - val_mse: 20008650.0000 - val_mae: 3880.3411\n",
            "Epoch 485/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6235.9915 - mse: 64172092.0000 - mae: 6235.9917 - val_loss: 3873.6069 - val_mse: 19883140.0000 - val_mae: 3873.6067\n",
            "Epoch 486/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5947.6499 - mse: 51519916.0000 - mae: 5947.6499 - val_loss: 3872.2423 - val_mse: 19861860.0000 - val_mae: 3872.2422\n",
            "Epoch 487/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4671.5185 - mse: 40651724.0000 - mae: 4671.5186 - val_loss: 3858.4921 - val_mse: 19617676.0000 - val_mae: 3858.4922\n",
            "Epoch 488/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6152.1012 - mse: 56667996.0000 - mae: 6152.1011 - val_loss: 3871.9044 - val_mse: 19858316.0000 - val_mae: 3871.9041\n",
            "Epoch 489/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6123.3397 - mse: 67007028.0000 - mae: 6123.3394 - val_loss: 3876.5076 - val_mse: 19945496.0000 - val_mae: 3876.5078\n",
            "Epoch 490/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6531.5713 - mse: 66619556.0000 - mae: 6531.5713 - val_loss: 3888.5424 - val_mse: 20169142.0000 - val_mae: 3888.5422\n",
            "Epoch 491/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4491.4062 - mse: 39720688.0000 - mae: 4491.4062 - val_loss: 3893.3936 - val_mse: 20263298.0000 - val_mae: 3893.3936\n",
            "Epoch 492/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7143.6073 - mse: 77901392.0000 - mae: 7143.6074 - val_loss: 3900.4135 - val_mse: 20388654.0000 - val_mae: 3900.4136\n",
            "Epoch 493/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5596.3219 - mse: 47679252.0000 - mae: 5596.3218 - val_loss: 3905.8948 - val_mse: 20491922.0000 - val_mae: 3905.8948\n",
            "Epoch 494/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5973.8365 - mse: 57357080.0000 - mae: 5973.8364 - val_loss: 3909.3106 - val_mse: 20557720.0000 - val_mae: 3909.3103\n",
            "Epoch 495/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6410.2770 - mse: 55942128.0000 - mae: 6410.2769 - val_loss: 3898.3004 - val_mse: 20349246.0000 - val_mae: 3898.3000\n",
            "Epoch 496/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6486.7689 - mse: 63068840.0000 - mae: 6486.7686 - val_loss: 3895.9223 - val_mse: 20302454.0000 - val_mae: 3895.9221\n",
            "Epoch 497/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6046.6473 - mse: 56588680.0000 - mae: 6046.6475 - val_loss: 3899.3592 - val_mse: 20366618.0000 - val_mae: 3899.3591\n",
            "Epoch 498/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5048.6088 - mse: 43859216.0000 - mae: 5048.6089 - val_loss: 3930.3036 - val_mse: 20943702.0000 - val_mae: 3930.3037\n",
            "Epoch 499/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6668.8258 - mse: 68203400.0000 - mae: 6668.8262 - val_loss: 3936.5388 - val_mse: 21067114.0000 - val_mae: 3936.5388\n",
            "Epoch 500/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5981.0741 - mse: 50091204.0000 - mae: 5981.0737 - val_loss: 3932.4115 - val_mse: 20989074.0000 - val_mae: 3932.4114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50 samples, validate on 40 samples\n",
            "Epoch 1/500\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 24047.3410 - mse: 597862144.0000 - mae: 24047.3398 - val_loss: 25553.9204 - val_mse: 666561152.0000 - val_mae: 25553.9199\n",
            "Epoch 2/500\n",
            "50/50 [==============================] - 0s 734us/step - loss: 24047.2805 - mse: 597859136.0000 - mae: 24047.2793 - val_loss: 25553.8184 - val_mse: 666555712.0000 - val_mae: 25553.8184\n",
            "Epoch 3/500\n",
            "50/50 [==============================] - 0s 850us/step - loss: 24046.9832 - mse: 597844928.0000 - mae: 24046.9824 - val_loss: 25553.0293 - val_mse: 666511168.0000 - val_mae: 25553.0273\n",
            "Epoch 4/500\n",
            "50/50 [==============================] - 0s 811us/step - loss: 24045.4691 - mse: 597772736.0000 - mae: 24045.4707 - val_loss: 25550.1621 - val_mse: 666351040.0000 - val_mae: 25550.1602\n",
            "Epoch 5/500\n",
            "50/50 [==============================] - 0s 856us/step - loss: 24041.9313 - mse: 597594624.0000 - mae: 24041.9316 - val_loss: 25545.3110 - val_mse: 666084544.0000 - val_mae: 25545.3105\n",
            "Epoch 6/500\n",
            "50/50 [==============================] - 0s 811us/step - loss: 24036.7367 - mse: 597340928.0000 - mae: 24036.7383 - val_loss: 25538.0879 - val_mse: 665698624.0000 - val_mae: 25538.0879\n",
            "Epoch 7/500\n",
            "50/50 [==============================] - 0s 757us/step - loss: 24029.0750 - mse: 596978880.0000 - mae: 24029.0742 - val_loss: 25527.7417 - val_mse: 665154752.0000 - val_mae: 25527.7402\n",
            "Epoch 8/500\n",
            "50/50 [==============================] - 0s 888us/step - loss: 24019.8836 - mse: 596530944.0000 - mae: 24019.8828 - val_loss: 25515.5479 - val_mse: 664515328.0000 - val_mae: 25515.5488\n",
            "Epoch 9/500\n",
            "50/50 [==============================] - 0s 794us/step - loss: 24006.7648 - mse: 595912512.0000 - mae: 24006.7656 - val_loss: 25500.3726 - val_mse: 663720960.0000 - val_mae: 25500.3711\n",
            "Epoch 10/500\n",
            "50/50 [==============================] - 0s 863us/step - loss: 23986.9094 - mse: 594975616.0000 - mae: 23986.9102 - val_loss: 25482.0840 - val_mse: 662763904.0000 - val_mae: 25482.0840\n",
            "Epoch 11/500\n",
            "50/50 [==============================] - 0s 737us/step - loss: 23965.4520 - mse: 593882112.0000 - mae: 23965.4531 - val_loss: 25461.5215 - val_mse: 661689472.0000 - val_mae: 25461.5215\n",
            "Epoch 12/500\n",
            "50/50 [==============================] - 0s 771us/step - loss: 23935.2961 - mse: 592548032.0000 - mae: 23935.2969 - val_loss: 25437.8521 - val_mse: 660455360.0000 - val_mae: 25437.8516\n",
            "Epoch 13/500\n",
            "50/50 [==============================] - 0s 753us/step - loss: 23915.7953 - mse: 591653824.0000 - mae: 23915.7949 - val_loss: 25413.6660 - val_mse: 659197568.0000 - val_mae: 25413.6680\n",
            "Epoch 14/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 23894.1410 - mse: 590527872.0000 - mae: 23894.1406 - val_loss: 25387.6064 - val_mse: 657845376.0000 - val_mae: 25387.6055\n",
            "Epoch 15/500\n",
            "50/50 [==============================] - 0s 803us/step - loss: 23862.5973 - mse: 588979264.0000 - mae: 23862.5977 - val_loss: 25359.2334 - val_mse: 656376960.0000 - val_mae: 25359.2344\n",
            "Epoch 16/500\n",
            "50/50 [==============================] - 0s 877us/step - loss: 23834.7094 - mse: 587767616.0000 - mae: 23834.7109 - val_loss: 25329.1392 - val_mse: 654822272.0000 - val_mae: 25329.1367\n",
            "Epoch 17/500\n",
            "50/50 [==============================] - 0s 895us/step - loss: 23801.8309 - mse: 586165888.0000 - mae: 23801.8320 - val_loss: 25297.2925 - val_mse: 653179456.0000 - val_mae: 25297.2930\n",
            "Epoch 18/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 23755.0422 - mse: 584034688.0000 - mae: 23755.0430 - val_loss: 25261.8550 - val_mse: 651356800.0000 - val_mae: 25261.8555\n",
            "Epoch 19/500\n",
            "50/50 [==============================] - 0s 806us/step - loss: 23713.8957 - mse: 582085888.0000 - mae: 23713.8945 - val_loss: 25224.4448 - val_mse: 649437952.0000 - val_mae: 25224.4434\n",
            "Epoch 20/500\n",
            "50/50 [==============================] - 0s 919us/step - loss: 23685.4949 - mse: 580659008.0000 - mae: 23685.4941 - val_loss: 25186.2676 - val_mse: 647484800.0000 - val_mae: 25186.2676\n",
            "Epoch 21/500\n",
            "50/50 [==============================] - 0s 807us/step - loss: 23656.8543 - mse: 579520640.0000 - mae: 23656.8555 - val_loss: 25147.2900 - val_mse: 645495296.0000 - val_mae: 25147.2910\n",
            "Epoch 22/500\n",
            "50/50 [==============================] - 0s 821us/step - loss: 23607.9152 - mse: 577273472.0000 - mae: 23607.9141 - val_loss: 25105.7192 - val_mse: 643377792.0000 - val_mae: 25105.7188\n",
            "Epoch 23/500\n",
            "50/50 [==============================] - 0s 865us/step - loss: 23569.7414 - mse: 575472384.0000 - mae: 23569.7402 - val_loss: 25062.8125 - val_mse: 641196096.0000 - val_mae: 25062.8125\n",
            "Epoch 24/500\n",
            "50/50 [==============================] - 0s 827us/step - loss: 23499.9320 - mse: 572016960.0000 - mae: 23499.9316 - val_loss: 25015.9419 - val_mse: 638818752.0000 - val_mae: 25015.9414\n",
            "Epoch 25/500\n",
            "50/50 [==============================] - 0s 820us/step - loss: 23504.3531 - mse: 572114688.0000 - mae: 23504.3516 - val_loss: 24971.2188 - val_mse: 636555904.0000 - val_mae: 24971.2188\n",
            "Epoch 26/500\n",
            "50/50 [==============================] - 0s 782us/step - loss: 23448.2582 - mse: 569379456.0000 - mae: 23448.2598 - val_loss: 24923.5728 - val_mse: 634152832.0000 - val_mae: 24923.5723\n",
            "Epoch 27/500\n",
            "50/50 [==============================] - 0s 775us/step - loss: 23343.0129 - mse: 564672384.0000 - mae: 23343.0117 - val_loss: 24870.7988 - val_mse: 631495104.0000 - val_mae: 24870.7969\n",
            "Epoch 28/500\n",
            "50/50 [==============================] - 0s 731us/step - loss: 23283.0863 - mse: 561626176.0000 - mae: 23283.0859 - val_loss: 24815.1348 - val_mse: 628702848.0000 - val_mae: 24815.1348\n",
            "Epoch 29/500\n",
            "50/50 [==============================] - 0s 875us/step - loss: 23263.2527 - mse: 560864512.0000 - mae: 23263.2559 - val_loss: 24760.4893 - val_mse: 625966208.0000 - val_mae: 24760.4883\n",
            "Epoch 30/500\n",
            "50/50 [==============================] - 0s 952us/step - loss: 23228.4691 - mse: 559080512.0000 - mae: 23228.4707 - val_loss: 24705.6772 - val_mse: 623228288.0000 - val_mae: 24705.6777\n",
            "Epoch 31/500\n",
            "50/50 [==============================] - 0s 877us/step - loss: 23146.5563 - mse: 555847936.0000 - mae: 23146.5547 - val_loss: 24647.0303 - val_mse: 620306560.0000 - val_mae: 24647.0293\n",
            "Epoch 32/500\n",
            "50/50 [==============================] - 0s 794us/step - loss: 23080.7719 - mse: 552388992.0000 - mae: 23080.7734 - val_loss: 24586.0879 - val_mse: 617281408.0000 - val_mae: 24586.0879\n",
            "Epoch 33/500\n",
            "50/50 [==============================] - 0s 926us/step - loss: 23030.4281 - mse: 550355904.0000 - mae: 23030.4297 - val_loss: 24524.5015 - val_mse: 614230848.0000 - val_mae: 24524.5020\n",
            "Epoch 34/500\n",
            "50/50 [==============================] - 0s 879us/step - loss: 22912.6855 - mse: 544913920.0000 - mae: 22912.6855 - val_loss: 24458.0889 - val_mse: 610949824.0000 - val_mae: 24458.0879\n",
            "Epoch 35/500\n",
            "50/50 [==============================] - 0s 822us/step - loss: 22932.8711 - mse: 546139776.0000 - mae: 22932.8730 - val_loss: 24394.8379 - val_mse: 607834624.0000 - val_mae: 24394.8379\n",
            "Epoch 36/500\n",
            "50/50 [==============================] - 0s 773us/step - loss: 22833.2863 - mse: 541064448.0000 - mae: 22833.2871 - val_loss: 24327.8379 - val_mse: 604542528.0000 - val_mae: 24327.8379\n",
            "Epoch 37/500\n",
            "50/50 [==============================] - 0s 784us/step - loss: 22844.6605 - mse: 542137280.0000 - mae: 22844.6602 - val_loss: 24263.3350 - val_mse: 601383680.0000 - val_mae: 24263.3359\n",
            "Epoch 38/500\n",
            "50/50 [==============================] - 0s 859us/step - loss: 22679.2000 - mse: 535044928.0000 - mae: 22679.1992 - val_loss: 24192.1104 - val_mse: 597904512.0000 - val_mae: 24192.1113\n",
            "Epoch 39/500\n",
            "50/50 [==============================] - 0s 895us/step - loss: 22643.1094 - mse: 533054016.0000 - mae: 22643.1094 - val_loss: 24120.2065 - val_mse: 594406592.0000 - val_mae: 24120.2070\n",
            "Epoch 40/500\n",
            "50/50 [==============================] - 0s 972us/step - loss: 22633.4949 - mse: 532222048.0000 - mae: 22633.4941 - val_loss: 24050.0933 - val_mse: 591004544.0000 - val_mae: 24050.0938\n",
            "Epoch 41/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 22397.8848 - mse: 522041504.0000 - mae: 22397.8848 - val_loss: 23970.6216 - val_mse: 587160064.0000 - val_mae: 23970.6211\n",
            "Epoch 42/500\n",
            "50/50 [==============================] - 0s 797us/step - loss: 22363.4746 - mse: 519806112.0000 - mae: 22363.4746 - val_loss: 23892.0435 - val_mse: 583372288.0000 - val_mae: 23892.0430\n",
            "Epoch 43/500\n",
            "50/50 [==============================] - 0s 816us/step - loss: 22344.2629 - mse: 519219648.0000 - mae: 22344.2656 - val_loss: 23813.8765 - val_mse: 579619648.0000 - val_mae: 23813.8750\n",
            "Epoch 44/500\n",
            "50/50 [==============================] - 0s 810us/step - loss: 22282.9508 - mse: 515894880.0000 - mae: 22282.9492 - val_loss: 23734.4292 - val_mse: 575819584.0000 - val_mae: 23734.4297\n",
            "Epoch 45/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 22232.4023 - mse: 515036064.0000 - mae: 22232.4023 - val_loss: 23654.5640 - val_mse: 572012672.0000 - val_mae: 23654.5645\n",
            "Epoch 46/500\n",
            "50/50 [==============================] - 0s 990us/step - loss: 22059.9359 - mse: 507199904.0000 - mae: 22059.9375 - val_loss: 23568.9424 - val_mse: 567945920.0000 - val_mae: 23568.9414\n",
            "Epoch 47/500\n",
            "50/50 [==============================] - 0s 787us/step - loss: 21974.5289 - mse: 502915072.0000 - mae: 21974.5293 - val_loss: 23481.4580 - val_mse: 563808128.0000 - val_mae: 23481.4570\n",
            "Epoch 48/500\n",
            "50/50 [==============================] - 0s 819us/step - loss: 21921.6223 - mse: 500339264.0000 - mae: 21921.6230 - val_loss: 23394.1519 - val_mse: 559692928.0000 - val_mae: 23394.1523\n",
            "Epoch 49/500\n",
            "50/50 [==============================] - 0s 927us/step - loss: 21677.6750 - mse: 489239744.0000 - mae: 21677.6758 - val_loss: 23299.1538 - val_mse: 555232192.0000 - val_mae: 23299.1523\n",
            "Epoch 50/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 21759.4648 - mse: 493593024.0000 - mae: 21759.4648 - val_loss: 23208.9375 - val_mse: 551015040.0000 - val_mae: 23208.9375\n",
            "Epoch 51/500\n",
            "50/50 [==============================] - 0s 930us/step - loss: 21630.3230 - mse: 488116320.0000 - mae: 21630.3223 - val_loss: 23115.7827 - val_mse: 546676736.0000 - val_mae: 23115.7812\n",
            "Epoch 52/500\n",
            "50/50 [==============================] - 0s 823us/step - loss: 21478.8445 - mse: 481054240.0000 - mae: 21478.8457 - val_loss: 23019.0186 - val_mse: 542191744.0000 - val_mae: 23019.0176\n",
            "Epoch 53/500\n",
            "50/50 [==============================] - 0s 928us/step - loss: 21596.0305 - mse: 488359968.0000 - mae: 21596.0293 - val_loss: 22928.1230 - val_mse: 537994624.0000 - val_mae: 22928.1230\n",
            "Epoch 54/500\n",
            "50/50 [==============================] - 0s 796us/step - loss: 21431.1840 - mse: 479531584.0000 - mae: 21431.1855 - val_loss: 22832.4917 - val_mse: 533598624.0000 - val_mae: 22832.4941\n",
            "Epoch 55/500\n",
            "50/50 [==============================] - 0s 948us/step - loss: 21229.7004 - mse: 471803168.0000 - mae: 21229.6992 - val_loss: 22731.9570 - val_mse: 528996352.0000 - val_mae: 22731.9570\n",
            "Epoch 56/500\n",
            "50/50 [==============================] - 0s 827us/step - loss: 21185.8465 - mse: 469311680.0000 - mae: 21185.8477 - val_loss: 22631.4146 - val_mse: 524414048.0000 - val_mae: 22631.4160\n",
            "Epoch 57/500\n",
            "50/50 [==============================] - 0s 696us/step - loss: 21154.3141 - mse: 466166656.0000 - mae: 21154.3125 - val_loss: 22531.3691 - val_mse: 519874752.0000 - val_mae: 22531.3691\n",
            "Epoch 58/500\n",
            "50/50 [==============================] - 0s 867us/step - loss: 21043.4414 - mse: 462982592.0000 - mae: 21043.4434 - val_loss: 22429.2114 - val_mse: 515262880.0000 - val_mae: 22429.2109\n",
            "Epoch 59/500\n",
            "50/50 [==============================] - 0s 749us/step - loss: 21007.4680 - mse: 462431104.0000 - mae: 21007.4668 - val_loss: 22327.1221 - val_mse: 510676480.0000 - val_mae: 22327.1230\n",
            "Epoch 60/500\n",
            "50/50 [==============================] - 0s 848us/step - loss: 20627.5918 - mse: 445530400.0000 - mae: 20627.5918 - val_loss: 22214.4785 - val_mse: 505638080.0000 - val_mae: 22214.4785\n",
            "Epoch 61/500\n",
            "50/50 [==============================] - 0s 716us/step - loss: 20595.3102 - mse: 444955200.0000 - mae: 20595.3105 - val_loss: 22103.3218 - val_mse: 500689920.0000 - val_mae: 22103.3223\n",
            "Epoch 62/500\n",
            "50/50 [==============================] - 0s 968us/step - loss: 20390.4289 - mse: 435361056.0000 - mae: 20390.4297 - val_loss: 21986.9175 - val_mse: 495538944.0000 - val_mae: 21986.9180\n",
            "Epoch 63/500\n",
            "50/50 [==============================] - 0s 798us/step - loss: 20504.8367 - mse: 439460672.0000 - mae: 20504.8359 - val_loss: 21876.1611 - val_mse: 490660672.0000 - val_mae: 21876.1602\n",
            "Epoch 64/500\n",
            "50/50 [==============================] - 0s 973us/step - loss: 20174.2387 - mse: 427632064.0000 - mae: 20174.2383 - val_loss: 21757.3105 - val_mse: 485452896.0000 - val_mae: 21757.3105\n",
            "Epoch 65/500\n",
            "50/50 [==============================] - 0s 992us/step - loss: 20273.1633 - mse: 433356512.0000 - mae: 20273.1621 - val_loss: 21643.3218 - val_mse: 480485312.0000 - val_mae: 21643.3223\n",
            "Epoch 66/500\n",
            "50/50 [==============================] - 0s 947us/step - loss: 20018.1551 - mse: 421796096.0000 - mae: 20018.1543 - val_loss: 21523.8633 - val_mse: 475308192.0000 - val_mae: 21523.8633\n",
            "Epoch 67/500\n",
            "50/50 [==============================] - 0s 787us/step - loss: 19750.4977 - mse: 413264160.0000 - mae: 19750.4980 - val_loss: 21397.8921 - val_mse: 469883296.0000 - val_mae: 21397.8945\n",
            "Epoch 68/500\n",
            "50/50 [==============================] - 0s 859us/step - loss: 19549.0883 - mse: 404420608.0000 - mae: 19549.0879 - val_loss: 21267.9873 - val_mse: 464321344.0000 - val_mae: 21267.9863\n",
            "Epoch 69/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 19557.7051 - mse: 401945792.0000 - mae: 19557.7051 - val_loss: 21140.9438 - val_mse: 458915328.0000 - val_mae: 21140.9434\n",
            "Epoch 70/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 19247.9418 - mse: 392275680.0000 - mae: 19247.9414 - val_loss: 21007.6001 - val_mse: 453274944.0000 - val_mae: 21007.5996\n",
            "Epoch 71/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 19195.9426 - mse: 388555200.0000 - mae: 19195.9434 - val_loss: 20874.2627 - val_mse: 447672928.0000 - val_mae: 20874.2617\n",
            "Epoch 72/500\n",
            "50/50 [==============================] - 0s 885us/step - loss: 19374.7244 - mse: 397875680.0000 - mae: 19374.7246 - val_loss: 20748.0103 - val_mse: 442399136.0000 - val_mae: 20748.0098\n",
            "Epoch 73/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 19067.5480 - mse: 385356256.0000 - mae: 19067.5469 - val_loss: 20615.4106 - val_mse: 436896864.0000 - val_mae: 20615.4102\n",
            "Epoch 74/500\n",
            "50/50 [==============================] - 0s 957us/step - loss: 18894.0059 - mse: 379907680.0000 - mae: 18894.0059 - val_loss: 20479.8809 - val_mse: 431309760.0000 - val_mae: 20479.8789\n",
            "Epoch 75/500\n",
            "50/50 [==============================] - 0s 782us/step - loss: 18936.6063 - mse: 380784736.0000 - mae: 18936.6055 - val_loss: 20347.0928 - val_mse: 425872064.0000 - val_mae: 20347.0938\n",
            "Epoch 76/500\n",
            "50/50 [==============================] - 0s 900us/step - loss: 18572.3605 - mse: 364616672.0000 - mae: 18572.3613 - val_loss: 20206.4741 - val_mse: 420156256.0000 - val_mae: 20206.4746\n",
            "Epoch 77/500\n",
            "50/50 [==============================] - 0s 887us/step - loss: 18211.6770 - mse: 355486016.0000 - mae: 18211.6758 - val_loss: 20059.6240 - val_mse: 414224576.0000 - val_mae: 20059.6250\n",
            "Epoch 78/500\n",
            "50/50 [==============================] - 0s 905us/step - loss: 18343.8844 - mse: 357421376.0000 - mae: 18343.8848 - val_loss: 19918.1699 - val_mse: 408551520.0000 - val_mae: 19918.1680\n",
            "Epoch 79/500\n",
            "50/50 [==============================] - 0s 802us/step - loss: 18176.4730 - mse: 356179840.0000 - mae: 18176.4727 - val_loss: 19774.3389 - val_mse: 402825088.0000 - val_mae: 19774.3398\n",
            "Epoch 80/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 18009.8531 - mse: 345736448.0000 - mae: 18009.8535 - val_loss: 19627.8992 - val_mse: 397039936.0000 - val_mae: 19627.9004\n",
            "Epoch 81/500\n",
            "50/50 [==============================] - 0s 925us/step - loss: 17444.1398 - mse: 327508896.0000 - mae: 17444.1406 - val_loss: 19470.4673 - val_mse: 390867648.0000 - val_mae: 19470.4688\n",
            "Epoch 82/500\n",
            "50/50 [==============================] - 0s 946us/step - loss: 17727.0908 - mse: 335697792.0000 - mae: 17727.0918 - val_loss: 19321.7473 - val_mse: 385084352.0000 - val_mae: 19321.7480\n",
            "Epoch 83/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 17615.1348 - mse: 332645792.0000 - mae: 17615.1367 - val_loss: 19172.0864 - val_mse: 379309760.0000 - val_mae: 19172.0879\n",
            "Epoch 84/500\n",
            "50/50 [==============================] - 0s 840us/step - loss: 17338.7107 - mse: 324231168.0000 - mae: 17338.7109 - val_loss: 19018.3057 - val_mse: 373419840.0000 - val_mae: 19018.3066\n",
            "Epoch 85/500\n",
            "50/50 [==============================] - 0s 973us/step - loss: 17435.4283 - mse: 327228384.0000 - mae: 17435.4277 - val_loss: 18867.7126 - val_mse: 367701568.0000 - val_mae: 18867.7129\n",
            "Epoch 86/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 17294.5600 - mse: 323413344.0000 - mae: 17294.5605 - val_loss: 18716.4099 - val_mse: 361999424.0000 - val_mae: 18716.4102\n",
            "Epoch 87/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 16782.3748 - mse: 303317408.0000 - mae: 16782.3750 - val_loss: 18555.5198 - val_mse: 355986944.0000 - val_mae: 18555.5195\n",
            "Epoch 88/500\n",
            "50/50 [==============================] - 0s 859us/step - loss: 17478.8189 - mse: 327848544.0000 - mae: 17478.8184 - val_loss: 18411.2000 - val_mse: 350637984.0000 - val_mae: 18411.1992\n",
            "Epoch 89/500\n",
            "50/50 [==============================] - 0s 880us/step - loss: 17158.8568 - mse: 319299616.0000 - mae: 17158.8574 - val_loss: 18261.9900 - val_mse: 345151648.0000 - val_mae: 18261.9883\n",
            "Epoch 90/500\n",
            "50/50 [==============================] - 0s 961us/step - loss: 16716.2141 - mse: 302970688.0000 - mae: 16716.2148 - val_loss: 18104.5132 - val_mse: 339410112.0000 - val_mae: 18104.5137\n",
            "Epoch 91/500\n",
            "50/50 [==============================] - 0s 890us/step - loss: 16869.2936 - mse: 309894496.0000 - mae: 16869.2930 - val_loss: 17951.6523 - val_mse: 333885184.0000 - val_mae: 17951.6523\n",
            "Epoch 92/500\n",
            "50/50 [==============================] - 0s 974us/step - loss: 16485.1238 - mse: 291537376.0000 - mae: 16485.1230 - val_loss: 17792.0281 - val_mse: 328167072.0000 - val_mae: 17792.0273\n",
            "Epoch 93/500\n",
            "50/50 [==============================] - 0s 975us/step - loss: 16080.0203 - mse: 285432128.0000 - mae: 16080.0195 - val_loss: 17625.3687 - val_mse: 322253152.0000 - val_mae: 17625.3691\n",
            "Epoch 94/500\n",
            "50/50 [==============================] - 0s 956us/step - loss: 15635.5045 - mse: 269233408.0000 - mae: 15635.5039 - val_loss: 17452.2061 - val_mse: 316163648.0000 - val_mae: 17452.2070\n",
            "Epoch 95/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 15330.5406 - mse: 261285072.0000 - mae: 15330.5400 - val_loss: 17273.8159 - val_mse: 309957312.0000 - val_mae: 17273.8164\n",
            "Epoch 96/500\n",
            "50/50 [==============================] - 0s 754us/step - loss: 15781.7080 - mse: 272202784.0000 - mae: 15781.7090 - val_loss: 17106.9734 - val_mse: 304209920.0000 - val_mae: 17106.9727\n",
            "Epoch 97/500\n",
            "50/50 [==============================] - 0s 988us/step - loss: 15577.2318 - mse: 267548768.0000 - mae: 15577.2314 - val_loss: 16937.5981 - val_mse: 298432416.0000 - val_mae: 16937.5977\n",
            "Epoch 98/500\n",
            "50/50 [==============================] - 0s 810us/step - loss: 15074.7486 - mse: 254625408.0000 - mae: 15074.7490 - val_loss: 16769.4734 - val_mse: 292753888.0000 - val_mae: 16769.4727\n",
            "Epoch 99/500\n",
            "50/50 [==============================] - 0s 962us/step - loss: 14963.0227 - mse: 248542816.0000 - mae: 14963.0225 - val_loss: 16591.3906 - val_mse: 286800064.0000 - val_mae: 16591.3906\n",
            "Epoch 100/500\n",
            "50/50 [==============================] - 0s 890us/step - loss: 15174.6373 - mse: 255696112.0000 - mae: 15174.6377 - val_loss: 16428.4592 - val_mse: 281407904.0000 - val_mae: 16428.4590\n",
            "Epoch 101/500\n",
            "50/50 [==============================] - 0s 825us/step - loss: 14291.4221 - mse: 225746480.0000 - mae: 14291.4209 - val_loss: 16240.9766 - val_mse: 275272544.0000 - val_mae: 16240.9766\n",
            "Epoch 102/500\n",
            "50/50 [==============================] - 0s 992us/step - loss: 14717.4506 - mse: 241759680.0000 - mae: 14717.4502 - val_loss: 16063.2981 - val_mse: 269523008.0000 - val_mae: 16063.2988\n",
            "Epoch 103/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 14092.3383 - mse: 224635120.0000 - mae: 14092.3389 - val_loss: 15876.4702 - val_mse: 263543504.0000 - val_mae: 15876.4707\n",
            "Epoch 104/500\n",
            "50/50 [==============================] - 0s 938us/step - loss: 14580.2895 - mse: 241040064.0000 - mae: 14580.2900 - val_loss: 15700.3574 - val_mse: 257969712.0000 - val_mae: 15700.3574\n",
            "Epoch 105/500\n",
            "50/50 [==============================] - 0s 875us/step - loss: 14209.5904 - mse: 228032560.0000 - mae: 14209.5898 - val_loss: 15518.4207 - val_mse: 252278784.0000 - val_mae: 15518.4199\n",
            "Epoch 106/500\n",
            "50/50 [==============================] - 0s 943us/step - loss: 14281.3793 - mse: 231357680.0000 - mae: 14281.3789 - val_loss: 15339.3982 - val_mse: 246746064.0000 - val_mae: 15339.3984\n",
            "Epoch 107/500\n",
            "50/50 [==============================] - 0s 938us/step - loss: 13621.8508 - mse: 207792368.0000 - mae: 13621.8516 - val_loss: 15149.9417 - val_mse: 240959312.0000 - val_mae: 15149.9424\n",
            "Epoch 108/500\n",
            "50/50 [==============================] - 0s 839us/step - loss: 13198.6045 - mse: 199691216.0000 - mae: 13198.6055 - val_loss: 14954.7996 - val_mse: 235072928.0000 - val_mae: 14954.7988\n",
            "Epoch 109/500\n",
            "50/50 [==============================] - 0s 978us/step - loss: 13083.7000 - mse: 196669808.0000 - mae: 13083.7002 - val_loss: 14778.7197 - val_mse: 229829024.0000 - val_mae: 14778.7207\n",
            "Epoch 110/500\n",
            "50/50 [==============================] - 0s 790us/step - loss: 12357.3625 - mse: 178076464.0000 - mae: 12357.3623 - val_loss: 14588.9456 - val_mse: 224247136.0000 - val_mae: 14588.9473\n",
            "Epoch 111/500\n",
            "50/50 [==============================] - 0s 760us/step - loss: 12815.6375 - mse: 187250608.0000 - mae: 12815.6377 - val_loss: 14400.0864 - val_mse: 218764320.0000 - val_mae: 14400.0859\n",
            "Epoch 112/500\n",
            "50/50 [==============================] - 0s 987us/step - loss: 13454.7350 - mse: 210615520.0000 - mae: 13454.7354 - val_loss: 14215.9985 - val_mse: 213486944.0000 - val_mae: 14215.9980\n",
            "Epoch 113/500\n",
            "50/50 [==============================] - 0s 868us/step - loss: 12471.4832 - mse: 178085968.0000 - mae: 12471.4824 - val_loss: 14025.2451 - val_mse: 208090928.0000 - val_mae: 14025.2451\n",
            "Epoch 114/500\n",
            "50/50 [==============================] - 0s 745us/step - loss: 12945.2799 - mse: 195875312.0000 - mae: 12945.2803 - val_loss: 13843.3274 - val_mse: 203012752.0000 - val_mae: 13843.3281\n",
            "Epoch 115/500\n",
            "50/50 [==============================] - 0s 887us/step - loss: 11564.5574 - mse: 155869456.0000 - mae: 11564.5576 - val_loss: 13647.4883 - val_mse: 197621536.0000 - val_mae: 13647.4893\n",
            "Epoch 116/500\n",
            "50/50 [==============================] - 0s 869us/step - loss: 12550.4000 - mse: 184011856.0000 - mae: 12550.4004 - val_loss: 13463.9619 - val_mse: 192638720.0000 - val_mae: 13463.9629\n",
            "Epoch 117/500\n",
            "50/50 [==============================] - 0s 870us/step - loss: 12357.4002 - mse: 181890336.0000 - mae: 12357.4004 - val_loss: 13275.7585 - val_mse: 187596672.0000 - val_mae: 13275.7598\n",
            "Epoch 118/500\n",
            "50/50 [==============================] - 0s 810us/step - loss: 11798.7873 - mse: 164183936.0000 - mae: 11798.7871 - val_loss: 13088.5005 - val_mse: 182651808.0000 - val_mae: 13088.5010\n",
            "Epoch 119/500\n",
            "50/50 [==============================] - 0s 718us/step - loss: 11579.9221 - mse: 158358944.0000 - mae: 11579.9229 - val_loss: 12897.6152 - val_mse: 177685840.0000 - val_mae: 12897.6152\n",
            "Epoch 120/500\n",
            "50/50 [==============================] - 0s 768us/step - loss: 11381.6398 - mse: 160139936.0000 - mae: 11381.6396 - val_loss: 12694.3274 - val_mse: 172471520.0000 - val_mae: 12694.3262\n",
            "Epoch 121/500\n",
            "50/50 [==============================] - 0s 796us/step - loss: 11691.9859 - mse: 157553936.0000 - mae: 11691.9863 - val_loss: 12517.5640 - val_mse: 168009392.0000 - val_mae: 12517.5645\n",
            "Epoch 122/500\n",
            "50/50 [==============================] - 0s 887us/step - loss: 10178.6971 - mse: 129138968.0000 - mae: 10178.6973 - val_loss: 12303.3953 - val_mse: 162684592.0000 - val_mae: 12303.3955\n",
            "Epoch 123/500\n",
            "50/50 [==============================] - 0s 825us/step - loss: 11538.7705 - mse: 161515024.0000 - mae: 11538.7695 - val_loss: 12113.6360 - val_mse: 158044880.0000 - val_mae: 12113.6357\n",
            "Epoch 124/500\n",
            "50/50 [==============================] - 0s 840us/step - loss: 10529.1609 - mse: 133739440.0000 - mae: 10529.1611 - val_loss: 11909.8225 - val_mse: 153142944.0000 - val_mae: 11909.8223\n",
            "Epoch 125/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 11286.8602 - mse: 156072768.0000 - mae: 11286.8604 - val_loss: 11722.0486 - val_mse: 148696704.0000 - val_mae: 11722.0488\n",
            "Epoch 126/500\n",
            "50/50 [==============================] - 0s 856us/step - loss: 10158.7904 - mse: 129340352.0000 - mae: 10158.7910 - val_loss: 11537.6580 - val_mse: 144404320.0000 - val_mae: 11537.6582\n",
            "Epoch 127/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 9604.2629 - mse: 126240688.0000 - mae: 9604.2637 - val_loss: 11343.5349 - val_mse: 139956400.0000 - val_mae: 11343.5352\n",
            "Epoch 128/500\n",
            "50/50 [==============================] - 0s 878us/step - loss: 10273.0809 - mse: 132856680.0000 - mae: 10273.0811 - val_loss: 11147.8973 - val_mse: 135551248.0000 - val_mae: 11147.8975\n",
            "Epoch 129/500\n",
            "50/50 [==============================] - 0s 969us/step - loss: 9420.0260 - mse: 114276280.0000 - mae: 9420.0264 - val_loss: 10950.0898 - val_mse: 131175896.0000 - val_mae: 10950.0908\n",
            "Epoch 130/500\n",
            "50/50 [==============================] - 0s 866us/step - loss: 10472.9635 - mse: 130785464.0000 - mae: 10472.9639 - val_loss: 10758.1820 - val_mse: 127004720.0000 - val_mae: 10758.1826\n",
            "Epoch 131/500\n",
            "50/50 [==============================] - 0s 893us/step - loss: 9952.6168 - mse: 116936112.0000 - mae: 9952.6172 - val_loss: 10551.7430 - val_mse: 122602048.0000 - val_mae: 10551.7432\n",
            "Epoch 132/500\n",
            "50/50 [==============================] - 0s 920us/step - loss: 8951.2494 - mse: 103711184.0000 - mae: 8951.2490 - val_loss: 10364.9778 - val_mse: 118693208.0000 - val_mae: 10364.9785\n",
            "Epoch 133/500\n",
            "50/50 [==============================] - 0s 879us/step - loss: 8221.3756 - mse: 93449280.0000 - mae: 8221.3760 - val_loss: 10158.0848 - val_mse: 114444520.0000 - val_mae: 10158.0850\n",
            "Epoch 134/500\n",
            "50/50 [==============================] - 0s 906us/step - loss: 8621.7041 - mse: 98836808.0000 - mae: 8621.7041 - val_loss: 9976.1908 - val_mse: 110774896.0000 - val_mae: 9976.1904\n",
            "Epoch 135/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 9197.0989 - mse: 110676992.0000 - mae: 9197.0996 - val_loss: 9778.8782 - val_mse: 106872840.0000 - val_mae: 9778.8779\n",
            "Epoch 136/500\n",
            "50/50 [==============================] - 0s 955us/step - loss: 8131.2277 - mse: 85543360.0000 - mae: 8131.2275 - val_loss: 9579.7969 - val_mse: 103014512.0000 - val_mae: 9579.7969\n",
            "Epoch 137/500\n",
            "50/50 [==============================] - 0s 870us/step - loss: 8094.1935 - mse: 88604896.0000 - mae: 8094.1934 - val_loss: 9401.4972 - val_mse: 99628776.0000 - val_mae: 9401.4980\n",
            "Epoch 138/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 9157.9854 - mse: 103303616.0000 - mae: 9157.9854 - val_loss: 9207.9260 - val_mse: 96022000.0000 - val_mae: 9207.9268\n",
            "Epoch 139/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 8639.6360 - mse: 96804160.0000 - mae: 8639.6367 - val_loss: 9006.9614 - val_mse: 92359512.0000 - val_mae: 9006.9619\n",
            "Epoch 140/500\n",
            "50/50 [==============================] - 0s 944us/step - loss: 8383.1740 - mse: 96849128.0000 - mae: 8383.1748 - val_loss: 8814.1853 - val_mse: 88920792.0000 - val_mae: 8814.1846\n",
            "Epoch 141/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 7484.6465 - mse: 76148752.0000 - mae: 7484.6465 - val_loss: 8589.3586 - val_mse: 85000872.0000 - val_mae: 8589.3584\n",
            "Epoch 142/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 7768.8126 - mse: 86441552.0000 - mae: 7768.8125 - val_loss: 8385.5397 - val_mse: 81537672.0000 - val_mae: 8385.5400\n",
            "Epoch 143/500\n",
            "50/50 [==============================] - 0s 915us/step - loss: 7337.1983 - mse: 73036088.0000 - mae: 7337.1973 - val_loss: 8195.0760 - val_mse: 78378080.0000 - val_mae: 8195.0762\n",
            "Epoch 144/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6463.2200 - mse: 60741840.0000 - mae: 6463.2202 - val_loss: 7967.1260 - val_mse: 74688400.0000 - val_mae: 7967.1260\n",
            "Epoch 145/500\n",
            "50/50 [==============================] - 0s 950us/step - loss: 7911.8434 - mse: 85598928.0000 - mae: 7911.8433 - val_loss: 7807.3452 - val_mse: 72168136.0000 - val_mae: 7807.3452\n",
            "Epoch 146/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 7467.4793 - mse: 80403224.0000 - mae: 7467.4795 - val_loss: 7662.4143 - val_mse: 69926128.0000 - val_mae: 7662.4141\n",
            "Epoch 147/500\n",
            "50/50 [==============================] - 0s 954us/step - loss: 7312.2847 - mse: 72457336.0000 - mae: 7312.2842 - val_loss: 7484.3981 - val_mse: 67226448.0000 - val_mae: 7484.3984\n",
            "Epoch 148/500\n",
            "50/50 [==============================] - 0s 847us/step - loss: 6686.3004 - mse: 61564592.0000 - mae: 6686.2998 - val_loss: 7312.7664 - val_mse: 64682856.0000 - val_mae: 7312.7666\n",
            "Epoch 149/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6787.5892 - mse: 67478496.0000 - mae: 6787.5889 - val_loss: 7090.8113 - val_mse: 61479180.0000 - val_mae: 7090.8110\n",
            "Epoch 150/500\n",
            "50/50 [==============================] - 0s 960us/step - loss: 7004.4562 - mse: 74555432.0000 - mae: 7004.4561 - val_loss: 6964.5724 - val_mse: 59705624.0000 - val_mae: 6964.5728\n",
            "Epoch 151/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6131.3991 - mse: 55587952.0000 - mae: 6131.3994 - val_loss: 6847.5425 - val_mse: 58086104.0000 - val_mae: 6847.5420\n",
            "Epoch 152/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6599.7071 - mse: 65126000.0000 - mae: 6599.7070 - val_loss: 6666.3962 - val_mse: 55636408.0000 - val_mae: 6666.3970\n",
            "Epoch 153/500\n",
            "50/50 [==============================] - 0s 955us/step - loss: 5999.8255 - mse: 53767192.0000 - mae: 5999.8257 - val_loss: 6530.0118 - val_mse: 53833592.0000 - val_mae: 6530.0117\n",
            "Epoch 154/500\n",
            "50/50 [==============================] - 0s 982us/step - loss: 6542.3930 - mse: 66619176.0000 - mae: 6542.3926 - val_loss: 6348.3104 - val_mse: 51493448.0000 - val_mae: 6348.3105\n",
            "Epoch 155/500\n",
            "50/50 [==============================] - 0s 741us/step - loss: 5974.6015 - mse: 59603972.0000 - mae: 5974.6011 - val_loss: 6231.2913 - val_mse: 50023192.0000 - val_mae: 6231.2915\n",
            "Epoch 156/500\n",
            "50/50 [==============================] - 0s 862us/step - loss: 6140.8105 - mse: 54776116.0000 - mae: 6140.8105 - val_loss: 6134.7853 - val_mse: 48828752.0000 - val_mae: 6134.7856\n",
            "Epoch 157/500\n",
            "50/50 [==============================] - 0s 950us/step - loss: 6342.0757 - mse: 62445416.0000 - mae: 6342.0757 - val_loss: 6010.7282 - val_mse: 47322232.0000 - val_mae: 6010.7280\n",
            "Epoch 158/500\n",
            "50/50 [==============================] - 0s 930us/step - loss: 6295.1305 - mse: 56301744.0000 - mae: 6295.1299 - val_loss: 5899.5715 - val_mse: 46004900.0000 - val_mae: 5899.5713\n",
            "Epoch 159/500\n",
            "50/50 [==============================] - 0s 893us/step - loss: 5384.4005 - mse: 45579344.0000 - mae: 5384.3999 - val_loss: 5753.1785 - val_mse: 44294608.0000 - val_mae: 5753.1787\n",
            "Epoch 160/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6514.9071 - mse: 64773816.0000 - mae: 6514.9077 - val_loss: 5637.1959 - val_mse: 42966536.0000 - val_mae: 5637.1958\n",
            "Epoch 161/500\n",
            "50/50 [==============================] - 0s 964us/step - loss: 5926.9142 - mse: 51314580.0000 - mae: 5926.9146 - val_loss: 5515.7612 - val_mse: 41609492.0000 - val_mae: 5515.7607\n",
            "Epoch 162/500\n",
            "50/50 [==============================] - 0s 903us/step - loss: 6188.3554 - mse: 59052196.0000 - mae: 6188.3550 - val_loss: 5357.1012 - val_mse: 39881512.0000 - val_mae: 5357.1011\n",
            "Epoch 163/500\n",
            "50/50 [==============================] - 0s 969us/step - loss: 6835.4394 - mse: 62029672.0000 - mae: 6835.4395 - val_loss: 5263.8912 - val_mse: 38896188.0000 - val_mae: 5263.8911\n",
            "Epoch 164/500\n",
            "50/50 [==============================] - 0s 860us/step - loss: 5730.2521 - mse: 48634224.0000 - mae: 5730.2524 - val_loss: 5122.8678 - val_mse: 37427312.0000 - val_mae: 5122.8682\n",
            "Epoch 165/500\n",
            "50/50 [==============================] - 0s 720us/step - loss: 5479.9766 - mse: 43253308.0000 - mae: 5479.9761 - val_loss: 4979.4137 - val_mse: 35977944.0000 - val_mae: 4979.4136\n",
            "Epoch 166/500\n",
            "50/50 [==============================] - 0s 817us/step - loss: 6692.7367 - mse: 64398276.0000 - mae: 6692.7363 - val_loss: 4883.5096 - val_mse: 35033304.0000 - val_mae: 4883.5098\n",
            "Epoch 167/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6454.2765 - mse: 62533576.0000 - mae: 6454.2764 - val_loss: 4726.0526 - val_mse: 33520918.0000 - val_mae: 4726.0527\n",
            "Epoch 168/500\n",
            "50/50 [==============================] - 0s 846us/step - loss: 5168.7590 - mse: 44652472.0000 - mae: 5168.7588 - val_loss: 4614.6564 - val_mse: 32488124.0000 - val_mae: 4614.6567\n",
            "Epoch 169/500\n",
            "50/50 [==============================] - 0s 924us/step - loss: 5835.5119 - mse: 54813912.0000 - mae: 5835.5117 - val_loss: 4554.6757 - val_mse: 31926716.0000 - val_mae: 4554.6758\n",
            "Epoch 170/500\n",
            "50/50 [==============================] - 0s 920us/step - loss: 6807.0003 - mse: 75793928.0000 - mae: 6807.0000 - val_loss: 4442.3100 - val_mse: 30866884.0000 - val_mae: 4442.3096\n",
            "Epoch 171/500\n",
            "50/50 [==============================] - 0s 964us/step - loss: 5742.8587 - mse: 52650056.0000 - mae: 5742.8589 - val_loss: 4375.7068 - val_mse: 30247924.0000 - val_mae: 4375.7065\n",
            "Epoch 172/500\n",
            "50/50 [==============================] - 0s 854us/step - loss: 6659.0806 - mse: 63247484.0000 - mae: 6659.0801 - val_loss: 4303.8819 - val_mse: 29601920.0000 - val_mae: 4303.8818\n",
            "Epoch 173/500\n",
            "50/50 [==============================] - 0s 906us/step - loss: 4866.2475 - mse: 41950900.0000 - mae: 4866.2476 - val_loss: 4215.0454 - val_mse: 28808810.0000 - val_mae: 4215.0454\n",
            "Epoch 174/500\n",
            "50/50 [==============================] - 0s 849us/step - loss: 5132.1298 - mse: 37021912.0000 - mae: 5132.1294 - val_loss: 4141.2989 - val_mse: 28161344.0000 - val_mae: 4141.2988\n",
            "Epoch 175/500\n",
            "50/50 [==============================] - 0s 773us/step - loss: 5697.9453 - mse: 46073672.0000 - mae: 5697.9448 - val_loss: 4028.4474 - val_mse: 27188128.0000 - val_mae: 4028.4473\n",
            "Epoch 176/500\n",
            "50/50 [==============================] - 0s 949us/step - loss: 5774.7642 - mse: 43469636.0000 - mae: 5774.7642 - val_loss: 3949.8067 - val_mse: 26465164.0000 - val_mae: 3949.8066\n",
            "Epoch 177/500\n",
            "50/50 [==============================] - 0s 784us/step - loss: 5644.1303 - mse: 45790196.0000 - mae: 5644.1299 - val_loss: 3908.7869 - val_mse: 26101140.0000 - val_mae: 3908.7866\n",
            "Epoch 178/500\n",
            "50/50 [==============================] - 0s 888us/step - loss: 5884.8377 - mse: 51897108.0000 - mae: 5884.8374 - val_loss: 3834.7125 - val_mse: 25435366.0000 - val_mae: 3834.7129\n",
            "Epoch 179/500\n",
            "50/50 [==============================] - 0s 889us/step - loss: 6163.6608 - mse: 54018976.0000 - mae: 6163.6606 - val_loss: 3743.3266 - val_mse: 24610914.0000 - val_mae: 3743.3269\n",
            "Epoch 180/500\n",
            "50/50 [==============================] - 0s 778us/step - loss: 7288.6102 - mse: 84319720.0000 - mae: 7288.6099 - val_loss: 3750.4569 - val_mse: 24688106.0000 - val_mae: 3750.4565\n",
            "Epoch 181/500\n",
            "50/50 [==============================] - 0s 841us/step - loss: 5462.0758 - mse: 46657312.0000 - mae: 5462.0757 - val_loss: 3706.6375 - val_mse: 24262686.0000 - val_mae: 3706.6375\n",
            "Epoch 182/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5622.5675 - mse: 52038092.0000 - mae: 5622.5674 - val_loss: 3697.9781 - val_mse: 24182460.0000 - val_mae: 3697.9780\n",
            "Epoch 183/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5668.5801 - mse: 45977692.0000 - mae: 5668.5801 - val_loss: 3673.8680 - val_mse: 23940262.0000 - val_mae: 3673.8679\n",
            "Epoch 184/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5537.5888 - mse: 39387260.0000 - mae: 5537.5889 - val_loss: 3584.1097 - val_mse: 23041346.0000 - val_mae: 3584.1099\n",
            "Epoch 185/500\n",
            "50/50 [==============================] - 0s 943us/step - loss: 6386.4368 - mse: 57873812.0000 - mae: 6386.4370 - val_loss: 3552.4928 - val_mse: 22685180.0000 - val_mae: 3552.4927\n",
            "Epoch 186/500\n",
            "50/50 [==============================] - 0s 844us/step - loss: 5117.9350 - mse: 46866052.0000 - mae: 5117.9351 - val_loss: 3531.3191 - val_mse: 22441564.0000 - val_mae: 3531.3191\n",
            "Epoch 187/500\n",
            "50/50 [==============================] - 0s 922us/step - loss: 5412.0477 - mse: 45217184.0000 - mae: 5412.0474 - val_loss: 3493.7370 - val_mse: 21991192.0000 - val_mae: 3493.7371\n",
            "Epoch 188/500\n",
            "50/50 [==============================] - 0s 981us/step - loss: 5523.4793 - mse: 51950460.0000 - mae: 5523.4795 - val_loss: 3463.1760 - val_mse: 21635844.0000 - val_mae: 3463.1758\n",
            "Epoch 189/500\n",
            "50/50 [==============================] - 0s 861us/step - loss: 5315.8819 - mse: 51474728.0000 - mae: 5315.8818 - val_loss: 3467.5764 - val_mse: 21686766.0000 - val_mae: 3467.5767\n",
            "Epoch 190/500\n",
            "50/50 [==============================] - 0s 787us/step - loss: 5034.5427 - mse: 39396660.0000 - mae: 5034.5425 - val_loss: 3486.9183 - val_mse: 21921628.0000 - val_mae: 3486.9185\n",
            "Epoch 191/500\n",
            "50/50 [==============================] - 0s 808us/step - loss: 5231.4708 - mse: 41553148.0000 - mae: 5231.4712 - val_loss: 3418.7352 - val_mse: 21084560.0000 - val_mae: 3418.7351\n",
            "Epoch 192/500\n",
            "50/50 [==============================] - 0s 822us/step - loss: 5379.4065 - mse: 45115148.0000 - mae: 5379.4067 - val_loss: 3435.6591 - val_mse: 21301772.0000 - val_mae: 3435.6589\n",
            "Epoch 193/500\n",
            "50/50 [==============================] - 0s 913us/step - loss: 5570.1360 - mse: 60859168.0000 - mae: 5570.1357 - val_loss: 3432.7063 - val_mse: 21266076.0000 - val_mae: 3432.7065\n",
            "Epoch 194/500\n",
            "50/50 [==============================] - 0s 898us/step - loss: 5731.6679 - mse: 50495152.0000 - mae: 5731.6680 - val_loss: 3427.6030 - val_mse: 21203850.0000 - val_mae: 3427.6030\n",
            "Epoch 195/500\n",
            "50/50 [==============================] - 0s 981us/step - loss: 5818.8942 - mse: 51326768.0000 - mae: 5818.8945 - val_loss: 3392.7862 - val_mse: 20772516.0000 - val_mae: 3392.7864\n",
            "Epoch 196/500\n",
            "50/50 [==============================] - 0s 875us/step - loss: 5312.3806 - mse: 45238836.0000 - mae: 5312.3809 - val_loss: 3391.7307 - val_mse: 20763306.0000 - val_mae: 3391.7310\n",
            "Epoch 197/500\n",
            "50/50 [==============================] - 0s 884us/step - loss: 5355.8351 - mse: 45249340.0000 - mae: 5355.8350 - val_loss: 3378.9122 - val_mse: 20606364.0000 - val_mae: 3378.9121\n",
            "Epoch 198/500\n",
            "50/50 [==============================] - 0s 999us/step - loss: 5072.2771 - mse: 38500428.0000 - mae: 5072.2773 - val_loss: 3343.2847 - val_mse: 20131482.0000 - val_mae: 3343.2847\n",
            "Epoch 199/500\n",
            "50/50 [==============================] - 0s 881us/step - loss: 6087.4515 - mse: 54780752.0000 - mae: 6087.4512 - val_loss: 3346.9362 - val_mse: 20181476.0000 - val_mae: 3346.9360\n",
            "Epoch 200/500\n",
            "50/50 [==============================] - 0s 939us/step - loss: 5745.7481 - mse: 55367436.0000 - mae: 5745.7480 - val_loss: 3320.7273 - val_mse: 19770556.0000 - val_mae: 3320.7273\n",
            "Epoch 201/500\n",
            "50/50 [==============================] - 0s 944us/step - loss: 5938.2674 - mse: 56500076.0000 - mae: 5938.2676 - val_loss: 3342.8975 - val_mse: 20129298.0000 - val_mae: 3342.8977\n",
            "Epoch 202/500\n",
            "50/50 [==============================] - 0s 949us/step - loss: 5612.6871 - mse: 42465128.0000 - mae: 5612.6870 - val_loss: 3362.3001 - val_mse: 20390706.0000 - val_mae: 3362.3000\n",
            "Epoch 203/500\n",
            "50/50 [==============================] - 0s 957us/step - loss: 5517.8997 - mse: 49435960.0000 - mae: 5517.8999 - val_loss: 3334.1310 - val_mse: 19994322.0000 - val_mae: 3334.1309\n",
            "Epoch 204/500\n",
            "50/50 [==============================] - 0s 969us/step - loss: 6046.3143 - mse: 52053800.0000 - mae: 6046.3145 - val_loss: 3358.9853 - val_mse: 20350002.0000 - val_mae: 3358.9856\n",
            "Epoch 205/500\n",
            "50/50 [==============================] - 0s 943us/step - loss: 5200.8258 - mse: 47187328.0000 - mae: 5200.8257 - val_loss: 3359.0990 - val_mse: 20352478.0000 - val_mae: 3359.0989\n",
            "Epoch 206/500\n",
            "50/50 [==============================] - 0s 916us/step - loss: 6149.4952 - mse: 54157344.0000 - mae: 6149.4951 - val_loss: 3373.9958 - val_mse: 20553150.0000 - val_mae: 3373.9961\n",
            "Epoch 207/500\n",
            "50/50 [==============================] - 0s 917us/step - loss: 5832.6310 - mse: 47917244.0000 - mae: 5832.6313 - val_loss: 3359.3845 - val_mse: 20358472.0000 - val_mae: 3359.3843\n",
            "Epoch 208/500\n",
            "50/50 [==============================] - 0s 835us/step - loss: 4797.6163 - mse: 35187504.0000 - mae: 4797.6167 - val_loss: 3347.7404 - val_mse: 20196332.0000 - val_mae: 3347.7407\n",
            "Epoch 209/500\n",
            "50/50 [==============================] - 0s 946us/step - loss: 5119.5434 - mse: 38140228.0000 - mae: 5119.5435 - val_loss: 3339.1882 - val_mse: 20069622.0000 - val_mae: 3339.1882\n",
            "Epoch 210/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5119.4032 - mse: 38382048.0000 - mae: 5119.4033 - val_loss: 3320.4852 - val_mse: 19757144.0000 - val_mae: 3320.4851\n",
            "Epoch 211/500\n",
            "50/50 [==============================] - 0s 963us/step - loss: 4895.2944 - mse: 38612136.0000 - mae: 4895.2944 - val_loss: 3306.7383 - val_mse: 19495500.0000 - val_mae: 3306.7383\n",
            "Epoch 212/500\n",
            "50/50 [==============================] - 0s 778us/step - loss: 5705.0003 - mse: 51376292.0000 - mae: 5705.0005 - val_loss: 3312.7034 - val_mse: 19607340.0000 - val_mae: 3312.7036\n",
            "Epoch 213/500\n",
            "50/50 [==============================] - 0s 931us/step - loss: 5371.7341 - mse: 46256500.0000 - mae: 5371.7339 - val_loss: 3322.9819 - val_mse: 19802972.0000 - val_mae: 3322.9819\n",
            "Epoch 214/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5461.9398 - mse: 45139744.0000 - mae: 5461.9399 - val_loss: 3338.5791 - val_mse: 20057486.0000 - val_mae: 3338.5793\n",
            "Epoch 215/500\n",
            "50/50 [==============================] - 0s 846us/step - loss: 5601.4215 - mse: 49379368.0000 - mae: 5601.4214 - val_loss: 3352.6961 - val_mse: 20264402.0000 - val_mae: 3352.6960\n",
            "Epoch 216/500\n",
            "50/50 [==============================] - 0s 988us/step - loss: 5344.5875 - mse: 42980084.0000 - mae: 5344.5874 - val_loss: 3345.6893 - val_mse: 20161082.0000 - val_mae: 3345.6890\n",
            "Epoch 217/500\n",
            "50/50 [==============================] - 0s 757us/step - loss: 5621.8268 - mse: 44211024.0000 - mae: 5621.8267 - val_loss: 3364.2810 - val_mse: 20433556.0000 - val_mae: 3364.2812\n",
            "Epoch 218/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5327.7414 - mse: 49785268.0000 - mae: 5327.7412 - val_loss: 3342.5300 - val_mse: 20112044.0000 - val_mae: 3342.5300\n",
            "Epoch 219/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5965.2482 - mse: 63175740.0000 - mae: 5965.2485 - val_loss: 3340.4494 - val_mse: 20081790.0000 - val_mae: 3340.4492\n",
            "Epoch 220/500\n",
            "50/50 [==============================] - 0s 829us/step - loss: 5771.9513 - mse: 48445244.0000 - mae: 5771.9512 - val_loss: 3331.1189 - val_mse: 19931004.0000 - val_mae: 3331.1187\n",
            "Epoch 221/500\n",
            "50/50 [==============================] - 0s 869us/step - loss: 5415.2843 - mse: 45693728.0000 - mae: 5415.2842 - val_loss: 3329.1475 - val_mse: 19897630.0000 - val_mae: 3329.1477\n",
            "Epoch 222/500\n",
            "50/50 [==============================] - 0s 983us/step - loss: 4892.8647 - mse: 37106176.0000 - mae: 4892.8652 - val_loss: 3333.6582 - val_mse: 19972994.0000 - val_mae: 3333.6582\n",
            "Epoch 223/500\n",
            "50/50 [==============================] - 0s 857us/step - loss: 5670.9132 - mse: 49922708.0000 - mae: 5670.9131 - val_loss: 3335.7455 - val_mse: 20009004.0000 - val_mae: 3335.7456\n",
            "Epoch 224/500\n",
            "50/50 [==============================] - 0s 902us/step - loss: 6889.2514 - mse: 71707520.0000 - mae: 6889.2515 - val_loss: 3342.7687 - val_mse: 20112916.0000 - val_mae: 3342.7688\n",
            "Epoch 225/500\n",
            "50/50 [==============================] - 0s 926us/step - loss: 5509.3630 - mse: 47278132.0000 - mae: 5509.3633 - val_loss: 3312.7918 - val_mse: 19603724.0000 - val_mae: 3312.7917\n",
            "Epoch 226/500\n",
            "50/50 [==============================] - 0s 952us/step - loss: 6173.5540 - mse: 57595032.0000 - mae: 6173.5537 - val_loss: 3309.1375 - val_mse: 19531984.0000 - val_mae: 3309.1375\n",
            "Epoch 227/500\n",
            "50/50 [==============================] - 0s 972us/step - loss: 5098.3144 - mse: 40058348.0000 - mae: 5098.3140 - val_loss: 3305.6372 - val_mse: 19464114.0000 - val_mae: 3305.6372\n",
            "Epoch 228/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5471.1877 - mse: 54924208.0000 - mae: 5471.1875 - val_loss: 3297.2810 - val_mse: 19308444.0000 - val_mae: 3297.2808\n",
            "Epoch 229/500\n",
            "50/50 [==============================] - 0s 940us/step - loss: 5759.6905 - mse: 46075652.0000 - mae: 5759.6904 - val_loss: 3298.2439 - val_mse: 19326448.0000 - val_mae: 3298.2437\n",
            "Epoch 230/500\n",
            "50/50 [==============================] - 0s 829us/step - loss: 4900.6475 - mse: 36182704.0000 - mae: 4900.6470 - val_loss: 3293.6346 - val_mse: 19239756.0000 - val_mae: 3293.6343\n",
            "Epoch 231/500\n",
            "50/50 [==============================] - 0s 919us/step - loss: 5280.8294 - mse: 42083580.0000 - mae: 5280.8296 - val_loss: 3290.5086 - val_mse: 19181510.0000 - val_mae: 3290.5085\n",
            "Epoch 232/500\n",
            "50/50 [==============================] - 0s 965us/step - loss: 5397.7179 - mse: 48844836.0000 - mae: 5397.7173 - val_loss: 3282.3807 - val_mse: 19031884.0000 - val_mae: 3282.3809\n",
            "Epoch 233/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5032.9325 - mse: 39615052.0000 - mae: 5032.9326 - val_loss: 3288.2056 - val_mse: 19138702.0000 - val_mae: 3288.2058\n",
            "Epoch 234/500\n",
            "50/50 [==============================] - 0s 854us/step - loss: 6048.7077 - mse: 63983128.0000 - mae: 6048.7075 - val_loss: 3276.3493 - val_mse: 18922820.0000 - val_mae: 3276.3491\n",
            "Epoch 235/500\n",
            "50/50 [==============================] - 0s 874us/step - loss: 7183.9659 - mse: 72293976.0000 - mae: 7183.9658 - val_loss: 3261.9172 - val_mse: 18626452.0000 - val_mae: 3261.9172\n",
            "Epoch 236/500\n",
            "50/50 [==============================] - 0s 945us/step - loss: 4666.4204 - mse: 39519196.0000 - mae: 4666.4204 - val_loss: 3259.4008 - val_mse: 18570018.0000 - val_mae: 3259.4009\n",
            "Epoch 237/500\n",
            "50/50 [==============================] - 0s 995us/step - loss: 6868.3479 - mse: 64169952.0000 - mae: 6868.3477 - val_loss: 3262.0052 - val_mse: 18632052.0000 - val_mae: 3262.0054\n",
            "Epoch 238/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5948.2168 - mse: 53964392.0000 - mae: 5948.2168 - val_loss: 3265.5723 - val_mse: 18709620.0000 - val_mae: 3265.5723\n",
            "Epoch 239/500\n",
            "50/50 [==============================] - 0s 777us/step - loss: 6507.7785 - mse: 62296192.0000 - mae: 6507.7788 - val_loss: 3273.2806 - val_mse: 18871352.0000 - val_mae: 3273.2808\n",
            "Epoch 240/500\n",
            "50/50 [==============================] - 0s 750us/step - loss: 5484.1551 - mse: 48114780.0000 - mae: 5484.1548 - val_loss: 3263.2992 - val_mse: 18662972.0000 - val_mae: 3263.2993\n",
            "Epoch 241/500\n",
            "50/50 [==============================] - 0s 863us/step - loss: 5833.1072 - mse: 52640460.0000 - mae: 5833.1074 - val_loss: 3252.7076 - val_mse: 18399874.0000 - val_mae: 3252.7075\n",
            "Epoch 242/500\n",
            "50/50 [==============================] - 0s 992us/step - loss: 4916.5647 - mse: 40982864.0000 - mae: 4916.5645 - val_loss: 3252.4978 - val_mse: 18396066.0000 - val_mae: 3252.4978\n",
            "Epoch 243/500\n",
            "50/50 [==============================] - 0s 881us/step - loss: 5219.8722 - mse: 41362400.0000 - mae: 5219.8721 - val_loss: 3256.9105 - val_mse: 18511382.0000 - val_mae: 3256.9106\n",
            "Epoch 244/500\n",
            "50/50 [==============================] - 0s 995us/step - loss: 6306.3759 - mse: 51917312.0000 - mae: 6306.3765 - val_loss: 3257.4767 - val_mse: 18529892.0000 - val_mae: 3257.4771\n",
            "Epoch 245/500\n",
            "50/50 [==============================] - 0s 932us/step - loss: 6564.8082 - mse: 63213244.0000 - mae: 6564.8086 - val_loss: 3252.5129 - val_mse: 18400662.0000 - val_mae: 3252.5129\n",
            "Epoch 246/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5154.4324 - mse: 46323952.0000 - mae: 5154.4326 - val_loss: 3257.3626 - val_mse: 18531572.0000 - val_mae: 3257.3628\n",
            "Epoch 247/500\n",
            "50/50 [==============================] - 0s 921us/step - loss: 4837.0158 - mse: 34399792.0000 - mae: 4837.0161 - val_loss: 3253.4966 - val_mse: 18431440.0000 - val_mae: 3253.4966\n",
            "Epoch 248/500\n",
            "50/50 [==============================] - 0s 985us/step - loss: 5115.9153 - mse: 40132236.0000 - mae: 5115.9155 - val_loss: 3244.5780 - val_mse: 18162236.0000 - val_mae: 3244.5781\n",
            "Epoch 249/500\n",
            "50/50 [==============================] - 0s 904us/step - loss: 6342.5490 - mse: 65894088.0000 - mae: 6342.5488 - val_loss: 3241.5486 - val_mse: 18056224.0000 - val_mae: 3241.5486\n",
            "Epoch 250/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5809.9645 - mse: 56459352.0000 - mae: 5809.9644 - val_loss: 3240.0850 - val_mse: 18003620.0000 - val_mae: 3240.0852\n",
            "Epoch 251/500\n",
            "50/50 [==============================] - 0s 829us/step - loss: 5634.5941 - mse: 57711648.0000 - mae: 5634.5938 - val_loss: 3242.6722 - val_mse: 18085962.0000 - val_mae: 3242.6724\n",
            "Epoch 252/500\n",
            "50/50 [==============================] - 0s 979us/step - loss: 5525.9689 - mse: 48434520.0000 - mae: 5525.9688 - val_loss: 3248.1397 - val_mse: 18268622.0000 - val_mae: 3248.1399\n",
            "Epoch 253/500\n",
            "50/50 [==============================] - 0s 811us/step - loss: 5344.1112 - mse: 43373912.0000 - mae: 5344.1113 - val_loss: 3250.7439 - val_mse: 18364004.0000 - val_mae: 3250.7439\n",
            "Epoch 254/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5439.9634 - mse: 50885556.0000 - mae: 5439.9639 - val_loss: 3253.7929 - val_mse: 18447946.0000 - val_mae: 3253.7925\n",
            "Epoch 255/500\n",
            "50/50 [==============================] - 0s 945us/step - loss: 5260.9685 - mse: 43759612.0000 - mae: 5260.9688 - val_loss: 3252.0350 - val_mse: 18404380.0000 - val_mae: 3252.0349\n",
            "Epoch 256/500\n",
            "50/50 [==============================] - 0s 871us/step - loss: 6374.4256 - mse: 63840184.0000 - mae: 6374.4258 - val_loss: 3261.3075 - val_mse: 18637462.0000 - val_mae: 3261.3076\n",
            "Epoch 257/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5956.9932 - mse: 52766004.0000 - mae: 5956.9932 - val_loss: 3259.2933 - val_mse: 18592192.0000 - val_mae: 3259.2935\n",
            "Epoch 258/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5995.0658 - mse: 59745640.0000 - mae: 5995.0654 - val_loss: 3254.7632 - val_mse: 18472128.0000 - val_mae: 3254.7632\n",
            "Epoch 259/500\n",
            "50/50 [==============================] - 0s 864us/step - loss: 4876.0153 - mse: 37994768.0000 - mae: 4876.0151 - val_loss: 3251.6633 - val_mse: 18375770.0000 - val_mae: 3251.6633\n",
            "Epoch 260/500\n",
            "50/50 [==============================] - 0s 897us/step - loss: 6413.1927 - mse: 57344536.0000 - mae: 6413.1924 - val_loss: 3248.5010 - val_mse: 18265932.0000 - val_mae: 3248.5010\n",
            "Epoch 261/500\n",
            "50/50 [==============================] - 0s 789us/step - loss: 5756.6509 - mse: 57551528.0000 - mae: 5756.6504 - val_loss: 3244.4599 - val_mse: 18128806.0000 - val_mae: 3244.4597\n",
            "Epoch 262/500\n",
            "50/50 [==============================] - 0s 959us/step - loss: 5222.9889 - mse: 45695500.0000 - mae: 5222.9888 - val_loss: 3241.5388 - val_mse: 18029148.0000 - val_mae: 3241.5386\n",
            "Epoch 263/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5364.0510 - mse: 48976636.0000 - mae: 5364.0513 - val_loss: 3246.1673 - val_mse: 18186768.0000 - val_mae: 3246.1675\n",
            "Epoch 264/500\n",
            "50/50 [==============================] - 0s 956us/step - loss: 5034.3210 - mse: 40171540.0000 - mae: 5034.3213 - val_loss: 3233.2544 - val_mse: 17751482.0000 - val_mae: 3233.2544\n",
            "Epoch 265/500\n",
            "50/50 [==============================] - 0s 969us/step - loss: 5499.2228 - mse: 46118584.0000 - mae: 5499.2227 - val_loss: 3233.6239 - val_mse: 17756084.0000 - val_mae: 3233.6243\n",
            "Epoch 266/500\n",
            "50/50 [==============================] - 0s 865us/step - loss: 6865.8867 - mse: 62741120.0000 - mae: 6865.8867 - val_loss: 3240.9377 - val_mse: 18003166.0000 - val_mae: 3240.9380\n",
            "Epoch 267/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5305.6708 - mse: 43501940.0000 - mae: 5305.6704 - val_loss: 3231.1340 - val_mse: 17547036.0000 - val_mae: 3231.1340\n",
            "Epoch 268/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5960.5106 - mse: 50053488.0000 - mae: 5960.5107 - val_loss: 3230.4907 - val_mse: 17473082.0000 - val_mae: 3230.4907\n",
            "Epoch 269/500\n",
            "50/50 [==============================] - 0s 799us/step - loss: 5934.5560 - mse: 55850968.0000 - mae: 5934.5562 - val_loss: 3232.9670 - val_mse: 17688224.0000 - val_mae: 3232.9668\n",
            "Epoch 270/500\n",
            "50/50 [==============================] - 0s 768us/step - loss: 5695.4953 - mse: 48045344.0000 - mae: 5695.4956 - val_loss: 3230.1394 - val_mse: 17401148.0000 - val_mae: 3230.1394\n",
            "Epoch 271/500\n",
            "50/50 [==============================] - 0s 807us/step - loss: 5298.2340 - mse: 46825640.0000 - mae: 5298.2339 - val_loss: 3227.8844 - val_mse: 17183502.0000 - val_mae: 3227.8843\n",
            "Epoch 272/500\n",
            "50/50 [==============================] - 0s 859us/step - loss: 7292.9039 - mse: 78224336.0000 - mae: 7292.9038 - val_loss: 3231.9381 - val_mse: 17488588.0000 - val_mae: 3231.9380\n",
            "Epoch 273/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5375.0494 - mse: 45789252.0000 - mae: 5375.0493 - val_loss: 3229.9218 - val_mse: 17322252.0000 - val_mae: 3229.9219\n",
            "Epoch 274/500\n",
            "50/50 [==============================] - 0s 849us/step - loss: 5534.5077 - mse: 49063044.0000 - mae: 5534.5073 - val_loss: 3230.3360 - val_mse: 17332280.0000 - val_mae: 3230.3359\n",
            "Epoch 275/500\n",
            "50/50 [==============================] - 0s 823us/step - loss: 6429.7108 - mse: 56745740.0000 - mae: 6429.7104 - val_loss: 3229.3580 - val_mse: 17229764.0000 - val_mae: 3229.3582\n",
            "Epoch 276/500\n",
            "50/50 [==============================] - 0s 855us/step - loss: 5900.0867 - mse: 49209304.0000 - mae: 5900.0869 - val_loss: 3230.5173 - val_mse: 17265532.0000 - val_mae: 3230.5171\n",
            "Epoch 277/500\n",
            "50/50 [==============================] - 0s 821us/step - loss: 5536.5597 - mse: 53295712.0000 - mae: 5536.5596 - val_loss: 3232.3546 - val_mse: 17397526.0000 - val_mae: 3232.3545\n",
            "Epoch 278/500\n",
            "50/50 [==============================] - 0s 846us/step - loss: 6060.2607 - mse: 62048548.0000 - mae: 6060.2607 - val_loss: 3230.0814 - val_mse: 17148628.0000 - val_mae: 3230.0815\n",
            "Epoch 279/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6085.6567 - mse: 62794408.0000 - mae: 6085.6567 - val_loss: 3230.9629 - val_mse: 17175906.0000 - val_mae: 3230.9629\n",
            "Epoch 280/500\n",
            "50/50 [==============================] - 0s 950us/step - loss: 5551.2934 - mse: 46886964.0000 - mae: 5551.2930 - val_loss: 3231.6425 - val_mse: 17198630.0000 - val_mae: 3231.6426\n",
            "Epoch 281/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6139.4753 - mse: 55000244.0000 - mae: 6139.4751 - val_loss: 3229.3859 - val_mse: 16982190.0000 - val_mae: 3229.3860\n",
            "Epoch 282/500\n",
            "50/50 [==============================] - 0s 904us/step - loss: 7036.1795 - mse: 74075432.0000 - mae: 7036.1802 - val_loss: 3235.7813 - val_mse: 17532888.0000 - val_mae: 3235.7817\n",
            "Epoch 283/500\n",
            "50/50 [==============================] - 0s 807us/step - loss: 5208.2406 - mse: 42238568.0000 - mae: 5208.2407 - val_loss: 3237.8694 - val_mse: 17724804.0000 - val_mae: 3237.8694\n",
            "Epoch 284/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5627.4087 - mse: 47272524.0000 - mae: 5627.4087 - val_loss: 3237.6852 - val_mse: 17724428.0000 - val_mae: 3237.6851\n",
            "Epoch 285/500\n",
            "50/50 [==============================] - 0s 984us/step - loss: 5011.3544 - mse: 39370928.0000 - mae: 5011.3545 - val_loss: 3234.9050 - val_mse: 17438926.0000 - val_mae: 3234.9050\n",
            "Epoch 286/500\n",
            "50/50 [==============================] - 0s 989us/step - loss: 5615.7688 - mse: 45863352.0000 - mae: 5615.7686 - val_loss: 3237.9075 - val_mse: 17669292.0000 - val_mae: 3237.9075\n",
            "Epoch 287/500\n",
            "50/50 [==============================] - 0s 860us/step - loss: 5720.0970 - mse: 51397692.0000 - mae: 5720.0967 - val_loss: 3242.9977 - val_mse: 17968500.0000 - val_mae: 3242.9978\n",
            "Epoch 288/500\n",
            "50/50 [==============================] - 0s 861us/step - loss: 5902.2395 - mse: 57387376.0000 - mae: 5902.2393 - val_loss: 3246.0662 - val_mse: 18102060.0000 - val_mae: 3246.0662\n",
            "Epoch 289/500\n",
            "50/50 [==============================] - 0s 811us/step - loss: 4854.1260 - mse: 32205788.0000 - mae: 4854.1265 - val_loss: 3242.9789 - val_mse: 17935444.0000 - val_mae: 3242.9790\n",
            "Epoch 290/500\n",
            "50/50 [==============================] - 0s 829us/step - loss: 6178.7106 - mse: 57303660.0000 - mae: 6178.7104 - val_loss: 3246.8203 - val_mse: 18120814.0000 - val_mae: 3246.8203\n",
            "Epoch 291/500\n",
            "50/50 [==============================] - 0s 798us/step - loss: 5768.5755 - mse: 49725288.0000 - mae: 5768.5752 - val_loss: 3241.6851 - val_mse: 17867700.0000 - val_mae: 3241.6851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60 samples, validate on 60 samples\n",
            "Epoch 1/500\n",
            "60/60 [==============================] - 2s 32ms/step - loss: 29102.6185 - mse: 994166912.0000 - mae: 29102.6211 - val_loss: 34679.6745 - val_mse: 1438883840.0000 - val_mae: 34679.6758\n",
            "Epoch 2/500\n",
            "60/60 [==============================] - 0s 777us/step - loss: 29101.7959 - mse: 994122688.0000 - mae: 29101.7949 - val_loss: 34676.8089 - val_mse: 1438705024.0000 - val_mae: 34676.8086\n",
            "Epoch 3/500\n",
            "60/60 [==============================] - 0s 697us/step - loss: 29095.2887 - mse: 993765184.0000 - mae: 29095.2891 - val_loss: 34666.2806 - val_mse: 1438018432.0000 - val_mae: 34666.2773\n",
            "Epoch 4/500\n",
            "60/60 [==============================] - 0s 749us/step - loss: 29083.3867 - mse: 993090816.0000 - mae: 29083.3867 - val_loss: 34651.7617 - val_mse: 1437043968.0000 - val_mae: 34651.7617\n",
            "Epoch 5/500\n",
            "60/60 [==============================] - 0s 777us/step - loss: 29069.4779 - mse: 992276032.0000 - mae: 29069.4766 - val_loss: 34633.2484 - val_mse: 1435788928.0000 - val_mae: 34633.2461\n",
            "Epoch 6/500\n",
            "60/60 [==============================] - 0s 785us/step - loss: 29037.7539 - mse: 990455744.0000 - mae: 29037.7539 - val_loss: 34604.0052 - val_mse: 1433802240.0000 - val_mae: 34604.0039\n",
            "Epoch 7/500\n",
            "60/60 [==============================] - 0s 760us/step - loss: 29010.8880 - mse: 988944640.0000 - mae: 29010.8867 - val_loss: 34573.4824 - val_mse: 1431712512.0000 - val_mae: 34573.4805\n",
            "Epoch 8/500\n",
            "60/60 [==============================] - 0s 822us/step - loss: 28982.4772 - mse: 987301760.0000 - mae: 28982.4766 - val_loss: 34541.2493 - val_mse: 1429499392.0000 - val_mae: 34541.2500\n",
            "Epoch 9/500\n",
            "60/60 [==============================] - 0s 699us/step - loss: 28949.2451 - mse: 985399488.0000 - mae: 28949.2422 - val_loss: 34506.4499 - val_mse: 1427110528.0000 - val_mae: 34506.4531\n",
            "Epoch 10/500\n",
            "60/60 [==============================] - 0s 753us/step - loss: 28909.9811 - mse: 983040192.0000 - mae: 28909.9824 - val_loss: 34468.5742 - val_mse: 1424503552.0000 - val_mae: 34468.5742\n",
            "Epoch 11/500\n",
            "60/60 [==============================] - 0s 628us/step - loss: 28882.2389 - mse: 981638784.0000 - mae: 28882.2402 - val_loss: 34430.0036 - val_mse: 1421853824.0000 - val_mae: 34430.0039\n",
            "Epoch 12/500\n",
            "60/60 [==============================] - 0s 982us/step - loss: 28819.6924 - mse: 978373504.0000 - mae: 28819.6934 - val_loss: 34385.9076 - val_mse: 1418828160.0000 - val_mae: 34385.9062\n",
            "Epoch 13/500\n",
            "60/60 [==============================] - 0s 823us/step - loss: 28804.3190 - mse: 976969408.0000 - mae: 28804.3184 - val_loss: 34343.2054 - val_mse: 1415896192.0000 - val_mae: 34343.2031\n",
            "Epoch 14/500\n",
            "60/60 [==============================] - 0s 995us/step - loss: 28745.9033 - mse: 973970496.0000 - mae: 28745.9004 - val_loss: 34296.2223 - val_mse: 1412673536.0000 - val_mae: 34296.2227\n",
            "Epoch 15/500\n",
            "60/60 [==============================] - 0s 741us/step - loss: 28698.0430 - mse: 970583360.0000 - mae: 28698.0410 - val_loss: 34247.1755 - val_mse: 1409311488.0000 - val_mae: 34247.1758\n",
            "Epoch 16/500\n",
            "60/60 [==============================] - 0s 724us/step - loss: 28632.8926 - mse: 967213440.0000 - mae: 28632.8945 - val_loss: 34194.1670 - val_mse: 1405686016.0000 - val_mae: 34194.1680\n",
            "Epoch 17/500\n",
            "60/60 [==============================] - 0s 652us/step - loss: 28578.9056 - mse: 964295040.0000 - mae: 28578.9043 - val_loss: 34139.3551 - val_mse: 1401940608.0000 - val_mae: 34139.3555\n",
            "Epoch 18/500\n",
            "60/60 [==============================] - 0s 805us/step - loss: 28527.7624 - mse: 961299008.0000 - mae: 28527.7617 - val_loss: 34081.3734 - val_mse: 1397986048.0000 - val_mae: 34081.3711\n",
            "Epoch 19/500\n",
            "60/60 [==============================] - 0s 853us/step - loss: 28498.7337 - mse: 960257728.0000 - mae: 28498.7324 - val_loss: 34022.6888 - val_mse: 1393991936.0000 - val_mae: 34022.6875\n",
            "Epoch 20/500\n",
            "60/60 [==============================] - 0s 816us/step - loss: 28379.6758 - mse: 953312320.0000 - mae: 28379.6758 - val_loss: 33956.7288 - val_mse: 1389507968.0000 - val_mae: 33956.7305\n",
            "Epoch 21/500\n",
            "60/60 [==============================] - 0s 764us/step - loss: 28370.8307 - mse: 952352448.0000 - mae: 28370.8320 - val_loss: 33892.9792 - val_mse: 1385180928.0000 - val_mae: 33892.9805\n",
            "Epoch 22/500\n",
            "60/60 [==============================] - 0s 657us/step - loss: 28282.2451 - mse: 946512960.0000 - mae: 28282.2461 - val_loss: 33824.9824 - val_mse: 1380573568.0000 - val_mae: 33824.9844\n",
            "Epoch 23/500\n",
            "60/60 [==============================] - 0s 798us/step - loss: 28246.7458 - mse: 945344064.0000 - mae: 28246.7461 - val_loss: 33757.0407 - val_mse: 1375982080.0000 - val_mae: 33757.0430\n",
            "Epoch 24/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 28171.2311 - mse: 939560192.0000 - mae: 28171.2305 - val_loss: 33685.4567 - val_mse: 1371151744.0000 - val_mae: 33685.4570\n",
            "Epoch 25/500\n",
            "60/60 [==============================] - 0s 696us/step - loss: 28093.3402 - mse: 936929856.0000 - mae: 28093.3398 - val_loss: 33611.3441 - val_mse: 1366163456.0000 - val_mae: 33611.3438\n",
            "Epoch 26/500\n",
            "60/60 [==============================] - 0s 767us/step - loss: 28032.9769 - mse: 932516800.0000 - mae: 28032.9746 - val_loss: 33535.8467 - val_mse: 1361094272.0000 - val_mae: 33535.8477\n",
            "Epoch 27/500\n",
            "60/60 [==============================] - 0s 774us/step - loss: 27922.4775 - mse: 927016704.0000 - mae: 27922.4766 - val_loss: 33455.8555 - val_mse: 1355735296.0000 - val_mae: 33455.8555\n",
            "Epoch 28/500\n",
            "60/60 [==============================] - 0s 807us/step - loss: 27813.0153 - mse: 922068032.0000 - mae: 27813.0176 - val_loss: 33371.2240 - val_mse: 1350079616.0000 - val_mae: 33371.2266\n",
            "Epoch 29/500\n",
            "60/60 [==============================] - 0s 935us/step - loss: 27747.9329 - mse: 918287680.0000 - mae: 27747.9336 - val_loss: 33286.2988 - val_mse: 1344417152.0000 - val_mae: 33286.3008\n",
            "Epoch 30/500\n",
            "60/60 [==============================] - 0s 681us/step - loss: 27662.4733 - mse: 913873984.0000 - mae: 27662.4727 - val_loss: 33198.4635 - val_mse: 1338576256.0000 - val_mae: 33198.4609\n",
            "Epoch 31/500\n",
            "60/60 [==============================] - 0s 768us/step - loss: 27596.4652 - mse: 907684160.0000 - mae: 27596.4648 - val_loss: 33109.2988 - val_mse: 1332662400.0000 - val_mae: 33109.3008\n",
            "Epoch 32/500\n",
            "60/60 [==============================] - 0s 979us/step - loss: 27473.1396 - mse: 904685312.0000 - mae: 27473.1387 - val_loss: 33016.3105 - val_mse: 1326512128.0000 - val_mae: 33016.3125\n",
            "Epoch 33/500\n",
            "60/60 [==============================] - 0s 745us/step - loss: 27405.4971 - mse: 900086528.0000 - mae: 27405.4961 - val_loss: 32922.7451 - val_mse: 1320342400.0000 - val_mae: 32922.7461\n",
            "Epoch 34/500\n",
            "60/60 [==============================] - 0s 844us/step - loss: 27340.8060 - mse: 895150080.0000 - mae: 27340.8047 - val_loss: 32828.1689 - val_mse: 1314122496.0000 - val_mae: 32828.1719\n",
            "Epoch 35/500\n",
            "60/60 [==============================] - 0s 745us/step - loss: 27161.7728 - mse: 883851904.0000 - mae: 27161.7754 - val_loss: 32726.9219 - val_mse: 1307483008.0000 - val_mae: 32726.9199\n",
            "Epoch 36/500\n",
            "60/60 [==============================] - 0s 730us/step - loss: 27165.7936 - mse: 883809152.0000 - mae: 27165.7910 - val_loss: 32628.7044 - val_mse: 1301062400.0000 - val_mae: 32628.7051\n",
            "Epoch 37/500\n",
            "60/60 [==============================] - 0s 962us/step - loss: 27028.7008 - mse: 879086208.0000 - mae: 27028.6973 - val_loss: 32526.1221 - val_mse: 1294377472.0000 - val_mae: 32526.1211\n",
            "Epoch 38/500\n",
            "60/60 [==============================] - 0s 799us/step - loss: 26944.4746 - mse: 872587840.0000 - mae: 26944.4746 - val_loss: 32422.4284 - val_mse: 1287640832.0000 - val_mae: 32422.4297\n",
            "Epoch 39/500\n",
            "60/60 [==============================] - 0s 827us/step - loss: 26806.7572 - mse: 866867072.0000 - mae: 26806.7559 - val_loss: 32312.6976 - val_mse: 1280536704.0000 - val_mae: 32312.6973\n",
            "Epoch 40/500\n",
            "60/60 [==============================] - 0s 793us/step - loss: 26595.2757 - mse: 854001280.0000 - mae: 26595.2754 - val_loss: 32194.6497 - val_mse: 1272920704.0000 - val_mae: 32194.6504\n",
            "Epoch 41/500\n",
            "60/60 [==============================] - 0s 770us/step - loss: 26419.7744 - mse: 847970688.0000 - mae: 26419.7754 - val_loss: 32071.8405 - val_mse: 1265027200.0000 - val_mae: 32071.8418\n",
            "Epoch 42/500\n",
            "60/60 [==============================] - 0s 859us/step - loss: 26609.4320 - mse: 854868672.0000 - mae: 26609.4316 - val_loss: 31960.2721 - val_mse: 1257881856.0000 - val_mae: 31960.2715\n",
            "Epoch 43/500\n",
            "60/60 [==============================] - 0s 845us/step - loss: 26287.3164 - mse: 839506752.0000 - mae: 26287.3184 - val_loss: 31836.5762 - val_mse: 1249989376.0000 - val_mae: 31836.5762\n",
            "Epoch 44/500\n",
            "60/60 [==============================] - 0s 769us/step - loss: 26337.6829 - mse: 841933888.0000 - mae: 26337.6836 - val_loss: 31717.9492 - val_mse: 1242448640.0000 - val_mae: 31717.9473\n",
            "Epoch 45/500\n",
            "60/60 [==============================] - 0s 779us/step - loss: 25872.1823 - mse: 818337088.0000 - mae: 25872.1836 - val_loss: 31582.6478 - val_mse: 1233881856.0000 - val_mae: 31582.6504\n",
            "Epoch 46/500\n",
            "60/60 [==============================] - 0s 817us/step - loss: 25867.8232 - mse: 817295360.0000 - mae: 25867.8242 - val_loss: 31450.4268 - val_mse: 1225546112.0000 - val_mae: 31450.4277\n",
            "Epoch 47/500\n",
            "60/60 [==============================] - 0s 858us/step - loss: 25989.6139 - mse: 824004480.0000 - mae: 25989.6133 - val_loss: 31325.9183 - val_mse: 1217728640.0000 - val_mae: 31325.9160\n",
            "Epoch 48/500\n",
            "60/60 [==============================] - 0s 752us/step - loss: 25727.3197 - mse: 808203904.0000 - mae: 25727.3203 - val_loss: 31194.4238 - val_mse: 1209505408.0000 - val_mae: 31194.4258\n",
            "Epoch 49/500\n",
            "60/60 [==============================] - 0s 800us/step - loss: 25699.1911 - mse: 814030976.0000 - mae: 25699.1914 - val_loss: 31064.0107 - val_mse: 1201385216.0000 - val_mae: 31064.0117\n",
            "Epoch 50/500\n",
            "60/60 [==============================] - 0s 740us/step - loss: 25287.8981 - mse: 791316736.0000 - mae: 25287.8984 - val_loss: 30920.3005 - val_mse: 1192476672.0000 - val_mae: 30920.3008\n",
            "Epoch 51/500\n",
            "60/60 [==============================] - 0s 803us/step - loss: 25261.5993 - mse: 782571392.0000 - mae: 25261.5996 - val_loss: 30779.2878 - val_mse: 1183774336.0000 - val_mae: 30779.2891\n",
            "Epoch 52/500\n",
            "60/60 [==============================] - 0s 682us/step - loss: 25123.7992 - mse: 775227136.0000 - mae: 25123.8008 - val_loss: 30635.1901 - val_mse: 1174922368.0000 - val_mae: 30635.1914\n",
            "Epoch 53/500\n",
            "60/60 [==============================] - 0s 817us/step - loss: 24987.7998 - mse: 772878656.0000 - mae: 24987.7988 - val_loss: 30489.4974 - val_mse: 1166016000.0000 - val_mae: 30489.4980\n",
            "Epoch 54/500\n",
            "60/60 [==============================] - 0s 774us/step - loss: 24862.5586 - mse: 766177856.0000 - mae: 24862.5605 - val_loss: 30341.5007 - val_mse: 1157011584.0000 - val_mae: 30341.5000\n",
            "Epoch 55/500\n",
            "60/60 [==============================] - 0s 772us/step - loss: 24616.0094 - mse: 753170688.0000 - mae: 24616.0098 - val_loss: 30188.0983 - val_mse: 1147724928.0000 - val_mae: 30188.0996\n",
            "Epoch 56/500\n",
            "60/60 [==============================] - 0s 827us/step - loss: 24403.8298 - mse: 739497344.0000 - mae: 24403.8301 - val_loss: 30030.9014 - val_mse: 1138257024.0000 - val_mae: 30030.9004\n",
            "Epoch 57/500\n",
            "60/60 [==============================] - 0s 701us/step - loss: 24199.0260 - mse: 736954240.0000 - mae: 24199.0273 - val_loss: 29869.9512 - val_mse: 1128615168.0000 - val_mae: 29869.9512\n",
            "Epoch 58/500\n",
            "60/60 [==============================] - 0s 816us/step - loss: 24445.6042 - mse: 745933376.0000 - mae: 24445.6035 - val_loss: 29719.7041 - val_mse: 1119660800.0000 - val_mae: 29719.7051\n",
            "Epoch 59/500\n",
            "60/60 [==============================] - 0s 675us/step - loss: 24171.1325 - mse: 735234048.0000 - mae: 24171.1309 - val_loss: 29562.9997 - val_mse: 1110369792.0000 - val_mae: 29563.0000\n",
            "Epoch 60/500\n",
            "60/60 [==============================] - 0s 820us/step - loss: 23823.5957 - mse: 718310144.0000 - mae: 23823.5938 - val_loss: 29398.5636 - val_mse: 1100673152.0000 - val_mae: 29398.5645\n",
            "Epoch 61/500\n",
            "60/60 [==============================] - 0s 858us/step - loss: 23714.6208 - mse: 716848320.0000 - mae: 23714.6211 - val_loss: 29233.1237 - val_mse: 1090972032.0000 - val_mae: 29233.1230\n",
            "Epoch 62/500\n",
            "60/60 [==============================] - 0s 931us/step - loss: 23784.8490 - mse: 709572800.0000 - mae: 23784.8477 - val_loss: 29072.0008 - val_mse: 1081576064.0000 - val_mae: 29072.0020\n",
            "Epoch 63/500\n",
            "60/60 [==============================] - 0s 723us/step - loss: 23584.1598 - mse: 705643840.0000 - mae: 23584.1582 - val_loss: 28906.5742 - val_mse: 1071983488.0000 - val_mae: 28906.5742\n",
            "Epoch 64/500\n",
            "60/60 [==============================] - 0s 783us/step - loss: 23409.8089 - mse: 696724480.0000 - mae: 23409.8086 - val_loss: 28738.8384 - val_mse: 1062312704.0000 - val_mae: 28738.8379\n",
            "Epoch 65/500\n",
            "60/60 [==============================] - 0s 883us/step - loss: 23218.2399 - mse: 693554432.0000 - mae: 23218.2422 - val_loss: 28568.0413 - val_mse: 1052523840.0000 - val_mae: 28568.0410\n",
            "Epoch 66/500\n",
            "60/60 [==============================] - 0s 893us/step - loss: 22667.9941 - mse: 657970496.0000 - mae: 22667.9941 - val_loss: 28384.3799 - val_mse: 1042062528.0000 - val_mae: 28384.3789\n",
            "Epoch 67/500\n",
            "60/60 [==============================] - 0s 719us/step - loss: 22704.7992 - mse: 661623488.0000 - mae: 22704.8008 - val_loss: 28205.2067 - val_mse: 1031921792.0000 - val_mae: 28205.2090\n",
            "Epoch 68/500\n",
            "60/60 [==============================] - 0s 745us/step - loss: 22681.2106 - mse: 668737152.0000 - mae: 22681.2109 - val_loss: 28027.5462 - val_mse: 1021930560.0000 - val_mae: 28027.5488\n",
            "Epoch 69/500\n",
            "60/60 [==============================] - 0s 718us/step - loss: 22197.3480 - mse: 636543872.0000 - mae: 22197.3496 - val_loss: 27839.4367 - val_mse: 1011419904.0000 - val_mae: 27839.4375\n",
            "Epoch 70/500\n",
            "60/60 [==============================] - 0s 824us/step - loss: 21905.3600 - mse: 636383872.0000 - mae: 21905.3613 - val_loss: 27646.1463 - val_mse: 1000693824.0000 - val_mae: 27646.1465\n",
            "Epoch 71/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 21852.4717 - mse: 624182592.0000 - mae: 21852.4727 - val_loss: 27454.7096 - val_mse: 990144192.0000 - val_mae: 27454.7109\n",
            "Epoch 72/500\n",
            "60/60 [==============================] - 0s 765us/step - loss: 21598.7861 - mse: 615816128.0000 - mae: 21598.7871 - val_loss: 27259.5793 - val_mse: 979466624.0000 - val_mae: 27259.5801\n",
            "Epoch 73/500\n",
            "60/60 [==============================] - 0s 761us/step - loss: 21433.7702 - mse: 617789760.0000 - mae: 21433.7715 - val_loss: 27062.6462 - val_mse: 968767872.0000 - val_mae: 27062.6465\n",
            "Epoch 74/500\n",
            "60/60 [==============================] - 0s 722us/step - loss: 21321.1025 - mse: 600490560.0000 - mae: 21321.1035 - val_loss: 26865.3050 - val_mse: 958124736.0000 - val_mae: 26865.3066\n",
            "Epoch 75/500\n",
            "60/60 [==============================] - 0s 746us/step - loss: 20821.8480 - mse: 586150592.0000 - mae: 20821.8477 - val_loss: 26658.7699 - val_mse: 947068736.0000 - val_mae: 26658.7715\n",
            "Epoch 76/500\n",
            "60/60 [==============================] - 0s 662us/step - loss: 21250.4660 - mse: 600877184.0000 - mae: 21250.4668 - val_loss: 26465.3478 - val_mse: 936792448.0000 - val_mae: 26465.3477\n",
            "Epoch 77/500\n",
            "60/60 [==============================] - 0s 785us/step - loss: 21158.7098 - mse: 599477504.0000 - mae: 21158.7090 - val_loss: 26272.0103 - val_mse: 926595328.0000 - val_mae: 26272.0098\n",
            "Epoch 78/500\n",
            "60/60 [==============================] - 0s 873us/step - loss: 20899.1650 - mse: 594722368.0000 - mae: 20899.1641 - val_loss: 26073.9408 - val_mse: 916226368.0000 - val_mae: 26073.9414\n",
            "Epoch 79/500\n",
            "60/60 [==============================] - 0s 840us/step - loss: 20427.9409 - mse: 565334592.0000 - mae: 20427.9395 - val_loss: 25864.3652 - val_mse: 905340480.0000 - val_mae: 25864.3672\n",
            "Epoch 80/500\n",
            "60/60 [==============================] - 0s 854us/step - loss: 20036.8242 - mse: 545582976.0000 - mae: 20036.8223 - val_loss: 25645.5288 - val_mse: 894066752.0000 - val_mae: 25645.5293\n",
            "Epoch 81/500\n",
            "60/60 [==============================] - 0s 732us/step - loss: 19821.6258 - mse: 543636160.0000 - mae: 19821.6250 - val_loss: 25424.4578 - val_mse: 882775680.0000 - val_mae: 25424.4590\n",
            "Epoch 82/500\n",
            "60/60 [==============================] - 0s 793us/step - loss: 19690.7943 - mse: 538222464.0000 - mae: 19690.7930 - val_loss: 25203.2132 - val_mse: 871573632.0000 - val_mae: 25203.2129\n",
            "Epoch 83/500\n",
            "60/60 [==============================] - 0s 811us/step - loss: 19185.5303 - mse: 523540448.0000 - mae: 19185.5312 - val_loss: 24972.9360 - val_mse: 860018432.0000 - val_mae: 24972.9375\n",
            "Epoch 84/500\n",
            "60/60 [==============================] - 0s 872us/step - loss: 19248.9922 - mse: 521890400.0000 - mae: 19248.9922 - val_loss: 24747.2541 - val_mse: 848796480.0000 - val_mae: 24747.2539\n",
            "Epoch 85/500\n",
            "60/60 [==============================] - 0s 703us/step - loss: 19441.2760 - mse: 537051264.0000 - mae: 19441.2754 - val_loss: 24540.3625 - val_mse: 838597888.0000 - val_mae: 24540.3633\n",
            "Epoch 86/500\n",
            "60/60 [==============================] - 0s 817us/step - loss: 18975.2733 - mse: 510593984.0000 - mae: 18975.2754 - val_loss: 24320.9593 - val_mse: 827876352.0000 - val_mae: 24320.9590\n",
            "Epoch 87/500\n",
            "60/60 [==============================] - 0s 803us/step - loss: 18705.8211 - mse: 505189792.0000 - mae: 18705.8203 - val_loss: 24089.4806 - val_mse: 816669632.0000 - val_mae: 24089.4785\n",
            "Epoch 88/500\n",
            "60/60 [==============================] - 0s 830us/step - loss: 18607.4051 - mse: 488294720.0000 - mae: 18607.4062 - val_loss: 23858.4971 - val_mse: 805593472.0000 - val_mae: 23858.4961\n",
            "Epoch 89/500\n",
            "60/60 [==============================] - 0s 983us/step - loss: 18637.9850 - mse: 509055776.0000 - mae: 18637.9863 - val_loss: 23639.1458 - val_mse: 795173696.0000 - val_mae: 23639.1465\n",
            "Epoch 90/500\n",
            "60/60 [==============================] - 0s 868us/step - loss: 18301.5762 - mse: 481948800.0000 - mae: 18301.5742 - val_loss: 23405.8729 - val_mse: 784198336.0000 - val_mae: 23405.8730\n",
            "Epoch 91/500\n",
            "60/60 [==============================] - 0s 776us/step - loss: 18019.8540 - mse: 477552288.0000 - mae: 18019.8535 - val_loss: 23177.4743 - val_mse: 773557696.0000 - val_mae: 23177.4746\n",
            "Epoch 92/500\n",
            "60/60 [==============================] - 0s 731us/step - loss: 17332.6367 - mse: 456763424.0000 - mae: 17332.6367 - val_loss: 22937.4878 - val_mse: 762489792.0000 - val_mae: 22937.4883\n",
            "Epoch 93/500\n",
            "60/60 [==============================] - 0s 838us/step - loss: 17355.5809 - mse: 453077632.0000 - mae: 17355.5801 - val_loss: 22691.8433 - val_mse: 751280384.0000 - val_mae: 22691.8418\n",
            "Epoch 94/500\n",
            "60/60 [==============================] - 0s 858us/step - loss: 16616.1113 - mse: 413657056.0000 - mae: 16616.1113 - val_loss: 22434.0745 - val_mse: 739647104.0000 - val_mae: 22434.0742\n",
            "Epoch 95/500\n",
            "60/60 [==============================] - 0s 837us/step - loss: 16994.7109 - mse: 430039392.0000 - mae: 16994.7109 - val_loss: 22187.4331 - val_mse: 728640320.0000 - val_mae: 22187.4336\n",
            "Epoch 96/500\n",
            "60/60 [==============================] - 0s 766us/step - loss: 16642.1673 - mse: 421245504.0000 - mae: 16642.1660 - val_loss: 21943.0789 - val_mse: 717855616.0000 - val_mae: 21943.0801\n",
            "Epoch 97/500\n",
            "60/60 [==============================] - 0s 873us/step - loss: 16320.5356 - mse: 416674752.0000 - mae: 16320.5342 - val_loss: 21717.4349 - val_mse: 708002624.0000 - val_mae: 21717.4336\n",
            "Epoch 98/500\n",
            "60/60 [==============================] - 0s 812us/step - loss: 16634.8215 - mse: 432913024.0000 - mae: 16634.8223 - val_loss: 21477.0195 - val_mse: 697617280.0000 - val_mae: 21477.0215\n",
            "Epoch 99/500\n",
            "60/60 [==============================] - 0s 807us/step - loss: 15354.0376 - mse: 381001888.0000 - mae: 15354.0371 - val_loss: 21222.9604 - val_mse: 686767872.0000 - val_mae: 21222.9609\n",
            "Epoch 100/500\n",
            "60/60 [==============================] - 0s 834us/step - loss: 16005.3306 - mse: 399092928.0000 - mae: 16005.3311 - val_loss: 20982.8870 - val_mse: 676634176.0000 - val_mae: 20982.8867\n",
            "Epoch 101/500\n",
            "60/60 [==============================] - 0s 754us/step - loss: 15910.0324 - mse: 379193632.0000 - mae: 15910.0322 - val_loss: 20752.1229 - val_mse: 667001664.0000 - val_mae: 20752.1230\n",
            "Epoch 102/500\n",
            "60/60 [==============================] - 0s 862us/step - loss: 15571.9600 - mse: 406757280.0000 - mae: 15571.9600 - val_loss: 20508.1773 - val_mse: 656935744.0000 - val_mae: 20508.1777\n",
            "Epoch 103/500\n",
            "60/60 [==============================] - 0s 914us/step - loss: 14325.6652 - mse: 333526528.0000 - mae: 14325.6650 - val_loss: 20241.3430 - val_mse: 646061312.0000 - val_mae: 20241.3418\n",
            "Epoch 104/500\n",
            "60/60 [==============================] - 0s 837us/step - loss: 14598.8918 - mse: 366543328.0000 - mae: 14598.8916 - val_loss: 20008.8694 - val_mse: 636703296.0000 - val_mae: 20008.8691\n",
            "Epoch 105/500\n",
            "60/60 [==============================] - 0s 857us/step - loss: 14429.9591 - mse: 351818688.0000 - mae: 14429.9580 - val_loss: 19748.8209 - val_mse: 626363584.0000 - val_mae: 19748.8203\n",
            "Epoch 106/500\n",
            "60/60 [==============================] - 0s 836us/step - loss: 14221.2882 - mse: 359514784.0000 - mae: 14221.2881 - val_loss: 19521.2612 - val_mse: 617426112.0000 - val_mae: 19521.2617\n",
            "Epoch 107/500\n",
            "60/60 [==============================] - 0s 840us/step - loss: 14424.0959 - mse: 355160128.0000 - mae: 14424.0957 - val_loss: 19283.5619 - val_mse: 608201152.0000 - val_mae: 19283.5625\n",
            "Epoch 108/500\n",
            "60/60 [==============================] - 0s 798us/step - loss: 14915.6541 - mse: 361841120.0000 - mae: 14915.6553 - val_loss: 19074.2734 - val_mse: 600171968.0000 - val_mae: 19074.2734\n",
            "Epoch 109/500\n",
            "60/60 [==============================] - 0s 803us/step - loss: 14320.5050 - mse: 370747264.0000 - mae: 14320.5049 - val_loss: 18851.9513 - val_mse: 591739840.0000 - val_mae: 18851.9492\n",
            "Epoch 110/500\n",
            "60/60 [==============================] - 0s 959us/step - loss: 12860.3215 - mse: 300414656.0000 - mae: 12860.3213 - val_loss: 18615.8044 - val_mse: 582890560.0000 - val_mae: 18615.8047\n",
            "Epoch 111/500\n",
            "60/60 [==============================] - 0s 779us/step - loss: 13176.9858 - mse: 332922176.0000 - mae: 13176.9854 - val_loss: 18381.2383 - val_mse: 574211072.0000 - val_mae: 18381.2383\n",
            "Epoch 112/500\n",
            "60/60 [==============================] - 0s 840us/step - loss: 12689.3225 - mse: 289259808.0000 - mae: 12689.3232 - val_loss: 18175.2324 - val_mse: 566679424.0000 - val_mae: 18175.2324\n",
            "Epoch 113/500\n",
            "60/60 [==============================] - 0s 716us/step - loss: 13106.0037 - mse: 321687456.0000 - mae: 13106.0039 - val_loss: 17960.9406 - val_mse: 558934976.0000 - val_mae: 17960.9414\n",
            "Epoch 114/500\n",
            "60/60 [==============================] - 0s 759us/step - loss: 13288.3700 - mse: 320445664.0000 - mae: 13288.3701 - val_loss: 17719.6716 - val_mse: 550325376.0000 - val_mae: 17719.6699\n",
            "Epoch 115/500\n",
            "60/60 [==============================] - 0s 801us/step - loss: 12816.3440 - mse: 309264992.0000 - mae: 12816.3438 - val_loss: 17516.7942 - val_mse: 543175360.0000 - val_mae: 17516.7930\n",
            "Epoch 116/500\n",
            "60/60 [==============================] - 0s 801us/step - loss: 12336.4340 - mse: 275852576.0000 - mae: 12336.4346 - val_loss: 17254.4520 - val_mse: 534052320.0000 - val_mae: 17254.4512\n",
            "Epoch 117/500\n",
            "60/60 [==============================] - 0s 942us/step - loss: 12555.9305 - mse: 312264576.0000 - mae: 12555.9307 - val_loss: 17048.9826 - val_mse: 527003232.0000 - val_mae: 17048.9824\n",
            "Epoch 118/500\n",
            "60/60 [==============================] - 0s 728us/step - loss: 13225.8231 - mse: 314027520.0000 - mae: 13225.8223 - val_loss: 16855.7778 - val_mse: 520451488.0000 - val_mae: 16855.7773\n",
            "Epoch 119/500\n",
            "60/60 [==============================] - 0s 709us/step - loss: 12548.6030 - mse: 294089696.0000 - mae: 12548.6025 - val_loss: 16638.4201 - val_mse: 513170784.0000 - val_mae: 16638.4199\n",
            "Epoch 120/500\n",
            "60/60 [==============================] - 0s 735us/step - loss: 11234.4397 - mse: 265355088.0000 - mae: 11234.4395 - val_loss: 16406.7054 - val_mse: 505512608.0000 - val_mae: 16406.7070\n",
            "Epoch 121/500\n",
            "60/60 [==============================] - 0s 762us/step - loss: 12118.3979 - mse: 273981152.0000 - mae: 12118.3975 - val_loss: 16169.0042 - val_mse: 497768384.0000 - val_mae: 16169.0039\n",
            "Epoch 122/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 11477.0046 - mse: 278053024.0000 - mae: 11477.0039 - val_loss: 15937.4127 - val_mse: 490331744.0000 - val_mae: 15937.4121\n",
            "Epoch 123/500\n",
            "60/60 [==============================] - 0s 860us/step - loss: 12081.0316 - mse: 270363008.0000 - mae: 12081.0312 - val_loss: 15724.1584 - val_mse: 483579296.0000 - val_mae: 15724.1582\n",
            "Epoch 124/500\n",
            "60/60 [==============================] - 0s 805us/step - loss: 11139.4197 - mse: 238990000.0000 - mae: 11139.4199 - val_loss: 15534.0150 - val_mse: 477634624.0000 - val_mae: 15534.0146\n",
            "Epoch 125/500\n",
            "60/60 [==============================] - 0s 780us/step - loss: 11620.6695 - mse: 269566368.0000 - mae: 11620.6699 - val_loss: 15341.5066 - val_mse: 471690112.0000 - val_mae: 15341.5059\n",
            "Epoch 126/500\n",
            "60/60 [==============================] - 0s 816us/step - loss: 10688.4072 - mse: 247921408.0000 - mae: 10688.4062 - val_loss: 15096.9904 - val_mse: 464246752.0000 - val_mae: 15096.9893\n",
            "Epoch 127/500\n",
            "60/60 [==============================] - 0s 902us/step - loss: 10740.4775 - mse: 260932048.0000 - mae: 10740.4775 - val_loss: 14856.9780 - val_mse: 457057088.0000 - val_mae: 14856.9785\n",
            "Epoch 128/500\n",
            "60/60 [==============================] - 0s 780us/step - loss: 11117.2329 - mse: 239149728.0000 - mae: 11117.2334 - val_loss: 14631.5521 - val_mse: 450408896.0000 - val_mae: 14631.5518\n",
            "Epoch 129/500\n",
            "60/60 [==============================] - 0s 961us/step - loss: 10553.5147 - mse: 252300800.0000 - mae: 10553.5146 - val_loss: 14486.8079 - val_mse: 446192640.0000 - val_mae: 14486.8086\n",
            "Epoch 130/500\n",
            "60/60 [==============================] - 0s 735us/step - loss: 10907.1216 - mse: 264627088.0000 - mae: 10907.1221 - val_loss: 14408.6360 - val_mse: 443933120.0000 - val_mae: 14408.6367\n",
            "Epoch 131/500\n",
            "60/60 [==============================] - 0s 799us/step - loss: 9839.8562 - mse: 227650272.0000 - mae: 9839.8564 - val_loss: 14229.0151 - val_mse: 438788160.0000 - val_mae: 14229.0146\n",
            "Epoch 132/500\n",
            "60/60 [==============================] - 0s 729us/step - loss: 11422.9136 - mse: 255648080.0000 - mae: 11422.9131 - val_loss: 14052.8535 - val_mse: 433805216.0000 - val_mae: 14052.8535\n",
            "Epoch 133/500\n",
            "60/60 [==============================] - 0s 724us/step - loss: 10538.6128 - mse: 259580064.0000 - mae: 10538.6123 - val_loss: 13827.4229 - val_mse: 427519328.0000 - val_mae: 13827.4229\n",
            "Epoch 134/500\n",
            "60/60 [==============================] - 0s 783us/step - loss: 10533.4954 - mse: 209305552.0000 - mae: 10533.4961 - val_loss: 13645.1876 - val_mse: 422428320.0000 - val_mae: 13645.1875\n",
            "Epoch 135/500\n",
            "60/60 [==============================] - 0s 742us/step - loss: 10428.1570 - mse: 251965168.0000 - mae: 10428.1572 - val_loss: 13545.5627 - val_mse: 419526048.0000 - val_mae: 13545.5625\n",
            "Epoch 136/500\n",
            "60/60 [==============================] - 0s 769us/step - loss: 10507.5507 - mse: 238033504.0000 - mae: 10507.5508 - val_loss: 13505.9246 - val_mse: 418377056.0000 - val_mae: 13505.9248\n",
            "Epoch 137/500\n",
            "60/60 [==============================] - 0s 889us/step - loss: 10533.5849 - mse: 232041216.0000 - mae: 10533.5840 - val_loss: 13408.0704 - val_mse: 415482048.0000 - val_mae: 13408.0693\n",
            "Epoch 138/500\n",
            "60/60 [==============================] - 0s 881us/step - loss: 10197.7349 - mse: 216019520.0000 - mae: 10197.7344 - val_loss: 13336.2463 - val_mse: 413302176.0000 - val_mae: 13336.2461\n",
            "Epoch 139/500\n",
            "60/60 [==============================] - 0s 890us/step - loss: 9685.5761 - mse: 229049120.0000 - mae: 9685.5762 - val_loss: 13155.5197 - val_mse: 407608928.0000 - val_mae: 13155.5195\n",
            "Epoch 140/500\n",
            "60/60 [==============================] - 0s 855us/step - loss: 10980.7875 - mse: 273579072.0000 - mae: 10980.7871 - val_loss: 13051.4503 - val_mse: 404255712.0000 - val_mae: 13051.4502\n",
            "Epoch 141/500\n",
            "60/60 [==============================] - 0s 751us/step - loss: 11711.1582 - mse: 281891936.0000 - mae: 11711.1582 - val_loss: 12962.7852 - val_mse: 401273792.0000 - val_mae: 12962.7842\n",
            "Epoch 142/500\n",
            "60/60 [==============================] - 0s 840us/step - loss: 10543.6689 - mse: 239054336.0000 - mae: 10543.6689 - val_loss: 12919.0088 - val_mse: 399799712.0000 - val_mae: 12919.0088\n",
            "Epoch 143/500\n",
            "60/60 [==============================] - 0s 696us/step - loss: 9882.1386 - mse: 213470208.0000 - mae: 9882.1387 - val_loss: 12833.6089 - val_mse: 396687488.0000 - val_mae: 12833.6084\n",
            "Epoch 144/500\n",
            "60/60 [==============================] - 0s 794us/step - loss: 9688.4608 - mse: 220222480.0000 - mae: 9688.4600 - val_loss: 12779.4959 - val_mse: 394582880.0000 - val_mae: 12779.4961\n",
            "Epoch 145/500\n",
            "60/60 [==============================] - 0s 791us/step - loss: 9861.4714 - mse: 225973856.0000 - mae: 9861.4707 - val_loss: 12733.4189 - val_mse: 392722560.0000 - val_mae: 12733.4189\n",
            "Epoch 146/500\n",
            "60/60 [==============================] - 0s 913us/step - loss: 10079.6821 - mse: 227321200.0000 - mae: 10079.6826 - val_loss: 12670.6656 - val_mse: 390003200.0000 - val_mae: 12670.6660\n",
            "Epoch 147/500\n",
            "60/60 [==============================] - 0s 888us/step - loss: 10277.8927 - mse: 233825344.0000 - mae: 10277.8926 - val_loss: 12604.0718 - val_mse: 386959840.0000 - val_mae: 12604.0723\n",
            "Epoch 148/500\n",
            "60/60 [==============================] - 0s 957us/step - loss: 10308.2333 - mse: 236977392.0000 - mae: 10308.2334 - val_loss: 12560.8634 - val_mse: 384935936.0000 - val_mae: 12560.8633\n",
            "Epoch 149/500\n",
            "60/60 [==============================] - 0s 716us/step - loss: 9137.4158 - mse: 183024640.0000 - mae: 9137.4160 - val_loss: 12493.4877 - val_mse: 381507040.0000 - val_mae: 12493.4873\n",
            "Epoch 150/500\n",
            "60/60 [==============================] - 0s 769us/step - loss: 10157.3185 - mse: 206712768.0000 - mae: 10157.3174 - val_loss: 12444.0648 - val_mse: 378964896.0000 - val_mae: 12444.0645\n",
            "Epoch 151/500\n",
            "60/60 [==============================] - 0s 725us/step - loss: 10357.6571 - mse: 251701136.0000 - mae: 10357.6582 - val_loss: 12413.9863 - val_mse: 377428672.0000 - val_mae: 12413.9854\n",
            "Epoch 152/500\n",
            "60/60 [==============================] - 0s 903us/step - loss: 10639.0769 - mse: 223847360.0000 - mae: 10639.0771 - val_loss: 12363.4006 - val_mse: 374864416.0000 - val_mae: 12363.4004\n",
            "Epoch 153/500\n",
            "60/60 [==============================] - 0s 837us/step - loss: 9708.2884 - mse: 233019296.0000 - mae: 9708.2881 - val_loss: 12346.9730 - val_mse: 374015104.0000 - val_mae: 12346.9727\n",
            "Epoch 154/500\n",
            "60/60 [==============================] - 0s 858us/step - loss: 10264.1921 - mse: 232973600.0000 - mae: 10264.1914 - val_loss: 12285.6787 - val_mse: 370497248.0000 - val_mae: 12285.6787\n",
            "Epoch 155/500\n",
            "60/60 [==============================] - 0s 879us/step - loss: 9444.7853 - mse: 196059728.0000 - mae: 9444.7852 - val_loss: 12272.9052 - val_mse: 369757792.0000 - val_mae: 12272.9053\n",
            "Epoch 156/500\n",
            "60/60 [==============================] - 0s 807us/step - loss: 10306.0911 - mse: 234111840.0000 - mae: 10306.0918 - val_loss: 12255.7557 - val_mse: 368709568.0000 - val_mae: 12255.7559\n",
            "Epoch 157/500\n",
            "60/60 [==============================] - 0s 707us/step - loss: 10008.1476 - mse: 178509536.0000 - mae: 10008.1475 - val_loss: 12223.4304 - val_mse: 366520352.0000 - val_mae: 12223.4316\n",
            "Epoch 158/500\n",
            "60/60 [==============================] - 0s 754us/step - loss: 9804.8410 - mse: 209039120.0000 - mae: 9804.8408 - val_loss: 12170.6315 - val_mse: 362930304.0000 - val_mae: 12170.6309\n",
            "Epoch 159/500\n",
            "60/60 [==============================] - 0s 816us/step - loss: 10319.7015 - mse: 234899184.0000 - mae: 10319.7012 - val_loss: 12153.2994 - val_mse: 361762464.0000 - val_mae: 12153.2988\n",
            "Epoch 160/500\n",
            "60/60 [==============================] - 0s 843us/step - loss: 10518.8586 - mse: 238269840.0000 - mae: 10518.8584 - val_loss: 12131.6206 - val_mse: 360309888.0000 - val_mae: 12131.6211\n",
            "Epoch 161/500\n",
            "60/60 [==============================] - 0s 844us/step - loss: 10684.5711 - mse: 208232816.0000 - mae: 10684.5723 - val_loss: 12131.6357 - val_mse: 360310624.0000 - val_mae: 12131.6357\n",
            "Epoch 162/500\n",
            "60/60 [==============================] - 0s 803us/step - loss: 9191.1838 - mse: 178412688.0000 - mae: 9191.1836 - val_loss: 12114.7601 - val_mse: 359185248.0000 - val_mae: 12114.7607\n",
            "Epoch 163/500\n",
            "60/60 [==============================] - 0s 797us/step - loss: 9832.4336 - mse: 196086352.0000 - mae: 9832.4336 - val_loss: 12114.9253 - val_mse: 359196064.0000 - val_mae: 12114.9248\n",
            "Epoch 164/500\n",
            "60/60 [==============================] - 0s 683us/step - loss: 9758.0701 - mse: 231416080.0000 - mae: 9758.0693 - val_loss: 12111.9940 - val_mse: 359000768.0000 - val_mae: 12111.9941\n",
            "Epoch 165/500\n",
            "60/60 [==============================] - 0s 964us/step - loss: 8812.3645 - mse: 179012112.0000 - mae: 8812.3643 - val_loss: 12107.6043 - val_mse: 358708960.0000 - val_mae: 12107.6045\n",
            "Epoch 166/500\n",
            "60/60 [==============================] - 0s 946us/step - loss: 9531.2774 - mse: 227574544.0000 - mae: 9531.2773 - val_loss: 12089.2599 - val_mse: 357493856.0000 - val_mae: 12089.2607\n",
            "Epoch 167/500\n",
            "60/60 [==============================] - 0s 933us/step - loss: 9784.1587 - mse: 207101584.0000 - mae: 9784.1582 - val_loss: 12061.6097 - val_mse: 355673920.0000 - val_mae: 12061.6094\n",
            "Epoch 168/500\n",
            "60/60 [==============================] - 0s 762us/step - loss: 10544.3398 - mse: 224422000.0000 - mae: 10544.3408 - val_loss: 12052.4799 - val_mse: 355075776.0000 - val_mae: 12052.4795\n",
            "Epoch 169/500\n",
            "60/60 [==============================] - 0s 780us/step - loss: 9181.0260 - mse: 182645328.0000 - mae: 9181.0273 - val_loss: 12042.0913 - val_mse: 354396960.0000 - val_mae: 12042.0918\n",
            "Epoch 170/500\n",
            "60/60 [==============================] - 0s 832us/step - loss: 9197.8092 - mse: 166889136.0000 - mae: 9197.8096 - val_loss: 12021.4187 - val_mse: 353052256.0000 - val_mae: 12021.4189\n",
            "Epoch 171/500\n",
            "60/60 [==============================] - 0s 870us/step - loss: 9528.9168 - mse: 205720512.0000 - mae: 9528.9170 - val_loss: 12005.4655 - val_mse: 352020288.0000 - val_mae: 12005.4658\n",
            "Epoch 172/500\n",
            "60/60 [==============================] - 0s 921us/step - loss: 10625.4013 - mse: 227336336.0000 - mae: 10625.4014 - val_loss: 11994.3274 - val_mse: 351302240.0000 - val_mae: 11994.3271\n",
            "Epoch 173/500\n",
            "60/60 [==============================] - 0s 870us/step - loss: 10366.5164 - mse: 232522000.0000 - mae: 10366.5166 - val_loss: 12005.8720 - val_mse: 352046080.0000 - val_mae: 12005.8721\n",
            "Epoch 174/500\n",
            "60/60 [==============================] - 0s 862us/step - loss: 9954.9632 - mse: 228405984.0000 - mae: 9954.9629 - val_loss: 11991.8376 - val_mse: 351141696.0000 - val_mae: 11991.8379\n",
            "Epoch 175/500\n",
            "60/60 [==============================] - 0s 830us/step - loss: 10046.0434 - mse: 204225904.0000 - mae: 10046.0439 - val_loss: 12011.0612 - val_mse: 352380512.0000 - val_mae: 12011.0615\n",
            "Epoch 176/500\n",
            "60/60 [==============================] - 0s 836us/step - loss: 9835.5617 - mse: 211567648.0000 - mae: 9835.5625 - val_loss: 12025.7457 - val_mse: 353331200.0000 - val_mae: 12025.7461\n",
            "Epoch 177/500\n",
            "60/60 [==============================] - 0s 900us/step - loss: 9531.2231 - mse: 215230576.0000 - mae: 9531.2227 - val_loss: 12013.6993 - val_mse: 352550176.0000 - val_mae: 12013.7002\n",
            "Epoch 178/500\n",
            "60/60 [==============================] - 0s 866us/step - loss: 9398.1854 - mse: 189343840.0000 - mae: 9398.1855 - val_loss: 12015.8785 - val_mse: 352690784.0000 - val_mae: 12015.8779\n",
            "Epoch 179/500\n",
            "60/60 [==============================] - 0s 806us/step - loss: 9586.8111 - mse: 206357072.0000 - mae: 9586.8105 - val_loss: 11999.8990 - val_mse: 351658368.0000 - val_mae: 11999.8994\n",
            "Epoch 180/500\n",
            "60/60 [==============================] - 0s 789us/step - loss: 9054.3984 - mse: 183164592.0000 - mae: 9054.3994 - val_loss: 11968.0962 - val_mse: 349617888.0000 - val_mae: 11968.0957\n",
            "Epoch 181/500\n",
            "60/60 [==============================] - 0s 807us/step - loss: 9560.4015 - mse: 211559920.0000 - mae: 9560.4014 - val_loss: 11958.7131 - val_mse: 349019200.0000 - val_mae: 11958.7129\n",
            "Epoch 182/500\n",
            "60/60 [==============================] - 0s 788us/step - loss: 8995.8060 - mse: 187231840.0000 - mae: 8995.8066 - val_loss: 11937.5692 - val_mse: 347676000.0000 - val_mae: 11937.5693\n",
            "Epoch 183/500\n",
            "60/60 [==============================] - 0s 741us/step - loss: 10253.0932 - mse: 202242240.0000 - mae: 10253.0938 - val_loss: 11948.1332 - val_mse: 348345504.0000 - val_mae: 11948.1338\n",
            "Epoch 184/500\n",
            "60/60 [==============================] - 0s 944us/step - loss: 10619.5089 - mse: 228676416.0000 - mae: 10619.5098 - val_loss: 11941.2921 - val_mse: 347911264.0000 - val_mae: 11941.2930\n",
            "Epoch 185/500\n",
            "60/60 [==============================] - 0s 796us/step - loss: 10902.2397 - mse: 217157232.0000 - mae: 10902.2402 - val_loss: 11923.3569 - val_mse: 346777024.0000 - val_mae: 11923.3564\n",
            "Epoch 186/500\n",
            "60/60 [==============================] - 0s 898us/step - loss: 9977.5495 - mse: 213259632.0000 - mae: 9977.5498 - val_loss: 11926.6471 - val_mse: 346984640.0000 - val_mae: 11926.6475\n",
            "Epoch 187/500\n",
            "60/60 [==============================] - 0s 762us/step - loss: 9274.5549 - mse: 198610864.0000 - mae: 9274.5537 - val_loss: 11920.5432 - val_mse: 346599328.0000 - val_mae: 11920.5430\n",
            "Epoch 188/500\n",
            "60/60 [==============================] - 0s 840us/step - loss: 9967.5175 - mse: 207820592.0000 - mae: 9967.5176 - val_loss: 11896.9711 - val_mse: 345118688.0000 - val_mae: 11896.9707\n",
            "Epoch 189/500\n",
            "60/60 [==============================] - 0s 898us/step - loss: 10127.5636 - mse: 205009712.0000 - mae: 10127.5635 - val_loss: 11896.4906 - val_mse: 345088160.0000 - val_mae: 11896.4902\n",
            "Epoch 190/500\n",
            "60/60 [==============================] - 0s 842us/step - loss: 10401.5251 - mse: 208520192.0000 - mae: 10401.5254 - val_loss: 11886.8309 - val_mse: 344484224.0000 - val_mae: 11886.8311\n",
            "Epoch 191/500\n",
            "60/60 [==============================] - 0s 805us/step - loss: 9811.0130 - mse: 201497568.0000 - mae: 9811.0127 - val_loss: 11885.5467 - val_mse: 344403808.0000 - val_mae: 11885.5459\n",
            "Epoch 192/500\n",
            "60/60 [==============================] - 0s 869us/step - loss: 9919.1376 - mse: 211838720.0000 - mae: 9919.1377 - val_loss: 11890.2386 - val_mse: 344696416.0000 - val_mae: 11890.2383\n",
            "Epoch 193/500\n",
            "60/60 [==============================] - 0s 876us/step - loss: 10214.3728 - mse: 208321824.0000 - mae: 10214.3730 - val_loss: 11869.2108 - val_mse: 343386592.0000 - val_mae: 11869.2100\n",
            "Epoch 194/500\n",
            "60/60 [==============================] - 0s 771us/step - loss: 9792.4529 - mse: 210113328.0000 - mae: 9792.4531 - val_loss: 11863.1139 - val_mse: 343007872.0000 - val_mae: 11863.1133\n",
            "Epoch 195/500\n",
            "60/60 [==============================] - 0s 818us/step - loss: 9416.7786 - mse: 171957152.0000 - mae: 9416.7793 - val_loss: 11850.8542 - val_mse: 342248896.0000 - val_mae: 11850.8545\n",
            "Epoch 196/500\n",
            "60/60 [==============================] - 0s 760us/step - loss: 9785.2753 - mse: 199377168.0000 - mae: 9785.2754 - val_loss: 11831.5067 - val_mse: 341056896.0000 - val_mae: 11831.5059\n",
            "Epoch 197/500\n",
            "60/60 [==============================] - 0s 770us/step - loss: 10925.9569 - mse: 219557184.0000 - mae: 10925.9561 - val_loss: 11835.1775 - val_mse: 341282144.0000 - val_mae: 11835.1768\n",
            "Epoch 198/500\n",
            "60/60 [==============================] - 0s 839us/step - loss: 9400.2275 - mse: 197434000.0000 - mae: 9400.2285 - val_loss: 11835.1645 - val_mse: 341281152.0000 - val_mae: 11835.1650\n",
            "Epoch 199/500\n",
            "60/60 [==============================] - 0s 828us/step - loss: 9191.9895 - mse: 177640080.0000 - mae: 9191.9893 - val_loss: 11842.0444 - val_mse: 341704320.0000 - val_mae: 11842.0439\n",
            "Epoch 200/500\n",
            "60/60 [==============================] - 0s 820us/step - loss: 9305.5319 - mse: 190724432.0000 - mae: 9305.5312 - val_loss: 11847.6436 - val_mse: 342049312.0000 - val_mae: 11847.6436\n",
            "Epoch 201/500\n",
            "60/60 [==============================] - 0s 748us/step - loss: 9668.3922 - mse: 188402240.0000 - mae: 9668.3916 - val_loss: 11843.8752 - val_mse: 341816640.0000 - val_mae: 11843.8750\n",
            "Epoch 202/500\n",
            "60/60 [==============================] - 0s 805us/step - loss: 10450.2492 - mse: 226794496.0000 - mae: 10450.2490 - val_loss: 11846.7302 - val_mse: 341992448.0000 - val_mae: 11846.7305\n",
            "Epoch 203/500\n",
            "60/60 [==============================] - 0s 925us/step - loss: 9033.3429 - mse: 191907008.0000 - mae: 9033.3428 - val_loss: 11829.8925 - val_mse: 340956000.0000 - val_mae: 11829.8926\n",
            "Epoch 204/500\n",
            "60/60 [==============================] - 0s 767us/step - loss: 10051.7447 - mse: 205657536.0000 - mae: 10051.7451 - val_loss: 11840.9257 - val_mse: 341634176.0000 - val_mae: 11840.9248\n",
            "Epoch 205/500\n",
            "60/60 [==============================] - 0s 989us/step - loss: 9497.9489 - mse: 198581168.0000 - mae: 9497.9492 - val_loss: 11814.7328 - val_mse: 340027008.0000 - val_mae: 11814.7324\n",
            "Epoch 206/500\n",
            "60/60 [==============================] - 0s 713us/step - loss: 10339.2742 - mse: 214375344.0000 - mae: 10339.2744 - val_loss: 11826.9147 - val_mse: 340772416.0000 - val_mae: 11826.9150\n",
            "Epoch 207/500\n",
            "60/60 [==============================] - 0s 872us/step - loss: 9242.5447 - mse: 183600032.0000 - mae: 9242.5449 - val_loss: 11818.0716 - val_mse: 340230496.0000 - val_mae: 11818.0713\n",
            "Epoch 208/500\n",
            "60/60 [==============================] - 0s 861us/step - loss: 9025.4368 - mse: 170879760.0000 - mae: 9025.4365 - val_loss: 11799.5819 - val_mse: 339102208.0000 - val_mae: 11799.5811\n",
            "Epoch 209/500\n",
            "60/60 [==============================] - 0s 808us/step - loss: 10072.5494 - mse: 197748672.0000 - mae: 10072.5498 - val_loss: 11802.5377 - val_mse: 339282144.0000 - val_mae: 11802.5371\n",
            "Epoch 210/500\n",
            "60/60 [==============================] - 0s 857us/step - loss: 9654.4661 - mse: 190332912.0000 - mae: 9654.4668 - val_loss: 11794.1158 - val_mse: 338769792.0000 - val_mae: 11794.1152\n",
            "Epoch 211/500\n",
            "60/60 [==============================] - 0s 787us/step - loss: 10365.1999 - mse: 204761024.0000 - mae: 10365.2002 - val_loss: 11782.3315 - val_mse: 338055136.0000 - val_mae: 11782.3311\n",
            "Epoch 212/500\n",
            "60/60 [==============================] - 0s 957us/step - loss: 10755.9865 - mse: 218133904.0000 - mae: 10755.9863 - val_loss: 11777.5383 - val_mse: 337764896.0000 - val_mae: 11777.5400\n",
            "Epoch 213/500\n",
            "60/60 [==============================] - 0s 848us/step - loss: 9509.1335 - mse: 193787952.0000 - mae: 9509.1338 - val_loss: 11765.2743 - val_mse: 337024928.0000 - val_mae: 11765.2754\n",
            "Epoch 214/500\n",
            "60/60 [==============================] - 0s 854us/step - loss: 10192.3070 - mse: 198427056.0000 - mae: 10192.3076 - val_loss: 11764.8870 - val_mse: 337001248.0000 - val_mae: 11764.8877\n",
            "Epoch 215/500\n",
            "60/60 [==============================] - 0s 974us/step - loss: 10277.7456 - mse: 208355056.0000 - mae: 10277.7461 - val_loss: 11780.4256 - val_mse: 337938624.0000 - val_mae: 11780.4248\n",
            "Epoch 216/500\n",
            "60/60 [==============================] - 0s 923us/step - loss: 9917.0327 - mse: 206447200.0000 - mae: 9917.0332 - val_loss: 11790.4490 - val_mse: 338545600.0000 - val_mae: 11790.4492\n",
            "Epoch 217/500\n",
            "60/60 [==============================] - 0s 890us/step - loss: 10365.9531 - mse: 227273456.0000 - mae: 10365.9531 - val_loss: 11784.7283 - val_mse: 338198656.0000 - val_mae: 11784.7285\n",
            "Epoch 218/500\n",
            "60/60 [==============================] - 0s 820us/step - loss: 9702.2351 - mse: 207998880.0000 - mae: 9702.2354 - val_loss: 11784.9194 - val_mse: 338210080.0000 - val_mae: 11784.9189\n",
            "Epoch 219/500\n",
            "60/60 [==============================] - 0s 826us/step - loss: 9459.6917 - mse: 182164400.0000 - mae: 9459.6914 - val_loss: 11776.6451 - val_mse: 337709216.0000 - val_mae: 11776.6455\n",
            "Epoch 220/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 9737.7497 - mse: 187538880.0000 - mae: 9737.7500 - val_loss: 11785.8261 - val_mse: 338264608.0000 - val_mae: 11785.8271\n",
            "Epoch 221/500\n",
            "60/60 [==============================] - 0s 836us/step - loss: 10244.2642 - mse: 217927264.0000 - mae: 10244.2637 - val_loss: 11784.7826 - val_mse: 338201184.0000 - val_mae: 11784.7822\n",
            "Epoch 222/500\n",
            "60/60 [==============================] - 0s 894us/step - loss: 10136.2289 - mse: 201134736.0000 - mae: 10136.2295 - val_loss: 11779.9262 - val_mse: 337907040.0000 - val_mae: 11779.9268\n",
            "Epoch 223/500\n",
            "60/60 [==============================] - 0s 975us/step - loss: 9701.7169 - mse: 200420768.0000 - mae: 9701.7168 - val_loss: 11774.4655 - val_mse: 337576640.0000 - val_mae: 11774.4648\n",
            "Epoch 224/500\n",
            "60/60 [==============================] - 0s 841us/step - loss: 9811.5834 - mse: 192809488.0000 - mae: 9811.5830 - val_loss: 11757.2598 - val_mse: 336540096.0000 - val_mae: 11757.2598\n",
            "Epoch 225/500\n",
            "60/60 [==============================] - 0s 774us/step - loss: 9706.3047 - mse: 210030800.0000 - mae: 9706.3037 - val_loss: 11764.4414 - val_mse: 336971872.0000 - val_mae: 11764.4414\n",
            "Epoch 226/500\n",
            "60/60 [==============================] - 0s 822us/step - loss: 10200.2440 - mse: 212453280.0000 - mae: 10200.2441 - val_loss: 11758.3330 - val_mse: 336604288.0000 - val_mae: 11758.3330\n",
            "Epoch 227/500\n",
            "60/60 [==============================] - 0s 837us/step - loss: 9484.5923 - mse: 183489744.0000 - mae: 9484.5928 - val_loss: 11781.6385 - val_mse: 338009504.0000 - val_mae: 11781.6387\n",
            "Epoch 228/500\n",
            "60/60 [==============================] - 0s 951us/step - loss: 10149.0524 - mse: 211660784.0000 - mae: 10149.0518 - val_loss: 11770.8872 - val_mse: 337359808.0000 - val_mae: 11770.8877\n",
            "Epoch 229/500\n",
            "60/60 [==============================] - 0s 878us/step - loss: 9925.0874 - mse: 197742656.0000 - mae: 9925.0879 - val_loss: 11740.7247 - val_mse: 335548288.0000 - val_mae: 11740.7246\n",
            "Epoch 230/500\n",
            "60/60 [==============================] - 0s 864us/step - loss: 10211.7446 - mse: 208001232.0000 - mae: 10211.7451 - val_loss: 11724.1764 - val_mse: 334561248.0000 - val_mae: 11724.1758\n",
            "Epoch 231/500\n",
            "60/60 [==============================] - 0s 826us/step - loss: 9343.9442 - mse: 195575920.0000 - mae: 9343.9434 - val_loss: 11720.5689 - val_mse: 334346624.0000 - val_mae: 11720.5684\n",
            "Epoch 232/500\n",
            "60/60 [==============================] - 0s 798us/step - loss: 10164.3844 - mse: 202053920.0000 - mae: 10164.3857 - val_loss: 11706.2921 - val_mse: 333499936.0000 - val_mae: 11706.2920\n",
            "Epoch 233/500\n",
            "60/60 [==============================] - 0s 830us/step - loss: 10221.2733 - mse: 211418384.0000 - mae: 10221.2725 - val_loss: 11722.2483 - val_mse: 334445984.0000 - val_mae: 11722.2480\n",
            "Epoch 234/500\n",
            "60/60 [==============================] - 0s 845us/step - loss: 9890.6680 - mse: 192778960.0000 - mae: 9890.6680 - val_loss: 11716.1136 - val_mse: 334081376.0000 - val_mae: 11716.1133\n",
            "Epoch 235/500\n",
            "60/60 [==============================] - 0s 924us/step - loss: 8888.9119 - mse: 191100240.0000 - mae: 8888.9121 - val_loss: 11715.3989 - val_mse: 334038688.0000 - val_mae: 11715.3994\n",
            "Epoch 236/500\n",
            "60/60 [==============================] - 0s 823us/step - loss: 10126.2630 - mse: 216852128.0000 - mae: 10126.2627 - val_loss: 11692.0250 - val_mse: 332656768.0000 - val_mae: 11692.0254\n",
            "Epoch 237/500\n",
            "60/60 [==============================] - 0s 813us/step - loss: 9173.2271 - mse: 162502080.0000 - mae: 9173.2275 - val_loss: 11683.9048 - val_mse: 332178784.0000 - val_mae: 11683.9043\n",
            "Epoch 238/500\n",
            "60/60 [==============================] - 0s 774us/step - loss: 9972.7722 - mse: 243591360.0000 - mae: 9972.7715 - val_loss: 11684.1337 - val_mse: 332192192.0000 - val_mae: 11684.1338\n",
            "Epoch 239/500\n",
            "60/60 [==============================] - 0s 844us/step - loss: 9032.4307 - mse: 168213360.0000 - mae: 9032.4316 - val_loss: 11693.2222 - val_mse: 332726752.0000 - val_mae: 11693.2227\n",
            "Epoch 240/500\n",
            "60/60 [==============================] - 0s 893us/step - loss: 9660.7500 - mse: 179368336.0000 - mae: 9660.7500 - val_loss: 11685.8160 - val_mse: 332290784.0000 - val_mae: 11685.8164\n",
            "Epoch 241/500\n",
            "60/60 [==============================] - 0s 924us/step - loss: 9861.7450 - mse: 209521456.0000 - mae: 9861.7451 - val_loss: 11702.4840 - val_mse: 333272960.0000 - val_mae: 11702.4834\n",
            "Epoch 242/500\n",
            "60/60 [==============================] - 0s 754us/step - loss: 9115.3652 - mse: 174563808.0000 - mae: 9115.3652 - val_loss: 11713.3414 - val_mse: 333915360.0000 - val_mae: 11713.3418\n",
            "Epoch 243/500\n",
            "60/60 [==============================] - 0s 841us/step - loss: 10572.5373 - mse: 204204432.0000 - mae: 10572.5371 - val_loss: 11716.2073 - val_mse: 334085280.0000 - val_mae: 11716.2070\n",
            "Epoch 244/500\n",
            "60/60 [==============================] - 0s 899us/step - loss: 9778.6126 - mse: 175627184.0000 - mae: 9778.6123 - val_loss: 11705.0047 - val_mse: 333421376.0000 - val_mae: 11705.0039\n",
            "Epoch 245/500\n",
            "60/60 [==============================] - 0s 861us/step - loss: 9795.9085 - mse: 196721152.0000 - mae: 9795.9082 - val_loss: 11704.4898 - val_mse: 333390560.0000 - val_mae: 11704.4893\n",
            "Epoch 246/500\n",
            "60/60 [==============================] - 0s 801us/step - loss: 10060.9857 - mse: 212327280.0000 - mae: 10060.9854 - val_loss: 11725.7870 - val_mse: 334653888.0000 - val_mae: 11725.7871\n",
            "Epoch 247/500\n",
            "60/60 [==============================] - 0s 825us/step - loss: 9775.9298 - mse: 182093104.0000 - mae: 9775.9307 - val_loss: 11694.0283 - val_mse: 332772704.0000 - val_mae: 11694.0283\n",
            "Epoch 248/500\n",
            "60/60 [==============================] - 0s 778us/step - loss: 9549.5967 - mse: 174260496.0000 - mae: 9549.5967 - val_loss: 11688.5859 - val_mse: 332452000.0000 - val_mae: 11688.5850\n",
            "Epoch 249/500\n",
            "60/60 [==============================] - 0s 920us/step - loss: 9591.9874 - mse: 200214208.0000 - mae: 9591.9873 - val_loss: 11673.1269 - val_mse: 331544256.0000 - val_mae: 11673.1270\n",
            "Epoch 250/500\n",
            "60/60 [==============================] - 0s 878us/step - loss: 8528.6200 - mse: 144860528.0000 - mae: 8528.6201 - val_loss: 11668.2592 - val_mse: 331259232.0000 - val_mae: 11668.2598\n",
            "Epoch 251/500\n",
            "60/60 [==============================] - 0s 759us/step - loss: 10876.1136 - mse: 217359680.0000 - mae: 10876.1133 - val_loss: 11674.9644 - val_mse: 331651520.0000 - val_mae: 11674.9648\n",
            "Epoch 252/500\n",
            "60/60 [==============================] - 0s 844us/step - loss: 9724.5295 - mse: 179719520.0000 - mae: 9724.5293 - val_loss: 11678.7842 - val_mse: 331875328.0000 - val_mae: 11678.7842\n",
            "Epoch 253/500\n",
            "60/60 [==============================] - 0s 890us/step - loss: 10537.7709 - mse: 236898208.0000 - mae: 10537.7705 - val_loss: 11700.4245 - val_mse: 333149056.0000 - val_mae: 11700.4248\n",
            "Epoch 254/500\n",
            "60/60 [==============================] - 0s 807us/step - loss: 10054.4315 - mse: 205427552.0000 - mae: 10054.4316 - val_loss: 11711.8179 - val_mse: 333823008.0000 - val_mae: 11711.8174\n",
            "Epoch 255/500\n",
            "60/60 [==============================] - 0s 818us/step - loss: 9435.7329 - mse: 181840800.0000 - mae: 9435.7334 - val_loss: 11713.5479 - val_mse: 333925376.0000 - val_mae: 11713.5479\n",
            "Epoch 256/500\n",
            "60/60 [==============================] - 0s 829us/step - loss: 11343.0389 - mse: 241277952.0000 - mae: 11343.0381 - val_loss: 11732.2713 - val_mse: 335038496.0000 - val_mae: 11732.2715\n",
            "Epoch 257/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 9975.5728 - mse: 224005296.0000 - mae: 9975.5732 - val_loss: 11738.7177 - val_mse: 335423136.0000 - val_mae: 11738.7178\n",
            "Epoch 258/500\n",
            "60/60 [==============================] - 0s 775us/step - loss: 9556.0905 - mse: 195661584.0000 - mae: 9556.0908 - val_loss: 11721.1756 - val_mse: 334377696.0000 - val_mae: 11721.1748\n",
            "Epoch 259/500\n",
            "60/60 [==============================] - 0s 854us/step - loss: 9865.7520 - mse: 191760112.0000 - mae: 9865.7520 - val_loss: 11736.4499 - val_mse: 335287360.0000 - val_mae: 11736.4502\n",
            "Epoch 260/500\n",
            "60/60 [==============================] - 0s 927us/step - loss: 9567.7153 - mse: 188741728.0000 - mae: 9567.7158 - val_loss: 11727.3861 - val_mse: 334746816.0000 - val_mae: 11727.3857\n",
            "Epoch 261/500\n",
            "60/60 [==============================] - 0s 962us/step - loss: 9048.4310 - mse: 177431456.0000 - mae: 9048.4316 - val_loss: 11709.5956 - val_mse: 333690048.0000 - val_mae: 11709.5957\n",
            "Epoch 262/500\n",
            "60/60 [==============================] - 0s 761us/step - loss: 9393.0889 - mse: 181574192.0000 - mae: 9393.0898 - val_loss: 11700.3853 - val_mse: 333145216.0000 - val_mae: 11700.3857\n",
            "Epoch 263/500\n",
            "60/60 [==============================] - 0s 883us/step - loss: 9921.5894 - mse: 194486704.0000 - mae: 9921.5898 - val_loss: 11694.8178 - val_mse: 332816576.0000 - val_mae: 11694.8174\n",
            "Epoch 264/500\n",
            "60/60 [==============================] - 0s 867us/step - loss: 9393.3552 - mse: 201655088.0000 - mae: 9393.3555 - val_loss: 11686.9577 - val_mse: 332353600.0000 - val_mae: 11686.9570\n",
            "Epoch 265/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 9259.6955 - mse: 170092368.0000 - mae: 9259.6943 - val_loss: 11703.5074 - val_mse: 333329312.0000 - val_mae: 11703.5068\n",
            "Epoch 266/500\n",
            "60/60 [==============================] - 0s 962us/step - loss: 10034.4784 - mse: 178682848.0000 - mae: 10034.4785 - val_loss: 11670.3984 - val_mse: 331381856.0000 - val_mae: 11670.3975\n",
            "Epoch 267/500\n",
            "60/60 [==============================] - 0s 851us/step - loss: 10599.9687 - mse: 193992672.0000 - mae: 10599.9688 - val_loss: 11653.1844 - val_mse: 330377024.0000 - val_mae: 11653.1846\n",
            "Epoch 268/500\n",
            "60/60 [==============================] - 0s 811us/step - loss: 9079.5969 - mse: 198437616.0000 - mae: 9079.5967 - val_loss: 11654.9362 - val_mse: 330478784.0000 - val_mae: 11654.9365\n",
            "Epoch 269/500\n",
            "60/60 [==============================] - 0s 804us/step - loss: 9747.7234 - mse: 172849424.0000 - mae: 9747.7236 - val_loss: 11640.2208 - val_mse: 329623616.0000 - val_mae: 11640.2207\n",
            "Epoch 270/500\n",
            "60/60 [==============================] - 0s 778us/step - loss: 9984.4716 - mse: 204070928.0000 - mae: 9984.4707 - val_loss: 11639.7039 - val_mse: 329593536.0000 - val_mae: 11639.7041\n",
            "Epoch 271/500\n",
            "60/60 [==============================] - 0s 929us/step - loss: 8873.4503 - mse: 173803968.0000 - mae: 8873.4502 - val_loss: 11642.8002 - val_mse: 329773024.0000 - val_mae: 11642.7998\n",
            "Epoch 272/500\n",
            "60/60 [==============================] - 0s 826us/step - loss: 9764.6653 - mse: 202203360.0000 - mae: 9764.6660 - val_loss: 11639.7706 - val_mse: 329597216.0000 - val_mae: 11639.7705\n",
            "Epoch 273/500\n",
            "60/60 [==============================] - 0s 881us/step - loss: 9579.8083 - mse: 189262432.0000 - mae: 9579.8086 - val_loss: 11628.7168 - val_mse: 328957408.0000 - val_mae: 11628.7168\n",
            "Epoch 274/500\n",
            "60/60 [==============================] - 0s 798us/step - loss: 8922.6261 - mse: 163191344.0000 - mae: 8922.6260 - val_loss: 11646.6005 - val_mse: 329993088.0000 - val_mae: 11646.5996\n",
            "Epoch 275/500\n",
            "60/60 [==============================] - 0s 859us/step - loss: 8993.2052 - mse: 183682576.0000 - mae: 8993.2041 - val_loss: 11627.1257 - val_mse: 328865152.0000 - val_mae: 11627.1250\n",
            "Epoch 276/500\n",
            "60/60 [==============================] - 0s 903us/step - loss: 10152.4933 - mse: 201393504.0000 - mae: 10152.4941 - val_loss: 11622.6761 - val_mse: 328608224.0000 - val_mae: 11622.6768\n",
            "Epoch 277/500\n",
            "60/60 [==============================] - 0s 979us/step - loss: 10074.7900 - mse: 205537120.0000 - mae: 10074.7900 - val_loss: 11631.9925 - val_mse: 329146208.0000 - val_mae: 11631.9932\n",
            "Epoch 278/500\n",
            "60/60 [==============================] - 0s 987us/step - loss: 9308.3398 - mse: 179920544.0000 - mae: 9308.3398 - val_loss: 11651.3302 - val_mse: 330267712.0000 - val_mae: 11651.3301\n",
            "Epoch 279/500\n",
            "60/60 [==============================] - 0s 912us/step - loss: 10345.7964 - mse: 206180176.0000 - mae: 10345.7959 - val_loss: 11648.7056 - val_mse: 330115040.0000 - val_mae: 11648.7061\n",
            "Epoch 280/500\n",
            "60/60 [==============================] - 0s 921us/step - loss: 9655.1396 - mse: 187307584.0000 - mae: 9655.1396 - val_loss: 11636.2200 - val_mse: 329390560.0000 - val_mae: 11636.2197\n",
            "Epoch 281/500\n",
            "60/60 [==============================] - 0s 907us/step - loss: 9898.2124 - mse: 211523120.0000 - mae: 9898.2129 - val_loss: 11630.3320 - val_mse: 329049824.0000 - val_mae: 11630.3311\n",
            "Epoch 282/500\n",
            "60/60 [==============================] - 0s 896us/step - loss: 9578.2983 - mse: 187798080.0000 - mae: 9578.2979 - val_loss: 11644.1591 - val_mse: 329850528.0000 - val_mae: 11644.1582\n",
            "Epoch 283/500\n",
            "60/60 [==============================] - 0s 927us/step - loss: 10147.9653 - mse: 213751200.0000 - mae: 10147.9658 - val_loss: 11651.8364 - val_mse: 330296800.0000 - val_mae: 11651.8379\n",
            "Epoch 284/500\n",
            "60/60 [==============================] - 0s 938us/step - loss: 10448.3840 - mse: 227475504.0000 - mae: 10448.3838 - val_loss: 11644.9443 - val_mse: 329896160.0000 - val_mae: 11644.9434\n",
            "Epoch 285/500\n",
            "60/60 [==============================] - 0s 853us/step - loss: 10284.7583 - mse: 230699856.0000 - mae: 10284.7588 - val_loss: 11651.1938 - val_mse: 330259232.0000 - val_mae: 11651.1934\n",
            "Epoch 286/500\n",
            "60/60 [==============================] - 0s 933us/step - loss: 8382.5865 - mse: 164112800.0000 - mae: 8382.5869 - val_loss: 11652.3593 - val_mse: 330326944.0000 - val_mae: 11652.3594\n",
            "Epoch 287/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 9850.1358 - mse: 217457904.0000 - mae: 9850.1357 - val_loss: 11644.1075 - val_mse: 329847168.0000 - val_mae: 11644.1074\n",
            "Epoch 288/500\n",
            "60/60 [==============================] - 0s 843us/step - loss: 10001.1327 - mse: 214459104.0000 - mae: 10001.1338 - val_loss: 11622.0465 - val_mse: 328570944.0000 - val_mae: 11622.0469\n",
            "Epoch 289/500\n",
            "60/60 [==============================] - 0s 790us/step - loss: 9038.6809 - mse: 163159328.0000 - mae: 9038.6816 - val_loss: 11625.4979 - val_mse: 328769728.0000 - val_mae: 11625.4980\n",
            "Epoch 290/500\n",
            "60/60 [==============================] - 0s 868us/step - loss: 10096.7104 - mse: 203728864.0000 - mae: 10096.7100 - val_loss: 11615.9607 - val_mse: 328219968.0000 - val_mae: 11615.9600\n",
            "Epoch 291/500\n",
            "60/60 [==============================] - 0s 888us/step - loss: 9627.1700 - mse: 182774000.0000 - mae: 9627.1709 - val_loss: 11632.4819 - val_mse: 329172928.0000 - val_mae: 11632.4814\n",
            "Epoch 292/500\n",
            "60/60 [==============================] - 0s 880us/step - loss: 9344.3486 - mse: 183982192.0000 - mae: 9344.3486 - val_loss: 11640.0425 - val_mse: 329610592.0000 - val_mae: 11640.0430\n",
            "Epoch 293/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 10043.3750 - mse: 201331264.0000 - mae: 10043.3750 - val_loss: 11621.0666 - val_mse: 328513600.0000 - val_mae: 11621.0664\n",
            "Epoch 294/500\n",
            "60/60 [==============================] - 0s 943us/step - loss: 9746.6631 - mse: 211209664.0000 - mae: 9746.6621 - val_loss: 11627.0804 - val_mse: 328860480.0000 - val_mae: 11627.0801\n",
            "Epoch 295/500\n",
            "60/60 [==============================] - 0s 829us/step - loss: 10307.2604 - mse: 213521152.0000 - mae: 10307.2607 - val_loss: 11643.2892 - val_mse: 329798432.0000 - val_mae: 11643.2900\n",
            "Epoch 296/500\n",
            "60/60 [==============================] - 0s 959us/step - loss: 10472.6738 - mse: 203862736.0000 - mae: 10472.6738 - val_loss: 11629.6921 - val_mse: 329010848.0000 - val_mae: 11629.6914\n",
            "Epoch 297/500\n",
            "60/60 [==============================] - 0s 789us/step - loss: 8890.7883 - mse: 176265520.0000 - mae: 8890.7881 - val_loss: 11620.0134 - val_mse: 328452288.0000 - val_mae: 11620.0127\n",
            "Epoch 298/500\n",
            "60/60 [==============================] - 0s 844us/step - loss: 10182.1243 - mse: 213361696.0000 - mae: 10182.1240 - val_loss: 11617.7071 - val_mse: 328319328.0000 - val_mae: 11617.7070\n",
            "Epoch 299/500\n",
            "60/60 [==============================] - 0s 805us/step - loss: 10165.2747 - mse: 200454128.0000 - mae: 10165.2754 - val_loss: 11635.1346 - val_mse: 329325248.0000 - val_mae: 11635.1348\n",
            "Epoch 300/500\n",
            "60/60 [==============================] - 0s 936us/step - loss: 9810.1449 - mse: 202150416.0000 - mae: 9810.1445 - val_loss: 11630.7188 - val_mse: 329069696.0000 - val_mae: 11630.7188\n",
            "Epoch 301/500\n",
            "60/60 [==============================] - 0s 916us/step - loss: 9485.2901 - mse: 199382160.0000 - mae: 9485.2910 - val_loss: 11627.9217 - val_mse: 328907904.0000 - val_mae: 11627.9209\n",
            "Epoch 302/500\n",
            "60/60 [==============================] - 0s 923us/step - loss: 9655.0175 - mse: 190753360.0000 - mae: 9655.0166 - val_loss: 11635.9309 - val_mse: 329370912.0000 - val_mae: 11635.9316\n",
            "Epoch 303/500\n",
            "60/60 [==============================] - 0s 811us/step - loss: 10264.8418 - mse: 210586992.0000 - mae: 10264.8428 - val_loss: 11625.5181 - val_mse: 328768864.0000 - val_mae: 11625.5186\n",
            "Epoch 304/500\n",
            "60/60 [==============================] - 0s 879us/step - loss: 9972.7006 - mse: 195414432.0000 - mae: 9972.7012 - val_loss: 11636.7548 - val_mse: 329418336.0000 - val_mae: 11636.7539\n",
            "Epoch 305/500\n",
            "60/60 [==============================] - 0s 834us/step - loss: 9352.0724 - mse: 174794688.0000 - mae: 9352.0723 - val_loss: 11650.1471 - val_mse: 330195328.0000 - val_mae: 11650.1475\n",
            "Epoch 306/500\n",
            "60/60 [==============================] - 0s 851us/step - loss: 9411.4696 - mse: 199990016.0000 - mae: 9411.4697 - val_loss: 11642.2779 - val_mse: 329738176.0000 - val_mae: 11642.2783\n",
            "Epoch 307/500\n",
            "60/60 [==============================] - 0s 914us/step - loss: 9901.2237 - mse: 193637584.0000 - mae: 9901.2236 - val_loss: 11643.1628 - val_mse: 329789408.0000 - val_mae: 11643.1621\n",
            "Epoch 308/500\n",
            "60/60 [==============================] - 0s 796us/step - loss: 9163.1298 - mse: 158738592.0000 - mae: 9163.1289 - val_loss: 11641.2315 - val_mse: 329677248.0000 - val_mae: 11641.2314\n",
            "Epoch 309/500\n",
            "60/60 [==============================] - 0s 799us/step - loss: 9067.3341 - mse: 186048528.0000 - mae: 9067.3330 - val_loss: 11637.0252 - val_mse: 329433408.0000 - val_mae: 11637.0254\n",
            "Epoch 310/500\n",
            "60/60 [==============================] - 0s 908us/step - loss: 10393.5203 - mse: 215546432.0000 - mae: 10393.5205 - val_loss: 11633.3054 - val_mse: 329217920.0000 - val_mae: 11633.3066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 80 samples, validate on 70 samples\n",
            "Epoch 1/500\n",
            "80/80 [==============================] - 2s 23ms/step - loss: 35870.6987 - mse: 1551459072.0000 - mae: 35870.6992 - val_loss: 37658.1875 - val_mse: 1674844544.0000 - val_mae: 37658.1875\n",
            "Epoch 2/500\n",
            "80/80 [==============================] - 0s 740us/step - loss: 35870.3269 - mse: 1551435008.0000 - mae: 35870.3281 - val_loss: 37656.9361 - val_mse: 1674758144.0000 - val_mae: 37656.9375\n",
            "Epoch 3/500\n",
            "80/80 [==============================] - 0s 714us/step - loss: 35865.5627 - mse: 1551120256.0000 - mae: 35865.5664 - val_loss: 37645.6655 - val_mse: 1673951872.0000 - val_mae: 37645.6641\n",
            "Epoch 4/500\n",
            "80/80 [==============================] - 0s 711us/step - loss: 35847.2859 - mse: 1549872768.0000 - mae: 35847.2891 - val_loss: 37618.9805 - val_mse: 1671981824.0000 - val_mae: 37618.9805\n",
            "Epoch 5/500\n",
            "80/80 [==============================] - 0s 738us/step - loss: 35813.9175 - mse: 1547563904.0000 - mae: 35813.9180 - val_loss: 37581.4269 - val_mse: 1669186944.0000 - val_mae: 37581.4258\n",
            "Epoch 6/500\n",
            "80/80 [==============================] - 0s 777us/step - loss: 35773.1460 - mse: 1544650368.0000 - mae: 35773.1445 - val_loss: 37532.1230 - val_mse: 1665512192.0000 - val_mae: 37532.1250\n",
            "Epoch 7/500\n",
            "80/80 [==============================] - 0s 887us/step - loss: 35720.5078 - mse: 1541185152.0000 - mae: 35720.5078 - val_loss: 37474.5608 - val_mse: 1661220736.0000 - val_mae: 37474.5625\n",
            "Epoch 8/500\n",
            "80/80 [==============================] - 0s 771us/step - loss: 35667.9727 - mse: 1536955520.0000 - mae: 35667.9727 - val_loss: 37410.0614 - val_mse: 1656410752.0000 - val_mae: 37410.0625\n",
            "Epoch 9/500\n",
            "80/80 [==============================] - 0s 720us/step - loss: 35595.0024 - mse: 1532331264.0000 - mae: 35595.0039 - val_loss: 37336.5583 - val_mse: 1650936832.0000 - val_mae: 37336.5625\n",
            "Epoch 10/500\n",
            "80/80 [==============================] - 0s 684us/step - loss: 35524.0154 - mse: 1527133440.0000 - mae: 35524.0156 - val_loss: 37256.2520 - val_mse: 1644961152.0000 - val_mae: 37256.2500\n",
            "Epoch 11/500\n",
            "80/80 [==============================] - 0s 651us/step - loss: 35425.0803 - mse: 1519914368.0000 - mae: 35425.0781 - val_loss: 37165.8622 - val_mse: 1638244864.0000 - val_mae: 37165.8633\n",
            "Epoch 12/500\n",
            "80/80 [==============================] - 0s 663us/step - loss: 35313.0935 - mse: 1512172032.0000 - mae: 35313.0898 - val_loss: 37065.6657 - val_mse: 1630814208.0000 - val_mae: 37065.6641\n",
            "Epoch 13/500\n",
            "80/80 [==============================] - 0s 709us/step - loss: 35227.8901 - mse: 1505764608.0000 - mae: 35227.8867 - val_loss: 36961.6562 - val_mse: 1623119872.0000 - val_mae: 36961.6562\n",
            "Epoch 14/500\n",
            "80/80 [==============================] - 0s 675us/step - loss: 35121.2512 - mse: 1498233216.0000 - mae: 35121.2539 - val_loss: 36849.9403 - val_mse: 1614878336.0000 - val_mae: 36849.9375\n",
            "Epoch 15/500\n",
            "80/80 [==============================] - 0s 678us/step - loss: 35014.4812 - mse: 1489693312.0000 - mae: 35014.4805 - val_loss: 36729.1842 - val_mse: 1605997312.0000 - val_mae: 36729.1875\n",
            "Epoch 16/500\n",
            "80/80 [==============================] - 0s 688us/step - loss: 34882.6924 - mse: 1480845568.0000 - mae: 34882.6953 - val_loss: 36600.1822 - val_mse: 1596541312.0000 - val_mae: 36600.1836\n",
            "Epoch 17/500\n",
            "80/80 [==============================] - 0s 787us/step - loss: 34691.2859 - mse: 1469953792.0000 - mae: 34691.2852 - val_loss: 36457.8398 - val_mse: 1586146944.0000 - val_mae: 36457.8398\n",
            "Epoch 18/500\n",
            "80/80 [==============================] - 0s 758us/step - loss: 34562.1482 - mse: 1460072192.0000 - mae: 34562.1484 - val_loss: 36310.2355 - val_mse: 1575408256.0000 - val_mae: 36310.2344\n",
            "Epoch 19/500\n",
            "80/80 [==============================] - 0s 747us/step - loss: 34449.8691 - mse: 1452461952.0000 - mae: 34449.8672 - val_loss: 36159.3192 - val_mse: 1564471936.0000 - val_mae: 36159.3164\n",
            "Epoch 20/500\n",
            "80/80 [==============================] - 0s 992us/step - loss: 34274.7654 - mse: 1441835776.0000 - mae: 34274.7656 - val_loss: 35999.5028 - val_mse: 1552942464.0000 - val_mae: 35999.5039\n",
            "Epoch 21/500\n",
            "80/80 [==============================] - 0s 814us/step - loss: 34160.0769 - mse: 1432644992.0000 - mae: 34160.0781 - val_loss: 35837.3979 - val_mse: 1541298048.0000 - val_mae: 35837.3984\n",
            "Epoch 22/500\n",
            "80/80 [==============================] - 0s 869us/step - loss: 33942.4106 - mse: 1417325824.0000 - mae: 33942.4141 - val_loss: 35662.8557 - val_mse: 1528818176.0000 - val_mae: 35662.8555\n",
            "Epoch 23/500\n",
            "80/80 [==============================] - 0s 726us/step - loss: 33855.9597 - mse: 1411729152.0000 - mae: 33855.9609 - val_loss: 35488.9160 - val_mse: 1516441984.0000 - val_mae: 35488.9141\n",
            "Epoch 24/500\n",
            "80/80 [==============================] - 0s 810us/step - loss: 33619.3120 - mse: 1391654656.0000 - mae: 33619.3125 - val_loss: 35302.6258 - val_mse: 1503252352.0000 - val_mae: 35302.6250\n",
            "Epoch 25/500\n",
            "80/80 [==============================] - 0s 817us/step - loss: 33425.1035 - mse: 1385079296.0000 - mae: 33425.1055 - val_loss: 35108.9559 - val_mse: 1489616256.0000 - val_mae: 35108.9570\n",
            "Epoch 26/500\n",
            "80/80 [==============================] - 0s 793us/step - loss: 33094.7639 - mse: 1365766912.0000 - mae: 33094.7617 - val_loss: 34899.1629 - val_mse: 1474929536.0000 - val_mae: 34899.1641\n",
            "Epoch 27/500\n",
            "80/80 [==============================] - 0s 836us/step - loss: 32989.9924 - mse: 1354660096.0000 - mae: 32989.9922 - val_loss: 34689.9414 - val_mse: 1460369408.0000 - val_mae: 34689.9414\n",
            "Epoch 28/500\n",
            "80/80 [==============================] - 0s 729us/step - loss: 32748.1545 - mse: 1337138688.0000 - mae: 32748.1562 - val_loss: 34472.7782 - val_mse: 1445348224.0000 - val_mae: 34472.7773\n",
            "Epoch 29/500\n",
            "80/80 [==============================] - 0s 919us/step - loss: 32637.1555 - mse: 1330277376.0000 - mae: 32637.1562 - val_loss: 34254.6342 - val_mse: 1430353664.0000 - val_mae: 34254.6328\n",
            "Epoch 30/500\n",
            "80/80 [==============================] - 0s 743us/step - loss: 32475.2783 - mse: 1313563136.0000 - mae: 32475.2773 - val_loss: 34033.1144 - val_mse: 1415223424.0000 - val_mae: 34033.1133\n",
            "Epoch 31/500\n",
            "80/80 [==============================] - 0s 766us/step - loss: 31983.0400 - mse: 1293620480.0000 - mae: 31983.0410 - val_loss: 33788.9046 - val_mse: 1398659584.0000 - val_mae: 33788.9023\n",
            "Epoch 32/500\n",
            "80/80 [==============================] - 0s 725us/step - loss: 31815.7388 - mse: 1281891456.0000 - mae: 31815.7402 - val_loss: 33542.5921 - val_mse: 1382073216.0000 - val_mae: 33542.5938\n",
            "Epoch 33/500\n",
            "80/80 [==============================] - 0s 742us/step - loss: 31612.4692 - mse: 1261148032.0000 - mae: 31612.4688 - val_loss: 33291.7310 - val_mse: 1365304192.0000 - val_mae: 33291.7305\n",
            "Epoch 34/500\n",
            "80/80 [==============================] - 0s 711us/step - loss: 31392.2671 - mse: 1250272384.0000 - mae: 31392.2656 - val_loss: 33036.8622 - val_mse: 1348397312.0000 - val_mae: 33036.8633\n",
            "Epoch 35/500\n",
            "80/80 [==============================] - 0s 795us/step - loss: 31034.8760 - mse: 1228028160.0000 - mae: 31034.8750 - val_loss: 32768.7849 - val_mse: 1330753664.0000 - val_mae: 32768.7852\n",
            "Epoch 36/500\n",
            "80/80 [==============================] - 0s 744us/step - loss: 30731.3752 - mse: 1206296832.0000 - mae: 30731.3750 - val_loss: 32492.3624 - val_mse: 1312711680.0000 - val_mae: 32492.3652\n",
            "Epoch 37/500\n",
            "80/80 [==============================] - 0s 728us/step - loss: 30744.7717 - mse: 1209762816.0000 - mae: 30744.7715 - val_loss: 32223.6105 - val_mse: 1295317120.0000 - val_mae: 32223.6113\n",
            "Epoch 38/500\n",
            "80/80 [==============================] - 0s 863us/step - loss: 30162.0696 - mse: 1172976896.0000 - mae: 30162.0684 - val_loss: 31933.5336 - val_mse: 1276704768.0000 - val_mae: 31933.5312\n",
            "Epoch 39/500\n",
            "80/80 [==============================] - 0s 763us/step - loss: 30057.0786 - mse: 1168655104.0000 - mae: 30057.0820 - val_loss: 31645.0360 - val_mse: 1258360960.0000 - val_mae: 31645.0352\n",
            "Epoch 40/500\n",
            "80/80 [==============================] - 0s 796us/step - loss: 29823.1963 - mse: 1158052736.0000 - mae: 29823.1973 - val_loss: 31352.7864 - val_mse: 1239948672.0000 - val_mae: 31352.7852\n",
            "Epoch 41/500\n",
            "80/80 [==============================] - 0s 628us/step - loss: 29744.5867 - mse: 1145806976.0000 - mae: 29744.5879 - val_loss: 31062.6989 - val_mse: 1221840384.0000 - val_mae: 31062.6992\n",
            "Epoch 42/500\n",
            "80/80 [==============================] - 0s 795us/step - loss: 29141.1499 - mse: 1112724608.0000 - mae: 29141.1504 - val_loss: 30751.4487 - val_mse: 1202598656.0000 - val_mae: 30751.4473\n",
            "Epoch 43/500\n",
            "80/80 [==============================] - 0s 771us/step - loss: 28702.3403 - mse: 1094285056.0000 - mae: 28702.3398 - val_loss: 30427.2885 - val_mse: 1182765696.0000 - val_mae: 30427.2852\n",
            "Epoch 44/500\n",
            "80/80 [==============================] - 0s 640us/step - loss: 28697.6689 - mse: 1096150272.0000 - mae: 28697.6680 - val_loss: 30110.6038 - val_mse: 1163592704.0000 - val_mae: 30110.6074\n",
            "Epoch 45/500\n",
            "80/80 [==============================] - 0s 621us/step - loss: 28130.3372 - mse: 1047664000.0000 - mae: 28130.3379 - val_loss: 29776.4189 - val_mse: 1143577472.0000 - val_mae: 29776.4180\n",
            "Epoch 46/500\n",
            "80/80 [==============================] - 0s 651us/step - loss: 27590.5491 - mse: 1029173632.0000 - mae: 27590.5508 - val_loss: 29427.7333 - val_mse: 1122932224.0000 - val_mae: 29427.7363\n",
            "Epoch 47/500\n",
            "80/80 [==============================] - 0s 689us/step - loss: 27404.3396 - mse: 1016205824.0000 - mae: 27404.3379 - val_loss: 29079.7178 - val_mse: 1102568320.0000 - val_mae: 29079.7188\n",
            "Epoch 48/500\n",
            "80/80 [==============================] - 0s 636us/step - loss: 27184.1802 - mse: 1006462144.0000 - mae: 27184.1816 - val_loss: 28729.9068 - val_mse: 1082344576.0000 - val_mae: 28729.9062\n",
            "Epoch 49/500\n",
            "80/80 [==============================] - 0s 662us/step - loss: 26842.1738 - mse: 981538944.0000 - mae: 26842.1758 - val_loss: 28373.5935 - val_mse: 1061995776.0000 - val_mae: 28373.5938\n",
            "Epoch 50/500\n",
            "80/80 [==============================] - 0s 718us/step - loss: 26088.7207 - mse: 945899392.0000 - mae: 26088.7207 - val_loss: 28006.1982 - val_mse: 1041280000.0000 - val_mae: 28006.1992\n",
            "Epoch 51/500\n",
            "80/80 [==============================] - 0s 857us/step - loss: 26570.7617 - mse: 972080256.0000 - mae: 26570.7617 - val_loss: 27652.1490 - val_mse: 1021572800.0000 - val_mae: 27652.1504\n",
            "Epoch 52/500\n",
            "80/80 [==============================] - 0s 812us/step - loss: 26094.5354 - mse: 947272896.0000 - mae: 26094.5352 - val_loss: 27286.3500 - val_mse: 1001474560.0000 - val_mae: 27286.3496\n",
            "Epoch 53/500\n",
            "80/80 [==============================] - 0s 745us/step - loss: 25759.0457 - mse: 934022272.0000 - mae: 25759.0469 - val_loss: 26914.2977 - val_mse: 981307456.0000 - val_mae: 26914.2988\n",
            "Epoch 54/500\n",
            "80/80 [==============================] - 0s 777us/step - loss: 25304.7756 - mse: 907943936.0000 - mae: 25304.7754 - val_loss: 26531.8179 - val_mse: 960863552.0000 - val_mae: 26531.8184\n",
            "Epoch 55/500\n",
            "80/80 [==============================] - 0s 747us/step - loss: 24747.7578 - mse: 888641920.0000 - mae: 24747.7559 - val_loss: 26147.5162 - val_mse: 940617024.0000 - val_mae: 26147.5156\n",
            "Epoch 56/500\n",
            "80/80 [==============================] - 0s 770us/step - loss: 24033.1637 - mse: 857338624.0000 - mae: 24033.1621 - val_loss: 25754.5370 - val_mse: 920218688.0000 - val_mae: 25754.5352\n",
            "Epoch 57/500\n",
            "80/80 [==============================] - 0s 716us/step - loss: 23792.4534 - mse: 840744064.0000 - mae: 23792.4551 - val_loss: 25360.0081 - val_mse: 900051200.0000 - val_mae: 25360.0098\n",
            "Epoch 58/500\n",
            "80/80 [==============================] - 0s 789us/step - loss: 23238.2617 - mse: 800872000.0000 - mae: 23238.2617 - val_loss: 24958.1805 - val_mse: 879829440.0000 - val_mae: 24958.1797\n",
            "Epoch 59/500\n",
            "80/80 [==============================] - 0s 814us/step - loss: 23453.3141 - mse: 800737280.0000 - mae: 23453.3125 - val_loss: 24567.0057 - val_mse: 860453952.0000 - val_mae: 24567.0039\n",
            "Epoch 60/500\n",
            "80/80 [==============================] - 0s 756us/step - loss: 23135.6259 - mse: 828235136.0000 - mae: 23135.6250 - val_loss: 24166.7737 - val_mse: 840947968.0000 - val_mae: 24166.7754\n",
            "Epoch 61/500\n",
            "80/80 [==============================] - 0s 680us/step - loss: 22035.5068 - mse: 746086592.0000 - mae: 22035.5078 - val_loss: 23738.2055 - val_mse: 820415680.0000 - val_mae: 23738.2031\n",
            "Epoch 62/500\n",
            "80/80 [==============================] - 0s 704us/step - loss: 21646.9119 - mse: 722398208.0000 - mae: 21646.9121 - val_loss: 23314.8055 - val_mse: 800491200.0000 - val_mae: 23314.8027\n",
            "Epoch 63/500\n",
            "80/80 [==============================] - 0s 706us/step - loss: 21043.6912 - mse: 706195456.0000 - mae: 21043.6914 - val_loss: 22891.0047 - val_mse: 780907008.0000 - val_mae: 22891.0039\n",
            "Epoch 64/500\n",
            "80/80 [==============================] - 0s 668us/step - loss: 21508.8909 - mse: 712002176.0000 - mae: 21508.8945 - val_loss: 22491.3989 - val_mse: 762769984.0000 - val_mae: 22491.4004\n",
            "Epoch 65/500\n",
            "80/80 [==============================] - 0s 853us/step - loss: 21346.4280 - mse: 717594048.0000 - mae: 21346.4277 - val_loss: 22091.0188 - val_mse: 744917888.0000 - val_mae: 22091.0176\n",
            "Epoch 66/500\n",
            "80/80 [==============================] - 0s 819us/step - loss: 19925.1823 - mse: 651643584.0000 - mae: 19925.1836 - val_loss: 21671.9574 - val_mse: 726576128.0000 - val_mae: 21671.9570\n",
            "Epoch 67/500\n",
            "80/80 [==============================] - 0s 621us/step - loss: 19959.0767 - mse: 677964672.0000 - mae: 19959.0762 - val_loss: 21220.2042 - val_mse: 707198528.0000 - val_mae: 21220.2031\n",
            "Epoch 68/500\n",
            "80/80 [==============================] - 0s 839us/step - loss: 20042.8663 - mse: 666031744.0000 - mae: 20042.8652 - val_loss: 20820.5376 - val_mse: 690394112.0000 - val_mae: 20820.5371\n",
            "Epoch 69/500\n",
            "80/80 [==============================] - 0s 847us/step - loss: 19770.0126 - mse: 672088448.0000 - mae: 19770.0117 - val_loss: 20385.6152 - val_mse: 672471232.0000 - val_mae: 20385.6152\n",
            "Epoch 70/500\n",
            "80/80 [==============================] - 0s 814us/step - loss: 19320.6801 - mse: 651682048.0000 - mae: 19320.6797 - val_loss: 19948.4167 - val_mse: 654835584.0000 - val_mae: 19948.4180\n",
            "Epoch 71/500\n",
            "80/80 [==============================] - 0s 766us/step - loss: 18670.2344 - mse: 603221184.0000 - mae: 18670.2363 - val_loss: 19573.3187 - val_mse: 640009024.0000 - val_mae: 19573.3184\n",
            "Epoch 72/500\n",
            "80/80 [==============================] - 0s 816us/step - loss: 18720.7953 - mse: 605158208.0000 - mae: 18720.7930 - val_loss: 19210.0249 - val_mse: 625917184.0000 - val_mae: 19210.0254\n",
            "Epoch 73/500\n",
            "80/80 [==============================] - 0s 879us/step - loss: 17594.6233 - mse: 566295040.0000 - mae: 17594.6250 - val_loss: 18796.6384 - val_mse: 610203968.0000 - val_mae: 18796.6367\n",
            "Epoch 74/500\n",
            "80/80 [==============================] - 0s 808us/step - loss: 17759.4766 - mse: 545332672.0000 - mae: 17759.4766 - val_loss: 18372.8719 - val_mse: 594450368.0000 - val_mae: 18372.8711\n",
            "Epoch 75/500\n",
            "80/80 [==============================] - 0s 726us/step - loss: 16915.2527 - mse: 543795584.0000 - mae: 16915.2539 - val_loss: 18017.0191 - val_mse: 581498816.0000 - val_mae: 18017.0176\n",
            "Epoch 76/500\n",
            "80/80 [==============================] - 0s 756us/step - loss: 17651.8724 - mse: 570216256.0000 - mae: 17651.8750 - val_loss: 17673.8550 - val_mse: 569248768.0000 - val_mae: 17673.8535\n",
            "Epoch 77/500\n",
            "80/80 [==============================] - 0s 698us/step - loss: 16574.3599 - mse: 547677824.0000 - mae: 16574.3594 - val_loss: 17326.9512 - val_mse: 557105472.0000 - val_mae: 17326.9512\n",
            "Epoch 78/500\n",
            "80/80 [==============================] - 0s 742us/step - loss: 16948.0347 - mse: 548261632.0000 - mae: 16948.0352 - val_loss: 17089.4644 - val_mse: 548929984.0000 - val_mae: 17089.4648\n",
            "Epoch 79/500\n",
            "80/80 [==============================] - 0s 869us/step - loss: 17199.8885 - mse: 555572416.0000 - mae: 17199.8867 - val_loss: 16769.5261 - val_mse: 538095360.0000 - val_mae: 16769.5254\n",
            "Epoch 80/500\n",
            "80/80 [==============================] - 0s 724us/step - loss: 16495.5391 - mse: 509508672.0000 - mae: 16495.5371 - val_loss: 16435.8341 - val_mse: 526535712.0000 - val_mae: 16435.8359\n",
            "Epoch 81/500\n",
            "80/80 [==============================] - 0s 709us/step - loss: 15873.0034 - mse: 474576576.0000 - mae: 15873.0049 - val_loss: 16145.1669 - val_mse: 515786400.0000 - val_mae: 16145.1680\n",
            "Epoch 82/500\n",
            "80/80 [==============================] - 0s 758us/step - loss: 15703.2865 - mse: 468959840.0000 - mae: 15703.2871 - val_loss: 15894.7891 - val_mse: 506053088.0000 - val_mae: 15894.7891\n",
            "Epoch 83/500\n",
            "80/80 [==============================] - 0s 801us/step - loss: 15525.9501 - mse: 477608608.0000 - mae: 15525.9482 - val_loss: 15701.7653 - val_mse: 497681952.0000 - val_mae: 15701.7646\n",
            "Epoch 84/500\n",
            "80/80 [==============================] - 0s 820us/step - loss: 15723.4949 - mse: 461482944.0000 - mae: 15723.4951 - val_loss: 15511.6620 - val_mse: 488284896.0000 - val_mae: 15511.6621\n",
            "Epoch 85/500\n",
            "80/80 [==============================] - 0s 781us/step - loss: 16325.8345 - mse: 497104192.0000 - mae: 16325.8359 - val_loss: 15387.7337 - val_mse: 481536672.0000 - val_mae: 15387.7344\n",
            "Epoch 86/500\n",
            "80/80 [==============================] - 0s 728us/step - loss: 16086.6273 - mse: 469948416.0000 - mae: 16086.6270 - val_loss: 15271.3487 - val_mse: 475154016.0000 - val_mae: 15271.3496\n",
            "Epoch 87/500\n",
            "80/80 [==============================] - 0s 671us/step - loss: 16040.7847 - mse: 470072672.0000 - mae: 16040.7842 - val_loss: 15118.9015 - val_mse: 466095040.0000 - val_mae: 15118.9033\n",
            "Epoch 88/500\n",
            "80/80 [==============================] - 0s 720us/step - loss: 16208.1078 - mse: 511106720.0000 - mae: 16208.1094 - val_loss: 15034.9102 - val_mse: 460462976.0000 - val_mae: 15034.9111\n",
            "Epoch 89/500\n",
            "80/80 [==============================] - 0s 703us/step - loss: 15407.1753 - mse: 425415040.0000 - mae: 15407.1748 - val_loss: 14924.8530 - val_mse: 453199584.0000 - val_mae: 14924.8535\n",
            "Epoch 90/500\n",
            "80/80 [==============================] - 0s 802us/step - loss: 15752.1449 - mse: 453344864.0000 - mae: 15752.1436 - val_loss: 14846.7392 - val_mse: 448123904.0000 - val_mae: 14846.7393\n",
            "Epoch 91/500\n",
            "80/80 [==============================] - 0s 766us/step - loss: 14558.5765 - mse: 418738688.0000 - mae: 14558.5762 - val_loss: 14726.5403 - val_mse: 440444288.0000 - val_mae: 14726.5400\n",
            "Epoch 92/500\n",
            "80/80 [==============================] - 0s 780us/step - loss: 16014.1357 - mse: 465791584.0000 - mae: 16014.1357 - val_loss: 14657.3659 - val_mse: 436095712.0000 - val_mae: 14657.3662\n",
            "Epoch 93/500\n",
            "80/80 [==============================] - 0s 794us/step - loss: 16076.6954 - mse: 451578976.0000 - mae: 16076.6953 - val_loss: 14598.0438 - val_mse: 432407104.0000 - val_mae: 14598.0430\n",
            "Epoch 94/500\n",
            "80/80 [==============================] - 0s 800us/step - loss: 15303.2169 - mse: 405608960.0000 - mae: 15303.2168 - val_loss: 14562.4996 - val_mse: 430215008.0000 - val_mae: 14562.5000\n",
            "Epoch 95/500\n",
            "80/80 [==============================] - 0s 682us/step - loss: 15117.7070 - mse: 424576768.0000 - mae: 15117.7061 - val_loss: 14507.1535 - val_mse: 426829920.0000 - val_mae: 14507.1533\n",
            "Epoch 96/500\n",
            "80/80 [==============================] - 0s 800us/step - loss: 14157.3817 - mse: 408067488.0000 - mae: 14157.3809 - val_loss: 14430.9863 - val_mse: 422225888.0000 - val_mae: 14430.9863\n",
            "Epoch 97/500\n",
            "80/80 [==============================] - 0s 648us/step - loss: 14962.8944 - mse: 434491456.0000 - mae: 14962.8936 - val_loss: 14343.6112 - val_mse: 417022880.0000 - val_mae: 14343.6104\n",
            "Epoch 98/500\n",
            "80/80 [==============================] - 0s 866us/step - loss: 14728.4601 - mse: 395075424.0000 - mae: 14728.4609 - val_loss: 14304.2245 - val_mse: 414703392.0000 - val_mae: 14304.2246\n",
            "Epoch 99/500\n",
            "80/80 [==============================] - 0s 787us/step - loss: 14992.6692 - mse: 410185888.0000 - mae: 14992.6689 - val_loss: 14253.5792 - val_mse: 411746560.0000 - val_mae: 14253.5791\n",
            "Epoch 100/500\n",
            "80/80 [==============================] - 0s 765us/step - loss: 14136.3702 - mse: 379468608.0000 - mae: 14136.3701 - val_loss: 14140.5066 - val_mse: 405247872.0000 - val_mae: 14140.5068\n",
            "Epoch 101/500\n",
            "80/80 [==============================] - 0s 894us/step - loss: 13532.6378 - mse: 364942016.0000 - mae: 13532.6377 - val_loss: 14053.9809 - val_mse: 400368736.0000 - val_mae: 14053.9814\n",
            "Epoch 102/500\n",
            "80/80 [==============================] - 0s 634us/step - loss: 15421.1387 - mse: 411512992.0000 - mae: 15421.1387 - val_loss: 14040.8012 - val_mse: 399631456.0000 - val_mae: 14040.8018\n",
            "Epoch 103/500\n",
            "80/80 [==============================] - 0s 781us/step - loss: 14417.4028 - mse: 383948640.0000 - mae: 14417.4033 - val_loss: 14024.3963 - val_mse: 398716576.0000 - val_mae: 14024.3965\n",
            "Epoch 104/500\n",
            "80/80 [==============================] - 0s 846us/step - loss: 14920.0251 - mse: 393694912.0000 - mae: 14920.0254 - val_loss: 13992.2710 - val_mse: 396935072.0000 - val_mae: 13992.2715\n",
            "Epoch 105/500\n",
            "80/80 [==============================] - 0s 698us/step - loss: 15104.4940 - mse: 395846752.0000 - mae: 15104.4941 - val_loss: 13933.2987 - val_mse: 393695552.0000 - val_mae: 13933.2988\n",
            "Epoch 106/500\n",
            "80/80 [==============================] - 0s 747us/step - loss: 14827.3679 - mse: 391388608.0000 - mae: 14827.3672 - val_loss: 13902.5897 - val_mse: 392023136.0000 - val_mae: 13902.5898\n",
            "Epoch 107/500\n",
            "80/80 [==============================] - 0s 791us/step - loss: 14159.9786 - mse: 353695072.0000 - mae: 14159.9785 - val_loss: 13835.3450 - val_mse: 388398176.0000 - val_mae: 13835.3457\n",
            "Epoch 108/500\n",
            "80/80 [==============================] - 0s 756us/step - loss: 15340.9886 - mse: 418505056.0000 - mae: 15340.9893 - val_loss: 13803.1106 - val_mse: 386677248.0000 - val_mae: 13803.1104\n",
            "Epoch 109/500\n",
            "80/80 [==============================] - 0s 858us/step - loss: 13956.8258 - mse: 356068576.0000 - mae: 13956.8262 - val_loss: 13775.7211 - val_mse: 385121696.0000 - val_mae: 13775.7207\n",
            "Epoch 110/500\n",
            "80/80 [==============================] - 0s 827us/step - loss: 15681.4326 - mse: 417615552.0000 - mae: 15681.4326 - val_loss: 13745.4138 - val_mse: 383410080.0000 - val_mae: 13745.4131\n",
            "Epoch 111/500\n",
            "80/80 [==============================] - 0s 717us/step - loss: 14266.7052 - mse: 364006912.0000 - mae: 14266.7061 - val_loss: 13701.6993 - val_mse: 380961920.0000 - val_mae: 13701.7002\n",
            "Epoch 112/500\n",
            "80/80 [==============================] - 0s 750us/step - loss: 14795.3083 - mse: 416904384.0000 - mae: 14795.3076 - val_loss: 13677.3393 - val_mse: 379539584.0000 - val_mae: 13677.3389\n",
            "Epoch 113/500\n",
            "80/80 [==============================] - 0s 753us/step - loss: 13905.7433 - mse: 352857408.0000 - mae: 13905.7422 - val_loss: 13597.5902 - val_mse: 374828128.0000 - val_mae: 13597.5908\n",
            "Epoch 114/500\n",
            "80/80 [==============================] - 0s 725us/step - loss: 14049.5483 - mse: 356453120.0000 - mae: 14049.5488 - val_loss: 13534.6875 - val_mse: 371176768.0000 - val_mae: 13534.6875\n",
            "Epoch 115/500\n",
            "80/80 [==============================] - 0s 726us/step - loss: 14963.7347 - mse: 370469440.0000 - mae: 14963.7344 - val_loss: 13481.1324 - val_mse: 368113152.0000 - val_mae: 13481.1318\n",
            "Epoch 116/500\n",
            "80/80 [==============================] - 0s 782us/step - loss: 14007.1492 - mse: 330764736.0000 - mae: 14007.1504 - val_loss: 13431.6781 - val_mse: 365320768.0000 - val_mae: 13431.6787\n",
            "Epoch 117/500\n",
            "80/80 [==============================] - 0s 782us/step - loss: 15071.9699 - mse: 378719616.0000 - mae: 15071.9707 - val_loss: 13405.8591 - val_mse: 363876512.0000 - val_mae: 13405.8594\n",
            "Epoch 118/500\n",
            "80/80 [==============================] - 0s 725us/step - loss: 15326.1836 - mse: 376076736.0000 - mae: 15326.1846 - val_loss: 13410.0035 - val_mse: 364106368.0000 - val_mae: 13410.0039\n",
            "Epoch 119/500\n",
            "80/80 [==============================] - 0s 915us/step - loss: 14528.2367 - mse: 390945440.0000 - mae: 14528.2363 - val_loss: 13373.8298 - val_mse: 362097568.0000 - val_mae: 13373.8291\n",
            "Epoch 120/500\n",
            "80/80 [==============================] - 0s 692us/step - loss: 13989.9233 - mse: 344157792.0000 - mae: 13989.9248 - val_loss: 13333.4468 - val_mse: 359716480.0000 - val_mae: 13333.4463\n",
            "Epoch 121/500\n",
            "80/80 [==============================] - 0s 744us/step - loss: 14461.4291 - mse: 370179008.0000 - mae: 14461.4297 - val_loss: 13303.1313 - val_mse: 357929600.0000 - val_mae: 13303.1309\n",
            "Epoch 122/500\n",
            "80/80 [==============================] - 0s 892us/step - loss: 14034.5146 - mse: 357718432.0000 - mae: 14034.5156 - val_loss: 13271.3679 - val_mse: 356074528.0000 - val_mae: 13271.3682\n",
            "Epoch 123/500\n",
            "80/80 [==============================] - 0s 808us/step - loss: 15523.8068 - mse: 413238272.0000 - mae: 15523.8066 - val_loss: 13216.5806 - val_mse: 352722624.0000 - val_mae: 13216.5801\n",
            "Epoch 124/500\n",
            "80/80 [==============================] - 0s 766us/step - loss: 14451.5636 - mse: 366277312.0000 - mae: 14451.5645 - val_loss: 13186.8930 - val_mse: 350710400.0000 - val_mae: 13186.8926\n",
            "Epoch 125/500\n",
            "80/80 [==============================] - 0s 849us/step - loss: 14735.3317 - mse: 350806464.0000 - mae: 14735.3330 - val_loss: 13179.4220 - val_mse: 350204896.0000 - val_mae: 13179.4229\n",
            "Epoch 126/500\n",
            "80/80 [==============================] - 0s 708us/step - loss: 14773.6628 - mse: 378938944.0000 - mae: 14773.6641 - val_loss: 13174.0607 - val_mse: 349842848.0000 - val_mae: 13174.0605\n",
            "Epoch 127/500\n",
            "80/80 [==============================] - 0s 788us/step - loss: 13690.6788 - mse: 350582720.0000 - mae: 13690.6797 - val_loss: 13173.3999 - val_mse: 349799040.0000 - val_mae: 13173.4004\n",
            "Epoch 128/500\n",
            "80/80 [==============================] - 0s 832us/step - loss: 14189.0630 - mse: 338521632.0000 - mae: 14189.0625 - val_loss: 13159.7200 - val_mse: 348788704.0000 - val_mae: 13159.7197\n",
            "Epoch 129/500\n",
            "80/80 [==============================] - 0s 864us/step - loss: 15126.8774 - mse: 380825504.0000 - mae: 15126.8770 - val_loss: 13145.6911 - val_mse: 347747936.0000 - val_mae: 13145.6914\n",
            "Epoch 130/500\n",
            "80/80 [==============================] - 0s 723us/step - loss: 14553.3068 - mse: 357028288.0000 - mae: 14553.3066 - val_loss: 13149.1322 - val_mse: 348007392.0000 - val_mae: 13149.1318\n",
            "Epoch 131/500\n",
            "80/80 [==============================] - 0s 783us/step - loss: 14496.4767 - mse: 356036928.0000 - mae: 14496.4746 - val_loss: 13148.1308 - val_mse: 347946240.0000 - val_mae: 13148.1299\n",
            "Epoch 132/500\n",
            "80/80 [==============================] - 0s 816us/step - loss: 15230.8536 - mse: 376371904.0000 - mae: 15230.8535 - val_loss: 13143.1738 - val_mse: 347595264.0000 - val_mae: 13143.1729\n",
            "Epoch 133/500\n",
            "80/80 [==============================] - 0s 830us/step - loss: 14792.9073 - mse: 367914944.0000 - mae: 14792.9082 - val_loss: 13115.8453 - val_mse: 345689056.0000 - val_mae: 13115.8457\n",
            "Epoch 134/500\n",
            "80/80 [==============================] - 0s 777us/step - loss: 14492.9132 - mse: 370985472.0000 - mae: 14492.9141 - val_loss: 13092.7156 - val_mse: 344334976.0000 - val_mae: 13092.7158\n",
            "Epoch 135/500\n",
            "80/80 [==============================] - 0s 785us/step - loss: 14776.4703 - mse: 397333760.0000 - mae: 14776.4707 - val_loss: 13066.5636 - val_mse: 342869920.0000 - val_mae: 13066.5645\n",
            "Epoch 136/500\n",
            "80/80 [==============================] - 0s 744us/step - loss: 14439.8689 - mse: 347448032.0000 - mae: 14439.8691 - val_loss: 13030.1107 - val_mse: 341257088.0000 - val_mae: 13030.1104\n",
            "Epoch 137/500\n",
            "80/80 [==============================] - 0s 909us/step - loss: 13362.1047 - mse: 314727488.0000 - mae: 13362.1045 - val_loss: 12991.7898 - val_mse: 340239520.0000 - val_mae: 12991.7891\n",
            "Epoch 138/500\n",
            "80/80 [==============================] - 0s 841us/step - loss: 14269.3046 - mse: 336414784.0000 - mae: 14269.3047 - val_loss: 12976.0075 - val_mse: 339369120.0000 - val_mae: 12976.0068\n",
            "Epoch 139/500\n",
            "80/80 [==============================] - 0s 793us/step - loss: 14795.9468 - mse: 378662208.0000 - mae: 14795.9473 - val_loss: 12955.9991 - val_mse: 338903584.0000 - val_mae: 12956.0000\n",
            "Epoch 140/500\n",
            "80/80 [==============================] - 0s 812us/step - loss: 15429.6150 - mse: 391558624.0000 - mae: 15429.6143 - val_loss: 12933.1145 - val_mse: 338102528.0000 - val_mae: 12933.1152\n",
            "Epoch 141/500\n",
            "80/80 [==============================] - 0s 682us/step - loss: 13359.3372 - mse: 326288928.0000 - mae: 13359.3379 - val_loss: 12891.6089 - val_mse: 336407840.0000 - val_mae: 12891.6094\n",
            "Epoch 142/500\n",
            "80/80 [==============================] - 0s 719us/step - loss: 14293.3279 - mse: 361888800.0000 - mae: 14293.3281 - val_loss: 12847.2519 - val_mse: 334297856.0000 - val_mae: 12847.2520\n",
            "Epoch 143/500\n",
            "80/80 [==============================] - 0s 687us/step - loss: 13879.6121 - mse: 329738176.0000 - mae: 13879.6123 - val_loss: 12813.3487 - val_mse: 333699744.0000 - val_mae: 12813.3486\n",
            "Epoch 144/500\n",
            "80/80 [==============================] - 0s 764us/step - loss: 12963.6604 - mse: 292148320.0000 - mae: 12963.6611 - val_loss: 12783.4287 - val_mse: 331699040.0000 - val_mae: 12783.4287\n",
            "Epoch 145/500\n",
            "80/80 [==============================] - 0s 634us/step - loss: 13530.9692 - mse: 318658752.0000 - mae: 13530.9707 - val_loss: 12751.2003 - val_mse: 330831936.0000 - val_mae: 12751.2002\n",
            "Epoch 146/500\n",
            "80/80 [==============================] - 0s 708us/step - loss: 13511.0252 - mse: 332246496.0000 - mae: 13511.0264 - val_loss: 12723.0259 - val_mse: 329033888.0000 - val_mae: 12723.0264\n",
            "Epoch 147/500\n",
            "80/80 [==============================] - 0s 804us/step - loss: 11964.4166 - mse: 250257120.0000 - mae: 11964.4170 - val_loss: 12687.5111 - val_mse: 327271296.0000 - val_mae: 12687.5107\n",
            "Epoch 148/500\n",
            "80/80 [==============================] - 0s 695us/step - loss: 14992.4829 - mse: 370559264.0000 - mae: 14992.4824 - val_loss: 12730.6885 - val_mse: 326345824.0000 - val_mae: 12730.6885\n",
            "Epoch 149/500\n",
            "80/80 [==============================] - 0s 893us/step - loss: 14695.5104 - mse: 371850976.0000 - mae: 14695.5098 - val_loss: 12690.9360 - val_mse: 323848896.0000 - val_mae: 12690.9355\n",
            "Epoch 150/500\n",
            "80/80 [==============================] - 0s 717us/step - loss: 14633.9913 - mse: 383364736.0000 - mae: 14633.9922 - val_loss: 12724.0449 - val_mse: 322318496.0000 - val_mae: 12724.0449\n",
            "Epoch 151/500\n",
            "80/80 [==============================] - 0s 698us/step - loss: 13790.9944 - mse: 328740832.0000 - mae: 13790.9951 - val_loss: 12732.0996 - val_mse: 320399872.0000 - val_mae: 12732.0996\n",
            "Epoch 152/500\n",
            "80/80 [==============================] - 0s 837us/step - loss: 13241.5504 - mse: 299092224.0000 - mae: 13241.5498 - val_loss: 12699.0192 - val_mse: 318436320.0000 - val_mae: 12699.0195\n",
            "Epoch 153/500\n",
            "80/80 [==============================] - 0s 752us/step - loss: 13449.7061 - mse: 329274496.0000 - mae: 13449.7051 - val_loss: 12664.3310 - val_mse: 316722272.0000 - val_mae: 12664.3301\n",
            "Epoch 154/500\n",
            "80/80 [==============================] - 0s 742us/step - loss: 13409.5671 - mse: 338915168.0000 - mae: 13409.5674 - val_loss: 12647.6250 - val_mse: 314966304.0000 - val_mae: 12647.6250\n",
            "Epoch 155/500\n",
            "80/80 [==============================] - 0s 837us/step - loss: 13100.9393 - mse: 284279392.0000 - mae: 13100.9395 - val_loss: 12621.9439 - val_mse: 313555232.0000 - val_mae: 12621.9424\n",
            "Epoch 156/500\n",
            "80/80 [==============================] - 0s 877us/step - loss: 13561.4015 - mse: 326603840.0000 - mae: 13561.4014 - val_loss: 12561.0012 - val_mse: 310691424.0000 - val_mae: 12561.0010\n",
            "Epoch 157/500\n",
            "80/80 [==============================] - 0s 721us/step - loss: 13357.1653 - mse: 311595264.0000 - mae: 13357.1660 - val_loss: 12513.2015 - val_mse: 308133024.0000 - val_mae: 12513.2021\n",
            "Epoch 158/500\n",
            "80/80 [==============================] - 0s 738us/step - loss: 14213.4960 - mse: 347555680.0000 - mae: 14213.4951 - val_loss: 12503.6059 - val_mse: 307174496.0000 - val_mae: 12503.6064\n",
            "Epoch 159/500\n",
            "80/80 [==============================] - 0s 814us/step - loss: 14172.9867 - mse: 355982592.0000 - mae: 14172.9873 - val_loss: 12454.8695 - val_mse: 304565472.0000 - val_mae: 12454.8701\n",
            "Epoch 160/500\n",
            "80/80 [==============================] - 0s 866us/step - loss: 13447.6265 - mse: 301586016.0000 - mae: 13447.6270 - val_loss: 12412.7983 - val_mse: 302352544.0000 - val_mae: 12412.7979\n",
            "Epoch 161/500\n",
            "80/80 [==============================] - 0s 732us/step - loss: 14333.7356 - mse: 364210144.0000 - mae: 14333.7344 - val_loss: 12371.6305 - val_mse: 300276224.0000 - val_mae: 12371.6309\n",
            "Epoch 162/500\n",
            "80/80 [==============================] - 0s 933us/step - loss: 14227.1853 - mse: 333591872.0000 - mae: 14227.1846 - val_loss: 12340.1518 - val_mse: 299184256.0000 - val_mae: 12340.1514\n",
            "Epoch 163/500\n",
            "80/80 [==============================] - 0s 787us/step - loss: 14126.0159 - mse: 337380608.0000 - mae: 14126.0156 - val_loss: 12320.7449 - val_mse: 298062656.0000 - val_mae: 12320.7451\n",
            "Epoch 164/500\n",
            "80/80 [==============================] - 0s 748us/step - loss: 12178.1634 - mse: 267250304.0000 - mae: 12178.1641 - val_loss: 12261.6063 - val_mse: 295322464.0000 - val_mae: 12261.6074\n",
            "Epoch 165/500\n",
            "80/80 [==============================] - 0s 927us/step - loss: 12878.7975 - mse: 291413760.0000 - mae: 12878.7969 - val_loss: 12217.0559 - val_mse: 293110656.0000 - val_mae: 12217.0557\n",
            "Epoch 166/500\n",
            "80/80 [==============================] - 0s 730us/step - loss: 14081.0734 - mse: 340594720.0000 - mae: 14081.0732 - val_loss: 12194.9939 - val_mse: 292195008.0000 - val_mae: 12194.9951\n",
            "Epoch 167/500\n",
            "80/80 [==============================] - 0s 782us/step - loss: 14871.2974 - mse: 384940416.0000 - mae: 14871.2969 - val_loss: 12184.2282 - val_mse: 291471456.0000 - val_mae: 12184.2285\n",
            "Epoch 168/500\n",
            "80/80 [==============================] - 0s 729us/step - loss: 12914.7444 - mse: 284397536.0000 - mae: 12914.7451 - val_loss: 12143.4011 - val_mse: 289672128.0000 - val_mae: 12143.4014\n",
            "Epoch 169/500\n",
            "80/80 [==============================] - 0s 741us/step - loss: 12540.9994 - mse: 268827968.0000 - mae: 12541.0000 - val_loss: 12124.2585 - val_mse: 288433536.0000 - val_mae: 12124.2578\n",
            "Epoch 170/500\n",
            "80/80 [==============================] - 0s 794us/step - loss: 13655.2949 - mse: 327180384.0000 - mae: 13655.2939 - val_loss: 12104.8172 - val_mse: 287431872.0000 - val_mae: 12104.8174\n",
            "Epoch 171/500\n",
            "80/80 [==============================] - 0s 840us/step - loss: 12881.4103 - mse: 294846016.0000 - mae: 12881.4092 - val_loss: 12077.7162 - val_mse: 286101664.0000 - val_mae: 12077.7148\n",
            "Epoch 172/500\n",
            "80/80 [==============================] - 0s 821us/step - loss: 13670.9528 - mse: 313647200.0000 - mae: 13670.9531 - val_loss: 12038.5797 - val_mse: 284420160.0000 - val_mae: 12038.5801\n",
            "Epoch 173/500\n",
            "80/80 [==============================] - 0s 786us/step - loss: 14398.1514 - mse: 344514368.0000 - mae: 14398.1514 - val_loss: 12040.0325 - val_mse: 284665824.0000 - val_mae: 12040.0322\n",
            "Epoch 174/500\n",
            "80/80 [==============================] - 0s 734us/step - loss: 12805.6322 - mse: 297636000.0000 - mae: 12805.6328 - val_loss: 11989.4468 - val_mse: 282491296.0000 - val_mae: 11989.4473\n",
            "Epoch 175/500\n",
            "80/80 [==============================] - 0s 756us/step - loss: 14052.6855 - mse: 313630272.0000 - mae: 14052.6846 - val_loss: 11988.4986 - val_mse: 282369120.0000 - val_mae: 11988.4980\n",
            "Epoch 176/500\n",
            "80/80 [==============================] - 0s 643us/step - loss: 12988.8064 - mse: 288455584.0000 - mae: 12988.8066 - val_loss: 11944.6420 - val_mse: 280588352.0000 - val_mae: 11944.6406\n",
            "Epoch 177/500\n",
            "80/80 [==============================] - 0s 932us/step - loss: 14569.3833 - mse: 361380096.0000 - mae: 14569.3828 - val_loss: 11931.8147 - val_mse: 280178336.0000 - val_mae: 11931.8145\n",
            "Epoch 178/500\n",
            "80/80 [==============================] - 0s 808us/step - loss: 13796.4554 - mse: 318835168.0000 - mae: 13796.4561 - val_loss: 11892.4088 - val_mse: 278265344.0000 - val_mae: 11892.4092\n",
            "Epoch 179/500\n",
            "80/80 [==============================] - 0s 759us/step - loss: 12876.2826 - mse: 291711904.0000 - mae: 12876.2832 - val_loss: 11871.2097 - val_mse: 277236992.0000 - val_mae: 11871.2100\n",
            "Epoch 180/500\n",
            "80/80 [==============================] - 0s 849us/step - loss: 13898.8975 - mse: 326494080.0000 - mae: 13898.8965 - val_loss: 11834.5424 - val_mse: 275299488.0000 - val_mae: 11834.5420\n",
            "Epoch 181/500\n",
            "80/80 [==============================] - 0s 839us/step - loss: 13365.1852 - mse: 284882240.0000 - mae: 13365.1846 - val_loss: 11832.7854 - val_mse: 274542304.0000 - val_mae: 11832.7861\n",
            "Epoch 182/500\n",
            "80/80 [==============================] - 0s 906us/step - loss: 13887.6410 - mse: 323971520.0000 - mae: 13887.6406 - val_loss: 11835.5597 - val_mse: 274020224.0000 - val_mae: 11835.5586\n",
            "Epoch 183/500\n",
            "80/80 [==============================] - 0s 758us/step - loss: 12452.4196 - mse: 282569280.0000 - mae: 12452.4189 - val_loss: 11796.7302 - val_mse: 271998368.0000 - val_mae: 11796.7305\n",
            "Epoch 184/500\n",
            "80/80 [==============================] - 0s 729us/step - loss: 11785.1574 - mse: 251206112.0000 - mae: 11785.1582 - val_loss: 11773.1287 - val_mse: 270832960.0000 - val_mae: 11773.1289\n",
            "Epoch 185/500\n",
            "80/80 [==============================] - 0s 786us/step - loss: 12945.3500 - mse: 294072160.0000 - mae: 12945.3506 - val_loss: 11746.5286 - val_mse: 269360992.0000 - val_mae: 11746.5283\n",
            "Epoch 186/500\n",
            "80/80 [==============================] - 0s 869us/step - loss: 12848.4916 - mse: 291893600.0000 - mae: 12848.4922 - val_loss: 11727.2113 - val_mse: 267800080.0000 - val_mae: 11727.2119\n",
            "Epoch 187/500\n",
            "80/80 [==============================] - 0s 884us/step - loss: 13651.0294 - mse: 318249504.0000 - mae: 13651.0293 - val_loss: 11703.8109 - val_mse: 266612736.0000 - val_mae: 11703.8105\n",
            "Epoch 188/500\n",
            "80/80 [==============================] - 0s 897us/step - loss: 12883.2307 - mse: 301655648.0000 - mae: 12883.2295 - val_loss: 11653.0141 - val_mse: 264236800.0000 - val_mae: 11653.0146\n",
            "Epoch 189/500\n",
            "80/80 [==============================] - 0s 846us/step - loss: 12817.4099 - mse: 279954880.0000 - mae: 12817.4092 - val_loss: 11592.9462 - val_mse: 261499312.0000 - val_mae: 11592.9463\n",
            "Epoch 190/500\n",
            "80/80 [==============================] - 0s 745us/step - loss: 12389.5446 - mse: 257190368.0000 - mae: 12389.5449 - val_loss: 11555.2015 - val_mse: 259511376.0000 - val_mae: 11555.2021\n",
            "Epoch 191/500\n",
            "80/80 [==============================] - 0s 754us/step - loss: 12467.9330 - mse: 261104176.0000 - mae: 12467.9326 - val_loss: 11573.7414 - val_mse: 260006240.0000 - val_mae: 11573.7412\n",
            "Epoch 192/500\n",
            "80/80 [==============================] - 0s 851us/step - loss: 12740.3393 - mse: 293570304.0000 - mae: 12740.3389 - val_loss: 11531.4129 - val_mse: 257944288.0000 - val_mae: 11531.4121\n",
            "Epoch 193/500\n",
            "80/80 [==============================] - 0s 817us/step - loss: 11754.7175 - mse: 244973216.0000 - mae: 11754.7168 - val_loss: 11510.9201 - val_mse: 256535936.0000 - val_mae: 11510.9209\n",
            "Epoch 194/500\n",
            "80/80 [==============================] - 0s 708us/step - loss: 12616.5084 - mse: 274545216.0000 - mae: 12616.5078 - val_loss: 11523.9381 - val_mse: 256534416.0000 - val_mae: 11523.9375\n",
            "Epoch 195/500\n",
            "80/80 [==============================] - 0s 801us/step - loss: 13556.9172 - mse: 300600864.0000 - mae: 13556.9170 - val_loss: 11505.7642 - val_mse: 255439280.0000 - val_mae: 11505.7637\n",
            "Epoch 196/500\n",
            "80/80 [==============================] - 0s 908us/step - loss: 11967.8186 - mse: 258691744.0000 - mae: 11967.8184 - val_loss: 11473.0909 - val_mse: 253795296.0000 - val_mae: 11473.0908\n",
            "Epoch 197/500\n",
            "80/80 [==============================] - 0s 855us/step - loss: 13495.5292 - mse: 294275040.0000 - mae: 13495.5293 - val_loss: 11448.4899 - val_mse: 252614064.0000 - val_mae: 11448.4902\n",
            "Epoch 198/500\n",
            "80/80 [==============================] - 0s 718us/step - loss: 12456.8279 - mse: 274448832.0000 - mae: 12456.8281 - val_loss: 11429.7461 - val_mse: 251557632.0000 - val_mae: 11429.7461\n",
            "Epoch 199/500\n",
            "80/80 [==============================] - 0s 765us/step - loss: 12958.3112 - mse: 293908768.0000 - mae: 12958.3105 - val_loss: 11420.7418 - val_mse: 251067648.0000 - val_mae: 11420.7422\n",
            "Epoch 200/500\n",
            "80/80 [==============================] - 0s 839us/step - loss: 12454.1027 - mse: 264226720.0000 - mae: 12454.1016 - val_loss: 11391.5472 - val_mse: 249797808.0000 - val_mae: 11391.5469\n",
            "Epoch 201/500\n",
            "80/80 [==============================] - 0s 679us/step - loss: 13115.3721 - mse: 295431648.0000 - mae: 13115.3721 - val_loss: 11336.0287 - val_mse: 247600016.0000 - val_mae: 11336.0283\n",
            "Epoch 202/500\n",
            "80/80 [==============================] - 0s 747us/step - loss: 13803.0009 - mse: 309802560.0000 - mae: 13803.0020 - val_loss: 11308.4509 - val_mse: 246043392.0000 - val_mae: 11308.4512\n",
            "Epoch 203/500\n",
            "80/80 [==============================] - 0s 712us/step - loss: 13206.5510 - mse: 288668224.0000 - mae: 13206.5518 - val_loss: 11287.9402 - val_mse: 245360640.0000 - val_mae: 11287.9404\n",
            "Epoch 204/500\n",
            "80/80 [==============================] - 0s 830us/step - loss: 12871.2338 - mse: 279075648.0000 - mae: 12871.2324 - val_loss: 11264.2808 - val_mse: 244199840.0000 - val_mae: 11264.2803\n",
            "Epoch 205/500\n",
            "80/80 [==============================] - 0s 786us/step - loss: 11997.9667 - mse: 244675248.0000 - mae: 11997.9668 - val_loss: 11233.3213 - val_mse: 242509680.0000 - val_mae: 11233.3213\n",
            "Epoch 206/500\n",
            "80/80 [==============================] - 0s 731us/step - loss: 13749.6326 - mse: 325481440.0000 - mae: 13749.6328 - val_loss: 11231.8946 - val_mse: 242275568.0000 - val_mae: 11231.8945\n",
            "Epoch 207/500\n",
            "80/80 [==============================] - 0s 926us/step - loss: 13003.8066 - mse: 303160576.0000 - mae: 13003.8066 - val_loss: 11214.6661 - val_mse: 241407840.0000 - val_mae: 11214.6660\n",
            "Epoch 208/500\n",
            "80/80 [==============================] - 0s 736us/step - loss: 13232.9830 - mse: 282543680.0000 - mae: 13232.9824 - val_loss: 11199.4684 - val_mse: 240634816.0000 - val_mae: 11199.4678\n",
            "Epoch 209/500\n",
            "80/80 [==============================] - 0s 697us/step - loss: 12398.4014 - mse: 270820384.0000 - mae: 12398.4014 - val_loss: 11186.3303 - val_mse: 240248064.0000 - val_mae: 11186.3301\n",
            "Epoch 210/500\n",
            "80/80 [==============================] - 0s 843us/step - loss: 12841.3839 - mse: 278716736.0000 - mae: 12841.3838 - val_loss: 11171.3802 - val_mse: 239889328.0000 - val_mae: 11171.3799\n",
            "Epoch 211/500\n",
            "80/80 [==============================] - 0s 778us/step - loss: 13061.7338 - mse: 272553536.0000 - mae: 13061.7334 - val_loss: 11146.0101 - val_mse: 238612400.0000 - val_mae: 11146.0098\n",
            "Epoch 212/500\n",
            "80/80 [==============================] - 0s 794us/step - loss: 12318.6785 - mse: 282486592.0000 - mae: 12318.6787 - val_loss: 11134.4799 - val_mse: 238234736.0000 - val_mae: 11134.4805\n",
            "Epoch 213/500\n",
            "80/80 [==============================] - 0s 776us/step - loss: 12585.9182 - mse: 265290976.0000 - mae: 12585.9180 - val_loss: 11115.6621 - val_mse: 237256416.0000 - val_mae: 11115.6621\n",
            "Epoch 214/500\n",
            "80/80 [==============================] - 0s 884us/step - loss: 11927.9444 - mse: 240895776.0000 - mae: 11927.9434 - val_loss: 11085.6554 - val_mse: 235403328.0000 - val_mae: 11085.6553\n",
            "Epoch 215/500\n",
            "80/80 [==============================] - 0s 676us/step - loss: 12229.8875 - mse: 267394864.0000 - mae: 12229.8877 - val_loss: 11066.7828 - val_mse: 234675632.0000 - val_mae: 11066.7822\n",
            "Epoch 216/500\n",
            "80/80 [==============================] - 0s 704us/step - loss: 11743.2615 - mse: 248527792.0000 - mae: 11743.2617 - val_loss: 11056.4362 - val_mse: 234190928.0000 - val_mae: 11056.4365\n",
            "Epoch 217/500\n",
            "80/80 [==============================] - 0s 813us/step - loss: 12875.2507 - mse: 252809984.0000 - mae: 12875.2510 - val_loss: 11049.7849 - val_mse: 233755456.0000 - val_mae: 11049.7852\n",
            "Epoch 218/500\n",
            "80/80 [==============================] - 0s 685us/step - loss: 12123.1248 - mse: 256871392.0000 - mae: 12123.1250 - val_loss: 11025.7484 - val_mse: 232205536.0000 - val_mae: 11025.7480\n",
            "Epoch 219/500\n",
            "80/80 [==============================] - 0s 745us/step - loss: 13821.8545 - mse: 293776576.0000 - mae: 13821.8545 - val_loss: 11015.9399 - val_mse: 231989712.0000 - val_mae: 11015.9395\n",
            "Epoch 220/500\n",
            "80/80 [==============================] - 0s 725us/step - loss: 11620.4611 - mse: 217436960.0000 - mae: 11620.4609 - val_loss: 10999.1720 - val_mse: 230941440.0000 - val_mae: 10999.1719\n",
            "Epoch 221/500\n",
            "80/80 [==============================] - 0s 763us/step - loss: 11977.0591 - mse: 241936976.0000 - mae: 11977.0576 - val_loss: 10986.7872 - val_mse: 230001664.0000 - val_mae: 10986.7871\n",
            "Epoch 222/500\n",
            "80/80 [==============================] - 0s 868us/step - loss: 12059.9663 - mse: 231784032.0000 - mae: 12059.9658 - val_loss: 10967.7876 - val_mse: 229207152.0000 - val_mae: 10967.7871\n",
            "Epoch 223/500\n",
            "80/80 [==============================] - 0s 973us/step - loss: 11978.0966 - mse: 234552416.0000 - mae: 11978.0957 - val_loss: 10950.2179 - val_mse: 228640208.0000 - val_mae: 10950.2178\n",
            "Epoch 224/500\n",
            "80/80 [==============================] - 0s 735us/step - loss: 13063.5115 - mse: 268033568.0000 - mae: 13063.5117 - val_loss: 10934.2828 - val_mse: 228236544.0000 - val_mae: 10934.2832\n",
            "Epoch 225/500\n",
            "80/80 [==============================] - 0s 879us/step - loss: 12694.7074 - mse: 260789072.0000 - mae: 12694.7070 - val_loss: 10917.7019 - val_mse: 227333568.0000 - val_mae: 10917.7021\n",
            "Epoch 226/500\n",
            "80/80 [==============================] - 0s 751us/step - loss: 13467.7942 - mse: 309384640.0000 - mae: 13467.7939 - val_loss: 10903.7710 - val_mse: 226719184.0000 - val_mae: 10903.7705\n",
            "Epoch 227/500\n",
            "80/80 [==============================] - 0s 871us/step - loss: 12372.0382 - mse: 259075328.0000 - mae: 12372.0381 - val_loss: 10872.5522 - val_mse: 225296112.0000 - val_mae: 10872.5518\n",
            "Epoch 228/500\n",
            "80/80 [==============================] - 0s 885us/step - loss: 11339.8303 - mse: 239532960.0000 - mae: 11339.8301 - val_loss: 10853.1486 - val_mse: 224321712.0000 - val_mae: 10853.1484\n",
            "Epoch 229/500\n",
            "80/80 [==============================] - 0s 819us/step - loss: 10838.9941 - mse: 225306112.0000 - mae: 10838.9941 - val_loss: 10840.5538 - val_mse: 223039104.0000 - val_mae: 10840.5537\n",
            "Epoch 230/500\n",
            "80/80 [==============================] - 0s 729us/step - loss: 12686.7837 - mse: 253226496.0000 - mae: 12686.7842 - val_loss: 10901.4176 - val_mse: 223068864.0000 - val_mae: 10901.4180\n",
            "Epoch 231/500\n",
            "80/80 [==============================] - 0s 748us/step - loss: 11869.3523 - mse: 225082928.0000 - mae: 11869.3525 - val_loss: 10923.5396 - val_mse: 222176096.0000 - val_mae: 10923.5391\n",
            "Epoch 232/500\n",
            "80/80 [==============================] - 0s 875us/step - loss: 11420.5557 - mse: 208351040.0000 - mae: 11420.5566 - val_loss: 10951.1328 - val_mse: 221291840.0000 - val_mae: 10951.1328\n",
            "Epoch 233/500\n",
            "80/80 [==============================] - 0s 765us/step - loss: 11827.1469 - mse: 248720288.0000 - mae: 11827.1465 - val_loss: 10939.9166 - val_mse: 220275024.0000 - val_mae: 10939.9170\n",
            "Epoch 234/500\n",
            "80/80 [==============================] - 0s 720us/step - loss: 11973.4839 - mse: 248638640.0000 - mae: 11973.4844 - val_loss: 10937.3701 - val_mse: 219376192.0000 - val_mae: 10937.3701\n",
            "Epoch 235/500\n",
            "80/80 [==============================] - 0s 797us/step - loss: 11869.9543 - mse: 226190544.0000 - mae: 11869.9531 - val_loss: 10934.1946 - val_mse: 219053664.0000 - val_mae: 10934.1943\n",
            "Epoch 236/500\n",
            "80/80 [==============================] - 0s 787us/step - loss: 12472.5245 - mse: 267583456.0000 - mae: 12472.5244 - val_loss: 10948.4284 - val_mse: 218979776.0000 - val_mae: 10948.4287\n",
            "Epoch 237/500\n",
            "80/80 [==============================] - 0s 906us/step - loss: 11775.8135 - mse: 234806480.0000 - mae: 11775.8135 - val_loss: 10930.9316 - val_mse: 217721696.0000 - val_mae: 10930.9326\n",
            "Epoch 238/500\n",
            "80/80 [==============================] - 0s 850us/step - loss: 12220.0814 - mse: 230463824.0000 - mae: 12220.0811 - val_loss: 10926.5867 - val_mse: 217246080.0000 - val_mae: 10926.5879\n",
            "Epoch 239/500\n",
            "80/80 [==============================] - 0s 905us/step - loss: 12045.5642 - mse: 250156960.0000 - mae: 12045.5645 - val_loss: 10921.2143 - val_mse: 216863152.0000 - val_mae: 10921.2139\n",
            "Epoch 240/500\n",
            "80/80 [==============================] - 0s 773us/step - loss: 12141.3560 - mse: 253335584.0000 - mae: 12141.3564 - val_loss: 10898.2562 - val_mse: 215755856.0000 - val_mae: 10898.2549\n",
            "Epoch 241/500\n",
            "80/80 [==============================] - 0s 697us/step - loss: 11546.7128 - mse: 239832320.0000 - mae: 11546.7119 - val_loss: 10873.7785 - val_mse: 214821152.0000 - val_mae: 10873.7783\n",
            "Epoch 242/500\n",
            "80/80 [==============================] - 0s 764us/step - loss: 11626.5167 - mse: 225110864.0000 - mae: 11626.5166 - val_loss: 10850.4222 - val_mse: 213748528.0000 - val_mae: 10850.4219\n",
            "Epoch 243/500\n",
            "80/80 [==============================] - 0s 969us/step - loss: 11903.4577 - mse: 250682208.0000 - mae: 11903.4580 - val_loss: 10812.1517 - val_mse: 211946544.0000 - val_mae: 10812.1514\n",
            "Epoch 244/500\n",
            "80/80 [==============================] - 0s 859us/step - loss: 12303.0369 - mse: 253035392.0000 - mae: 12303.0371 - val_loss: 10799.7273 - val_mse: 211009104.0000 - val_mae: 10799.7266\n",
            "Epoch 245/500\n",
            "80/80 [==============================] - 0s 794us/step - loss: 11462.1638 - mse: 220302896.0000 - mae: 11462.1641 - val_loss: 10771.4626 - val_mse: 209860816.0000 - val_mae: 10771.4629\n",
            "Epoch 246/500\n",
            "80/80 [==============================] - 0s 728us/step - loss: 11793.8743 - mse: 249133104.0000 - mae: 11793.8750 - val_loss: 10735.7565 - val_mse: 208875936.0000 - val_mae: 10735.7559\n",
            "Epoch 247/500\n",
            "80/80 [==============================] - 0s 815us/step - loss: 11031.0345 - mse: 221736064.0000 - mae: 11031.0342 - val_loss: 10724.9343 - val_mse: 208253136.0000 - val_mae: 10724.9346\n",
            "Epoch 248/500\n",
            "80/80 [==============================] - 0s 796us/step - loss: 10283.2477 - mse: 203356128.0000 - mae: 10283.2480 - val_loss: 10681.9423 - val_mse: 206341856.0000 - val_mae: 10681.9424\n",
            "Epoch 249/500\n",
            "80/80 [==============================] - 0s 805us/step - loss: 12783.8823 - mse: 263948032.0000 - mae: 12783.8809 - val_loss: 10682.1591 - val_mse: 206130176.0000 - val_mae: 10682.1592\n",
            "Epoch 250/500\n",
            "80/80 [==============================] - 0s 767us/step - loss: 11055.9016 - mse: 197976256.0000 - mae: 11055.9014 - val_loss: 10668.9796 - val_mse: 205561776.0000 - val_mae: 10668.9805\n",
            "Epoch 251/500\n",
            "80/80 [==============================] - 0s 751us/step - loss: 12102.6956 - mse: 239764896.0000 - mae: 12102.6953 - val_loss: 10669.4958 - val_mse: 205115888.0000 - val_mae: 10669.4961\n",
            "Epoch 252/500\n",
            "80/80 [==============================] - 0s 918us/step - loss: 10576.8264 - mse: 227227520.0000 - mae: 10576.8262 - val_loss: 10653.2837 - val_mse: 204153088.0000 - val_mae: 10653.2842\n",
            "Epoch 253/500\n",
            "80/80 [==============================] - 0s 801us/step - loss: 12196.5161 - mse: 246828464.0000 - mae: 12196.5156 - val_loss: 10650.7829 - val_mse: 203672464.0000 - val_mae: 10650.7832\n",
            "Epoch 254/500\n",
            "80/80 [==============================] - 0s 708us/step - loss: 12156.7919 - mse: 264486304.0000 - mae: 12156.7920 - val_loss: 10643.1738 - val_mse: 203182736.0000 - val_mae: 10643.1729\n",
            "Epoch 255/500\n",
            "80/80 [==============================] - 0s 783us/step - loss: 12644.6956 - mse: 270670688.0000 - mae: 12644.6953 - val_loss: 10631.7142 - val_mse: 202730816.0000 - val_mae: 10631.7139\n",
            "Epoch 256/500\n",
            "80/80 [==============================] - 0s 929us/step - loss: 10761.7621 - mse: 216698848.0000 - mae: 10761.7607 - val_loss: 10622.1277 - val_mse: 202213312.0000 - val_mae: 10622.1279\n",
            "Epoch 257/500\n",
            "80/80 [==============================] - 0s 900us/step - loss: 11625.8992 - mse: 218728368.0000 - mae: 11625.8994 - val_loss: 10618.1918 - val_mse: 201587776.0000 - val_mae: 10618.1914\n",
            "Epoch 258/500\n",
            "80/80 [==============================] - 0s 913us/step - loss: 11667.2512 - mse: 235586768.0000 - mae: 11667.2510 - val_loss: 10608.9477 - val_mse: 200831280.0000 - val_mae: 10608.9482\n",
            "Epoch 259/500\n",
            "80/80 [==============================] - 0s 742us/step - loss: 12221.1259 - mse: 252436992.0000 - mae: 12221.1260 - val_loss: 10589.2583 - val_mse: 200255328.0000 - val_mae: 10589.2568\n",
            "Epoch 260/500\n",
            "80/80 [==============================] - 0s 747us/step - loss: 11154.8800 - mse: 208767424.0000 - mae: 11154.8799 - val_loss: 10587.5167 - val_mse: 200065632.0000 - val_mae: 10587.5166\n",
            "Epoch 261/500\n",
            "80/80 [==============================] - 0s 786us/step - loss: 13352.6840 - mse: 275217408.0000 - mae: 13352.6846 - val_loss: 10580.2604 - val_mse: 199521040.0000 - val_mae: 10580.2607\n",
            "Epoch 262/500\n",
            "80/80 [==============================] - 0s 782us/step - loss: 12329.9622 - mse: 254489600.0000 - mae: 12329.9629 - val_loss: 10586.8715 - val_mse: 199361680.0000 - val_mae: 10586.8721\n",
            "Epoch 263/500\n",
            "80/80 [==============================] - 0s 780us/step - loss: 11422.2227 - mse: 234471008.0000 - mae: 11422.2227 - val_loss: 10618.5583 - val_mse: 200078832.0000 - val_mae: 10618.5576\n",
            "Epoch 264/500\n",
            "80/80 [==============================] - 0s 745us/step - loss: 12270.0295 - mse: 236013344.0000 - mae: 12270.0303 - val_loss: 10616.4906 - val_mse: 199800768.0000 - val_mae: 10616.4902\n",
            "Epoch 265/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 10048.4585 - mse: 156795984.0000 - mae: 10048.4590 - val_loss: 10580.1996 - val_mse: 198030896.0000 - val_mae: 10580.2002\n",
            "Epoch 266/500\n",
            "80/80 [==============================] - 0s 849us/step - loss: 11238.3555 - mse: 226092880.0000 - mae: 11238.3545 - val_loss: 10580.8583 - val_mse: 197609696.0000 - val_mae: 10580.8584\n",
            "Epoch 267/500\n",
            "80/80 [==============================] - 0s 815us/step - loss: 11942.7639 - mse: 235749728.0000 - mae: 11942.7637 - val_loss: 10550.3791 - val_mse: 196289632.0000 - val_mae: 10550.3789\n",
            "Epoch 268/500\n",
            "80/80 [==============================] - 0s 736us/step - loss: 11626.0100 - mse: 234880000.0000 - mae: 11626.0098 - val_loss: 10544.0501 - val_mse: 195672512.0000 - val_mae: 10544.0498\n",
            "Epoch 269/500\n",
            "80/80 [==============================] - 0s 814us/step - loss: 11391.0024 - mse: 232654624.0000 - mae: 11391.0029 - val_loss: 10540.9380 - val_mse: 195172240.0000 - val_mae: 10540.9385\n",
            "Epoch 270/500\n",
            "80/80 [==============================] - 0s 703us/step - loss: 11614.6150 - mse: 228361440.0000 - mae: 11614.6152 - val_loss: 10533.2596 - val_mse: 194652560.0000 - val_mae: 10533.2588\n",
            "Epoch 271/500\n",
            "80/80 [==============================] - 0s 897us/step - loss: 11875.2868 - mse: 242733776.0000 - mae: 11875.2871 - val_loss: 10529.3174 - val_mse: 194461584.0000 - val_mae: 10529.3174\n",
            "Epoch 272/500\n",
            "80/80 [==============================] - 0s 879us/step - loss: 10753.9587 - mse: 180064464.0000 - mae: 10753.9590 - val_loss: 10544.1178 - val_mse: 194882672.0000 - val_mae: 10544.1182\n",
            "Epoch 273/500\n",
            "80/80 [==============================] - 0s 831us/step - loss: 11115.0916 - mse: 229273344.0000 - mae: 11115.0908 - val_loss: 10514.7245 - val_mse: 194068912.0000 - val_mae: 10514.7246\n",
            "Epoch 274/500\n",
            "80/80 [==============================] - 0s 723us/step - loss: 11944.8216 - mse: 242801104.0000 - mae: 11944.8213 - val_loss: 10497.1527 - val_mse: 193264736.0000 - val_mae: 10497.1533\n",
            "Epoch 275/500\n",
            "80/80 [==============================] - 0s 842us/step - loss: 11007.3456 - mse: 236417920.0000 - mae: 11007.3457 - val_loss: 10523.7811 - val_mse: 193868624.0000 - val_mae: 10523.7812\n",
            "Epoch 276/500\n",
            "80/80 [==============================] - 0s 914us/step - loss: 11456.2258 - mse: 217430656.0000 - mae: 11456.2256 - val_loss: 10501.3814 - val_mse: 193048544.0000 - val_mae: 10501.3818\n",
            "Epoch 277/500\n",
            "80/80 [==============================] - 0s 710us/step - loss: 11063.1921 - mse: 239458512.0000 - mae: 11063.1914 - val_loss: 10472.7065 - val_mse: 192291136.0000 - val_mae: 10472.7070\n",
            "Epoch 278/500\n",
            "80/80 [==============================] - 0s 839us/step - loss: 10751.1675 - mse: 206754720.0000 - mae: 10751.1670 - val_loss: 10466.8265 - val_mse: 191626560.0000 - val_mae: 10466.8262\n",
            "Epoch 279/500\n",
            "80/80 [==============================] - 0s 912us/step - loss: 12292.8454 - mse: 265240704.0000 - mae: 12292.8457 - val_loss: 10458.8017 - val_mse: 191267888.0000 - val_mae: 10458.8018\n",
            "Epoch 280/500\n",
            "80/80 [==============================] - 0s 870us/step - loss: 11304.0349 - mse: 206241536.0000 - mae: 11304.0352 - val_loss: 10454.1184 - val_mse: 190895520.0000 - val_mae: 10454.1191\n",
            "Epoch 281/500\n",
            "80/80 [==============================] - 0s 912us/step - loss: 10610.7921 - mse: 193570016.0000 - mae: 10610.7920 - val_loss: 10436.5028 - val_mse: 190442128.0000 - val_mae: 10436.5039\n",
            "Epoch 282/500\n",
            "80/80 [==============================] - 0s 733us/step - loss: 12985.4490 - mse: 266256720.0000 - mae: 12985.4482 - val_loss: 10423.5487 - val_mse: 189743536.0000 - val_mae: 10423.5479\n",
            "Epoch 283/500\n",
            "80/80 [==============================] - 0s 811us/step - loss: 11807.0643 - mse: 211221856.0000 - mae: 11807.0645 - val_loss: 10402.0562 - val_mse: 189080448.0000 - val_mae: 10402.0566\n",
            "Epoch 284/500\n",
            "80/80 [==============================] - 0s 717us/step - loss: 11287.6050 - mse: 203099680.0000 - mae: 11287.6055 - val_loss: 10396.3832 - val_mse: 188936048.0000 - val_mae: 10396.3818\n",
            "Epoch 285/500\n",
            "80/80 [==============================] - 0s 823us/step - loss: 11866.3171 - mse: 235897600.0000 - mae: 11866.3174 - val_loss: 10383.9208 - val_mse: 188530768.0000 - val_mae: 10383.9209\n",
            "Epoch 286/500\n",
            "80/80 [==============================] - 0s 816us/step - loss: 10483.4861 - mse: 187676832.0000 - mae: 10483.4863 - val_loss: 10373.3484 - val_mse: 188173696.0000 - val_mae: 10373.3486\n",
            "Epoch 287/500\n",
            "80/80 [==============================] - 0s 826us/step - loss: 12481.0542 - mse: 256851504.0000 - mae: 12481.0547 - val_loss: 10367.2834 - val_mse: 187785168.0000 - val_mae: 10367.2842\n",
            "Epoch 288/500\n",
            "80/80 [==============================] - 0s 874us/step - loss: 11638.4775 - mse: 226280352.0000 - mae: 11638.4785 - val_loss: 10352.0130 - val_mse: 186989424.0000 - val_mae: 10352.0127\n",
            "Epoch 289/500\n",
            "80/80 [==============================] - 0s 755us/step - loss: 12118.6688 - mse: 227178544.0000 - mae: 12118.6680 - val_loss: 10344.4834 - val_mse: 186569296.0000 - val_mae: 10344.4834\n",
            "Epoch 290/500\n",
            "80/80 [==============================] - 0s 721us/step - loss: 12399.3966 - mse: 268036304.0000 - mae: 12399.3965 - val_loss: 10343.0844 - val_mse: 186115920.0000 - val_mae: 10343.0850\n",
            "Epoch 291/500\n",
            "80/80 [==============================] - 0s 757us/step - loss: 12252.6718 - mse: 258053216.0000 - mae: 12252.6719 - val_loss: 10327.1728 - val_mse: 185852000.0000 - val_mae: 10327.1719\n",
            "Epoch 292/500\n",
            "80/80 [==============================] - 0s 767us/step - loss: 11460.2070 - mse: 224509616.0000 - mae: 11460.2070 - val_loss: 10300.4410 - val_mse: 185100240.0000 - val_mae: 10300.4414\n",
            "Epoch 293/500\n",
            "80/80 [==============================] - 0s 776us/step - loss: 10155.2122 - mse: 186556672.0000 - mae: 10155.2129 - val_loss: 10290.2147 - val_mse: 184614512.0000 - val_mae: 10290.2139\n",
            "Epoch 294/500\n",
            "80/80 [==============================] - 0s 757us/step - loss: 11165.9098 - mse: 207198272.0000 - mae: 11165.9092 - val_loss: 10270.6176 - val_mse: 183986256.0000 - val_mae: 10270.6172\n",
            "Epoch 295/500\n",
            "80/80 [==============================] - 0s 818us/step - loss: 11600.8450 - mse: 235447328.0000 - mae: 11600.8438 - val_loss: 10259.4176 - val_mse: 183462832.0000 - val_mae: 10259.4180\n",
            "Epoch 296/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 11108.8613 - mse: 205542272.0000 - mae: 11108.8613 - val_loss: 10270.8304 - val_mse: 183691072.0000 - val_mae: 10270.8301\n",
            "Epoch 297/500\n",
            "80/80 [==============================] - 0s 765us/step - loss: 11462.9320 - mse: 204305152.0000 - mae: 11462.9316 - val_loss: 10264.9447 - val_mse: 183601296.0000 - val_mae: 10264.9453\n",
            "Epoch 298/500\n",
            "80/80 [==============================] - 0s 820us/step - loss: 10257.7542 - mse: 186633984.0000 - mae: 10257.7539 - val_loss: 10255.0028 - val_mse: 183492896.0000 - val_mae: 10255.0039\n",
            "Epoch 299/500\n",
            "80/80 [==============================] - 0s 742us/step - loss: 9977.8533 - mse: 179811520.0000 - mae: 9977.8535 - val_loss: 10240.4555 - val_mse: 183166640.0000 - val_mae: 10240.4551\n",
            "Epoch 300/500\n",
            "80/80 [==============================] - 0s 842us/step - loss: 11207.0194 - mse: 205392016.0000 - mae: 11207.0205 - val_loss: 10234.7705 - val_mse: 182869904.0000 - val_mae: 10234.7695\n",
            "Epoch 301/500\n",
            "80/80 [==============================] - 0s 782us/step - loss: 11619.1482 - mse: 220597664.0000 - mae: 11619.1484 - val_loss: 10216.9369 - val_mse: 182264976.0000 - val_mae: 10216.9365\n",
            "Epoch 302/500\n",
            "80/80 [==============================] - 0s 858us/step - loss: 12188.6052 - mse: 235901792.0000 - mae: 12188.6055 - val_loss: 10215.2061 - val_mse: 182260096.0000 - val_mae: 10215.2051\n",
            "Epoch 303/500\n",
            "80/80 [==============================] - 0s 807us/step - loss: 11218.2982 - mse: 218001872.0000 - mae: 11218.2988 - val_loss: 10193.0010 - val_mse: 181428896.0000 - val_mae: 10193.0020\n",
            "Epoch 304/500\n",
            "80/80 [==============================] - 0s 913us/step - loss: 10344.2243 - mse: 216793904.0000 - mae: 10344.2236 - val_loss: 10186.9202 - val_mse: 181393248.0000 - val_mae: 10186.9199\n",
            "Epoch 305/500\n",
            "80/80 [==============================] - 0s 892us/step - loss: 11564.2632 - mse: 226059472.0000 - mae: 11564.2637 - val_loss: 10174.9907 - val_mse: 181199040.0000 - val_mae: 10174.9912\n",
            "Epoch 306/500\n",
            "80/80 [==============================] - 0s 777us/step - loss: 11426.9374 - mse: 202966944.0000 - mae: 11426.9375 - val_loss: 10176.7529 - val_mse: 181084592.0000 - val_mae: 10176.7520\n",
            "Epoch 307/500\n",
            "80/80 [==============================] - 0s 847us/step - loss: 10362.2348 - mse: 180680832.0000 - mae: 10362.2354 - val_loss: 10171.9087 - val_mse: 180936144.0000 - val_mae: 10171.9092\n",
            "Epoch 308/500\n",
            "80/80 [==============================] - 0s 704us/step - loss: 11947.1030 - mse: 237927936.0000 - mae: 11947.1035 - val_loss: 10174.3857 - val_mse: 181067536.0000 - val_mae: 10174.3857\n",
            "Epoch 309/500\n",
            "80/80 [==============================] - 0s 768us/step - loss: 11014.1995 - mse: 208299376.0000 - mae: 11014.1992 - val_loss: 10163.2184 - val_mse: 180368656.0000 - val_mae: 10163.2178\n",
            "Epoch 310/500\n",
            "80/80 [==============================] - 0s 761us/step - loss: 9748.5854 - mse: 172942832.0000 - mae: 9748.5850 - val_loss: 10154.4756 - val_mse: 179858256.0000 - val_mae: 10154.4756\n",
            "Epoch 311/500\n",
            "80/80 [==============================] - 0s 722us/step - loss: 9419.1226 - mse: 141261184.0000 - mae: 9419.1221 - val_loss: 10150.7238 - val_mse: 179793136.0000 - val_mae: 10150.7236\n",
            "Epoch 312/500\n",
            "80/80 [==============================] - 0s 774us/step - loss: 10459.6481 - mse: 197345568.0000 - mae: 10459.6475 - val_loss: 10142.7176 - val_mse: 179412384.0000 - val_mae: 10142.7178\n",
            "Epoch 313/500\n",
            "80/80 [==============================] - 0s 756us/step - loss: 11458.3657 - mse: 208890416.0000 - mae: 11458.3662 - val_loss: 10135.4725 - val_mse: 179276448.0000 - val_mae: 10135.4727\n",
            "Epoch 314/500\n",
            "80/80 [==============================] - 0s 785us/step - loss: 11535.6447 - mse: 210231552.0000 - mae: 11535.6445 - val_loss: 10120.7227 - val_mse: 178990160.0000 - val_mae: 10120.7227\n",
            "Epoch 315/500\n",
            "80/80 [==============================] - 0s 858us/step - loss: 11152.5342 - mse: 203119664.0000 - mae: 11152.5342 - val_loss: 10112.5075 - val_mse: 178699184.0000 - val_mae: 10112.5068\n",
            "Epoch 316/500\n",
            "80/80 [==============================] - 0s 749us/step - loss: 11127.9188 - mse: 196821648.0000 - mae: 11127.9189 - val_loss: 10103.6558 - val_mse: 178329600.0000 - val_mae: 10103.6553\n",
            "Epoch 317/500\n",
            "80/80 [==============================] - 0s 770us/step - loss: 10756.2578 - mse: 195800160.0000 - mae: 10756.2588 - val_loss: 10101.5997 - val_mse: 178153584.0000 - val_mae: 10101.5996\n",
            "Epoch 318/500\n",
            "80/80 [==============================] - 0s 924us/step - loss: 10557.0947 - mse: 206611232.0000 - mae: 10557.0957 - val_loss: 10094.1196 - val_mse: 177846512.0000 - val_mae: 10094.1201\n",
            "Epoch 319/500\n",
            "80/80 [==============================] - 0s 787us/step - loss: 10767.9011 - mse: 200745872.0000 - mae: 10767.9004 - val_loss: 10080.6709 - val_mse: 177657424.0000 - val_mae: 10080.6709\n",
            "Epoch 320/500\n",
            "80/80 [==============================] - 0s 776us/step - loss: 11845.7789 - mse: 231353392.0000 - mae: 11845.7793 - val_loss: 10074.8135 - val_mse: 177371616.0000 - val_mae: 10074.8135\n",
            "Epoch 321/500\n",
            "80/80 [==============================] - 0s 801us/step - loss: 10170.4613 - mse: 176596480.0000 - mae: 10170.4619 - val_loss: 10069.0406 - val_mse: 176917648.0000 - val_mae: 10069.0410\n",
            "Epoch 322/500\n",
            "80/80 [==============================] - 0s 718us/step - loss: 9863.7014 - mse: 164225152.0000 - mae: 9863.7012 - val_loss: 10060.7876 - val_mse: 176533248.0000 - val_mae: 10060.7881\n",
            "Epoch 323/500\n",
            "80/80 [==============================] - 0s 757us/step - loss: 11013.3881 - mse: 201330912.0000 - mae: 11013.3887 - val_loss: 10056.7258 - val_mse: 176305088.0000 - val_mae: 10056.7266\n",
            "Epoch 324/500\n",
            "80/80 [==============================] - 0s 915us/step - loss: 10408.6734 - mse: 185332592.0000 - mae: 10408.6738 - val_loss: 10035.0801 - val_mse: 175743056.0000 - val_mae: 10035.0791\n",
            "Epoch 325/500\n",
            "80/80 [==============================] - 0s 939us/step - loss: 11319.1608 - mse: 212185776.0000 - mae: 11319.1611 - val_loss: 10036.7179 - val_mse: 175673264.0000 - val_mae: 10036.7178\n",
            "Epoch 326/500\n",
            "80/80 [==============================] - 0s 776us/step - loss: 10659.2042 - mse: 198678224.0000 - mae: 10659.2051 - val_loss: 10037.0718 - val_mse: 176009040.0000 - val_mae: 10037.0723\n",
            "Epoch 327/500\n",
            "80/80 [==============================] - 0s 801us/step - loss: 12110.3455 - mse: 252381136.0000 - mae: 12110.3467 - val_loss: 10021.5473 - val_mse: 175287040.0000 - val_mae: 10021.5469\n",
            "Epoch 328/500\n",
            "80/80 [==============================] - 0s 754us/step - loss: 11417.4725 - mse: 210366112.0000 - mae: 11417.4717 - val_loss: 10012.2520 - val_mse: 174941040.0000 - val_mae: 10012.2520\n",
            "Epoch 329/500\n",
            "80/80 [==============================] - 0s 767us/step - loss: 10116.4310 - mse: 179314816.0000 - mae: 10116.4316 - val_loss: 10008.9776 - val_mse: 175042720.0000 - val_mae: 10008.9766\n",
            "Epoch 330/500\n",
            "80/80 [==============================] - 0s 844us/step - loss: 11174.9288 - mse: 218161024.0000 - mae: 11174.9277 - val_loss: 9985.1926 - val_mse: 174491056.0000 - val_mae: 9985.1924\n",
            "Epoch 331/500\n",
            "80/80 [==============================] - 0s 777us/step - loss: 11496.8269 - mse: 224199632.0000 - mae: 11496.8262 - val_loss: 9974.7131 - val_mse: 174372048.0000 - val_mae: 9974.7129\n",
            "Epoch 332/500\n",
            "80/80 [==============================] - 0s 725us/step - loss: 11360.7119 - mse: 206261520.0000 - mae: 11360.7119 - val_loss: 9978.6868 - val_mse: 174462720.0000 - val_mae: 9978.6875\n",
            "Epoch 333/500\n",
            "80/80 [==============================] - 0s 823us/step - loss: 10146.9111 - mse: 182974768.0000 - mae: 10146.9111 - val_loss: 9971.5982 - val_mse: 174416688.0000 - val_mae: 9971.5977\n",
            "Epoch 334/500\n",
            "80/80 [==============================] - 0s 933us/step - loss: 11217.7648 - mse: 212998560.0000 - mae: 11217.7637 - val_loss: 9977.4137 - val_mse: 174592416.0000 - val_mae: 9977.4131\n",
            "Epoch 335/500\n",
            "80/80 [==============================] - 0s 892us/step - loss: 11818.1893 - mse: 229108960.0000 - mae: 11818.1885 - val_loss: 9968.1742 - val_mse: 174455968.0000 - val_mae: 9968.1738\n",
            "Epoch 336/500\n",
            "80/80 [==============================] - 0s 739us/step - loss: 11687.7627 - mse: 224423520.0000 - mae: 11687.7637 - val_loss: 9972.0308 - val_mse: 174651328.0000 - val_mae: 9972.0303\n",
            "Epoch 337/500\n",
            "80/80 [==============================] - 0s 774us/step - loss: 11935.5496 - mse: 230576592.0000 - mae: 11935.5498 - val_loss: 9971.8504 - val_mse: 174306272.0000 - val_mae: 9971.8496\n",
            "Epoch 338/500\n",
            "80/80 [==============================] - 0s 783us/step - loss: 10992.9825 - mse: 204344304.0000 - mae: 10992.9824 - val_loss: 9966.1164 - val_mse: 174180240.0000 - val_mae: 9966.1162\n",
            "Epoch 339/500\n",
            "80/80 [==============================] - 0s 893us/step - loss: 10240.7822 - mse: 177760096.0000 - mae: 10240.7812 - val_loss: 9957.7211 - val_mse: 173894080.0000 - val_mae: 9957.7217\n",
            "Epoch 340/500\n",
            "80/80 [==============================] - 0s 875us/step - loss: 10328.7693 - mse: 187579968.0000 - mae: 10328.7705 - val_loss: 9957.8215 - val_mse: 173884080.0000 - val_mae: 9957.8213\n",
            "Epoch 341/500\n",
            "80/80 [==============================] - 0s 848us/step - loss: 12210.3962 - mse: 239844400.0000 - mae: 12210.3955 - val_loss: 9958.8675 - val_mse: 173953568.0000 - val_mae: 9958.8672\n",
            "Epoch 342/500\n",
            "80/80 [==============================] - 0s 719us/step - loss: 11237.5560 - mse: 221182688.0000 - mae: 11237.5566 - val_loss: 9963.2650 - val_mse: 174023936.0000 - val_mae: 9963.2656\n",
            "Epoch 343/500\n",
            "80/80 [==============================] - 0s 760us/step - loss: 10326.8416 - mse: 174291728.0000 - mae: 10326.8418 - val_loss: 9963.2529 - val_mse: 173974832.0000 - val_mae: 9963.2539\n",
            "Epoch 344/500\n",
            "80/80 [==============================] - 0s 781us/step - loss: 9734.2862 - mse: 170733872.0000 - mae: 9734.2861 - val_loss: 9954.0308 - val_mse: 173669536.0000 - val_mae: 9954.0312\n",
            "Epoch 345/500\n",
            "80/80 [==============================] - 0s 917us/step - loss: 10766.0172 - mse: 181051904.0000 - mae: 10766.0176 - val_loss: 9962.7997 - val_mse: 173936944.0000 - val_mae: 9962.7988\n",
            "Epoch 346/500\n",
            "80/80 [==============================] - 0s 829us/step - loss: 10312.3613 - mse: 175751328.0000 - mae: 10312.3613 - val_loss: 9939.9284 - val_mse: 173352656.0000 - val_mae: 9939.9287\n",
            "Epoch 347/500\n",
            "80/80 [==============================] - 0s 716us/step - loss: 11709.1425 - mse: 212161024.0000 - mae: 11709.1426 - val_loss: 9945.3852 - val_mse: 173490784.0000 - val_mae: 9945.3848\n",
            "Epoch 348/500\n",
            "80/80 [==============================] - 0s 799us/step - loss: 10820.1848 - mse: 194889792.0000 - mae: 10820.1846 - val_loss: 9932.7897 - val_mse: 173295776.0000 - val_mae: 9932.7900\n",
            "Epoch 349/500\n",
            "80/80 [==============================] - 0s 883us/step - loss: 11222.1030 - mse: 228953472.0000 - mae: 11222.1035 - val_loss: 9930.4353 - val_mse: 173115472.0000 - val_mae: 9930.4355\n",
            "Epoch 350/500\n",
            "80/80 [==============================] - 0s 925us/step - loss: 11298.9223 - mse: 203447968.0000 - mae: 11298.9219 - val_loss: 9927.9893 - val_mse: 173158896.0000 - val_mae: 9927.9902\n",
            "Epoch 351/500\n",
            "80/80 [==============================] - 0s 749us/step - loss: 11316.5447 - mse: 197889472.0000 - mae: 11316.5449 - val_loss: 9921.5825 - val_mse: 173062224.0000 - val_mae: 9921.5820\n",
            "Epoch 352/500\n",
            "80/80 [==============================] - 0s 824us/step - loss: 11381.0375 - mse: 220864096.0000 - mae: 11381.0371 - val_loss: 9923.9810 - val_mse: 173250320.0000 - val_mae: 9923.9805\n",
            "Epoch 353/500\n",
            "80/80 [==============================] - 0s 834us/step - loss: 11762.3464 - mse: 224129440.0000 - mae: 11762.3467 - val_loss: 9927.0298 - val_mse: 173274224.0000 - val_mae: 9927.0303\n",
            "Epoch 354/500\n",
            "80/80 [==============================] - 0s 871us/step - loss: 12527.8789 - mse: 240098784.0000 - mae: 12527.8789 - val_loss: 9912.6450 - val_mse: 172903296.0000 - val_mae: 9912.6445\n",
            "Epoch 355/500\n",
            "80/80 [==============================] - 0s 727us/step - loss: 11144.9116 - mse: 211237904.0000 - mae: 11144.9121 - val_loss: 9888.4119 - val_mse: 172497696.0000 - val_mae: 9888.4111\n",
            "Epoch 356/500\n",
            "80/80 [==============================] - 0s 762us/step - loss: 11241.8044 - mse: 204408352.0000 - mae: 11241.8047 - val_loss: 9887.0782 - val_mse: 172448576.0000 - val_mae: 9887.0781\n",
            "Epoch 357/500\n",
            "80/80 [==============================] - 0s 937us/step - loss: 10858.0432 - mse: 208432048.0000 - mae: 10858.0439 - val_loss: 9879.2145 - val_mse: 172121440.0000 - val_mae: 9879.2139\n",
            "Epoch 358/500\n",
            "80/80 [==============================] - 0s 840us/step - loss: 10768.0665 - mse: 201074432.0000 - mae: 10768.0674 - val_loss: 9898.4104 - val_mse: 172495024.0000 - val_mae: 9898.4111\n",
            "Epoch 359/500\n",
            "80/80 [==============================] - 0s 809us/step - loss: 11462.9514 - mse: 228268592.0000 - mae: 11462.9521 - val_loss: 9894.5412 - val_mse: 172308736.0000 - val_mae: 9894.5410\n",
            "Epoch 360/500\n",
            "80/80 [==============================] - 0s 895us/step - loss: 9989.7302 - mse: 181634960.0000 - mae: 9989.7305 - val_loss: 9900.1843 - val_mse: 172268080.0000 - val_mae: 9900.1836\n",
            "Epoch 361/500\n",
            "80/80 [==============================] - 0s 830us/step - loss: 9289.2440 - mse: 156099424.0000 - mae: 9289.2441 - val_loss: 9896.9156 - val_mse: 172137504.0000 - val_mae: 9896.9160\n",
            "Epoch 362/500\n",
            "80/80 [==============================] - 0s 812us/step - loss: 10494.1051 - mse: 180049920.0000 - mae: 10494.1045 - val_loss: 9894.4546 - val_mse: 172225360.0000 - val_mae: 9894.4541\n",
            "Epoch 363/500\n",
            "80/80 [==============================] - 0s 819us/step - loss: 10385.6799 - mse: 178186736.0000 - mae: 10385.6797 - val_loss: 9878.1710 - val_mse: 171813600.0000 - val_mae: 9878.1719\n",
            "Epoch 364/500\n",
            "80/80 [==============================] - 0s 823us/step - loss: 9442.8810 - mse: 152286400.0000 - mae: 9442.8809 - val_loss: 9874.2497 - val_mse: 171691968.0000 - val_mae: 9874.2490\n",
            "Epoch 365/500\n",
            "80/80 [==============================] - 0s 781us/step - loss: 9865.2225 - mse: 179590432.0000 - mae: 9865.2217 - val_loss: 9870.1147 - val_mse: 171569888.0000 - val_mae: 9870.1143\n",
            "Epoch 366/500\n",
            "80/80 [==============================] - 0s 823us/step - loss: 10716.2031 - mse: 181548768.0000 - mae: 10716.2031 - val_loss: 9863.3600 - val_mse: 171223104.0000 - val_mae: 9863.3604\n",
            "Epoch 367/500\n",
            "80/80 [==============================] - 0s 874us/step - loss: 11000.3687 - mse: 210823984.0000 - mae: 11000.3691 - val_loss: 9862.4384 - val_mse: 171138720.0000 - val_mae: 9862.4385\n",
            "Epoch 368/500\n",
            "80/80 [==============================] - 0s 782us/step - loss: 10530.7161 - mse: 204594576.0000 - mae: 10530.7158 - val_loss: 9857.2804 - val_mse: 171038592.0000 - val_mae: 9857.2803\n",
            "Epoch 369/500\n",
            "80/80 [==============================] - 0s 850us/step - loss: 10817.6242 - mse: 192460848.0000 - mae: 10817.6230 - val_loss: 9848.2453 - val_mse: 171081792.0000 - val_mae: 9848.2451\n",
            "Epoch 370/500\n",
            "80/80 [==============================] - 0s 919us/step - loss: 10535.5317 - mse: 190414704.0000 - mae: 10535.5312 - val_loss: 9823.5311 - val_mse: 170675792.0000 - val_mae: 9823.5312\n",
            "Epoch 371/500\n",
            "80/80 [==============================] - 0s 809us/step - loss: 11241.4999 - mse: 199931488.0000 - mae: 11241.5000 - val_loss: 9829.2640 - val_mse: 170713680.0000 - val_mae: 9829.2637\n",
            "Epoch 372/500\n",
            "80/80 [==============================] - 0s 797us/step - loss: 11448.4427 - mse: 217694000.0000 - mae: 11448.4424 - val_loss: 9834.5379 - val_mse: 170757712.0000 - val_mae: 9834.5381\n",
            "Epoch 373/500\n",
            "80/80 [==============================] - 0s 962us/step - loss: 10770.4279 - mse: 195832512.0000 - mae: 10770.4277 - val_loss: 9839.4560 - val_mse: 170894368.0000 - val_mae: 9839.4561\n",
            "Epoch 374/500\n",
            "80/80 [==============================] - 0s 742us/step - loss: 10904.7083 - mse: 196789456.0000 - mae: 10904.7090 - val_loss: 9828.5704 - val_mse: 170694704.0000 - val_mae: 9828.5703\n",
            "Epoch 375/500\n",
            "80/80 [==============================] - 0s 821us/step - loss: 12853.9071 - mse: 279906112.0000 - mae: 12853.9072 - val_loss: 9823.3660 - val_mse: 170555360.0000 - val_mae: 9823.3662\n",
            "Epoch 376/500\n",
            "80/80 [==============================] - 0s 833us/step - loss: 9819.3167 - mse: 155845344.0000 - mae: 9819.3174 - val_loss: 9822.4773 - val_mse: 170464848.0000 - val_mae: 9822.4766\n",
            "Epoch 377/500\n",
            "80/80 [==============================] - 0s 801us/step - loss: 10634.7590 - mse: 179347824.0000 - mae: 10634.7588 - val_loss: 9825.3571 - val_mse: 170560528.0000 - val_mae: 9825.3574\n",
            "Epoch 378/500\n",
            "80/80 [==============================] - 0s 824us/step - loss: 10725.6669 - mse: 194446896.0000 - mae: 10725.6660 - val_loss: 9827.3580 - val_mse: 170610256.0000 - val_mae: 9827.3584\n",
            "Epoch 379/500\n",
            "80/80 [==============================] - 0s 846us/step - loss: 11183.4144 - mse: 196297440.0000 - mae: 11183.4141 - val_loss: 9836.9521 - val_mse: 170840560.0000 - val_mae: 9836.9521\n",
            "Epoch 380/500\n",
            "80/80 [==============================] - 0s 886us/step - loss: 11752.2779 - mse: 216615888.0000 - mae: 11752.2783 - val_loss: 9845.0375 - val_mse: 170903648.0000 - val_mae: 9845.0371\n",
            "Epoch 381/500\n",
            "80/80 [==============================] - 0s 935us/step - loss: 11741.8962 - mse: 230269664.0000 - mae: 11741.8965 - val_loss: 9852.1194 - val_mse: 170949984.0000 - val_mae: 9852.1191\n",
            "Epoch 382/500\n",
            "80/80 [==============================] - 0s 829us/step - loss: 12251.8546 - mse: 262908192.0000 - mae: 12251.8535 - val_loss: 9858.1071 - val_mse: 171316512.0000 - val_mae: 9858.1074\n",
            "Epoch 383/500\n",
            "80/80 [==============================] - 0s 830us/step - loss: 11268.3736 - mse: 212987472.0000 - mae: 11268.3740 - val_loss: 9865.4911 - val_mse: 171423952.0000 - val_mae: 9865.4912\n",
            "Epoch 384/500\n",
            "80/80 [==============================] - 0s 820us/step - loss: 10593.2214 - mse: 180266208.0000 - mae: 10593.2207 - val_loss: 9859.8685 - val_mse: 171188752.0000 - val_mae: 9859.8691\n",
            "Epoch 385/500\n",
            "80/80 [==============================] - 0s 708us/step - loss: 10391.1643 - mse: 185051536.0000 - mae: 10391.1641 - val_loss: 9852.3126 - val_mse: 171064928.0000 - val_mae: 9852.3125\n",
            "Epoch 386/500\n",
            "80/80 [==============================] - 0s 780us/step - loss: 10747.4377 - mse: 208981312.0000 - mae: 10747.4375 - val_loss: 9856.9201 - val_mse: 171103040.0000 - val_mae: 9856.9199\n",
            "Epoch 387/500\n",
            "80/80 [==============================] - 0s 770us/step - loss: 9432.6154 - mse: 156472832.0000 - mae: 9432.6152 - val_loss: 9847.1713 - val_mse: 170980864.0000 - val_mae: 9847.1719\n",
            "Epoch 388/500\n",
            "80/80 [==============================] - 0s 886us/step - loss: 12067.0429 - mse: 254995712.0000 - mae: 12067.0420 - val_loss: 9854.3412 - val_mse: 170860064.0000 - val_mae: 9854.3408\n",
            "Epoch 389/500\n",
            "80/80 [==============================] - 0s 771us/step - loss: 10844.0176 - mse: 180805584.0000 - mae: 10844.0176 - val_loss: 9856.3139 - val_mse: 170920848.0000 - val_mae: 9856.3135\n",
            "Epoch 390/500\n",
            "80/80 [==============================] - 0s 773us/step - loss: 11333.6617 - mse: 236315696.0000 - mae: 11333.6621 - val_loss: 9862.9639 - val_mse: 170983744.0000 - val_mae: 9862.9639\n",
            "Epoch 391/500\n",
            "80/80 [==============================] - 0s 920us/step - loss: 10925.7365 - mse: 194048896.0000 - mae: 10925.7363 - val_loss: 9856.2433 - val_mse: 170671600.0000 - val_mae: 9856.2432\n",
            "Epoch 392/500\n",
            "80/80 [==============================] - 0s 780us/step - loss: 10864.2726 - mse: 205466592.0000 - mae: 10864.2715 - val_loss: 9861.8115 - val_mse: 170847040.0000 - val_mae: 9861.8115\n",
            "Epoch 393/500\n",
            "80/80 [==============================] - 0s 892us/step - loss: 11571.5071 - mse: 231865440.0000 - mae: 11571.5068 - val_loss: 9858.7386 - val_mse: 170770112.0000 - val_mae: 9858.7383\n",
            "Epoch 394/500\n",
            "80/80 [==============================] - 0s 765us/step - loss: 12257.6901 - mse: 258391776.0000 - mae: 12257.6895 - val_loss: 9873.4165 - val_mse: 171264080.0000 - val_mae: 9873.4160\n",
            "Epoch 395/500\n",
            "80/80 [==============================] - 0s 890us/step - loss: 11655.3940 - mse: 243324928.0000 - mae: 11655.3936 - val_loss: 9866.0885 - val_mse: 171297184.0000 - val_mae: 9866.0889\n",
            "Epoch 396/500\n",
            "80/80 [==============================] - 0s 763us/step - loss: 10935.5872 - mse: 205991344.0000 - mae: 10935.5879 - val_loss: 9857.6878 - val_mse: 171175088.0000 - val_mae: 9857.6875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 90 samples, validate on 90 samples\n",
            "Epoch 1/500\n",
            "90/90 [==============================] - 2s 23ms/step - loss: 36156.3874 - mse: 1543868928.0000 - mae: 36156.3867 - val_loss: 38268.4759 - val_mse: 1666708480.0000 - val_mae: 38268.4766\n",
            "Epoch 2/500\n",
            "90/90 [==============================] - 0s 714us/step - loss: 36155.8457 - mse: 1543838976.0000 - mae: 36155.8438 - val_loss: 38267.0673 - val_mse: 1666625024.0000 - val_mae: 38267.0664\n",
            "Epoch 3/500\n",
            "90/90 [==============================] - 0s 673us/step - loss: 36151.1630 - mse: 1543563264.0000 - mae: 36151.1641 - val_loss: 38259.0026 - val_mse: 1666109312.0000 - val_mae: 38259.0039\n",
            "Epoch 4/500\n",
            "90/90 [==============================] - 0s 836us/step - loss: 36139.7355 - mse: 1542775936.0000 - mae: 36139.7344 - val_loss: 38243.5145 - val_mse: 1664999936.0000 - val_mae: 38243.5156\n",
            "Epoch 5/500\n",
            "90/90 [==============================] - 0s 831us/step - loss: 36117.9833 - mse: 1541349376.0000 - mae: 36117.9844 - val_loss: 38222.5102 - val_mse: 1663440768.0000 - val_mae: 38222.5117\n",
            "Epoch 6/500\n",
            "90/90 [==============================] - 0s 849us/step - loss: 36092.4627 - mse: 1539376768.0000 - mae: 36092.4609 - val_loss: 38197.5931 - val_mse: 1661561472.0000 - val_mae: 38197.5898\n",
            "Epoch 7/500\n",
            "90/90 [==============================] - 0s 688us/step - loss: 36070.6812 - mse: 1537804544.0000 - mae: 36070.6836 - val_loss: 38170.1526 - val_mse: 1659484544.0000 - val_mae: 38170.1562\n",
            "Epoch 8/500\n",
            "90/90 [==============================] - 0s 667us/step - loss: 36045.1209 - mse: 1536060672.0000 - mae: 36045.1211 - val_loss: 38137.5267 - val_mse: 1657022720.0000 - val_mae: 38137.5234\n",
            "Epoch 9/500\n",
            "90/90 [==============================] - 0s 805us/step - loss: 36007.0990 - mse: 1533366912.0000 - mae: 36007.1016 - val_loss: 38098.3110 - val_mse: 1654062208.0000 - val_mae: 38098.3125\n",
            "Epoch 10/500\n",
            "90/90 [==============================] - 0s 754us/step - loss: 35960.5243 - mse: 1530177664.0000 - mae: 35960.5234 - val_loss: 38052.6849 - val_mse: 1650608128.0000 - val_mae: 38052.6875\n",
            "Epoch 11/500\n",
            "90/90 [==============================] - 0s 666us/step - loss: 35910.8928 - mse: 1526769920.0000 - mae: 35910.8945 - val_loss: 38002.2396 - val_mse: 1646786432.0000 - val_mae: 38002.2383\n",
            "Epoch 12/500\n",
            "90/90 [==============================] - 0s 743us/step - loss: 35861.8158 - mse: 1522750848.0000 - mae: 35861.8164 - val_loss: 37947.8689 - val_mse: 1642667904.0000 - val_mae: 37947.8711\n",
            "Epoch 13/500\n",
            "90/90 [==============================] - 0s 724us/step - loss: 35803.7057 - mse: 1518319232.0000 - mae: 35803.7070 - val_loss: 37887.8902 - val_mse: 1638124800.0000 - val_mae: 37887.8906\n",
            "Epoch 14/500\n",
            "90/90 [==============================] - 0s 665us/step - loss: 35746.2754 - mse: 1513816832.0000 - mae: 35746.2734 - val_loss: 37823.5343 - val_mse: 1633253120.0000 - val_mae: 37823.5352\n",
            "Epoch 15/500\n",
            "90/90 [==============================] - 0s 832us/step - loss: 35646.4531 - mse: 1507542144.0000 - mae: 35646.4492 - val_loss: 37750.1914 - val_mse: 1627718784.0000 - val_mae: 37750.1914\n",
            "Epoch 16/500\n",
            "90/90 [==============================] - 0s 700us/step - loss: 35580.8876 - mse: 1502708736.0000 - mae: 35580.8867 - val_loss: 37671.3921 - val_mse: 1621780736.0000 - val_mae: 37671.3945\n",
            "Epoch 17/500\n",
            "90/90 [==============================] - 0s 671us/step - loss: 35499.5582 - mse: 1497000960.0000 - mae: 35499.5586 - val_loss: 37586.4106 - val_mse: 1615392256.0000 - val_mae: 37586.4102\n",
            "Epoch 18/500\n",
            "90/90 [==============================] - 0s 720us/step - loss: 35415.3737 - mse: 1491779072.0000 - mae: 35415.3750 - val_loss: 37494.3971 - val_mse: 1608493696.0000 - val_mae: 37494.3984\n",
            "Epoch 19/500\n",
            "90/90 [==============================] - 0s 808us/step - loss: 35325.1747 - mse: 1485298560.0000 - mae: 35325.1758 - val_loss: 37397.1005 - val_mse: 1601212928.0000 - val_mae: 37397.1016\n",
            "Epoch 20/500\n",
            "90/90 [==============================] - 0s 781us/step - loss: 35223.5382 - mse: 1477810944.0000 - mae: 35223.5391 - val_loss: 37294.0605 - val_mse: 1593518976.0000 - val_mae: 37294.0547\n",
            "Epoch 21/500\n",
            "90/90 [==============================] - 0s 770us/step - loss: 35182.6934 - mse: 1475254272.0000 - mae: 35182.6953 - val_loss: 37191.9763 - val_mse: 1585916928.0000 - val_mae: 37191.9766\n",
            "Epoch 22/500\n",
            "90/90 [==============================] - 0s 690us/step - loss: 35002.8414 - mse: 1462702848.0000 - mae: 35002.8398 - val_loss: 37077.3140 - val_mse: 1577403648.0000 - val_mae: 37077.3125\n",
            "Epoch 23/500\n",
            "90/90 [==============================] - 0s 764us/step - loss: 34873.6630 - mse: 1453116288.0000 - mae: 34873.6602 - val_loss: 36952.7977 - val_mse: 1568188672.0000 - val_mae: 36952.7930\n",
            "Epoch 24/500\n",
            "90/90 [==============================] - 0s 683us/step - loss: 34760.8392 - mse: 1444091264.0000 - mae: 34760.8398 - val_loss: 36821.9010 - val_mse: 1558534528.0000 - val_mae: 36821.9023\n",
            "Epoch 25/500\n",
            "90/90 [==============================] - 0s 754us/step - loss: 34624.7233 - mse: 1436934656.0000 - mae: 34624.7266 - val_loss: 36684.7331 - val_mse: 1548454784.0000 - val_mae: 36684.7344\n",
            "Epoch 26/500\n",
            "90/90 [==============================] - 0s 823us/step - loss: 34537.8116 - mse: 1429420672.0000 - mae: 34537.8125 - val_loss: 36546.2986 - val_mse: 1538318464.0000 - val_mae: 36546.3008\n",
            "Epoch 27/500\n",
            "90/90 [==============================] - 0s 656us/step - loss: 34337.0382 - mse: 1416451840.0000 - mae: 34337.0352 - val_loss: 36396.7298 - val_mse: 1527408640.0000 - val_mae: 36396.7305\n",
            "Epoch 28/500\n",
            "90/90 [==============================] - 0s 679us/step - loss: 34220.3140 - mse: 1407315840.0000 - mae: 34220.3164 - val_loss: 36243.6411 - val_mse: 1516289280.0000 - val_mae: 36243.6406\n",
            "Epoch 29/500\n",
            "90/90 [==============================] - 0s 669us/step - loss: 33966.8976 - mse: 1390674688.0000 - mae: 33966.8984 - val_loss: 36078.2465 - val_mse: 1504327424.0000 - val_mae: 36078.2461\n",
            "Epoch 30/500\n",
            "90/90 [==============================] - 0s 664us/step - loss: 33855.3952 - mse: 1384039552.0000 - mae: 33855.3984 - val_loss: 35911.0204 - val_mse: 1492290176.0000 - val_mae: 35911.0195\n",
            "Epoch 31/500\n",
            "90/90 [==============================] - 0s 642us/step - loss: 33624.0790 - mse: 1364377728.0000 - mae: 33624.0781 - val_loss: 35734.4763 - val_mse: 1479639168.0000 - val_mae: 35734.4766\n",
            "Epoch 32/500\n",
            "90/90 [==============================] - 0s 764us/step - loss: 33538.9540 - mse: 1365639936.0000 - mae: 33538.9570 - val_loss: 35558.5547 - val_mse: 1467099136.0000 - val_mae: 35558.5586\n",
            "Epoch 33/500\n",
            "90/90 [==============================] - 0s 864us/step - loss: 33301.1478 - mse: 1343896320.0000 - mae: 33301.1484 - val_loss: 35372.8581 - val_mse: 1453926656.0000 - val_mae: 35372.8594\n",
            "Epoch 34/500\n",
            "90/90 [==============================] - 0s 797us/step - loss: 33164.6345 - mse: 1336456576.0000 - mae: 33164.6328 - val_loss: 35183.9954 - val_mse: 1440600320.0000 - val_mae: 35183.9961\n",
            "Epoch 35/500\n",
            "90/90 [==============================] - 0s 874us/step - loss: 33059.7049 - mse: 1328615168.0000 - mae: 33059.7070 - val_loss: 34994.6042 - val_mse: 1427307008.0000 - val_mae: 34994.6055\n",
            "Epoch 36/500\n",
            "90/90 [==============================] - 0s 857us/step - loss: 32850.8132 - mse: 1310101504.0000 - mae: 32850.8125 - val_loss: 34797.5215 - val_mse: 1413551232.0000 - val_mae: 34797.5234\n",
            "Epoch 37/500\n",
            "90/90 [==============================] - 0s 777us/step - loss: 32788.7146 - mse: 1312298880.0000 - mae: 32788.7148 - val_loss: 34602.8279 - val_mse: 1400040832.0000 - val_mae: 34602.8281\n",
            "Epoch 38/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 32541.3201 - mse: 1296506368.0000 - mae: 32541.3203 - val_loss: 34399.0671 - val_mse: 1385982208.0000 - val_mae: 34399.0664\n",
            "Epoch 39/500\n",
            "90/90 [==============================] - 0s 787us/step - loss: 32417.5731 - mse: 1290142848.0000 - mae: 32417.5723 - val_loss: 34190.0605 - val_mse: 1371646592.0000 - val_mae: 34190.0625\n",
            "Epoch 40/500\n",
            "90/90 [==============================] - 0s 760us/step - loss: 31963.1751 - mse: 1256659968.0000 - mae: 31963.1777 - val_loss: 33959.0158 - val_mse: 1355901056.0000 - val_mae: 33959.0156\n",
            "Epoch 41/500\n",
            "90/90 [==============================] - 0s 741us/step - loss: 31665.2680 - mse: 1242337536.0000 - mae: 31665.2676 - val_loss: 33714.5388 - val_mse: 1339357824.0000 - val_mae: 33714.5391\n",
            "Epoch 42/500\n",
            "90/90 [==============================] - 0s 781us/step - loss: 31578.0998 - mse: 1238944640.0000 - mae: 31578.0977 - val_loss: 33474.7250 - val_mse: 1323245440.0000 - val_mae: 33474.7266\n",
            "Epoch 43/500\n",
            "90/90 [==============================] - 0s 858us/step - loss: 31213.5323 - mse: 1216999552.0000 - mae: 31213.5273 - val_loss: 33221.2780 - val_mse: 1306341376.0000 - val_mae: 33221.2734\n",
            "Epoch 44/500\n",
            "90/90 [==============================] - 0s 714us/step - loss: 31005.6482 - mse: 1193984768.0000 - mae: 31005.6504 - val_loss: 32961.5188 - val_mse: 1289147520.0000 - val_mae: 32961.5156\n",
            "Epoch 45/500\n",
            "90/90 [==============================] - 0s 741us/step - loss: 30832.2339 - mse: 1189714560.0000 - mae: 30832.2363 - val_loss: 32700.7006 - val_mse: 1272021376.0000 - val_mae: 32700.7051\n",
            "Epoch 46/500\n",
            "90/90 [==============================] - 0s 683us/step - loss: 30471.9258 - mse: 1170811136.0000 - mae: 30471.9219 - val_loss: 32427.3985 - val_mse: 1254221440.0000 - val_mae: 32427.4004\n",
            "Epoch 47/500\n",
            "90/90 [==============================] - 0s 732us/step - loss: 30248.3353 - mse: 1154335872.0000 - mae: 30248.3340 - val_loss: 32150.1809 - val_mse: 1236318848.0000 - val_mae: 32150.1836\n",
            "Epoch 48/500\n",
            "90/90 [==============================] - 0s 837us/step - loss: 30017.8537 - mse: 1143996928.0000 - mae: 30017.8535 - val_loss: 31869.9961 - val_mse: 1218381056.0000 - val_mae: 31869.9980\n",
            "Epoch 49/500\n",
            "90/90 [==============================] - 0s 863us/step - loss: 29792.0176 - mse: 1137077632.0000 - mae: 29792.0195 - val_loss: 31585.6853 - val_mse: 1200340992.0000 - val_mae: 31585.6895\n",
            "Epoch 50/500\n",
            "90/90 [==============================] - 0s 664us/step - loss: 29303.3082 - mse: 1107209856.0000 - mae: 29303.3047 - val_loss: 31286.5034 - val_mse: 1181530752.0000 - val_mae: 31286.5020\n",
            "Epoch 51/500\n",
            "90/90 [==============================] - 0s 814us/step - loss: 29269.1899 - mse: 1096824064.0000 - mae: 29269.1895 - val_loss: 30991.4324 - val_mse: 1163153280.0000 - val_mae: 30991.4336\n",
            "Epoch 52/500\n",
            "90/90 [==============================] - 0s 841us/step - loss: 28823.8926 - mse: 1074896640.0000 - mae: 28823.8945 - val_loss: 30683.8377 - val_mse: 1144182144.0000 - val_mae: 30683.8398\n",
            "Epoch 53/500\n",
            "90/90 [==============================] - 0s 753us/step - loss: 28783.3633 - mse: 1066241664.0000 - mae: 28783.3613 - val_loss: 30378.9876 - val_mse: 1125565568.0000 - val_mae: 30378.9863\n",
            "Epoch 54/500\n",
            "90/90 [==============================] - 0s 763us/step - loss: 27949.3670 - mse: 1025733952.0000 - mae: 27949.3672 - val_loss: 30047.8597 - val_mse: 1105555584.0000 - val_mae: 30047.8555\n",
            "Epoch 55/500\n",
            "90/90 [==============================] - 0s 683us/step - loss: 27868.3073 - mse: 1020114624.0000 - mae: 27868.3047 - val_loss: 29720.5268 - val_mse: 1085990144.0000 - val_mae: 29720.5273\n",
            "Epoch 56/500\n",
            "90/90 [==============================] - 0s 700us/step - loss: 27628.4740 - mse: 1004150784.0000 - mae: 27628.4727 - val_loss: 29389.1841 - val_mse: 1066403456.0000 - val_mae: 29389.1855\n",
            "Epoch 57/500\n",
            "90/90 [==============================] - 0s 767us/step - loss: 26958.2038 - mse: 973331584.0000 - mae: 26958.2031 - val_loss: 29049.8511 - val_mse: 1046571392.0000 - val_mae: 29049.8496\n",
            "Epoch 58/500\n",
            "90/90 [==============================] - 0s 708us/step - loss: 27447.0354 - mse: 1011378432.0000 - mae: 27447.0332 - val_loss: 28731.4845 - val_mse: 1028175360.0000 - val_mae: 28731.4824\n",
            "Epoch 59/500\n",
            "90/90 [==============================] - 0s 668us/step - loss: 26342.0109 - mse: 939600640.0000 - mae: 26342.0117 - val_loss: 28368.9436 - val_mse: 1007473344.0000 - val_mae: 28368.9453\n",
            "Epoch 60/500\n",
            "90/90 [==============================] - 0s 712us/step - loss: 26105.5378 - mse: 923232512.0000 - mae: 26105.5391 - val_loss: 28005.9329 - val_mse: 987006848.0000 - val_mae: 28005.9297\n",
            "Epoch 61/500\n",
            "90/90 [==============================] - 0s 850us/step - loss: 25698.0039 - mse: 909667328.0000 - mae: 25698.0059 - val_loss: 27634.8199 - val_mse: 966356544.0000 - val_mae: 27634.8203\n",
            "Epoch 62/500\n",
            "90/90 [==============================] - 0s 818us/step - loss: 25434.1497 - mse: 888129792.0000 - mae: 25434.1504 - val_loss: 27270.0177 - val_mse: 946324480.0000 - val_mae: 27270.0176\n",
            "Epoch 63/500\n",
            "90/90 [==============================] - 0s 737us/step - loss: 24995.8214 - mse: 897919296.0000 - mae: 24995.8223 - val_loss: 26904.6329 - val_mse: 926529216.0000 - val_mae: 26904.6309\n",
            "Epoch 64/500\n",
            "90/90 [==============================] - 0s 685us/step - loss: 24307.2329 - mse: 825252992.0000 - mae: 24307.2324 - val_loss: 26503.5686 - val_mse: 905105600.0000 - val_mae: 26503.5703\n",
            "Epoch 65/500\n",
            "90/90 [==============================] - 0s 733us/step - loss: 24327.2614 - mse: 839534848.0000 - mae: 24327.2637 - val_loss: 26117.1574 - val_mse: 884769856.0000 - val_mae: 26117.1582\n",
            "Epoch 66/500\n",
            "90/90 [==============================] - 0s 761us/step - loss: 24267.3979 - mse: 829052416.0000 - mae: 24267.3965 - val_loss: 25736.3152 - val_mse: 865018496.0000 - val_mae: 25736.3164\n",
            "Epoch 67/500\n",
            "90/90 [==============================] - 0s 742us/step - loss: 23474.4707 - mse: 794370240.0000 - mae: 23474.4688 - val_loss: 25340.9425 - val_mse: 844820032.0000 - val_mae: 25340.9453\n",
            "Epoch 68/500\n",
            "90/90 [==============================] - 0s 794us/step - loss: 23398.5519 - mse: 792607232.0000 - mae: 23398.5527 - val_loss: 24962.2763 - val_mse: 825767424.0000 - val_mae: 24962.2773\n",
            "Epoch 69/500\n",
            "90/90 [==============================] - 0s 670us/step - loss: 22886.3722 - mse: 768069440.0000 - mae: 22886.3730 - val_loss: 24564.4026 - val_mse: 806058048.0000 - val_mae: 24564.4023\n",
            "Epoch 70/500\n",
            "90/90 [==============================] - 0s 680us/step - loss: 22119.1235 - mse: 749108800.0000 - mae: 22119.1230 - val_loss: 24170.2082 - val_mse: 786843328.0000 - val_mae: 24170.2090\n",
            "Epoch 71/500\n",
            "90/90 [==============================] - 0s 740us/step - loss: 22326.9218 - mse: 754615552.0000 - mae: 22326.9199 - val_loss: 23785.9906 - val_mse: 768412992.0000 - val_mae: 23785.9883\n",
            "Epoch 72/500\n",
            "90/90 [==============================] - 0s 743us/step - loss: 21614.8228 - mse: 708843072.0000 - mae: 21614.8223 - val_loss: 23375.6570 - val_mse: 749056512.0000 - val_mae: 23375.6562\n",
            "Epoch 73/500\n",
            "90/90 [==============================] - 0s 677us/step - loss: 21570.2875 - mse: 707936768.0000 - mae: 21570.2852 - val_loss: 22976.0991 - val_mse: 730532352.0000 - val_mae: 22976.0996\n",
            "Epoch 74/500\n",
            "90/90 [==============================] - 0s 691us/step - loss: 20911.3405 - mse: 671662016.0000 - mae: 20911.3398 - val_loss: 22589.5401 - val_mse: 712912896.0000 - val_mae: 22589.5410\n",
            "Epoch 75/500\n",
            "90/90 [==============================] - 0s 687us/step - loss: 20896.1490 - mse: 663491136.0000 - mae: 20896.1504 - val_loss: 22166.5459 - val_mse: 693977664.0000 - val_mae: 22166.5449\n",
            "Epoch 76/500\n",
            "90/90 [==============================] - 0s 699us/step - loss: 20973.7687 - mse: 672855936.0000 - mae: 20973.7676 - val_loss: 21750.7299 - val_mse: 675713408.0000 - val_mae: 21750.7305\n",
            "Epoch 77/500\n",
            "90/90 [==============================] - 0s 782us/step - loss: 19192.8227 - mse: 579900672.0000 - mae: 19192.8223 - val_loss: 21319.7745 - val_mse: 657147712.0000 - val_mae: 21319.7734\n",
            "Epoch 78/500\n",
            "90/90 [==============================] - 0s 815us/step - loss: 19874.5087 - mse: 633485568.0000 - mae: 19874.5078 - val_loss: 20931.4772 - val_mse: 640738752.0000 - val_mae: 20931.4766\n",
            "Epoch 79/500\n",
            "90/90 [==============================] - 0s 732us/step - loss: 19499.4404 - mse: 617329216.0000 - mae: 19499.4395 - val_loss: 20511.3982 - val_mse: 623327424.0000 - val_mae: 20511.3965\n",
            "Epoch 80/500\n",
            "90/90 [==============================] - 0s 744us/step - loss: 19261.2411 - mse: 598688320.0000 - mae: 19261.2422 - val_loss: 20114.4657 - val_mse: 607198464.0000 - val_mae: 20114.4668\n",
            "Epoch 81/500\n",
            "90/90 [==============================] - 0s 753us/step - loss: 18031.7393 - mse: 543712576.0000 - mae: 18031.7383 - val_loss: 19741.3910 - val_mse: 592324352.0000 - val_mae: 19741.3926\n",
            "Epoch 82/500\n",
            "90/90 [==============================] - 0s 775us/step - loss: 17949.6432 - mse: 560922176.0000 - mae: 17949.6445 - val_loss: 19343.6534 - val_mse: 576776064.0000 - val_mae: 19343.6523\n",
            "Epoch 83/500\n",
            "90/90 [==============================] - 0s 675us/step - loss: 17679.0390 - mse: 552248704.0000 - mae: 17679.0410 - val_loss: 19011.8507 - val_mse: 564045120.0000 - val_mae: 19011.8496\n",
            "Epoch 84/500\n",
            "90/90 [==============================] - 0s 634us/step - loss: 17147.6497 - mse: 488743584.0000 - mae: 17147.6504 - val_loss: 18621.0325 - val_mse: 549332416.0000 - val_mae: 18621.0312\n",
            "Epoch 85/500\n",
            "90/90 [==============================] - 0s 818us/step - loss: 18155.9567 - mse: 556589504.0000 - mae: 18155.9551 - val_loss: 18221.7226 - val_mse: 534617664.0000 - val_mae: 18221.7227\n",
            "Epoch 86/500\n",
            "90/90 [==============================] - 0s 757us/step - loss: 17767.4138 - mse: 528365856.0000 - mae: 17767.4141 - val_loss: 17890.6023 - val_mse: 522654976.0000 - val_mae: 17890.6035\n",
            "Epoch 87/500\n",
            "90/90 [==============================] - 0s 736us/step - loss: 17732.5991 - mse: 548223168.0000 - mae: 17732.5977 - val_loss: 17546.3052 - val_mse: 510450400.0000 - val_mae: 17546.3047\n",
            "Epoch 88/500\n",
            "90/90 [==============================] - 0s 860us/step - loss: 15994.9409 - mse: 460608160.0000 - mae: 15994.9414 - val_loss: 17231.6553 - val_mse: 499401888.0000 - val_mae: 17231.6543\n",
            "Epoch 89/500\n",
            "90/90 [==============================] - 0s 773us/step - loss: 15565.0763 - mse: 455293664.0000 - mae: 15565.0762 - val_loss: 16938.6673 - val_mse: 488780448.0000 - val_mae: 16938.6660\n",
            "Epoch 90/500\n",
            "90/90 [==============================] - 0s 726us/step - loss: 16016.7675 - mse: 459827072.0000 - mae: 16016.7666 - val_loss: 16698.7857 - val_mse: 479755456.0000 - val_mae: 16698.7852\n",
            "Epoch 91/500\n",
            "90/90 [==============================] - 0s 815us/step - loss: 16853.2974 - mse: 499207872.0000 - mae: 16853.2969 - val_loss: 16425.8766 - val_mse: 469078784.0000 - val_mae: 16425.8770\n",
            "Epoch 92/500\n",
            "90/90 [==============================] - 0s 665us/step - loss: 15694.7684 - mse: 418498912.0000 - mae: 15694.7676 - val_loss: 16225.4254 - val_mse: 460595104.0000 - val_mae: 16225.4268\n",
            "Epoch 93/500\n",
            "90/90 [==============================] - 0s 750us/step - loss: 15893.3343 - mse: 442695264.0000 - mae: 15893.3330 - val_loss: 16057.5436 - val_mse: 452938048.0000 - val_mae: 16057.5420\n",
            "Epoch 94/500\n",
            "90/90 [==============================] - 0s 837us/step - loss: 15976.0144 - mse: 453344512.0000 - mae: 15976.0156 - val_loss: 15871.8855 - val_mse: 443998432.0000 - val_mae: 15871.8848\n",
            "Epoch 95/500\n",
            "90/90 [==============================] - 0s 854us/step - loss: 15292.3369 - mse: 407215584.0000 - mae: 15292.3359 - val_loss: 15724.4931 - val_mse: 436905536.0000 - val_mae: 15724.4932\n",
            "Epoch 96/500\n",
            "90/90 [==============================] - 0s 803us/step - loss: 15261.4571 - mse: 433484672.0000 - mae: 15261.4580 - val_loss: 15561.4064 - val_mse: 428492896.0000 - val_mae: 15561.4072\n",
            "Epoch 97/500\n",
            "90/90 [==============================] - 0s 852us/step - loss: 15386.1170 - mse: 426859968.0000 - mae: 15386.1162 - val_loss: 15428.9505 - val_mse: 421380768.0000 - val_mae: 15428.9502\n",
            "Epoch 98/500\n",
            "90/90 [==============================] - 0s 893us/step - loss: 15336.9012 - mse: 399249088.0000 - mae: 15336.9014 - val_loss: 15304.9702 - val_mse: 414827040.0000 - val_mae: 15304.9707\n",
            "Epoch 99/500\n",
            "90/90 [==============================] - 0s 979us/step - loss: 15663.9632 - mse: 445291584.0000 - mae: 15663.9639 - val_loss: 15196.4315 - val_mse: 409170976.0000 - val_mae: 15196.4307\n",
            "Epoch 100/500\n",
            "90/90 [==============================] - 0s 743us/step - loss: 15584.3389 - mse: 427865056.0000 - mae: 15584.3389 - val_loss: 15122.1160 - val_mse: 405342624.0000 - val_mae: 15122.1152\n",
            "Epoch 101/500\n",
            "90/90 [==============================] - 0s 793us/step - loss: 15578.8134 - mse: 427841664.0000 - mae: 15578.8135 - val_loss: 14985.0860 - val_mse: 398377088.0000 - val_mae: 14985.0859\n",
            "Epoch 102/500\n",
            "90/90 [==============================] - 0s 737us/step - loss: 14801.0111 - mse: 376099392.0000 - mae: 14801.0127 - val_loss: 14885.6793 - val_mse: 393400128.0000 - val_mae: 14885.6807\n",
            "Epoch 103/500\n",
            "90/90 [==============================] - 0s 701us/step - loss: 15227.1482 - mse: 388671456.0000 - mae: 15227.1484 - val_loss: 14767.1310 - val_mse: 387549280.0000 - val_mae: 14767.1309\n",
            "Epoch 104/500\n",
            "90/90 [==============================] - 0s 700us/step - loss: 14751.5421 - mse: 383808992.0000 - mae: 14751.5420 - val_loss: 14614.4715 - val_mse: 380149440.0000 - val_mae: 14614.4707\n",
            "Epoch 105/500\n",
            "90/90 [==============================] - 0s 754us/step - loss: 14627.7535 - mse: 377512000.0000 - mae: 14627.7539 - val_loss: 14470.1737 - val_mse: 373293152.0000 - val_mae: 14470.1738\n",
            "Epoch 106/500\n",
            "90/90 [==============================] - 0s 760us/step - loss: 14415.1187 - mse: 358536256.0000 - mae: 14415.1191 - val_loss: 14369.7240 - val_mse: 368600320.0000 - val_mae: 14369.7246\n",
            "Epoch 107/500\n",
            "90/90 [==============================] - 0s 811us/step - loss: 13927.9484 - mse: 362923680.0000 - mae: 13927.9482 - val_loss: 14270.1347 - val_mse: 364012096.0000 - val_mae: 14270.1357\n",
            "Epoch 108/500\n",
            "90/90 [==============================] - 0s 926us/step - loss: 13855.0118 - mse: 330733120.0000 - mae: 13855.0107 - val_loss: 14152.4257 - val_mse: 358670944.0000 - val_mae: 14152.4268\n",
            "Epoch 109/500\n",
            "90/90 [==============================] - 0s 750us/step - loss: 15165.1800 - mse: 394418624.0000 - mae: 15165.1807 - val_loss: 13995.8603 - val_mse: 351706240.0000 - val_mae: 13995.8613\n",
            "Epoch 110/500\n",
            "90/90 [==============================] - 0s 692us/step - loss: 14908.8655 - mse: 379355104.0000 - mae: 14908.8652 - val_loss: 13942.3916 - val_mse: 349364768.0000 - val_mae: 13942.3906\n",
            "Epoch 111/500\n",
            "90/90 [==============================] - 0s 738us/step - loss: 15183.0280 - mse: 407437952.0000 - mae: 15183.0273 - val_loss: 13871.8755 - val_mse: 346292192.0000 - val_mae: 13871.8750\n",
            "Epoch 112/500\n",
            "90/90 [==============================] - 0s 733us/step - loss: 14800.4863 - mse: 375940256.0000 - mae: 14800.4873 - val_loss: 13862.4390 - val_mse: 345874304.0000 - val_mae: 13862.4385\n",
            "Epoch 113/500\n",
            "90/90 [==============================] - 0s 840us/step - loss: 14852.2715 - mse: 383247744.0000 - mae: 14852.2705 - val_loss: 13782.2620 - val_mse: 342313632.0000 - val_mae: 13782.2627\n",
            "Epoch 114/500\n",
            "90/90 [==============================] - 0s 782us/step - loss: 14616.6770 - mse: 346980800.0000 - mae: 14616.6768 - val_loss: 13681.5441 - val_mse: 337794112.0000 - val_mae: 13681.5449\n",
            "Epoch 115/500\n",
            "90/90 [==============================] - 0s 765us/step - loss: 14780.9646 - mse: 358774432.0000 - mae: 14780.9648 - val_loss: 13601.6813 - val_mse: 334128864.0000 - val_mae: 13601.6807\n",
            "Epoch 116/500\n",
            "90/90 [==============================] - 0s 715us/step - loss: 13881.4413 - mse: 321559840.0000 - mae: 13881.4385 - val_loss: 13502.5768 - val_mse: 329683136.0000 - val_mae: 13502.5762\n",
            "Epoch 117/500\n",
            "90/90 [==============================] - 0s 770us/step - loss: 15199.0392 - mse: 377593088.0000 - mae: 15199.0391 - val_loss: 13445.3528 - val_mse: 327332128.0000 - val_mae: 13445.3516\n",
            "Epoch 118/500\n",
            "90/90 [==============================] - 0s 721us/step - loss: 15803.1785 - mse: 412836128.0000 - mae: 15803.1777 - val_loss: 13281.0092 - val_mse: 322635584.0000 - val_mae: 13281.0098\n",
            "Epoch 119/500\n",
            "90/90 [==============================] - 0s 695us/step - loss: 12814.8809 - mse: 332624864.0000 - mae: 12814.8809 - val_loss: 13172.4382 - val_mse: 317587296.0000 - val_mae: 13172.4385\n",
            "Epoch 120/500\n",
            "90/90 [==============================] - 0s 678us/step - loss: 13487.2820 - mse: 348371136.0000 - mae: 13487.2822 - val_loss: 13150.8952 - val_mse: 314538336.0000 - val_mae: 13150.8955\n",
            "Epoch 121/500\n",
            "90/90 [==============================] - 0s 731us/step - loss: 14483.1666 - mse: 388865216.0000 - mae: 14483.1680 - val_loss: 13015.7489 - val_mse: 311086048.0000 - val_mae: 13015.7490\n",
            "Epoch 122/500\n",
            "90/90 [==============================] - 0s 901us/step - loss: 13428.7504 - mse: 313369568.0000 - mae: 13428.7529 - val_loss: 12898.9123 - val_mse: 307110272.0000 - val_mae: 12898.9121\n",
            "Epoch 123/500\n",
            "90/90 [==============================] - 0s 797us/step - loss: 13774.8869 - mse: 328461696.0000 - mae: 13774.8857 - val_loss: 12735.1493 - val_mse: 302790016.0000 - val_mae: 12735.1504\n",
            "Epoch 124/500\n",
            "90/90 [==============================] - 0s 772us/step - loss: 14364.0885 - mse: 373134688.0000 - mae: 14364.0889 - val_loss: 12633.9720 - val_mse: 300121952.0000 - val_mae: 12633.9707\n",
            "Epoch 125/500\n",
            "90/90 [==============================] - 0s 676us/step - loss: 12546.5378 - mse: 278176832.0000 - mae: 12546.5371 - val_loss: 12499.7487 - val_mse: 294162464.0000 - val_mae: 12499.7490\n",
            "Epoch 126/500\n",
            "90/90 [==============================] - 0s 643us/step - loss: 12656.1977 - mse: 314665088.0000 - mae: 12656.1973 - val_loss: 12404.3405 - val_mse: 291088192.0000 - val_mae: 12404.3398\n",
            "Epoch 127/500\n",
            "90/90 [==============================] - 0s 808us/step - loss: 13371.0887 - mse: 339807968.0000 - mae: 13371.0889 - val_loss: 12262.2295 - val_mse: 286599392.0000 - val_mae: 12262.2295\n",
            "Epoch 128/500\n",
            "90/90 [==============================] - 0s 781us/step - loss: 13528.9557 - mse: 315890880.0000 - mae: 13528.9551 - val_loss: 12196.1018 - val_mse: 281622496.0000 - val_mae: 12196.1016\n",
            "Epoch 129/500\n",
            "90/90 [==============================] - 0s 765us/step - loss: 13265.8760 - mse: 312735008.0000 - mae: 13265.8760 - val_loss: 12130.6884 - val_mse: 278322048.0000 - val_mae: 12130.6885\n",
            "Epoch 130/500\n",
            "90/90 [==============================] - 0s 753us/step - loss: 13656.6719 - mse: 319200704.0000 - mae: 13656.6709 - val_loss: 12212.3045 - val_mse: 276289120.0000 - val_mae: 12212.3037\n",
            "Epoch 131/500\n",
            "90/90 [==============================] - 0s 793us/step - loss: 13492.2805 - mse: 317602656.0000 - mae: 13492.2822 - val_loss: 12079.8590 - val_mse: 272542528.0000 - val_mae: 12079.8584\n",
            "Epoch 132/500\n",
            "90/90 [==============================] - 0s 703us/step - loss: 13983.7926 - mse: 340587040.0000 - mae: 13983.7930 - val_loss: 11992.1661 - val_mse: 268431312.0000 - val_mae: 11992.1670\n",
            "Epoch 133/500\n",
            "90/90 [==============================] - 0s 695us/step - loss: 12581.8083 - mse: 304798752.0000 - mae: 12581.8086 - val_loss: 11848.1571 - val_mse: 263522848.0000 - val_mae: 11848.1572\n",
            "Epoch 134/500\n",
            "90/90 [==============================] - 0s 784us/step - loss: 12748.1022 - mse: 300235808.0000 - mae: 12748.1025 - val_loss: 11699.3509 - val_mse: 258648608.0000 - val_mae: 11699.3516\n",
            "Epoch 135/500\n",
            "90/90 [==============================] - 0s 719us/step - loss: 12503.5072 - mse: 281482560.0000 - mae: 12503.5068 - val_loss: 11592.1697 - val_mse: 254696992.0000 - val_mae: 11592.1699\n",
            "Epoch 136/500\n",
            "90/90 [==============================] - 0s 731us/step - loss: 12860.3345 - mse: 285444480.0000 - mae: 12860.3350 - val_loss: 11558.6143 - val_mse: 251561072.0000 - val_mae: 11558.6143\n",
            "Epoch 137/500\n",
            "90/90 [==============================] - 0s 712us/step - loss: 13547.1388 - mse: 324310016.0000 - mae: 13547.1387 - val_loss: 11463.8273 - val_mse: 247754656.0000 - val_mae: 11463.8271\n",
            "Epoch 138/500\n",
            "90/90 [==============================] - 0s 748us/step - loss: 12204.0089 - mse: 264107984.0000 - mae: 12204.0088 - val_loss: 11367.5073 - val_mse: 243964400.0000 - val_mae: 11367.5078\n",
            "Epoch 139/500\n",
            "90/90 [==============================] - 0s 752us/step - loss: 13721.7263 - mse: 325095488.0000 - mae: 13721.7266 - val_loss: 11230.0901 - val_mse: 240271520.0000 - val_mae: 11230.0898\n",
            "Epoch 140/500\n",
            "90/90 [==============================] - 0s 704us/step - loss: 11799.8369 - mse: 253319504.0000 - mae: 11799.8359 - val_loss: 11045.6317 - val_mse: 234849920.0000 - val_mae: 11045.6309\n",
            "Epoch 141/500\n",
            "90/90 [==============================] - 0s 729us/step - loss: 12427.8357 - mse: 289680128.0000 - mae: 12427.8359 - val_loss: 10956.0810 - val_mse: 231865072.0000 - val_mae: 10956.0811\n",
            "Epoch 142/500\n",
            "90/90 [==============================] - 0s 778us/step - loss: 11261.8946 - mse: 244292896.0000 - mae: 11261.8955 - val_loss: 10824.9275 - val_mse: 227396768.0000 - val_mae: 10824.9277\n",
            "Epoch 143/500\n",
            "90/90 [==============================] - 0s 779us/step - loss: 12808.6838 - mse: 303459168.0000 - mae: 12808.6846 - val_loss: 10740.2318 - val_mse: 224901664.0000 - val_mae: 10740.2324\n",
            "Epoch 144/500\n",
            "90/90 [==============================] - 0s 744us/step - loss: 12252.0660 - mse: 271233568.0000 - mae: 12252.0664 - val_loss: 10620.5690 - val_mse: 221102880.0000 - val_mae: 10620.5684\n",
            "Epoch 145/500\n",
            "90/90 [==============================] - 0s 789us/step - loss: 12323.4028 - mse: 280133760.0000 - mae: 12323.4023 - val_loss: 10526.0624 - val_mse: 218162464.0000 - val_mae: 10526.0625\n",
            "Epoch 146/500\n",
            "90/90 [==============================] - 0s 747us/step - loss: 12214.3464 - mse: 277421408.0000 - mae: 12214.3477 - val_loss: 10440.1583 - val_mse: 216397552.0000 - val_mae: 10440.1572\n",
            "Epoch 147/500\n",
            "90/90 [==============================] - 0s 630us/step - loss: 13076.4927 - mse: 287253440.0000 - mae: 13076.4932 - val_loss: 10448.3130 - val_mse: 215257360.0000 - val_mae: 10448.3135\n",
            "Epoch 148/500\n",
            "90/90 [==============================] - 0s 799us/step - loss: 11328.1729 - mse: 240001312.0000 - mae: 11328.1719 - val_loss: 10386.1722 - val_mse: 212538160.0000 - val_mae: 10386.1719\n",
            "Epoch 149/500\n",
            "90/90 [==============================] - 0s 650us/step - loss: 12158.0572 - mse: 271337184.0000 - mae: 12158.0566 - val_loss: 10318.1208 - val_mse: 210202608.0000 - val_mae: 10318.1211\n",
            "Epoch 150/500\n",
            "90/90 [==============================] - 0s 742us/step - loss: 11773.5364 - mse: 233688160.0000 - mae: 11773.5361 - val_loss: 10309.3467 - val_mse: 208822112.0000 - val_mae: 10309.3467\n",
            "Epoch 151/500\n",
            "90/90 [==============================] - 0s 785us/step - loss: 12439.1828 - mse: 258404192.0000 - mae: 12439.1836 - val_loss: 10224.5859 - val_mse: 205733872.0000 - val_mae: 10224.5859\n",
            "Epoch 152/500\n",
            "90/90 [==============================] - 0s 850us/step - loss: 11562.8086 - mse: 239730144.0000 - mae: 11562.8086 - val_loss: 10097.2169 - val_mse: 202434704.0000 - val_mae: 10097.2168\n",
            "Epoch 153/500\n",
            "90/90 [==============================] - 0s 770us/step - loss: 10965.1881 - mse: 218911008.0000 - mae: 10965.1885 - val_loss: 10109.0357 - val_mse: 200657184.0000 - val_mae: 10109.0352\n",
            "Epoch 154/500\n",
            "90/90 [==============================] - 0s 779us/step - loss: 10879.8322 - mse: 212260720.0000 - mae: 10879.8330 - val_loss: 10080.7536 - val_mse: 198970896.0000 - val_mae: 10080.7539\n",
            "Epoch 155/500\n",
            "90/90 [==============================] - 0s 749us/step - loss: 11912.6776 - mse: 247451264.0000 - mae: 11912.6777 - val_loss: 10058.2321 - val_mse: 197494416.0000 - val_mae: 10058.2324\n",
            "Epoch 156/500\n",
            "90/90 [==============================] - 0s 707us/step - loss: 12933.4348 - mse: 271126208.0000 - mae: 12933.4365 - val_loss: 9966.1115 - val_mse: 194723264.0000 - val_mae: 9966.1113\n",
            "Epoch 157/500\n",
            "90/90 [==============================] - 0s 702us/step - loss: 11511.5002 - mse: 235017200.0000 - mae: 11511.5010 - val_loss: 9891.0613 - val_mse: 192794288.0000 - val_mae: 9891.0625\n",
            "Epoch 158/500\n",
            "90/90 [==============================] - 0s 725us/step - loss: 12150.5567 - mse: 266099664.0000 - mae: 12150.5566 - val_loss: 9914.3536 - val_mse: 192047424.0000 - val_mae: 9914.3535\n",
            "Epoch 159/500\n",
            "90/90 [==============================] - 0s 768us/step - loss: 12116.8872 - mse: 250551040.0000 - mae: 12116.8857 - val_loss: 9843.0552 - val_mse: 190096976.0000 - val_mae: 9843.0537\n",
            "Epoch 160/500\n",
            "90/90 [==============================] - 0s 773us/step - loss: 11441.3996 - mse: 229143184.0000 - mae: 11441.4004 - val_loss: 9825.2692 - val_mse: 188391488.0000 - val_mae: 9825.2686\n",
            "Epoch 161/500\n",
            "90/90 [==============================] - 0s 777us/step - loss: 11543.1867 - mse: 239277200.0000 - mae: 11543.1875 - val_loss: 9670.6517 - val_mse: 184891808.0000 - val_mae: 9670.6523\n",
            "Epoch 162/500\n",
            "90/90 [==============================] - 0s 795us/step - loss: 10746.2829 - mse: 214157568.0000 - mae: 10746.2822 - val_loss: 9611.4573 - val_mse: 183059008.0000 - val_mae: 9611.4580\n",
            "Epoch 163/500\n",
            "90/90 [==============================] - 0s 723us/step - loss: 11786.2069 - mse: 250155552.0000 - mae: 11786.2080 - val_loss: 9514.0963 - val_mse: 180931840.0000 - val_mae: 9514.0957\n",
            "Epoch 164/500\n",
            "90/90 [==============================] - 0s 678us/step - loss: 10851.8181 - mse: 208603776.0000 - mae: 10851.8184 - val_loss: 9406.4368 - val_mse: 178145936.0000 - val_mae: 9406.4365\n",
            "Epoch 165/500\n",
            "90/90 [==============================] - 0s 945us/step - loss: 12488.4217 - mse: 253109584.0000 - mae: 12488.4219 - val_loss: 9377.7841 - val_mse: 177027216.0000 - val_mae: 9377.7842\n",
            "Epoch 166/500\n",
            "90/90 [==============================] - 0s 722us/step - loss: 12744.4602 - mse: 272515552.0000 - mae: 12744.4609 - val_loss: 9365.8783 - val_mse: 175724912.0000 - val_mae: 9365.8779\n",
            "Epoch 167/500\n",
            "90/90 [==============================] - 0s 677us/step - loss: 11048.4641 - mse: 225307040.0000 - mae: 11048.4639 - val_loss: 9166.9272 - val_mse: 171864992.0000 - val_mae: 9166.9268\n",
            "Epoch 168/500\n",
            "90/90 [==============================] - 0s 693us/step - loss: 11337.7561 - mse: 201806672.0000 - mae: 11337.7559 - val_loss: 9106.9984 - val_mse: 169421328.0000 - val_mae: 9106.9980\n",
            "Epoch 169/500\n",
            "90/90 [==============================] - 0s 778us/step - loss: 13000.9689 - mse: 267478304.0000 - mae: 13000.9678 - val_loss: 9057.7305 - val_mse: 167865632.0000 - val_mae: 9057.7295\n",
            "Epoch 170/500\n",
            "90/90 [==============================] - 0s 715us/step - loss: 10281.2899 - mse: 184313696.0000 - mae: 10281.2900 - val_loss: 9119.2672 - val_mse: 168196240.0000 - val_mae: 9119.2676\n",
            "Epoch 171/500\n",
            "90/90 [==============================] - 0s 804us/step - loss: 12341.1214 - mse: 276450752.0000 - mae: 12341.1221 - val_loss: 9076.6102 - val_mse: 166996752.0000 - val_mae: 9076.6104\n",
            "Epoch 172/500\n",
            "90/90 [==============================] - 0s 711us/step - loss: 10518.3302 - mse: 205253424.0000 - mae: 10518.3301 - val_loss: 9207.8199 - val_mse: 166492832.0000 - val_mae: 9207.8213\n",
            "Epoch 173/500\n",
            "90/90 [==============================] - 0s 699us/step - loss: 11344.4890 - mse: 225182272.0000 - mae: 11344.4893 - val_loss: 9004.5974 - val_mse: 163393072.0000 - val_mae: 9004.5977\n",
            "Epoch 174/500\n",
            "90/90 [==============================] - 0s 719us/step - loss: 11568.3692 - mse: 219974592.0000 - mae: 11568.3691 - val_loss: 9006.3041 - val_mse: 163049456.0000 - val_mae: 9006.3037\n",
            "Epoch 175/500\n",
            "90/90 [==============================] - 0s 746us/step - loss: 11737.8006 - mse: 230258784.0000 - mae: 11737.8018 - val_loss: 8969.4202 - val_mse: 161821888.0000 - val_mae: 8969.4199\n",
            "Epoch 176/500\n",
            "90/90 [==============================] - 0s 726us/step - loss: 11304.8062 - mse: 221636176.0000 - mae: 11304.8057 - val_loss: 8953.0657 - val_mse: 160892816.0000 - val_mae: 8953.0664\n",
            "Epoch 177/500\n",
            "90/90 [==============================] - 0s 731us/step - loss: 11650.8516 - mse: 238055440.0000 - mae: 11650.8516 - val_loss: 8923.6462 - val_mse: 159643616.0000 - val_mae: 8923.6465\n",
            "Epoch 178/500\n",
            "90/90 [==============================] - 0s 792us/step - loss: 11952.1989 - mse: 248012256.0000 - mae: 11952.1982 - val_loss: 8861.4763 - val_mse: 158373440.0000 - val_mae: 8861.4756\n",
            "Epoch 179/500\n",
            "90/90 [==============================] - 0s 769us/step - loss: 11241.3792 - mse: 213591856.0000 - mae: 11241.3789 - val_loss: 8970.8811 - val_mse: 158334784.0000 - val_mae: 8970.8809\n",
            "Epoch 180/500\n",
            "90/90 [==============================] - 0s 746us/step - loss: 11185.1296 - mse: 227922576.0000 - mae: 11185.1289 - val_loss: 9132.2085 - val_mse: 157716944.0000 - val_mae: 9132.2080\n",
            "Epoch 181/500\n",
            "90/90 [==============================] - 0s 771us/step - loss: 10954.4739 - mse: 199689008.0000 - mae: 10954.4746 - val_loss: 8839.5287 - val_mse: 154835168.0000 - val_mae: 8839.5283\n",
            "Epoch 182/500\n",
            "90/90 [==============================] - 0s 741us/step - loss: 12957.7965 - mse: 287857952.0000 - mae: 12957.7959 - val_loss: 8759.1122 - val_mse: 153537232.0000 - val_mae: 8759.1123\n",
            "Epoch 183/500\n",
            "90/90 [==============================] - 0s 706us/step - loss: 11066.5141 - mse: 205591136.0000 - mae: 11066.5137 - val_loss: 8857.7992 - val_mse: 153354608.0000 - val_mae: 8857.7988\n",
            "Epoch 184/500\n",
            "90/90 [==============================] - 0s 764us/step - loss: 12321.4309 - mse: 256500464.0000 - mae: 12321.4307 - val_loss: 8694.1528 - val_mse: 151520240.0000 - val_mae: 8694.1523\n",
            "Epoch 185/500\n",
            "90/90 [==============================] - 0s 854us/step - loss: 12131.4739 - mse: 239329440.0000 - mae: 12131.4736 - val_loss: 8728.8516 - val_mse: 150645024.0000 - val_mae: 8728.8525\n",
            "Epoch 186/500\n",
            "90/90 [==============================] - 0s 709us/step - loss: 10937.6314 - mse: 228710880.0000 - mae: 10937.6309 - val_loss: 8507.8491 - val_mse: 148091376.0000 - val_mae: 8507.8496\n",
            "Epoch 187/500\n",
            "90/90 [==============================] - 0s 771us/step - loss: 10766.8385 - mse: 208762816.0000 - mae: 10766.8389 - val_loss: 8493.6704 - val_mse: 147328480.0000 - val_mae: 8493.6699\n",
            "Epoch 188/500\n",
            "90/90 [==============================] - 0s 740us/step - loss: 11224.9339 - mse: 203180208.0000 - mae: 11224.9346 - val_loss: 8432.5348 - val_mse: 145783936.0000 - val_mae: 8432.5352\n",
            "Epoch 189/500\n",
            "90/90 [==============================] - 0s 689us/step - loss: 11273.7201 - mse: 234853856.0000 - mae: 11273.7197 - val_loss: 8332.9339 - val_mse: 144137824.0000 - val_mae: 8332.9346\n",
            "Epoch 190/500\n",
            "90/90 [==============================] - 0s 752us/step - loss: 11378.2015 - mse: 225555088.0000 - mae: 11378.2021 - val_loss: 8363.4682 - val_mse: 144116992.0000 - val_mae: 8363.4678\n",
            "Epoch 191/500\n",
            "90/90 [==============================] - 0s 789us/step - loss: 10756.2384 - mse: 222168272.0000 - mae: 10756.2393 - val_loss: 8319.4251 - val_mse: 143140384.0000 - val_mae: 8319.4258\n",
            "Epoch 192/500\n",
            "90/90 [==============================] - 0s 777us/step - loss: 10977.7620 - mse: 208330224.0000 - mae: 10977.7627 - val_loss: 8342.4773 - val_mse: 143111552.0000 - val_mae: 8342.4775\n",
            "Epoch 193/500\n",
            "90/90 [==============================] - 0s 810us/step - loss: 11236.9921 - mse: 204529344.0000 - mae: 11236.9912 - val_loss: 8251.3985 - val_mse: 141393952.0000 - val_mae: 8251.3984\n",
            "Epoch 194/500\n",
            "90/90 [==============================] - 0s 719us/step - loss: 11024.3874 - mse: 216064320.0000 - mae: 11024.3877 - val_loss: 8310.6011 - val_mse: 141252448.0000 - val_mae: 8310.6016\n",
            "Epoch 195/500\n",
            "90/90 [==============================] - 0s 769us/step - loss: 10828.5552 - mse: 206607888.0000 - mae: 10828.5557 - val_loss: 8313.1608 - val_mse: 140647792.0000 - val_mae: 8313.1611\n",
            "Epoch 196/500\n",
            "90/90 [==============================] - 0s 734us/step - loss: 11187.1656 - mse: 215486784.0000 - mae: 11187.1660 - val_loss: 8299.9757 - val_mse: 139939072.0000 - val_mae: 8299.9756\n",
            "Epoch 197/500\n",
            "90/90 [==============================] - 0s 734us/step - loss: 11277.3204 - mse: 232701968.0000 - mae: 11277.3203 - val_loss: 8196.5859 - val_mse: 138631504.0000 - val_mae: 8196.5859\n",
            "Epoch 198/500\n",
            "90/90 [==============================] - 0s 771us/step - loss: 11299.4992 - mse: 209933952.0000 - mae: 11299.4990 - val_loss: 8237.3192 - val_mse: 139157280.0000 - val_mae: 8237.3193\n",
            "Epoch 199/500\n",
            "90/90 [==============================] - 0s 733us/step - loss: 11854.6203 - mse: 230971232.0000 - mae: 11854.6211 - val_loss: 8265.2734 - val_mse: 139385920.0000 - val_mae: 8265.2725\n",
            "Epoch 200/500\n",
            "90/90 [==============================] - 0s 728us/step - loss: 11863.2035 - mse: 217518576.0000 - mae: 11863.2031 - val_loss: 8363.4583 - val_mse: 140227088.0000 - val_mae: 8363.4580\n",
            "Epoch 201/500\n",
            "90/90 [==============================] - 0s 692us/step - loss: 11760.1295 - mse: 213041536.0000 - mae: 11760.1289 - val_loss: 8324.1274 - val_mse: 139667824.0000 - val_mae: 8324.1270\n",
            "Epoch 202/500\n",
            "90/90 [==============================] - 0s 786us/step - loss: 10853.9836 - mse: 195163840.0000 - mae: 10853.9834 - val_loss: 8421.9516 - val_mse: 140077008.0000 - val_mae: 8421.9512\n",
            "Epoch 203/500\n",
            "90/90 [==============================] - 0s 751us/step - loss: 10898.9140 - mse: 188553808.0000 - mae: 10898.9141 - val_loss: 8497.9473 - val_mse: 141015104.0000 - val_mae: 8497.9473\n",
            "Epoch 204/500\n",
            "90/90 [==============================] - 0s 738us/step - loss: 9547.2912 - mse: 169797824.0000 - mae: 9547.2920 - val_loss: 8398.4551 - val_mse: 140221504.0000 - val_mae: 8398.4551\n",
            "Epoch 205/500\n",
            "90/90 [==============================] - 0s 785us/step - loss: 11130.2450 - mse: 199857952.0000 - mae: 11130.2441 - val_loss: 8308.5712 - val_mse: 138777344.0000 - val_mae: 8308.5703\n",
            "Epoch 206/500\n",
            "90/90 [==============================] - 0s 769us/step - loss: 10490.2306 - mse: 202876176.0000 - mae: 10490.2314 - val_loss: 8296.0540 - val_mse: 138322848.0000 - val_mae: 8296.0537\n",
            "Epoch 207/500\n",
            "90/90 [==============================] - 0s 799us/step - loss: 10181.5470 - mse: 175546192.0000 - mae: 10181.5469 - val_loss: 8228.8929 - val_mse: 137357488.0000 - val_mae: 8228.8926\n",
            "Epoch 208/500\n",
            "90/90 [==============================] - 0s 834us/step - loss: 10859.1606 - mse: 198527184.0000 - mae: 10859.1602 - val_loss: 8285.4493 - val_mse: 137750976.0000 - val_mae: 8285.4492\n",
            "Epoch 209/500\n",
            "90/90 [==============================] - 0s 771us/step - loss: 9548.7326 - mse: 142618128.0000 - mae: 9548.7324 - val_loss: 8269.5221 - val_mse: 137319872.0000 - val_mae: 8269.5225\n",
            "Epoch 210/500\n",
            "90/90 [==============================] - 0s 813us/step - loss: 12120.9788 - mse: 231223088.0000 - mae: 12120.9795 - val_loss: 8240.5358 - val_mse: 136706288.0000 - val_mae: 8240.5361\n",
            "Epoch 211/500\n",
            "90/90 [==============================] - 0s 846us/step - loss: 10352.4290 - mse: 191974816.0000 - mae: 10352.4287 - val_loss: 8182.7822 - val_mse: 135899744.0000 - val_mae: 8182.7827\n",
            "Epoch 212/500\n",
            "90/90 [==============================] - 0s 807us/step - loss: 10196.2475 - mse: 180828736.0000 - mae: 10196.2471 - val_loss: 8188.6275 - val_mse: 135787952.0000 - val_mae: 8188.6284\n",
            "Epoch 213/500\n",
            "90/90 [==============================] - 0s 821us/step - loss: 11273.5583 - mse: 232147808.0000 - mae: 11273.5586 - val_loss: 8119.4188 - val_mse: 134685408.0000 - val_mae: 8119.4189\n",
            "Epoch 214/500\n",
            "90/90 [==============================] - 0s 782us/step - loss: 10288.8765 - mse: 179696496.0000 - mae: 10288.8760 - val_loss: 8177.3085 - val_mse: 135406032.0000 - val_mae: 8177.3081\n",
            "Epoch 215/500\n",
            "90/90 [==============================] - 0s 839us/step - loss: 9765.2753 - mse: 166786464.0000 - mae: 9765.2754 - val_loss: 8154.4247 - val_mse: 134495680.0000 - val_mae: 8154.4243\n",
            "Epoch 216/500\n",
            "90/90 [==============================] - 0s 709us/step - loss: 10031.4036 - mse: 173364992.0000 - mae: 10031.4023 - val_loss: 8347.7485 - val_mse: 136132672.0000 - val_mae: 8347.7480\n",
            "Epoch 217/500\n",
            "90/90 [==============================] - 0s 722us/step - loss: 10431.0094 - mse: 193633936.0000 - mae: 10431.0098 - val_loss: 8163.7107 - val_mse: 133766576.0000 - val_mae: 8163.7100\n",
            "Epoch 218/500\n",
            "90/90 [==============================] - 0s 840us/step - loss: 9668.7327 - mse: 178475616.0000 - mae: 9668.7324 - val_loss: 8248.9407 - val_mse: 134359136.0000 - val_mae: 8248.9414\n",
            "Epoch 219/500\n",
            "90/90 [==============================] - 0s 719us/step - loss: 10161.1415 - mse: 165784528.0000 - mae: 10161.1426 - val_loss: 8155.7194 - val_mse: 133394432.0000 - val_mae: 8155.7192\n",
            "Epoch 220/500\n",
            "90/90 [==============================] - 0s 741us/step - loss: 10096.1379 - mse: 186164720.0000 - mae: 10096.1387 - val_loss: 8233.6096 - val_mse: 133546928.0000 - val_mae: 8233.6094\n",
            "Epoch 221/500\n",
            "90/90 [==============================] - 0s 901us/step - loss: 10994.9284 - mse: 218068256.0000 - mae: 10994.9277 - val_loss: 8264.8188 - val_mse: 133178984.0000 - val_mae: 8264.8193\n",
            "Epoch 222/500\n",
            "90/90 [==============================] - 0s 728us/step - loss: 10922.8071 - mse: 198046272.0000 - mae: 10922.8076 - val_loss: 8199.5852 - val_mse: 132224720.0000 - val_mae: 8199.5850\n",
            "Epoch 223/500\n",
            "90/90 [==============================] - 0s 773us/step - loss: 10303.8862 - mse: 184726560.0000 - mae: 10303.8857 - val_loss: 8254.8367 - val_mse: 132507728.0000 - val_mae: 8254.8359\n",
            "Epoch 224/500\n",
            "90/90 [==============================] - 0s 672us/step - loss: 11643.5059 - mse: 210629472.0000 - mae: 11643.5059 - val_loss: 8172.1533 - val_mse: 131826952.0000 - val_mae: 8172.1543\n",
            "Epoch 225/500\n",
            "90/90 [==============================] - 0s 739us/step - loss: 10838.9403 - mse: 199405184.0000 - mae: 10838.9414 - val_loss: 8056.9098 - val_mse: 130589152.0000 - val_mae: 8056.9106\n",
            "Epoch 226/500\n",
            "90/90 [==============================] - 0s 946us/step - loss: 9002.7942 - mse: 158914784.0000 - mae: 9002.7949 - val_loss: 8022.6950 - val_mse: 129640888.0000 - val_mae: 8022.6943\n",
            "Epoch 227/500\n",
            "90/90 [==============================] - 0s 705us/step - loss: 10659.0991 - mse: 196693536.0000 - mae: 10659.0996 - val_loss: 8054.8452 - val_mse: 129967608.0000 - val_mae: 8054.8442\n",
            "Epoch 228/500\n",
            "90/90 [==============================] - 0s 693us/step - loss: 10060.0700 - mse: 176027472.0000 - mae: 10060.0703 - val_loss: 8141.8191 - val_mse: 130517640.0000 - val_mae: 8141.8179\n",
            "Epoch 229/500\n",
            "90/90 [==============================] - 0s 770us/step - loss: 10638.7016 - mse: 190168000.0000 - mae: 10638.7012 - val_loss: 8063.5578 - val_mse: 129488568.0000 - val_mae: 8063.5571\n",
            "Epoch 230/500\n",
            "90/90 [==============================] - 0s 703us/step - loss: 10615.9495 - mse: 204249728.0000 - mae: 10615.9502 - val_loss: 8028.2222 - val_mse: 128574176.0000 - val_mae: 8028.2222\n",
            "Epoch 231/500\n",
            "90/90 [==============================] - 0s 797us/step - loss: 8568.4355 - mse: 120645576.0000 - mae: 8568.4346 - val_loss: 8003.1063 - val_mse: 127965480.0000 - val_mae: 8003.1064\n",
            "Epoch 232/500\n",
            "90/90 [==============================] - 0s 741us/step - loss: 10246.0117 - mse: 174937888.0000 - mae: 10246.0117 - val_loss: 7951.9254 - val_mse: 126973496.0000 - val_mae: 7951.9248\n",
            "Epoch 233/500\n",
            "90/90 [==============================] - 0s 781us/step - loss: 9540.5696 - mse: 162584832.0000 - mae: 9540.5693 - val_loss: 7889.4602 - val_mse: 125870008.0000 - val_mae: 7889.4604\n",
            "Epoch 234/500\n",
            "90/90 [==============================] - 0s 788us/step - loss: 10981.5519 - mse: 209276832.0000 - mae: 10981.5518 - val_loss: 7919.1838 - val_mse: 126234160.0000 - val_mae: 7919.1831\n",
            "Epoch 235/500\n",
            "90/90 [==============================] - 0s 765us/step - loss: 10962.4669 - mse: 194933584.0000 - mae: 10962.4678 - val_loss: 7948.7919 - val_mse: 126608736.0000 - val_mae: 7948.7915\n",
            "Epoch 236/500\n",
            "90/90 [==============================] - 0s 983us/step - loss: 11148.9509 - mse: 230563936.0000 - mae: 11148.9512 - val_loss: 8024.8596 - val_mse: 127195512.0000 - val_mae: 8024.8589\n",
            "Epoch 237/500\n",
            "90/90 [==============================] - 0s 772us/step - loss: 9765.4690 - mse: 164760320.0000 - mae: 9765.4697 - val_loss: 8150.3419 - val_mse: 128280784.0000 - val_mae: 8150.3418\n",
            "Epoch 238/500\n",
            "90/90 [==============================] - 0s 722us/step - loss: 10532.9466 - mse: 194426608.0000 - mae: 10532.9463 - val_loss: 8152.1183 - val_mse: 128403568.0000 - val_mae: 8152.1182\n",
            "Epoch 239/500\n",
            "90/90 [==============================] - 0s 692us/step - loss: 10767.9170 - mse: 199837632.0000 - mae: 10767.9170 - val_loss: 8057.5791 - val_mse: 126994896.0000 - val_mae: 8057.5791\n",
            "Epoch 240/500\n",
            "90/90 [==============================] - 0s 780us/step - loss: 10769.4811 - mse: 191665680.0000 - mae: 10769.4814 - val_loss: 7981.7870 - val_mse: 126176544.0000 - val_mae: 7981.7876\n",
            "Epoch 241/500\n",
            "90/90 [==============================] - 0s 697us/step - loss: 11083.2944 - mse: 208796256.0000 - mae: 11083.2930 - val_loss: 8025.1980 - val_mse: 126657616.0000 - val_mae: 8025.1978\n",
            "Epoch 242/500\n",
            "90/90 [==============================] - 0s 690us/step - loss: 9356.9236 - mse: 151301888.0000 - mae: 9356.9229 - val_loss: 7981.8703 - val_mse: 125728528.0000 - val_mae: 7981.8696\n",
            "Epoch 243/500\n",
            "90/90 [==============================] - 0s 908us/step - loss: 9228.5874 - mse: 151406096.0000 - mae: 9228.5879 - val_loss: 7934.7866 - val_mse: 124964272.0000 - val_mae: 7934.7861\n",
            "Epoch 244/500\n",
            "90/90 [==============================] - 0s 827us/step - loss: 10108.3439 - mse: 179081872.0000 - mae: 10108.3438 - val_loss: 7988.5917 - val_mse: 125600176.0000 - val_mae: 7988.5918\n",
            "Epoch 245/500\n",
            "90/90 [==============================] - 0s 775us/step - loss: 12468.3729 - mse: 274978304.0000 - mae: 12468.3721 - val_loss: 7965.6750 - val_mse: 125107688.0000 - val_mae: 7965.6748\n",
            "Epoch 246/500\n",
            "90/90 [==============================] - 0s 843us/step - loss: 10523.4361 - mse: 189891808.0000 - mae: 10523.4365 - val_loss: 8012.1711 - val_mse: 126059312.0000 - val_mae: 8012.1724\n",
            "Epoch 247/500\n",
            "90/90 [==============================] - 0s 789us/step - loss: 10149.7569 - mse: 186355776.0000 - mae: 10149.7568 - val_loss: 7943.0445 - val_mse: 125347192.0000 - val_mae: 7943.0439\n",
            "Epoch 248/500\n",
            "90/90 [==============================] - 0s 791us/step - loss: 11097.6815 - mse: 203345488.0000 - mae: 11097.6816 - val_loss: 7983.7718 - val_mse: 126169352.0000 - val_mae: 7983.7715\n",
            "Epoch 249/500\n",
            "90/90 [==============================] - 0s 787us/step - loss: 10665.5407 - mse: 190490960.0000 - mae: 10665.5410 - val_loss: 8013.3833 - val_mse: 126421944.0000 - val_mae: 8013.3833\n",
            "Epoch 250/500\n",
            "90/90 [==============================] - 0s 751us/step - loss: 10437.7618 - mse: 201743408.0000 - mae: 10437.7617 - val_loss: 8008.1303 - val_mse: 125886864.0000 - val_mae: 8008.1304\n",
            "Epoch 251/500\n",
            "90/90 [==============================] - 0s 714us/step - loss: 11563.6234 - mse: 224945328.0000 - mae: 11563.6230 - val_loss: 8015.9473 - val_mse: 125981136.0000 - val_mae: 8015.9473\n",
            "Epoch 252/500\n",
            "90/90 [==============================] - 0s 728us/step - loss: 10478.9551 - mse: 170605632.0000 - mae: 10478.9551 - val_loss: 8086.0711 - val_mse: 126874488.0000 - val_mae: 8086.0713\n",
            "Epoch 253/500\n",
            "90/90 [==============================] - 0s 747us/step - loss: 9608.4200 - mse: 149013408.0000 - mae: 9608.4199 - val_loss: 8047.5859 - val_mse: 126073208.0000 - val_mae: 8047.5859\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 20 samples, validate on 10 samples\n",
            "Epoch 1/500\n",
            "20/20 [==============================] - 2s 89ms/step - loss: 738.9727 - mse: 829283.1250 - mae: 738.9728 - val_loss: 711.0131 - val_mse: 527260.3750 - val_mae: 711.0131\n",
            "Epoch 2/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 738.9482 - mse: 829246.5000 - mae: 738.9482 - val_loss: 710.9930 - val_mse: 527232.0000 - val_mae: 710.9930\n",
            "Epoch 3/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 738.9283 - mse: 829218.6250 - mae: 738.9283 - val_loss: 710.9742 - val_mse: 527205.3750 - val_mae: 710.9742\n",
            "Epoch 4/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 738.9111 - mse: 829190.8750 - mae: 738.9111 - val_loss: 710.9550 - val_mse: 527178.3125 - val_mae: 710.9550\n",
            "Epoch 5/500\n",
            "20/20 [==============================] - 0s 964us/step - loss: 738.8858 - mse: 829157.5000 - mae: 738.8859 - val_loss: 710.9313 - val_mse: 527145.3750 - val_mae: 710.9313\n",
            "Epoch 6/500\n",
            "20/20 [==============================] - 0s 968us/step - loss: 738.8515 - mse: 829118.3125 - mae: 738.8514 - val_loss: 710.9004 - val_mse: 527103.3750 - val_mae: 710.9004\n",
            "Epoch 7/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 738.7740 - mse: 829053.0000 - mae: 738.7739 - val_loss: 710.8492 - val_mse: 527036.6250 - val_mae: 710.8492\n",
            "Epoch 8/500\n",
            "20/20 [==============================] - 0s 972us/step - loss: 738.5669 - mse: 828926.1250 - mae: 738.5670 - val_loss: 710.7416 - val_mse: 526903.5000 - val_mae: 710.7416\n",
            "Epoch 9/500\n",
            "20/20 [==============================] - 0s 952us/step - loss: 737.9808 - mse: 828638.1250 - mae: 737.9808 - val_loss: 710.4939 - val_mse: 526609.1875 - val_mae: 710.4939\n",
            "Epoch 10/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 736.8573 - mse: 828077.6250 - mae: 736.8573 - val_loss: 709.8987 - val_mse: 525907.3750 - val_mae: 709.8987\n",
            "Epoch 11/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 733.9170 - mse: 826691.2500 - mae: 733.9169 - val_loss: 708.6935 - val_mse: 524491.0000 - val_mae: 708.6935\n",
            "Epoch 12/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 732.9564 - mse: 826111.8750 - mae: 732.9564 - val_loss: 707.2773 - val_mse: 522816.3438 - val_mae: 707.2773\n",
            "Epoch 13/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 731.0559 - mse: 824437.1875 - mae: 731.0560 - val_loss: 705.6395 - val_mse: 520867.7500 - val_mae: 705.6395\n",
            "Epoch 14/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 728.3626 - mse: 823063.1250 - mae: 728.3627 - val_loss: 703.6943 - val_mse: 518541.9375 - val_mae: 703.6943\n",
            "Epoch 15/500\n",
            "20/20 [==============================] - 0s 958us/step - loss: 719.4868 - mse: 817854.0000 - mae: 719.4868 - val_loss: 701.0325 - val_mse: 515357.0000 - val_mae: 701.0325\n",
            "Epoch 16/500\n",
            "20/20 [==============================] - 0s 971us/step - loss: 722.0445 - mse: 817671.3750 - mae: 722.0446 - val_loss: 698.9118 - val_mse: 512800.0625 - val_mae: 698.9118\n",
            "Epoch 17/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 718.5325 - mse: 814998.1875 - mae: 718.5325 - val_loss: 696.6456 - val_mse: 510000.3125 - val_mae: 696.6456\n",
            "Epoch 18/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 715.5711 - mse: 811889.5000 - mae: 715.5711 - val_loss: 694.1733 - val_mse: 506934.8438 - val_mae: 694.1733\n",
            "Epoch 19/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 707.2525 - mse: 809061.8750 - mae: 707.2525 - val_loss: 691.1443 - val_mse: 503239.3125 - val_mae: 691.1443\n",
            "Epoch 20/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 703.7200 - mse: 805359.0000 - mae: 703.7200 - val_loss: 688.0267 - val_mse: 499286.3125 - val_mae: 688.0267\n",
            "Epoch 21/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 702.8527 - mse: 799124.1250 - mae: 702.8527 - val_loss: 684.9149 - val_mse: 495281.6875 - val_mae: 684.9149\n",
            "Epoch 22/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 700.3430 - mse: 798365.6250 - mae: 700.3430 - val_loss: 681.7145 - val_mse: 491177.6875 - val_mae: 681.7145\n",
            "Epoch 23/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 688.6631 - mse: 789118.0000 - mae: 688.6631 - val_loss: 677.8415 - val_mse: 486241.3125 - val_mae: 677.8415\n",
            "Epoch 24/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 680.4584 - mse: 781973.4375 - mae: 680.4584 - val_loss: 674.8870 - val_mse: 482343.0000 - val_mae: 674.8870\n",
            "Epoch 25/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 679.8562 - mse: 779134.3750 - mae: 679.8561 - val_loss: 671.8688 - val_mse: 478317.9062 - val_mae: 671.8688\n",
            "Epoch 26/500\n",
            "20/20 [==============================] - 0s 933us/step - loss: 673.9323 - mse: 764196.8750 - mae: 673.9323 - val_loss: 667.6987 - val_mse: 472844.0938 - val_mae: 667.6987\n",
            "Epoch 27/500\n",
            "20/20 [==============================] - 0s 952us/step - loss: 682.1871 - mse: 768368.9375 - mae: 682.1871 - val_loss: 664.2069 - val_mse: 468239.5938 - val_mae: 664.2069\n",
            "Epoch 28/500\n",
            "20/20 [==============================] - 0s 913us/step - loss: 680.4856 - mse: 770112.1250 - mae: 680.4856 - val_loss: 661.3018 - val_mse: 464419.8125 - val_mae: 661.3018\n",
            "Epoch 29/500\n",
            "20/20 [==============================] - 0s 951us/step - loss: 664.9258 - mse: 762165.1250 - mae: 664.9258 - val_loss: 656.8541 - val_mse: 458704.6875 - val_mae: 656.8541\n",
            "Epoch 30/500\n",
            "20/20 [==============================] - 0s 999us/step - loss: 650.3391 - mse: 738149.8750 - mae: 650.3392 - val_loss: 652.2980 - val_mse: 452783.7500 - val_mae: 652.2980\n",
            "Epoch 31/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 653.0923 - mse: 737262.8750 - mae: 653.0923 - val_loss: 647.8683 - val_mse: 447018.9375 - val_mae: 647.8683\n",
            "Epoch 32/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 650.1655 - mse: 733981.1875 - mae: 650.1655 - val_loss: 642.6635 - val_mse: 440406.2500 - val_mae: 642.6635\n",
            "Epoch 33/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 633.9121 - mse: 720542.5000 - mae: 633.9120 - val_loss: 639.5999 - val_mse: 436297.0938 - val_mae: 639.5999\n",
            "Epoch 34/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 648.4035 - mse: 715202.6250 - mae: 648.4035 - val_loss: 634.9454 - val_mse: 430228.9062 - val_mae: 634.9454\n",
            "Epoch 35/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 639.0850 - mse: 713303.0000 - mae: 639.0850 - val_loss: 629.8485 - val_mse: 423684.4062 - val_mae: 629.8485\n",
            "Epoch 36/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 634.6960 - mse: 699145.6875 - mae: 634.6959 - val_loss: 625.9203 - val_mse: 418477.8125 - val_mae: 625.9203\n",
            "Epoch 37/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 623.5911 - mse: 682145.2500 - mae: 623.5911 - val_loss: 622.4240 - val_mse: 413782.0312 - val_mae: 622.4240\n",
            "Epoch 38/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 625.6576 - mse: 690212.8750 - mae: 625.6576 - val_loss: 617.5573 - val_mse: 407640.6875 - val_mae: 617.5573\n",
            "Epoch 39/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 630.7196 - mse: 683438.0000 - mae: 630.7196 - val_loss: 614.3846 - val_mse: 403406.7188 - val_mae: 614.3846\n",
            "Epoch 40/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 618.8098 - mse: 680609.1250 - mae: 618.8098 - val_loss: 610.0726 - val_mse: 397946.5312 - val_mae: 610.0726\n",
            "Epoch 41/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 621.1294 - mse: 676572.1875 - mae: 621.1294 - val_loss: 606.7848 - val_mse: 393743.6562 - val_mae: 606.7848\n",
            "Epoch 42/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 610.4187 - mse: 661227.6875 - mae: 610.4187 - val_loss: 603.9801 - val_mse: 389945.8125 - val_mae: 603.9801\n",
            "Epoch 43/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 616.5099 - mse: 659269.0000 - mae: 616.5099 - val_loss: 598.7201 - val_mse: 383369.4375 - val_mae: 598.7201\n",
            "Epoch 44/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 615.2654 - mse: 668321.8125 - mae: 615.2653 - val_loss: 594.2706 - val_mse: 377859.2812 - val_mae: 594.2706\n",
            "Epoch 45/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 607.9020 - mse: 637747.8750 - mae: 607.9020 - val_loss: 588.9010 - val_mse: 371198.8125 - val_mae: 588.9010\n",
            "Epoch 46/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 595.6122 - mse: 637174.8125 - mae: 595.6122 - val_loss: 584.6143 - val_mse: 365865.2188 - val_mae: 584.6143\n",
            "Epoch 47/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 602.2063 - mse: 623483.6875 - mae: 602.2062 - val_loss: 582.3075 - val_mse: 362534.3125 - val_mae: 582.3075\n",
            "Epoch 48/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 595.0905 - mse: 609951.3750 - mae: 595.0905 - val_loss: 578.3219 - val_mse: 357475.2188 - val_mae: 578.3219\n",
            "Epoch 49/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 568.5003 - mse: 596973.5000 - mae: 568.5003 - val_loss: 574.6506 - val_mse: 352759.7500 - val_mae: 574.6506\n",
            "Epoch 50/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 576.8063 - mse: 598993.1250 - mae: 576.8063 - val_loss: 571.5028 - val_mse: 348603.0625 - val_mae: 571.5028\n",
            "Epoch 51/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 583.1544 - mse: 606563.0000 - mae: 583.1544 - val_loss: 568.7697 - val_mse: 344912.8438 - val_mae: 568.7697\n",
            "Epoch 52/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 606.3838 - mse: 617998.4375 - mae: 606.3838 - val_loss: 567.7886 - val_mse: 342945.8125 - val_mae: 567.7886\n",
            "Epoch 53/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 582.8812 - mse: 573836.8750 - mae: 582.8812 - val_loss: 567.6300 - val_mse: 341732.2812 - val_mae: 567.6300\n",
            "Epoch 54/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 582.6108 - mse: 592877.8125 - mae: 582.6108 - val_loss: 570.6867 - val_mse: 343856.1250 - val_mae: 570.6867\n",
            "Epoch 55/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 575.3017 - mse: 576437.3750 - mae: 575.3017 - val_loss: 566.9855 - val_mse: 339388.2500 - val_mae: 566.9855\n",
            "Epoch 56/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 568.4725 - mse: 592308.8125 - mae: 568.4725 - val_loss: 578.0284 - val_mse: 349981.5625 - val_mae: 578.0284\n",
            "Epoch 57/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 562.5280 - mse: 577397.0000 - mae: 562.5280 - val_loss: 575.2421 - val_mse: 346350.6250 - val_mae: 575.2421\n",
            "Epoch 58/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 551.3070 - mse: 550074.6250 - mae: 551.3070 - val_loss: 569.6583 - val_mse: 339780.2500 - val_mae: 569.6583\n",
            "Epoch 59/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 546.6015 - mse: 558285.3125 - mae: 546.6015 - val_loss: 565.8426 - val_mse: 335029.7500 - val_mae: 565.8426\n",
            "Epoch 60/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 560.7084 - mse: 559175.1875 - mae: 560.7084 - val_loss: 561.9866 - val_mse: 330323.0625 - val_mae: 561.9866\n",
            "Epoch 61/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 552.0870 - mse: 546317.0000 - mae: 552.0870 - val_loss: 552.0267 - val_mse: 319565.4688 - val_mae: 552.0267\n",
            "Epoch 62/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 548.0628 - mse: 531961.4375 - mae: 548.0627 - val_loss: 556.9557 - val_mse: 323891.1562 - val_mae: 556.9557\n",
            "Epoch 63/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 507.4135 - mse: 477954.4062 - mae: 507.4135 - val_loss: 548.8232 - val_mse: 314850.5625 - val_mae: 548.8232\n",
            "Epoch 64/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 528.6159 - mse: 523196.8125 - mae: 528.6159 - val_loss: 546.9120 - val_mse: 312310.4062 - val_mae: 546.9120\n",
            "Epoch 65/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 547.1860 - mse: 534071.1250 - mae: 547.1860 - val_loss: 544.6095 - val_mse: 309541.1875 - val_mae: 544.6095\n",
            "Epoch 66/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 542.0584 - mse: 515738.5938 - mae: 542.0585 - val_loss: 545.1688 - val_mse: 309655.4062 - val_mae: 545.1688\n",
            "Epoch 67/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 514.9208 - mse: 475625.1562 - mae: 514.9208 - val_loss: 539.6560 - val_mse: 303392.0312 - val_mae: 539.6560\n",
            "Epoch 68/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 514.7958 - mse: 492094.9375 - mae: 514.7958 - val_loss: 539.5893 - val_mse: 302744.6250 - val_mae: 539.5893\n",
            "Epoch 69/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 509.4235 - mse: 488013.8438 - mae: 509.4235 - val_loss: 534.2219 - val_mse: 296660.4062 - val_mae: 534.2219\n",
            "Epoch 70/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 522.8658 - mse: 513387.6875 - mae: 522.8658 - val_loss: 540.7383 - val_mse: 302908.0000 - val_mae: 540.7383\n",
            "Epoch 71/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 518.6604 - mse: 496904.6875 - mae: 518.6604 - val_loss: 532.0869 - val_mse: 293565.9375 - val_mae: 532.0869\n",
            "Epoch 72/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 496.5033 - mse: 457702.7500 - mae: 496.5033 - val_loss: 527.6670 - val_mse: 288399.8750 - val_mae: 527.6670\n",
            "Epoch 73/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 512.5365 - mse: 476000.0938 - mae: 512.5365 - val_loss: 524.0707 - val_mse: 284300.8750 - val_mae: 524.0707\n",
            "Epoch 74/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 497.4866 - mse: 458683.8125 - mae: 497.4866 - val_loss: 529.9206 - val_mse: 289647.9062 - val_mae: 529.9206\n",
            "Epoch 75/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 514.2053 - mse: 466067.5625 - mae: 514.2053 - val_loss: 526.6011 - val_mse: 285728.0625 - val_mae: 526.6011\n",
            "Epoch 76/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 506.1109 - mse: 457745.8125 - mae: 506.1109 - val_loss: 524.2465 - val_mse: 283064.5312 - val_mae: 524.2465\n",
            "Epoch 77/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 494.0672 - mse: 466051.8125 - mae: 494.0672 - val_loss: 510.5085 - val_mse: 268938.0625 - val_mae: 510.5085\n",
            "Epoch 78/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 521.2532 - mse: 491285.7500 - mae: 521.2532 - val_loss: 519.5723 - val_mse: 277758.3125 - val_mae: 519.5723\n",
            "Epoch 79/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 473.4827 - mse: 396781.0625 - mae: 473.4827 - val_loss: 510.4075 - val_mse: 267827.6875 - val_mae: 510.4075\n",
            "Epoch 80/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 474.4154 - mse: 443979.5938 - mae: 474.4154 - val_loss: 508.2723 - val_mse: 265448.3438 - val_mae: 508.2723\n",
            "Epoch 81/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 497.8493 - mse: 431130.3125 - mae: 497.8493 - val_loss: 505.1360 - val_mse: 261758.5938 - val_mae: 505.1360\n",
            "Epoch 82/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 515.1840 - mse: 480980.5938 - mae: 515.1840 - val_loss: 501.5221 - val_mse: 257658.0312 - val_mae: 501.5221\n",
            "Epoch 83/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 488.0660 - mse: 450183.5938 - mae: 488.0660 - val_loss: 502.8484 - val_mse: 258696.7500 - val_mae: 502.8484\n",
            "Epoch 84/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 496.3653 - mse: 440512.5000 - mae: 496.3653 - val_loss: 499.5151 - val_mse: 254905.5938 - val_mae: 499.5151\n",
            "Epoch 85/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 470.1516 - mse: 415025.4375 - mae: 470.1517 - val_loss: 496.6179 - val_mse: 251712.0312 - val_mae: 496.6179\n",
            "Epoch 86/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 478.1567 - mse: 408331.8125 - mae: 478.1566 - val_loss: 489.4951 - val_mse: 244246.0000 - val_mae: 489.4951\n",
            "Epoch 87/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 447.3605 - mse: 369225.5000 - mae: 447.3604 - val_loss: 493.5760 - val_mse: 248011.1250 - val_mae: 493.5760\n",
            "Epoch 88/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 485.7882 - mse: 394739.7812 - mae: 485.7882 - val_loss: 488.1766 - val_mse: 242187.1719 - val_mae: 488.1766\n",
            "Epoch 89/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 459.2211 - mse: 384015.9375 - mae: 459.2211 - val_loss: 486.7458 - val_mse: 240614.9688 - val_mae: 486.7458\n",
            "Epoch 90/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 412.7811 - mse: 327968.6875 - mae: 412.7811 - val_loss: 484.1738 - val_mse: 237901.9531 - val_mae: 484.1738\n",
            "Epoch 91/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 483.7693 - mse: 421366.0938 - mae: 483.7693 - val_loss: 488.7310 - val_mse: 242178.4219 - val_mae: 488.7310\n",
            "Epoch 92/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 497.2807 - mse: 426098.3125 - mae: 497.2807 - val_loss: 479.5712 - val_mse: 232934.5000 - val_mae: 479.5712\n",
            "Epoch 93/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 465.3989 - mse: 393458.3438 - mae: 465.3989 - val_loss: 474.2881 - val_mse: 227725.0938 - val_mae: 474.2881\n",
            "Epoch 94/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 445.3508 - mse: 381456.1875 - mae: 445.3508 - val_loss: 476.6961 - val_mse: 230235.3750 - val_mae: 476.6961\n",
            "Epoch 95/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 475.7464 - mse: 407302.0938 - mae: 475.7464 - val_loss: 477.9619 - val_mse: 231443.7031 - val_mae: 477.9619\n",
            "Epoch 96/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 462.1991 - mse: 380283.1875 - mae: 462.1991 - val_loss: 472.2238 - val_mse: 225938.4688 - val_mae: 472.2238\n",
            "Epoch 97/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 423.0676 - mse: 323603.5000 - mae: 423.0676 - val_loss: 464.2858 - val_mse: 218357.5312 - val_mae: 464.2858\n",
            "Epoch 98/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 460.5283 - mse: 401059.6875 - mae: 460.5283 - val_loss: 466.5924 - val_mse: 220822.4062 - val_mae: 466.5924\n",
            "Epoch 99/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 455.8748 - mse: 366535.3125 - mae: 455.8748 - val_loss: 459.9789 - val_mse: 214584.4219 - val_mae: 459.9789\n",
            "Epoch 100/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 409.7022 - mse: 314280.5312 - mae: 409.7021 - val_loss: 463.1876 - val_mse: 218339.2500 - val_mae: 463.1876\n",
            "Epoch 101/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 430.3688 - mse: 348269.4062 - mae: 430.3688 - val_loss: 461.0009 - val_mse: 216743.1250 - val_mae: 461.0009\n",
            "Epoch 102/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 396.2503 - mse: 282254.1562 - mae: 396.2503 - val_loss: 457.2657 - val_mse: 213717.4531 - val_mae: 457.2657\n",
            "Epoch 103/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 377.5947 - mse: 255101.1562 - mae: 377.5947 - val_loss: 454.1990 - val_mse: 211735.4688 - val_mae: 454.1990\n",
            "Epoch 104/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 378.9706 - mse: 276085.4062 - mae: 378.9706 - val_loss: 451.3326 - val_mse: 209927.2969 - val_mae: 451.3326\n",
            "Epoch 105/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 405.8291 - mse: 294509.4688 - mae: 405.8292 - val_loss: 446.3673 - val_mse: 206129.0469 - val_mae: 446.3673\n",
            "Epoch 106/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 387.5967 - mse: 277749.9062 - mae: 387.5967 - val_loss: 443.2747 - val_mse: 204322.8438 - val_mae: 443.2747\n",
            "Epoch 107/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 312.9601 - mse: 198194.5156 - mae: 312.9601 - val_loss: 434.2458 - val_mse: 197107.7031 - val_mae: 434.2458\n",
            "Epoch 108/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 376.5831 - mse: 288643.9062 - mae: 376.5831 - val_loss: 429.9818 - val_mse: 194525.3281 - val_mae: 429.9818\n",
            "Epoch 109/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 365.5714 - mse: 256364.7812 - mae: 365.5714 - val_loss: 429.6375 - val_mse: 195917.9062 - val_mae: 429.6375\n",
            "Epoch 110/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 412.7297 - mse: 304305.5938 - mae: 412.7297 - val_loss: 430.2632 - val_mse: 198054.4531 - val_mae: 430.2632\n",
            "Epoch 111/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 383.7203 - mse: 260876.7188 - mae: 383.7203 - val_loss: 422.1619 - val_mse: 191868.7188 - val_mae: 422.1619\n",
            "Epoch 112/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 331.0427 - mse: 231686.0938 - mae: 331.0427 - val_loss: 410.7474 - val_mse: 182219.9375 - val_mae: 410.7474\n",
            "Epoch 113/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 332.8988 - mse: 242447.0938 - mae: 332.8988 - val_loss: 413.9745 - val_mse: 187772.6562 - val_mae: 413.9745\n",
            "Epoch 114/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 322.7739 - mse: 225058.5469 - mae: 322.7739 - val_loss: 405.6479 - val_mse: 182123.4375 - val_mae: 405.6479\n",
            "Epoch 115/500\n",
            "20/20 [==============================] - 0s 985us/step - loss: 349.5863 - mse: 258652.2031 - mae: 349.5863 - val_loss: 409.5084 - val_mse: 187711.5000 - val_mae: 409.5084\n",
            "Epoch 116/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 356.6185 - mse: 269697.7812 - mae: 356.6185 - val_loss: 404.2444 - val_mse: 184876.4688 - val_mae: 404.2444\n",
            "Epoch 117/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 355.5714 - mse: 235962.8281 - mae: 355.5714 - val_loss: 394.1770 - val_mse: 177349.5938 - val_mae: 394.1770\n",
            "Epoch 118/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 344.4836 - mse: 242586.7969 - mae: 344.4836 - val_loss: 391.4946 - val_mse: 177550.9688 - val_mae: 391.4946\n",
            "Epoch 119/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 344.1868 - mse: 251596.6250 - mae: 344.1868 - val_loss: 385.1836 - val_mse: 174087.8906 - val_mae: 385.1836\n",
            "Epoch 120/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 329.0573 - mse: 227020.7969 - mae: 329.0573 - val_loss: 374.1435 - val_mse: 166396.0000 - val_mae: 374.1435\n",
            "Epoch 121/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 368.6029 - mse: 243345.4531 - mae: 368.6029 - val_loss: 369.2243 - val_mse: 164544.6250 - val_mae: 369.2243\n",
            "Epoch 122/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 266.8615 - mse: 142711.9688 - mae: 266.8615 - val_loss: 370.6512 - val_mse: 169791.6406 - val_mae: 370.6512\n",
            "Epoch 123/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 292.0098 - mse: 170660.2188 - mae: 292.0098 - val_loss: 367.6580 - val_mse: 169707.5312 - val_mae: 367.6580\n",
            "Epoch 124/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 282.5379 - mse: 184255.5938 - mae: 282.5379 - val_loss: 361.0266 - val_mse: 167006.0781 - val_mae: 361.0266\n",
            "Epoch 125/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 281.3622 - mse: 174036.0781 - mae: 281.3622 - val_loss: 351.6165 - val_mse: 161821.1094 - val_mae: 351.6165\n",
            "Epoch 126/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 281.5374 - mse: 160330.6719 - mae: 281.5374 - val_loss: 348.3677 - val_mse: 161888.0469 - val_mae: 348.3677\n",
            "Epoch 127/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 332.8544 - mse: 246574.2500 - mae: 332.8544 - val_loss: 347.6169 - val_mse: 163854.7188 - val_mae: 347.6169\n",
            "Epoch 128/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 295.1297 - mse: 178440.7812 - mae: 295.1297 - val_loss: 342.1628 - val_mse: 160830.2344 - val_mae: 342.1628\n",
            "Epoch 129/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 269.6946 - mse: 142023.4531 - mae: 269.6946 - val_loss: 336.9879 - val_mse: 160452.6719 - val_mae: 336.9879\n",
            "Epoch 130/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 287.9851 - mse: 190698.5312 - mae: 287.9851 - val_loss: 326.3741 - val_mse: 153328.0000 - val_mae: 326.3741\n",
            "Epoch 131/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 247.3446 - mse: 141392.7812 - mae: 247.3446 - val_loss: 322.1619 - val_mse: 154379.0781 - val_mae: 322.1619\n",
            "Epoch 132/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 252.8936 - mse: 159784.1406 - mae: 252.8936 - val_loss: 318.5622 - val_mse: 155093.3438 - val_mae: 318.5622\n",
            "Epoch 133/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 211.7777 - mse: 102012.0000 - mae: 211.7777 - val_loss: 315.9023 - val_mse: 154789.0781 - val_mae: 315.9023\n",
            "Epoch 134/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 284.6678 - mse: 167361.5938 - mae: 284.6678 - val_loss: 310.0026 - val_mse: 151054.6406 - val_mae: 310.0026\n",
            "Epoch 135/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 260.4775 - mse: 152481.9219 - mae: 260.4775 - val_loss: 312.4837 - val_mse: 155174.3906 - val_mae: 312.4837\n",
            "Epoch 136/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 306.1528 - mse: 215517.2500 - mae: 306.1528 - val_loss: 311.5732 - val_mse: 155079.6875 - val_mae: 311.5732\n",
            "Epoch 137/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 247.0147 - mse: 159848.4531 - mae: 247.0146 - val_loss: 301.5250 - val_mse: 148566.2500 - val_mae: 301.5250\n",
            "Epoch 138/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 240.4128 - mse: 109396.2266 - mae: 240.4128 - val_loss: 307.2487 - val_mse: 155673.0625 - val_mae: 307.2487\n",
            "Epoch 139/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 217.5011 - mse: 108170.0391 - mae: 217.5011 - val_loss: 302.0373 - val_mse: 151916.8125 - val_mae: 302.0373\n",
            "Epoch 140/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 237.4027 - mse: 132663.2656 - mae: 237.4027 - val_loss: 303.3848 - val_mse: 153364.8281 - val_mae: 303.3848\n",
            "Epoch 141/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 210.7024 - mse: 88195.8516 - mae: 210.7023 - val_loss: 303.1365 - val_mse: 153503.5625 - val_mae: 303.1365\n",
            "Epoch 142/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 199.8565 - mse: 94378.3516 - mae: 199.8565 - val_loss: 303.0580 - val_mse: 152275.1875 - val_mae: 303.0580\n",
            "Epoch 143/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 186.1372 - mse: 90461.3359 - mae: 186.1372 - val_loss: 303.2055 - val_mse: 151963.1250 - val_mae: 303.2055\n",
            "Epoch 144/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 220.6213 - mse: 140198.7656 - mae: 220.6213 - val_loss: 305.4742 - val_mse: 154253.9219 - val_mae: 305.4742\n",
            "Epoch 145/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 201.5210 - mse: 105776.3594 - mae: 201.5210 - val_loss: 303.3192 - val_mse: 150813.2188 - val_mae: 303.3192\n",
            "Epoch 146/500\n",
            "20/20 [==============================] - 0s 975us/step - loss: 188.6841 - mse: 84641.2266 - mae: 188.6841 - val_loss: 304.6501 - val_mse: 151543.8906 - val_mae: 304.6501\n",
            "Epoch 147/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 215.8125 - mse: 101352.2188 - mae: 215.8125 - val_loss: 306.6872 - val_mse: 152203.2188 - val_mae: 306.6872\n",
            "Epoch 148/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 204.0131 - mse: 94089.4297 - mae: 204.0131 - val_loss: 307.2212 - val_mse: 151765.7969 - val_mae: 307.2212\n",
            "Epoch 149/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 194.6201 - mse: 92213.4141 - mae: 194.6201 - val_loss: 311.4626 - val_mse: 155492.7812 - val_mae: 311.4626\n",
            "Epoch 150/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 225.6538 - mse: 130473.8594 - mae: 225.6538 - val_loss: 310.8973 - val_mse: 153228.7812 - val_mae: 310.8973\n",
            "Epoch 151/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 213.3159 - mse: 102871.3984 - mae: 213.3159 - val_loss: 311.3469 - val_mse: 153767.2500 - val_mae: 311.3469\n",
            "Epoch 152/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 236.1121 - mse: 104954.5000 - mae: 236.1121 - val_loss: 313.0607 - val_mse: 153840.5000 - val_mae: 313.0607\n",
            "Epoch 153/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 216.5816 - mse: 116077.0000 - mae: 216.5816 - val_loss: 317.4363 - val_mse: 156511.9062 - val_mae: 317.4363\n",
            "Epoch 154/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 241.7249 - mse: 115161.1250 - mae: 241.7249 - val_loss: 316.7144 - val_mse: 154794.9219 - val_mae: 316.7144\n",
            "Epoch 155/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 199.5312 - mse: 107967.1016 - mae: 199.5312 - val_loss: 320.3907 - val_mse: 158658.7188 - val_mae: 320.3907\n",
            "Epoch 156/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 203.2815 - mse: 91486.2734 - mae: 203.2815 - val_loss: 317.0395 - val_mse: 155414.9688 - val_mae: 317.0395\n",
            "Epoch 157/500\n",
            "20/20 [==============================] - 0s 974us/step - loss: 232.5156 - mse: 125689.7734 - mae: 232.5156 - val_loss: 321.0508 - val_mse: 157908.2812 - val_mae: 321.0508\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 30 samples, validate on 30 samples\n",
            "Epoch 1/500\n",
            "30/30 [==============================] - 2s 71ms/step - loss: 1637.3320 - mse: 4521326.0000 - mae: 1637.3320 - val_loss: 2644.3892 - val_mse: 10816034.0000 - val_mae: 2644.3892\n",
            "Epoch 2/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1637.3040 - mse: 4521227.5000 - mae: 1637.3040 - val_loss: 2644.3615 - val_mse: 10815879.0000 - val_mae: 2644.3616\n",
            "Epoch 3/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1637.2753 - mse: 4521128.5000 - mae: 1637.2753 - val_loss: 2644.3267 - val_mse: 10815667.0000 - val_mae: 2644.3267\n",
            "Epoch 4/500\n",
            "30/30 [==============================] - 0s 970us/step - loss: 1637.2347 - mse: 4520990.5000 - mae: 1637.2349 - val_loss: 2644.2536 - val_mse: 10815211.0000 - val_mae: 2644.2537\n",
            "Epoch 5/500\n",
            "30/30 [==============================] - 0s 997us/step - loss: 1637.1276 - mse: 4520523.5000 - mae: 1637.1276 - val_loss: 2644.0318 - val_mse: 10813778.0000 - val_mae: 2644.0317\n",
            "Epoch 6/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1636.7925 - mse: 4519289.0000 - mae: 1636.7925 - val_loss: 2643.3412 - val_mse: 10809722.0000 - val_mae: 2643.3411\n",
            "Epoch 7/500\n",
            "30/30 [==============================] - 0s 827us/step - loss: 1636.0826 - mse: 4516819.0000 - mae: 1636.0825 - val_loss: 2641.8094 - val_mse: 10801587.0000 - val_mae: 2641.8093\n",
            "Epoch 8/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1634.1670 - mse: 4509111.5000 - mae: 1634.1669 - val_loss: 2638.6413 - val_mse: 10785420.0000 - val_mae: 2638.6411\n",
            "Epoch 9/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1629.9365 - mse: 4496967.0000 - mae: 1629.9364 - val_loss: 2633.9423 - val_mse: 10763994.0000 - val_mae: 2633.9421\n",
            "Epoch 10/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1624.5880 - mse: 4478242.0000 - mae: 1624.5880 - val_loss: 2628.5290 - val_mse: 10738895.0000 - val_mae: 2628.5291\n",
            "Epoch 11/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1618.2204 - mse: 4461851.5000 - mae: 1618.2203 - val_loss: 2622.9511 - val_mse: 10718301.0000 - val_mae: 2622.9509\n",
            "Epoch 12/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1612.1315 - mse: 4443837.0000 - mae: 1612.1315 - val_loss: 2616.6779 - val_mse: 10694054.0000 - val_mae: 2616.6780\n",
            "Epoch 13/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 1604.0922 - mse: 4414661.5000 - mae: 1604.0922 - val_loss: 2609.4365 - val_mse: 10662524.0000 - val_mae: 2609.4365\n",
            "Epoch 14/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1596.2131 - mse: 4393718.0000 - mae: 1596.2131 - val_loss: 2601.8500 - val_mse: 10632423.0000 - val_mae: 2601.8501\n",
            "Epoch 15/500\n",
            "30/30 [==============================] - 0s 978us/step - loss: 1584.7478 - mse: 4354989.0000 - mae: 1584.7479 - val_loss: 2593.3000 - val_mse: 10597415.0000 - val_mae: 2593.3000\n",
            "Epoch 16/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1575.1022 - mse: 4339675.5000 - mae: 1575.1023 - val_loss: 2584.6092 - val_mse: 10564945.0000 - val_mae: 2584.6091\n",
            "Epoch 17/500\n",
            "30/30 [==============================] - 0s 938us/step - loss: 1563.7620 - mse: 4309082.5000 - mae: 1563.7620 - val_loss: 2575.0679 - val_mse: 10527315.0000 - val_mae: 2575.0676\n",
            "Epoch 18/500\n",
            "30/30 [==============================] - 0s 961us/step - loss: 1557.2050 - mse: 4282138.0000 - mae: 1557.2050 - val_loss: 2565.6432 - val_mse: 10485679.0000 - val_mae: 2565.6433\n",
            "Epoch 19/500\n",
            "30/30 [==============================] - 0s 948us/step - loss: 1540.5599 - mse: 4220213.0000 - mae: 1540.5599 - val_loss: 2555.5639 - val_mse: 10444287.0000 - val_mae: 2555.5640\n",
            "Epoch 20/500\n",
            "30/30 [==============================] - 0s 966us/step - loss: 1541.4738 - mse: 4242496.0000 - mae: 1541.4738 - val_loss: 2546.4834 - val_mse: 10410571.0000 - val_mae: 2546.4834\n",
            "Epoch 21/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1536.8304 - mse: 4195787.0000 - mae: 1536.8303 - val_loss: 2537.5966 - val_mse: 10367923.0000 - val_mae: 2537.5967\n",
            "Epoch 22/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1523.3821 - mse: 4169182.5000 - mae: 1523.3822 - val_loss: 2527.4823 - val_mse: 10325597.0000 - val_mae: 2527.4822\n",
            "Epoch 23/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1521.9324 - mse: 4135416.2500 - mae: 1521.9324 - val_loss: 2517.5697 - val_mse: 10277278.0000 - val_mae: 2517.5698\n",
            "Epoch 24/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1511.4806 - mse: 4111858.5000 - mae: 1511.4805 - val_loss: 2507.9880 - val_mse: 10231116.0000 - val_mae: 2507.9880\n",
            "Epoch 25/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1520.7235 - mse: 4154141.7500 - mae: 1520.7234 - val_loss: 2499.9663 - val_mse: 10189543.0000 - val_mae: 2499.9661\n",
            "Epoch 26/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1489.6142 - mse: 4048008.0000 - mae: 1489.6141 - val_loss: 2490.1605 - val_mse: 10145022.0000 - val_mae: 2490.1604\n",
            "Epoch 27/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1482.4804 - mse: 4000113.0000 - mae: 1482.4805 - val_loss: 2480.5405 - val_mse: 10096146.0000 - val_mae: 2480.5405\n",
            "Epoch 28/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1479.8071 - mse: 3980323.5000 - mae: 1479.8071 - val_loss: 2470.2324 - val_mse: 10048594.0000 - val_mae: 2470.2322\n",
            "Epoch 29/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1451.2447 - mse: 3923755.2500 - mae: 1451.2446 - val_loss: 2457.3637 - val_mse: 9995069.0000 - val_mae: 2457.3638\n",
            "Epoch 30/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1453.1132 - mse: 3865467.2500 - mae: 1453.1132 - val_loss: 2445.1870 - val_mse: 9939735.0000 - val_mae: 2445.1870\n",
            "Epoch 31/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1445.8256 - mse: 3846441.5000 - mae: 1445.8256 - val_loss: 2432.5796 - val_mse: 9882814.0000 - val_mae: 2432.5796\n",
            "Epoch 32/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1443.8196 - mse: 3838441.5000 - mae: 1443.8197 - val_loss: 2421.0085 - val_mse: 9828367.0000 - val_mae: 2421.0083\n",
            "Epoch 33/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1437.3521 - mse: 3774238.5000 - mae: 1437.3521 - val_loss: 2407.8703 - val_mse: 9767211.0000 - val_mae: 2407.8704\n",
            "Epoch 34/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1427.1877 - mse: 3703901.7500 - mae: 1427.1877 - val_loss: 2394.5645 - val_mse: 9706354.0000 - val_mae: 2394.5647\n",
            "Epoch 35/500\n",
            "30/30 [==============================] - 0s 970us/step - loss: 1426.4924 - mse: 3686950.2500 - mae: 1426.4924 - val_loss: 2381.1832 - val_mse: 9631359.0000 - val_mae: 2381.1833\n",
            "Epoch 36/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1421.7130 - mse: 3716887.2500 - mae: 1421.7130 - val_loss: 2366.2663 - val_mse: 9544029.0000 - val_mae: 2366.2664\n",
            "Epoch 37/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1414.3543 - mse: 3619082.7500 - mae: 1414.3542 - val_loss: 2351.2696 - val_mse: 9443805.0000 - val_mae: 2351.2695\n",
            "Epoch 38/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1388.4311 - mse: 3534872.5000 - mae: 1388.4310 - val_loss: 2333.3654 - val_mse: 9331793.0000 - val_mae: 2333.3655\n",
            "Epoch 39/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1372.7497 - mse: 3470372.7500 - mae: 1372.7496 - val_loss: 2317.9206 - val_mse: 9218654.0000 - val_mae: 2317.9207\n",
            "Epoch 40/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1380.4255 - mse: 3542453.0000 - mae: 1380.4255 - val_loss: 2304.2364 - val_mse: 9146918.0000 - val_mae: 2304.2366\n",
            "Epoch 41/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1378.1549 - mse: 3504174.7500 - mae: 1378.1549 - val_loss: 2291.9809 - val_mse: 9081130.0000 - val_mae: 2291.9807\n",
            "Epoch 42/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1356.5959 - mse: 3449029.2500 - mae: 1356.5959 - val_loss: 2283.4468 - val_mse: 9034032.0000 - val_mae: 2283.4468\n",
            "Epoch 43/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1370.7646 - mse: 3497887.2500 - mae: 1370.7645 - val_loss: 2270.4932 - val_mse: 8972847.0000 - val_mae: 2270.4932\n",
            "Epoch 44/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1364.1834 - mse: 3516772.2500 - mae: 1364.1833 - val_loss: 2265.1034 - val_mse: 8941148.0000 - val_mae: 2265.1033\n",
            "Epoch 45/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1343.5700 - mse: 3335168.7500 - mae: 1343.5701 - val_loss: 2259.5068 - val_mse: 8906358.0000 - val_mae: 2259.5068\n",
            "Epoch 46/500\n",
            "30/30 [==============================] - 0s 963us/step - loss: 1343.7073 - mse: 3302075.7500 - mae: 1343.7073 - val_loss: 2255.3743 - val_mse: 8876271.0000 - val_mae: 2255.3745\n",
            "Epoch 47/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1317.2664 - mse: 3257861.2500 - mae: 1317.2664 - val_loss: 2259.6226 - val_mse: 8877524.0000 - val_mae: 2259.6223\n",
            "Epoch 48/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1332.8390 - mse: 3311328.0000 - mae: 1332.8391 - val_loss: 2311.8862 - val_mse: 9044609.0000 - val_mae: 2311.8862\n",
            "Epoch 49/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1318.5575 - mse: 3276240.2500 - mae: 1318.5575 - val_loss: 2370.8439 - val_mse: 9216046.0000 - val_mae: 2370.8438\n",
            "Epoch 50/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1263.7732 - mse: 3066391.5000 - mae: 1263.7732 - val_loss: 2367.3952 - val_mse: 9178368.0000 - val_mae: 2367.3953\n",
            "Epoch 51/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1265.3239 - mse: 3059282.2500 - mae: 1265.3240 - val_loss: 2349.1297 - val_mse: 9093530.0000 - val_mae: 2349.1296\n",
            "Epoch 52/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1292.0601 - mse: 3249547.5000 - mae: 1292.0602 - val_loss: 2337.7420 - val_mse: 9033547.0000 - val_mae: 2337.7422\n",
            "Epoch 53/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1256.2687 - mse: 3041111.0000 - mae: 1256.2688 - val_loss: 2326.8698 - val_mse: 8972581.0000 - val_mae: 2326.8699\n",
            "Epoch 54/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1230.3638 - mse: 3009657.0000 - mae: 1230.3638 - val_loss: 2332.9171 - val_mse: 8972267.0000 - val_mae: 2332.9172\n",
            "Epoch 55/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1245.7034 - mse: 3071347.2500 - mae: 1245.7034 - val_loss: 2315.5186 - val_mse: 8889217.0000 - val_mae: 2315.5188\n",
            "Epoch 56/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1234.6695 - mse: 2947272.0000 - mae: 1234.6694 - val_loss: 2330.4235 - val_mse: 8923431.0000 - val_mae: 2330.4233\n",
            "Epoch 57/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1192.9702 - mse: 2846963.7500 - mae: 1192.9702 - val_loss: 2319.0553 - val_mse: 8858656.0000 - val_mae: 2319.0552\n",
            "Epoch 58/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1193.6346 - mse: 2883688.7500 - mae: 1193.6346 - val_loss: 2287.6392 - val_mse: 8719877.0000 - val_mae: 2287.6394\n",
            "Epoch 59/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1191.6985 - mse: 2864940.7500 - mae: 1191.6985 - val_loss: 2276.9492 - val_mse: 8656813.0000 - val_mae: 2276.9492\n",
            "Epoch 60/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1176.2125 - mse: 2815002.7500 - mae: 1176.2125 - val_loss: 2271.2683 - val_mse: 8618586.0000 - val_mae: 2271.2683\n",
            "Epoch 61/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1180.8336 - mse: 2929494.5000 - mae: 1180.8336 - val_loss: 2263.5250 - val_mse: 8569261.0000 - val_mae: 2263.5249\n",
            "Epoch 62/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1126.8157 - mse: 2559794.2500 - mae: 1126.8157 - val_loss: 2259.0224 - val_mse: 8528846.0000 - val_mae: 2259.0225\n",
            "Epoch 63/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1092.7761 - mse: 2567738.7500 - mae: 1092.7760 - val_loss: 2238.7893 - val_mse: 8429117.0000 - val_mae: 2238.7893\n",
            "Epoch 64/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1079.7693 - mse: 2549784.0000 - mae: 1079.7693 - val_loss: 2233.6118 - val_mse: 8386318.0000 - val_mae: 2233.6118\n",
            "Epoch 65/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 1026.6213 - mse: 2268493.2500 - mae: 1026.6213 - val_loss: 2221.9755 - val_mse: 8318559.0000 - val_mae: 2221.9756\n",
            "Epoch 66/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1078.1100 - mse: 2502722.7500 - mae: 1078.1100 - val_loss: 2212.7988 - val_mse: 8265511.0000 - val_mae: 2212.7991\n",
            "Epoch 67/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1021.5591 - mse: 2131353.2500 - mae: 1021.5591 - val_loss: 2208.1487 - val_mse: 8222688.0000 - val_mae: 2208.1487\n",
            "Epoch 68/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 978.3643 - mse: 2210180.2500 - mae: 978.3643 - val_loss: 2195.0955 - val_mse: 8147191.0000 - val_mae: 2195.0955\n",
            "Epoch 69/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1033.9721 - mse: 2279771.5000 - mae: 1033.9722 - val_loss: 2168.4281 - val_mse: 8024176.5000 - val_mae: 2168.4282\n",
            "Epoch 70/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1026.9520 - mse: 2251671.0000 - mae: 1026.9520 - val_loss: 2159.2342 - val_mse: 7970500.5000 - val_mae: 2159.2341\n",
            "Epoch 71/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 983.0214 - mse: 2181049.0000 - mae: 983.0214 - val_loss: 2177.2069 - val_mse: 8019614.5000 - val_mae: 2177.2068\n",
            "Epoch 72/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 975.9661 - mse: 2285876.2500 - mae: 975.9661 - val_loss: 2176.6188 - val_mse: 8002061.5000 - val_mae: 2176.6189\n",
            "Epoch 73/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 945.4612 - mse: 2043499.1250 - mae: 945.4612 - val_loss: 2151.8001 - val_mse: 7891197.0000 - val_mae: 2151.8000\n",
            "Epoch 74/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 959.8936 - mse: 2075196.5000 - mae: 959.8936 - val_loss: 2139.7996 - val_mse: 7830382.0000 - val_mae: 2139.7996\n",
            "Epoch 75/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 885.4128 - mse: 1907040.1250 - mae: 885.4128 - val_loss: 2145.2134 - val_mse: 7829534.5000 - val_mae: 2145.2134\n",
            "Epoch 76/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 885.1195 - mse: 1866871.8750 - mae: 885.1195 - val_loss: 2112.8383 - val_mse: 7688190.0000 - val_mae: 2112.8384\n",
            "Epoch 77/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 917.3375 - mse: 1921469.6250 - mae: 917.3375 - val_loss: 2108.0832 - val_mse: 7652787.5000 - val_mae: 2108.0833\n",
            "Epoch 78/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 938.8909 - mse: 1955790.8750 - mae: 938.8909 - val_loss: 2089.1850 - val_mse: 7573742.0000 - val_mae: 2089.1851\n",
            "Epoch 79/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 858.3984 - mse: 1856066.3750 - mae: 858.3984 - val_loss: 2093.6252 - val_mse: 7572324.5000 - val_mae: 2093.6252\n",
            "Epoch 80/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 891.9788 - mse: 1926979.3750 - mae: 891.9788 - val_loss: 2070.6285 - val_mse: 7470947.0000 - val_mae: 2070.6284\n",
            "Epoch 81/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 861.5814 - mse: 1741233.1250 - mae: 861.5814 - val_loss: 2033.7655 - val_mse: 7331086.0000 - val_mae: 2033.7654\n",
            "Epoch 82/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 818.8690 - mse: 1667472.5000 - mae: 818.8689 - val_loss: 2053.9104 - val_mse: 7391422.5000 - val_mae: 2053.9104\n",
            "Epoch 83/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 855.8232 - mse: 1724928.1250 - mae: 855.8232 - val_loss: 2049.5541 - val_mse: 7367776.0000 - val_mae: 2049.5542\n",
            "Epoch 84/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 825.4667 - mse: 1514580.0000 - mae: 825.4667 - val_loss: 2037.9383 - val_mse: 7307089.5000 - val_mae: 2037.9382\n",
            "Epoch 85/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 799.8504 - mse: 1522213.1250 - mae: 799.8504 - val_loss: 2036.1272 - val_mse: 7294806.5000 - val_mae: 2036.1271\n",
            "Epoch 86/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 885.6755 - mse: 1939791.2500 - mae: 885.6755 - val_loss: 2036.0539 - val_mse: 7282883.5000 - val_mae: 2036.0538\n",
            "Epoch 87/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 790.6621 - mse: 1474243.5000 - mae: 790.6621 - val_loss: 2032.3659 - val_mse: 7259569.0000 - val_mae: 2032.3658\n",
            "Epoch 88/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 774.7337 - mse: 1440493.3750 - mae: 774.7336 - val_loss: 2022.4731 - val_mse: 7211996.5000 - val_mae: 2022.4730\n",
            "Epoch 89/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 896.8067 - mse: 1679250.1250 - mae: 896.8066 - val_loss: 2003.2353 - val_mse: 7133204.5000 - val_mae: 2003.2352\n",
            "Epoch 90/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 756.9474 - mse: 1357948.0000 - mae: 756.9474 - val_loss: 1986.3719 - val_mse: 7064207.0000 - val_mae: 1986.3719\n",
            "Epoch 91/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 790.6483 - mse: 1479453.8750 - mae: 790.6483 - val_loss: 1986.1962 - val_mse: 7054714.0000 - val_mae: 1986.1962\n",
            "Epoch 92/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 864.2519 - mse: 1650945.6250 - mae: 864.2519 - val_loss: 1994.8194 - val_mse: 7083383.5000 - val_mae: 1994.8192\n",
            "Epoch 93/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 779.3578 - mse: 1461939.6250 - mae: 779.3578 - val_loss: 1989.1961 - val_mse: 7052966.0000 - val_mae: 1989.1960\n",
            "Epoch 94/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 815.1725 - mse: 1531653.8750 - mae: 815.1725 - val_loss: 2003.5426 - val_mse: 7097008.0000 - val_mae: 2003.5426\n",
            "Epoch 95/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 795.4392 - mse: 1364172.6250 - mae: 795.4393 - val_loss: 1992.5590 - val_mse: 7059256.5000 - val_mae: 1992.5590\n",
            "Epoch 96/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 907.6783 - mse: 1813535.5000 - mae: 907.6783 - val_loss: 1980.1775 - val_mse: 7009126.0000 - val_mae: 1980.1776\n",
            "Epoch 97/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 747.2792 - mse: 1273081.8750 - mae: 747.2792 - val_loss: 1948.9818 - val_mse: 6888745.5000 - val_mae: 1948.9818\n",
            "Epoch 98/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 879.4520 - mse: 1581851.1250 - mae: 879.4520 - val_loss: 1955.9550 - val_mse: 6902910.0000 - val_mae: 1955.9550\n",
            "Epoch 99/500\n",
            "30/30 [==============================] - 0s 951us/step - loss: 820.7672 - mse: 1443793.6250 - mae: 820.7672 - val_loss: 1975.7665 - val_mse: 6966928.0000 - val_mae: 1975.7665\n",
            "Epoch 100/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 790.0890 - mse: 1394672.5000 - mae: 790.0891 - val_loss: 1959.9992 - val_mse: 6897808.5000 - val_mae: 1959.9993\n",
            "Epoch 101/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 898.4068 - mse: 1631970.1250 - mae: 898.4069 - val_loss: 1971.8615 - val_mse: 6935337.0000 - val_mae: 1971.8615\n",
            "Epoch 102/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 803.6770 - mse: 1321984.1250 - mae: 803.6769 - val_loss: 1962.9040 - val_mse: 6904911.5000 - val_mae: 1962.9039\n",
            "Epoch 103/500\n",
            "30/30 [==============================] - 0s 876us/step - loss: 794.9398 - mse: 1340394.5000 - mae: 794.9398 - val_loss: 1957.8087 - val_mse: 6877457.5000 - val_mae: 1957.8087\n",
            "Epoch 104/500\n",
            "30/30 [==============================] - 0s 931us/step - loss: 789.2906 - mse: 1440305.6250 - mae: 789.2906 - val_loss: 1958.1243 - val_mse: 6869802.5000 - val_mae: 1958.1243\n",
            "Epoch 105/500\n",
            "30/30 [==============================] - 0s 875us/step - loss: 706.2851 - mse: 1097557.2500 - mae: 706.2852 - val_loss: 1954.5575 - val_mse: 6850472.0000 - val_mae: 1954.5575\n",
            "Epoch 106/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 794.8961 - mse: 1279842.3750 - mae: 794.8961 - val_loss: 1965.5778 - val_mse: 6886915.5000 - val_mae: 1965.5779\n",
            "Epoch 107/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 776.3007 - mse: 1323070.3750 - mae: 776.3007 - val_loss: 1965.5168 - val_mse: 6883550.5000 - val_mae: 1965.5170\n",
            "Epoch 108/500\n",
            "30/30 [==============================] - 0s 921us/step - loss: 844.3800 - mse: 1500324.1250 - mae: 844.3800 - val_loss: 1949.2715 - val_mse: 6824593.0000 - val_mae: 1949.2715\n",
            "Epoch 109/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 717.2952 - mse: 1190707.6250 - mae: 717.2952 - val_loss: 1930.4405 - val_mse: 6751130.5000 - val_mae: 1930.4406\n",
            "Epoch 110/500\n",
            "30/30 [==============================] - 0s 978us/step - loss: 898.7407 - mse: 1626382.6250 - mae: 898.7406 - val_loss: 1936.0459 - val_mse: 6769336.5000 - val_mae: 1936.0458\n",
            "Epoch 111/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 761.7519 - mse: 1377554.6250 - mae: 761.7519 - val_loss: 1934.0422 - val_mse: 6751155.0000 - val_mae: 1934.0422\n",
            "Epoch 112/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 754.8716 - mse: 1080240.7500 - mae: 754.8716 - val_loss: 1924.8363 - val_mse: 6708689.0000 - val_mae: 1924.8363\n",
            "Epoch 113/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 821.6056 - mse: 1229609.1250 - mae: 821.6056 - val_loss: 1919.0499 - val_mse: 6684002.5000 - val_mae: 1919.0499\n",
            "Epoch 114/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 769.0432 - mse: 1342076.2500 - mae: 769.0432 - val_loss: 1919.6627 - val_mse: 6679469.5000 - val_mae: 1919.6627\n",
            "Epoch 115/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 822.7838 - mse: 1445677.3750 - mae: 822.7839 - val_loss: 1925.0870 - val_mse: 6692282.5000 - val_mae: 1925.0870\n",
            "Epoch 116/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 692.3339 - mse: 1071166.7500 - mae: 692.3339 - val_loss: 1932.3325 - val_mse: 6714994.0000 - val_mae: 1932.3324\n",
            "Epoch 117/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 793.8768 - mse: 1170499.5000 - mae: 793.8768 - val_loss: 1934.4743 - val_mse: 6721733.0000 - val_mae: 1934.4744\n",
            "Epoch 118/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 776.3007 - mse: 1248606.0000 - mae: 776.3007 - val_loss: 1931.7319 - val_mse: 6702815.0000 - val_mae: 1931.7319\n",
            "Epoch 119/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 764.1630 - mse: 1274003.8750 - mae: 764.1630 - val_loss: 1912.3373 - val_mse: 6628130.0000 - val_mae: 1912.3374\n",
            "Epoch 120/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 697.8764 - mse: 1097866.8750 - mae: 697.8764 - val_loss: 1929.5871 - val_mse: 6679503.0000 - val_mae: 1929.5872\n",
            "Epoch 121/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 776.3265 - mse: 1340843.7500 - mae: 776.3265 - val_loss: 1928.3539 - val_mse: 6672409.5000 - val_mae: 1928.3539\n",
            "Epoch 122/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 794.1052 - mse: 1213632.2500 - mae: 794.1052 - val_loss: 1919.6176 - val_mse: 6637464.5000 - val_mae: 1919.6177\n",
            "Epoch 123/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 766.9940 - mse: 1201455.3750 - mae: 766.9941 - val_loss: 1926.6804 - val_mse: 6659462.5000 - val_mae: 1926.6804\n",
            "Epoch 124/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 750.0279 - mse: 1175565.3750 - mae: 750.0279 - val_loss: 1914.8415 - val_mse: 6610294.0000 - val_mae: 1914.8416\n",
            "Epoch 125/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 654.4600 - mse: 973726.7500 - mae: 654.4600 - val_loss: 1895.5412 - val_mse: 6527104.0000 - val_mae: 1895.5413\n",
            "Epoch 126/500\n",
            "30/30 [==============================] - 0s 961us/step - loss: 714.2051 - mse: 1130730.6250 - mae: 714.2051 - val_loss: 1914.9813 - val_mse: 6585974.5000 - val_mae: 1914.9812\n",
            "Epoch 127/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 822.5335 - mse: 1287064.7500 - mae: 822.5335 - val_loss: 1914.1569 - val_mse: 6576241.0000 - val_mae: 1914.1569\n",
            "Epoch 128/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 756.9657 - mse: 1200769.6250 - mae: 756.9657 - val_loss: 1926.0965 - val_mse: 6612771.5000 - val_mae: 1926.0964\n",
            "Epoch 129/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 761.6300 - mse: 1099575.8750 - mae: 761.6300 - val_loss: 1928.9076 - val_mse: 6610954.5000 - val_mae: 1928.9076\n",
            "Epoch 130/500\n",
            "30/30 [==============================] - 0s 956us/step - loss: 785.5823 - mse: 1320922.0000 - mae: 785.5822 - val_loss: 1931.5916 - val_mse: 6613757.5000 - val_mae: 1931.5917\n",
            "Epoch 131/500\n",
            "30/30 [==============================] - 0s 987us/step - loss: 733.7823 - mse: 1213010.5000 - mae: 733.7823 - val_loss: 1919.8319 - val_mse: 6561011.0000 - val_mae: 1919.8319\n",
            "Epoch 132/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 748.0546 - mse: 1231759.5000 - mae: 748.0546 - val_loss: 1915.2225 - val_mse: 6534286.0000 - val_mae: 1915.2225\n",
            "Epoch 133/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 616.3950 - mse: 932709.6250 - mae: 616.3951 - val_loss: 1896.1458 - val_mse: 6450211.0000 - val_mae: 1896.1459\n",
            "Epoch 134/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 703.9426 - mse: 1132217.8750 - mae: 703.9426 - val_loss: 1902.7550 - val_mse: 6461838.0000 - val_mae: 1902.7551\n",
            "Epoch 135/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 732.3977 - mse: 1156578.3750 - mae: 732.3977 - val_loss: 1912.0904 - val_mse: 6479446.0000 - val_mae: 1912.0903\n",
            "Epoch 136/500\n",
            "30/30 [==============================] - 0s 995us/step - loss: 779.6829 - mse: 1200959.1250 - mae: 779.6828 - val_loss: 1911.4842 - val_mse: 6461829.0000 - val_mae: 1911.4841\n",
            "Epoch 137/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 615.0299 - mse: 1047165.3750 - mae: 615.0299 - val_loss: 1889.0589 - val_mse: 6365306.0000 - val_mae: 1889.0588\n",
            "Epoch 138/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 714.3196 - mse: 1053335.3750 - mae: 714.3195 - val_loss: 1903.5789 - val_mse: 6412345.5000 - val_mae: 1903.5789\n",
            "Epoch 139/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 635.7088 - mse: 863639.1875 - mae: 635.7088 - val_loss: 1913.9966 - val_mse: 6437432.5000 - val_mae: 1913.9966\n",
            "Epoch 140/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 676.0213 - mse: 900753.6250 - mae: 676.0213 - val_loss: 1920.0885 - val_mse: 6445719.0000 - val_mae: 1920.0885\n",
            "Epoch 141/500\n",
            "30/30 [==============================] - 0s 964us/step - loss: 651.7969 - mse: 979320.8125 - mae: 651.7969 - val_loss: 1905.0238 - val_mse: 6375591.5000 - val_mae: 1905.0238\n",
            "Epoch 142/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 750.3892 - mse: 1067860.6250 - mae: 750.3892 - val_loss: 1909.3482 - val_mse: 6382166.5000 - val_mae: 1909.3481\n",
            "Epoch 143/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 571.2520 - mse: 857667.8750 - mae: 571.2520 - val_loss: 1890.7663 - val_mse: 6304487.5000 - val_mae: 1890.7662\n",
            "Epoch 144/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 692.0225 - mse: 945196.5625 - mae: 692.0225 - val_loss: 1895.6993 - val_mse: 6310090.5000 - val_mae: 1895.6993\n",
            "Epoch 145/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 604.3932 - mse: 877156.1875 - mae: 604.3932 - val_loss: 1889.2090 - val_mse: 6270766.5000 - val_mae: 1889.2090\n",
            "Epoch 146/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 651.6094 - mse: 1021328.0625 - mae: 651.6094 - val_loss: 1873.8155 - val_mse: 6200118.0000 - val_mae: 1873.8153\n",
            "Epoch 147/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 716.5927 - mse: 1020418.2500 - mae: 716.5927 - val_loss: 1890.8390 - val_mse: 6249607.0000 - val_mae: 1890.8391\n",
            "Epoch 148/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 653.3122 - mse: 845517.0625 - mae: 653.3123 - val_loss: 1882.2199 - val_mse: 6201993.0000 - val_mae: 1882.2200\n",
            "Epoch 149/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 694.2236 - mse: 1012259.8750 - mae: 694.2236 - val_loss: 1891.8979 - val_mse: 6231625.0000 - val_mae: 1891.8979\n",
            "Epoch 150/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 609.5888 - mse: 752643.8750 - mae: 609.5888 - val_loss: 1892.1286 - val_mse: 6217903.5000 - val_mae: 1892.1287\n",
            "Epoch 151/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 674.6820 - mse: 946514.0625 - mae: 674.6819 - val_loss: 1888.6241 - val_mse: 6193117.0000 - val_mae: 1888.6241\n",
            "Epoch 152/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 617.2962 - mse: 824468.2500 - mae: 617.2962 - val_loss: 1899.8989 - val_mse: 6226239.0000 - val_mae: 1899.8989\n",
            "Epoch 153/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 611.9133 - mse: 759184.1250 - mae: 611.9133 - val_loss: 1897.9503 - val_mse: 6203759.0000 - val_mae: 1897.9503\n",
            "Epoch 154/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 651.9658 - mse: 896275.1875 - mae: 651.9659 - val_loss: 1896.3661 - val_mse: 6179995.5000 - val_mae: 1896.3660\n",
            "Epoch 155/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 563.7668 - mse: 752176.2500 - mae: 563.7667 - val_loss: 1880.0363 - val_mse: 6102321.0000 - val_mae: 1880.0363\n",
            "Epoch 156/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 579.4053 - mse: 727418.7500 - mae: 579.4053 - val_loss: 1879.8438 - val_mse: 6091632.0000 - val_mae: 1879.8438\n",
            "Epoch 157/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 526.1200 - mse: 639927.3125 - mae: 526.1201 - val_loss: 1871.9161 - val_mse: 6045229.0000 - val_mae: 1871.9161\n",
            "Epoch 158/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 591.4460 - mse: 769892.1250 - mae: 591.4460 - val_loss: 1846.1163 - val_mse: 5934016.0000 - val_mae: 1846.1163\n",
            "Epoch 159/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 548.0299 - mse: 874207.1875 - mae: 548.0299 - val_loss: 1838.6961 - val_mse: 5887310.5000 - val_mae: 1838.6960\n",
            "Epoch 160/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 666.7489 - mse: 988787.4375 - mae: 666.7490 - val_loss: 1844.8480 - val_mse: 5904536.5000 - val_mae: 1844.8479\n",
            "Epoch 161/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 527.3440 - mse: 642233.0625 - mae: 527.3440 - val_loss: 1838.7031 - val_mse: 5868722.0000 - val_mae: 1838.7031\n",
            "Epoch 162/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 506.4789 - mse: 520427.1250 - mae: 506.4789 - val_loss: 1833.3437 - val_mse: 5834903.5000 - val_mae: 1833.3438\n",
            "Epoch 163/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 634.4823 - mse: 872842.8125 - mae: 634.4823 - val_loss: 1824.0512 - val_mse: 5794379.0000 - val_mae: 1824.0511\n",
            "Epoch 164/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 575.1726 - mse: 785127.2500 - mae: 575.1726 - val_loss: 1792.9222 - val_mse: 5663323.5000 - val_mae: 1792.9221\n",
            "Epoch 165/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 500.0955 - mse: 557813.4375 - mae: 500.0955 - val_loss: 1825.0970 - val_mse: 5767347.0000 - val_mae: 1825.0972\n",
            "Epoch 166/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 592.8378 - mse: 786434.5625 - mae: 592.8378 - val_loss: 1826.6886 - val_mse: 5766083.5000 - val_mae: 1826.6886\n",
            "Epoch 167/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 525.5347 - mse: 603803.9375 - mae: 525.5347 - val_loss: 1797.7270 - val_mse: 5642199.5000 - val_mae: 1797.7271\n",
            "Epoch 168/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 463.7345 - mse: 512273.4688 - mae: 463.7345 - val_loss: 1797.6898 - val_mse: 5637700.5000 - val_mae: 1797.6898\n",
            "Epoch 169/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 547.7805 - mse: 727697.1875 - mae: 547.7805 - val_loss: 1809.6494 - val_mse: 5670601.0000 - val_mae: 1809.6495\n",
            "Epoch 170/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 514.5603 - mse: 617032.1250 - mae: 514.5602 - val_loss: 1791.5055 - val_mse: 5588888.0000 - val_mae: 1791.5055\n",
            "Epoch 171/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 500.4862 - mse: 525131.9375 - mae: 500.4862 - val_loss: 1792.1446 - val_mse: 5579275.5000 - val_mae: 1792.1447\n",
            "Epoch 172/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 473.4865 - mse: 452616.1250 - mae: 473.4865 - val_loss: 1796.3764 - val_mse: 5581766.0000 - val_mae: 1796.3763\n",
            "Epoch 173/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 515.8193 - mse: 743861.5625 - mae: 515.8193 - val_loss: 1780.1068 - val_mse: 5503619.0000 - val_mae: 1780.1069\n",
            "Epoch 174/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 445.5688 - mse: 425683.8438 - mae: 445.5688 - val_loss: 1790.0141 - val_mse: 5522735.0000 - val_mae: 1790.0140\n",
            "Epoch 175/500\n",
            "30/30 [==============================] - 0s 972us/step - loss: 522.2130 - mse: 617906.1250 - mae: 522.2130 - val_loss: 1780.1172 - val_mse: 5477787.0000 - val_mae: 1780.1172\n",
            "Epoch 176/500\n",
            "30/30 [==============================] - 0s 925us/step - loss: 521.4587 - mse: 571707.8125 - mae: 521.4587 - val_loss: 1779.8963 - val_mse: 5477611.5000 - val_mae: 1779.8962\n",
            "Epoch 177/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 442.7636 - mse: 402313.8750 - mae: 442.7636 - val_loss: 1784.8827 - val_mse: 5480368.0000 - val_mae: 1784.8827\n",
            "Epoch 178/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 539.7633 - mse: 645718.6875 - mae: 539.7633 - val_loss: 1786.5301 - val_mse: 5470641.5000 - val_mae: 1786.5300\n",
            "Epoch 179/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 398.5817 - mse: 398609.4062 - mae: 398.5817 - val_loss: 1762.1023 - val_mse: 5369808.0000 - val_mae: 1762.1022\n",
            "Epoch 180/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 425.3478 - mse: 402092.0625 - mae: 425.3478 - val_loss: 1738.9356 - val_mse: 5279656.0000 - val_mae: 1738.9357\n",
            "Epoch 181/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 420.6095 - mse: 366285.9375 - mae: 420.6095 - val_loss: 1755.1270 - val_mse: 5337381.0000 - val_mae: 1755.1270\n",
            "Epoch 182/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 434.0280 - mse: 454291.6562 - mae: 434.0280 - val_loss: 1754.9192 - val_mse: 5323010.5000 - val_mae: 1754.9192\n",
            "Epoch 183/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 383.1241 - mse: 362754.6250 - mae: 383.1241 - val_loss: 1774.5289 - val_mse: 5390992.0000 - val_mae: 1774.5289\n",
            "Epoch 184/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 424.5516 - mse: 428712.2812 - mae: 424.5516 - val_loss: 1766.8028 - val_mse: 5350131.0000 - val_mae: 1766.8029\n",
            "Epoch 185/500\n",
            "30/30 [==============================] - 0s 980us/step - loss: 479.9099 - mse: 485840.3438 - mae: 479.9098 - val_loss: 1777.7381 - val_mse: 5387165.0000 - val_mae: 1777.7380\n",
            "Epoch 186/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 503.2046 - mse: 495028.0000 - mae: 503.2046 - val_loss: 1761.9335 - val_mse: 5325013.0000 - val_mae: 1761.9335\n",
            "Epoch 187/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 376.9764 - mse: 271376.4062 - mae: 376.9764 - val_loss: 1743.9126 - val_mse: 5239257.5000 - val_mae: 1743.9126\n",
            "Epoch 188/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 413.9363 - mse: 377008.1875 - mae: 413.9363 - val_loss: 1736.2377 - val_mse: 5206625.0000 - val_mae: 1736.2378\n",
            "Epoch 189/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 416.1934 - mse: 359343.1875 - mae: 416.1934 - val_loss: 1734.2847 - val_mse: 5199766.0000 - val_mae: 1734.2847\n",
            "Epoch 190/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 439.7246 - mse: 552501.6875 - mae: 439.7246 - val_loss: 1738.1005 - val_mse: 5203120.5000 - val_mae: 1738.1005\n",
            "Epoch 191/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 350.7764 - mse: 283634.8438 - mae: 350.7764 - val_loss: 1731.0214 - val_mse: 5170388.5000 - val_mae: 1731.0214\n",
            "Epoch 192/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 353.7016 - mse: 336786.8125 - mae: 353.7016 - val_loss: 1725.8075 - val_mse: 5148660.5000 - val_mae: 1725.8075\n",
            "Epoch 193/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 323.3701 - mse: 237349.1875 - mae: 323.3701 - val_loss: 1714.3508 - val_mse: 5106651.0000 - val_mae: 1714.3508\n",
            "Epoch 194/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 364.6540 - mse: 304325.5938 - mae: 364.6540 - val_loss: 1721.2509 - val_mse: 5122917.0000 - val_mae: 1721.2509\n",
            "Epoch 195/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 357.2355 - mse: 356752.1250 - mae: 357.2355 - val_loss: 1707.7655 - val_mse: 5060656.5000 - val_mae: 1707.7655\n",
            "Epoch 196/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 408.2773 - mse: 311462.7188 - mae: 408.2773 - val_loss: 1703.7124 - val_mse: 5036539.5000 - val_mae: 1703.7124\n",
            "Epoch 197/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 409.5072 - mse: 333229.4062 - mae: 409.5072 - val_loss: 1712.1283 - val_mse: 5056298.5000 - val_mae: 1712.1283\n",
            "Epoch 198/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 322.0490 - mse: 212798.7031 - mae: 322.0490 - val_loss: 1718.0597 - val_mse: 5076889.0000 - val_mae: 1718.0598\n",
            "Epoch 199/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 358.4557 - mse: 275812.8750 - mae: 358.4557 - val_loss: 1724.7618 - val_mse: 5103119.0000 - val_mae: 1724.7617\n",
            "Epoch 200/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 322.1609 - mse: 198112.4688 - mae: 322.1609 - val_loss: 1713.3917 - val_mse: 5063042.0000 - val_mae: 1713.3917\n",
            "Epoch 201/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 297.4423 - mse: 327531.4375 - mae: 297.4423 - val_loss: 1691.9184 - val_mse: 4969150.0000 - val_mae: 1691.9183\n",
            "Epoch 202/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 398.9609 - mse: 406663.2812 - mae: 398.9610 - val_loss: 1667.6634 - val_mse: 4888867.5000 - val_mae: 1667.6635\n",
            "Epoch 203/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 320.6949 - mse: 265435.3125 - mae: 320.6949 - val_loss: 1667.6570 - val_mse: 4887647.0000 - val_mae: 1667.6570\n",
            "Epoch 204/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 358.4840 - mse: 245852.7031 - mae: 358.4840 - val_loss: 1670.6767 - val_mse: 4894843.5000 - val_mae: 1670.6766\n",
            "Epoch 205/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 377.3040 - mse: 361558.0312 - mae: 377.3040 - val_loss: 1664.8430 - val_mse: 4861621.5000 - val_mae: 1664.8430\n",
            "Epoch 206/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 362.9278 - mse: 209361.0000 - mae: 362.9278 - val_loss: 1673.6977 - val_mse: 4890581.5000 - val_mae: 1673.6976\n",
            "Epoch 207/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 437.8397 - mse: 459047.8125 - mae: 437.8397 - val_loss: 1685.8430 - val_mse: 4934125.5000 - val_mae: 1685.8430\n",
            "Epoch 208/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 385.7974 - mse: 364899.4688 - mae: 385.7975 - val_loss: 1675.3374 - val_mse: 4897873.0000 - val_mae: 1675.3374\n",
            "Epoch 209/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 326.1075 - mse: 279390.4062 - mae: 326.1075 - val_loss: 1667.8162 - val_mse: 4868169.0000 - val_mae: 1667.8162\n",
            "Epoch 210/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 432.0600 - mse: 438213.9688 - mae: 432.0600 - val_loss: 1658.4849 - val_mse: 4827039.5000 - val_mae: 1658.4849\n",
            "Epoch 211/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 372.2788 - mse: 287473.6250 - mae: 372.2788 - val_loss: 1671.5088 - val_mse: 4880191.5000 - val_mae: 1671.5089\n",
            "Epoch 212/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 370.3355 - mse: 336925.0312 - mae: 370.3355 - val_loss: 1651.0009 - val_mse: 4800859.5000 - val_mae: 1651.0007\n",
            "Epoch 213/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 436.2451 - mse: 470313.3438 - mae: 436.2451 - val_loss: 1670.7528 - val_mse: 4874055.0000 - val_mae: 1670.7528\n",
            "Epoch 214/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 324.4779 - mse: 193992.6406 - mae: 324.4779 - val_loss: 1688.3941 - val_mse: 4948906.5000 - val_mae: 1688.3940\n",
            "Epoch 215/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 281.1248 - mse: 185742.7031 - mae: 281.1248 - val_loss: 1667.5977 - val_mse: 4868019.0000 - val_mae: 1667.5977\n",
            "Epoch 216/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 355.8639 - mse: 381051.3438 - mae: 355.8639 - val_loss: 1686.9547 - val_mse: 4933346.5000 - val_mae: 1686.9547\n",
            "Epoch 217/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 448.2639 - mse: 510544.9688 - mae: 448.2639 - val_loss: 1661.4665 - val_mse: 4833417.5000 - val_mae: 1661.4666\n",
            "Epoch 218/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 386.0326 - mse: 289378.5938 - mae: 386.0326 - val_loss: 1673.3175 - val_mse: 4865339.5000 - val_mae: 1673.3175\n",
            "Epoch 219/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 380.4645 - mse: 286551.9375 - mae: 380.4645 - val_loss: 1659.0642 - val_mse: 4808628.5000 - val_mae: 1659.0642\n",
            "Epoch 220/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 358.9046 - mse: 357278.9688 - mae: 358.9046 - val_loss: 1663.0317 - val_mse: 4830787.0000 - val_mae: 1663.0317\n",
            "Epoch 221/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 307.8816 - mse: 181376.0000 - mae: 307.8817 - val_loss: 1670.0131 - val_mse: 4853448.5000 - val_mae: 1670.0131\n",
            "Epoch 222/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 341.3703 - mse: 232226.0000 - mae: 341.3703 - val_loss: 1673.4405 - val_mse: 4865211.0000 - val_mae: 1673.4407\n",
            "Epoch 223/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 452.0657 - mse: 524475.1250 - mae: 452.0656 - val_loss: 1660.4898 - val_mse: 4804615.5000 - val_mae: 1660.4899\n",
            "Epoch 224/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 294.0963 - mse: 168135.2656 - mae: 294.0963 - val_loss: 1645.0648 - val_mse: 4744326.5000 - val_mae: 1645.0648\n",
            "Epoch 225/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 198.8224 - mse: 99164.1328 - mae: 198.8224 - val_loss: 1628.6526 - val_mse: 4683013.0000 - val_mae: 1628.6526\n",
            "Epoch 226/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 371.0558 - mse: 304484.3750 - mae: 371.0558 - val_loss: 1645.5571 - val_mse: 4743279.0000 - val_mae: 1645.5570\n",
            "Epoch 227/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 292.0448 - mse: 258810.2344 - mae: 292.0448 - val_loss: 1635.4475 - val_mse: 4709587.5000 - val_mae: 1635.4474\n",
            "Epoch 228/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 248.1561 - mse: 142213.3906 - mae: 248.1561 - val_loss: 1640.6975 - val_mse: 4721112.5000 - val_mae: 1640.6975\n",
            "Epoch 229/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 332.2055 - mse: 242637.4219 - mae: 332.2055 - val_loss: 1652.0405 - val_mse: 4768538.0000 - val_mae: 1652.0405\n",
            "Epoch 230/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 407.1916 - mse: 392119.3750 - mae: 407.1917 - val_loss: 1644.4309 - val_mse: 4733636.5000 - val_mae: 1644.4309\n",
            "Epoch 231/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 421.6873 - mse: 389827.9062 - mae: 421.6873 - val_loss: 1627.2207 - val_mse: 4664427.5000 - val_mae: 1627.2206\n",
            "Epoch 232/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 311.7420 - mse: 183127.0000 - mae: 311.7420 - val_loss: 1626.8528 - val_mse: 4663658.5000 - val_mae: 1626.8529\n",
            "Epoch 233/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 304.8448 - mse: 154791.2656 - mae: 304.8447 - val_loss: 1641.0326 - val_mse: 4707470.5000 - val_mae: 1641.0326\n",
            "Epoch 234/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 380.9833 - mse: 323176.4375 - mae: 380.9833 - val_loss: 1635.3800 - val_mse: 4681097.5000 - val_mae: 1635.3800\n",
            "Epoch 235/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 323.4378 - mse: 220138.8125 - mae: 323.4378 - val_loss: 1654.7378 - val_mse: 4750322.0000 - val_mae: 1654.7378\n",
            "Epoch 236/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 289.3035 - mse: 167139.7500 - mae: 289.3034 - val_loss: 1661.7812 - val_mse: 4780588.5000 - val_mae: 1661.7812\n",
            "Epoch 237/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 353.5018 - mse: 290217.9375 - mae: 353.5018 - val_loss: 1673.2416 - val_mse: 4830593.0000 - val_mae: 1673.2417\n",
            "Epoch 238/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 454.5255 - mse: 541062.6250 - mae: 454.5255 - val_loss: 1648.2747 - val_mse: 4729111.0000 - val_mae: 1648.2747\n",
            "Epoch 239/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 468.6937 - mse: 611363.0000 - mae: 468.6937 - val_loss: 1655.1422 - val_mse: 4750080.0000 - val_mae: 1655.1422\n",
            "Epoch 240/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 359.7326 - mse: 314813.3438 - mae: 359.7325 - val_loss: 1643.6934 - val_mse: 4711594.0000 - val_mae: 1643.6935\n",
            "Epoch 241/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 340.6909 - mse: 254730.5312 - mae: 340.6909 - val_loss: 1632.9302 - val_mse: 4676403.5000 - val_mae: 1632.9302\n",
            "Epoch 242/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 393.5305 - mse: 288482.5000 - mae: 393.5305 - val_loss: 1629.0759 - val_mse: 4664239.5000 - val_mae: 1629.0759\n",
            "Epoch 243/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 353.5009 - mse: 289248.0312 - mae: 353.5009 - val_loss: 1615.6630 - val_mse: 4608941.0000 - val_mae: 1615.6630\n",
            "Epoch 244/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 368.3280 - mse: 241062.0000 - mae: 368.3280 - val_loss: 1651.5492 - val_mse: 4741875.5000 - val_mae: 1651.5492\n",
            "Epoch 245/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 311.7675 - mse: 211699.9062 - mae: 311.7675 - val_loss: 1639.1352 - val_mse: 4700228.5000 - val_mae: 1639.1351\n",
            "Epoch 246/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 391.1803 - mse: 375995.1250 - mae: 391.1803 - val_loss: 1636.8625 - val_mse: 4684287.0000 - val_mae: 1636.8625\n",
            "Epoch 247/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 356.4478 - mse: 283371.7188 - mae: 356.4478 - val_loss: 1636.1481 - val_mse: 4682431.0000 - val_mae: 1636.1482\n",
            "Epoch 248/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 411.0929 - mse: 349340.0000 - mae: 411.0929 - val_loss: 1654.7652 - val_mse: 4761067.0000 - val_mae: 1654.7651\n",
            "Epoch 249/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 337.3108 - mse: 265293.3438 - mae: 337.3109 - val_loss: 1646.8133 - val_mse: 4734457.5000 - val_mae: 1646.8132\n",
            "Epoch 250/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 279.3897 - mse: 131935.7656 - mae: 279.3896 - val_loss: 1645.0638 - val_mse: 4730583.0000 - val_mae: 1645.0638\n",
            "Epoch 251/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 353.8920 - mse: 252581.2188 - mae: 353.8920 - val_loss: 1645.7923 - val_mse: 4735314.5000 - val_mae: 1645.7925\n",
            "Epoch 252/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 416.0857 - mse: 368108.1875 - mae: 416.0857 - val_loss: 1644.6987 - val_mse: 4727750.5000 - val_mae: 1644.6987\n",
            "Epoch 253/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 354.8857 - mse: 244992.3125 - mae: 354.8857 - val_loss: 1650.3433 - val_mse: 4745858.0000 - val_mae: 1650.3433\n",
            "Epoch 254/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 331.4338 - mse: 262006.2344 - mae: 331.4338 - val_loss: 1647.8018 - val_mse: 4735067.0000 - val_mae: 1647.8019\n",
            "Epoch 255/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 391.5408 - mse: 327053.4688 - mae: 391.5408 - val_loss: 1630.6204 - val_mse: 4672664.0000 - val_mae: 1630.6206\n",
            "Epoch 256/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 361.9207 - mse: 271624.6562 - mae: 361.9207 - val_loss: 1653.1426 - val_mse: 4762496.0000 - val_mae: 1653.1426\n",
            "Epoch 257/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 403.4220 - mse: 327498.0938 - mae: 403.4220 - val_loss: 1634.5642 - val_mse: 4698929.5000 - val_mae: 1634.5643\n",
            "Epoch 258/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 389.1942 - mse: 359541.4688 - mae: 389.1942 - val_loss: 1635.6379 - val_mse: 4699808.0000 - val_mae: 1635.6378\n",
            "Epoch 259/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 289.1019 - mse: 183589.7031 - mae: 289.1019 - val_loss: 1616.3332 - val_mse: 4629695.0000 - val_mae: 1616.3333\n",
            "Epoch 260/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 324.3943 - mse: 213614.5156 - mae: 324.3943 - val_loss: 1614.0692 - val_mse: 4621871.5000 - val_mae: 1614.0690\n",
            "Epoch 261/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 189.4078 - mse: 58334.3828 - mae: 189.4077 - val_loss: 1617.9297 - val_mse: 4628745.5000 - val_mae: 1617.9297\n",
            "Epoch 262/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 358.1985 - mse: 292438.9688 - mae: 358.1985 - val_loss: 1633.2558 - val_mse: 4691531.0000 - val_mae: 1633.2559\n",
            "Epoch 263/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 271.0398 - mse: 161418.6562 - mae: 271.0398 - val_loss: 1621.0934 - val_mse: 4651049.0000 - val_mae: 1621.0933\n",
            "Epoch 264/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 355.3715 - mse: 290323.5000 - mae: 355.3715 - val_loss: 1595.9113 - val_mse: 4563494.5000 - val_mae: 1595.9114\n",
            "Epoch 265/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 329.3389 - mse: 211153.0781 - mae: 329.3389 - val_loss: 1606.4476 - val_mse: 4599177.0000 - val_mae: 1606.4476\n",
            "Epoch 266/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 302.3300 - mse: 179375.7656 - mae: 302.3300 - val_loss: 1605.7126 - val_mse: 4599225.5000 - val_mae: 1605.7125\n",
            "Epoch 267/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 346.1229 - mse: 194923.7656 - mae: 346.1229 - val_loss: 1609.3063 - val_mse: 4616118.5000 - val_mae: 1609.3063\n",
            "Epoch 268/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 380.0500 - mse: 362916.1562 - mae: 380.0500 - val_loss: 1592.0348 - val_mse: 4554959.0000 - val_mae: 1592.0348\n",
            "Epoch 269/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 334.2292 - mse: 270700.4062 - mae: 334.2292 - val_loss: 1599.5728 - val_mse: 4580410.0000 - val_mae: 1599.5729\n",
            "Epoch 270/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 376.2838 - mse: 318739.0625 - mae: 376.2838 - val_loss: 1603.0799 - val_mse: 4591794.5000 - val_mae: 1603.0800\n",
            "Epoch 271/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 266.6066 - mse: 106306.8750 - mae: 266.6066 - val_loss: 1621.8168 - val_mse: 4654917.5000 - val_mae: 1621.8168\n",
            "Epoch 272/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 355.9418 - mse: 219782.2344 - mae: 355.9418 - val_loss: 1634.7419 - val_mse: 4702961.0000 - val_mae: 1634.7419\n",
            "Epoch 273/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 347.1788 - mse: 244013.3438 - mae: 347.1788 - val_loss: 1623.1923 - val_mse: 4656525.0000 - val_mae: 1623.1921\n",
            "Epoch 274/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 422.5344 - mse: 479986.0000 - mae: 422.5344 - val_loss: 1622.6622 - val_mse: 4649775.5000 - val_mae: 1622.6622\n",
            "Epoch 275/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 303.2175 - mse: 260105.5625 - mae: 303.2175 - val_loss: 1610.2684 - val_mse: 4608284.5000 - val_mae: 1610.2684\n",
            "Epoch 276/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 358.5682 - mse: 222788.3594 - mae: 358.5682 - val_loss: 1600.8685 - val_mse: 4572120.5000 - val_mae: 1600.8685\n",
            "Epoch 277/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 397.4173 - mse: 298538.8125 - mae: 397.4174 - val_loss: 1605.2319 - val_mse: 4590624.0000 - val_mae: 1605.2319\n",
            "Epoch 278/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 434.4847 - mse: 434720.0000 - mae: 434.4847 - val_loss: 1607.6180 - val_mse: 4598349.0000 - val_mae: 1607.6179\n",
            "Epoch 279/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 355.2415 - mse: 287418.5312 - mae: 355.2415 - val_loss: 1605.5851 - val_mse: 4587449.0000 - val_mae: 1605.5852\n",
            "Epoch 280/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 328.6553 - mse: 256963.1719 - mae: 328.6553 - val_loss: 1601.3333 - val_mse: 4574419.0000 - val_mae: 1601.3334\n",
            "Epoch 281/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 359.2625 - mse: 311724.4688 - mae: 359.2624 - val_loss: 1629.7092 - val_mse: 4674767.0000 - val_mae: 1629.7092\n",
            "Epoch 282/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 474.7301 - mse: 456069.5312 - mae: 474.7301 - val_loss: 1640.8215 - val_mse: 4713592.5000 - val_mae: 1640.8217\n",
            "Epoch 283/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 342.6770 - mse: 256804.5469 - mae: 342.6770 - val_loss: 1651.3760 - val_mse: 4746670.0000 - val_mae: 1651.3761\n",
            "Epoch 284/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 437.1982 - mse: 431657.9062 - mae: 437.1982 - val_loss: 1658.8618 - val_mse: 4770918.0000 - val_mae: 1658.8617\n",
            "Epoch 285/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 404.0455 - mse: 429767.1875 - mae: 404.0455 - val_loss: 1637.1160 - val_mse: 4693262.0000 - val_mae: 1637.1161\n",
            "Epoch 286/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 361.9839 - mse: 266778.0625 - mae: 361.9839 - val_loss: 1635.5909 - val_mse: 4684400.5000 - val_mae: 1635.5909\n",
            "Epoch 287/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 416.4222 - mse: 424342.0000 - mae: 416.4222 - val_loss: 1632.9550 - val_mse: 4671409.0000 - val_mae: 1632.9550\n",
            "Epoch 288/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 385.7631 - mse: 313212.4062 - mae: 385.7632 - val_loss: 1637.0229 - val_mse: 4688287.0000 - val_mae: 1637.0229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50 samples, validate on 40 samples\n",
            "Epoch 1/500\n",
            "50/50 [==============================] - 2s 35ms/step - loss: 5100.9227 - mse: 48605900.0000 - mae: 5100.9229 - val_loss: 4477.9179 - val_mse: 33087478.0000 - val_mae: 4477.9180\n",
            "Epoch 2/500\n",
            "50/50 [==============================] - 0s 899us/step - loss: 5100.8400 - mse: 48605136.0000 - mae: 5100.8398 - val_loss: 4477.7320 - val_mse: 33086240.0000 - val_mae: 4477.7319\n",
            "Epoch 3/500\n",
            "50/50 [==============================] - 0s 937us/step - loss: 5100.4015 - mse: 48601788.0000 - mae: 5100.4014 - val_loss: 4476.4669 - val_mse: 33078972.0000 - val_mae: 4476.4673\n",
            "Epoch 4/500\n",
            "50/50 [==============================] - 0s 934us/step - loss: 5098.3113 - mse: 48590416.0000 - mae: 5098.3110 - val_loss: 4472.5010 - val_mse: 33054176.0000 - val_mae: 4472.5010\n",
            "Epoch 5/500\n",
            "50/50 [==============================] - 0s 890us/step - loss: 5093.7101 - mse: 48557772.0000 - mae: 5093.7104 - val_loss: 4466.1779 - val_mse: 33007716.0000 - val_mae: 4466.1782\n",
            "Epoch 6/500\n",
            "50/50 [==============================] - 0s 989us/step - loss: 5086.6806 - mse: 48493536.0000 - mae: 5086.6807 - val_loss: 4458.0541 - val_mse: 32941952.0000 - val_mae: 4458.0537\n",
            "Epoch 7/500\n",
            "50/50 [==============================] - 0s 816us/step - loss: 5080.1302 - mse: 48434396.0000 - mae: 5080.1304 - val_loss: 4449.5034 - val_mse: 32868890.0000 - val_mae: 4449.5029\n",
            "Epoch 8/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5071.1165 - mse: 48315924.0000 - mae: 5071.1162 - val_loss: 4440.0576 - val_mse: 32785728.0000 - val_mae: 4440.0576\n",
            "Epoch 9/500\n",
            "50/50 [==============================] - 0s 799us/step - loss: 5063.0617 - mse: 48229720.0000 - mae: 5063.0615 - val_loss: 4430.2202 - val_mse: 32699466.0000 - val_mae: 4430.2202\n",
            "Epoch 10/500\n",
            "50/50 [==============================] - 0s 831us/step - loss: 5049.9660 - mse: 48137980.0000 - mae: 5049.9658 - val_loss: 4419.3151 - val_mse: 32604948.0000 - val_mae: 4419.3154\n",
            "Epoch 11/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5042.6604 - mse: 48062580.0000 - mae: 5042.6606 - val_loss: 4408.2772 - val_mse: 32509094.0000 - val_mae: 4408.2773\n",
            "Epoch 12/500\n",
            "50/50 [==============================] - 0s 835us/step - loss: 5027.5751 - mse: 47895008.0000 - mae: 5027.5752 - val_loss: 4396.0367 - val_mse: 32402166.0000 - val_mae: 4396.0366\n",
            "Epoch 13/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5015.6469 - mse: 47807468.0000 - mae: 5015.6470 - val_loss: 4384.3855 - val_mse: 32300940.0000 - val_mae: 4384.3857\n",
            "Epoch 14/500\n",
            "50/50 [==============================] - 0s 918us/step - loss: 5012.7270 - mse: 47734268.0000 - mae: 5012.7271 - val_loss: 4373.4523 - val_mse: 32205572.0000 - val_mae: 4373.4521\n",
            "Epoch 15/500\n",
            "50/50 [==============================] - 0s 934us/step - loss: 4995.7192 - mse: 47552040.0000 - mae: 4995.7192 - val_loss: 4361.0504 - val_mse: 32096618.0000 - val_mae: 4361.0503\n",
            "Epoch 16/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4978.3275 - mse: 47380428.0000 - mae: 4978.3276 - val_loss: 4347.7148 - val_mse: 31980486.0000 - val_mae: 4347.7148\n",
            "Epoch 17/500\n",
            "50/50 [==============================] - 0s 990us/step - loss: 4969.7730 - mse: 47243068.0000 - mae: 4969.7729 - val_loss: 4332.6953 - val_mse: 31850676.0000 - val_mae: 4332.6953\n",
            "Epoch 18/500\n",
            "50/50 [==============================] - 0s 978us/step - loss: 4951.2176 - mse: 47122008.0000 - mae: 4951.2173 - val_loss: 4317.9161 - val_mse: 31723142.0000 - val_mae: 4317.9160\n",
            "Epoch 19/500\n",
            "50/50 [==============================] - 0s 879us/step - loss: 4941.7193 - mse: 46906500.0000 - mae: 4941.7192 - val_loss: 4302.5588 - val_mse: 31590790.0000 - val_mae: 4302.5586\n",
            "Epoch 20/500\n",
            "50/50 [==============================] - 0s 905us/step - loss: 4932.7860 - mse: 46775748.0000 - mae: 4932.7861 - val_loss: 4287.0110 - val_mse: 31456756.0000 - val_mae: 4287.0107\n",
            "Epoch 21/500\n",
            "50/50 [==============================] - 0s 869us/step - loss: 4925.5595 - mse: 46637104.0000 - mae: 4925.5596 - val_loss: 4270.6444 - val_mse: 31316538.0000 - val_mae: 4270.6445\n",
            "Epoch 22/500\n",
            "50/50 [==============================] - 0s 784us/step - loss: 4890.2903 - mse: 46211396.0000 - mae: 4890.2900 - val_loss: 4253.1926 - val_mse: 31167072.0000 - val_mae: 4253.1924\n",
            "Epoch 23/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4893.3615 - mse: 46423316.0000 - mae: 4893.3613 - val_loss: 4235.9407 - val_mse: 31021260.0000 - val_mae: 4235.9404\n",
            "Epoch 24/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4880.1917 - mse: 46090508.0000 - mae: 4880.1914 - val_loss: 4219.5750 - val_mse: 30882368.0000 - val_mae: 4219.5752\n",
            "Epoch 25/500\n",
            "50/50 [==============================] - 0s 933us/step - loss: 4866.1075 - mse: 46027992.0000 - mae: 4866.1074 - val_loss: 4201.5231 - val_mse: 30729924.0000 - val_mae: 4201.5229\n",
            "Epoch 26/500\n",
            "50/50 [==============================] - 0s 813us/step - loss: 4857.3653 - mse: 45944336.0000 - mae: 4857.3657 - val_loss: 4182.2859 - val_mse: 30569082.0000 - val_mae: 4182.2861\n",
            "Epoch 27/500\n",
            "50/50 [==============================] - 0s 775us/step - loss: 4836.5581 - mse: 45466828.0000 - mae: 4836.5581 - val_loss: 4161.8314 - val_mse: 30397972.0000 - val_mae: 4161.8311\n",
            "Epoch 28/500\n",
            "50/50 [==============================] - 0s 994us/step - loss: 4813.0759 - mse: 45312024.0000 - mae: 4813.0757 - val_loss: 4140.4007 - val_mse: 30220000.0000 - val_mae: 4140.4009\n",
            "Epoch 29/500\n",
            "50/50 [==============================] - 0s 878us/step - loss: 4814.4652 - mse: 45299332.0000 - mae: 4814.4648 - val_loss: 4119.9728 - val_mse: 30050992.0000 - val_mae: 4119.9731\n",
            "Epoch 30/500\n",
            "50/50 [==============================] - 0s 785us/step - loss: 4812.9473 - mse: 45061792.0000 - mae: 4812.9468 - val_loss: 4100.3814 - val_mse: 29889392.0000 - val_mae: 4100.3813\n",
            "Epoch 31/500\n",
            "50/50 [==============================] - 0s 824us/step - loss: 4812.7846 - mse: 44997412.0000 - mae: 4812.7847 - val_loss: 4082.2617 - val_mse: 29740342.0000 - val_mae: 4082.2617\n",
            "Epoch 32/500\n",
            "50/50 [==============================] - 0s 932us/step - loss: 4767.4339 - mse: 44514448.0000 - mae: 4767.4336 - val_loss: 4059.8454 - val_mse: 29557434.0000 - val_mae: 4059.8452\n",
            "Epoch 33/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4726.7034 - mse: 44127668.0000 - mae: 4726.7036 - val_loss: 4035.2743 - val_mse: 29358170.0000 - val_mae: 4035.2742\n",
            "Epoch 34/500\n",
            "50/50 [==============================] - 0s 849us/step - loss: 4717.4043 - mse: 44048292.0000 - mae: 4717.4043 - val_loss: 4011.3672 - val_mse: 29163916.0000 - val_mae: 4011.3672\n",
            "Epoch 35/500\n",
            "50/50 [==============================] - 0s 839us/step - loss: 4700.0021 - mse: 43989692.0000 - mae: 4700.0020 - val_loss: 3986.7889 - val_mse: 28956902.0000 - val_mae: 3986.7891\n",
            "Epoch 36/500\n",
            "50/50 [==============================] - 0s 982us/step - loss: 4733.9378 - mse: 44321808.0000 - mae: 4733.9380 - val_loss: 3964.5970 - val_mse: 28769830.0000 - val_mae: 3964.5969\n",
            "Epoch 37/500\n",
            "50/50 [==============================] - 0s 943us/step - loss: 4720.0030 - mse: 43617072.0000 - mae: 4720.0029 - val_loss: 3940.8143 - val_mse: 28560804.0000 - val_mae: 3940.8140\n",
            "Epoch 38/500\n",
            "50/50 [==============================] - 0s 998us/step - loss: 4748.2707 - mse: 44256144.0000 - mae: 4748.2705 - val_loss: 3922.3741 - val_mse: 28399706.0000 - val_mae: 3922.3743\n",
            "Epoch 39/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4636.8123 - mse: 42821208.0000 - mae: 4636.8125 - val_loss: 3896.1230 - val_mse: 28171910.0000 - val_mae: 3896.1230\n",
            "Epoch 40/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4629.6699 - mse: 42850464.0000 - mae: 4629.6699 - val_loss: 3871.7462 - val_mse: 27953914.0000 - val_mae: 3871.7461\n",
            "Epoch 41/500\n",
            "50/50 [==============================] - 0s 765us/step - loss: 4600.7552 - mse: 42273516.0000 - mae: 4600.7554 - val_loss: 3846.9985 - val_mse: 27723488.0000 - val_mae: 3846.9985\n",
            "Epoch 42/500\n",
            "50/50 [==============================] - 0s 944us/step - loss: 4633.6089 - mse: 42474308.0000 - mae: 4633.6089 - val_loss: 3824.8760 - val_mse: 27512308.0000 - val_mae: 3824.8762\n",
            "Epoch 43/500\n",
            "50/50 [==============================] - 0s 853us/step - loss: 4562.5879 - mse: 41469276.0000 - mae: 4562.5879 - val_loss: 3800.4680 - val_mse: 27267590.0000 - val_mae: 3800.4680\n",
            "Epoch 44/500\n",
            "50/50 [==============================] - 0s 826us/step - loss: 4564.5188 - mse: 41984408.0000 - mae: 4564.5186 - val_loss: 3777.1615 - val_mse: 27034416.0000 - val_mae: 3777.1616\n",
            "Epoch 45/500\n",
            "50/50 [==============================] - 0s 828us/step - loss: 4552.3385 - mse: 41808084.0000 - mae: 4552.3389 - val_loss: 3757.0016 - val_mse: 26820598.0000 - val_mae: 3757.0015\n",
            "Epoch 46/500\n",
            "50/50 [==============================] - 0s 962us/step - loss: 4540.3916 - mse: 40760804.0000 - mae: 4540.3921 - val_loss: 3734.6618 - val_mse: 26584618.0000 - val_mae: 3734.6616\n",
            "Epoch 47/500\n",
            "50/50 [==============================] - 0s 969us/step - loss: 4564.0650 - mse: 41473224.0000 - mae: 4564.0649 - val_loss: 3717.3884 - val_mse: 26390230.0000 - val_mae: 3717.3882\n",
            "Epoch 48/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4552.4325 - mse: 41469912.0000 - mae: 4552.4326 - val_loss: 3699.4368 - val_mse: 26172564.0000 - val_mae: 3699.4368\n",
            "Epoch 49/500\n",
            "50/50 [==============================] - 0s 978us/step - loss: 4499.3745 - mse: 40652784.0000 - mae: 4499.3745 - val_loss: 3681.8312 - val_mse: 25960786.0000 - val_mae: 3681.8313\n",
            "Epoch 50/500\n",
            "50/50 [==============================] - 0s 984us/step - loss: 4500.0242 - mse: 39986660.0000 - mae: 4500.0244 - val_loss: 3663.9270 - val_mse: 25728776.0000 - val_mae: 3663.9270\n",
            "Epoch 51/500\n",
            "50/50 [==============================] - 0s 909us/step - loss: 4411.6256 - mse: 39932452.0000 - mae: 4411.6255 - val_loss: 3647.7957 - val_mse: 25500134.0000 - val_mae: 3647.7957\n",
            "Epoch 52/500\n",
            "50/50 [==============================] - 0s 853us/step - loss: 4476.7842 - mse: 39643460.0000 - mae: 4476.7842 - val_loss: 3632.3466 - val_mse: 25283094.0000 - val_mae: 3632.3469\n",
            "Epoch 53/500\n",
            "50/50 [==============================] - 0s 935us/step - loss: 4446.6170 - mse: 39253380.0000 - mae: 4446.6172 - val_loss: 3619.6170 - val_mse: 25105526.0000 - val_mae: 3619.6172\n",
            "Epoch 54/500\n",
            "50/50 [==============================] - 0s 918us/step - loss: 4426.4691 - mse: 40007428.0000 - mae: 4426.4692 - val_loss: 3603.3344 - val_mse: 24880580.0000 - val_mae: 3603.3345\n",
            "Epoch 55/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4404.2376 - mse: 38803968.0000 - mae: 4404.2373 - val_loss: 3586.5745 - val_mse: 24651190.0000 - val_mae: 3586.5742\n",
            "Epoch 56/500\n",
            "50/50 [==============================] - 0s 969us/step - loss: 4409.7439 - mse: 39636308.0000 - mae: 4409.7437 - val_loss: 3573.1298 - val_mse: 24468810.0000 - val_mae: 3573.1296\n",
            "Epoch 57/500\n",
            "50/50 [==============================] - 0s 957us/step - loss: 4351.0951 - mse: 38574724.0000 - mae: 4351.0952 - val_loss: 3560.7078 - val_mse: 24301432.0000 - val_mae: 3560.7078\n",
            "Epoch 58/500\n",
            "50/50 [==============================] - 0s 881us/step - loss: 4379.8154 - mse: 38569248.0000 - mae: 4379.8154 - val_loss: 3545.3683 - val_mse: 24096610.0000 - val_mae: 3545.3684\n",
            "Epoch 59/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4305.5192 - mse: 37817652.0000 - mae: 4305.5195 - val_loss: 3531.9801 - val_mse: 23919224.0000 - val_mae: 3531.9805\n",
            "Epoch 60/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4361.3829 - mse: 37565652.0000 - mae: 4361.3833 - val_loss: 3520.4560 - val_mse: 23767532.0000 - val_mae: 3520.4558\n",
            "Epoch 61/500\n",
            "50/50 [==============================] - 0s 938us/step - loss: 4341.9226 - mse: 37882072.0000 - mae: 4341.9229 - val_loss: 3507.0788 - val_mse: 23593006.0000 - val_mae: 3507.0789\n",
            "Epoch 62/500\n",
            "50/50 [==============================] - 0s 946us/step - loss: 4280.1398 - mse: 37918096.0000 - mae: 4280.1401 - val_loss: 3492.8221 - val_mse: 23408650.0000 - val_mae: 3492.8223\n",
            "Epoch 63/500\n",
            "50/50 [==============================] - 0s 829us/step - loss: 4322.9917 - mse: 37195284.0000 - mae: 4322.9917 - val_loss: 3479.1069 - val_mse: 23232770.0000 - val_mae: 3479.1069\n",
            "Epoch 64/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4284.6849 - mse: 36645756.0000 - mae: 4284.6851 - val_loss: 3467.1362 - val_mse: 23080398.0000 - val_mae: 3467.1360\n",
            "Epoch 65/500\n",
            "50/50 [==============================] - 0s 916us/step - loss: 4265.5299 - mse: 36466804.0000 - mae: 4265.5298 - val_loss: 3453.0146 - val_mse: 22902196.0000 - val_mae: 3453.0144\n",
            "Epoch 66/500\n",
            "50/50 [==============================] - 0s 918us/step - loss: 4177.7021 - mse: 35274620.0000 - mae: 4177.7017 - val_loss: 3439.1463 - val_mse: 22728694.0000 - val_mae: 3439.1460\n",
            "Epoch 67/500\n",
            "50/50 [==============================] - 0s 966us/step - loss: 4183.0058 - mse: 36354236.0000 - mae: 4183.0059 - val_loss: 3422.1286 - val_mse: 22518118.0000 - val_mae: 3422.1284\n",
            "Epoch 68/500\n",
            "50/50 [==============================] - 0s 870us/step - loss: 4254.1133 - mse: 36422528.0000 - mae: 4254.1133 - val_loss: 3408.7800 - val_mse: 22354440.0000 - val_mae: 3408.7798\n",
            "Epoch 69/500\n",
            "50/50 [==============================] - 0s 959us/step - loss: 4218.1937 - mse: 35187916.0000 - mae: 4218.1938 - val_loss: 3396.8687 - val_mse: 22209434.0000 - val_mae: 3396.8687\n",
            "Epoch 70/500\n",
            "50/50 [==============================] - 0s 938us/step - loss: 4292.4476 - mse: 36478832.0000 - mae: 4292.4478 - val_loss: 3389.5186 - val_mse: 22120378.0000 - val_mae: 3389.5188\n",
            "Epoch 71/500\n",
            "50/50 [==============================] - 0s 990us/step - loss: 4270.5819 - mse: 36238484.0000 - mae: 4270.5820 - val_loss: 3380.6169 - val_mse: 22013202.0000 - val_mae: 3380.6167\n",
            "Epoch 72/500\n",
            "50/50 [==============================] - 0s 979us/step - loss: 4289.6973 - mse: 36600224.0000 - mae: 4289.6973 - val_loss: 3373.6663 - val_mse: 21929868.0000 - val_mae: 3373.6665\n",
            "Epoch 73/500\n",
            "50/50 [==============================] - 0s 821us/step - loss: 4291.3575 - mse: 36302068.0000 - mae: 4291.3574 - val_loss: 3361.7153 - val_mse: 21787830.0000 - val_mae: 3361.7156\n",
            "Epoch 74/500\n",
            "50/50 [==============================] - 0s 867us/step - loss: 4190.3893 - mse: 34990464.0000 - mae: 4190.3892 - val_loss: 3349.2736 - val_mse: 21641128.0000 - val_mae: 3349.2734\n",
            "Epoch 75/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4223.5802 - mse: 34975372.0000 - mae: 4223.5806 - val_loss: 3336.5021 - val_mse: 21483498.0000 - val_mae: 3336.5024\n",
            "Epoch 76/500\n",
            "50/50 [==============================] - 0s 828us/step - loss: 4205.0782 - mse: 35150048.0000 - mae: 4205.0781 - val_loss: 3327.2564 - val_mse: 21351236.0000 - val_mae: 3327.2563\n",
            "Epoch 77/500\n",
            "50/50 [==============================] - 0s 924us/step - loss: 4122.7280 - mse: 33813132.0000 - mae: 4122.7280 - val_loss: 3315.4189 - val_mse: 21181588.0000 - val_mae: 3315.4187\n",
            "Epoch 78/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4290.3742 - mse: 35344376.0000 - mae: 4290.3740 - val_loss: 3308.0228 - val_mse: 21074452.0000 - val_mae: 3308.0227\n",
            "Epoch 79/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4240.8556 - mse: 34884896.0000 - mae: 4240.8555 - val_loss: 3302.4018 - val_mse: 20983940.0000 - val_mae: 3302.4019\n",
            "Epoch 80/500\n",
            "50/50 [==============================] - 0s 828us/step - loss: 4264.8891 - mse: 34723080.0000 - mae: 4264.8887 - val_loss: 3298.1659 - val_mse: 20916136.0000 - val_mae: 3298.1660\n",
            "Epoch 81/500\n",
            "50/50 [==============================] - 0s 881us/step - loss: 4125.9773 - mse: 33076654.0000 - mae: 4125.9771 - val_loss: 3290.9165 - val_mse: 20800616.0000 - val_mae: 3290.9165\n",
            "Epoch 82/500\n",
            "50/50 [==============================] - 0s 900us/step - loss: 4116.7051 - mse: 34817260.0000 - mae: 4116.7051 - val_loss: 3281.5874 - val_mse: 20653076.0000 - val_mae: 3281.5874\n",
            "Epoch 83/500\n",
            "50/50 [==============================] - 0s 798us/step - loss: 4218.1607 - mse: 34418800.0000 - mae: 4218.1606 - val_loss: 3276.6068 - val_mse: 20575110.0000 - val_mae: 3276.6069\n",
            "Epoch 84/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4307.2248 - mse: 35947880.0000 - mae: 4307.2246 - val_loss: 3271.0715 - val_mse: 20475138.0000 - val_mae: 3271.0715\n",
            "Epoch 85/500\n",
            "50/50 [==============================] - 0s 973us/step - loss: 4156.4131 - mse: 33816004.0000 - mae: 4156.4126 - val_loss: 3265.2076 - val_mse: 20369744.0000 - val_mae: 3265.2075\n",
            "Epoch 86/500\n",
            "50/50 [==============================] - 0s 975us/step - loss: 4108.4435 - mse: 33193948.0000 - mae: 4108.4434 - val_loss: 3259.3903 - val_mse: 20266176.0000 - val_mae: 3259.3901\n",
            "Epoch 87/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4199.9023 - mse: 34525552.0000 - mae: 4199.9023 - val_loss: 3254.8653 - val_mse: 20186556.0000 - val_mae: 3254.8652\n",
            "Epoch 88/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4035.1713 - mse: 31436784.0000 - mae: 4035.1711 - val_loss: 3247.2305 - val_mse: 20052656.0000 - val_mae: 3247.2305\n",
            "Epoch 89/500\n",
            "50/50 [==============================] - 0s 900us/step - loss: 4067.7762 - mse: 33496994.0000 - mae: 4067.7764 - val_loss: 3238.6626 - val_mse: 19903188.0000 - val_mae: 3238.6626\n",
            "Epoch 90/500\n",
            "50/50 [==============================] - 0s 849us/step - loss: 4146.6066 - mse: 33650572.0000 - mae: 4146.6064 - val_loss: 3233.5108 - val_mse: 19802458.0000 - val_mae: 3233.5110\n",
            "Epoch 91/500\n",
            "50/50 [==============================] - 0s 847us/step - loss: 4174.3746 - mse: 33842300.0000 - mae: 4174.3745 - val_loss: 3226.5146 - val_mse: 19678876.0000 - val_mae: 3226.5146\n",
            "Epoch 92/500\n",
            "50/50 [==============================] - 0s 871us/step - loss: 4261.5817 - mse: 33860100.0000 - mae: 4261.5815 - val_loss: 3204.7941 - val_mse: 19572124.0000 - val_mae: 3204.7942\n",
            "Epoch 93/500\n",
            "50/50 [==============================] - 0s 816us/step - loss: 4168.9948 - mse: 34296320.0000 - mae: 4168.9951 - val_loss: 3209.9336 - val_mse: 19550582.0000 - val_mae: 3209.9336\n",
            "Epoch 94/500\n",
            "50/50 [==============================] - 0s 823us/step - loss: 3949.1239 - mse: 30951436.0000 - mae: 3949.1238 - val_loss: 3112.9143 - val_mse: 19182452.0000 - val_mae: 3112.9143\n",
            "Epoch 95/500\n",
            "50/50 [==============================] - 0s 874us/step - loss: 4067.5177 - mse: 33534254.0000 - mae: 4067.5178 - val_loss: 3238.7325 - val_mse: 19181454.0000 - val_mae: 3238.7324\n",
            "Epoch 96/500\n",
            "50/50 [==============================] - 0s 813us/step - loss: 4035.6234 - mse: 32786716.0000 - mae: 4035.6235 - val_loss: 3056.1922 - val_mse: 18680030.0000 - val_mae: 3056.1921\n",
            "Epoch 97/500\n",
            "50/50 [==============================] - 0s 788us/step - loss: 3893.3892 - mse: 31042152.0000 - mae: 3893.3892 - val_loss: 3047.1211 - val_mse: 18434814.0000 - val_mae: 3047.1211\n",
            "Epoch 98/500\n",
            "50/50 [==============================] - 0s 890us/step - loss: 3766.4188 - mse: 29707244.0000 - mae: 3766.4187 - val_loss: 2967.6525 - val_mse: 18029274.0000 - val_mae: 2967.6526\n",
            "Epoch 99/500\n",
            "50/50 [==============================] - 0s 852us/step - loss: 3907.6181 - mse: 31849206.0000 - mae: 3907.6182 - val_loss: 3111.0731 - val_mse: 18069960.0000 - val_mae: 3111.0730\n",
            "Epoch 100/500\n",
            "50/50 [==============================] - 0s 985us/step - loss: 3770.7514 - mse: 29513428.0000 - mae: 3770.7512 - val_loss: 2912.5320 - val_mse: 17492036.0000 - val_mae: 2912.5320\n",
            "Epoch 101/500\n",
            "50/50 [==============================] - 0s 824us/step - loss: 3749.0912 - mse: 30242242.0000 - mae: 3749.0913 - val_loss: 2970.5623 - val_mse: 17378884.0000 - val_mae: 2970.5623\n",
            "Epoch 102/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3731.4271 - mse: 31081534.0000 - mae: 3731.4272 - val_loss: 3172.2036 - val_mse: 17535948.0000 - val_mae: 3172.2036\n",
            "Epoch 103/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3781.6095 - mse: 30477018.0000 - mae: 3781.6094 - val_loss: 3145.8522 - val_mse: 17278486.0000 - val_mae: 3145.8521\n",
            "Epoch 104/500\n",
            "50/50 [==============================] - 0s 894us/step - loss: 3679.4172 - mse: 28258156.0000 - mae: 3679.4172 - val_loss: 3037.0819 - val_mse: 16814052.0000 - val_mae: 3037.0820\n",
            "Epoch 105/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3607.4839 - mse: 29004468.0000 - mae: 3607.4836 - val_loss: 2931.1418 - val_mse: 16401440.0000 - val_mae: 2931.1418\n",
            "Epoch 106/500\n",
            "50/50 [==============================] - 0s 848us/step - loss: 3719.1494 - mse: 29540040.0000 - mae: 3719.1494 - val_loss: 3004.7884 - val_mse: 16330682.0000 - val_mae: 3004.7883\n",
            "Epoch 107/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3694.8891 - mse: 29275956.0000 - mae: 3694.8892 - val_loss: 2994.7843 - val_mse: 16118248.0000 - val_mae: 2994.7844\n",
            "Epoch 108/500\n",
            "50/50 [==============================] - 0s 800us/step - loss: 3447.9024 - mse: 26433920.0000 - mae: 3447.9026 - val_loss: 2770.8512 - val_mse: 15469526.0000 - val_mae: 2770.8511\n",
            "Epoch 109/500\n",
            "50/50 [==============================] - 0s 828us/step - loss: 3483.3286 - mse: 28389666.0000 - mae: 3483.3289 - val_loss: 2983.7473 - val_mse: 15637482.0000 - val_mae: 2983.7473\n",
            "Epoch 110/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3535.8297 - mse: 27972844.0000 - mae: 3535.8301 - val_loss: 2944.6850 - val_mse: 15327123.0000 - val_mae: 2944.6851\n",
            "Epoch 111/500\n",
            "50/50 [==============================] - 0s 963us/step - loss: 3443.2181 - mse: 26489076.0000 - mae: 3443.2180 - val_loss: 2731.6127 - val_mse: 14673555.0000 - val_mae: 2731.6128\n",
            "Epoch 112/500\n",
            "50/50 [==============================] - 0s 790us/step - loss: 3504.2236 - mse: 27721562.0000 - mae: 3504.2236 - val_loss: 2712.5843 - val_mse: 14435250.0000 - val_mae: 2712.5845\n",
            "Epoch 113/500\n",
            "50/50 [==============================] - 0s 859us/step - loss: 3316.4878 - mse: 24497006.0000 - mae: 3316.4880 - val_loss: 2776.0595 - val_mse: 14322235.0000 - val_mae: 2776.0593\n",
            "Epoch 114/500\n",
            "50/50 [==============================] - 0s 907us/step - loss: 3390.3954 - mse: 25827576.0000 - mae: 3390.3953 - val_loss: 2727.9508 - val_mse: 13993574.0000 - val_mae: 2727.9507\n",
            "Epoch 115/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3332.5320 - mse: 26067316.0000 - mae: 3332.5320 - val_loss: 2758.9129 - val_mse: 13856955.0000 - val_mae: 2758.9128\n",
            "Epoch 116/500\n",
            "50/50 [==============================] - 0s 955us/step - loss: 3349.3857 - mse: 24511240.0000 - mae: 3349.3857 - val_loss: 2683.5592 - val_mse: 13476826.0000 - val_mae: 2683.5591\n",
            "Epoch 117/500\n",
            "50/50 [==============================] - 0s 832us/step - loss: 3196.7442 - mse: 24614202.0000 - mae: 3196.7444 - val_loss: 2779.1966 - val_mse: 13525048.0000 - val_mae: 2779.1968\n",
            "Epoch 118/500\n",
            "50/50 [==============================] - 0s 845us/step - loss: 3200.8002 - mse: 23063342.0000 - mae: 3200.8000 - val_loss: 2506.4727 - val_mse: 12743880.0000 - val_mae: 2506.4727\n",
            "Epoch 119/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3273.9975 - mse: 23764276.0000 - mae: 3273.9976 - val_loss: 2719.0133 - val_mse: 12993198.0000 - val_mae: 2719.0132\n",
            "Epoch 120/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3301.2829 - mse: 24378864.0000 - mae: 3301.2827 - val_loss: 2721.6577 - val_mse: 12852999.0000 - val_mae: 2721.6577\n",
            "Epoch 121/500\n",
            "50/50 [==============================] - 0s 867us/step - loss: 3176.7943 - mse: 23471706.0000 - mae: 3176.7944 - val_loss: 2627.4120 - val_mse: 12451864.0000 - val_mae: 2627.4119\n",
            "Epoch 122/500\n",
            "50/50 [==============================] - 0s 815us/step - loss: 3169.3273 - mse: 22882146.0000 - mae: 3169.3274 - val_loss: 2635.7180 - val_mse: 12302088.0000 - val_mae: 2635.7180\n",
            "Epoch 123/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3101.9178 - mse: 22794484.0000 - mae: 3101.9177 - val_loss: 2550.7903 - val_mse: 11909594.0000 - val_mae: 2550.7903\n",
            "Epoch 124/500\n",
            "50/50 [==============================] - 0s 995us/step - loss: 2981.3984 - mse: 21930498.0000 - mae: 2981.3984 - val_loss: 2236.1224 - val_mse: 11222991.0000 - val_mae: 2236.1223\n",
            "Epoch 125/500\n",
            "50/50 [==============================] - 0s 990us/step - loss: 3158.6202 - mse: 23066120.0000 - mae: 3158.6204 - val_loss: 2486.0774 - val_mse: 11435558.0000 - val_mae: 2486.0774\n",
            "Epoch 126/500\n",
            "50/50 [==============================] - 0s 859us/step - loss: 2990.8810 - mse: 20253964.0000 - mae: 2990.8809 - val_loss: 2404.7621 - val_mse: 11074137.0000 - val_mae: 2404.7622\n",
            "Epoch 127/500\n",
            "50/50 [==============================] - 0s 852us/step - loss: 3038.1870 - mse: 20040968.0000 - mae: 3038.1873 - val_loss: 2482.3758 - val_mse: 11119298.0000 - val_mae: 2482.3757\n",
            "Epoch 128/500\n",
            "50/50 [==============================] - 0s 888us/step - loss: 2852.6986 - mse: 19653306.0000 - mae: 2852.6985 - val_loss: 2434.2113 - val_mse: 10850954.0000 - val_mae: 2434.2114\n",
            "Epoch 129/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2934.4959 - mse: 21517644.0000 - mae: 2934.4958 - val_loss: 2299.9704 - val_mse: 10387934.0000 - val_mae: 2299.9702\n",
            "Epoch 130/500\n",
            "50/50 [==============================] - 0s 980us/step - loss: 2951.6655 - mse: 21029990.0000 - mae: 2951.6653 - val_loss: 2335.8541 - val_mse: 10320898.0000 - val_mae: 2335.8540\n",
            "Epoch 131/500\n",
            "50/50 [==============================] - 0s 727us/step - loss: 3010.9139 - mse: 20675436.0000 - mae: 3010.9141 - val_loss: 2320.8034 - val_mse: 10122119.0000 - val_mae: 2320.8032\n",
            "Epoch 132/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2881.6659 - mse: 20529822.0000 - mae: 2881.6660 - val_loss: 2167.4887 - val_mse: 9625706.0000 - val_mae: 2167.4888\n",
            "Epoch 133/500\n",
            "50/50 [==============================] - 0s 878us/step - loss: 2887.2595 - mse: 19342152.0000 - mae: 2887.2593 - val_loss: 2288.5920 - val_mse: 9748256.0000 - val_mae: 2288.5920\n",
            "Epoch 134/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2950.0013 - mse: 21684848.0000 - mae: 2950.0012 - val_loss: 2173.3567 - val_mse: 9358359.0000 - val_mae: 2173.3567\n",
            "Epoch 135/500\n",
            "50/50 [==============================] - 0s 927us/step - loss: 2868.7371 - mse: 19451242.0000 - mae: 2868.7373 - val_loss: 2196.4548 - val_mse: 9309504.0000 - val_mae: 2196.4548\n",
            "Epoch 136/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2940.9330 - mse: 20653682.0000 - mae: 2940.9331 - val_loss: 2197.5826 - val_mse: 9215588.0000 - val_mae: 2197.5825\n",
            "Epoch 137/500\n",
            "50/50 [==============================] - 0s 815us/step - loss: 2917.1452 - mse: 21782594.0000 - mae: 2917.1450 - val_loss: 2164.5710 - val_mse: 9017806.0000 - val_mae: 2164.5710\n",
            "Epoch 138/500\n",
            "50/50 [==============================] - 0s 792us/step - loss: 2670.9159 - mse: 16712431.0000 - mae: 2670.9155 - val_loss: 2217.3100 - val_mse: 9087135.0000 - val_mae: 2217.3101\n",
            "Epoch 139/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2723.3255 - mse: 17769360.0000 - mae: 2723.3252 - val_loss: 1954.2597 - val_mse: 8392947.0000 - val_mae: 1954.2598\n",
            "Epoch 140/500\n",
            "50/50 [==============================] - 0s 994us/step - loss: 2786.1653 - mse: 19618246.0000 - mae: 2786.1650 - val_loss: 2114.7350 - val_mse: 8567107.0000 - val_mae: 2114.7349\n",
            "Epoch 141/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2612.9544 - mse: 16368361.0000 - mae: 2612.9543 - val_loss: 2074.6558 - val_mse: 8332032.0000 - val_mae: 2074.6558\n",
            "Epoch 142/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2762.4967 - mse: 19144332.0000 - mae: 2762.4966 - val_loss: 2044.5369 - val_mse: 8141786.5000 - val_mae: 2044.5369\n",
            "Epoch 143/500\n",
            "50/50 [==============================] - 0s 936us/step - loss: 2602.2524 - mse: 15978222.0000 - mae: 2602.2524 - val_loss: 1896.6755 - val_mse: 7723970.5000 - val_mae: 1896.6754\n",
            "Epoch 144/500\n",
            "50/50 [==============================] - 0s 975us/step - loss: 2907.3439 - mse: 19166266.0000 - mae: 2907.3438 - val_loss: 2009.9034 - val_mse: 7849699.0000 - val_mae: 2009.9033\n",
            "Epoch 145/500\n",
            "50/50 [==============================] - 0s 908us/step - loss: 2494.1678 - mse: 14957820.0000 - mae: 2494.1677 - val_loss: 2027.2544 - val_mse: 7790183.0000 - val_mae: 2027.2543\n",
            "Epoch 146/500\n",
            "50/50 [==============================] - 0s 891us/step - loss: 2435.5710 - mse: 14563203.0000 - mae: 2435.5710 - val_loss: 2077.8978 - val_mse: 7849609.0000 - val_mae: 2077.8979\n",
            "Epoch 147/500\n",
            "50/50 [==============================] - 0s 820us/step - loss: 2617.9585 - mse: 17210506.0000 - mae: 2617.9585 - val_loss: 1911.3839 - val_mse: 7327424.0000 - val_mae: 1911.3840\n",
            "Epoch 148/500\n",
            "50/50 [==============================] - 0s 985us/step - loss: 2541.3428 - mse: 16455478.0000 - mae: 2541.3428 - val_loss: 1928.2188 - val_mse: 7284462.5000 - val_mae: 1928.2188\n",
            "Epoch 149/500\n",
            "50/50 [==============================] - 0s 947us/step - loss: 2693.9204 - mse: 18057252.0000 - mae: 2693.9204 - val_loss: 2013.7381 - val_mse: 7426401.5000 - val_mae: 2013.7380\n",
            "Epoch 150/500\n",
            "50/50 [==============================] - 0s 826us/step - loss: 2362.9447 - mse: 13825503.0000 - mae: 2362.9446 - val_loss: 1899.9111 - val_mse: 7005861.0000 - val_mae: 1899.9111\n",
            "Epoch 151/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2639.0415 - mse: 14886350.0000 - mae: 2639.0415 - val_loss: 1956.4186 - val_mse: 7067541.5000 - val_mae: 1956.4186\n",
            "Epoch 152/500\n",
            "50/50 [==============================] - 0s 813us/step - loss: 2377.9287 - mse: 14843165.0000 - mae: 2377.9287 - val_loss: 1916.7500 - val_mse: 6885816.0000 - val_mae: 1916.7500\n",
            "Epoch 153/500\n",
            "50/50 [==============================] - 0s 938us/step - loss: 2672.9799 - mse: 17112094.0000 - mae: 2672.9797 - val_loss: 1823.0865 - val_mse: 6569443.0000 - val_mae: 1823.0865\n",
            "Epoch 154/500\n",
            "50/50 [==============================] - 0s 746us/step - loss: 2530.1353 - mse: 14664346.0000 - mae: 2530.1353 - val_loss: 1965.4570 - val_mse: 6870745.5000 - val_mae: 1965.4570\n",
            "Epoch 155/500\n",
            "50/50 [==============================] - 0s 789us/step - loss: 2513.0811 - mse: 15492945.0000 - mae: 2513.0811 - val_loss: 1920.7246 - val_mse: 6639153.0000 - val_mae: 1920.7246\n",
            "Epoch 156/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2619.9693 - mse: 15614258.0000 - mae: 2619.9695 - val_loss: 1866.9564 - val_mse: 6388807.0000 - val_mae: 1866.9563\n",
            "Epoch 157/500\n",
            "50/50 [==============================] - 0s 976us/step - loss: 2478.5143 - mse: 17427994.0000 - mae: 2478.5142 - val_loss: 1895.1457 - val_mse: 6390684.5000 - val_mae: 1895.1458\n",
            "Epoch 158/500\n",
            "50/50 [==============================] - 0s 992us/step - loss: 2322.6643 - mse: 13459872.0000 - mae: 2322.6643 - val_loss: 1796.3540 - val_mse: 6036581.5000 - val_mae: 1796.3541\n",
            "Epoch 159/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2556.6393 - mse: 15241742.0000 - mae: 2556.6392 - val_loss: 1838.2558 - val_mse: 6052246.0000 - val_mae: 1838.2559\n",
            "Epoch 160/500\n",
            "50/50 [==============================] - 0s 948us/step - loss: 2401.2335 - mse: 14582102.0000 - mae: 2401.2334 - val_loss: 1792.5894 - val_mse: 5862565.0000 - val_mae: 1792.5895\n",
            "Epoch 161/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2372.1229 - mse: 12319291.0000 - mae: 2372.1230 - val_loss: 1707.0966 - val_mse: 5567504.0000 - val_mae: 1707.0967\n",
            "Epoch 162/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2326.2751 - mse: 13707116.0000 - mae: 2326.2749 - val_loss: 1761.8246 - val_mse: 5608177.0000 - val_mae: 1761.8246\n",
            "Epoch 163/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2438.6625 - mse: 13570772.0000 - mae: 2438.6626 - val_loss: 1696.8744 - val_mse: 5393319.0000 - val_mae: 1696.8743\n",
            "Epoch 164/500\n",
            "50/50 [==============================] - 0s 865us/step - loss: 2200.5018 - mse: 11814132.0000 - mae: 2200.5020 - val_loss: 1762.5803 - val_mse: 5475277.0000 - val_mae: 1762.5803\n",
            "Epoch 165/500\n",
            "50/50 [==============================] - 0s 935us/step - loss: 2557.5697 - mse: 15360739.0000 - mae: 2557.5696 - val_loss: 1794.4966 - val_mse: 5517119.0000 - val_mae: 1794.4965\n",
            "Epoch 166/500\n",
            "50/50 [==============================] - 0s 772us/step - loss: 2119.2476 - mse: 11773499.0000 - mae: 2119.2476 - val_loss: 1800.5529 - val_mse: 5476733.0000 - val_mae: 1800.5530\n",
            "Epoch 167/500\n",
            "50/50 [==============================] - 0s 941us/step - loss: 2184.6241 - mse: 12588932.0000 - mae: 2184.6243 - val_loss: 1559.6323 - val_mse: 4910133.0000 - val_mae: 1559.6324\n",
            "Epoch 168/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2383.8797 - mse: 15241385.0000 - mae: 2383.8796 - val_loss: 1790.9503 - val_mse: 5333913.0000 - val_mae: 1790.9502\n",
            "Epoch 169/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2159.8050 - mse: 10299814.0000 - mae: 2159.8049 - val_loss: 1735.2334 - val_mse: 5103399.0000 - val_mae: 1735.2334\n",
            "Epoch 170/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2219.0026 - mse: 13791608.0000 - mae: 2219.0027 - val_loss: 1687.6153 - val_mse: 4920963.0000 - val_mae: 1687.6152\n",
            "Epoch 171/500\n",
            "50/50 [==============================] - 0s 822us/step - loss: 2335.0687 - mse: 13571183.0000 - mae: 2335.0688 - val_loss: 1662.6415 - val_mse: 4842709.0000 - val_mae: 1662.6416\n",
            "Epoch 172/500\n",
            "50/50 [==============================] - 0s 943us/step - loss: 1979.1284 - mse: 9301356.0000 - mae: 1979.1283 - val_loss: 1684.7431 - val_mse: 4825320.0000 - val_mae: 1684.7429\n",
            "Epoch 173/500\n",
            "50/50 [==============================] - 0s 987us/step - loss: 2223.0767 - mse: 13863590.0000 - mae: 2223.0767 - val_loss: 1730.1485 - val_mse: 4917388.5000 - val_mae: 1730.1484\n",
            "Epoch 174/500\n",
            "50/50 [==============================] - 0s 822us/step - loss: 2239.6874 - mse: 12465979.0000 - mae: 2239.6875 - val_loss: 1642.7977 - val_mse: 4635766.5000 - val_mae: 1642.7976\n",
            "Epoch 175/500\n",
            "50/50 [==============================] - 0s 841us/step - loss: 2377.6767 - mse: 15007724.0000 - mae: 2377.6768 - val_loss: 1648.7396 - val_mse: 4615997.5000 - val_mae: 1648.7395\n",
            "Epoch 176/500\n",
            "50/50 [==============================] - 0s 870us/step - loss: 2372.4889 - mse: 13883722.0000 - mae: 2372.4888 - val_loss: 1626.5367 - val_mse: 4505379.5000 - val_mae: 1626.5367\n",
            "Epoch 177/500\n",
            "50/50 [==============================] - 0s 874us/step - loss: 2204.4408 - mse: 12535638.0000 - mae: 2204.4409 - val_loss: 1573.7092 - val_mse: 4402618.0000 - val_mae: 1573.7092\n",
            "Epoch 178/500\n",
            "50/50 [==============================] - 0s 926us/step - loss: 2194.9072 - mse: 12332861.0000 - mae: 2194.9072 - val_loss: 1436.8579 - val_mse: 4040256.0000 - val_mae: 1436.8578\n",
            "Epoch 179/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1970.4093 - mse: 10232694.0000 - mae: 1970.4092 - val_loss: 1568.1919 - val_mse: 4306419.5000 - val_mae: 1568.1920\n",
            "Epoch 180/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2483.4336 - mse: 16381046.0000 - mae: 2483.4333 - val_loss: 1424.2429 - val_mse: 3937177.2500 - val_mae: 1424.2429\n",
            "Epoch 181/500\n",
            "50/50 [==============================] - 0s 874us/step - loss: 2148.4410 - mse: 11328426.0000 - mae: 2148.4412 - val_loss: 1570.3558 - val_mse: 4151938.0000 - val_mae: 1570.3557\n",
            "Epoch 182/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2166.6310 - mse: 11864873.0000 - mae: 2166.6309 - val_loss: 1670.9453 - val_mse: 4424338.5000 - val_mae: 1670.9453\n",
            "Epoch 183/500\n",
            "50/50 [==============================] - 0s 888us/step - loss: 1907.7067 - mse: 8896979.0000 - mae: 1907.7067 - val_loss: 1528.6269 - val_mse: 3926947.2500 - val_mae: 1528.6270\n",
            "Epoch 184/500\n",
            "50/50 [==============================] - 0s 885us/step - loss: 2106.1220 - mse: 11419290.0000 - mae: 2106.1218 - val_loss: 1561.5512 - val_mse: 3971098.7500 - val_mae: 1561.5511\n",
            "Epoch 185/500\n",
            "50/50 [==============================] - 0s 897us/step - loss: 2062.2881 - mse: 11338962.0000 - mae: 2062.2881 - val_loss: 1498.5583 - val_mse: 3790750.7500 - val_mae: 1498.5583\n",
            "Epoch 186/500\n",
            "50/50 [==============================] - 0s 854us/step - loss: 2151.9986 - mse: 12314409.0000 - mae: 2151.9985 - val_loss: 1806.1395 - val_mse: 4760704.0000 - val_mae: 1806.1394\n",
            "Epoch 187/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1850.0103 - mse: 10666484.0000 - mae: 1850.0103 - val_loss: 1435.4671 - val_mse: 3716058.0000 - val_mae: 1435.4670\n",
            "Epoch 188/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1939.1515 - mse: 10575360.0000 - mae: 1939.1516 - val_loss: 1747.3723 - val_mse: 4498835.0000 - val_mae: 1747.3723\n",
            "Epoch 189/500\n",
            "50/50 [==============================] - 0s 809us/step - loss: 2175.6185 - mse: 11219259.0000 - mae: 2175.6187 - val_loss: 1656.9002 - val_mse: 4056699.2500 - val_mae: 1656.9001\n",
            "Epoch 190/500\n",
            "50/50 [==============================] - 0s 891us/step - loss: 2105.5377 - mse: 12408376.0000 - mae: 2105.5376 - val_loss: 1674.2145 - val_mse: 4029901.2500 - val_mae: 1674.2145\n",
            "Epoch 191/500\n",
            "50/50 [==============================] - 0s 813us/step - loss: 1994.6833 - mse: 9033498.0000 - mae: 1994.6831 - val_loss: 1512.5229 - val_mse: 3586859.2500 - val_mae: 1512.5229\n",
            "Epoch 192/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2028.8200 - mse: 10635672.0000 - mae: 2028.8199 - val_loss: 1690.6435 - val_mse: 4005026.7500 - val_mae: 1690.6433\n",
            "Epoch 193/500\n",
            "50/50 [==============================] - 0s 915us/step - loss: 2069.0636 - mse: 9217825.0000 - mae: 2069.0635 - val_loss: 1786.3980 - val_mse: 4395453.5000 - val_mae: 1786.3981\n",
            "Epoch 194/500\n",
            "50/50 [==============================] - 0s 815us/step - loss: 1830.0758 - mse: 7144484.0000 - mae: 1830.0758 - val_loss: 1625.0251 - val_mse: 3770506.7500 - val_mae: 1625.0251\n",
            "Epoch 195/500\n",
            "50/50 [==============================] - 0s 804us/step - loss: 1795.1184 - mse: 7771309.0000 - mae: 1795.1184 - val_loss: 1682.8585 - val_mse: 3887724.0000 - val_mae: 1682.8586\n",
            "Epoch 196/500\n",
            "50/50 [==============================] - 0s 925us/step - loss: 1783.1706 - mse: 7917008.5000 - mae: 1783.1707 - val_loss: 1683.7142 - val_mse: 3854494.0000 - val_mae: 1683.7141\n",
            "Epoch 197/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1845.3083 - mse: 8196087.5000 - mae: 1845.3082 - val_loss: 1709.9871 - val_mse: 3911340.0000 - val_mae: 1709.9871\n",
            "Epoch 198/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2308.9912 - mse: 13989944.0000 - mae: 2308.9912 - val_loss: 1701.2121 - val_mse: 3934442.7500 - val_mae: 1701.2122\n",
            "Epoch 199/500\n",
            "50/50 [==============================] - 0s 904us/step - loss: 1883.8038 - mse: 7821862.5000 - mae: 1883.8037 - val_loss: 1644.4826 - val_mse: 3661579.2500 - val_mae: 1644.4827\n",
            "Epoch 200/500\n",
            "50/50 [==============================] - 0s 906us/step - loss: 1831.3057 - mse: 8418709.0000 - mae: 1831.3057 - val_loss: 1537.3477 - val_mse: 3302774.5000 - val_mae: 1537.3478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60 samples, validate on 60 samples\n",
            "Epoch 1/500\n",
            "60/60 [==============================] - 2s 36ms/step - loss: 8366.7909 - mse: 142962656.0000 - mae: 8366.7910 - val_loss: 12068.7657 - val_mse: 298673728.0000 - val_mae: 12068.7656\n",
            "Epoch 2/500\n",
            "60/60 [==============================] - 0s 987us/step - loss: 8366.3043 - mse: 142958656.0000 - mae: 8366.3037 - val_loss: 12066.5468 - val_mse: 298652832.0000 - val_mae: 12066.5459\n",
            "Epoch 3/500\n",
            "60/60 [==============================] - 0s 808us/step - loss: 8361.3346 - mse: 142921120.0000 - mae: 8361.3340 - val_loss: 12057.5299 - val_mse: 298560864.0000 - val_mae: 12057.5293\n",
            "Epoch 4/500\n",
            "60/60 [==============================] - 0s 793us/step - loss: 8350.0826 - mse: 142826192.0000 - mae: 8350.0820 - val_loss: 12045.3015 - val_mse: 298406176.0000 - val_mae: 12045.3008\n",
            "Epoch 5/500\n",
            "60/60 [==============================] - 0s 816us/step - loss: 8339.5107 - mse: 142714944.0000 - mae: 8339.5107 - val_loss: 12030.6914 - val_mse: 298168192.0000 - val_mae: 12030.6914\n",
            "Epoch 6/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 8323.7922 - mse: 142561776.0000 - mae: 8323.7930 - val_loss: 12012.1699 - val_mse: 297840160.0000 - val_mae: 12012.1699\n",
            "Epoch 7/500\n",
            "60/60 [==============================] - 0s 780us/step - loss: 8300.4186 - mse: 142258800.0000 - mae: 8300.4189 - val_loss: 11989.4010 - val_mse: 297415168.0000 - val_mae: 11989.4014\n",
            "Epoch 8/500\n",
            "60/60 [==============================] - 0s 746us/step - loss: 8279.3937 - mse: 141958496.0000 - mae: 8279.3936 - val_loss: 11964.7956 - val_mse: 296927584.0000 - val_mae: 11964.7959\n",
            "Epoch 9/500\n",
            "60/60 [==============================] - 0s 865us/step - loss: 8250.1861 - mse: 141493072.0000 - mae: 8250.1865 - val_loss: 11936.1701 - val_mse: 296346272.0000 - val_mae: 11936.1699\n",
            "Epoch 10/500\n",
            "60/60 [==============================] - 0s 911us/step - loss: 8235.9937 - mse: 141150320.0000 - mae: 8235.9941 - val_loss: 11910.2752 - val_mse: 295796544.0000 - val_mae: 11910.2754\n",
            "Epoch 11/500\n",
            "60/60 [==============================] - 0s 801us/step - loss: 8210.2945 - mse: 140664016.0000 - mae: 8210.2949 - val_loss: 11879.6488 - val_mse: 295147296.0000 - val_mae: 11879.6494\n",
            "Epoch 12/500\n",
            "60/60 [==============================] - 0s 841us/step - loss: 8180.1736 - mse: 140313760.0000 - mae: 8180.1738 - val_loss: 11845.5937 - val_mse: 294427520.0000 - val_mae: 11845.5938\n",
            "Epoch 13/500\n",
            "60/60 [==============================] - 0s 828us/step - loss: 8152.4490 - mse: 139595344.0000 - mae: 8152.4487 - val_loss: 11812.7813 - val_mse: 293713184.0000 - val_mae: 11812.7812\n",
            "Epoch 14/500\n",
            "60/60 [==============================] - 0s 782us/step - loss: 8124.9872 - mse: 139027168.0000 - mae: 8124.9873 - val_loss: 11776.7603 - val_mse: 292927072.0000 - val_mae: 11776.7607\n",
            "Epoch 15/500\n",
            "60/60 [==============================] - 0s 924us/step - loss: 8097.1661 - mse: 138582944.0000 - mae: 8097.1660 - val_loss: 11737.6458 - val_mse: 292075456.0000 - val_mae: 11737.6455\n",
            "Epoch 16/500\n",
            "60/60 [==============================] - 0s 817us/step - loss: 8071.6183 - mse: 138205744.0000 - mae: 8071.6182 - val_loss: 11699.1536 - val_mse: 291234880.0000 - val_mae: 11699.1543\n",
            "Epoch 17/500\n",
            "60/60 [==============================] - 0s 769us/step - loss: 8019.1937 - mse: 136974864.0000 - mae: 8019.1934 - val_loss: 11654.6936 - val_mse: 290259104.0000 - val_mae: 11654.6934\n",
            "Epoch 18/500\n",
            "60/60 [==============================] - 0s 816us/step - loss: 8008.5749 - mse: 136222528.0000 - mae: 8008.5752 - val_loss: 11614.2739 - val_mse: 289315392.0000 - val_mae: 11614.2744\n",
            "Epoch 19/500\n",
            "60/60 [==============================] - 0s 767us/step - loss: 7968.5412 - mse: 135973792.0000 - mae: 7968.5405 - val_loss: 11571.0831 - val_mse: 288297312.0000 - val_mae: 11571.0830\n",
            "Epoch 20/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 7929.8429 - mse: 135037312.0000 - mae: 7929.8428 - val_loss: 11521.2422 - val_mse: 286964192.0000 - val_mae: 11521.2432\n",
            "Epoch 21/500\n",
            "60/60 [==============================] - 0s 773us/step - loss: 7915.8952 - mse: 134211864.0000 - mae: 7915.8950 - val_loss: 11475.7193 - val_mse: 285775552.0000 - val_mae: 11475.7188\n",
            "Epoch 22/500\n",
            "60/60 [==============================] - 0s 779us/step - loss: 7863.8351 - mse: 133056368.0000 - mae: 7863.8350 - val_loss: 11427.9853 - val_mse: 284473216.0000 - val_mae: 11427.9854\n",
            "Epoch 23/500\n",
            "60/60 [==============================] - 0s 798us/step - loss: 7850.5242 - mse: 132279480.0000 - mae: 7850.5244 - val_loss: 11374.8941 - val_mse: 282743040.0000 - val_mae: 11374.8936\n",
            "Epoch 24/500\n",
            "60/60 [==============================] - 0s 916us/step - loss: 7796.3979 - mse: 130809528.0000 - mae: 7796.3979 - val_loss: 11330.2031 - val_mse: 281447744.0000 - val_mae: 11330.2041\n",
            "Epoch 25/500\n",
            "60/60 [==============================] - 0s 976us/step - loss: 7735.0104 - mse: 129593568.0000 - mae: 7735.0103 - val_loss: 11283.6752 - val_mse: 280049760.0000 - val_mae: 11283.6748\n",
            "Epoch 26/500\n",
            "60/60 [==============================] - 0s 785us/step - loss: 7738.3350 - mse: 130105976.0000 - mae: 7738.3354 - val_loss: 11245.6274 - val_mse: 278855264.0000 - val_mae: 11245.6270\n",
            "Epoch 27/500\n",
            "60/60 [==============================] - 0s 803us/step - loss: 7707.3597 - mse: 129055256.0000 - mae: 7707.3594 - val_loss: 11205.9735 - val_mse: 277528480.0000 - val_mae: 11205.9727\n",
            "Epoch 28/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 7652.5227 - mse: 127643872.0000 - mae: 7652.5229 - val_loss: 11161.9121 - val_mse: 275982656.0000 - val_mae: 11161.9121\n",
            "Epoch 29/500\n",
            "60/60 [==============================] - 0s 855us/step - loss: 7604.4160 - mse: 126163464.0000 - mae: 7604.4155 - val_loss: 11129.5942 - val_mse: 274874144.0000 - val_mae: 11129.5938\n",
            "Epoch 30/500\n",
            "60/60 [==============================] - 0s 783us/step - loss: 7590.9742 - mse: 126998616.0000 - mae: 7590.9741 - val_loss: 11088.8638 - val_mse: 273538176.0000 - val_mae: 11088.8633\n",
            "Epoch 31/500\n",
            "60/60 [==============================] - 0s 865us/step - loss: 7555.2699 - mse: 125788264.0000 - mae: 7555.2705 - val_loss: 11062.3107 - val_mse: 272590048.0000 - val_mae: 11062.3105\n",
            "Epoch 32/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 7590.2424 - mse: 125446232.0000 - mae: 7590.2422 - val_loss: 11024.8360 - val_mse: 271287328.0000 - val_mae: 11024.8350\n",
            "Epoch 33/500\n",
            "60/60 [==============================] - 0s 833us/step - loss: 7499.8110 - mse: 124537136.0000 - mae: 7499.8110 - val_loss: 10983.0699 - val_mse: 269927616.0000 - val_mae: 10983.0693\n",
            "Epoch 34/500\n",
            "60/60 [==============================] - 0s 811us/step - loss: 7542.6490 - mse: 124779848.0000 - mae: 7542.6484 - val_loss: 10947.9213 - val_mse: 268772544.0000 - val_mae: 10947.9219\n",
            "Epoch 35/500\n",
            "60/60 [==============================] - 0s 767us/step - loss: 7503.7188 - mse: 123135912.0000 - mae: 7503.7188 - val_loss: 10914.4415 - val_mse: 267650080.0000 - val_mae: 10914.4414\n",
            "Epoch 36/500\n",
            "60/60 [==============================] - 0s 861us/step - loss: 7433.9939 - mse: 122788240.0000 - mae: 7433.9937 - val_loss: 10873.3901 - val_mse: 266358304.0000 - val_mae: 10873.3896\n",
            "Epoch 37/500\n",
            "60/60 [==============================] - 0s 800us/step - loss: 7342.2538 - mse: 119509752.0000 - mae: 7342.2534 - val_loss: 10832.2185 - val_mse: 265007600.0000 - val_mae: 10832.2188\n",
            "Epoch 38/500\n",
            "60/60 [==============================] - 0s 806us/step - loss: 7299.4392 - mse: 119399984.0000 - mae: 7299.4390 - val_loss: 10782.7998 - val_mse: 263465952.0000 - val_mae: 10782.7998\n",
            "Epoch 39/500\n",
            "60/60 [==============================] - 0s 773us/step - loss: 7320.6823 - mse: 119071848.0000 - mae: 7320.6821 - val_loss: 10735.2397 - val_mse: 261995248.0000 - val_mae: 10735.2393\n",
            "Epoch 40/500\n",
            "60/60 [==============================] - 0s 803us/step - loss: 7267.1813 - mse: 117770296.0000 - mae: 7267.1812 - val_loss: 10693.1622 - val_mse: 260634384.0000 - val_mae: 10693.1621\n",
            "Epoch 41/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 7285.1250 - mse: 118543088.0000 - mae: 7285.1250 - val_loss: 10739.7383 - val_mse: 259788640.0000 - val_mae: 10739.7383\n",
            "Epoch 42/500\n",
            "60/60 [==============================] - 0s 940us/step - loss: 7228.0625 - mse: 117354040.0000 - mae: 7228.0625 - val_loss: 10609.5745 - val_mse: 257912384.0000 - val_mae: 10609.5752\n",
            "Epoch 43/500\n",
            "60/60 [==============================] - 0s 943us/step - loss: 7188.7833 - mse: 114504968.0000 - mae: 7188.7837 - val_loss: 10569.1326 - val_mse: 256556032.0000 - val_mae: 10569.1338\n",
            "Epoch 44/500\n",
            "60/60 [==============================] - 0s 741us/step - loss: 7013.2108 - mse: 111610744.0000 - mae: 7013.2109 - val_loss: 10507.6506 - val_mse: 254732976.0000 - val_mae: 10507.6504\n",
            "Epoch 45/500\n",
            "60/60 [==============================] - 0s 891us/step - loss: 7029.6483 - mse: 112502696.0000 - mae: 7029.6479 - val_loss: 10449.3888 - val_mse: 253019792.0000 - val_mae: 10449.3887\n",
            "Epoch 46/500\n",
            "60/60 [==============================] - 0s 855us/step - loss: 6964.9567 - mse: 110237184.0000 - mae: 6964.9565 - val_loss: 10409.9408 - val_mse: 251547408.0000 - val_mae: 10409.9404\n",
            "Epoch 47/500\n",
            "60/60 [==============================] - 0s 796us/step - loss: 7100.1129 - mse: 113323944.0000 - mae: 7100.1128 - val_loss: 10391.9769 - val_mse: 250165264.0000 - val_mae: 10391.9775\n",
            "Epoch 48/500\n",
            "60/60 [==============================] - 0s 831us/step - loss: 7061.3780 - mse: 112944712.0000 - mae: 7061.3779 - val_loss: 10295.0970 - val_mse: 248266192.0000 - val_mae: 10295.0967\n",
            "Epoch 49/500\n",
            "60/60 [==============================] - 0s 884us/step - loss: 6919.7212 - mse: 110209160.0000 - mae: 6919.7212 - val_loss: 10230.2215 - val_mse: 246401872.0000 - val_mae: 10230.2207\n",
            "Epoch 50/500\n",
            "60/60 [==============================] - 0s 818us/step - loss: 6768.3699 - mse: 106673792.0000 - mae: 6768.3701 - val_loss: 10165.5331 - val_mse: 244414416.0000 - val_mae: 10165.5332\n",
            "Epoch 51/500\n",
            "60/60 [==============================] - 0s 819us/step - loss: 6817.0401 - mse: 106033112.0000 - mae: 6817.0396 - val_loss: 10100.4955 - val_mse: 242495632.0000 - val_mae: 10100.4961\n",
            "Epoch 52/500\n",
            "60/60 [==============================] - 0s 802us/step - loss: 6700.7021 - mse: 103741984.0000 - mae: 6700.7017 - val_loss: 10033.1533 - val_mse: 240552544.0000 - val_mae: 10033.1533\n",
            "Epoch 53/500\n",
            "60/60 [==============================] - 0s 928us/step - loss: 6686.2228 - mse: 103867576.0000 - mae: 6686.2222 - val_loss: 10057.5500 - val_mse: 238715840.0000 - val_mae: 10057.5498\n",
            "Epoch 54/500\n",
            "60/60 [==============================] - 0s 863us/step - loss: 6697.0648 - mse: 104210104.0000 - mae: 6697.0645 - val_loss: 9905.6450 - val_mse: 236825264.0000 - val_mae: 9905.6455\n",
            "Epoch 55/500\n",
            "60/60 [==============================] - 0s 838us/step - loss: 6373.1140 - mse: 98676456.0000 - mae: 6373.1147 - val_loss: 9866.9159 - val_mse: 234547856.0000 - val_mae: 9866.9170\n",
            "Epoch 56/500\n",
            "60/60 [==============================] - 0s 824us/step - loss: 6363.2114 - mse: 98770272.0000 - mae: 6363.2119 - val_loss: 9842.1719 - val_mse: 232436160.0000 - val_mae: 9842.1719\n",
            "Epoch 57/500\n",
            "60/60 [==============================] - 0s 841us/step - loss: 6497.7775 - mse: 99704184.0000 - mae: 6497.7769 - val_loss: 9677.8820 - val_mse: 230271920.0000 - val_mae: 9677.8809\n",
            "Epoch 58/500\n",
            "60/60 [==============================] - 0s 859us/step - loss: 6451.4647 - mse: 100119960.0000 - mae: 6451.4644 - val_loss: 9641.0704 - val_mse: 228261040.0000 - val_mae: 9641.0713\n",
            "Epoch 59/500\n",
            "60/60 [==============================] - 0s 898us/step - loss: 6213.8748 - mse: 94064352.0000 - mae: 6213.8745 - val_loss: 9527.2065 - val_mse: 226085120.0000 - val_mae: 9527.2061\n",
            "Epoch 60/500\n",
            "60/60 [==============================] - 0s 880us/step - loss: 6273.1492 - mse: 94215024.0000 - mae: 6273.1494 - val_loss: 9523.3310 - val_mse: 224291824.0000 - val_mae: 9523.3311\n",
            "Epoch 61/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 6255.6525 - mse: 95796504.0000 - mae: 6255.6519 - val_loss: 9407.1671 - val_mse: 221891648.0000 - val_mae: 9407.1670\n",
            "Epoch 62/500\n",
            "60/60 [==============================] - 0s 867us/step - loss: 6092.1949 - mse: 94448672.0000 - mae: 6092.1953 - val_loss: 9407.4789 - val_mse: 219866944.0000 - val_mae: 9407.4795\n",
            "Epoch 63/500\n",
            "60/60 [==============================] - 0s 783us/step - loss: 6022.7301 - mse: 91570872.0000 - mae: 6022.7295 - val_loss: 9379.4290 - val_mse: 217705568.0000 - val_mae: 9379.4287\n",
            "Epoch 64/500\n",
            "60/60 [==============================] - 0s 838us/step - loss: 5801.1161 - mse: 85873184.0000 - mae: 5801.1157 - val_loss: 9341.9814 - val_mse: 215496768.0000 - val_mae: 9341.9814\n",
            "Epoch 65/500\n",
            "60/60 [==============================] - 0s 892us/step - loss: 5924.8100 - mse: 89082672.0000 - mae: 5924.8101 - val_loss: 9065.2492 - val_mse: 212956752.0000 - val_mae: 9065.2490\n",
            "Epoch 66/500\n",
            "60/60 [==============================] - 0s 953us/step - loss: 5892.0282 - mse: 88247336.0000 - mae: 5892.0283 - val_loss: 9021.1425 - val_mse: 210923616.0000 - val_mae: 9021.1426\n",
            "Epoch 67/500\n",
            "60/60 [==============================] - 0s 856us/step - loss: 5692.6071 - mse: 86301512.0000 - mae: 5692.6074 - val_loss: 9054.5028 - val_mse: 208915824.0000 - val_mae: 9054.5029\n",
            "Epoch 68/500\n",
            "60/60 [==============================] - 0s 773us/step - loss: 5627.4285 - mse: 83196960.0000 - mae: 5627.4282 - val_loss: 8926.8396 - val_mse: 206676416.0000 - val_mae: 8926.8398\n",
            "Epoch 69/500\n",
            "60/60 [==============================] - 0s 786us/step - loss: 5495.3303 - mse: 80061440.0000 - mae: 5495.3301 - val_loss: 8823.4514 - val_mse: 204374992.0000 - val_mae: 8823.4512\n",
            "Epoch 70/500\n",
            "60/60 [==============================] - 0s 837us/step - loss: 5862.4950 - mse: 86195104.0000 - mae: 5862.4951 - val_loss: 8635.3860 - val_mse: 202353776.0000 - val_mae: 8635.3857\n",
            "Epoch 71/500\n",
            "60/60 [==============================] - 0s 748us/step - loss: 5491.2828 - mse: 79063184.0000 - mae: 5491.2827 - val_loss: 8797.0790 - val_mse: 200484416.0000 - val_mae: 8797.0791\n",
            "Epoch 72/500\n",
            "60/60 [==============================] - 0s 902us/step - loss: 5193.3357 - mse: 74211464.0000 - mae: 5193.3359 - val_loss: 8453.3950 - val_mse: 197606432.0000 - val_mae: 8453.3955\n",
            "Epoch 73/500\n",
            "60/60 [==============================] - 0s 847us/step - loss: 5331.9007 - mse: 78701128.0000 - mae: 5331.9004 - val_loss: 8594.8508 - val_mse: 195769312.0000 - val_mae: 8594.8506\n",
            "Epoch 74/500\n",
            "60/60 [==============================] - 0s 775us/step - loss: 5381.9589 - mse: 79833320.0000 - mae: 5381.9595 - val_loss: 8308.4781 - val_mse: 193421024.0000 - val_mae: 8308.4785\n",
            "Epoch 75/500\n",
            "60/60 [==============================] - 0s 877us/step - loss: 5251.4035 - mse: 74567064.0000 - mae: 5251.4038 - val_loss: 8503.9524 - val_mse: 191723808.0000 - val_mae: 8503.9521\n",
            "Epoch 76/500\n",
            "60/60 [==============================] - 0s 875us/step - loss: 5025.5462 - mse: 72898200.0000 - mae: 5025.5459 - val_loss: 8253.7394 - val_mse: 189471904.0000 - val_mae: 8253.7393\n",
            "Epoch 77/500\n",
            "60/60 [==============================] - 0s 860us/step - loss: 4959.5955 - mse: 71703792.0000 - mae: 4959.5957 - val_loss: 8434.2029 - val_mse: 187748160.0000 - val_mae: 8434.2031\n",
            "Epoch 78/500\n",
            "60/60 [==============================] - 0s 753us/step - loss: 5100.8224 - mse: 72800144.0000 - mae: 5100.8228 - val_loss: 8175.7336 - val_mse: 185690176.0000 - val_mae: 8175.7334\n",
            "Epoch 79/500\n",
            "60/60 [==============================] - 0s 912us/step - loss: 5073.1428 - mse: 69274048.0000 - mae: 5073.1426 - val_loss: 7993.9993 - val_mse: 183614752.0000 - val_mae: 7993.9990\n",
            "Epoch 80/500\n",
            "60/60 [==============================] - 0s 883us/step - loss: 4912.3724 - mse: 67124424.0000 - mae: 4912.3730 - val_loss: 7930.0740 - val_mse: 181585760.0000 - val_mae: 7930.0737\n",
            "Epoch 81/500\n",
            "60/60 [==============================] - 0s 872us/step - loss: 4883.0069 - mse: 66534264.0000 - mae: 4883.0073 - val_loss: 7887.5385 - val_mse: 179906064.0000 - val_mae: 7887.5386\n",
            "Epoch 82/500\n",
            "60/60 [==============================] - 0s 805us/step - loss: 4832.1258 - mse: 67747696.0000 - mae: 4832.1260 - val_loss: 8052.5588 - val_mse: 178389968.0000 - val_mae: 8052.5581\n",
            "Epoch 83/500\n",
            "60/60 [==============================] - 0s 787us/step - loss: 4675.2464 - mse: 60773652.0000 - mae: 4675.2466 - val_loss: 7793.8629 - val_mse: 175892960.0000 - val_mae: 7793.8623\n",
            "Epoch 84/500\n",
            "60/60 [==============================] - 0s 785us/step - loss: 4934.1184 - mse: 71227848.0000 - mae: 4934.1182 - val_loss: 7803.1312 - val_mse: 174124384.0000 - val_mae: 7803.1313\n",
            "Epoch 85/500\n",
            "60/60 [==============================] - 0s 778us/step - loss: 4816.8832 - mse: 68117280.0000 - mae: 4816.8833 - val_loss: 8057.2376 - val_mse: 173088352.0000 - val_mae: 8057.2373\n",
            "Epoch 86/500\n",
            "60/60 [==============================] - 0s 956us/step - loss: 4793.8607 - mse: 62797544.0000 - mae: 4793.8608 - val_loss: 7688.4235 - val_mse: 171230992.0000 - val_mae: 7688.4233\n",
            "Epoch 87/500\n",
            "60/60 [==============================] - 0s 798us/step - loss: 4526.0411 - mse: 61593932.0000 - mae: 4526.0410 - val_loss: 7892.7203 - val_mse: 169766736.0000 - val_mae: 7892.7202\n",
            "Epoch 88/500\n",
            "60/60 [==============================] - 0s 843us/step - loss: 4449.3305 - mse: 55191888.0000 - mae: 4449.3301 - val_loss: 7567.1556 - val_mse: 167402400.0000 - val_mae: 7567.1558\n",
            "Epoch 89/500\n",
            "60/60 [==============================] - 0s 849us/step - loss: 4577.3331 - mse: 62195284.0000 - mae: 4577.3335 - val_loss: 7768.1980 - val_mse: 166380224.0000 - val_mae: 7768.1978\n",
            "Epoch 90/500\n",
            "60/60 [==============================] - 0s 850us/step - loss: 4464.9383 - mse: 58270932.0000 - mae: 4464.9380 - val_loss: 7494.1649 - val_mse: 164803264.0000 - val_mae: 7494.1650\n",
            "Epoch 91/500\n",
            "60/60 [==============================] - 0s 871us/step - loss: 4573.3263 - mse: 62044944.0000 - mae: 4573.3267 - val_loss: 7450.9793 - val_mse: 163306912.0000 - val_mae: 7450.9790\n",
            "Epoch 92/500\n",
            "60/60 [==============================] - 0s 827us/step - loss: 4619.3623 - mse: 64242484.0000 - mae: 4619.3623 - val_loss: 7482.7598 - val_mse: 162133184.0000 - val_mae: 7482.7598\n",
            "Epoch 93/500\n",
            "60/60 [==============================] - 0s 873us/step - loss: 4551.7671 - mse: 58928488.0000 - mae: 4551.7671 - val_loss: 7572.6656 - val_mse: 160946144.0000 - val_mae: 7572.6655\n",
            "Epoch 94/500\n",
            "60/60 [==============================] - 0s 913us/step - loss: 4477.1681 - mse: 58936612.0000 - mae: 4477.1685 - val_loss: 7350.8436 - val_mse: 159471456.0000 - val_mae: 7350.8438\n",
            "Epoch 95/500\n",
            "60/60 [==============================] - 0s 873us/step - loss: 4314.9360 - mse: 55368268.0000 - mae: 4314.9360 - val_loss: 7297.9774 - val_mse: 157958240.0000 - val_mae: 7297.9775\n",
            "Epoch 96/500\n",
            "60/60 [==============================] - 0s 924us/step - loss: 4139.2728 - mse: 52920816.0000 - mae: 4139.2729 - val_loss: 7409.1159 - val_mse: 156933856.0000 - val_mae: 7409.1157\n",
            "Epoch 97/500\n",
            "60/60 [==============================] - 0s 851us/step - loss: 4051.8499 - mse: 48016144.0000 - mae: 4051.8499 - val_loss: 7382.2051 - val_mse: 155453408.0000 - val_mae: 7382.2051\n",
            "Epoch 98/500\n",
            "60/60 [==============================] - 0s 872us/step - loss: 4779.6922 - mse: 64936464.0000 - mae: 4779.6924 - val_loss: 7352.1310 - val_mse: 154719264.0000 - val_mae: 7352.1313\n",
            "Epoch 99/500\n",
            "60/60 [==============================] - 0s 876us/step - loss: 4252.0572 - mse: 56975408.0000 - mae: 4252.0571 - val_loss: 7509.8865 - val_mse: 153538736.0000 - val_mae: 7509.8862\n",
            "Epoch 100/500\n",
            "60/60 [==============================] - 0s 951us/step - loss: 4286.3982 - mse: 52119256.0000 - mae: 4286.3979 - val_loss: 7314.3776 - val_mse: 151788640.0000 - val_mae: 7314.3770\n",
            "Epoch 101/500\n",
            "60/60 [==============================] - 0s 796us/step - loss: 4291.8129 - mse: 54933224.0000 - mae: 4291.8130 - val_loss: 7061.8857 - val_mse: 150195712.0000 - val_mae: 7061.8853\n",
            "Epoch 102/500\n",
            "60/60 [==============================] - 0s 904us/step - loss: 4165.9009 - mse: 54267968.0000 - mae: 4165.9009 - val_loss: 7040.9623 - val_mse: 149333744.0000 - val_mae: 7040.9624\n",
            "Epoch 103/500\n",
            "60/60 [==============================] - 0s 967us/step - loss: 4227.9625 - mse: 49753412.0000 - mae: 4227.9624 - val_loss: 7062.7135 - val_mse: 148645904.0000 - val_mae: 7062.7134\n",
            "Epoch 104/500\n",
            "60/60 [==============================] - 0s 867us/step - loss: 4235.4289 - mse: 48304012.0000 - mae: 4235.4292 - val_loss: 7152.9236 - val_mse: 147717392.0000 - val_mae: 7152.9233\n",
            "Epoch 105/500\n",
            "60/60 [==============================] - 0s 846us/step - loss: 4083.3085 - mse: 48683712.0000 - mae: 4083.3086 - val_loss: 6934.8623 - val_mse: 146102128.0000 - val_mae: 6934.8623\n",
            "Epoch 106/500\n",
            "60/60 [==============================] - 0s 831us/step - loss: 3860.3885 - mse: 46692620.0000 - mae: 3860.3884 - val_loss: 6936.7953 - val_mse: 144852512.0000 - val_mae: 6936.7954\n",
            "Epoch 107/500\n",
            "60/60 [==============================] - 0s 943us/step - loss: 4115.5343 - mse: 50667972.0000 - mae: 4115.5342 - val_loss: 6857.7734 - val_mse: 143661312.0000 - val_mae: 6857.7734\n",
            "Epoch 108/500\n",
            "60/60 [==============================] - 0s 815us/step - loss: 3981.1673 - mse: 47114484.0000 - mae: 3981.1675 - val_loss: 6832.9392 - val_mse: 142439968.0000 - val_mae: 6832.9390\n",
            "Epoch 109/500\n",
            "60/60 [==============================] - 0s 787us/step - loss: 4287.0324 - mse: 52176056.0000 - mae: 4287.0327 - val_loss: 6971.5733 - val_mse: 141842368.0000 - val_mae: 6971.5728\n",
            "Epoch 110/500\n",
            "60/60 [==============================] - 0s 788us/step - loss: 3940.6676 - mse: 45843376.0000 - mae: 3940.6677 - val_loss: 6931.1205 - val_mse: 140623744.0000 - val_mae: 6931.1206\n",
            "Epoch 111/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3881.7252 - mse: 44175796.0000 - mae: 3881.7253 - val_loss: 6841.6152 - val_mse: 139702784.0000 - val_mae: 6841.6152\n",
            "Epoch 112/500\n",
            "60/60 [==============================] - 0s 979us/step - loss: 4148.0402 - mse: 47589108.0000 - mae: 4148.0405 - val_loss: 6669.0746 - val_mse: 138401584.0000 - val_mae: 6669.0747\n",
            "Epoch 113/500\n",
            "60/60 [==============================] - 0s 952us/step - loss: 3791.0996 - mse: 43488000.0000 - mae: 3791.0994 - val_loss: 6666.8982 - val_mse: 137725840.0000 - val_mae: 6666.8979\n",
            "Epoch 114/500\n",
            "60/60 [==============================] - 0s 925us/step - loss: 4353.1768 - mse: 50572472.0000 - mae: 4353.1772 - val_loss: 6814.0075 - val_mse: 137526016.0000 - val_mae: 6814.0073\n",
            "Epoch 115/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4136.4639 - mse: 47106340.0000 - mae: 4136.4639 - val_loss: 6721.8263 - val_mse: 136691904.0000 - val_mae: 6721.8267\n",
            "Epoch 116/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3920.6635 - mse: 46190356.0000 - mae: 3920.6636 - val_loss: 6629.3280 - val_mse: 135347952.0000 - val_mae: 6629.3281\n",
            "Epoch 117/500\n",
            "60/60 [==============================] - 0s 971us/step - loss: 3720.1364 - mse: 41832468.0000 - mae: 3720.1362 - val_loss: 6550.0927 - val_mse: 134070928.0000 - val_mae: 6550.0928\n",
            "Epoch 118/500\n",
            "60/60 [==============================] - 0s 917us/step - loss: 4406.7884 - mse: 50431720.0000 - mae: 4406.7886 - val_loss: 6520.4947 - val_mse: 133482464.0000 - val_mae: 6520.4946\n",
            "Epoch 119/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4006.5940 - mse: 46632320.0000 - mae: 4006.5942 - val_loss: 6537.5764 - val_mse: 133072768.0000 - val_mae: 6537.5767\n",
            "Epoch 120/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4127.7899 - mse: 49274308.0000 - mae: 4127.7900 - val_loss: 6494.0730 - val_mse: 132916896.0000 - val_mae: 6494.0728\n",
            "Epoch 121/500\n",
            "60/60 [==============================] - 0s 980us/step - loss: 3581.1865 - mse: 38882732.0000 - mae: 3581.1865 - val_loss: 6519.2388 - val_mse: 132284056.0000 - val_mae: 6519.2388\n",
            "Epoch 122/500\n",
            "60/60 [==============================] - 0s 951us/step - loss: 3591.5573 - mse: 39679088.0000 - mae: 3591.5574 - val_loss: 6475.8249 - val_mse: 131041640.0000 - val_mae: 6475.8252\n",
            "Epoch 123/500\n",
            "60/60 [==============================] - 0s 990us/step - loss: 4021.1381 - mse: 48067236.0000 - mae: 4021.1382 - val_loss: 6475.5265 - val_mse: 130503032.0000 - val_mae: 6475.5264\n",
            "Epoch 124/500\n",
            "60/60 [==============================] - 0s 881us/step - loss: 3961.2602 - mse: 44547912.0000 - mae: 3961.2603 - val_loss: 6470.5162 - val_mse: 130507368.0000 - val_mae: 6470.5161\n",
            "Epoch 125/500\n",
            "60/60 [==============================] - 0s 831us/step - loss: 3805.8359 - mse: 43509512.0000 - mae: 3805.8359 - val_loss: 6429.5946 - val_mse: 129586736.0000 - val_mae: 6429.5947\n",
            "Epoch 126/500\n",
            "60/60 [==============================] - 0s 822us/step - loss: 3812.4302 - mse: 44554320.0000 - mae: 3812.4302 - val_loss: 6404.6296 - val_mse: 128869856.0000 - val_mae: 6404.6299\n",
            "Epoch 127/500\n",
            "60/60 [==============================] - 0s 892us/step - loss: 3707.3822 - mse: 42032064.0000 - mae: 3707.3823 - val_loss: 6348.3386 - val_mse: 128123304.0000 - val_mae: 6348.3384\n",
            "Epoch 128/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3817.5720 - mse: 40804856.0000 - mae: 3817.5718 - val_loss: 6389.8877 - val_mse: 127458440.0000 - val_mae: 6389.8877\n",
            "Epoch 129/500\n",
            "60/60 [==============================] - 0s 853us/step - loss: 3865.3923 - mse: 39826724.0000 - mae: 3865.3921 - val_loss: 6347.6424 - val_mse: 127063936.0000 - val_mae: 6347.6421\n",
            "Epoch 130/500\n",
            "60/60 [==============================] - 0s 812us/step - loss: 4020.1126 - mse: 45288780.0000 - mae: 4020.1125 - val_loss: 6301.5930 - val_mse: 126483976.0000 - val_mae: 6301.5928\n",
            "Epoch 131/500\n",
            "60/60 [==============================] - 0s 886us/step - loss: 4003.0816 - mse: 46152656.0000 - mae: 4003.0818 - val_loss: 6323.9656 - val_mse: 126036664.0000 - val_mae: 6323.9658\n",
            "Epoch 132/500\n",
            "60/60 [==============================] - 0s 793us/step - loss: 3764.3946 - mse: 40126588.0000 - mae: 3764.3948 - val_loss: 6316.6627 - val_mse: 125649576.0000 - val_mae: 6316.6626\n",
            "Epoch 133/500\n",
            "60/60 [==============================] - 0s 953us/step - loss: 3597.3304 - mse: 42556504.0000 - mae: 3597.3306 - val_loss: 6299.7113 - val_mse: 125367160.0000 - val_mae: 6299.7114\n",
            "Epoch 134/500\n",
            "60/60 [==============================] - 0s 928us/step - loss: 3945.6178 - mse: 47126000.0000 - mae: 3945.6177 - val_loss: 6245.4269 - val_mse: 124763080.0000 - val_mae: 6245.4272\n",
            "Epoch 135/500\n",
            "60/60 [==============================] - 0s 937us/step - loss: 3776.3959 - mse: 43122092.0000 - mae: 3776.3958 - val_loss: 6294.4121 - val_mse: 124305488.0000 - val_mae: 6294.4121\n",
            "Epoch 136/500\n",
            "60/60 [==============================] - 0s 962us/step - loss: 3860.2621 - mse: 42880804.0000 - mae: 3860.2622 - val_loss: 6399.2779 - val_mse: 123965032.0000 - val_mae: 6399.2783\n",
            "Epoch 137/500\n",
            "60/60 [==============================] - 0s 950us/step - loss: 3797.5898 - mse: 43284992.0000 - mae: 3797.5898 - val_loss: 6227.9591 - val_mse: 123033000.0000 - val_mae: 6227.9595\n",
            "Epoch 138/500\n",
            "60/60 [==============================] - 0s 881us/step - loss: 3852.2843 - mse: 41772216.0000 - mae: 3852.2844 - val_loss: 6255.6739 - val_mse: 122613064.0000 - val_mae: 6255.6738\n",
            "Epoch 139/500\n",
            "60/60 [==============================] - 0s 928us/step - loss: 3715.2654 - mse: 39409144.0000 - mae: 3715.2654 - val_loss: 6195.6809 - val_mse: 122124856.0000 - val_mae: 6195.6812\n",
            "Epoch 140/500\n",
            "60/60 [==============================] - 0s 819us/step - loss: 3953.2328 - mse: 45628800.0000 - mae: 3953.2329 - val_loss: 6295.0630 - val_mse: 122200864.0000 - val_mae: 6295.0630\n",
            "Epoch 141/500\n",
            "60/60 [==============================] - 0s 797us/step - loss: 3602.5655 - mse: 39859916.0000 - mae: 3602.5654 - val_loss: 6217.2148 - val_mse: 121223064.0000 - val_mae: 6217.2144\n",
            "Epoch 142/500\n",
            "60/60 [==============================] - 0s 849us/step - loss: 3769.0154 - mse: 38320420.0000 - mae: 3769.0156 - val_loss: 6192.2069 - val_mse: 120686856.0000 - val_mae: 6192.2065\n",
            "Epoch 143/500\n",
            "60/60 [==============================] - 0s 900us/step - loss: 3749.0657 - mse: 39933460.0000 - mae: 3749.0659 - val_loss: 6205.1689 - val_mse: 120119328.0000 - val_mae: 6205.1689\n",
            "Epoch 144/500\n",
            "60/60 [==============================] - 0s 845us/step - loss: 3872.5259 - mse: 45572236.0000 - mae: 3872.5261 - val_loss: 6202.8546 - val_mse: 120119632.0000 - val_mae: 6202.8545\n",
            "Epoch 145/500\n",
            "60/60 [==============================] - 0s 815us/step - loss: 3840.5353 - mse: 43838092.0000 - mae: 3840.5352 - val_loss: 6129.7103 - val_mse: 119435408.0000 - val_mae: 6129.7104\n",
            "Epoch 146/500\n",
            "60/60 [==============================] - 0s 922us/step - loss: 3653.7032 - mse: 37811884.0000 - mae: 3653.7031 - val_loss: 6176.3781 - val_mse: 119030616.0000 - val_mae: 6176.3779\n",
            "Epoch 147/500\n",
            "60/60 [==============================] - 0s 893us/step - loss: 3810.2463 - mse: 39348864.0000 - mae: 3810.2461 - val_loss: 6191.9809 - val_mse: 118720712.0000 - val_mae: 6191.9810\n",
            "Epoch 148/500\n",
            "60/60 [==============================] - 0s 954us/step - loss: 3434.0359 - mse: 34556708.0000 - mae: 3434.0359 - val_loss: 6148.0908 - val_mse: 118301616.0000 - val_mae: 6148.0908\n",
            "Epoch 149/500\n",
            "60/60 [==============================] - 0s 866us/step - loss: 3682.1702 - mse: 39577724.0000 - mae: 3682.1704 - val_loss: 6150.8031 - val_mse: 118005768.0000 - val_mae: 6150.8032\n",
            "Epoch 150/500\n",
            "60/60 [==============================] - 0s 858us/step - loss: 3762.9889 - mse: 38545884.0000 - mae: 3762.9890 - val_loss: 6116.2348 - val_mse: 117786832.0000 - val_mae: 6116.2349\n",
            "Epoch 151/500\n",
            "60/60 [==============================] - 0s 962us/step - loss: 3892.5132 - mse: 42655176.0000 - mae: 3892.5134 - val_loss: 6217.7234 - val_mse: 117632912.0000 - val_mae: 6217.7236\n",
            "Epoch 152/500\n",
            "60/60 [==============================] - 0s 932us/step - loss: 3511.5899 - mse: 34687388.0000 - mae: 3511.5901 - val_loss: 6102.2206 - val_mse: 116982664.0000 - val_mae: 6102.2207\n",
            "Epoch 153/500\n",
            "60/60 [==============================] - 0s 971us/step - loss: 4154.8140 - mse: 42754840.0000 - mae: 4154.8140 - val_loss: 6087.2406 - val_mse: 117052376.0000 - val_mae: 6087.2407\n",
            "Epoch 154/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3773.6213 - mse: 41524520.0000 - mae: 3773.6213 - val_loss: 6080.8778 - val_mse: 116621752.0000 - val_mae: 6080.8779\n",
            "Epoch 155/500\n",
            "60/60 [==============================] - 0s 963us/step - loss: 3700.5661 - mse: 39078748.0000 - mae: 3700.5662 - val_loss: 6129.8564 - val_mse: 116428168.0000 - val_mae: 6129.8564\n",
            "Epoch 156/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3449.5511 - mse: 35366332.0000 - mae: 3449.5513 - val_loss: 6145.0844 - val_mse: 116170704.0000 - val_mae: 6145.0845\n",
            "Epoch 157/500\n",
            "60/60 [==============================] - 0s 843us/step - loss: 3717.6508 - mse: 38241700.0000 - mae: 3717.6511 - val_loss: 6099.6855 - val_mse: 115640472.0000 - val_mae: 6099.6855\n",
            "Epoch 158/500\n",
            "60/60 [==============================] - 0s 851us/step - loss: 4042.9602 - mse: 42353744.0000 - mae: 4042.9604 - val_loss: 6105.3135 - val_mse: 115562584.0000 - val_mae: 6105.3135\n",
            "Epoch 159/500\n",
            "60/60 [==============================] - 0s 836us/step - loss: 3335.1128 - mse: 29731428.0000 - mae: 3335.1130 - val_loss: 6042.3643 - val_mse: 114817880.0000 - val_mae: 6042.3647\n",
            "Epoch 160/500\n",
            "60/60 [==============================] - 0s 893us/step - loss: 3705.0908 - mse: 40619156.0000 - mae: 3705.0906 - val_loss: 6053.0904 - val_mse: 114651152.0000 - val_mae: 6053.0908\n",
            "Epoch 161/500\n",
            "60/60 [==============================] - 0s 815us/step - loss: 3759.1644 - mse: 37860032.0000 - mae: 3759.1643 - val_loss: 6002.9964 - val_mse: 114325264.0000 - val_mae: 6002.9966\n",
            "Epoch 162/500\n",
            "60/60 [==============================] - 0s 951us/step - loss: 3719.3166 - mse: 38844100.0000 - mae: 3719.3167 - val_loss: 5980.8650 - val_mse: 113938688.0000 - val_mae: 5980.8652\n",
            "Epoch 163/500\n",
            "60/60 [==============================] - 0s 877us/step - loss: 3671.6669 - mse: 38499632.0000 - mae: 3671.6672 - val_loss: 6018.7573 - val_mse: 113931216.0000 - val_mae: 6018.7573\n",
            "Epoch 164/500\n",
            "60/60 [==============================] - 0s 842us/step - loss: 4048.6424 - mse: 42717348.0000 - mae: 4048.6423 - val_loss: 6073.2934 - val_mse: 113543928.0000 - val_mae: 6073.2939\n",
            "Epoch 165/500\n",
            "60/60 [==============================] - 0s 849us/step - loss: 3684.0082 - mse: 38805616.0000 - mae: 3684.0083 - val_loss: 6131.5914 - val_mse: 113606552.0000 - val_mae: 6131.5918\n",
            "Epoch 166/500\n",
            "60/60 [==============================] - 0s 831us/step - loss: 3400.4217 - mse: 33302442.0000 - mae: 3400.4219 - val_loss: 6076.2083 - val_mse: 113146344.0000 - val_mae: 6076.2085\n",
            "Epoch 167/500\n",
            "60/60 [==============================] - 0s 903us/step - loss: 3306.6938 - mse: 28507862.0000 - mae: 3306.6941 - val_loss: 6055.1001 - val_mse: 112596976.0000 - val_mae: 6055.1001\n",
            "Epoch 168/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3623.1330 - mse: 35049224.0000 - mae: 3623.1328 - val_loss: 6074.1137 - val_mse: 112452936.0000 - val_mae: 6074.1138\n",
            "Epoch 169/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3563.7876 - mse: 35224308.0000 - mae: 3563.7876 - val_loss: 5928.3761 - val_mse: 111667128.0000 - val_mae: 5928.3760\n",
            "Epoch 170/500\n",
            "60/60 [==============================] - 0s 925us/step - loss: 3313.5475 - mse: 33813284.0000 - mae: 3313.5474 - val_loss: 5979.6162 - val_mse: 111573992.0000 - val_mae: 5979.6162\n",
            "Epoch 171/500\n",
            "60/60 [==============================] - 0s 799us/step - loss: 3347.4507 - mse: 36938276.0000 - mae: 3347.4504 - val_loss: 5884.7834 - val_mse: 111328488.0000 - val_mae: 5884.7832\n",
            "Epoch 172/500\n",
            "60/60 [==============================] - 0s 887us/step - loss: 3888.2992 - mse: 38043124.0000 - mae: 3888.2993 - val_loss: 5917.3550 - val_mse: 111389960.0000 - val_mae: 5917.3550\n",
            "Epoch 173/500\n",
            "60/60 [==============================] - 0s 904us/step - loss: 3789.3755 - mse: 38591556.0000 - mae: 3789.3755 - val_loss: 5957.4814 - val_mse: 111537840.0000 - val_mae: 5957.4814\n",
            "Epoch 174/500\n",
            "60/60 [==============================] - 0s 832us/step - loss: 3775.6508 - mse: 41242276.0000 - mae: 3775.6509 - val_loss: 5919.4098 - val_mse: 111324376.0000 - val_mae: 5919.4092\n",
            "Epoch 175/500\n",
            "60/60 [==============================] - 0s 900us/step - loss: 3440.2001 - mse: 32487128.0000 - mae: 3440.2002 - val_loss: 5975.0265 - val_mse: 111235016.0000 - val_mae: 5975.0264\n",
            "Epoch 176/500\n",
            "60/60 [==============================] - 0s 912us/step - loss: 3495.2777 - mse: 33204754.0000 - mae: 3495.2776 - val_loss: 5907.0089 - val_mse: 110666800.0000 - val_mae: 5907.0088\n",
            "Epoch 177/500\n",
            "60/60 [==============================] - 0s 912us/step - loss: 3094.0070 - mse: 27759886.0000 - mae: 3094.0068 - val_loss: 5763.3446 - val_mse: 109539472.0000 - val_mae: 5763.3447\n",
            "Epoch 178/500\n",
            "60/60 [==============================] - 0s 899us/step - loss: 3625.8804 - mse: 36498360.0000 - mae: 3625.8804 - val_loss: 5792.9379 - val_mse: 109120600.0000 - val_mae: 5792.9375\n",
            "Epoch 179/500\n",
            "60/60 [==============================] - 0s 881us/step - loss: 3567.6668 - mse: 34766348.0000 - mae: 3567.6670 - val_loss: 5773.6333 - val_mse: 108785376.0000 - val_mae: 5773.6333\n",
            "Epoch 180/500\n",
            "60/60 [==============================] - 0s 796us/step - loss: 3695.8960 - mse: 37523372.0000 - mae: 3695.8958 - val_loss: 5747.7611 - val_mse: 108533616.0000 - val_mae: 5747.7607\n",
            "Epoch 181/500\n",
            "60/60 [==============================] - 0s 979us/step - loss: 3472.4301 - mse: 33801544.0000 - mae: 3472.4299 - val_loss: 5612.4166 - val_mse: 107875960.0000 - val_mae: 5612.4165\n",
            "Epoch 182/500\n",
            "60/60 [==============================] - 0s 905us/step - loss: 3357.0955 - mse: 31156522.0000 - mae: 3357.0959 - val_loss: 5627.1101 - val_mse: 107273936.0000 - val_mae: 5627.1104\n",
            "Epoch 183/500\n",
            "60/60 [==============================] - 0s 863us/step - loss: 3483.9983 - mse: 34481996.0000 - mae: 3483.9985 - val_loss: 5485.3996 - val_mse: 106649832.0000 - val_mae: 5485.3999\n",
            "Epoch 184/500\n",
            "60/60 [==============================] - 0s 815us/step - loss: 3720.1845 - mse: 37849508.0000 - mae: 3720.1843 - val_loss: 5432.7490 - val_mse: 106489040.0000 - val_mae: 5432.7490\n",
            "Epoch 185/500\n",
            "60/60 [==============================] - 0s 844us/step - loss: 3071.4189 - mse: 27325646.0000 - mae: 3071.4189 - val_loss: 5392.6119 - val_mse: 105599944.0000 - val_mae: 5392.6118\n",
            "Epoch 186/500\n",
            "60/60 [==============================] - 0s 865us/step - loss: 3329.2833 - mse: 30707334.0000 - mae: 3329.2834 - val_loss: 5367.0764 - val_mse: 105216504.0000 - val_mae: 5367.0762\n",
            "Epoch 187/500\n",
            "60/60 [==============================] - 0s 919us/step - loss: 3500.0061 - mse: 33451846.0000 - mae: 3500.0063 - val_loss: 5325.7059 - val_mse: 104511384.0000 - val_mae: 5325.7056\n",
            "Epoch 188/500\n",
            "60/60 [==============================] - 0s 847us/step - loss: 2902.5931 - mse: 27632986.0000 - mae: 2902.5933 - val_loss: 5341.4153 - val_mse: 104357536.0000 - val_mae: 5341.4155\n",
            "Epoch 189/500\n",
            "60/60 [==============================] - 0s 864us/step - loss: 3119.7592 - mse: 31061812.0000 - mae: 3119.7593 - val_loss: 5336.2679 - val_mse: 103829408.0000 - val_mae: 5336.2676\n",
            "Epoch 190/500\n",
            "60/60 [==============================] - 0s 930us/step - loss: 3030.8415 - mse: 27689120.0000 - mae: 3030.8413 - val_loss: 5407.8289 - val_mse: 103454104.0000 - val_mae: 5407.8291\n",
            "Epoch 191/500\n",
            "60/60 [==============================] - 0s 799us/step - loss: 3127.2169 - mse: 31923916.0000 - mae: 3127.2170 - val_loss: 5331.3862 - val_mse: 102783872.0000 - val_mae: 5331.3862\n",
            "Epoch 192/500\n",
            "60/60 [==============================] - 0s 863us/step - loss: 3111.7491 - mse: 27692920.0000 - mae: 3111.7493 - val_loss: 5435.0678 - val_mse: 102573456.0000 - val_mae: 5435.0679\n",
            "Epoch 193/500\n",
            "60/60 [==============================] - 0s 859us/step - loss: 3130.9348 - mse: 28611552.0000 - mae: 3130.9346 - val_loss: 5338.6201 - val_mse: 102092920.0000 - val_mae: 5338.6196\n",
            "Epoch 194/500\n",
            "60/60 [==============================] - 0s 898us/step - loss: 3192.1875 - mse: 30082602.0000 - mae: 3192.1875 - val_loss: 5246.2478 - val_mse: 101172792.0000 - val_mae: 5246.2480\n",
            "Epoch 195/500\n",
            "60/60 [==============================] - 0s 951us/step - loss: 3076.8545 - mse: 29400442.0000 - mae: 3076.8547 - val_loss: 5299.7374 - val_mse: 100611056.0000 - val_mae: 5299.7373\n",
            "Epoch 196/500\n",
            "60/60 [==============================] - 0s 857us/step - loss: 2523.7423 - mse: 17555482.0000 - mae: 2523.7422 - val_loss: 5191.5821 - val_mse: 99369896.0000 - val_mae: 5191.5825\n",
            "Epoch 197/500\n",
            "60/60 [==============================] - 0s 975us/step - loss: 3263.4071 - mse: 29321230.0000 - mae: 3263.4072 - val_loss: 5408.5517 - val_mse: 99850040.0000 - val_mae: 5408.5518\n",
            "Epoch 198/500\n",
            "60/60 [==============================] - 0s 882us/step - loss: 3088.0522 - mse: 25998826.0000 - mae: 3088.0522 - val_loss: 5283.0024 - val_mse: 98882984.0000 - val_mae: 5283.0020\n",
            "Epoch 199/500\n",
            "60/60 [==============================] - 0s 960us/step - loss: 3057.6430 - mse: 27910218.0000 - mae: 3057.6431 - val_loss: 5336.6395 - val_mse: 98079672.0000 - val_mae: 5336.6396\n",
            "Epoch 200/500\n",
            "60/60 [==============================] - 0s 914us/step - loss: 3091.0927 - mse: 30236966.0000 - mae: 3091.0925 - val_loss: 5139.4899 - val_mse: 97657840.0000 - val_mae: 5139.4902\n",
            "Epoch 201/500\n",
            "60/60 [==============================] - 0s 922us/step - loss: 2813.4705 - mse: 24384026.0000 - mae: 2813.4705 - val_loss: 5161.7289 - val_mse: 97351336.0000 - val_mae: 5161.7285\n",
            "Epoch 202/500\n",
            "60/60 [==============================] - 0s 802us/step - loss: 3087.3895 - mse: 28726072.0000 - mae: 3087.3896 - val_loss: 5291.7426 - val_mse: 97032888.0000 - val_mae: 5291.7427\n",
            "Epoch 203/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3131.7330 - mse: 30073262.0000 - mae: 3131.7329 - val_loss: 5165.2088 - val_mse: 96637008.0000 - val_mae: 5165.2090\n",
            "Epoch 204/500\n",
            "60/60 [==============================] - 0s 841us/step - loss: 3215.1934 - mse: 33384708.0000 - mae: 3215.1931 - val_loss: 5274.0238 - val_mse: 96224336.0000 - val_mae: 5274.0239\n",
            "Epoch 205/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3393.0535 - mse: 30812948.0000 - mae: 3393.0537 - val_loss: 5135.7651 - val_mse: 96233672.0000 - val_mae: 5135.7651\n",
            "Epoch 206/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2867.1626 - mse: 26090228.0000 - mae: 2867.1626 - val_loss: 5122.1577 - val_mse: 95985784.0000 - val_mae: 5122.1577\n",
            "Epoch 207/500\n",
            "60/60 [==============================] - 0s 912us/step - loss: 2831.6212 - mse: 23605336.0000 - mae: 2831.6211 - val_loss: 5122.5813 - val_mse: 95211432.0000 - val_mae: 5122.5811\n",
            "Epoch 208/500\n",
            "60/60 [==============================] - 0s 931us/step - loss: 2735.9463 - mse: 22957710.0000 - mae: 2735.9463 - val_loss: 5076.8132 - val_mse: 94324584.0000 - val_mae: 5076.8130\n",
            "Epoch 209/500\n",
            "60/60 [==============================] - 0s 872us/step - loss: 3389.9592 - mse: 32065330.0000 - mae: 3389.9592 - val_loss: 5044.7027 - val_mse: 93821024.0000 - val_mae: 5044.7026\n",
            "Epoch 210/500\n",
            "60/60 [==============================] - 0s 849us/step - loss: 2703.0675 - mse: 23285206.0000 - mae: 2703.0676 - val_loss: 5022.1736 - val_mse: 93388192.0000 - val_mae: 5022.1738\n",
            "Epoch 211/500\n",
            "60/60 [==============================] - 0s 924us/step - loss: 2671.2147 - mse: 22609828.0000 - mae: 2671.2146 - val_loss: 5057.8878 - val_mse: 93233072.0000 - val_mae: 5057.8877\n",
            "Epoch 212/500\n",
            "60/60 [==============================] - 0s 914us/step - loss: 2902.1169 - mse: 24805036.0000 - mae: 2902.1167 - val_loss: 5018.6758 - val_mse: 92773032.0000 - val_mae: 5018.6763\n",
            "Epoch 213/500\n",
            "60/60 [==============================] - 0s 775us/step - loss: 2872.5178 - mse: 24794730.0000 - mae: 2872.5178 - val_loss: 5095.0562 - val_mse: 92279560.0000 - val_mae: 5095.0562\n",
            "Epoch 214/500\n",
            "60/60 [==============================] - 0s 848us/step - loss: 2554.2015 - mse: 20891730.0000 - mae: 2554.2017 - val_loss: 5141.9675 - val_mse: 92026168.0000 - val_mae: 5141.9678\n",
            "Epoch 215/500\n",
            "60/60 [==============================] - 0s 787us/step - loss: 2930.2102 - mse: 22624916.0000 - mae: 2930.2102 - val_loss: 4946.3179 - val_mse: 90798504.0000 - val_mae: 4946.3179\n",
            "Epoch 216/500\n",
            "60/60 [==============================] - 0s 833us/step - loss: 2989.4556 - mse: 24168966.0000 - mae: 2989.4558 - val_loss: 5053.2472 - val_mse: 90612848.0000 - val_mae: 5053.2476\n",
            "Epoch 217/500\n",
            "60/60 [==============================] - 0s 923us/step - loss: 3153.7630 - mse: 26429286.0000 - mae: 3153.7629 - val_loss: 5108.2425 - val_mse: 90866432.0000 - val_mae: 5108.2427\n",
            "Epoch 218/500\n",
            "60/60 [==============================] - 0s 853us/step - loss: 3004.4301 - mse: 27081350.0000 - mae: 3004.4299 - val_loss: 4975.3395 - val_mse: 89778456.0000 - val_mae: 4975.3394\n",
            "Epoch 219/500\n",
            "60/60 [==============================] - 0s 868us/step - loss: 2597.9067 - mse: 22202218.0000 - mae: 2597.9067 - val_loss: 5051.8728 - val_mse: 89204720.0000 - val_mae: 5051.8730\n",
            "Epoch 220/500\n",
            "60/60 [==============================] - 0s 919us/step - loss: 3071.7835 - mse: 27166430.0000 - mae: 3071.7834 - val_loss: 4989.3369 - val_mse: 88797600.0000 - val_mae: 4989.3364\n",
            "Epoch 221/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3057.1120 - mse: 28168696.0000 - mae: 3057.1121 - val_loss: 4859.4337 - val_mse: 88418280.0000 - val_mae: 4859.4341\n",
            "Epoch 222/500\n",
            "60/60 [==============================] - 0s 841us/step - loss: 2942.2399 - mse: 21974220.0000 - mae: 2942.2400 - val_loss: 4827.7721 - val_mse: 87947920.0000 - val_mae: 4827.7720\n",
            "Epoch 223/500\n",
            "60/60 [==============================] - 0s 887us/step - loss: 3226.8553 - mse: 30586756.0000 - mae: 3226.8552 - val_loss: 4855.5506 - val_mse: 87957120.0000 - val_mae: 4855.5513\n",
            "Epoch 224/500\n",
            "60/60 [==============================] - 0s 989us/step - loss: 2397.3604 - mse: 17933722.0000 - mae: 2397.3604 - val_loss: 4821.9631 - val_mse: 87421160.0000 - val_mae: 4821.9629\n",
            "Epoch 225/500\n",
            "60/60 [==============================] - 0s 947us/step - loss: 3368.3128 - mse: 28186030.0000 - mae: 3368.3130 - val_loss: 4905.1237 - val_mse: 87154032.0000 - val_mae: 4905.1240\n",
            "Epoch 226/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2906.4577 - mse: 25522118.0000 - mae: 2906.4578 - val_loss: 5037.7766 - val_mse: 87019176.0000 - val_mae: 5037.7769\n",
            "Epoch 227/500\n",
            "60/60 [==============================] - 0s 833us/step - loss: 3024.7113 - mse: 29872862.0000 - mae: 3024.7114 - val_loss: 4984.6512 - val_mse: 86833024.0000 - val_mae: 4984.6509\n",
            "Epoch 228/500\n",
            "60/60 [==============================] - 0s 863us/step - loss: 2619.3266 - mse: 20354130.0000 - mae: 2619.3267 - val_loss: 4934.0594 - val_mse: 86327968.0000 - val_mae: 4934.0596\n",
            "Epoch 229/500\n",
            "60/60 [==============================] - 0s 929us/step - loss: 2638.2044 - mse: 21876538.0000 - mae: 2638.2046 - val_loss: 4799.5625 - val_mse: 85744784.0000 - val_mae: 4799.5625\n",
            "Epoch 230/500\n",
            "60/60 [==============================] - 0s 950us/step - loss: 2771.9822 - mse: 23534380.0000 - mae: 2771.9819 - val_loss: 4907.5220 - val_mse: 85557816.0000 - val_mae: 4907.5220\n",
            "Epoch 231/500\n",
            "60/60 [==============================] - 0s 873us/step - loss: 2787.6577 - mse: 24213898.0000 - mae: 2787.6577 - val_loss: 4747.3919 - val_mse: 85140448.0000 - val_mae: 4747.3916\n",
            "Epoch 232/500\n",
            "60/60 [==============================] - 0s 996us/step - loss: 2696.3636 - mse: 20494518.0000 - mae: 2696.3635 - val_loss: 4784.7765 - val_mse: 85237080.0000 - val_mae: 4784.7759\n",
            "Epoch 233/500\n",
            "60/60 [==============================] - 0s 853us/step - loss: 2457.0470 - mse: 16551189.0000 - mae: 2457.0471 - val_loss: 4698.6834 - val_mse: 84513232.0000 - val_mae: 4698.6831\n",
            "Epoch 234/500\n",
            "60/60 [==============================] - 0s 865us/step - loss: 3000.0689 - mse: 28258090.0000 - mae: 3000.0688 - val_loss: 4731.6621 - val_mse: 84357712.0000 - val_mae: 4731.6626\n",
            "Epoch 235/500\n",
            "60/60 [==============================] - 0s 828us/step - loss: 3117.3977 - mse: 29535168.0000 - mae: 3117.3977 - val_loss: 4713.3070 - val_mse: 84121816.0000 - val_mae: 4713.3071\n",
            "Epoch 236/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3178.1310 - mse: 26837278.0000 - mae: 3178.1306 - val_loss: 4862.8258 - val_mse: 84328792.0000 - val_mae: 4862.8257\n",
            "Epoch 237/500\n",
            "60/60 [==============================] - 0s 862us/step - loss: 2839.7989 - mse: 23569690.0000 - mae: 2839.7986 - val_loss: 4843.3742 - val_mse: 83587048.0000 - val_mae: 4843.3745\n",
            "Epoch 238/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2992.5114 - mse: 23510476.0000 - mae: 2992.5115 - val_loss: 4646.6800 - val_mse: 82912816.0000 - val_mae: 4646.6802\n",
            "Epoch 239/500\n",
            "60/60 [==============================] - 0s 975us/step - loss: 2704.9823 - mse: 19853912.0000 - mae: 2704.9822 - val_loss: 4645.1518 - val_mse: 82986632.0000 - val_mae: 4645.1519\n",
            "Epoch 240/500\n",
            "60/60 [==============================] - 0s 916us/step - loss: 3334.7061 - mse: 33507788.0000 - mae: 3334.7063 - val_loss: 4742.9531 - val_mse: 83166704.0000 - val_mae: 4742.9531\n",
            "Epoch 241/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3103.1598 - mse: 26730502.0000 - mae: 3103.1597 - val_loss: 4912.9458 - val_mse: 83033872.0000 - val_mae: 4912.9458\n",
            "Epoch 242/500\n",
            "60/60 [==============================] - 0s 898us/step - loss: 3098.3813 - mse: 27652490.0000 - mae: 3098.3816 - val_loss: 4696.7362 - val_mse: 82669672.0000 - val_mae: 4696.7358\n",
            "Epoch 243/500\n",
            "60/60 [==============================] - 0s 887us/step - loss: 2811.1217 - mse: 21310344.0000 - mae: 2811.1218 - val_loss: 4637.3948 - val_mse: 82102368.0000 - val_mae: 4637.3950\n",
            "Epoch 244/500\n",
            "60/60 [==============================] - 0s 944us/step - loss: 3201.6537 - mse: 29255976.0000 - mae: 3201.6536 - val_loss: 4609.0480 - val_mse: 82043432.0000 - val_mae: 4609.0479\n",
            "Epoch 245/500\n",
            "60/60 [==============================] - 0s 978us/step - loss: 2923.7149 - mse: 24453614.0000 - mae: 2923.7148 - val_loss: 4694.9965 - val_mse: 81607920.0000 - val_mae: 4694.9971\n",
            "Epoch 246/500\n",
            "60/60 [==============================] - 0s 849us/step - loss: 3172.4861 - mse: 27210898.0000 - mae: 3172.4858 - val_loss: 4743.6683 - val_mse: 81622488.0000 - val_mae: 4743.6685\n",
            "Epoch 247/500\n",
            "60/60 [==============================] - 0s 903us/step - loss: 2868.7488 - mse: 20534310.0000 - mae: 2868.7490 - val_loss: 4618.4136 - val_mse: 81005848.0000 - val_mae: 4618.4136\n",
            "Epoch 248/500\n",
            "60/60 [==============================] - 0s 887us/step - loss: 2922.1750 - mse: 23235094.0000 - mae: 2922.1750 - val_loss: 4797.9289 - val_mse: 81017056.0000 - val_mae: 4797.9292\n",
            "Epoch 249/500\n",
            "60/60 [==============================] - 0s 941us/step - loss: 2916.3613 - mse: 24705250.0000 - mae: 2916.3616 - val_loss: 4550.8281 - val_mse: 80426856.0000 - val_mae: 4550.8281\n",
            "Epoch 250/500\n",
            "60/60 [==============================] - 0s 931us/step - loss: 2915.8778 - mse: 24226850.0000 - mae: 2915.8779 - val_loss: 4782.7352 - val_mse: 80449264.0000 - val_mae: 4782.7354\n",
            "Epoch 251/500\n",
            "60/60 [==============================] - 0s 897us/step - loss: 2761.1453 - mse: 23378108.0000 - mae: 2761.1453 - val_loss: 4511.5006 - val_mse: 79741824.0000 - val_mae: 4511.5005\n",
            "Epoch 252/500\n",
            "60/60 [==============================] - 0s 928us/step - loss: 2806.5872 - mse: 23309836.0000 - mae: 2806.5874 - val_loss: 4446.4277 - val_mse: 79194912.0000 - val_mae: 4446.4277\n",
            "Epoch 253/500\n",
            "60/60 [==============================] - 0s 973us/step - loss: 2889.2908 - mse: 28398484.0000 - mae: 2889.2908 - val_loss: 4409.7600 - val_mse: 79009744.0000 - val_mae: 4409.7598\n",
            "Epoch 254/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2598.3742 - mse: 20038824.0000 - mae: 2598.3740 - val_loss: 4418.7690 - val_mse: 78466896.0000 - val_mae: 4418.7686\n",
            "Epoch 255/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2966.6583 - mse: 30792290.0000 - mae: 2966.6584 - val_loss: 4389.0751 - val_mse: 78253176.0000 - val_mae: 4389.0752\n",
            "Epoch 256/500\n",
            "60/60 [==============================] - 0s 824us/step - loss: 2841.8413 - mse: 22222934.0000 - mae: 2841.8413 - val_loss: 4514.1223 - val_mse: 78494480.0000 - val_mae: 4514.1226\n",
            "Epoch 257/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3031.0797 - mse: 23625676.0000 - mae: 3031.0796 - val_loss: 4453.4817 - val_mse: 78721112.0000 - val_mae: 4453.4819\n",
            "Epoch 258/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2509.9786 - mse: 22844990.0000 - mae: 2509.9788 - val_loss: 4655.0734 - val_mse: 78403784.0000 - val_mae: 4655.0732\n",
            "Epoch 259/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2678.2239 - mse: 21702536.0000 - mae: 2678.2239 - val_loss: 4553.5150 - val_mse: 77823344.0000 - val_mae: 4553.5151\n",
            "Epoch 260/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2632.3256 - mse: 18750598.0000 - mae: 2632.3254 - val_loss: 4345.0222 - val_mse: 77171272.0000 - val_mae: 4345.0225\n",
            "Epoch 261/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2777.4661 - mse: 21049688.0000 - mae: 2777.4661 - val_loss: 4484.5635 - val_mse: 77552752.0000 - val_mae: 4484.5635\n",
            "Epoch 262/500\n",
            "60/60 [==============================] - 0s 951us/step - loss: 2977.1762 - mse: 25290106.0000 - mae: 2977.1760 - val_loss: 4394.7021 - val_mse: 77321296.0000 - val_mae: 4394.7021\n",
            "Epoch 263/500\n",
            "60/60 [==============================] - 0s 930us/step - loss: 3337.0425 - mse: 33398626.0000 - mae: 3337.0425 - val_loss: 4522.5925 - val_mse: 77470320.0000 - val_mae: 4522.5928\n",
            "Epoch 264/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2746.8590 - mse: 22987118.0000 - mae: 2746.8591 - val_loss: 4393.2169 - val_mse: 77200592.0000 - val_mae: 4393.2168\n",
            "Epoch 265/500\n",
            "60/60 [==============================] - 0s 989us/step - loss: 2370.7337 - mse: 14848388.0000 - mae: 2370.7339 - val_loss: 4404.4229 - val_mse: 77273704.0000 - val_mae: 4404.4229\n",
            "Epoch 266/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2919.6421 - mse: 24784024.0000 - mae: 2919.6421 - val_loss: 4401.8101 - val_mse: 77064808.0000 - val_mae: 4401.8101\n",
            "Epoch 267/500\n",
            "60/60 [==============================] - 0s 981us/step - loss: 2570.9140 - mse: 16710411.0000 - mae: 2570.9141 - val_loss: 4426.3271 - val_mse: 76918488.0000 - val_mae: 4426.3271\n",
            "Epoch 268/500\n",
            "60/60 [==============================] - 0s 906us/step - loss: 2647.3537 - mse: 17806870.0000 - mae: 2647.3538 - val_loss: 4688.2646 - val_mse: 76961576.0000 - val_mae: 4688.2646\n",
            "Epoch 269/500\n",
            "60/60 [==============================] - 0s 955us/step - loss: 2796.1985 - mse: 23708552.0000 - mae: 2796.1985 - val_loss: 4490.7695 - val_mse: 76334424.0000 - val_mae: 4490.7690\n",
            "Epoch 270/500\n",
            "60/60 [==============================] - 0s 964us/step - loss: 2694.3007 - mse: 21568704.0000 - mae: 2694.3005 - val_loss: 4585.2022 - val_mse: 76442224.0000 - val_mae: 4585.2021\n",
            "Epoch 271/500\n",
            "60/60 [==============================] - 0s 900us/step - loss: 2727.4521 - mse: 20056252.0000 - mae: 2727.4521 - val_loss: 4431.1882 - val_mse: 75591216.0000 - val_mae: 4431.1880\n",
            "Epoch 272/500\n",
            "60/60 [==============================] - 0s 827us/step - loss: 2671.6372 - mse: 17987978.0000 - mae: 2671.6372 - val_loss: 4384.5672 - val_mse: 75332056.0000 - val_mae: 4384.5674\n",
            "Epoch 273/500\n",
            "60/60 [==============================] - 0s 952us/step - loss: 2396.2229 - mse: 15984425.0000 - mae: 2396.2229 - val_loss: 4288.8917 - val_mse: 74569992.0000 - val_mae: 4288.8921\n",
            "Epoch 274/500\n",
            "60/60 [==============================] - 0s 891us/step - loss: 2915.6485 - mse: 24970986.0000 - mae: 2915.6484 - val_loss: 4719.9345 - val_mse: 75167392.0000 - val_mae: 4719.9346\n",
            "Epoch 275/500\n",
            "60/60 [==============================] - 0s 974us/step - loss: 2924.1970 - mse: 22519918.0000 - mae: 2924.1970 - val_loss: 4365.8912 - val_mse: 74377584.0000 - val_mae: 4365.8911\n",
            "Epoch 276/500\n",
            "60/60 [==============================] - 0s 902us/step - loss: 2497.3014 - mse: 19932096.0000 - mae: 2497.3013 - val_loss: 4403.4275 - val_mse: 74135384.0000 - val_mae: 4403.4277\n",
            "Epoch 277/500\n",
            "60/60 [==============================] - 0s 865us/step - loss: 2806.3905 - mse: 21397342.0000 - mae: 2806.3904 - val_loss: 4329.7113 - val_mse: 73731640.0000 - val_mae: 4329.7114\n",
            "Epoch 278/500\n",
            "60/60 [==============================] - 0s 844us/step - loss: 2869.2199 - mse: 24894134.0000 - mae: 2869.2197 - val_loss: 4290.2224 - val_mse: 73746712.0000 - val_mae: 4290.2222\n",
            "Epoch 279/500\n",
            "60/60 [==============================] - 0s 935us/step - loss: 2265.2768 - mse: 15628985.0000 - mae: 2265.2769 - val_loss: 4549.8182 - val_mse: 73963144.0000 - val_mae: 4549.8184\n",
            "Epoch 280/500\n",
            "60/60 [==============================] - 0s 917us/step - loss: 3052.5566 - mse: 25927118.0000 - mae: 3052.5564 - val_loss: 4295.1958 - val_mse: 73721360.0000 - val_mae: 4295.1958\n",
            "Epoch 281/500\n",
            "60/60 [==============================] - 0s 852us/step - loss: 2535.8624 - mse: 19622336.0000 - mae: 2535.8625 - val_loss: 4555.9558 - val_mse: 73992616.0000 - val_mae: 4555.9556\n",
            "Epoch 282/500\n",
            "60/60 [==============================] - 0s 949us/step - loss: 2441.5993 - mse: 16533485.0000 - mae: 2441.5991 - val_loss: 4448.7599 - val_mse: 73253112.0000 - val_mae: 4448.7598\n",
            "Epoch 283/500\n",
            "60/60 [==============================] - 0s 882us/step - loss: 2399.7325 - mse: 18103528.0000 - mae: 2399.7327 - val_loss: 4455.6280 - val_mse: 73006904.0000 - val_mae: 4455.6279\n",
            "Epoch 284/500\n",
            "60/60 [==============================] - 0s 930us/step - loss: 2615.3580 - mse: 19755032.0000 - mae: 2615.3582 - val_loss: 4284.7535 - val_mse: 72420352.0000 - val_mae: 4284.7534\n",
            "Epoch 285/500\n",
            "60/60 [==============================] - 0s 900us/step - loss: 2885.3822 - mse: 20737494.0000 - mae: 2885.3821 - val_loss: 4454.0501 - val_mse: 72851808.0000 - val_mae: 4454.0498\n",
            "Epoch 286/500\n",
            "60/60 [==============================] - 0s 894us/step - loss: 2620.2844 - mse: 17443780.0000 - mae: 2620.2844 - val_loss: 4247.3912 - val_mse: 72433776.0000 - val_mae: 4247.3911\n",
            "Epoch 287/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2613.2799 - mse: 16671506.0000 - mae: 2613.2800 - val_loss: 4232.3162 - val_mse: 72168296.0000 - val_mae: 4232.3159\n",
            "Epoch 288/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2335.2032 - mse: 16232547.0000 - mae: 2335.2031 - val_loss: 4493.2331 - val_mse: 72283448.0000 - val_mae: 4493.2329\n",
            "Epoch 289/500\n",
            "60/60 [==============================] - 0s 988us/step - loss: 2830.3170 - mse: 20304486.0000 - mae: 2830.3171 - val_loss: 4287.7609 - val_mse: 71934896.0000 - val_mae: 4287.7607\n",
            "Epoch 290/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2540.2231 - mse: 18445702.0000 - mae: 2540.2231 - val_loss: 4382.6963 - val_mse: 71869792.0000 - val_mae: 4382.6963\n",
            "Epoch 291/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2583.9107 - mse: 19727266.0000 - mae: 2583.9106 - val_loss: 4229.4654 - val_mse: 71227240.0000 - val_mae: 4229.4658\n",
            "Epoch 292/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2623.4626 - mse: 17796378.0000 - mae: 2623.4624 - val_loss: 4188.4340 - val_mse: 71252568.0000 - val_mae: 4188.4341\n",
            "Epoch 293/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2709.6832 - mse: 18054888.0000 - mae: 2709.6833 - val_loss: 4256.3476 - val_mse: 70980952.0000 - val_mae: 4256.3477\n",
            "Epoch 294/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2720.5934 - mse: 17428220.0000 - mae: 2720.5933 - val_loss: 4302.9730 - val_mse: 71305424.0000 - val_mae: 4302.9731\n",
            "Epoch 295/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2791.3447 - mse: 20844478.0000 - mae: 2791.3447 - val_loss: 4388.9163 - val_mse: 71248720.0000 - val_mae: 4388.9160\n",
            "Epoch 296/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2750.0740 - mse: 20276890.0000 - mae: 2750.0740 - val_loss: 4137.6556 - val_mse: 70662712.0000 - val_mae: 4137.6558\n",
            "Epoch 297/500\n",
            "60/60 [==============================] - 0s 862us/step - loss: 2637.4455 - mse: 20642666.0000 - mae: 2637.4456 - val_loss: 4197.2080 - val_mse: 70580104.0000 - val_mae: 4197.2080\n",
            "Epoch 298/500\n",
            "60/60 [==============================] - 0s 847us/step - loss: 2574.4530 - mse: 20285050.0000 - mae: 2574.4531 - val_loss: 4191.7687 - val_mse: 70167200.0000 - val_mae: 4191.7686\n",
            "Epoch 299/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2747.0738 - mse: 20990868.0000 - mae: 2747.0740 - val_loss: 4196.9215 - val_mse: 70275224.0000 - val_mae: 4196.9214\n",
            "Epoch 300/500\n",
            "60/60 [==============================] - 0s 954us/step - loss: 3013.7105 - mse: 24081358.0000 - mae: 3013.7104 - val_loss: 4083.4202 - val_mse: 69921760.0000 - val_mae: 4083.4204\n",
            "Epoch 301/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2796.8865 - mse: 21481604.0000 - mae: 2796.8867 - val_loss: 4179.4978 - val_mse: 70105560.0000 - val_mae: 4179.4980\n",
            "Epoch 302/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2651.4509 - mse: 16617744.0000 - mae: 2651.4509 - val_loss: 4100.0231 - val_mse: 69544216.0000 - val_mae: 4100.0229\n",
            "Epoch 303/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2966.5293 - mse: 24234374.0000 - mae: 2966.5291 - val_loss: 4420.1936 - val_mse: 70267584.0000 - val_mae: 4420.1938\n",
            "Epoch 304/500\n",
            "60/60 [==============================] - 0s 927us/step - loss: 2483.7640 - mse: 17019166.0000 - mae: 2483.7642 - val_loss: 4177.5003 - val_mse: 69805648.0000 - val_mae: 4177.5005\n",
            "Epoch 305/500\n",
            "60/60 [==============================] - 0s 888us/step - loss: 2875.5954 - mse: 19381596.0000 - mae: 2875.5955 - val_loss: 4191.4563 - val_mse: 70125784.0000 - val_mae: 4191.4561\n",
            "Epoch 306/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2460.2164 - mse: 16751089.0000 - mae: 2460.2163 - val_loss: 4206.8997 - val_mse: 69809128.0000 - val_mae: 4206.8994\n",
            "Epoch 307/500\n",
            "60/60 [==============================] - 0s 871us/step - loss: 2212.7495 - mse: 13294670.0000 - mae: 2212.7495 - val_loss: 4321.7100 - val_mse: 69831288.0000 - val_mae: 4321.7100\n",
            "Epoch 308/500\n",
            "60/60 [==============================] - 0s 907us/step - loss: 3206.9987 - mse: 29687092.0000 - mae: 3206.9990 - val_loss: 4180.6302 - val_mse: 69524144.0000 - val_mae: 4180.6304\n",
            "Epoch 309/500\n",
            "60/60 [==============================] - 0s 960us/step - loss: 2375.8286 - mse: 15728484.0000 - mae: 2375.8286 - val_loss: 4469.7254 - val_mse: 69740424.0000 - val_mae: 4469.7256\n",
            "Epoch 310/500\n",
            "60/60 [==============================] - 0s 955us/step - loss: 2632.6310 - mse: 18199378.0000 - mae: 2632.6311 - val_loss: 4243.0212 - val_mse: 69099696.0000 - val_mae: 4243.0215\n",
            "Epoch 311/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2581.9002 - mse: 18480190.0000 - mae: 2581.9001 - val_loss: 4025.3446 - val_mse: 68426008.0000 - val_mae: 4025.3445\n",
            "Epoch 312/500\n",
            "60/60 [==============================] - 0s 954us/step - loss: 2837.4279 - mse: 22382284.0000 - mae: 2837.4280 - val_loss: 4085.7096 - val_mse: 68701304.0000 - val_mae: 4085.7097\n",
            "Epoch 313/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3161.6050 - mse: 28646220.0000 - mae: 3161.6050 - val_loss: 4124.6129 - val_mse: 68767392.0000 - val_mae: 4124.6128\n",
            "Epoch 314/500\n",
            "60/60 [==============================] - 0s 994us/step - loss: 2272.7831 - mse: 12553309.0000 - mae: 2272.7830 - val_loss: 4059.3965 - val_mse: 68476536.0000 - val_mae: 4059.3967\n",
            "Epoch 315/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2832.7916 - mse: 26457158.0000 - mae: 2832.7917 - val_loss: 4056.4029 - val_mse: 68438160.0000 - val_mae: 4056.4026\n",
            "Epoch 316/500\n",
            "60/60 [==============================] - 0s 917us/step - loss: 2623.3660 - mse: 16914178.0000 - mae: 2623.3660 - val_loss: 4232.0498 - val_mse: 68589344.0000 - val_mae: 4232.0498\n",
            "Epoch 317/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2968.9018 - mse: 25288554.0000 - mae: 2968.9019 - val_loss: 4047.4127 - val_mse: 68393400.0000 - val_mae: 4047.4128\n",
            "Epoch 318/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2536.3893 - mse: 17219944.0000 - mae: 2536.3892 - val_loss: 4515.9713 - val_mse: 69353400.0000 - val_mae: 4515.9712\n",
            "Epoch 319/500\n",
            "60/60 [==============================] - 0s 873us/step - loss: 2661.0441 - mse: 18247386.0000 - mae: 2661.0439 - val_loss: 4110.0447 - val_mse: 68729384.0000 - val_mae: 4110.0449\n",
            "Epoch 320/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2791.9328 - mse: 20530148.0000 - mae: 2791.9329 - val_loss: 4467.5437 - val_mse: 69156320.0000 - val_mae: 4467.5439\n",
            "Epoch 321/500\n",
            "60/60 [==============================] - 0s 916us/step - loss: 3082.1153 - mse: 24397542.0000 - mae: 3082.1150 - val_loss: 4233.5999 - val_mse: 68634088.0000 - val_mae: 4233.5996\n",
            "Epoch 322/500\n",
            "60/60 [==============================] - 0s 884us/step - loss: 3001.4904 - mse: 22017180.0000 - mae: 3001.4907 - val_loss: 4237.1986 - val_mse: 68722960.0000 - val_mae: 4237.1982\n",
            "Epoch 323/500\n",
            "60/60 [==============================] - 0s 918us/step - loss: 2618.8803 - mse: 19039426.0000 - mae: 2618.8804 - val_loss: 4276.4238 - val_mse: 68653840.0000 - val_mae: 4276.4238\n",
            "Epoch 324/500\n",
            "60/60 [==============================] - 0s 939us/step - loss: 2656.0344 - mse: 19718602.0000 - mae: 2656.0344 - val_loss: 4288.5630 - val_mse: 68728784.0000 - val_mae: 4288.5630\n",
            "Epoch 325/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2916.4966 - mse: 23373824.0000 - mae: 2916.4966 - val_loss: 4114.2022 - val_mse: 68227112.0000 - val_mae: 4114.2021\n",
            "Epoch 326/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2546.6639 - mse: 20018986.0000 - mae: 2546.6638 - val_loss: 4190.3655 - val_mse: 68329888.0000 - val_mae: 4190.3652\n",
            "Epoch 327/500\n",
            "60/60 [==============================] - 0s 979us/step - loss: 3200.7121 - mse: 28262834.0000 - mae: 3200.7119 - val_loss: 4080.4749 - val_mse: 68250048.0000 - val_mae: 4080.4751\n",
            "Epoch 328/500\n",
            "60/60 [==============================] - 0s 802us/step - loss: 2993.1851 - mse: 23463044.0000 - mae: 2993.1848 - val_loss: 4120.1834 - val_mse: 68412552.0000 - val_mae: 4120.1831\n",
            "Epoch 329/500\n",
            "60/60 [==============================] - 0s 930us/step - loss: 2839.2687 - mse: 22163202.0000 - mae: 2839.2686 - val_loss: 4177.0802 - val_mse: 68394168.0000 - val_mae: 4177.0801\n",
            "Epoch 330/500\n",
            "60/60 [==============================] - 0s 950us/step - loss: 2654.3749 - mse: 17473090.0000 - mae: 2654.3750 - val_loss: 4435.4272 - val_mse: 68841568.0000 - val_mae: 4435.4272\n",
            "Epoch 331/500\n",
            "60/60 [==============================] - 0s 866us/step - loss: 2831.1630 - mse: 21383422.0000 - mae: 2831.1633 - val_loss: 4177.7033 - val_mse: 68175200.0000 - val_mae: 4177.7031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 80 samples, validate on 70 samples\n",
            "Epoch 1/500\n",
            "80/80 [==============================] - 2s 23ms/step - loss: 20378.7620 - mse: 916130112.0000 - mae: 20378.7617 - val_loss: 18686.5015 - val_mse: 743997504.0000 - val_mae: 18686.5020\n",
            "Epoch 2/500\n",
            "80/80 [==============================] - 0s 875us/step - loss: 20377.7231 - mse: 916112896.0000 - mae: 20377.7227 - val_loss: 18683.3140 - val_mse: 743952512.0000 - val_mae: 18683.3145\n",
            "Epoch 3/500\n",
            "80/80 [==============================] - 0s 794us/step - loss: 20372.3534 - mse: 916030464.0000 - mae: 20372.3535 - val_loss: 18674.6824 - val_mse: 743790080.0000 - val_mae: 18674.6816\n",
            "Epoch 4/500\n",
            "80/80 [==============================] - 0s 686us/step - loss: 20361.5498 - mse: 915776704.0000 - mae: 20361.5508 - val_loss: 18660.7556 - val_mse: 743407936.0000 - val_mae: 18660.7559\n",
            "Epoch 5/500\n",
            "80/80 [==============================] - 0s 677us/step - loss: 20347.0885 - mse: 915352256.0000 - mae: 20347.0879 - val_loss: 18643.1678 - val_mse: 742919744.0000 - val_mae: 18643.1680\n",
            "Epoch 6/500\n",
            "80/80 [==============================] - 0s 904us/step - loss: 20327.8025 - mse: 914771840.0000 - mae: 20327.8008 - val_loss: 18621.0313 - val_mse: 742294528.0000 - val_mae: 18621.0312\n",
            "Epoch 7/500\n",
            "80/80 [==============================] - 0s 765us/step - loss: 20300.9595 - mse: 913932672.0000 - mae: 20300.9590 - val_loss: 18593.1202 - val_mse: 741489536.0000 - val_mae: 18593.1211\n",
            "Epoch 8/500\n",
            "80/80 [==============================] - 0s 799us/step - loss: 20275.5685 - mse: 912992576.0000 - mae: 20275.5684 - val_loss: 18562.1166 - val_mse: 740550912.0000 - val_mae: 18562.1172\n",
            "Epoch 9/500\n",
            "80/80 [==============================] - 0s 785us/step - loss: 20248.8094 - mse: 911806080.0000 - mae: 20248.8086 - val_loss: 18525.9601 - val_mse: 739424576.0000 - val_mae: 18525.9609\n",
            "Epoch 10/500\n",
            "80/80 [==============================] - 0s 759us/step - loss: 20208.7699 - mse: 910536192.0000 - mae: 20208.7695 - val_loss: 18482.0904 - val_mse: 738088832.0000 - val_mae: 18482.0898\n",
            "Epoch 11/500\n",
            "80/80 [==============================] - 0s 798us/step - loss: 20164.8247 - mse: 908900544.0000 - mae: 20164.8242 - val_loss: 18428.3176 - val_mse: 736501056.0000 - val_mae: 18428.3184\n",
            "Epoch 12/500\n",
            "80/80 [==============================] - 0s 807us/step - loss: 20126.9297 - mse: 907462656.0000 - mae: 20126.9297 - val_loss: 18370.6663 - val_mse: 734747904.0000 - val_mae: 18370.6660\n",
            "Epoch 13/500\n",
            "80/80 [==============================] - 0s 808us/step - loss: 20080.9648 - mse: 906112704.0000 - mae: 20080.9648 - val_loss: 18306.9004 - val_mse: 732773568.0000 - val_mae: 18306.9004\n",
            "Epoch 14/500\n",
            "80/80 [==============================] - 0s 770us/step - loss: 20023.4344 - mse: 903104000.0000 - mae: 20023.4336 - val_loss: 18235.3120 - val_mse: 730429376.0000 - val_mae: 18235.3105\n",
            "Epoch 15/500\n",
            "80/80 [==============================] - 0s 807us/step - loss: 19981.0625 - mse: 900939264.0000 - mae: 19981.0625 - val_loss: 18161.4431 - val_mse: 727873280.0000 - val_mae: 18161.4434\n",
            "Epoch 16/500\n",
            "80/80 [==============================] - 0s 803us/step - loss: 19918.2943 - mse: 897223872.0000 - mae: 19918.2930 - val_loss: 18081.3942 - val_mse: 724966400.0000 - val_mae: 18081.3945\n",
            "Epoch 17/500\n",
            "80/80 [==============================] - 0s 883us/step - loss: 19818.7018 - mse: 892894080.0000 - mae: 19818.6992 - val_loss: 17995.9011 - val_mse: 721569472.0000 - val_mae: 17995.9004\n",
            "Epoch 18/500\n",
            "80/80 [==============================] - 0s 698us/step - loss: 19751.1686 - mse: 889033408.0000 - mae: 19751.1680 - val_loss: 17915.3660 - val_mse: 718059072.0000 - val_mae: 17915.3652\n",
            "Epoch 19/500\n",
            "80/80 [==============================] - 0s 785us/step - loss: 19675.1484 - mse: 883839296.0000 - mae: 19675.1484 - val_loss: 17832.3403 - val_mse: 714128896.0000 - val_mae: 17832.3398\n",
            "Epoch 20/500\n",
            "80/80 [==============================] - 0s 702us/step - loss: 19597.6475 - mse: 881508160.0000 - mae: 19597.6465 - val_loss: 17755.9123 - val_mse: 710487360.0000 - val_mae: 17755.9121\n",
            "Epoch 21/500\n",
            "80/80 [==============================] - 0s 901us/step - loss: 19557.5109 - mse: 878376448.0000 - mae: 19557.5098 - val_loss: 17675.1118 - val_mse: 706618048.0000 - val_mae: 17675.1113\n",
            "Epoch 22/500\n",
            "80/80 [==============================] - 0s 922us/step - loss: 19466.3557 - mse: 873558848.0000 - mae: 19466.3555 - val_loss: 17590.3661 - val_mse: 702557568.0000 - val_mae: 17590.3672\n",
            "Epoch 23/500\n",
            "80/80 [==============================] - 0s 692us/step - loss: 19466.0691 - mse: 871715648.0000 - mae: 19466.0684 - val_loss: 17508.3851 - val_mse: 698607936.0000 - val_mae: 17508.3848\n",
            "Epoch 24/500\n",
            "80/80 [==============================] - 0s 865us/step - loss: 19403.2601 - mse: 866585600.0000 - mae: 19403.2617 - val_loss: 17435.1498 - val_mse: 694995136.0000 - val_mae: 17435.1504\n",
            "Epoch 25/500\n",
            "80/80 [==============================] - 0s 835us/step - loss: 19245.2402 - mse: 856476224.0000 - mae: 19245.2402 - val_loss: 17361.9210 - val_mse: 691019008.0000 - val_mae: 17361.9219\n",
            "Epoch 26/500\n",
            "80/80 [==============================] - 0s 917us/step - loss: 19246.0244 - mse: 860785536.0000 - mae: 19246.0234 - val_loss: 17292.4425 - val_mse: 687134272.0000 - val_mae: 17292.4434\n",
            "Epoch 27/500\n",
            "80/80 [==============================] - 0s 764us/step - loss: 19123.4862 - mse: 848325504.0000 - mae: 19123.4863 - val_loss: 17220.1348 - val_mse: 682901248.0000 - val_mae: 17220.1348\n",
            "Epoch 28/500\n",
            "80/80 [==============================] - 0s 697us/step - loss: 19117.2688 - mse: 849695040.0000 - mae: 19117.2695 - val_loss: 17150.2166 - val_mse: 678593472.0000 - val_mae: 17150.2188\n",
            "Epoch 29/500\n",
            "80/80 [==============================] - 0s 901us/step - loss: 19069.3699 - mse: 845594496.0000 - mae: 19069.3711 - val_loss: 17076.0600 - val_mse: 673980864.0000 - val_mae: 17076.0605\n",
            "Epoch 30/500\n",
            "80/80 [==============================] - 0s 808us/step - loss: 18939.0337 - mse: 834848192.0000 - mae: 18939.0332 - val_loss: 16998.7600 - val_mse: 668550848.0000 - val_mae: 16998.7598\n",
            "Epoch 31/500\n",
            "80/80 [==============================] - 0s 831us/step - loss: 18975.8103 - mse: 834146432.0000 - mae: 18975.8105 - val_loss: 16931.5164 - val_mse: 663942144.0000 - val_mae: 16931.5156\n",
            "Epoch 32/500\n",
            "80/80 [==============================] - 0s 924us/step - loss: 18784.8685 - mse: 826103744.0000 - mae: 18784.8691 - val_loss: 16861.5232 - val_mse: 659056704.0000 - val_mae: 16861.5234\n",
            "Epoch 33/500\n",
            "80/80 [==============================] - 0s 729us/step - loss: 18754.9003 - mse: 823518592.0000 - mae: 18754.8984 - val_loss: 16796.7986 - val_mse: 654408576.0000 - val_mae: 16796.7988\n",
            "Epoch 34/500\n",
            "80/80 [==============================] - 0s 718us/step - loss: 18709.5372 - mse: 818615424.0000 - mae: 18709.5371 - val_loss: 16735.6176 - val_mse: 649981056.0000 - val_mae: 16735.6172\n",
            "Epoch 35/500\n",
            "80/80 [==============================] - 0s 953us/step - loss: 18642.0795 - mse: 816424064.0000 - mae: 18642.0781 - val_loss: 16488.8099 - val_mse: 644465408.0000 - val_mae: 16488.8086\n",
            "Epoch 36/500\n",
            "80/80 [==============================] - 0s 826us/step - loss: 18456.6213 - mse: 800736256.0000 - mae: 18456.6211 - val_loss: 16616.3110 - val_mse: 639805120.0000 - val_mae: 16616.3105\n",
            "Epoch 37/500\n",
            "80/80 [==============================] - 0s 736us/step - loss: 18179.5229 - mse: 792912064.0000 - mae: 18179.5234 - val_loss: 16348.6261 - val_mse: 632291200.0000 - val_mae: 16348.6250\n",
            "Epoch 38/500\n",
            "80/80 [==============================] - 0s 776us/step - loss: 18066.8289 - mse: 790127104.0000 - mae: 18066.8301 - val_loss: 16340.9743 - val_mse: 626127936.0000 - val_mae: 16340.9746\n",
            "Epoch 39/500\n",
            "80/80 [==============================] - 0s 864us/step - loss: 18067.7048 - mse: 791000704.0000 - mae: 18067.7051 - val_loss: 16271.3827 - val_mse: 619866432.0000 - val_mae: 16271.3818\n",
            "Epoch 40/500\n",
            "80/80 [==============================] - 0s 729us/step - loss: 17716.8582 - mse: 777249664.0000 - mae: 17716.8594 - val_loss: 15761.9006 - val_mse: 610851200.0000 - val_mae: 15761.9004\n",
            "Epoch 41/500\n",
            "80/80 [==============================] - 0s 699us/step - loss: 17599.3406 - mse: 768771328.0000 - mae: 17599.3398 - val_loss: 15660.1885 - val_mse: 603451520.0000 - val_mae: 15660.1895\n",
            "Epoch 42/500\n",
            "80/80 [==============================] - 0s 809us/step - loss: 17528.9639 - mse: 763268224.0000 - mae: 17528.9648 - val_loss: 15709.4616 - val_mse: 596708672.0000 - val_mae: 15709.4609\n",
            "Epoch 43/500\n",
            "80/80 [==============================] - 0s 795us/step - loss: 17273.6997 - mse: 745980928.0000 - mae: 17273.6992 - val_loss: 15552.7992 - val_mse: 588852224.0000 - val_mae: 15552.7998\n",
            "Epoch 44/500\n",
            "80/80 [==============================] - 0s 684us/step - loss: 17009.8325 - mse: 734664640.0000 - mae: 17009.8340 - val_loss: 15236.8944 - val_mse: 580123008.0000 - val_mae: 15236.8945\n",
            "Epoch 45/500\n",
            "80/80 [==============================] - 0s 942us/step - loss: 16854.9600 - mse: 719724800.0000 - mae: 16854.9590 - val_loss: 15048.1293 - val_mse: 572111872.0000 - val_mae: 15048.1289\n",
            "Epoch 46/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16775.1442 - mse: 725879616.0000 - mae: 16775.1445 - val_loss: 15136.1241 - val_mse: 565552960.0000 - val_mae: 15136.1250\n",
            "Epoch 47/500\n",
            "80/80 [==============================] - 0s 794us/step - loss: 16764.7293 - mse: 720030144.0000 - mae: 16764.7285 - val_loss: 14684.4375 - val_mse: 556722048.0000 - val_mae: 14684.4375\n",
            "Epoch 48/500\n",
            "80/80 [==============================] - 0s 854us/step - loss: 16437.6290 - mse: 702361280.0000 - mae: 16437.6289 - val_loss: 14560.8921 - val_mse: 548881408.0000 - val_mae: 14560.8916\n",
            "Epoch 49/500\n",
            "80/80 [==============================] - 0s 748us/step - loss: 16438.6579 - mse: 698791296.0000 - mae: 16438.6582 - val_loss: 14555.8119 - val_mse: 541472960.0000 - val_mae: 14555.8115\n",
            "Epoch 50/500\n",
            "80/80 [==============================] - 0s 851us/step - loss: 16439.3988 - mse: 704610816.0000 - mae: 16439.3984 - val_loss: 14483.7627 - val_mse: 535283776.0000 - val_mae: 14483.7627\n",
            "Epoch 51/500\n",
            "80/80 [==============================] - 0s 731us/step - loss: 15891.1741 - mse: 677184896.0000 - mae: 15891.1748 - val_loss: 14085.0467 - val_mse: 526214240.0000 - val_mae: 14085.0469\n",
            "Epoch 52/500\n",
            "80/80 [==============================] - 0s 835us/step - loss: 15945.9151 - mse: 669129344.0000 - mae: 15945.9160 - val_loss: 13929.1123 - val_mse: 519108384.0000 - val_mae: 13929.1123\n",
            "Epoch 53/500\n",
            "80/80 [==============================] - 0s 806us/step - loss: 15535.1584 - mse: 662216640.0000 - mae: 15535.1592 - val_loss: 13666.5781 - val_mse: 511616256.0000 - val_mae: 13666.5781\n",
            "Epoch 54/500\n",
            "80/80 [==============================] - 0s 707us/step - loss: 15634.3774 - mse: 664703232.0000 - mae: 15634.3779 - val_loss: 13792.0379 - val_mse: 505503968.0000 - val_mae: 13792.0371\n",
            "Epoch 55/500\n",
            "80/80 [==============================] - 0s 781us/step - loss: 15506.2198 - mse: 653796224.0000 - mae: 15506.2188 - val_loss: 13521.4900 - val_mse: 497627776.0000 - val_mae: 13521.4893\n",
            "Epoch 56/500\n",
            "80/80 [==============================] - 0s 888us/step - loss: 15061.3085 - mse: 631142720.0000 - mae: 15061.3076 - val_loss: 13303.0955 - val_mse: 490827104.0000 - val_mae: 13303.0957\n",
            "Epoch 57/500\n",
            "80/80 [==============================] - 0s 865us/step - loss: 15003.6078 - mse: 623978624.0000 - mae: 15003.6074 - val_loss: 13213.3415 - val_mse: 483793696.0000 - val_mae: 13213.3408\n",
            "Epoch 58/500\n",
            "80/80 [==============================] - 0s 718us/step - loss: 15097.5662 - mse: 623116480.0000 - mae: 15097.5654 - val_loss: 13143.3206 - val_mse: 477865056.0000 - val_mae: 13143.3203\n",
            "Epoch 59/500\n",
            "80/80 [==============================] - 0s 848us/step - loss: 14975.8354 - mse: 626654656.0000 - mae: 14975.8340 - val_loss: 12913.5602 - val_mse: 471152800.0000 - val_mae: 12913.5596\n",
            "Epoch 60/500\n",
            "80/80 [==============================] - 0s 688us/step - loss: 14875.7820 - mse: 623129728.0000 - mae: 14875.7812 - val_loss: 12796.0474 - val_mse: 465531456.0000 - val_mae: 12796.0479\n",
            "Epoch 61/500\n",
            "80/80 [==============================] - 0s 781us/step - loss: 14742.8787 - mse: 606199616.0000 - mae: 14742.8779 - val_loss: 12715.9208 - val_mse: 460155392.0000 - val_mae: 12715.9219\n",
            "Epoch 62/500\n",
            "80/80 [==============================] - 0s 928us/step - loss: 14466.9584 - mse: 585604416.0000 - mae: 14466.9580 - val_loss: 12509.6518 - val_mse: 453481344.0000 - val_mae: 12509.6514\n",
            "Epoch 63/500\n",
            "80/80 [==============================] - 0s 780us/step - loss: 14422.6130 - mse: 589732736.0000 - mae: 14422.6123 - val_loss: 12453.7161 - val_mse: 447313536.0000 - val_mae: 12453.7158\n",
            "Epoch 64/500\n",
            "80/80 [==============================] - 0s 678us/step - loss: 14394.9211 - mse: 591416192.0000 - mae: 14394.9199 - val_loss: 12347.2534 - val_mse: 442529120.0000 - val_mae: 12347.2539\n",
            "Epoch 65/500\n",
            "80/80 [==============================] - 0s 682us/step - loss: 13952.3917 - mse: 558876992.0000 - mae: 13952.3926 - val_loss: 12259.6123 - val_mse: 436603936.0000 - val_mae: 12259.6123\n",
            "Epoch 66/500\n",
            "80/80 [==============================] - 0s 789us/step - loss: 14459.5468 - mse: 579127104.0000 - mae: 14459.5469 - val_loss: 12216.8234 - val_mse: 431955488.0000 - val_mae: 12216.8232\n",
            "Epoch 67/500\n",
            "80/80 [==============================] - 0s 761us/step - loss: 14199.9240 - mse: 575988544.0000 - mae: 14199.9238 - val_loss: 12130.9683 - val_mse: 426696192.0000 - val_mae: 12130.9678\n",
            "Epoch 68/500\n",
            "80/80 [==============================] - 0s 821us/step - loss: 13937.9587 - mse: 542648768.0000 - mae: 13937.9590 - val_loss: 12085.1228 - val_mse: 421605216.0000 - val_mae: 12085.1230\n",
            "Epoch 69/500\n",
            "80/80 [==============================] - 0s 782us/step - loss: 13834.7299 - mse: 533345088.0000 - mae: 13834.7295 - val_loss: 12061.3033 - val_mse: 416765728.0000 - val_mae: 12061.3037\n",
            "Epoch 70/500\n",
            "80/80 [==============================] - 0s 681us/step - loss: 14017.0867 - mse: 550431744.0000 - mae: 14017.0859 - val_loss: 11902.5334 - val_mse: 411832192.0000 - val_mae: 11902.5342\n",
            "Epoch 71/500\n",
            "80/80 [==============================] - 0s 816us/step - loss: 14533.0677 - mse: 569792320.0000 - mae: 14533.0674 - val_loss: 11908.3386 - val_mse: 408996960.0000 - val_mae: 11908.3389\n",
            "Epoch 72/500\n",
            "80/80 [==============================] - 0s 767us/step - loss: 13978.9694 - mse: 543844736.0000 - mae: 13978.9688 - val_loss: 11826.1785 - val_mse: 404903776.0000 - val_mae: 11826.1787\n",
            "Epoch 73/500\n",
            "80/80 [==============================] - 0s 752us/step - loss: 13650.3720 - mse: 526631168.0000 - mae: 13650.3721 - val_loss: 11708.7146 - val_mse: 400598336.0000 - val_mae: 11708.7139\n",
            "Epoch 74/500\n",
            "80/80 [==============================] - 0s 751us/step - loss: 13568.3749 - mse: 513305248.0000 - mae: 13568.3770 - val_loss: 11683.2141 - val_mse: 396005152.0000 - val_mae: 11683.2139\n",
            "Epoch 75/500\n",
            "80/80 [==============================] - 0s 925us/step - loss: 14079.6398 - mse: 553383488.0000 - mae: 14079.6406 - val_loss: 11580.7824 - val_mse: 392765536.0000 - val_mae: 11580.7822\n",
            "Epoch 76/500\n",
            "80/80 [==============================] - 0s 907us/step - loss: 12953.9570 - mse: 488709216.0000 - mae: 12953.9580 - val_loss: 11598.7510 - val_mse: 387335680.0000 - val_mae: 11598.7520\n",
            "Epoch 77/500\n",
            "80/80 [==============================] - 0s 733us/step - loss: 13275.2037 - mse: 496114272.0000 - mae: 13275.2031 - val_loss: 11517.3837 - val_mse: 382824160.0000 - val_mae: 11517.3838\n",
            "Epoch 78/500\n",
            "80/80 [==============================] - 0s 821us/step - loss: 13161.7785 - mse: 495112864.0000 - mae: 13161.7793 - val_loss: 11458.8592 - val_mse: 378078048.0000 - val_mae: 11458.8594\n",
            "Epoch 79/500\n",
            "80/80 [==============================] - 0s 802us/step - loss: 13502.7124 - mse: 515333536.0000 - mae: 13502.7129 - val_loss: 11418.7273 - val_mse: 374671200.0000 - val_mae: 11418.7266\n",
            "Epoch 80/500\n",
            "80/80 [==============================] - 0s 788us/step - loss: 12952.7342 - mse: 493887168.0000 - mae: 12952.7344 - val_loss: 11214.9512 - val_mse: 369417056.0000 - val_mae: 11214.9512\n",
            "Epoch 81/500\n",
            "80/80 [==============================] - 0s 726us/step - loss: 12732.2875 - mse: 471556800.0000 - mae: 12732.2871 - val_loss: 11154.0190 - val_mse: 364458176.0000 - val_mae: 11154.0195\n",
            "Epoch 82/500\n",
            "80/80 [==============================] - 0s 918us/step - loss: 12735.6022 - mse: 478792864.0000 - mae: 12735.6025 - val_loss: 11097.6000 - val_mse: 360064672.0000 - val_mae: 11097.5996\n",
            "Epoch 83/500\n",
            "80/80 [==============================] - 0s 774us/step - loss: 12869.1992 - mse: 483978304.0000 - mae: 12869.1992 - val_loss: 10986.4762 - val_mse: 355036192.0000 - val_mae: 10986.4766\n",
            "Epoch 84/500\n",
            "80/80 [==============================] - 0s 719us/step - loss: 12732.0552 - mse: 489411648.0000 - mae: 12732.0566 - val_loss: 10898.2809 - val_mse: 349552576.0000 - val_mae: 10898.2812\n",
            "Epoch 85/500\n",
            "80/80 [==============================] - 0s 908us/step - loss: 13360.1213 - mse: 498150816.0000 - mae: 13360.1221 - val_loss: 10850.4225 - val_mse: 345640800.0000 - val_mae: 10850.4219\n",
            "Epoch 86/500\n",
            "80/80 [==============================] - 0s 879us/step - loss: 12277.5649 - mse: 442634080.0000 - mae: 12277.5645 - val_loss: 10771.4697 - val_mse: 340718944.0000 - val_mae: 10771.4697\n",
            "Epoch 87/500\n",
            "80/80 [==============================] - 0s 747us/step - loss: 12299.4709 - mse: 448538016.0000 - mae: 12299.4707 - val_loss: 10738.1465 - val_mse: 335575040.0000 - val_mae: 10738.1465\n",
            "Epoch 88/500\n",
            "80/80 [==============================] - 0s 816us/step - loss: 12165.8411 - mse: 442196832.0000 - mae: 12165.8418 - val_loss: 10551.8580 - val_mse: 330530144.0000 - val_mae: 10551.8574\n",
            "Epoch 89/500\n",
            "80/80 [==============================] - 0s 808us/step - loss: 12019.4719 - mse: 432647584.0000 - mae: 12019.4717 - val_loss: 10465.9427 - val_mse: 325330624.0000 - val_mae: 10465.9424\n",
            "Epoch 90/500\n",
            "80/80 [==============================] - 0s 787us/step - loss: 12435.3798 - mse: 456261984.0000 - mae: 12435.3799 - val_loss: 10371.3561 - val_mse: 321103008.0000 - val_mae: 10371.3564\n",
            "Epoch 91/500\n",
            "80/80 [==============================] - 0s 789us/step - loss: 12444.7164 - mse: 449515936.0000 - mae: 12444.7158 - val_loss: 10249.4180 - val_mse: 317327616.0000 - val_mae: 10249.4180\n",
            "Epoch 92/500\n",
            "80/80 [==============================] - 0s 792us/step - loss: 12099.1960 - mse: 449096128.0000 - mae: 12099.1963 - val_loss: 10152.6771 - val_mse: 313004608.0000 - val_mae: 10152.6768\n",
            "Epoch 93/500\n",
            "80/80 [==============================] - 0s 776us/step - loss: 11697.0350 - mse: 423096288.0000 - mae: 11697.0352 - val_loss: 9977.3282 - val_mse: 307430240.0000 - val_mae: 9977.3281\n",
            "Epoch 94/500\n",
            "80/80 [==============================] - 0s 776us/step - loss: 11935.3975 - mse: 425440544.0000 - mae: 11935.3975 - val_loss: 9819.2692 - val_mse: 302185920.0000 - val_mae: 9819.2695\n",
            "Epoch 95/500\n",
            "80/80 [==============================] - 0s 822us/step - loss: 11597.9268 - mse: 403519296.0000 - mae: 11597.9277 - val_loss: 9529.6091 - val_mse: 296413280.0000 - val_mae: 9529.6094\n",
            "Epoch 96/500\n",
            "80/80 [==============================] - 0s 768us/step - loss: 11726.7332 - mse: 389933120.0000 - mae: 11726.7344 - val_loss: 9413.7766 - val_mse: 290985600.0000 - val_mae: 9413.7764\n",
            "Epoch 97/500\n",
            "80/80 [==============================] - 0s 739us/step - loss: 11366.9494 - mse: 405281280.0000 - mae: 11366.9502 - val_loss: 9299.8899 - val_mse: 285430048.0000 - val_mae: 9299.8896\n",
            "Epoch 98/500\n",
            "80/80 [==============================] - 0s 866us/step - loss: 11356.0859 - mse: 395961280.0000 - mae: 11356.0859 - val_loss: 9202.8103 - val_mse: 280509248.0000 - val_mae: 9202.8105\n",
            "Epoch 99/500\n",
            "80/80 [==============================] - 0s 823us/step - loss: 10728.7653 - mse: 379693920.0000 - mae: 10728.7656 - val_loss: 8990.6195 - val_mse: 274180160.0000 - val_mae: 8990.6201\n",
            "Epoch 100/500\n",
            "80/80 [==============================] - 0s 737us/step - loss: 10934.1789 - mse: 357557056.0000 - mae: 10934.1787 - val_loss: 8927.2020 - val_mse: 268812384.0000 - val_mae: 8927.2021\n",
            "Epoch 101/500\n",
            "80/80 [==============================] - 0s 697us/step - loss: 10768.1708 - mse: 371943488.0000 - mae: 10768.1709 - val_loss: 8778.4149 - val_mse: 264106624.0000 - val_mae: 8778.4150\n",
            "Epoch 102/500\n",
            "80/80 [==============================] - 0s 823us/step - loss: 11346.8657 - mse: 383412448.0000 - mae: 11346.8652 - val_loss: 8839.7477 - val_mse: 259960272.0000 - val_mae: 8839.7480\n",
            "Epoch 103/500\n",
            "80/80 [==============================] - 0s 789us/step - loss: 11082.2907 - mse: 384834336.0000 - mae: 11082.2900 - val_loss: 8628.8231 - val_mse: 254963632.0000 - val_mae: 8628.8232\n",
            "Epoch 104/500\n",
            "80/80 [==============================] - 0s 777us/step - loss: 10998.5091 - mse: 368413792.0000 - mae: 10998.5088 - val_loss: 8638.8921 - val_mse: 251110928.0000 - val_mae: 8638.8916\n",
            "Epoch 105/500\n",
            "80/80 [==============================] - 0s 844us/step - loss: 10545.8656 - mse: 335240000.0000 - mae: 10545.8652 - val_loss: 8639.1053 - val_mse: 246665184.0000 - val_mae: 8639.1055\n",
            "Epoch 106/500\n",
            "80/80 [==============================] - 0s 829us/step - loss: 10759.8946 - mse: 367767744.0000 - mae: 10759.8945 - val_loss: 8570.8125 - val_mse: 241464128.0000 - val_mae: 8570.8125\n",
            "Epoch 107/500\n",
            "80/80 [==============================] - 0s 743us/step - loss: 10710.2083 - mse: 361596352.0000 - mae: 10710.2080 - val_loss: 8358.1564 - val_mse: 237513248.0000 - val_mae: 8358.1562\n",
            "Epoch 108/500\n",
            "80/80 [==============================] - 0s 762us/step - loss: 10654.5906 - mse: 357743168.0000 - mae: 10654.5908 - val_loss: 8255.2538 - val_mse: 232679152.0000 - val_mae: 8255.2539\n",
            "Epoch 109/500\n",
            "80/80 [==============================] - 0s 880us/step - loss: 9971.6734 - mse: 321055904.0000 - mae: 9971.6738 - val_loss: 8143.5747 - val_mse: 227479520.0000 - val_mae: 8143.5752\n",
            "Epoch 110/500\n",
            "80/80 [==============================] - 0s 813us/step - loss: 10111.3144 - mse: 325155264.0000 - mae: 10111.3145 - val_loss: 8254.6281 - val_mse: 222774640.0000 - val_mae: 8254.6279\n",
            "Epoch 111/500\n",
            "80/80 [==============================] - 0s 745us/step - loss: 9851.1659 - mse: 319140512.0000 - mae: 9851.1660 - val_loss: 8193.0499 - val_mse: 218257136.0000 - val_mae: 8193.0498\n",
            "Epoch 112/500\n",
            "80/80 [==============================] - 0s 767us/step - loss: 9744.4551 - mse: 307273120.0000 - mae: 9744.4551 - val_loss: 8114.0020 - val_mse: 214912080.0000 - val_mae: 8114.0020\n",
            "Epoch 113/500\n",
            "80/80 [==============================] - 0s 937us/step - loss: 10040.1688 - mse: 322121792.0000 - mae: 10040.1680 - val_loss: 8081.9651 - val_mse: 210988416.0000 - val_mae: 8081.9653\n",
            "Epoch 114/500\n",
            "80/80 [==============================] - 0s 777us/step - loss: 9530.4501 - mse: 311154176.0000 - mae: 9530.4502 - val_loss: 7971.3763 - val_mse: 208584496.0000 - val_mae: 7971.3770\n",
            "Epoch 115/500\n",
            "80/80 [==============================] - 0s 690us/step - loss: 9910.1208 - mse: 317839360.0000 - mae: 9910.1221 - val_loss: 7696.1901 - val_mse: 203465936.0000 - val_mae: 7696.1899\n",
            "Epoch 116/500\n",
            "80/80 [==============================] - 0s 776us/step - loss: 9964.6377 - mse: 309137856.0000 - mae: 9964.6377 - val_loss: 7686.8220 - val_mse: 201728000.0000 - val_mae: 7686.8213\n",
            "Epoch 117/500\n",
            "80/80 [==============================] - 0s 833us/step - loss: 9479.6280 - mse: 285853888.0000 - mae: 9479.6279 - val_loss: 7739.3463 - val_mse: 198201840.0000 - val_mae: 7739.3462\n",
            "Epoch 118/500\n",
            "80/80 [==============================] - 0s 771us/step - loss: 9394.2383 - mse: 280137152.0000 - mae: 9394.2383 - val_loss: 7746.6411 - val_mse: 194831440.0000 - val_mae: 7746.6411\n",
            "Epoch 119/500\n",
            "80/80 [==============================] - 0s 733us/step - loss: 9441.3169 - mse: 295326784.0000 - mae: 9441.3174 - val_loss: 7882.7110 - val_mse: 193854416.0000 - val_mae: 7882.7109\n",
            "Epoch 120/500\n",
            "80/80 [==============================] - 0s 723us/step - loss: 9818.5383 - mse: 295569344.0000 - mae: 9818.5381 - val_loss: 7675.6596 - val_mse: 190499120.0000 - val_mae: 7675.6587\n",
            "Epoch 121/500\n",
            "80/80 [==============================] - 0s 896us/step - loss: 9808.4600 - mse: 306223040.0000 - mae: 9808.4590 - val_loss: 7606.2743 - val_mse: 187493216.0000 - val_mae: 7606.2749\n",
            "Epoch 122/500\n",
            "80/80 [==============================] - 0s 845us/step - loss: 9143.3628 - mse: 273436512.0000 - mae: 9143.3633 - val_loss: 7603.0544 - val_mse: 184314304.0000 - val_mae: 7603.0547\n",
            "Epoch 123/500\n",
            "80/80 [==============================] - 0s 793us/step - loss: 9267.4261 - mse: 283011648.0000 - mae: 9267.4268 - val_loss: 7426.4317 - val_mse: 180692144.0000 - val_mae: 7426.4316\n",
            "Epoch 124/500\n",
            "80/80 [==============================] - 0s 767us/step - loss: 9784.6523 - mse: 289697088.0000 - mae: 9784.6514 - val_loss: 7325.4971 - val_mse: 177552400.0000 - val_mae: 7325.4971\n",
            "Epoch 125/500\n",
            "80/80 [==============================] - 0s 737us/step - loss: 9189.7563 - mse: 244205360.0000 - mae: 9189.7568 - val_loss: 7504.7228 - val_mse: 174776016.0000 - val_mae: 7504.7231\n",
            "Epoch 126/500\n",
            "80/80 [==============================] - 0s 880us/step - loss: 9133.0055 - mse: 258446128.0000 - mae: 9133.0059 - val_loss: 7499.5576 - val_mse: 171403184.0000 - val_mae: 7499.5571\n",
            "Epoch 127/500\n",
            "80/80 [==============================] - 0s 753us/step - loss: 8589.5302 - mse: 234767712.0000 - mae: 8589.5293 - val_loss: 7288.2396 - val_mse: 168383296.0000 - val_mae: 7288.2397\n",
            "Epoch 128/500\n",
            "80/80 [==============================] - 0s 763us/step - loss: 8681.9552 - mse: 220828000.0000 - mae: 8681.9551 - val_loss: 7200.8229 - val_mse: 165631856.0000 - val_mae: 7200.8232\n",
            "Epoch 129/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 9553.3760 - mse: 280078464.0000 - mae: 9553.3770 - val_loss: 7352.1040 - val_mse: 164163344.0000 - val_mae: 7352.1040\n",
            "Epoch 130/500\n",
            "80/80 [==============================] - 0s 826us/step - loss: 8961.0101 - mse: 244209248.0000 - mae: 8961.0098 - val_loss: 7302.3883 - val_mse: 162591504.0000 - val_mae: 7302.3882\n",
            "Epoch 131/500\n",
            "80/80 [==============================] - 0s 781us/step - loss: 8292.9566 - mse: 212740320.0000 - mae: 8292.9561 - val_loss: 7125.9527 - val_mse: 159515088.0000 - val_mae: 7125.9526\n",
            "Epoch 132/500\n",
            "80/80 [==============================] - 0s 757us/step - loss: 9252.5782 - mse: 262802896.0000 - mae: 9252.5781 - val_loss: 7075.0747 - val_mse: 157671984.0000 - val_mae: 7075.0747\n",
            "Epoch 133/500\n",
            "80/80 [==============================] - 0s 824us/step - loss: 8470.6477 - mse: 227010656.0000 - mae: 8470.6475 - val_loss: 7050.9032 - val_mse: 155080944.0000 - val_mae: 7050.9033\n",
            "Epoch 134/500\n",
            "80/80 [==============================] - 0s 808us/step - loss: 8949.3240 - mse: 232890064.0000 - mae: 8949.3232 - val_loss: 7156.8030 - val_mse: 153439024.0000 - val_mae: 7156.8032\n",
            "Epoch 135/500\n",
            "80/80 [==============================] - 0s 876us/step - loss: 8758.1739 - mse: 240544976.0000 - mae: 8758.1738 - val_loss: 7078.1227 - val_mse: 151770544.0000 - val_mae: 7078.1226\n",
            "Epoch 136/500\n",
            "80/80 [==============================] - 0s 852us/step - loss: 8727.2446 - mse: 236697552.0000 - mae: 8727.2441 - val_loss: 7083.0212 - val_mse: 149176256.0000 - val_mae: 7083.0210\n",
            "Epoch 137/500\n",
            "80/80 [==============================] - 0s 911us/step - loss: 9326.0756 - mse: 243416544.0000 - mae: 9326.0762 - val_loss: 6954.9105 - val_mse: 147276320.0000 - val_mae: 6954.9106\n",
            "Epoch 138/500\n",
            "80/80 [==============================] - 0s 789us/step - loss: 8159.1715 - mse: 221244768.0000 - mae: 8159.1719 - val_loss: 7013.0324 - val_mse: 144703744.0000 - val_mae: 7013.0322\n",
            "Epoch 139/500\n",
            "80/80 [==============================] - 0s 843us/step - loss: 9115.1116 - mse: 246558304.0000 - mae: 9115.1113 - val_loss: 6996.9141 - val_mse: 142658704.0000 - val_mae: 6996.9141\n",
            "Epoch 140/500\n",
            "80/80 [==============================] - 0s 784us/step - loss: 8775.8990 - mse: 229480928.0000 - mae: 8775.8984 - val_loss: 6829.5522 - val_mse: 140407360.0000 - val_mae: 6829.5522\n",
            "Epoch 141/500\n",
            "80/80 [==============================] - 0s 712us/step - loss: 8707.3812 - mse: 237327792.0000 - mae: 8707.3809 - val_loss: 6989.5221 - val_mse: 139539760.0000 - val_mae: 6989.5220\n",
            "Epoch 142/500\n",
            "80/80 [==============================] - 0s 947us/step - loss: 8133.1466 - mse: 187162576.0000 - mae: 8133.1470 - val_loss: 6873.0715 - val_mse: 137337696.0000 - val_mae: 6873.0713\n",
            "Epoch 143/500\n",
            "80/80 [==============================] - 0s 825us/step - loss: 9233.1779 - mse: 248613712.0000 - mae: 9233.1777 - val_loss: 6719.3431 - val_mse: 135767824.0000 - val_mae: 6719.3428\n",
            "Epoch 144/500\n",
            "80/80 [==============================] - 0s 731us/step - loss: 8512.1449 - mse: 219096544.0000 - mae: 8512.1455 - val_loss: 6671.5713 - val_mse: 133983672.0000 - val_mae: 6671.5713\n",
            "Epoch 145/500\n",
            "80/80 [==============================] - 0s 850us/step - loss: 9264.1400 - mse: 251030432.0000 - mae: 9264.1396 - val_loss: 6658.5961 - val_mse: 132694864.0000 - val_mae: 6658.5962\n",
            "Epoch 146/500\n",
            "80/80 [==============================] - 0s 798us/step - loss: 8028.1989 - mse: 205093104.0000 - mae: 8028.1992 - val_loss: 6535.0872 - val_mse: 131051624.0000 - val_mae: 6535.0874\n",
            "Epoch 147/500\n",
            "80/80 [==============================] - 0s 856us/step - loss: 8838.3372 - mse: 233634976.0000 - mae: 8838.3369 - val_loss: 6632.1412 - val_mse: 130389800.0000 - val_mae: 6632.1411\n",
            "Epoch 148/500\n",
            "80/80 [==============================] - 0s 833us/step - loss: 7985.1820 - mse: 200971168.0000 - mae: 7985.1821 - val_loss: 6565.6615 - val_mse: 128721192.0000 - val_mae: 6565.6616\n",
            "Epoch 149/500\n",
            "80/80 [==============================] - 0s 789us/step - loss: 8170.0331 - mse: 201312288.0000 - mae: 8170.0327 - val_loss: 6613.2722 - val_mse: 127432576.0000 - val_mae: 6613.2725\n",
            "Epoch 150/500\n",
            "80/80 [==============================] - 0s 889us/step - loss: 8202.5724 - mse: 190100288.0000 - mae: 8202.5723 - val_loss: 6634.7415 - val_mse: 126484200.0000 - val_mae: 6634.7417\n",
            "Epoch 151/500\n",
            "80/80 [==============================] - 0s 733us/step - loss: 8204.5307 - mse: 195107552.0000 - mae: 8204.5312 - val_loss: 6389.5437 - val_mse: 124471336.0000 - val_mae: 6389.5439\n",
            "Epoch 152/500\n",
            "80/80 [==============================] - 0s 794us/step - loss: 7364.8633 - mse: 155782688.0000 - mae: 7364.8633 - val_loss: 6451.1427 - val_mse: 122765896.0000 - val_mae: 6451.1431\n",
            "Epoch 153/500\n",
            "80/80 [==============================] - 0s 742us/step - loss: 8045.5088 - mse: 195379056.0000 - mae: 8045.5093 - val_loss: 6365.2749 - val_mse: 121344424.0000 - val_mae: 6365.2749\n",
            "Epoch 154/500\n",
            "80/80 [==============================] - 0s 823us/step - loss: 8029.3625 - mse: 196174304.0000 - mae: 8029.3623 - val_loss: 6491.2557 - val_mse: 120689800.0000 - val_mae: 6491.2554\n",
            "Epoch 155/500\n",
            "80/80 [==============================] - 0s 956us/step - loss: 7538.7211 - mse: 160366416.0000 - mae: 7538.7212 - val_loss: 6495.4754 - val_mse: 120468552.0000 - val_mae: 6495.4751\n",
            "Epoch 156/500\n",
            "80/80 [==============================] - 0s 719us/step - loss: 8715.8438 - mse: 213089504.0000 - mae: 8715.8438 - val_loss: 6397.2554 - val_mse: 119143560.0000 - val_mae: 6397.2554\n",
            "Epoch 157/500\n",
            "80/80 [==============================] - 0s 724us/step - loss: 7788.2202 - mse: 170813024.0000 - mae: 7788.2202 - val_loss: 6441.2798 - val_mse: 118314368.0000 - val_mae: 6441.2798\n",
            "Epoch 158/500\n",
            "80/80 [==============================] - 0s 869us/step - loss: 8244.8245 - mse: 179882784.0000 - mae: 8244.8252 - val_loss: 6311.7381 - val_mse: 116545536.0000 - val_mae: 6311.7378\n",
            "Epoch 159/500\n",
            "80/80 [==============================] - 0s 901us/step - loss: 7740.7483 - mse: 174663072.0000 - mae: 7740.7485 - val_loss: 6248.4659 - val_mse: 115561808.0000 - val_mae: 6248.4663\n",
            "Epoch 160/500\n",
            "80/80 [==============================] - 0s 847us/step - loss: 7862.0775 - mse: 174607104.0000 - mae: 7862.0781 - val_loss: 6203.4352 - val_mse: 114123152.0000 - val_mae: 6203.4351\n",
            "Epoch 161/500\n",
            "80/80 [==============================] - 0s 807us/step - loss: 8082.2202 - mse: 200379568.0000 - mae: 8082.2197 - val_loss: 6376.0486 - val_mse: 113911592.0000 - val_mae: 6376.0483\n",
            "Epoch 162/500\n",
            "80/80 [==============================] - 0s 811us/step - loss: 9020.5372 - mse: 224755632.0000 - mae: 9020.5371 - val_loss: 6349.1726 - val_mse: 113215048.0000 - val_mae: 6349.1724\n",
            "Epoch 163/500\n",
            "80/80 [==============================] - 0s 937us/step - loss: 8488.2265 - mse: 201526192.0000 - mae: 8488.2266 - val_loss: 6313.6580 - val_mse: 112196512.0000 - val_mae: 6313.6582\n",
            "Epoch 164/500\n",
            "80/80 [==============================] - 0s 837us/step - loss: 7641.6175 - mse: 169163232.0000 - mae: 7641.6172 - val_loss: 6238.5797 - val_mse: 111270256.0000 - val_mae: 6238.5796\n",
            "Epoch 165/500\n",
            "80/80 [==============================] - 0s 802us/step - loss: 8312.4949 - mse: 194166432.0000 - mae: 8312.4951 - val_loss: 6194.6516 - val_mse: 110388744.0000 - val_mae: 6194.6519\n",
            "Epoch 166/500\n",
            "80/80 [==============================] - 0s 702us/step - loss: 7882.9852 - mse: 191502528.0000 - mae: 7882.9854 - val_loss: 6102.9480 - val_mse: 109043528.0000 - val_mae: 6102.9482\n",
            "Epoch 167/500\n",
            "80/80 [==============================] - 0s 765us/step - loss: 8012.2254 - mse: 174798112.0000 - mae: 8012.2256 - val_loss: 6235.1651 - val_mse: 109021184.0000 - val_mae: 6235.1650\n",
            "Epoch 168/500\n",
            "80/80 [==============================] - 0s 671us/step - loss: 8112.3390 - mse: 193267744.0000 - mae: 8112.3389 - val_loss: 6348.0486 - val_mse: 108557072.0000 - val_mae: 6348.0483\n",
            "Epoch 169/500\n",
            "80/80 [==============================] - 0s 901us/step - loss: 8089.5091 - mse: 173383280.0000 - mae: 8089.5093 - val_loss: 6474.5329 - val_mse: 109351664.0000 - val_mae: 6474.5332\n",
            "Epoch 170/500\n",
            "80/80 [==============================] - 0s 720us/step - loss: 7310.6577 - mse: 154702896.0000 - mae: 7310.6572 - val_loss: 6252.4462 - val_mse: 106954104.0000 - val_mae: 6252.4458\n",
            "Epoch 171/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 7373.8183 - mse: 159421824.0000 - mae: 7373.8179 - val_loss: 6157.2422 - val_mse: 105524880.0000 - val_mae: 6157.2422\n",
            "Epoch 172/500\n",
            "80/80 [==============================] - 0s 735us/step - loss: 8219.1025 - mse: 165975904.0000 - mae: 8219.1035 - val_loss: 6058.3409 - val_mse: 105329384.0000 - val_mae: 6058.3413\n",
            "Epoch 173/500\n",
            "80/80 [==============================] - 0s 777us/step - loss: 8404.7251 - mse: 201725408.0000 - mae: 8404.7246 - val_loss: 6224.0164 - val_mse: 105377688.0000 - val_mae: 6224.0166\n",
            "Epoch 174/500\n",
            "80/80 [==============================] - 0s 763us/step - loss: 8098.4116 - mse: 185294368.0000 - mae: 8098.4116 - val_loss: 6315.2783 - val_mse: 105850776.0000 - val_mae: 6315.2788\n",
            "Epoch 175/500\n",
            "80/80 [==============================] - 0s 832us/step - loss: 8418.4955 - mse: 197154208.0000 - mae: 8418.4961 - val_loss: 6416.5811 - val_mse: 105685544.0000 - val_mae: 6416.5811\n",
            "Epoch 176/500\n",
            "80/80 [==============================] - 0s 878us/step - loss: 7384.0542 - mse: 146042272.0000 - mae: 7384.0537 - val_loss: 6453.8709 - val_mse: 105356808.0000 - val_mae: 6453.8711\n",
            "Epoch 177/500\n",
            "80/80 [==============================] - 0s 797us/step - loss: 7676.7863 - mse: 178633328.0000 - mae: 7676.7866 - val_loss: 6387.7653 - val_mse: 105008432.0000 - val_mae: 6387.7651\n",
            "Epoch 178/500\n",
            "80/80 [==============================] - 0s 764us/step - loss: 8145.2507 - mse: 186805536.0000 - mae: 8145.2500 - val_loss: 6232.2390 - val_mse: 104325864.0000 - val_mae: 6232.2393\n",
            "Epoch 179/500\n",
            "80/80 [==============================] - 0s 900us/step - loss: 8157.0513 - mse: 183337568.0000 - mae: 8157.0518 - val_loss: 6195.3658 - val_mse: 103490328.0000 - val_mae: 6195.3662\n",
            "Epoch 180/500\n",
            "80/80 [==============================] - 0s 850us/step - loss: 6962.3774 - mse: 144104784.0000 - mae: 6962.3774 - val_loss: 6233.4260 - val_mse: 103346688.0000 - val_mae: 6233.4258\n",
            "Epoch 181/500\n",
            "80/80 [==============================] - 0s 831us/step - loss: 7722.4997 - mse: 168156176.0000 - mae: 7722.4990 - val_loss: 6215.3253 - val_mse: 102761432.0000 - val_mae: 6215.3252\n",
            "Epoch 182/500\n",
            "80/80 [==============================] - 0s 773us/step - loss: 7659.1748 - mse: 160814176.0000 - mae: 7659.1748 - val_loss: 6362.0301 - val_mse: 102726088.0000 - val_mae: 6362.0303\n",
            "Epoch 183/500\n",
            "80/80 [==============================] - 0s 877us/step - loss: 7419.3152 - mse: 149708368.0000 - mae: 7419.3149 - val_loss: 6182.2074 - val_mse: 101393728.0000 - val_mae: 6182.2070\n",
            "Epoch 184/500\n",
            "80/80 [==============================] - 0s 738us/step - loss: 7678.0787 - mse: 164177696.0000 - mae: 7678.0791 - val_loss: 6125.8267 - val_mse: 101200928.0000 - val_mae: 6125.8267\n",
            "Epoch 185/500\n",
            "80/80 [==============================] - 0s 835us/step - loss: 7669.5951 - mse: 154368864.0000 - mae: 7669.5947 - val_loss: 6316.9132 - val_mse: 102152456.0000 - val_mae: 6316.9136\n",
            "Epoch 186/500\n",
            "80/80 [==============================] - 0s 795us/step - loss: 6428.0496 - mse: 113799552.0000 - mae: 6428.0498 - val_loss: 6288.3524 - val_mse: 102044544.0000 - val_mae: 6288.3525\n",
            "Epoch 187/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 7618.4334 - mse: 165918672.0000 - mae: 7618.4336 - val_loss: 6186.8773 - val_mse: 101656960.0000 - val_mae: 6186.8774\n",
            "Epoch 188/500\n",
            "80/80 [==============================] - 0s 823us/step - loss: 6430.8593 - mse: 124768064.0000 - mae: 6430.8589 - val_loss: 6367.8011 - val_mse: 100882184.0000 - val_mae: 6367.8008\n",
            "Epoch 189/500\n",
            "80/80 [==============================] - 0s 822us/step - loss: 7458.4958 - mse: 174473040.0000 - mae: 7458.4951 - val_loss: 6337.5543 - val_mse: 99840176.0000 - val_mae: 6337.5547\n",
            "Epoch 190/500\n",
            "80/80 [==============================] - 0s 805us/step - loss: 7178.1526 - mse: 163125216.0000 - mae: 7178.1523 - val_loss: 6097.3764 - val_mse: 99302968.0000 - val_mae: 6097.3760\n",
            "Epoch 191/500\n",
            "80/80 [==============================] - 0s 935us/step - loss: 6574.3061 - mse: 123915152.0000 - mae: 6574.3062 - val_loss: 6263.8297 - val_mse: 100674256.0000 - val_mae: 6263.8296\n",
            "Epoch 192/500\n",
            "80/80 [==============================] - 0s 841us/step - loss: 8158.5538 - mse: 184121936.0000 - mae: 8158.5537 - val_loss: 6177.2477 - val_mse: 99551536.0000 - val_mae: 6177.2480\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 90 samples, validate on 90 samples\n",
            "Epoch 1/500\n",
            "90/90 [==============================] - 2s 26ms/step - loss: 27657.0602 - mse: 1635659904.0000 - mae: 27657.0605 - val_loss: 32678.2256 - val_mse: 2082540032.0000 - val_mae: 32678.2285\n",
            "Epoch 2/500\n",
            "90/90 [==============================] - 0s 819us/step - loss: 27650.0822 - mse: 1635537152.0000 - mae: 27650.0801 - val_loss: 32663.3263 - val_mse: 2082175232.0000 - val_mae: 32663.3281\n",
            "Epoch 3/500\n",
            "90/90 [==============================] - 0s 782us/step - loss: 27632.1408 - mse: 1635237888.0000 - mae: 27632.1387 - val_loss: 32644.7546 - val_mse: 2081692800.0000 - val_mae: 32644.7559\n",
            "Epoch 4/500\n",
            "90/90 [==============================] - 0s 802us/step - loss: 27609.7287 - mse: 1634817536.0000 - mae: 27609.7285 - val_loss: 32617.3246 - val_mse: 2080969728.0000 - val_mae: 32617.3242\n",
            "Epoch 5/500\n",
            "90/90 [==============================] - 0s 779us/step - loss: 27578.0419 - mse: 1634167808.0000 - mae: 27578.0410 - val_loss: 32588.2152 - val_mse: 2080138880.0000 - val_mae: 32588.2168\n",
            "Epoch 6/500\n",
            "90/90 [==============================] - 0s 825us/step - loss: 27559.1428 - mse: 1633585280.0000 - mae: 27559.1426 - val_loss: 32560.7805 - val_mse: 2079264256.0000 - val_mae: 32560.7812\n",
            "Epoch 7/500\n",
            "90/90 [==============================] - 0s 803us/step - loss: 27541.6766 - mse: 1633078656.0000 - mae: 27541.6758 - val_loss: 32532.2257 - val_mse: 2078276864.0000 - val_mae: 32532.2246\n",
            "Epoch 8/500\n",
            "90/90 [==============================] - 0s 748us/step - loss: 27498.4863 - mse: 1632235776.0000 - mae: 27498.4863 - val_loss: 32499.6112 - val_mse: 2077065984.0000 - val_mae: 32499.6113\n",
            "Epoch 9/500\n",
            "90/90 [==============================] - 0s 886us/step - loss: 27476.6209 - mse: 1630905216.0000 - mae: 27476.6230 - val_loss: 32465.8598 - val_mse: 2075765376.0000 - val_mae: 32465.8613\n",
            "Epoch 10/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 27445.9281 - mse: 1629772416.0000 - mae: 27445.9277 - val_loss: 32429.5248 - val_mse: 2074294912.0000 - val_mae: 32429.5254\n",
            "Epoch 11/500\n",
            "90/90 [==============================] - 0s 998us/step - loss: 27432.1602 - mse: 1628660096.0000 - mae: 27432.1602 - val_loss: 32392.4880 - val_mse: 2072714368.0000 - val_mae: 32392.4883\n",
            "Epoch 12/500\n",
            "90/90 [==============================] - 0s 954us/step - loss: 27402.2292 - mse: 1627222144.0000 - mae: 27402.2285 - val_loss: 32354.8057 - val_mse: 2071062784.0000 - val_mae: 32354.8047\n",
            "Epoch 13/500\n",
            "90/90 [==============================] - 0s 820us/step - loss: 27356.2358 - mse: 1625686144.0000 - mae: 27356.2363 - val_loss: 32310.9126 - val_mse: 2069092096.0000 - val_mae: 32310.9102\n",
            "Epoch 14/500\n",
            "90/90 [==============================] - 0s 722us/step - loss: 27322.8899 - mse: 1624335104.0000 - mae: 27322.8926 - val_loss: 32265.1494 - val_mse: 2066938112.0000 - val_mae: 32265.1504\n",
            "Epoch 15/500\n",
            "90/90 [==============================] - 0s 745us/step - loss: 27354.0658 - mse: 1623536256.0000 - mae: 27354.0664 - val_loss: 32226.1547 - val_mse: 2064940288.0000 - val_mae: 32226.1562\n",
            "Epoch 16/500\n",
            "90/90 [==============================] - 0s 743us/step - loss: 27254.5673 - mse: 1621038464.0000 - mae: 27254.5645 - val_loss: 32178.2827 - val_mse: 2062461952.0000 - val_mae: 32178.2832\n",
            "Epoch 17/500\n",
            "90/90 [==============================] - 0s 776us/step - loss: 27232.0525 - mse: 1619747200.0000 - mae: 27232.0527 - val_loss: 32133.7337 - val_mse: 2060012032.0000 - val_mae: 32133.7324\n",
            "Epoch 18/500\n",
            "90/90 [==============================] - 0s 738us/step - loss: 27197.1024 - mse: 1617518464.0000 - mae: 27197.1035 - val_loss: 32085.3577 - val_mse: 2057207424.0000 - val_mae: 32085.3574\n",
            "Epoch 19/500\n",
            "90/90 [==============================] - 0s 797us/step - loss: 27178.3848 - mse: 1615159808.0000 - mae: 27178.3867 - val_loss: 32040.2883 - val_mse: 2054384512.0000 - val_mae: 32040.2891\n",
            "Epoch 20/500\n",
            "90/90 [==============================] - 0s 886us/step - loss: 27117.3273 - mse: 1611725568.0000 - mae: 27117.3281 - val_loss: 31988.5653 - val_mse: 2050842240.0000 - val_mae: 31988.5664\n",
            "Epoch 21/500\n",
            "90/90 [==============================] - 0s 841us/step - loss: 26995.7466 - mse: 1606282240.0000 - mae: 26995.7441 - val_loss: 31911.9824 - val_mse: 2043563264.0000 - val_mae: 31911.9824\n",
            "Epoch 22/500\n",
            "90/90 [==============================] - 0s 840us/step - loss: 27028.4884 - mse: 1604003968.0000 - mae: 27028.4883 - val_loss: 31834.8472 - val_mse: 2035779328.0000 - val_mae: 31834.8496\n",
            "Epoch 23/500\n",
            "90/90 [==============================] - 0s 736us/step - loss: 26913.2145 - mse: 1594031744.0000 - mae: 26913.2148 - val_loss: 31754.3322 - val_mse: 2027522176.0000 - val_mae: 31754.3340\n",
            "Epoch 24/500\n",
            "90/90 [==============================] - 0s 732us/step - loss: 26784.3276 - mse: 1583777792.0000 - mae: 26784.3301 - val_loss: 31648.9100 - val_mse: 2016335360.0000 - val_mae: 31648.9102\n",
            "Epoch 25/500\n",
            "90/90 [==============================] - 0s 792us/step - loss: 26686.1704 - mse: 1571901952.0000 - mae: 26686.1719 - val_loss: 31541.1786 - val_mse: 2005315456.0000 - val_mae: 31541.1777\n",
            "Epoch 26/500\n",
            "90/90 [==============================] - 0s 953us/step - loss: 26569.2630 - mse: 1563608192.0000 - mae: 26569.2617 - val_loss: 31444.6189 - val_mse: 1996389888.0000 - val_mae: 31444.6191\n",
            "Epoch 27/500\n",
            "90/90 [==============================] - 0s 773us/step - loss: 26589.5773 - mse: 1558801408.0000 - mae: 26589.5781 - val_loss: 31368.6412 - val_mse: 1989880320.0000 - val_mae: 31368.6387\n",
            "Epoch 28/500\n",
            "90/90 [==============================] - 0s 847us/step - loss: 26482.1366 - mse: 1557719680.0000 - mae: 26482.1387 - val_loss: 31294.3585 - val_mse: 1983382912.0000 - val_mae: 31294.3574\n",
            "Epoch 29/500\n",
            "90/90 [==============================] - 0s 868us/step - loss: 26503.8108 - mse: 1555591808.0000 - mae: 26503.8086 - val_loss: 31223.0159 - val_mse: 1976927104.0000 - val_mae: 31223.0137\n",
            "Epoch 30/500\n",
            "90/90 [==============================] - 0s 732us/step - loss: 26344.7145 - mse: 1542554752.0000 - mae: 26344.7148 - val_loss: 31150.8114 - val_mse: 1970310400.0000 - val_mae: 31150.8105\n",
            "Epoch 31/500\n",
            "90/90 [==============================] - 0s 850us/step - loss: 26353.6743 - mse: 1540519296.0000 - mae: 26353.6719 - val_loss: 31084.1743 - val_mse: 1964100096.0000 - val_mae: 31084.1758\n",
            "Epoch 32/500\n",
            "90/90 [==============================] - 0s 764us/step - loss: 26331.4201 - mse: 1533221760.0000 - mae: 26331.4199 - val_loss: 31025.3758 - val_mse: 1958415872.0000 - val_mae: 31025.3750\n",
            "Epoch 33/500\n",
            "90/90 [==============================] - 0s 747us/step - loss: 26127.1478 - mse: 1521017600.0000 - mae: 26127.1465 - val_loss: 30955.0621 - val_mse: 1951658624.0000 - val_mae: 30955.0605\n",
            "Epoch 34/500\n",
            "90/90 [==============================] - 0s 899us/step - loss: 26203.2357 - mse: 1529097600.0000 - mae: 26203.2363 - val_loss: 30895.6472 - val_mse: 1945980672.0000 - val_mae: 30895.6465\n",
            "Epoch 35/500\n",
            "90/90 [==============================] - 0s 780us/step - loss: 26063.5942 - mse: 1524016896.0000 - mae: 26063.5938 - val_loss: 30829.0359 - val_mse: 1939418624.0000 - val_mae: 30829.0352\n",
            "Epoch 36/500\n",
            "90/90 [==============================] - 0s 764us/step - loss: 25877.4695 - mse: 1496994304.0000 - mae: 25877.4688 - val_loss: 30756.3547 - val_mse: 1932052096.0000 - val_mae: 30756.3555\n",
            "Epoch 37/500\n",
            "90/90 [==============================] - 0s 720us/step - loss: 26051.0052 - mse: 1511865344.0000 - mae: 26051.0078 - val_loss: 30695.4488 - val_mse: 1925858048.0000 - val_mae: 30695.4492\n",
            "Epoch 38/500\n",
            "90/90 [==============================] - 0s 786us/step - loss: 25890.3606 - mse: 1496447616.0000 - mae: 25890.3613 - val_loss: 30628.5154 - val_mse: 1918875520.0000 - val_mae: 30628.5137\n",
            "Epoch 39/500\n",
            "90/90 [==============================] - 0s 755us/step - loss: 25920.9005 - mse: 1505259648.0000 - mae: 25920.9004 - val_loss: 30573.6846 - val_mse: 1913060224.0000 - val_mae: 30573.6836\n",
            "Epoch 40/500\n",
            "90/90 [==============================] - 0s 924us/step - loss: 25854.6155 - mse: 1494859904.0000 - mae: 25854.6133 - val_loss: 30523.3257 - val_mse: 1907625216.0000 - val_mae: 30523.3281\n",
            "Epoch 41/500\n",
            "90/90 [==============================] - 0s 849us/step - loss: 25765.2113 - mse: 1485124736.0000 - mae: 25765.2109 - val_loss: 30458.6513 - val_mse: 1900660096.0000 - val_mae: 30458.6504\n",
            "Epoch 42/500\n",
            "90/90 [==============================] - 0s 820us/step - loss: 25729.2266 - mse: 1487724672.0000 - mae: 25729.2285 - val_loss: 30405.2508 - val_mse: 1894922880.0000 - val_mae: 30405.2500\n",
            "Epoch 43/500\n",
            "90/90 [==============================] - 0s 812us/step - loss: 25702.5859 - mse: 1487298176.0000 - mae: 25702.5859 - val_loss: 30352.3545 - val_mse: 1889275136.0000 - val_mae: 30352.3555\n",
            "Epoch 44/500\n",
            "90/90 [==============================] - 0s 780us/step - loss: 25634.3543 - mse: 1474032128.0000 - mae: 25634.3535 - val_loss: 30304.0799 - val_mse: 1884131968.0000 - val_mae: 30304.0781\n",
            "Epoch 45/500\n",
            "90/90 [==============================] - 0s 736us/step - loss: 25615.9796 - mse: 1476622464.0000 - mae: 25615.9805 - val_loss: 30240.6781 - val_mse: 1877423104.0000 - val_mae: 30240.6777\n",
            "Epoch 46/500\n",
            "90/90 [==============================] - 0s 982us/step - loss: 25554.3320 - mse: 1467842816.0000 - mae: 25554.3340 - val_loss: 30186.8762 - val_mse: 1871739136.0000 - val_mae: 30186.8770\n",
            "Epoch 47/500\n",
            "90/90 [==============================] - 0s 766us/step - loss: 25489.7125 - mse: 1458625408.0000 - mae: 25489.7109 - val_loss: 30136.8964 - val_mse: 1866471168.0000 - val_mae: 30136.8945\n",
            "Epoch 48/500\n",
            "90/90 [==============================] - 0s 793us/step - loss: 25436.1170 - mse: 1447462144.0000 - mae: 25436.1172 - val_loss: 30092.5821 - val_mse: 1861793792.0000 - val_mae: 30092.5840\n",
            "Epoch 49/500\n",
            "90/90 [==============================] - 0s 863us/step - loss: 25453.3787 - mse: 1457935104.0000 - mae: 25453.3809 - val_loss: 30040.5646 - val_mse: 1856388352.0000 - val_mae: 30040.5664\n",
            "Epoch 50/500\n",
            "90/90 [==============================] - 0s 754us/step - loss: 25414.1122 - mse: 1440496384.0000 - mae: 25414.1133 - val_loss: 29970.2886 - val_mse: 1849925120.0000 - val_mae: 29970.2891\n",
            "Epoch 51/500\n",
            "90/90 [==============================] - 0s 810us/step - loss: 25366.3741 - mse: 1434007040.0000 - mae: 25366.3730 - val_loss: 29855.9114 - val_mse: 1843990784.0000 - val_mae: 29855.9102\n",
            "Epoch 52/500\n",
            "90/90 [==============================] - 0s 832us/step - loss: 25150.8726 - mse: 1434042496.0000 - mae: 25150.8730 - val_loss: 29771.8332 - val_mse: 1836994816.0000 - val_mae: 29771.8340\n",
            "Epoch 53/500\n",
            "90/90 [==============================] - 0s 892us/step - loss: 24891.4540 - mse: 1408828288.0000 - mae: 24891.4551 - val_loss: 29706.7889 - val_mse: 1827568896.0000 - val_mae: 29706.7891\n",
            "Epoch 54/500\n",
            "90/90 [==============================] - 0s 802us/step - loss: 25026.4912 - mse: 1435722880.0000 - mae: 25026.4883 - val_loss: 29594.0760 - val_mse: 1821133312.0000 - val_mae: 29594.0742\n",
            "Epoch 55/500\n",
            "90/90 [==============================] - 0s 803us/step - loss: 24966.9149 - mse: 1416233088.0000 - mae: 24966.9141 - val_loss: 29515.1360 - val_mse: 1812449536.0000 - val_mae: 29515.1367\n",
            "Epoch 56/500\n",
            "90/90 [==============================] - 0s 718us/step - loss: 24910.4604 - mse: 1414608256.0000 - mae: 24910.4609 - val_loss: 29423.6168 - val_mse: 1805704320.0000 - val_mae: 29423.6172\n",
            "Epoch 57/500\n",
            "90/90 [==============================] - 0s 743us/step - loss: 24810.7093 - mse: 1411207040.0000 - mae: 24810.7109 - val_loss: 29304.9564 - val_mse: 1795910272.0000 - val_mae: 29304.9551\n",
            "Epoch 58/500\n",
            "90/90 [==============================] - 0s 855us/step - loss: 24768.7097 - mse: 1398177920.0000 - mae: 24768.7109 - val_loss: 29213.1942 - val_mse: 1788064896.0000 - val_mae: 29213.1953\n",
            "Epoch 59/500\n",
            "90/90 [==============================] - 0s 829us/step - loss: 24413.1653 - mse: 1383127680.0000 - mae: 24413.1641 - val_loss: 29094.6332 - val_mse: 1777496832.0000 - val_mae: 29094.6328\n",
            "Epoch 60/500\n",
            "90/90 [==============================] - 0s 746us/step - loss: 24411.6284 - mse: 1389624064.0000 - mae: 24411.6270 - val_loss: 29134.9959 - val_mse: 1770934144.0000 - val_mae: 29134.9941\n",
            "Epoch 61/500\n",
            "90/90 [==============================] - 0s 805us/step - loss: 24222.1491 - mse: 1368173952.0000 - mae: 24222.1504 - val_loss: 28882.1164 - val_mse: 1758315648.0000 - val_mae: 28882.1172\n",
            "Epoch 62/500\n",
            "90/90 [==============================] - 0s 773us/step - loss: 24144.8666 - mse: 1354964224.0000 - mae: 24144.8672 - val_loss: 28754.1019 - val_mse: 1748431104.0000 - val_mae: 28754.1035\n",
            "Epoch 63/500\n",
            "90/90 [==============================] - 0s 764us/step - loss: 24187.6043 - mse: 1357767168.0000 - mae: 24187.6055 - val_loss: 28636.5626 - val_mse: 1738712448.0000 - val_mae: 28636.5645\n",
            "Epoch 64/500\n",
            "90/90 [==============================] - 0s 855us/step - loss: 23996.4058 - mse: 1343041024.0000 - mae: 23996.4062 - val_loss: 28595.5350 - val_mse: 1728616704.0000 - val_mae: 28595.5352\n",
            "Epoch 65/500\n",
            "90/90 [==============================] - 0s 741us/step - loss: 23889.0665 - mse: 1345999360.0000 - mae: 23889.0664 - val_loss: 28595.5306 - val_mse: 1721918336.0000 - val_mae: 28595.5312\n",
            "Epoch 66/500\n",
            "90/90 [==============================] - 0s 716us/step - loss: 23811.4636 - mse: 1325806080.0000 - mae: 23811.4609 - val_loss: 28339.4912 - val_mse: 1708699136.0000 - val_mae: 28339.4922\n",
            "Epoch 67/500\n",
            "90/90 [==============================] - 0s 990us/step - loss: 23490.8050 - mse: 1311316736.0000 - mae: 23490.8047 - val_loss: 28154.1249 - val_mse: 1698692224.0000 - val_mae: 28154.1250\n",
            "Epoch 68/500\n",
            "90/90 [==============================] - 0s 731us/step - loss: 23388.2194 - mse: 1309491072.0000 - mae: 23388.2188 - val_loss: 28033.3253 - val_mse: 1687590272.0000 - val_mae: 28033.3242\n",
            "Epoch 69/500\n",
            "90/90 [==============================] - 0s 839us/step - loss: 23251.5948 - mse: 1298226048.0000 - mae: 23251.5938 - val_loss: 27967.3495 - val_mse: 1676290688.0000 - val_mae: 27967.3496\n",
            "Epoch 70/500\n",
            "90/90 [==============================] - 0s 842us/step - loss: 22952.5616 - mse: 1277602816.0000 - mae: 22952.5625 - val_loss: 27825.2605 - val_mse: 1666519296.0000 - val_mae: 27825.2617\n",
            "Epoch 71/500\n",
            "90/90 [==============================] - 0s 785us/step - loss: 23210.9486 - mse: 1281730688.0000 - mae: 23210.9473 - val_loss: 27738.1255 - val_mse: 1654568704.0000 - val_mae: 27738.1250\n",
            "Epoch 72/500\n",
            "90/90 [==============================] - 0s 748us/step - loss: 23050.9499 - mse: 1291905280.0000 - mae: 23050.9492 - val_loss: 27534.4536 - val_mse: 1644453376.0000 - val_mae: 27534.4551\n",
            "Epoch 73/500\n",
            "90/90 [==============================] - 0s 954us/step - loss: 23018.2841 - mse: 1276367360.0000 - mae: 23018.2812 - val_loss: 27381.3167 - val_mse: 1632861056.0000 - val_mae: 27381.3164\n",
            "Epoch 74/500\n",
            "90/90 [==============================] - 0s 994us/step - loss: 22657.7903 - mse: 1248944640.0000 - mae: 22657.7910 - val_loss: 27300.2408 - val_mse: 1622048000.0000 - val_mae: 27300.2383\n",
            "Epoch 75/500\n",
            "90/90 [==============================] - 0s 777us/step - loss: 22716.4328 - mse: 1246205696.0000 - mae: 22716.4336 - val_loss: 27275.9897 - val_mse: 1613692928.0000 - val_mae: 27275.9883\n",
            "Epoch 76/500\n",
            "90/90 [==============================] - 0s 855us/step - loss: 22539.7683 - mse: 1244136064.0000 - mae: 22539.7695 - val_loss: 26946.3123 - val_mse: 1600413568.0000 - val_mae: 26946.3105\n",
            "Epoch 77/500\n",
            "90/90 [==============================] - 0s 853us/step - loss: 22607.0957 - mse: 1249894528.0000 - mae: 22607.0977 - val_loss: 27044.1652 - val_mse: 1591263616.0000 - val_mae: 27044.1641\n",
            "Epoch 78/500\n",
            "90/90 [==============================] - 0s 785us/step - loss: 22120.4590 - mse: 1217797376.0000 - mae: 22120.4590 - val_loss: 26880.7171 - val_mse: 1579301504.0000 - val_mae: 26880.7168\n",
            "Epoch 79/500\n",
            "90/90 [==============================] - 0s 843us/step - loss: 22055.6887 - mse: 1229966208.0000 - mae: 22055.6895 - val_loss: 26660.4953 - val_mse: 1567222400.0000 - val_mae: 26660.4941\n",
            "Epoch 80/500\n",
            "90/90 [==============================] - 0s 747us/step - loss: 21841.4059 - mse: 1204879232.0000 - mae: 21841.4062 - val_loss: 26378.3212 - val_mse: 1555126912.0000 - val_mae: 26378.3223\n",
            "Epoch 81/500\n",
            "90/90 [==============================] - 0s 804us/step - loss: 22422.8563 - mse: 1250347264.0000 - mae: 22422.8574 - val_loss: 26239.4479 - val_mse: 1544128640.0000 - val_mae: 26239.4473\n",
            "Epoch 82/500\n",
            "90/90 [==============================] - 0s 739us/step - loss: 21541.7206 - mse: 1166749056.0000 - mae: 21541.7207 - val_loss: 26096.8696 - val_mse: 1531251584.0000 - val_mae: 26096.8691\n",
            "Epoch 83/500\n",
            "90/90 [==============================] - 0s 905us/step - loss: 21909.3364 - mse: 1195470976.0000 - mae: 21909.3359 - val_loss: 26013.0516 - val_mse: 1520950912.0000 - val_mae: 26013.0508\n",
            "Epoch 84/500\n",
            "90/90 [==============================] - 0s 784us/step - loss: 21695.9810 - mse: 1201049088.0000 - mae: 21695.9805 - val_loss: 25791.5808 - val_mse: 1510562432.0000 - val_mae: 25791.5801\n",
            "Epoch 85/500\n",
            "90/90 [==============================] - 0s 821us/step - loss: 21406.8026 - mse: 1154848640.0000 - mae: 21406.8008 - val_loss: 25653.3182 - val_mse: 1499616256.0000 - val_mae: 25653.3164\n",
            "Epoch 86/500\n",
            "90/90 [==============================] - 0s 757us/step - loss: 21021.6710 - mse: 1148237184.0000 - mae: 21021.6719 - val_loss: 25706.9383 - val_mse: 1489977088.0000 - val_mae: 25706.9395\n",
            "Epoch 87/500\n",
            "90/90 [==============================] - 0s 713us/step - loss: 21026.7932 - mse: 1125637120.0000 - mae: 21026.7930 - val_loss: 25386.1459 - val_mse: 1478870272.0000 - val_mae: 25386.1445\n",
            "Epoch 88/500\n",
            "90/90 [==============================] - 0s 990us/step - loss: 21022.6000 - mse: 1126765440.0000 - mae: 21022.5996 - val_loss: 25224.4915 - val_mse: 1466739584.0000 - val_mae: 25224.4922\n",
            "Epoch 89/500\n",
            "90/90 [==============================] - 0s 796us/step - loss: 21079.2529 - mse: 1107832320.0000 - mae: 21079.2520 - val_loss: 25224.3823 - val_mse: 1457020800.0000 - val_mae: 25224.3828\n",
            "Epoch 90/500\n",
            "90/90 [==============================] - 0s 772us/step - loss: 20748.5211 - mse: 1104454528.0000 - mae: 20748.5215 - val_loss: 25002.0859 - val_mse: 1446422784.0000 - val_mae: 25002.0859\n",
            "Epoch 91/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 20500.0948 - mse: 1084773120.0000 - mae: 20500.0938 - val_loss: 24827.0558 - val_mse: 1435899008.0000 - val_mae: 24827.0547\n",
            "Epoch 92/500\n",
            "90/90 [==============================] - 0s 853us/step - loss: 20749.1111 - mse: 1108894080.0000 - mae: 20749.1113 - val_loss: 24682.6176 - val_mse: 1424377856.0000 - val_mae: 24682.6172\n",
            "Epoch 93/500\n",
            "90/90 [==============================] - 0s 843us/step - loss: 20573.6349 - mse: 1120269952.0000 - mae: 20573.6348 - val_loss: 24613.3366 - val_mse: 1415226240.0000 - val_mae: 24613.3398\n",
            "Epoch 94/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 19844.2405 - mse: 1045206080.0000 - mae: 19844.2383 - val_loss: 24418.9006 - val_mse: 1403593088.0000 - val_mae: 24418.9004\n",
            "Epoch 95/500\n",
            "90/90 [==============================] - 0s 755us/step - loss: 20639.5850 - mse: 1105567232.0000 - mae: 20639.5840 - val_loss: 24426.5105 - val_mse: 1394711296.0000 - val_mae: 24426.5117\n",
            "Epoch 96/500\n",
            "90/90 [==============================] - 0s 751us/step - loss: 20530.7860 - mse: 1074213760.0000 - mae: 20530.7852 - val_loss: 24226.0775 - val_mse: 1383834368.0000 - val_mae: 24226.0781\n",
            "Epoch 97/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 20295.8740 - mse: 1063430464.0000 - mae: 20295.8750 - val_loss: 24148.5209 - val_mse: 1373502336.0000 - val_mae: 24148.5215\n",
            "Epoch 98/500\n",
            "90/90 [==============================] - 0s 762us/step - loss: 19754.2619 - mse: 1034196736.0000 - mae: 19754.2617 - val_loss: 23955.3267 - val_mse: 1363004288.0000 - val_mae: 23955.3281\n",
            "Epoch 99/500\n",
            "90/90 [==============================] - 0s 813us/step - loss: 19688.5440 - mse: 1022680704.0000 - mae: 19688.5449 - val_loss: 23845.6416 - val_mse: 1352394624.0000 - val_mae: 23845.6426\n",
            "Epoch 100/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 19762.6013 - mse: 1031587328.0000 - mae: 19762.5996 - val_loss: 23727.3165 - val_mse: 1341751168.0000 - val_mae: 23727.3164\n",
            "Epoch 101/500\n",
            "90/90 [==============================] - 0s 801us/step - loss: 19980.7835 - mse: 1064791680.0000 - mae: 19980.7852 - val_loss: 23645.5575 - val_mse: 1332267904.0000 - val_mae: 23645.5547\n",
            "Epoch 102/500\n",
            "90/90 [==============================] - 0s 803us/step - loss: 19844.7533 - mse: 1036773056.0000 - mae: 19844.7520 - val_loss: 23683.7848 - val_mse: 1323421056.0000 - val_mae: 23683.7832\n",
            "Epoch 103/500\n",
            "90/90 [==============================] - 0s 860us/step - loss: 19440.1002 - mse: 1004731264.0000 - mae: 19440.0996 - val_loss: 23469.5614 - val_mse: 1314573184.0000 - val_mae: 23469.5605\n",
            "Epoch 104/500\n",
            "90/90 [==============================] - 0s 835us/step - loss: 19339.2549 - mse: 1013730496.0000 - mae: 19339.2559 - val_loss: 23487.3585 - val_mse: 1304825984.0000 - val_mae: 23487.3574\n",
            "Epoch 105/500\n",
            "90/90 [==============================] - 0s 841us/step - loss: 19438.3171 - mse: 1004110208.0000 - mae: 19438.3164 - val_loss: 23201.3172 - val_mse: 1293944576.0000 - val_mae: 23201.3164\n",
            "Epoch 106/500\n",
            "90/90 [==============================] - 0s 796us/step - loss: 19803.0503 - mse: 1003745728.0000 - mae: 19803.0508 - val_loss: 23230.3082 - val_mse: 1286643840.0000 - val_mae: 23230.3086\n",
            "Epoch 107/500\n",
            "90/90 [==============================] - 0s 768us/step - loss: 19328.2990 - mse: 985085696.0000 - mae: 19328.3008 - val_loss: 22991.6964 - val_mse: 1275061376.0000 - val_mae: 22991.6953\n",
            "Epoch 108/500\n",
            "90/90 [==============================] - 0s 748us/step - loss: 18679.4580 - mse: 932415808.0000 - mae: 18679.4590 - val_loss: 22978.4894 - val_mse: 1263078912.0000 - val_mae: 22978.4883\n",
            "Epoch 109/500\n",
            "90/90 [==============================] - 0s 835us/step - loss: 18943.3108 - mse: 978281728.0000 - mae: 18943.3105 - val_loss: 22768.4013 - val_mse: 1254891648.0000 - val_mae: 22768.4023\n",
            "Epoch 110/500\n",
            "90/90 [==============================] - 0s 755us/step - loss: 18601.7238 - mse: 922578688.0000 - mae: 18601.7246 - val_loss: 22901.9836 - val_mse: 1245605632.0000 - val_mae: 22901.9824\n",
            "Epoch 111/500\n",
            "90/90 [==============================] - 0s 786us/step - loss: 19126.9749 - mse: 979208320.0000 - mae: 19126.9746 - val_loss: 22537.3497 - val_mse: 1234735360.0000 - val_mae: 22537.3496\n",
            "Epoch 112/500\n",
            "90/90 [==============================] - 0s 843us/step - loss: 19070.2134 - mse: 954680192.0000 - mae: 19070.2129 - val_loss: 22450.5222 - val_mse: 1225372800.0000 - val_mae: 22450.5215\n",
            "Epoch 113/500\n",
            "90/90 [==============================] - 0s 759us/step - loss: 18461.9976 - mse: 914075072.0000 - mae: 18461.9980 - val_loss: 22417.1272 - val_mse: 1217508992.0000 - val_mae: 22417.1270\n",
            "Epoch 114/500\n",
            "90/90 [==============================] - 0s 742us/step - loss: 18229.8597 - mse: 909393280.0000 - mae: 18229.8613 - val_loss: 22320.3280 - val_mse: 1205715328.0000 - val_mae: 22320.3262\n",
            "Epoch 115/500\n",
            "90/90 [==============================] - 0s 795us/step - loss: 18482.9735 - mse: 934464512.0000 - mae: 18482.9727 - val_loss: 22096.5261 - val_mse: 1196771840.0000 - val_mae: 22096.5254\n",
            "Epoch 116/500\n",
            "90/90 [==============================] - 0s 801us/step - loss: 18092.7397 - mse: 884743104.0000 - mae: 18092.7402 - val_loss: 21974.5298 - val_mse: 1185526784.0000 - val_mae: 21974.5312\n",
            "Epoch 117/500\n",
            "90/90 [==============================] - 0s 839us/step - loss: 19056.9480 - mse: 947594496.0000 - mae: 19056.9492 - val_loss: 21916.3103 - val_mse: 1176745856.0000 - val_mae: 21916.3105\n",
            "Epoch 118/500\n",
            "90/90 [==============================] - 0s 828us/step - loss: 18538.3985 - mse: 900530752.0000 - mae: 18538.3984 - val_loss: 21748.1984 - val_mse: 1167151104.0000 - val_mae: 21748.1992\n",
            "Epoch 119/500\n",
            "90/90 [==============================] - 0s 769us/step - loss: 18014.6172 - mse: 873042304.0000 - mae: 18014.6172 - val_loss: 21642.3856 - val_mse: 1156120192.0000 - val_mae: 21642.3867\n",
            "Epoch 120/500\n",
            "90/90 [==============================] - 0s 816us/step - loss: 18003.3338 - mse: 862424896.0000 - mae: 18003.3340 - val_loss: 21528.8803 - val_mse: 1148775808.0000 - val_mae: 21528.8809\n",
            "Epoch 121/500\n",
            "90/90 [==============================] - 0s 808us/step - loss: 17315.3116 - mse: 796361088.0000 - mae: 17315.3105 - val_loss: 21640.0501 - val_mse: 1140577664.0000 - val_mae: 21640.0508\n",
            "Epoch 122/500\n",
            "90/90 [==============================] - 0s 855us/step - loss: 17680.0911 - mse: 852399424.0000 - mae: 17680.0898 - val_loss: 21439.7449 - val_mse: 1129276288.0000 - val_mae: 21439.7441\n",
            "Epoch 123/500\n",
            "90/90 [==============================] - 0s 779us/step - loss: 17794.7403 - mse: 836055040.0000 - mae: 17794.7402 - val_loss: 21171.3296 - val_mse: 1119826432.0000 - val_mae: 21171.3301\n",
            "Epoch 124/500\n",
            "90/90 [==============================] - 0s 860us/step - loss: 17855.8115 - mse: 861983808.0000 - mae: 17855.8105 - val_loss: 21117.7664 - val_mse: 1112104576.0000 - val_mae: 21117.7676\n",
            "Epoch 125/500\n",
            "90/90 [==============================] - 0s 741us/step - loss: 17582.1796 - mse: 861827456.0000 - mae: 17582.1797 - val_loss: 20947.2279 - val_mse: 1101914112.0000 - val_mae: 20947.2285\n",
            "Epoch 126/500\n",
            "90/90 [==============================] - 0s 735us/step - loss: 17093.5285 - mse: 801065216.0000 - mae: 17093.5273 - val_loss: 20817.9928 - val_mse: 1091144704.0000 - val_mae: 20817.9941\n",
            "Epoch 127/500\n",
            "90/90 [==============================] - 0s 837us/step - loss: 17645.6101 - mse: 848959168.0000 - mae: 17645.6094 - val_loss: 20707.8082 - val_mse: 1081692544.0000 - val_mae: 20707.8086\n",
            "Epoch 128/500\n",
            "90/90 [==============================] - 0s 791us/step - loss: 17339.2127 - mse: 808714816.0000 - mae: 17339.2109 - val_loss: 20649.5048 - val_mse: 1073911168.0000 - val_mae: 20649.5039\n",
            "Epoch 129/500\n",
            "90/90 [==============================] - 0s 815us/step - loss: 17417.6641 - mse: 776449792.0000 - mae: 17417.6660 - val_loss: 20500.1091 - val_mse: 1063571200.0000 - val_mae: 20500.1074\n",
            "Epoch 130/500\n",
            "90/90 [==============================] - 0s 842us/step - loss: 17739.1011 - mse: 870213056.0000 - mae: 17739.1016 - val_loss: 20411.0183 - val_mse: 1056168896.0000 - val_mae: 20411.0195\n",
            "Epoch 131/500\n",
            "90/90 [==============================] - 0s 712us/step - loss: 16833.6316 - mse: 815469568.0000 - mae: 16833.6309 - val_loss: 20301.7985 - val_mse: 1047365568.0000 - val_mae: 20301.7988\n",
            "Epoch 132/500\n",
            "90/90 [==============================] - 0s 977us/step - loss: 17079.7369 - mse: 807797440.0000 - mae: 17079.7363 - val_loss: 20219.2292 - val_mse: 1040403392.0000 - val_mae: 20219.2285\n",
            "Epoch 133/500\n",
            "90/90 [==============================] - 0s 796us/step - loss: 17437.2528 - mse: 835647808.0000 - mae: 17437.2520 - val_loss: 20108.1739 - val_mse: 1031166080.0000 - val_mae: 20108.1738\n",
            "Epoch 134/500\n",
            "90/90 [==============================] - 0s 799us/step - loss: 16369.8763 - mse: 729275072.0000 - mae: 16369.8760 - val_loss: 20025.7141 - val_mse: 1022108224.0000 - val_mae: 20025.7148\n",
            "Epoch 135/500\n",
            "90/90 [==============================] - 0s 737us/step - loss: 16675.5187 - mse: 770486464.0000 - mae: 16675.5195 - val_loss: 20053.7197 - val_mse: 1012443648.0000 - val_mae: 20053.7207\n",
            "Epoch 136/500\n",
            "90/90 [==============================] - 0s 783us/step - loss: 17178.0147 - mse: 794955200.0000 - mae: 17178.0137 - val_loss: 19843.1673 - val_mse: 1006819904.0000 - val_mae: 19843.1660\n",
            "Epoch 137/500\n",
            "90/90 [==============================] - 0s 817us/step - loss: 16780.8332 - mse: 804906688.0000 - mae: 16780.8340 - val_loss: 19743.3382 - val_mse: 998221056.0000 - val_mae: 19743.3398\n",
            "Epoch 138/500\n",
            "90/90 [==============================] - 0s 836us/step - loss: 16160.5335 - mse: 698472832.0000 - mae: 16160.5332 - val_loss: 19621.4533 - val_mse: 990420800.0000 - val_mae: 19621.4531\n",
            "Epoch 139/500\n",
            "90/90 [==============================] - 0s 870us/step - loss: 16639.6406 - mse: 746265728.0000 - mae: 16639.6426 - val_loss: 19523.8792 - val_mse: 981573440.0000 - val_mae: 19523.8770\n",
            "Epoch 140/500\n",
            "90/90 [==============================] - 0s 931us/step - loss: 16784.6248 - mse: 773299328.0000 - mae: 16784.6250 - val_loss: 19431.7139 - val_mse: 971377152.0000 - val_mae: 19431.7148\n",
            "Epoch 141/500\n",
            "90/90 [==============================] - 0s 851us/step - loss: 16513.6067 - mse: 723136448.0000 - mae: 16513.6055 - val_loss: 19290.5491 - val_mse: 964255744.0000 - val_mae: 19290.5488\n",
            "Epoch 142/500\n",
            "90/90 [==============================] - 0s 827us/step - loss: 16206.3664 - mse: 698181248.0000 - mae: 16206.3652 - val_loss: 19242.8647 - val_mse: 955733120.0000 - val_mae: 19242.8652\n",
            "Epoch 143/500\n",
            "90/90 [==============================] - 0s 784us/step - loss: 16189.2202 - mse: 714169216.0000 - mae: 16189.2207 - val_loss: 19098.9636 - val_mse: 949059968.0000 - val_mae: 19098.9648\n",
            "Epoch 144/500\n",
            "90/90 [==============================] - 0s 745us/step - loss: 16419.5486 - mse: 745920192.0000 - mae: 16419.5488 - val_loss: 19032.2873 - val_mse: 941141376.0000 - val_mae: 19032.2852\n",
            "Epoch 145/500\n",
            "90/90 [==============================] - 0s 981us/step - loss: 16238.1130 - mse: 733080832.0000 - mae: 16238.1123 - val_loss: 18877.9792 - val_mse: 931734400.0000 - val_mae: 18877.9785\n",
            "Epoch 146/500\n",
            "90/90 [==============================] - 0s 732us/step - loss: 15934.1978 - mse: 701366208.0000 - mae: 15934.1973 - val_loss: 18788.4238 - val_mse: 922818624.0000 - val_mae: 18788.4258\n",
            "Epoch 147/500\n",
            "90/90 [==============================] - 0s 731us/step - loss: 15730.7250 - mse: 663305856.0000 - mae: 15730.7236 - val_loss: 18638.7844 - val_mse: 913683072.0000 - val_mae: 18638.7852\n",
            "Epoch 148/500\n",
            "90/90 [==============================] - 0s 785us/step - loss: 15228.3887 - mse: 665001344.0000 - mae: 15228.3887 - val_loss: 18610.7043 - val_mse: 907510400.0000 - val_mae: 18610.7051\n",
            "Epoch 149/500\n",
            "90/90 [==============================] - 0s 919us/step - loss: 15678.9975 - mse: 672651200.0000 - mae: 15678.9990 - val_loss: 18787.0229 - val_mse: 902167488.0000 - val_mae: 18787.0215\n",
            "Epoch 150/500\n",
            "90/90 [==============================] - 0s 760us/step - loss: 15386.0841 - mse: 687154176.0000 - mae: 15386.0850 - val_loss: 18469.7528 - val_mse: 893635456.0000 - val_mae: 18469.7520\n",
            "Epoch 151/500\n",
            "90/90 [==============================] - 0s 880us/step - loss: 15892.5774 - mse: 679330368.0000 - mae: 15892.5791 - val_loss: 18309.7337 - val_mse: 885679744.0000 - val_mae: 18309.7324\n",
            "Epoch 152/500\n",
            "90/90 [==============================] - 0s 787us/step - loss: 15729.0924 - mse: 677715392.0000 - mae: 15729.0918 - val_loss: 18274.7934 - val_mse: 878565632.0000 - val_mae: 18274.7930\n",
            "Epoch 153/500\n",
            "90/90 [==============================] - 0s 883us/step - loss: 15791.5222 - mse: 677222464.0000 - mae: 15791.5225 - val_loss: 18272.3779 - val_mse: 869982784.0000 - val_mae: 18272.3770\n",
            "Epoch 154/500\n",
            "90/90 [==============================] - 0s 834us/step - loss: 15667.8867 - mse: 689783168.0000 - mae: 15667.8857 - val_loss: 18077.4051 - val_mse: 860984128.0000 - val_mae: 18077.4062\n",
            "Epoch 155/500\n",
            "90/90 [==============================] - 0s 828us/step - loss: 15477.2865 - mse: 651443968.0000 - mae: 15477.2861 - val_loss: 18045.3342 - val_mse: 850805376.0000 - val_mae: 18045.3340\n",
            "Epoch 156/500\n",
            "90/90 [==============================] - 0s 794us/step - loss: 14923.0018 - mse: 626992896.0000 - mae: 14923.0029 - val_loss: 18007.6304 - val_mse: 842627840.0000 - val_mae: 18007.6309\n",
            "Epoch 157/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 14467.0788 - mse: 611108928.0000 - mae: 14467.0781 - val_loss: 17822.1342 - val_mse: 833066496.0000 - val_mae: 17822.1348\n",
            "Epoch 158/500\n",
            "90/90 [==============================] - 0s 934us/step - loss: 14867.0513 - mse: 625696064.0000 - mae: 14867.0518 - val_loss: 17889.6532 - val_mse: 826494144.0000 - val_mae: 17889.6523\n",
            "Epoch 159/500\n",
            "90/90 [==============================] - 0s 977us/step - loss: 15282.9169 - mse: 634551872.0000 - mae: 15282.9170 - val_loss: 17849.9290 - val_mse: 819036352.0000 - val_mae: 17849.9297\n",
            "Epoch 160/500\n",
            "90/90 [==============================] - 0s 794us/step - loss: 15247.6430 - mse: 655014336.0000 - mae: 15247.6426 - val_loss: 17719.0175 - val_mse: 810430080.0000 - val_mae: 17719.0176\n",
            "Epoch 161/500\n",
            "90/90 [==============================] - 0s 753us/step - loss: 14531.7979 - mse: 619319296.0000 - mae: 14531.7988 - val_loss: 17758.5822 - val_mse: 803590592.0000 - val_mae: 17758.5840\n",
            "Epoch 162/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 14812.2345 - mse: 644339904.0000 - mae: 14812.2334 - val_loss: 17646.9160 - val_mse: 796208512.0000 - val_mae: 17646.9160\n",
            "Epoch 163/500\n",
            "90/90 [==============================] - 0s 816us/step - loss: 14549.3610 - mse: 593709632.0000 - mae: 14549.3594 - val_loss: 17567.8791 - val_mse: 788932544.0000 - val_mae: 17567.8789\n",
            "Epoch 164/500\n",
            "90/90 [==============================] - 0s 766us/step - loss: 13618.1843 - mse: 524358144.0000 - mae: 13618.1846 - val_loss: 17677.9230 - val_mse: 782276544.0000 - val_mae: 17677.9219\n",
            "Epoch 165/500\n",
            "90/90 [==============================] - 0s 783us/step - loss: 14295.7305 - mse: 579787776.0000 - mae: 14295.7305 - val_loss: 17770.0366 - val_mse: 776719552.0000 - val_mae: 17770.0352\n",
            "Epoch 166/500\n",
            "90/90 [==============================] - 0s 843us/step - loss: 14041.0648 - mse: 547300352.0000 - mae: 14041.0664 - val_loss: 17315.4843 - val_mse: 764511488.0000 - val_mae: 17315.4844\n",
            "Epoch 167/500\n",
            "90/90 [==============================] - 0s 899us/step - loss: 14481.5792 - mse: 564934016.0000 - mae: 14481.5801 - val_loss: 17447.5564 - val_mse: 759469888.0000 - val_mae: 17447.5566\n",
            "Epoch 168/500\n",
            "90/90 [==============================] - 0s 768us/step - loss: 13808.5777 - mse: 535328256.0000 - mae: 13808.5781 - val_loss: 17289.5551 - val_mse: 751703680.0000 - val_mae: 17289.5547\n",
            "Epoch 169/500\n",
            "90/90 [==============================] - 0s 794us/step - loss: 13773.1387 - mse: 517151936.0000 - mae: 13773.1387 - val_loss: 17356.3472 - val_mse: 746436160.0000 - val_mae: 17356.3477\n",
            "Epoch 170/500\n",
            "90/90 [==============================] - 0s 821us/step - loss: 14653.2197 - mse: 586260544.0000 - mae: 14653.2197 - val_loss: 17186.3647 - val_mse: 738364800.0000 - val_mae: 17186.3633\n",
            "Epoch 171/500\n",
            "90/90 [==============================] - 0s 773us/step - loss: 13736.8379 - mse: 560441280.0000 - mae: 13736.8379 - val_loss: 16861.7552 - val_mse: 727544448.0000 - val_mae: 16861.7559\n",
            "Epoch 172/500\n",
            "90/90 [==============================] - 0s 831us/step - loss: 14467.9696 - mse: 573685184.0000 - mae: 14467.9697 - val_loss: 16696.9482 - val_mse: 718818752.0000 - val_mae: 16696.9473\n",
            "Epoch 173/500\n",
            "90/90 [==============================] - 0s 846us/step - loss: 13442.2065 - mse: 498605632.0000 - mae: 13442.2051 - val_loss: 17173.2917 - val_mse: 715889728.0000 - val_mae: 17173.2910\n",
            "Epoch 174/500\n",
            "90/90 [==============================] - 0s 851us/step - loss: 14973.8480 - mse: 601775168.0000 - mae: 14973.8486 - val_loss: 16472.0495 - val_mse: 702109568.0000 - val_mae: 16472.0508\n",
            "Epoch 175/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 14386.4347 - mse: 572086400.0000 - mae: 14386.4346 - val_loss: 16383.3369 - val_mse: 693205760.0000 - val_mae: 16383.3359\n",
            "Epoch 176/500\n",
            "90/90 [==============================] - 0s 945us/step - loss: 13254.5020 - mse: 529967872.0000 - mae: 13254.5010 - val_loss: 16206.5230 - val_mse: 685635776.0000 - val_mae: 16206.5225\n",
            "Epoch 177/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 13651.4308 - mse: 532782272.0000 - mae: 13651.4307 - val_loss: 16300.7752 - val_mse: 680083200.0000 - val_mae: 16300.7754\n",
            "Epoch 178/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 13373.8134 - mse: 515349216.0000 - mae: 13373.8135 - val_loss: 16443.0305 - val_mse: 674741760.0000 - val_mae: 16443.0312\n",
            "Epoch 179/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 12928.1474 - mse: 497080992.0000 - mae: 12928.1475 - val_loss: 16898.7611 - val_mse: 673745920.0000 - val_mae: 16898.7617\n",
            "Epoch 180/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 14669.0007 - mse: 569935296.0000 - mae: 14669.0000 - val_loss: 16451.7773 - val_mse: 662410688.0000 - val_mae: 16451.7773\n",
            "Epoch 181/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 13321.2645 - mse: 522474336.0000 - mae: 13321.2637 - val_loss: 16617.8268 - val_mse: 658425728.0000 - val_mae: 16617.8281\n",
            "Epoch 182/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 13089.8320 - mse: 489846240.0000 - mae: 13089.8320 - val_loss: 16207.8921 - val_mse: 647833920.0000 - val_mae: 16207.8916\n",
            "Epoch 183/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 12373.0565 - mse: 414885600.0000 - mae: 12373.0566 - val_loss: 16140.2488 - val_mse: 642080256.0000 - val_mae: 16140.2490\n",
            "Epoch 184/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 12431.1743 - mse: 469962560.0000 - mae: 12431.1748 - val_loss: 16331.7457 - val_mse: 636257152.0000 - val_mae: 16331.7461\n",
            "Epoch 185/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 13704.3462 - mse: 539752512.0000 - mae: 13704.3457 - val_loss: 16618.9257 - val_mse: 636179008.0000 - val_mae: 16618.9258\n",
            "Epoch 186/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 13229.9491 - mse: 516400320.0000 - mae: 13229.9502 - val_loss: 16152.6476 - val_mse: 622981184.0000 - val_mae: 16152.6475\n",
            "Epoch 187/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 13044.5403 - mse: 495976160.0000 - mae: 13044.5400 - val_loss: 15673.2763 - val_mse: 611250880.0000 - val_mae: 15673.2754\n",
            "Epoch 188/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 13501.8204 - mse: 523160512.0000 - mae: 13501.8193 - val_loss: 15936.3347 - val_mse: 607662336.0000 - val_mae: 15936.3350\n",
            "Epoch 189/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 13244.3641 - mse: 488227808.0000 - mae: 13244.3643 - val_loss: 15534.9443 - val_mse: 596383168.0000 - val_mae: 15534.9443\n",
            "Epoch 190/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 13427.3343 - mse: 518017664.0000 - mae: 13427.3350 - val_loss: 16051.3070 - val_mse: 599237184.0000 - val_mae: 16051.3066\n",
            "Epoch 191/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 12777.8224 - mse: 468050112.0000 - mae: 12777.8223 - val_loss: 15728.1532 - val_mse: 589056128.0000 - val_mae: 15728.1523\n",
            "Epoch 192/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 11122.3698 - mse: 378860128.0000 - mae: 11122.3691 - val_loss: 15412.1925 - val_mse: 575933376.0000 - val_mae: 15412.1934\n",
            "Epoch 193/500\n",
            "90/90 [==============================] - 0s 890us/step - loss: 12881.5189 - mse: 457546304.0000 - mae: 12881.5166 - val_loss: 15801.6684 - val_mse: 576980288.0000 - val_mae: 15801.6680\n",
            "Epoch 194/500\n",
            "90/90 [==============================] - 0s 821us/step - loss: 11728.3949 - mse: 398224640.0000 - mae: 11728.3945 - val_loss: 15084.5798 - val_mse: 560008832.0000 - val_mae: 15084.5801\n",
            "Epoch 195/500\n",
            "90/90 [==============================] - 0s 927us/step - loss: 12098.4566 - mse: 406832416.0000 - mae: 12098.4570 - val_loss: 15219.6616 - val_mse: 557379328.0000 - val_mae: 15219.6611\n",
            "Epoch 196/500\n",
            "90/90 [==============================] - 0s 996us/step - loss: 12215.5390 - mse: 463959872.0000 - mae: 12215.5400 - val_loss: 15076.1583 - val_mse: 549050240.0000 - val_mae: 15076.1582\n",
            "Epoch 197/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 11248.5887 - mse: 390555360.0000 - mae: 11248.5889 - val_loss: 15293.4183 - val_mse: 547536192.0000 - val_mae: 15293.4180\n",
            "Epoch 198/500\n",
            "90/90 [==============================] - 0s 867us/step - loss: 11727.1985 - mse: 396640704.0000 - mae: 11727.1982 - val_loss: 14970.7351 - val_mse: 537173760.0000 - val_mae: 14970.7344\n",
            "Epoch 199/500\n",
            "90/90 [==============================] - 0s 885us/step - loss: 12159.6447 - mse: 417098144.0000 - mae: 12159.6445 - val_loss: 14728.0478 - val_mse: 527194240.0000 - val_mae: 14728.0469\n",
            "Epoch 200/500\n",
            "90/90 [==============================] - 0s 819us/step - loss: 10950.8601 - mse: 395904384.0000 - mae: 10950.8604 - val_loss: 14307.2034 - val_mse: 512056064.0000 - val_mae: 14307.2031\n",
            "Epoch 201/500\n",
            "90/90 [==============================] - 0s 807us/step - loss: 11530.0737 - mse: 421791008.0000 - mae: 11530.0752 - val_loss: 14514.3896 - val_mse: 508444320.0000 - val_mae: 14514.3887\n",
            "Epoch 202/500\n",
            "90/90 [==============================] - 0s 871us/step - loss: 10688.3131 - mse: 344946144.0000 - mae: 10688.3135 - val_loss: 14779.0790 - val_mse: 507330272.0000 - val_mae: 14779.0791\n",
            "Epoch 203/500\n",
            "90/90 [==============================] - 0s 897us/step - loss: 11746.8700 - mse: 390183488.0000 - mae: 11746.8711 - val_loss: 14064.3324 - val_mse: 489530176.0000 - val_mae: 14064.3330\n",
            "Epoch 204/500\n",
            "90/90 [==============================] - 0s 993us/step - loss: 9675.0946 - mse: 307846592.0000 - mae: 9675.0947 - val_loss: 14235.6030 - val_mse: 488378144.0000 - val_mae: 14235.6025\n",
            "Epoch 205/500\n",
            "90/90 [==============================] - 0s 989us/step - loss: 11003.4770 - mse: 350637088.0000 - mae: 11003.4775 - val_loss: 14617.6947 - val_mse: 489543360.0000 - val_mae: 14617.6943\n",
            "Epoch 206/500\n",
            "90/90 [==============================] - 0s 928us/step - loss: 10714.7766 - mse: 350288672.0000 - mae: 10714.7764 - val_loss: 14221.4426 - val_mse: 480251680.0000 - val_mae: 14221.4414\n",
            "Epoch 207/500\n",
            "90/90 [==============================] - 0s 896us/step - loss: 12176.9840 - mse: 445940576.0000 - mae: 12176.9834 - val_loss: 14288.1956 - val_mse: 476234688.0000 - val_mae: 14288.1963\n",
            "Epoch 208/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 10752.8626 - mse: 359215296.0000 - mae: 10752.8623 - val_loss: 14113.0913 - val_mse: 469698464.0000 - val_mae: 14113.0918\n",
            "Epoch 209/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 11074.4042 - mse: 356523776.0000 - mae: 11074.4043 - val_loss: 13986.6738 - val_mse: 461324736.0000 - val_mae: 13986.6738\n",
            "Epoch 210/500\n",
            "90/90 [==============================] - 0s 979us/step - loss: 10468.6448 - mse: 360543360.0000 - mae: 10468.6455 - val_loss: 14054.0020 - val_mse: 458354880.0000 - val_mae: 14054.0029\n",
            "Epoch 211/500\n",
            "90/90 [==============================] - 0s 851us/step - loss: 10594.5808 - mse: 339366720.0000 - mae: 10594.5801 - val_loss: 13444.4789 - val_mse: 442899936.0000 - val_mae: 13444.4795\n",
            "Epoch 212/500\n",
            "90/90 [==============================] - 0s 857us/step - loss: 9904.6426 - mse: 307902752.0000 - mae: 9904.6426 - val_loss: 13164.2876 - val_mse: 431884384.0000 - val_mae: 13164.2871\n",
            "Epoch 213/500\n",
            "90/90 [==============================] - 0s 795us/step - loss: 10510.0212 - mse: 346757024.0000 - mae: 10510.0205 - val_loss: 13301.3943 - val_mse: 430927360.0000 - val_mae: 13301.3945\n",
            "Epoch 214/500\n",
            "90/90 [==============================] - 0s 854us/step - loss: 9397.1974 - mse: 284167904.0000 - mae: 9397.1973 - val_loss: 13338.1911 - val_mse: 425639424.0000 - val_mae: 13338.1914\n",
            "Epoch 215/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 9938.8462 - mse: 345458720.0000 - mae: 9938.8457 - val_loss: 12899.1402 - val_mse: 414884480.0000 - val_mae: 12899.1406\n",
            "Epoch 216/500\n",
            "90/90 [==============================] - 0s 864us/step - loss: 12009.5527 - mse: 438892896.0000 - mae: 12009.5527 - val_loss: 12693.3583 - val_mse: 407170784.0000 - val_mae: 12693.3584\n",
            "Epoch 217/500\n",
            "90/90 [==============================] - 0s 802us/step - loss: 10312.0031 - mse: 335083808.0000 - mae: 10312.0039 - val_loss: 13201.2447 - val_mse: 409069536.0000 - val_mae: 13201.2441\n",
            "Epoch 218/500\n",
            "90/90 [==============================] - 0s 965us/step - loss: 9751.9909 - mse: 305376224.0000 - mae: 9751.9912 - val_loss: 12786.3708 - val_mse: 400770176.0000 - val_mae: 12786.3711\n",
            "Epoch 219/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 10522.0275 - mse: 355509792.0000 - mae: 10522.0273 - val_loss: 13026.9843 - val_mse: 401073088.0000 - val_mae: 13026.9844\n",
            "Epoch 220/500\n",
            "90/90 [==============================] - 0s 840us/step - loss: 9828.0106 - mse: 294595040.0000 - mae: 9828.0107 - val_loss: 12581.2300 - val_mse: 389510592.0000 - val_mae: 12581.2305\n",
            "Epoch 221/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 9639.2542 - mse: 306682944.0000 - mae: 9639.2539 - val_loss: 13037.8036 - val_mse: 391866592.0000 - val_mae: 13037.8037\n",
            "Epoch 222/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 11865.5512 - mse: 379748160.0000 - mae: 11865.5518 - val_loss: 12450.5725 - val_mse: 379507584.0000 - val_mae: 12450.5723\n",
            "Epoch 223/500\n",
            "90/90 [==============================] - 0s 849us/step - loss: 10971.9964 - mse: 377637632.0000 - mae: 10971.9961 - val_loss: 12863.7126 - val_mse: 381606624.0000 - val_mae: 12863.7129\n",
            "Epoch 224/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 10357.5828 - mse: 332435584.0000 - mae: 10357.5830 - val_loss: 12424.8156 - val_mse: 371292832.0000 - val_mae: 12424.8164\n",
            "Epoch 225/500\n",
            "90/90 [==============================] - 0s 859us/step - loss: 9913.1451 - mse: 239992496.0000 - mae: 9913.1455 - val_loss: 11975.0451 - val_mse: 357693568.0000 - val_mae: 11975.0449\n",
            "Epoch 226/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 9253.7020 - mse: 253673760.0000 - mae: 9253.7021 - val_loss: 12255.9788 - val_mse: 357365728.0000 - val_mae: 12255.9795\n",
            "Epoch 227/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 9357.4651 - mse: 251358592.0000 - mae: 9357.4648 - val_loss: 12470.4507 - val_mse: 359320928.0000 - val_mae: 12470.4502\n",
            "Epoch 228/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 9851.6739 - mse: 333337888.0000 - mae: 9851.6738 - val_loss: 12362.3357 - val_mse: 354412192.0000 - val_mae: 12362.3359\n",
            "Epoch 229/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 10261.2592 - mse: 338896384.0000 - mae: 10261.2598 - val_loss: 11962.2413 - val_mse: 344105088.0000 - val_mae: 11962.2412\n",
            "Epoch 230/500\n",
            "90/90 [==============================] - 0s 975us/step - loss: 9719.9332 - mse: 300629184.0000 - mae: 9719.9336 - val_loss: 11611.5713 - val_mse: 334081856.0000 - val_mae: 11611.5713\n",
            "Epoch 231/500\n",
            "90/90 [==============================] - 0s 873us/step - loss: 8723.6074 - mse: 272439264.0000 - mae: 8723.6074 - val_loss: 11628.9002 - val_mse: 329207616.0000 - val_mae: 11628.9004\n",
            "Epoch 232/500\n",
            "90/90 [==============================] - 0s 893us/step - loss: 8140.3931 - mse: 217303040.0000 - mae: 8140.3926 - val_loss: 11293.2592 - val_mse: 320253568.0000 - val_mae: 11293.2588\n",
            "Epoch 233/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8468.4149 - mse: 211089456.0000 - mae: 8468.4150 - val_loss: 12141.3395 - val_mse: 332561664.0000 - val_mae: 12141.3389\n",
            "Epoch 234/500\n",
            "90/90 [==============================] - 0s 806us/step - loss: 7586.8037 - mse: 180282304.0000 - mae: 7586.8037 - val_loss: 11370.7548 - val_mse: 317185472.0000 - val_mae: 11370.7549\n",
            "Epoch 235/500\n",
            "90/90 [==============================] - 0s 895us/step - loss: 9085.9808 - mse: 267919952.0000 - mae: 9085.9805 - val_loss: 11593.3487 - val_mse: 316439360.0000 - val_mae: 11593.3486\n",
            "Epoch 236/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 9197.0626 - mse: 264600528.0000 - mae: 9197.0625 - val_loss: 11812.4980 - val_mse: 317481696.0000 - val_mae: 11812.4971\n",
            "Epoch 237/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7995.3553 - mse: 188919488.0000 - mae: 7995.3555 - val_loss: 11136.7226 - val_mse: 306184416.0000 - val_mae: 11136.7227\n",
            "Epoch 238/500\n",
            "90/90 [==============================] - 0s 771us/step - loss: 8777.7676 - mse: 245020832.0000 - mae: 8777.7666 - val_loss: 11003.7212 - val_mse: 301222560.0000 - val_mae: 11003.7207\n",
            "Epoch 239/500\n",
            "90/90 [==============================] - 0s 879us/step - loss: 9840.1468 - mse: 343332352.0000 - mae: 9840.1475 - val_loss: 10940.6831 - val_mse: 298128896.0000 - val_mae: 10940.6836\n",
            "Epoch 240/500\n",
            "90/90 [==============================] - 0s 795us/step - loss: 7390.8297 - mse: 155734080.0000 - mae: 7390.8301 - val_loss: 11822.2301 - val_mse: 311448160.0000 - val_mae: 11822.2305\n",
            "Epoch 241/500\n",
            "90/90 [==============================] - 0s 845us/step - loss: 8830.4492 - mse: 266978352.0000 - mae: 8830.4492 - val_loss: 11462.0922 - val_mse: 305624128.0000 - val_mae: 11462.0918\n",
            "Epoch 242/500\n",
            "90/90 [==============================] - 0s 797us/step - loss: 7839.0832 - mse: 176813312.0000 - mae: 7839.0835 - val_loss: 11022.9045 - val_mse: 295872320.0000 - val_mae: 11022.9043\n",
            "Epoch 243/500\n",
            "90/90 [==============================] - 0s 852us/step - loss: 8326.1706 - mse: 222633760.0000 - mae: 8326.1709 - val_loss: 11123.8959 - val_mse: 296370336.0000 - val_mae: 11123.8955\n",
            "Epoch 244/500\n",
            "90/90 [==============================] - 0s 882us/step - loss: 7867.9722 - mse: 187196944.0000 - mae: 7867.9731 - val_loss: 10959.7945 - val_mse: 291173920.0000 - val_mae: 10959.7949\n",
            "Epoch 245/500\n",
            "90/90 [==============================] - 0s 778us/step - loss: 7635.3259 - mse: 178352112.0000 - mae: 7635.3257 - val_loss: 10673.5423 - val_mse: 282699200.0000 - val_mae: 10673.5420\n",
            "Epoch 246/500\n",
            "90/90 [==============================] - 0s 824us/step - loss: 8084.8542 - mse: 232520912.0000 - mae: 8084.8540 - val_loss: 10410.3462 - val_mse: 270649024.0000 - val_mae: 10410.3457\n",
            "Epoch 247/500\n",
            "90/90 [==============================] - 0s 831us/step - loss: 9471.8924 - mse: 301371264.0000 - mae: 9471.8916 - val_loss: 11146.5720 - val_mse: 284620416.0000 - val_mae: 11146.5713\n",
            "Epoch 248/500\n",
            "90/90 [==============================] - 0s 947us/step - loss: 8314.6619 - mse: 191839440.0000 - mae: 8314.6621 - val_loss: 10521.3731 - val_mse: 271642496.0000 - val_mae: 10521.3730\n",
            "Epoch 249/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8625.1855 - mse: 246192016.0000 - mae: 8625.1846 - val_loss: 10342.6943 - val_mse: 267397856.0000 - val_mae: 10342.6943\n",
            "Epoch 250/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7817.5738 - mse: 217411680.0000 - mae: 7817.5737 - val_loss: 10537.6386 - val_mse: 269955936.0000 - val_mae: 10537.6387\n",
            "Epoch 251/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7353.8456 - mse: 171878912.0000 - mae: 7353.8452 - val_loss: 10721.7135 - val_mse: 272072288.0000 - val_mae: 10721.7129\n",
            "Epoch 252/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8747.1810 - mse: 230210816.0000 - mae: 8747.1816 - val_loss: 10939.3021 - val_mse: 273719712.0000 - val_mae: 10939.3018\n",
            "Epoch 253/500\n",
            "90/90 [==============================] - 0s 918us/step - loss: 7363.3314 - mse: 153261584.0000 - mae: 7363.3311 - val_loss: 10400.4728 - val_mse: 263338960.0000 - val_mae: 10400.4727\n",
            "Epoch 254/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8707.7984 - mse: 226406816.0000 - mae: 8707.7988 - val_loss: 10278.2148 - val_mse: 259256688.0000 - val_mae: 10278.2139\n",
            "Epoch 255/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7319.2602 - mse: 165027072.0000 - mae: 7319.2598 - val_loss: 10401.0686 - val_mse: 258914160.0000 - val_mae: 10401.0684\n",
            "Epoch 256/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7977.9273 - mse: 172645216.0000 - mae: 7977.9272 - val_loss: 9852.1644 - val_mse: 246065312.0000 - val_mae: 9852.1650\n",
            "Epoch 257/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8811.2094 - mse: 240317968.0000 - mae: 8811.2100 - val_loss: 9871.9414 - val_mse: 247026752.0000 - val_mae: 9871.9414\n",
            "Epoch 258/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7531.7798 - mse: 165388144.0000 - mae: 7531.7798 - val_loss: 9775.9795 - val_mse: 243992624.0000 - val_mae: 9775.9795\n",
            "Epoch 259/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8041.9443 - mse: 193337952.0000 - mae: 8041.9453 - val_loss: 9894.8157 - val_mse: 245574912.0000 - val_mae: 9894.8154\n",
            "Epoch 260/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8882.0978 - mse: 207869264.0000 - mae: 8882.0986 - val_loss: 9881.0152 - val_mse: 244360208.0000 - val_mae: 9881.0156\n",
            "Epoch 261/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7783.8656 - mse: 187263392.0000 - mae: 7783.8652 - val_loss: 9667.8490 - val_mse: 237738528.0000 - val_mae: 9667.8486\n",
            "Epoch 262/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8837.1232 - mse: 221054480.0000 - mae: 8837.1230 - val_loss: 9746.6032 - val_mse: 238063120.0000 - val_mae: 9746.6035\n",
            "Epoch 263/500\n",
            "90/90 [==============================] - 0s 886us/step - loss: 7909.8194 - mse: 224865888.0000 - mae: 7909.8193 - val_loss: 9558.3461 - val_mse: 233986496.0000 - val_mae: 9558.3457\n",
            "Epoch 264/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7501.3264 - mse: 169940432.0000 - mae: 7501.3262 - val_loss: 9744.3855 - val_mse: 234054752.0000 - val_mae: 9744.3857\n",
            "Epoch 265/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8092.6845 - mse: 202553616.0000 - mae: 8092.6846 - val_loss: 9439.8837 - val_mse: 230164960.0000 - val_mae: 9439.8838\n",
            "Epoch 266/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 6897.4896 - mse: 150991664.0000 - mae: 6897.4902 - val_loss: 9488.7477 - val_mse: 229288432.0000 - val_mae: 9488.7471\n",
            "Epoch 267/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7834.1058 - mse: 201564544.0000 - mae: 7834.1064 - val_loss: 9178.8695 - val_mse: 222673584.0000 - val_mae: 9178.8691\n",
            "Epoch 268/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8427.8840 - mse: 203494736.0000 - mae: 8427.8838 - val_loss: 9925.5881 - val_mse: 231659264.0000 - val_mae: 9925.5889\n",
            "Epoch 269/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7410.1911 - mse: 191139936.0000 - mae: 7410.1909 - val_loss: 9675.5704 - val_mse: 229597872.0000 - val_mae: 9675.5703\n",
            "Epoch 270/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8160.9903 - mse: 189930112.0000 - mae: 8160.9902 - val_loss: 9513.1445 - val_mse: 229532016.0000 - val_mae: 9513.1445\n",
            "Epoch 271/500\n",
            "90/90 [==============================] - 0s 995us/step - loss: 7490.2401 - mse: 160160912.0000 - mae: 7490.2402 - val_loss: 9291.7948 - val_mse: 223051168.0000 - val_mae: 9291.7949\n",
            "Epoch 272/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7286.2654 - mse: 167902992.0000 - mae: 7286.2651 - val_loss: 9570.7367 - val_mse: 225478064.0000 - val_mae: 9570.7363\n",
            "Epoch 273/500\n",
            "90/90 [==============================] - 0s 994us/step - loss: 8407.5544 - mse: 210392000.0000 - mae: 8407.5537 - val_loss: 9302.0361 - val_mse: 222472032.0000 - val_mae: 9302.0361\n",
            "Epoch 274/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 9234.5576 - mse: 281967808.0000 - mae: 9234.5576 - val_loss: 10170.9326 - val_mse: 236619200.0000 - val_mae: 10170.9316\n",
            "Epoch 275/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8038.4096 - mse: 208980656.0000 - mae: 8038.4092 - val_loss: 9268.9379 - val_mse: 221048624.0000 - val_mae: 9268.9375\n",
            "Epoch 276/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8123.1352 - mse: 202951088.0000 - mae: 8123.1353 - val_loss: 9879.1566 - val_mse: 230220464.0000 - val_mae: 9879.1572\n",
            "Epoch 277/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8694.0983 - mse: 224865552.0000 - mae: 8694.0977 - val_loss: 9246.6827 - val_mse: 220034624.0000 - val_mae: 9246.6826\n",
            "Epoch 278/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7657.7528 - mse: 163327312.0000 - mae: 7657.7520 - val_loss: 9146.7198 - val_mse: 216881520.0000 - val_mae: 9146.7197\n",
            "Epoch 279/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7898.8369 - mse: 164853632.0000 - mae: 7898.8369 - val_loss: 9249.8156 - val_mse: 219135856.0000 - val_mae: 9249.8164\n",
            "Epoch 280/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7986.6440 - mse: 209169152.0000 - mae: 7986.6445 - val_loss: 9036.0825 - val_mse: 213135504.0000 - val_mae: 9036.0830\n",
            "Epoch 281/500\n",
            "90/90 [==============================] - 0s 856us/step - loss: 7301.8707 - mse: 157506256.0000 - mae: 7301.8706 - val_loss: 9445.0736 - val_mse: 220020464.0000 - val_mae: 9445.0732\n",
            "Epoch 282/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7824.5200 - mse: 212989888.0000 - mae: 7824.5200 - val_loss: 9208.8870 - val_mse: 216277008.0000 - val_mae: 9208.8877\n",
            "Epoch 283/500\n",
            "90/90 [==============================] - 0s 808us/step - loss: 7994.0795 - mse: 201834000.0000 - mae: 7994.0791 - val_loss: 9161.2399 - val_mse: 212993984.0000 - val_mae: 9161.2393\n",
            "Epoch 284/500\n",
            "90/90 [==============================] - 0s 804us/step - loss: 6494.3950 - mse: 136803248.0000 - mae: 6494.3950 - val_loss: 9040.9017 - val_mse: 212076704.0000 - val_mae: 9040.9014\n",
            "Epoch 285/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8358.0987 - mse: 203700912.0000 - mae: 8358.0977 - val_loss: 9060.1203 - val_mse: 212545808.0000 - val_mae: 9060.1211\n",
            "Epoch 286/500\n",
            "90/90 [==============================] - 0s 913us/step - loss: 7017.1213 - mse: 145020960.0000 - mae: 7017.1216 - val_loss: 9532.8691 - val_mse: 221960560.0000 - val_mae: 9532.8691\n",
            "Epoch 287/500\n",
            "90/90 [==============================] - 0s 822us/step - loss: 7407.0854 - mse: 179897456.0000 - mae: 7407.0850 - val_loss: 9158.6574 - val_mse: 214761200.0000 - val_mae: 9158.6572\n",
            "Epoch 288/500\n",
            "90/90 [==============================] - 0s 922us/step - loss: 8334.4148 - mse: 189159856.0000 - mae: 8334.4150 - val_loss: 9065.2210 - val_mse: 212571456.0000 - val_mae: 9065.2207\n",
            "Epoch 289/500\n",
            "90/90 [==============================] - 0s 807us/step - loss: 7727.8935 - mse: 193841520.0000 - mae: 7727.8931 - val_loss: 9252.1924 - val_mse: 214807024.0000 - val_mae: 9252.1924\n",
            "Epoch 290/500\n",
            "90/90 [==============================] - 0s 837us/step - loss: 7512.7872 - mse: 176362832.0000 - mae: 7512.7876 - val_loss: 9018.8317 - val_mse: 211371536.0000 - val_mae: 9018.8320\n",
            "Epoch 291/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7580.8808 - mse: 161434576.0000 - mae: 7580.8804 - val_loss: 8920.9856 - val_mse: 206988864.0000 - val_mae: 8920.9854\n",
            "Epoch 292/500\n",
            "90/90 [==============================] - 0s 975us/step - loss: 7384.4769 - mse: 156074496.0000 - mae: 7384.4771 - val_loss: 9385.8176 - val_mse: 215005408.0000 - val_mae: 9385.8174\n",
            "Epoch 293/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7131.0642 - mse: 153744464.0000 - mae: 7131.0645 - val_loss: 8986.0223 - val_mse: 208681728.0000 - val_mae: 8986.0225\n",
            "Epoch 294/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7886.6574 - mse: 192147568.0000 - mae: 7886.6577 - val_loss: 9324.6857 - val_mse: 214097120.0000 - val_mae: 9324.6865\n",
            "Epoch 295/500\n",
            "90/90 [==============================] - 0s 853us/step - loss: 7183.9561 - mse: 180062992.0000 - mae: 7183.9556 - val_loss: 9491.1929 - val_mse: 217050320.0000 - val_mae: 9491.1934\n",
            "Epoch 296/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7521.4899 - mse: 155072944.0000 - mae: 7521.4897 - val_loss: 9717.4199 - val_mse: 217892384.0000 - val_mae: 9717.4199\n",
            "Epoch 297/500\n",
            "90/90 [==============================] - 0s 958us/step - loss: 6249.9768 - mse: 118167424.0000 - mae: 6249.9771 - val_loss: 9050.5758 - val_mse: 209208208.0000 - val_mae: 9050.5762\n",
            "Epoch 298/500\n",
            "90/90 [==============================] - 0s 834us/step - loss: 8869.5439 - mse: 235741024.0000 - mae: 8869.5449 - val_loss: 8998.3347 - val_mse: 207600528.0000 - val_mae: 8998.3350\n",
            "Epoch 299/500\n",
            "90/90 [==============================] - 0s 837us/step - loss: 7649.6051 - mse: 182680800.0000 - mae: 7649.6055 - val_loss: 9317.5124 - val_mse: 210192432.0000 - val_mae: 9317.5127\n",
            "Epoch 300/500\n",
            "90/90 [==============================] - 0s 829us/step - loss: 7556.1560 - mse: 186380064.0000 - mae: 7556.1562 - val_loss: 8917.6689 - val_mse: 204224096.0000 - val_mae: 8917.6699\n",
            "Epoch 301/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7440.8040 - mse: 164605024.0000 - mae: 7440.8042 - val_loss: 9144.5954 - val_mse: 208769520.0000 - val_mae: 9144.5957\n",
            "Epoch 302/500\n",
            "90/90 [==============================] - 0s 972us/step - loss: 7751.0351 - mse: 160042208.0000 - mae: 7751.0361 - val_loss: 9010.2656 - val_mse: 204940112.0000 - val_mae: 9010.2656\n",
            "Epoch 303/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 9628.9486 - mse: 274184736.0000 - mae: 9628.9482 - val_loss: 9070.0180 - val_mse: 204368160.0000 - val_mae: 9070.0176\n",
            "Epoch 304/500\n",
            "90/90 [==============================] - 0s 923us/step - loss: 7717.7104 - mse: 167239376.0000 - mae: 7717.7109 - val_loss: 8584.9927 - val_mse: 194495216.0000 - val_mae: 8584.9932\n",
            "Epoch 305/500\n",
            "90/90 [==============================] - 0s 987us/step - loss: 7841.0614 - mse: 170139072.0000 - mae: 7841.0605 - val_loss: 8761.4079 - val_mse: 196317344.0000 - val_mae: 8761.4082\n",
            "Epoch 306/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8054.3436 - mse: 171574416.0000 - mae: 8054.3438 - val_loss: 8763.5245 - val_mse: 194856528.0000 - val_mae: 8763.5244\n",
            "Epoch 307/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 6706.3492 - mse: 146277312.0000 - mae: 6706.3491 - val_loss: 8390.9605 - val_mse: 187561440.0000 - val_mae: 8390.9600\n",
            "Epoch 308/500\n",
            "90/90 [==============================] - 0s 912us/step - loss: 7198.3519 - mse: 151694768.0000 - mae: 7198.3521 - val_loss: 8329.3093 - val_mse: 185394320.0000 - val_mae: 8329.3096\n",
            "Epoch 309/500\n",
            "90/90 [==============================] - 0s 819us/step - loss: 7144.7174 - mse: 169208688.0000 - mae: 7144.7173 - val_loss: 8262.1165 - val_mse: 182184032.0000 - val_mae: 8262.1162\n",
            "Epoch 310/500\n",
            "90/90 [==============================] - 0s 903us/step - loss: 7743.9975 - mse: 184967136.0000 - mae: 7743.9980 - val_loss: 8632.0137 - val_mse: 189874928.0000 - val_mae: 8632.0137\n",
            "Epoch 311/500\n",
            "90/90 [==============================] - 0s 980us/step - loss: 8486.6640 - mse: 201816832.0000 - mae: 8486.6641 - val_loss: 8358.8765 - val_mse: 185588480.0000 - val_mae: 8358.8760\n",
            "Epoch 312/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7507.3461 - mse: 171445536.0000 - mae: 7507.3457 - val_loss: 8395.0898 - val_mse: 185196320.0000 - val_mae: 8395.0898\n",
            "Epoch 313/500\n",
            "90/90 [==============================] - 0s 925us/step - loss: 6871.8789 - mse: 127416176.0000 - mae: 6871.8794 - val_loss: 8437.8914 - val_mse: 185192672.0000 - val_mae: 8437.8916\n",
            "Epoch 314/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7303.4819 - mse: 165069744.0000 - mae: 7303.4819 - val_loss: 8353.0163 - val_mse: 184447248.0000 - val_mae: 8353.0166\n",
            "Epoch 315/500\n",
            "90/90 [==============================] - 0s 847us/step - loss: 7658.5250 - mse: 163786688.0000 - mae: 7658.5249 - val_loss: 8495.9211 - val_mse: 184921040.0000 - val_mae: 8495.9209\n",
            "Epoch 316/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7294.0701 - mse: 134120224.0000 - mae: 7294.0703 - val_loss: 8453.1271 - val_mse: 186466528.0000 - val_mae: 8453.1270\n",
            "Epoch 317/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8629.7499 - mse: 225207008.0000 - mae: 8629.7500 - val_loss: 8400.4864 - val_mse: 186207936.0000 - val_mae: 8400.4863\n",
            "Epoch 318/500\n",
            "90/90 [==============================] - 0s 976us/step - loss: 6751.5801 - mse: 139286896.0000 - mae: 6751.5801 - val_loss: 8292.7068 - val_mse: 183201744.0000 - val_mae: 8292.7061\n",
            "Epoch 319/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8553.7050 - mse: 228927328.0000 - mae: 8553.7051 - val_loss: 8651.3105 - val_mse: 189406976.0000 - val_mae: 8651.3105\n",
            "Epoch 320/500\n",
            "90/90 [==============================] - 0s 893us/step - loss: 8039.3186 - mse: 192440656.0000 - mae: 8039.3188 - val_loss: 8374.9320 - val_mse: 184679696.0000 - val_mae: 8374.9316\n",
            "Epoch 321/500\n",
            "90/90 [==============================] - 0s 878us/step - loss: 8084.2778 - mse: 168892272.0000 - mae: 8084.2778 - val_loss: 8965.8576 - val_mse: 193416304.0000 - val_mae: 8965.8574\n",
            "Epoch 322/500\n",
            "90/90 [==============================] - 0s 979us/step - loss: 8108.0546 - mse: 190569840.0000 - mae: 8108.0547 - val_loss: 8675.3262 - val_mse: 191150832.0000 - val_mae: 8675.3262\n",
            "Epoch 323/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8210.3482 - mse: 163598976.0000 - mae: 8210.3477 - val_loss: 8556.0951 - val_mse: 187221984.0000 - val_mae: 8556.0947\n",
            "Epoch 324/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7923.0146 - mse: 198053440.0000 - mae: 7923.0146 - val_loss: 8214.8883 - val_mse: 180090224.0000 - val_mae: 8214.8887\n",
            "Epoch 325/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8084.2888 - mse: 199402240.0000 - mae: 8084.2891 - val_loss: 8144.8101 - val_mse: 178343360.0000 - val_mae: 8144.8096\n",
            "Epoch 326/500\n",
            "90/90 [==============================] - 0s 859us/step - loss: 8846.7850 - mse: 231634832.0000 - mae: 8846.7852 - val_loss: 8531.9660 - val_mse: 184287040.0000 - val_mae: 8531.9658\n",
            "Epoch 327/500\n",
            "90/90 [==============================] - 0s 884us/step - loss: 8524.0113 - mse: 219439856.0000 - mae: 8524.0117 - val_loss: 8726.6623 - val_mse: 187544576.0000 - val_mae: 8726.6621\n",
            "Epoch 328/500\n",
            "90/90 [==============================] - 0s 809us/step - loss: 7123.4219 - mse: 173463312.0000 - mae: 7123.4214 - val_loss: 8193.3732 - val_mse: 178056400.0000 - val_mae: 8193.3740\n",
            "Epoch 329/500\n",
            "90/90 [==============================] - 0s 894us/step - loss: 6896.2147 - mse: 143210608.0000 - mae: 6896.2144 - val_loss: 8500.4937 - val_mse: 182400960.0000 - val_mae: 8500.4941\n",
            "Epoch 330/500\n",
            "90/90 [==============================] - 0s 985us/step - loss: 7635.9596 - mse: 196021584.0000 - mae: 7635.9600 - val_loss: 8062.3839 - val_mse: 173603728.0000 - val_mae: 8062.3833\n",
            "Epoch 331/500\n",
            "90/90 [==============================] - 0s 872us/step - loss: 9182.0568 - mse: 232848400.0000 - mae: 9182.0566 - val_loss: 8055.7158 - val_mse: 174474512.0000 - val_mae: 8055.7158\n",
            "Epoch 332/500\n",
            "90/90 [==============================] - 0s 809us/step - loss: 7599.8363 - mse: 181231728.0000 - mae: 7599.8359 - val_loss: 8329.0759 - val_mse: 180091840.0000 - val_mae: 8329.0762\n",
            "Epoch 333/500\n",
            "90/90 [==============================] - 0s 938us/step - loss: 8543.6779 - mse: 219501232.0000 - mae: 8543.6787 - val_loss: 8257.7207 - val_mse: 178549440.0000 - val_mae: 8257.7207\n",
            "Epoch 334/500\n",
            "90/90 [==============================] - 0s 985us/step - loss: 8495.9962 - mse: 198281952.0000 - mae: 8495.9961 - val_loss: 8874.2980 - val_mse: 187544720.0000 - val_mae: 8874.2988\n",
            "Epoch 335/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7092.7796 - mse: 153353744.0000 - mae: 7092.7798 - val_loss: 8694.5884 - val_mse: 186392256.0000 - val_mae: 8694.5889\n",
            "Epoch 336/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 6656.8323 - mse: 131991952.0000 - mae: 6656.8325 - val_loss: 8326.8520 - val_mse: 179895664.0000 - val_mae: 8326.8525\n",
            "Epoch 337/500\n",
            "90/90 [==============================] - 0s 888us/step - loss: 7280.2879 - mse: 195153040.0000 - mae: 7280.2876 - val_loss: 8806.8066 - val_mse: 188113840.0000 - val_mae: 8806.8066\n",
            "Epoch 338/500\n",
            "90/90 [==============================] - 0s 996us/step - loss: 8273.2014 - mse: 178302016.0000 - mae: 8273.2012 - val_loss: 8674.8267 - val_mse: 187054336.0000 - val_mae: 8674.8262\n",
            "Epoch 339/500\n",
            "90/90 [==============================] - 0s 952us/step - loss: 8080.2066 - mse: 209979776.0000 - mae: 8080.2056 - val_loss: 8359.8092 - val_mse: 180107040.0000 - val_mae: 8359.8086\n",
            "Epoch 340/500\n",
            "90/90 [==============================] - 0s 884us/step - loss: 7190.1495 - mse: 165096656.0000 - mae: 7190.1494 - val_loss: 8255.0879 - val_mse: 177625104.0000 - val_mae: 8255.0879\n",
            "Epoch 341/500\n",
            "90/90 [==============================] - 0s 907us/step - loss: 7944.8092 - mse: 189579504.0000 - mae: 7944.8091 - val_loss: 9004.0408 - val_mse: 189066064.0000 - val_mae: 9004.0410\n",
            "Epoch 342/500\n",
            "90/90 [==============================] - 0s 906us/step - loss: 8959.8980 - mse: 237322672.0000 - mae: 8959.8975 - val_loss: 9021.7772 - val_mse: 189114000.0000 - val_mae: 9021.7773\n",
            "Epoch 343/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7290.1287 - mse: 166301168.0000 - mae: 7290.1294 - val_loss: 8348.2682 - val_mse: 180796400.0000 - val_mae: 8348.2676\n",
            "Epoch 344/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8770.8432 - mse: 202509312.0000 - mae: 8770.8428 - val_loss: 8878.6809 - val_mse: 187622064.0000 - val_mae: 8878.6807\n",
            "Epoch 345/500\n",
            "90/90 [==============================] - 0s 912us/step - loss: 7906.0505 - mse: 198455248.0000 - mae: 7906.0508 - val_loss: 8698.4202 - val_mse: 184427024.0000 - val_mae: 8698.4199\n",
            "Epoch 346/500\n",
            "90/90 [==============================] - 0s 794us/step - loss: 7837.8204 - mse: 195706288.0000 - mae: 7837.8208 - val_loss: 8569.8506 - val_mse: 182292944.0000 - val_mae: 8569.8496\n",
            "Epoch 347/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7201.4314 - mse: 168409328.0000 - mae: 7201.4312 - val_loss: 8396.1972 - val_mse: 179115616.0000 - val_mae: 8396.1973\n",
            "Epoch 348/500\n",
            "90/90 [==============================] - 0s 884us/step - loss: 7795.9740 - mse: 171477840.0000 - mae: 7795.9736 - val_loss: 8451.1101 - val_mse: 181429536.0000 - val_mae: 8451.1094\n",
            "Epoch 349/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7477.7483 - mse: 165679056.0000 - mae: 7477.7485 - val_loss: 8274.5216 - val_mse: 176300672.0000 - val_mae: 8274.5215\n",
            "Epoch 350/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7437.4910 - mse: 164437712.0000 - mae: 7437.4912 - val_loss: 8412.3365 - val_mse: 177877280.0000 - val_mae: 8412.3359\n",
            "Epoch 351/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 7120.4339 - mse: 142675680.0000 - mae: 7120.4341 - val_loss: 8186.3940 - val_mse: 175694480.0000 - val_mae: 8186.3945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 20 samples, validate on 10 samples\n",
            "Epoch 1/500\n",
            "20/20 [==============================] - 2s 97ms/step - loss: 1510.0013 - mse: 3238689.5000 - mae: 1510.0013 - val_loss: 1417.4679 - val_mse: 2062843.2500 - val_mae: 1417.4679\n",
            "Epoch 2/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1509.9703 - mse: 3238595.2500 - mae: 1509.9703 - val_loss: 1417.4417 - val_mse: 2062770.0000 - val_mae: 1417.4417\n",
            "Epoch 3/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1509.9443 - mse: 3238518.5000 - mae: 1509.9443 - val_loss: 1417.4153 - val_mse: 2062696.7500 - val_mae: 1417.4153\n",
            "Epoch 4/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1509.9023 - mse: 3238420.0000 - mae: 1509.9022 - val_loss: 1417.3777 - val_mse: 2062595.7500 - val_mae: 1417.3777\n",
            "Epoch 5/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1509.7986 - mse: 3238217.5000 - mae: 1509.7986 - val_loss: 1417.3069 - val_mse: 2062414.3750 - val_mae: 1417.3069\n",
            "Epoch 6/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1509.5499 - mse: 3237758.0000 - mae: 1509.5498 - val_loss: 1417.1442 - val_mse: 2062012.0000 - val_mae: 1417.1442\n",
            "Epoch 7/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1508.7137 - mse: 3236478.7500 - mae: 1508.7136 - val_loss: 1416.6898 - val_mse: 2060914.3750 - val_mae: 1416.6898\n",
            "Epoch 8/500\n",
            "20/20 [==============================] - 0s 995us/step - loss: 1506.8937 - mse: 3233590.0000 - mae: 1506.8937 - val_loss: 1415.6888 - val_mse: 2058502.3750 - val_mae: 1415.6888\n",
            "Epoch 9/500\n",
            "20/20 [==============================] - 0s 937us/step - loss: 1504.4682 - mse: 3228047.2500 - mae: 1504.4681 - val_loss: 1413.9479 - val_mse: 2054265.7500 - val_mae: 1413.9479\n",
            "Epoch 10/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1500.1537 - mse: 3220882.7500 - mae: 1500.1537 - val_loss: 1411.4192 - val_mse: 2048055.0000 - val_mae: 1411.4192\n",
            "Epoch 11/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1498.4224 - mse: 3214023.5000 - mae: 1498.4225 - val_loss: 1408.8438 - val_mse: 2041679.6250 - val_mae: 1408.8438\n",
            "Epoch 12/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1494.3087 - mse: 3210977.7500 - mae: 1494.3088 - val_loss: 1406.0254 - val_mse: 2034706.2500 - val_mae: 1406.0254\n",
            "Epoch 13/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1482.3181 - mse: 3184397.2500 - mae: 1482.3181 - val_loss: 1401.8162 - val_mse: 2024261.2500 - val_mae: 1401.8162\n",
            "Epoch 14/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1479.6126 - mse: 3186025.5000 - mae: 1479.6125 - val_loss: 1397.9529 - val_mse: 2014690.2500 - val_mae: 1397.9529\n",
            "Epoch 15/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1477.0577 - mse: 3175744.5000 - mae: 1477.0577 - val_loss: 1394.1217 - val_mse: 2005209.3750 - val_mae: 1394.1217\n",
            "Epoch 16/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1480.0581 - mse: 3187429.5000 - mae: 1480.0582 - val_loss: 1390.9548 - val_mse: 1997393.3750 - val_mae: 1390.9548\n",
            "Epoch 17/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1469.5113 - mse: 3150504.5000 - mae: 1469.5114 - val_loss: 1386.8401 - val_mse: 1987256.0000 - val_mae: 1386.8401\n",
            "Epoch 18/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1460.4642 - mse: 3134563.0000 - mae: 1460.4641 - val_loss: 1382.2889 - val_mse: 1976120.0000 - val_mae: 1382.2889\n",
            "Epoch 19/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1458.9233 - mse: 3139226.7500 - mae: 1458.9232 - val_loss: 1377.4644 - val_mse: 1964396.3750 - val_mae: 1377.4644\n",
            "Epoch 20/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1445.3030 - mse: 3107558.7500 - mae: 1445.3030 - val_loss: 1371.1543 - val_mse: 1949164.0000 - val_mae: 1371.1543\n",
            "Epoch 21/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1438.6865 - mse: 3108903.5000 - mae: 1438.6865 - val_loss: 1364.8508 - val_mse: 1933994.6250 - val_mae: 1364.8508\n",
            "Epoch 22/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1435.5364 - mse: 3089327.2500 - mae: 1435.5364 - val_loss: 1358.7865 - val_mse: 1919501.6250 - val_mae: 1358.7865\n",
            "Epoch 23/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1427.5368 - mse: 3087766.5000 - mae: 1427.5369 - val_loss: 1352.6904 - val_mse: 1904922.3750 - val_mae: 1352.6904\n",
            "Epoch 24/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1413.0574 - mse: 3019442.2500 - mae: 1413.0574 - val_loss: 1345.6139 - val_mse: 1888031.2500 - val_mae: 1345.6139\n",
            "Epoch 25/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1411.3524 - mse: 3028621.5000 - mae: 1411.3523 - val_loss: 1338.8577 - val_mse: 1871950.0000 - val_mae: 1338.8577\n",
            "Epoch 26/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1402.3543 - mse: 3016959.2500 - mae: 1402.3542 - val_loss: 1331.8064 - val_mse: 1855209.3750 - val_mae: 1331.8064\n",
            "Epoch 27/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1401.7795 - mse: 2990068.5000 - mae: 1401.7795 - val_loss: 1324.9075 - val_mse: 1838857.6250 - val_mae: 1324.9075\n",
            "Epoch 28/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1387.2299 - mse: 2966434.5000 - mae: 1387.2299 - val_loss: 1316.9490 - val_mse: 1820025.6250 - val_mae: 1316.9490\n",
            "Epoch 29/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1374.7911 - mse: 2967349.2500 - mae: 1374.7911 - val_loss: 1309.1340 - val_mse: 1801670.2500 - val_mae: 1309.1340\n",
            "Epoch 30/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1368.5107 - mse: 2982320.0000 - mae: 1368.5107 - val_loss: 1301.2776 - val_mse: 1783374.0000 - val_mae: 1301.2776\n",
            "Epoch 31/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1350.6914 - mse: 2876950.0000 - mae: 1350.6914 - val_loss: 1292.2859 - val_mse: 1762347.3750 - val_mae: 1292.2859\n",
            "Epoch 32/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1337.2806 - mse: 2877398.0000 - mae: 1337.2806 - val_loss: 1283.2242 - val_mse: 1741583.7500 - val_mae: 1283.2242\n",
            "Epoch 33/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1302.4405 - mse: 2809096.5000 - mae: 1302.4404 - val_loss: 1272.8763 - val_mse: 1718032.6250 - val_mae: 1272.8763\n",
            "Epoch 34/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1303.0389 - mse: 2807035.2500 - mae: 1303.0388 - val_loss: 1262.7914 - val_mse: 1695119.3750 - val_mae: 1262.7914\n",
            "Epoch 35/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1299.2302 - mse: 2728808.5000 - mae: 1299.2301 - val_loss: 1252.7214 - val_mse: 1672256.8750 - val_mae: 1252.7214\n",
            "Epoch 36/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1286.4132 - mse: 2720949.5000 - mae: 1286.4133 - val_loss: 1244.8250 - val_mse: 1654062.0000 - val_mae: 1244.8250\n",
            "Epoch 37/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1278.4101 - mse: 2740127.2500 - mae: 1278.4102 - val_loss: 1234.2496 - val_mse: 1630846.5000 - val_mae: 1234.2496\n",
            "Epoch 38/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1267.0134 - mse: 2680133.5000 - mae: 1267.0134 - val_loss: 1224.7273 - val_mse: 1609712.6250 - val_mae: 1224.7273\n",
            "Epoch 39/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1261.0341 - mse: 2750412.5000 - mae: 1261.0339 - val_loss: 1213.7281 - val_mse: 1585364.6250 - val_mae: 1213.7281\n",
            "Epoch 40/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1262.3142 - mse: 2687653.0000 - mae: 1262.3142 - val_loss: 1202.7460 - val_mse: 1561206.3750 - val_mae: 1202.7460\n",
            "Epoch 41/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1207.9793 - mse: 2522737.2500 - mae: 1207.9792 - val_loss: 1194.2395 - val_mse: 1542502.6250 - val_mae: 1194.2395\n",
            "Epoch 42/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1196.0279 - mse: 2474662.5000 - mae: 1196.0280 - val_loss: 1184.8676 - val_mse: 1521869.5000 - val_mae: 1184.8676\n",
            "Epoch 43/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1171.4368 - mse: 2482205.2500 - mae: 1171.4369 - val_loss: 1175.5564 - val_mse: 1501499.3750 - val_mae: 1175.5564\n",
            "Epoch 44/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1185.4625 - mse: 2496827.5000 - mae: 1185.4625 - val_loss: 1162.3843 - val_mse: 1473804.2500 - val_mae: 1162.3843\n",
            "Epoch 45/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1163.7931 - mse: 2413192.5000 - mae: 1163.7931 - val_loss: 1152.5894 - val_mse: 1453225.3750 - val_mae: 1152.5894\n",
            "Epoch 46/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1189.0734 - mse: 2503638.7500 - mae: 1189.0735 - val_loss: 1141.7405 - val_mse: 1430512.3750 - val_mae: 1141.7405\n",
            "Epoch 47/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1190.3575 - mse: 2476035.0000 - mae: 1190.3574 - val_loss: 1134.1744 - val_mse: 1414570.7500 - val_mae: 1134.1744\n",
            "Epoch 48/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1160.1450 - mse: 2415157.7500 - mae: 1160.1450 - val_loss: 1123.2893 - val_mse: 1392357.6250 - val_mae: 1123.2893\n",
            "Epoch 49/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1162.2315 - mse: 2390082.7500 - mae: 1162.2316 - val_loss: 1112.0969 - val_mse: 1369602.2500 - val_mae: 1112.0969\n",
            "Epoch 50/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1148.5549 - mse: 2274021.5000 - mae: 1148.5549 - val_loss: 1101.5276 - val_mse: 1348212.6250 - val_mae: 1101.5276\n",
            "Epoch 51/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1165.1800 - mse: 2364482.2500 - mae: 1165.1801 - val_loss: 1091.9463 - val_mse: 1328941.2500 - val_mae: 1091.9463\n",
            "Epoch 52/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1142.6035 - mse: 2238844.7500 - mae: 1142.6035 - val_loss: 1082.8499 - val_mse: 1310645.6250 - val_mae: 1082.8499\n",
            "Epoch 53/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1166.2427 - mse: 2385457.7500 - mae: 1166.2427 - val_loss: 1073.0262 - val_mse: 1291508.6250 - val_mae: 1073.0262\n",
            "Epoch 54/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1131.5401 - mse: 2283513.5000 - mae: 1131.5400 - val_loss: 1062.4532 - val_mse: 1270792.1250 - val_mae: 1062.4532\n",
            "Epoch 55/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1076.9730 - mse: 2183544.5000 - mae: 1076.9730 - val_loss: 1051.8751 - val_mse: 1250137.7500 - val_mae: 1051.8751\n",
            "Epoch 56/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1078.4680 - mse: 2141471.5000 - mae: 1078.4680 - val_loss: 1037.7822 - val_mse: 1223799.3750 - val_mae: 1037.7822\n",
            "Epoch 57/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1125.7024 - mse: 2220128.0000 - mae: 1125.7024 - val_loss: 1029.2366 - val_mse: 1207268.2500 - val_mae: 1029.2366\n",
            "Epoch 58/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1046.8686 - mse: 1995042.3750 - mae: 1046.8685 - val_loss: 1017.2443 - val_mse: 1184774.6250 - val_mae: 1017.2443\n",
            "Epoch 59/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1118.4041 - mse: 2189424.5000 - mae: 1118.4041 - val_loss: 1008.5324 - val_mse: 1168348.3750 - val_mae: 1008.5324\n",
            "Epoch 60/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1142.4840 - mse: 2184510.0000 - mae: 1142.4840 - val_loss: 1004.6844 - val_mse: 1160414.3750 - val_mae: 1004.6844\n",
            "Epoch 61/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1133.4842 - mse: 2185342.7500 - mae: 1133.4841 - val_loss: 999.7178 - val_mse: 1150449.1250 - val_mae: 999.7178\n",
            "Epoch 62/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1027.1174 - mse: 1915554.3750 - mae: 1027.1174 - val_loss: 991.5676 - val_mse: 1135042.2500 - val_mae: 991.5676\n",
            "Epoch 63/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1112.8882 - mse: 2065459.2500 - mae: 1112.8883 - val_loss: 983.8359 - val_mse: 1120476.7500 - val_mae: 983.8359\n",
            "Epoch 64/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1106.6921 - mse: 2077364.3750 - mae: 1106.6921 - val_loss: 978.9431 - val_mse: 1110814.7500 - val_mae: 978.9431\n",
            "Epoch 65/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1094.6506 - mse: 2100620.7500 - mae: 1094.6506 - val_loss: 974.8737 - val_mse: 1102713.0000 - val_mae: 974.8737\n",
            "Epoch 66/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1069.1625 - mse: 2087415.7500 - mae: 1069.1625 - val_loss: 965.0939 - val_mse: 1085418.3750 - val_mae: 965.0939\n",
            "Epoch 67/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1073.7714 - mse: 1934384.7500 - mae: 1073.7714 - val_loss: 956.5820 - val_mse: 1069751.6250 - val_mae: 956.5820\n",
            "Epoch 68/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1045.0505 - mse: 2073564.6250 - mae: 1045.0505 - val_loss: 953.5873 - val_mse: 1063443.6250 - val_mae: 953.5873\n",
            "Epoch 69/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1106.9712 - mse: 2082826.7500 - mae: 1106.9712 - val_loss: 950.4510 - val_mse: 1057112.3750 - val_mae: 950.4510\n",
            "Epoch 70/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1099.1735 - mse: 2065763.6250 - mae: 1099.1735 - val_loss: 940.0930 - val_mse: 1039149.8125 - val_mae: 940.0930\n",
            "Epoch 71/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1091.2641 - mse: 2112684.7500 - mae: 1091.2640 - val_loss: 934.9476 - val_mse: 1029815.0000 - val_mae: 934.9476\n",
            "Epoch 72/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1067.2949 - mse: 1983269.2500 - mae: 1067.2949 - val_loss: 932.3317 - val_mse: 1024133.5000 - val_mae: 932.3317\n",
            "Epoch 73/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1052.9624 - mse: 1968947.2500 - mae: 1052.9624 - val_loss: 925.9518 - val_mse: 1013048.0000 - val_mae: 925.9518\n",
            "Epoch 74/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1037.0335 - mse: 1965374.3750 - mae: 1037.0334 - val_loss: 922.6068 - val_mse: 1006602.3750 - val_mae: 922.6068\n",
            "Epoch 75/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1090.3886 - mse: 2140366.7500 - mae: 1090.3887 - val_loss: 916.2664 - val_mse: 995758.0000 - val_mae: 916.2664\n",
            "Epoch 76/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1127.7540 - mse: 2147528.7500 - mae: 1127.7539 - val_loss: 909.3760 - val_mse: 983962.5000 - val_mae: 909.3760\n",
            "Epoch 77/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1078.6526 - mse: 1969546.0000 - mae: 1078.6526 - val_loss: 902.5416 - val_mse: 971829.1875 - val_mae: 902.5416\n",
            "Epoch 78/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 993.9560 - mse: 1830910.7500 - mae: 993.9559 - val_loss: 898.1050 - val_mse: 963826.8125 - val_mae: 898.1050\n",
            "Epoch 79/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1047.0467 - mse: 1898848.6250 - mae: 1047.0466 - val_loss: 889.0349 - val_mse: 948681.1875 - val_mae: 889.0349\n",
            "Epoch 80/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1127.1035 - mse: 2081582.3750 - mae: 1127.1035 - val_loss: 889.0968 - val_mse: 947461.8750 - val_mae: 889.0968\n",
            "Epoch 81/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1060.7383 - mse: 1928413.7500 - mae: 1060.7383 - val_loss: 879.9006 - val_mse: 932483.1250 - val_mae: 879.9006\n",
            "Epoch 82/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1084.4194 - mse: 1987048.3750 - mae: 1084.4193 - val_loss: 880.9789 - val_mse: 932838.1875 - val_mae: 880.9789\n",
            "Epoch 83/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1031.5081 - mse: 1882125.6250 - mae: 1031.5082 - val_loss: 872.7646 - val_mse: 919347.1875 - val_mae: 872.7646\n",
            "Epoch 84/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1116.2508 - mse: 2116817.5000 - mae: 1116.2507 - val_loss: 867.8291 - val_mse: 911230.5000 - val_mae: 867.8291\n",
            "Epoch 85/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1049.6234 - mse: 1875410.0000 - mae: 1049.6234 - val_loss: 859.9929 - val_mse: 898582.1250 - val_mae: 859.9929\n",
            "Epoch 86/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1083.4810 - mse: 1900906.0000 - mae: 1083.4811 - val_loss: 853.6028 - val_mse: 888053.3125 - val_mae: 853.6028\n",
            "Epoch 87/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1092.0482 - mse: 1984762.7500 - mae: 1092.0482 - val_loss: 852.4330 - val_mse: 885094.6875 - val_mae: 852.4330\n",
            "Epoch 88/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1054.5633 - mse: 1872142.0000 - mae: 1054.5632 - val_loss: 845.3693 - val_mse: 873748.8750 - val_mae: 845.3693\n",
            "Epoch 89/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 986.1667 - mse: 1747915.0000 - mae: 986.1667 - val_loss: 835.3146 - val_mse: 858181.3750 - val_mae: 835.3146\n",
            "Epoch 90/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1110.6426 - mse: 2091999.3750 - mae: 1110.6426 - val_loss: 829.7272 - val_mse: 849136.3750 - val_mae: 829.7272\n",
            "Epoch 91/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 995.8468 - mse: 1688429.6250 - mae: 995.8469 - val_loss: 822.8268 - val_mse: 838236.4375 - val_mae: 822.8268\n",
            "Epoch 92/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 966.6690 - mse: 1703539.2500 - mae: 966.6691 - val_loss: 815.8329 - val_mse: 827211.1875 - val_mae: 815.8329\n",
            "Epoch 93/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1033.8736 - mse: 1704373.0000 - mae: 1033.8737 - val_loss: 812.3353 - val_mse: 820963.1250 - val_mae: 812.3353\n",
            "Epoch 94/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1012.8494 - mse: 1738297.3750 - mae: 1012.8494 - val_loss: 803.9020 - val_mse: 808302.5625 - val_mae: 803.9020\n",
            "Epoch 95/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1000.3240 - mse: 1704660.3750 - mae: 1000.3240 - val_loss: 800.7328 - val_mse: 802710.6250 - val_mae: 800.7328\n",
            "Epoch 96/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1008.8236 - mse: 1795089.2500 - mae: 1008.8236 - val_loss: 793.3075 - val_mse: 791707.8125 - val_mae: 793.3075\n",
            "Epoch 97/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1002.3870 - mse: 1775821.7500 - mae: 1002.3869 - val_loss: 791.8669 - val_mse: 788409.0000 - val_mae: 791.8669\n",
            "Epoch 98/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 987.7579 - mse: 1788800.7500 - mae: 987.7579 - val_loss: 784.6376 - val_mse: 777525.5000 - val_mae: 784.6376\n",
            "Epoch 99/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 982.6221 - mse: 1656678.2500 - mae: 982.6221 - val_loss: 777.9703 - val_mse: 767662.6875 - val_mae: 777.9703\n",
            "Epoch 100/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1094.9465 - mse: 2031655.6250 - mae: 1094.9465 - val_loss: 775.5483 - val_mse: 763397.3750 - val_mae: 775.5483\n",
            "Epoch 101/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 967.6545 - mse: 1729564.0000 - mae: 967.6545 - val_loss: 771.7615 - val_mse: 757249.3750 - val_mae: 771.7615\n",
            "Epoch 102/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1037.3485 - mse: 1645511.3750 - mae: 1037.3484 - val_loss: 769.1691 - val_mse: 752417.5000 - val_mae: 769.1691\n",
            "Epoch 103/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1013.3494 - mse: 1760547.6250 - mae: 1013.3494 - val_loss: 768.6033 - val_mse: 750573.5000 - val_mae: 768.6033\n",
            "Epoch 104/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1026.1142 - mse: 1710643.3750 - mae: 1026.1143 - val_loss: 765.0076 - val_mse: 744869.6250 - val_mae: 765.0076\n",
            "Epoch 105/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 949.1180 - mse: 1524949.7500 - mae: 949.1180 - val_loss: 756.5343 - val_mse: 732650.8125 - val_mae: 756.5343\n",
            "Epoch 106/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 943.8566 - mse: 1589168.2500 - mae: 943.8566 - val_loss: 751.4653 - val_mse: 725036.3750 - val_mae: 751.4653\n",
            "Epoch 107/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 893.7212 - mse: 1562703.2500 - mae: 893.7213 - val_loss: 740.5293 - val_mse: 710008.3125 - val_mae: 740.5293\n",
            "Epoch 108/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1067.6107 - mse: 1820519.6250 - mae: 1067.6107 - val_loss: 743.7845 - val_mse: 713098.5000 - val_mae: 743.7845\n",
            "Epoch 109/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1090.0752 - mse: 1810516.0000 - mae: 1090.0752 - val_loss: 744.9657 - val_mse: 713603.8125 - val_mae: 744.9657\n",
            "Epoch 110/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1031.1585 - mse: 1691207.6250 - mae: 1031.1586 - val_loss: 737.8308 - val_mse: 703372.8125 - val_mae: 737.8308\n",
            "Epoch 111/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 945.3714 - mse: 1569913.2500 - mae: 945.3715 - val_loss: 732.2295 - val_mse: 695335.6875 - val_mae: 732.2295\n",
            "Epoch 112/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1026.0722 - mse: 1810665.2500 - mae: 1026.0723 - val_loss: 731.6310 - val_mse: 693689.2500 - val_mae: 731.6310\n",
            "Epoch 113/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1016.3803 - mse: 1676905.3750 - mae: 1016.3802 - val_loss: 728.3553 - val_mse: 688512.8750 - val_mae: 728.3553\n",
            "Epoch 114/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 987.9445 - mse: 1602717.6250 - mae: 987.9445 - val_loss: 721.8754 - val_mse: 679207.5000 - val_mae: 721.8754\n",
            "Epoch 115/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 995.8428 - mse: 1724933.2500 - mae: 995.8428 - val_loss: 720.6041 - val_mse: 676742.3750 - val_mae: 720.6041\n",
            "Epoch 116/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1074.9662 - mse: 1807399.2500 - mae: 1074.9662 - val_loss: 722.0925 - val_mse: 677519.6250 - val_mae: 722.0925\n",
            "Epoch 117/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1025.5174 - mse: 1719109.2500 - mae: 1025.5173 - val_loss: 714.9061 - val_mse: 667598.3750 - val_mae: 714.9061\n",
            "Epoch 118/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1012.4553 - mse: 1544542.2500 - mae: 1012.4553 - val_loss: 711.3900 - val_mse: 662058.6250 - val_mae: 711.3900\n",
            "Epoch 119/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1016.3764 - mse: 1684952.0000 - mae: 1016.3765 - val_loss: 709.2507 - val_mse: 658539.6875 - val_mae: 709.2507\n",
            "Epoch 120/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1074.1838 - mse: 1861464.3750 - mae: 1074.1838 - val_loss: 707.5446 - val_mse: 655755.8750 - val_mae: 707.5446\n",
            "Epoch 121/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1007.5513 - mse: 1728166.7500 - mae: 1007.5513 - val_loss: 709.1613 - val_mse: 656917.8750 - val_mae: 709.1613\n",
            "Epoch 122/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1094.5061 - mse: 1765690.3750 - mae: 1094.5061 - val_loss: 707.9283 - val_mse: 654453.8750 - val_mae: 707.9283\n",
            "Epoch 123/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1016.3807 - mse: 1735519.0000 - mae: 1016.3807 - val_loss: 707.0743 - val_mse: 652650.1250 - val_mae: 707.0743\n",
            "Epoch 124/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 964.9102 - mse: 1513789.3750 - mae: 964.9103 - val_loss: 704.6105 - val_mse: 648804.6250 - val_mae: 704.6105\n",
            "Epoch 125/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 968.8156 - mse: 1623008.2500 - mae: 968.8156 - val_loss: 700.0416 - val_mse: 642521.6875 - val_mae: 700.0416\n",
            "Epoch 126/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1087.6286 - mse: 1775559.6250 - mae: 1087.6287 - val_loss: 701.1442 - val_mse: 642948.0625 - val_mae: 701.1442\n",
            "Epoch 127/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1002.6822 - mse: 1635796.5000 - mae: 1002.6821 - val_loss: 700.2998 - val_mse: 641094.5000 - val_mae: 700.2998\n",
            "Epoch 128/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 911.1935 - mse: 1406503.3750 - mae: 911.1935 - val_loss: 694.5212 - val_mse: 633277.1875 - val_mae: 694.5212\n",
            "Epoch 129/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 932.8540 - mse: 1372920.7500 - mae: 932.8540 - val_loss: 691.2398 - val_mse: 628400.5000 - val_mae: 691.2398\n",
            "Epoch 130/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 963.0657 - mse: 1488560.7500 - mae: 963.0657 - val_loss: 689.0356 - val_mse: 624885.6250 - val_mae: 689.0356\n",
            "Epoch 131/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1033.4700 - mse: 1639051.2500 - mae: 1033.4700 - val_loss: 688.7499 - val_mse: 623907.3125 - val_mae: 688.7499\n",
            "Epoch 132/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 949.2527 - mse: 1562466.3750 - mae: 949.2527 - val_loss: 683.8718 - val_mse: 617337.1875 - val_mae: 683.8718\n",
            "Epoch 133/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1006.9576 - mse: 1621828.1250 - mae: 1006.9576 - val_loss: 683.2025 - val_mse: 615914.3750 - val_mae: 683.2025\n",
            "Epoch 134/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1008.3910 - mse: 1651132.6250 - mae: 1008.3910 - val_loss: 680.3456 - val_mse: 612012.8125 - val_mae: 680.3456\n",
            "Epoch 135/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1001.4117 - mse: 1755728.6250 - mae: 1001.4117 - val_loss: 673.1423 - val_mse: 602869.8750 - val_mae: 673.1423\n",
            "Epoch 136/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1023.2950 - mse: 1693920.3750 - mae: 1023.2950 - val_loss: 675.2165 - val_mse: 604682.8750 - val_mae: 675.2165\n",
            "Epoch 137/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 937.0565 - mse: 1399177.8750 - mae: 937.0565 - val_loss: 671.0236 - val_mse: 598886.8750 - val_mae: 671.0236\n",
            "Epoch 138/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1029.0045 - mse: 1516781.2500 - mae: 1029.0045 - val_loss: 668.2478 - val_mse: 594859.7500 - val_mae: 668.2478\n",
            "Epoch 139/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 950.7564 - mse: 1507689.7500 - mae: 950.7565 - val_loss: 666.4952 - val_mse: 592187.1250 - val_mae: 666.4952\n",
            "Epoch 140/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1028.4888 - mse: 1655445.5000 - mae: 1028.4889 - val_loss: 661.3465 - val_mse: 585576.1250 - val_mae: 661.3465\n",
            "Epoch 141/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 984.6153 - mse: 1506381.5000 - mae: 984.6152 - val_loss: 659.6832 - val_mse: 583002.3125 - val_mae: 659.6832\n",
            "Epoch 142/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 996.8875 - mse: 1635671.7500 - mae: 996.8875 - val_loss: 656.9271 - val_mse: 579269.1875 - val_mae: 656.9271\n",
            "Epoch 143/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 980.7846 - mse: 1456935.8750 - mae: 980.7847 - val_loss: 653.1483 - val_mse: 574250.3125 - val_mae: 653.1483\n",
            "Epoch 144/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 974.0388 - mse: 1564367.3750 - mae: 974.0389 - val_loss: 643.7770 - val_mse: 563220.4375 - val_mae: 643.7770\n",
            "Epoch 145/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 883.7694 - mse: 1327188.7500 - mae: 883.7694 - val_loss: 640.2816 - val_mse: 558686.8750 - val_mae: 640.2816\n",
            "Epoch 146/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1057.2926 - mse: 1657337.2500 - mae: 1057.2926 - val_loss: 636.4818 - val_mse: 553767.8750 - val_mae: 636.4818\n",
            "Epoch 147/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1005.5796 - mse: 1561691.0000 - mae: 1005.5796 - val_loss: 637.7383 - val_mse: 554349.6875 - val_mae: 637.7383\n",
            "Epoch 148/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1109.7489 - mse: 1805932.6250 - mae: 1109.7488 - val_loss: 637.5755 - val_mse: 553610.6250 - val_mae: 637.5755\n",
            "Epoch 149/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1000.2857 - mse: 1530652.0000 - mae: 1000.2858 - val_loss: 636.3252 - val_mse: 551604.8125 - val_mae: 636.3252\n",
            "Epoch 150/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1036.8194 - mse: 1681224.7500 - mae: 1036.8193 - val_loss: 634.2330 - val_mse: 548794.3750 - val_mae: 634.2330\n",
            "Epoch 151/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1017.5649 - mse: 1519398.5000 - mae: 1017.5649 - val_loss: 631.7687 - val_mse: 545463.6250 - val_mae: 631.7687\n",
            "Epoch 152/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 927.7080 - mse: 1367195.8750 - mae: 927.7080 - val_loss: 631.0019 - val_mse: 543868.6875 - val_mae: 631.0019\n",
            "Epoch 153/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1010.1873 - mse: 1649935.7500 - mae: 1010.1873 - val_loss: 632.7521 - val_mse: 545227.5625 - val_mae: 632.7521\n",
            "Epoch 154/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 932.2276 - mse: 1529873.7500 - mae: 932.2275 - val_loss: 627.6180 - val_mse: 539006.1875 - val_mae: 627.6180\n",
            "Epoch 155/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 899.5065 - mse: 1378699.3750 - mae: 899.5065 - val_loss: 623.8839 - val_mse: 534307.1250 - val_mae: 623.8839\n",
            "Epoch 156/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 965.8200 - mse: 1385646.6250 - mae: 965.8200 - val_loss: 619.7150 - val_mse: 529058.1250 - val_mae: 619.7150\n",
            "Epoch 157/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 964.9980 - mse: 1385900.7500 - mae: 964.9980 - val_loss: 620.2428 - val_mse: 528975.8125 - val_mae: 620.2428\n",
            "Epoch 158/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 981.8394 - mse: 1454447.7500 - mae: 981.8395 - val_loss: 618.9559 - val_mse: 527088.8125 - val_mae: 618.9559\n",
            "Epoch 159/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 919.8454 - mse: 1388596.1250 - mae: 919.8454 - val_loss: 613.9025 - val_mse: 521201.1875 - val_mae: 613.9025\n",
            "Epoch 160/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 973.0779 - mse: 1388138.2500 - mae: 973.0779 - val_loss: 612.1152 - val_mse: 518680.0000 - val_mae: 612.1152\n",
            "Epoch 161/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 946.0139 - mse: 1492669.7500 - mae: 946.0140 - val_loss: 610.6902 - val_mse: 516670.6875 - val_mae: 610.6902\n",
            "Epoch 162/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 982.2430 - mse: 1477183.2500 - mae: 982.2430 - val_loss: 609.5150 - val_mse: 514897.0625 - val_mae: 609.5150\n",
            "Epoch 163/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 958.1484 - mse: 1542934.3750 - mae: 958.1484 - val_loss: 607.6383 - val_mse: 512485.4062 - val_mae: 607.6383\n",
            "Epoch 164/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1066.8475 - mse: 1628479.0000 - mae: 1066.8474 - val_loss: 610.8094 - val_mse: 515344.5000 - val_mae: 610.8094\n",
            "Epoch 165/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 957.9264 - mse: 1403605.0000 - mae: 957.9264 - val_loss: 602.3991 - val_mse: 505939.0625 - val_mae: 602.3991\n",
            "Epoch 166/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 963.3380 - mse: 1442857.6250 - mae: 963.3380 - val_loss: 602.7791 - val_mse: 505746.5000 - val_mae: 602.7791\n",
            "Epoch 167/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1081.6735 - mse: 1676201.2500 - mae: 1081.6735 - val_loss: 604.1593 - val_mse: 506788.5000 - val_mae: 604.1593\n",
            "Epoch 168/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1005.7891 - mse: 1442697.7500 - mae: 1005.7891 - val_loss: 601.6063 - val_mse: 503602.6875 - val_mae: 601.6063\n",
            "Epoch 169/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 989.8247 - mse: 1512088.7500 - mae: 989.8247 - val_loss: 601.1009 - val_mse: 502624.3125 - val_mae: 601.1009\n",
            "Epoch 170/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 976.0926 - mse: 1443378.1250 - mae: 976.0926 - val_loss: 599.6959 - val_mse: 500542.4062 - val_mae: 599.6959\n",
            "Epoch 171/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 969.5674 - mse: 1521357.7500 - mae: 969.5674 - val_loss: 593.8346 - val_mse: 493887.3125 - val_mae: 593.8346\n",
            "Epoch 172/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 961.9760 - mse: 1422092.7500 - mae: 961.9760 - val_loss: 592.1359 - val_mse: 491555.6875 - val_mae: 592.1359\n",
            "Epoch 173/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 974.2922 - mse: 1387516.0000 - mae: 974.2922 - val_loss: 587.7863 - val_mse: 486503.1875 - val_mae: 587.7863\n",
            "Epoch 174/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1017.7788 - mse: 1526060.3750 - mae: 1017.7788 - val_loss: 589.3943 - val_mse: 487686.7500 - val_mae: 589.3943\n",
            "Epoch 175/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 987.5688 - mse: 1425351.2500 - mae: 987.5687 - val_loss: 588.5150 - val_mse: 486155.4062 - val_mae: 588.5150\n",
            "Epoch 176/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1033.0033 - mse: 1540120.1250 - mae: 1033.0033 - val_loss: 588.6312 - val_mse: 485792.9062 - val_mae: 588.6312\n",
            "Epoch 177/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 985.2777 - mse: 1445561.2500 - mae: 985.2777 - val_loss: 585.9779 - val_mse: 482513.0000 - val_mae: 585.9779\n",
            "Epoch 178/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 984.1500 - mse: 1373935.2500 - mae: 984.1500 - val_loss: 587.0148 - val_mse: 483129.9375 - val_mae: 587.0148\n",
            "Epoch 179/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 907.7927 - mse: 1321996.6250 - mae: 907.7927 - val_loss: 586.1028 - val_mse: 481742.7500 - val_mae: 586.1028\n",
            "Epoch 180/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1027.3228 - mse: 1629946.0000 - mae: 1027.3229 - val_loss: 585.9258 - val_mse: 481201.0938 - val_mae: 585.9258\n",
            "Epoch 181/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1003.8478 - mse: 1423344.6250 - mae: 1003.8478 - val_loss: 581.1473 - val_mse: 475768.9062 - val_mae: 581.1473\n",
            "Epoch 182/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 983.0546 - mse: 1448583.8750 - mae: 983.0546 - val_loss: 579.7604 - val_mse: 473873.0000 - val_mae: 579.7604\n",
            "Epoch 183/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1039.6217 - mse: 1530908.7500 - mae: 1039.6217 - val_loss: 581.9235 - val_mse: 475567.5938 - val_mae: 581.9235\n",
            "Epoch 184/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1081.7042 - mse: 1709138.3750 - mae: 1081.7042 - val_loss: 581.7514 - val_mse: 475079.5938 - val_mae: 581.7514\n",
            "Epoch 185/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 973.6926 - mse: 1476244.0000 - mae: 973.6926 - val_loss: 582.5199 - val_mse: 475429.9062 - val_mae: 582.5199\n",
            "Epoch 186/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 907.8076 - mse: 1281603.0000 - mae: 907.8076 - val_loss: 576.9419 - val_mse: 469275.7500 - val_mae: 576.9419\n",
            "Epoch 187/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1031.3326 - mse: 1568976.3750 - mae: 1031.3326 - val_loss: 579.4268 - val_mse: 471463.0625 - val_mae: 579.4268\n",
            "Epoch 188/500\n",
            "20/20 [==============================] - 0s 978us/step - loss: 937.3031 - mse: 1300245.0000 - mae: 937.3030 - val_loss: 577.4642 - val_mse: 468982.4062 - val_mae: 577.4642\n",
            "Epoch 189/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 936.7431 - mse: 1308683.0000 - mae: 936.7432 - val_loss: 577.1431 - val_mse: 468113.5625 - val_mae: 577.1431\n",
            "Epoch 190/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1022.8743 - mse: 1507034.2500 - mae: 1022.8743 - val_loss: 577.9523 - val_mse: 468524.8125 - val_mae: 577.9523\n",
            "Epoch 191/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 944.1253 - mse: 1313268.6250 - mae: 944.1254 - val_loss: 575.5858 - val_mse: 465677.5000 - val_mae: 575.5858\n",
            "Epoch 192/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1013.1083 - mse: 1421790.2500 - mae: 1013.1082 - val_loss: 575.1802 - val_mse: 464854.0938 - val_mae: 575.1802\n",
            "Epoch 193/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 933.9000 - mse: 1358350.0000 - mae: 933.9000 - val_loss: 574.7897 - val_mse: 464095.0000 - val_mae: 574.7897\n",
            "Epoch 194/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 955.0729 - mse: 1369173.0000 - mae: 955.0729 - val_loss: 571.6757 - val_mse: 460543.4062 - val_mae: 571.6757\n",
            "Epoch 195/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1025.5173 - mse: 1570952.0000 - mae: 1025.5173 - val_loss: 571.3480 - val_mse: 459845.5938 - val_mae: 571.3480\n",
            "Epoch 196/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 909.5640 - mse: 1316875.1250 - mae: 909.5640 - val_loss: 570.4016 - val_mse: 458452.8125 - val_mae: 570.4016\n",
            "Epoch 197/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1018.0329 - mse: 1589888.6250 - mae: 1018.0328 - val_loss: 570.4123 - val_mse: 458114.1875 - val_mae: 570.4123\n",
            "Epoch 198/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1027.1224 - mse: 1522236.2500 - mae: 1027.1224 - val_loss: 572.8514 - val_mse: 460164.0938 - val_mae: 572.8514\n",
            "Epoch 199/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 849.7843 - mse: 1298512.3750 - mae: 849.7844 - val_loss: 565.7345 - val_mse: 452649.5938 - val_mae: 565.7345\n",
            "Epoch 200/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 908.9654 - mse: 1297432.8750 - mae: 908.9655 - val_loss: 564.7934 - val_mse: 451324.0938 - val_mae: 564.7934\n",
            "Epoch 201/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 970.9655 - mse: 1502140.7500 - mae: 970.9655 - val_loss: 565.3804 - val_mse: 451576.1875 - val_mae: 565.3804\n",
            "Epoch 202/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 996.0056 - mse: 1423725.2500 - mae: 996.0056 - val_loss: 562.6920 - val_mse: 448516.0000 - val_mae: 562.6920\n",
            "Epoch 203/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 981.9300 - mse: 1553107.2500 - mae: 981.9300 - val_loss: 559.9576 - val_mse: 444915.1562 - val_mae: 559.9576\n",
            "Epoch 204/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 921.1833 - mse: 1316357.0000 - mae: 921.1833 - val_loss: 556.2966 - val_mse: 440024.7500 - val_mae: 556.2966\n",
            "Epoch 205/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 979.1628 - mse: 1463940.6250 - mae: 979.1627 - val_loss: 555.3602 - val_mse: 438590.1875 - val_mae: 555.3602\n",
            "Epoch 206/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1036.2384 - mse: 1459004.6250 - mae: 1036.2384 - val_loss: 555.4458 - val_mse: 438446.4062 - val_mae: 555.4458\n",
            "Epoch 207/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 998.1462 - mse: 1401695.7500 - mae: 998.1462 - val_loss: 554.5588 - val_mse: 437063.9062 - val_mae: 554.5588\n",
            "Epoch 208/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1032.3386 - mse: 1550072.1250 - mae: 1032.3386 - val_loss: 554.7155 - val_mse: 437032.3438 - val_mae: 554.7155\n",
            "Epoch 209/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1055.9200 - mse: 1542221.2500 - mae: 1055.9200 - val_loss: 553.6354 - val_mse: 435426.8125 - val_mae: 553.6354\n",
            "Epoch 210/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 962.2465 - mse: 1519598.7500 - mae: 962.2466 - val_loss: 552.0665 - val_mse: 433254.5938 - val_mae: 552.0665\n",
            "Epoch 211/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 939.0385 - mse: 1383244.0000 - mae: 939.0385 - val_loss: 550.8397 - val_mse: 431500.0000 - val_mae: 550.8397\n",
            "Epoch 212/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 932.4352 - mse: 1359880.0000 - mae: 932.4352 - val_loss: 550.5599 - val_mse: 430904.9375 - val_mae: 550.5599\n",
            "Epoch 213/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1027.1003 - mse: 1473825.0000 - mae: 1027.1003 - val_loss: 550.5759 - val_mse: 430675.4062 - val_mae: 550.5759\n",
            "Epoch 214/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 982.7894 - mse: 1535075.2500 - mae: 982.7894 - val_loss: 549.6568 - val_mse: 429332.4062 - val_mae: 549.6568\n",
            "Epoch 215/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1060.1023 - mse: 1611055.5000 - mae: 1060.1023 - val_loss: 550.2238 - val_mse: 429827.5000 - val_mae: 550.2238\n",
            "Epoch 216/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 882.1264 - mse: 1197420.2500 - mae: 882.1263 - val_loss: 545.9000 - val_mse: 424153.0938 - val_mae: 545.9000\n",
            "Epoch 217/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 896.1870 - mse: 1224194.3750 - mae: 896.1869 - val_loss: 544.8957 - val_mse: 422668.3125 - val_mae: 544.8957\n",
            "Epoch 218/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 842.7282 - mse: 1271331.2500 - mae: 842.7282 - val_loss: 536.0718 - val_mse: 411613.9062 - val_mae: 536.0718\n",
            "Epoch 219/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1000.1850 - mse: 1457249.0000 - mae: 1000.1849 - val_loss: 535.4133 - val_mse: 410590.8438 - val_mae: 535.4133\n",
            "Epoch 220/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 917.3155 - mse: 1391953.3750 - mae: 917.3154 - val_loss: 531.5514 - val_mse: 405687.5625 - val_mae: 531.5514\n",
            "Epoch 221/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 955.1973 - mse: 1303359.2500 - mae: 955.1974 - val_loss: 531.6248 - val_mse: 405510.8438 - val_mae: 531.6248\n",
            "Epoch 222/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 948.3449 - mse: 1428970.7500 - mae: 948.3449 - val_loss: 530.0452 - val_mse: 403383.6875 - val_mae: 530.0452\n",
            "Epoch 223/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 972.2236 - mse: 1351374.7500 - mae: 972.2236 - val_loss: 530.3882 - val_mse: 403532.3438 - val_mae: 530.3882\n",
            "Epoch 224/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1026.4904 - mse: 1382338.6250 - mae: 1026.4905 - val_loss: 530.3148 - val_mse: 403142.7188 - val_mae: 530.3148\n",
            "Epoch 225/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 839.3868 - mse: 1154184.6250 - mae: 839.3868 - val_loss: 527.0510 - val_mse: 399021.3438 - val_mae: 527.0510\n",
            "Epoch 226/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 987.7775 - mse: 1358234.2500 - mae: 987.7775 - val_loss: 526.1978 - val_mse: 397791.3750 - val_mae: 526.1978\n",
            "Epoch 227/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 960.5949 - mse: 1325610.0000 - mae: 960.5949 - val_loss: 526.4937 - val_mse: 397859.2812 - val_mae: 526.4937\n",
            "Epoch 228/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 987.1902 - mse: 1497339.5000 - mae: 987.1902 - val_loss: 525.7141 - val_mse: 396754.0312 - val_mae: 525.7141\n",
            "Epoch 229/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 997.4722 - mse: 1429639.0000 - mae: 997.4722 - val_loss: 526.5511 - val_mse: 397508.5625 - val_mae: 526.5511\n",
            "Epoch 230/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 877.6061 - mse: 1112150.3750 - mae: 877.6061 - val_loss: 522.2946 - val_mse: 392172.4062 - val_mae: 522.2946\n",
            "Epoch 231/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1029.3109 - mse: 1502191.1250 - mae: 1029.3108 - val_loss: 523.8788 - val_mse: 393824.7500 - val_mae: 523.8788\n",
            "Epoch 232/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 940.7227 - mse: 1275234.7500 - mae: 940.7227 - val_loss: 523.7468 - val_mse: 393446.9062 - val_mae: 523.7468\n",
            "Epoch 233/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 881.9486 - mse: 1264720.0000 - mae: 881.9486 - val_loss: 518.0507 - val_mse: 386542.4062 - val_mae: 518.0507\n",
            "Epoch 234/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 974.7443 - mse: 1304134.0000 - mae: 974.7443 - val_loss: 518.7379 - val_mse: 387111.0312 - val_mae: 518.7379\n",
            "Epoch 235/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 906.9767 - mse: 1148417.3750 - mae: 906.9767 - val_loss: 516.4689 - val_mse: 384209.9375 - val_mae: 516.4689\n",
            "Epoch 236/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 932.7443 - mse: 1370204.7500 - mae: 932.7443 - val_loss: 513.6503 - val_mse: 380732.9688 - val_mae: 513.6503\n",
            "Epoch 237/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 958.6749 - mse: 1287959.0000 - mae: 958.6749 - val_loss: 512.3416 - val_mse: 378963.1875 - val_mae: 512.3416\n",
            "Epoch 238/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 991.1072 - mse: 1425137.6250 - mae: 991.1072 - val_loss: 511.6976 - val_mse: 378013.8125 - val_mae: 511.6976\n",
            "Epoch 239/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 910.9319 - mse: 1260256.7500 - mae: 910.9318 - val_loss: 511.0478 - val_mse: 377055.1875 - val_mae: 511.0478\n",
            "Epoch 240/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1083.8006 - mse: 1609098.2500 - mae: 1083.8005 - val_loss: 513.5428 - val_mse: 379744.6875 - val_mae: 513.5428\n",
            "Epoch 241/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 908.1448 - mse: 1263962.3750 - mae: 908.1448 - val_loss: 510.3062 - val_mse: 375780.2500 - val_mae: 510.3062\n",
            "Epoch 242/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 969.9783 - mse: 1314537.0000 - mae: 969.9783 - val_loss: 509.1312 - val_mse: 374201.1562 - val_mae: 509.1312\n",
            "Epoch 243/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1023.9829 - mse: 1419008.6250 - mae: 1023.9829 - val_loss: 508.9570 - val_mse: 373777.3750 - val_mae: 508.9570\n",
            "Epoch 244/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 908.2629 - mse: 1165064.2500 - mae: 908.2629 - val_loss: 506.6456 - val_mse: 370899.2812 - val_mae: 506.6456\n",
            "Epoch 245/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 945.5975 - mse: 1265193.0000 - mae: 945.5975 - val_loss: 504.7313 - val_mse: 368491.1562 - val_mae: 504.7313\n",
            "Epoch 246/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 832.5735 - mse: 1119503.3750 - mae: 832.5735 - val_loss: 502.6603 - val_mse: 365954.2500 - val_mae: 502.6603\n",
            "Epoch 247/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 897.6983 - mse: 1221315.0000 - mae: 897.6982 - val_loss: 497.4057 - val_mse: 359884.8438 - val_mae: 497.4057\n",
            "Epoch 248/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1006.3335 - mse: 1326873.0000 - mae: 1006.3336 - val_loss: 498.3744 - val_mse: 360687.7500 - val_mae: 498.3744\n",
            "Epoch 249/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 909.2650 - mse: 1257474.6250 - mae: 909.2650 - val_loss: 496.1277 - val_mse: 357992.5625 - val_mae: 496.1277\n",
            "Epoch 250/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 870.7881 - mse: 1142427.1250 - mae: 870.7881 - val_loss: 493.1368 - val_mse: 354466.7188 - val_mae: 493.1368\n",
            "Epoch 251/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 964.5599 - mse: 1365679.6250 - mae: 964.5599 - val_loss: 493.4467 - val_mse: 354611.3125 - val_mae: 493.4467\n",
            "Epoch 252/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 970.2572 - mse: 1416697.2500 - mae: 970.2572 - val_loss: 492.3097 - val_mse: 353200.0938 - val_mae: 492.3097\n",
            "Epoch 253/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1008.1291 - mse: 1417226.7500 - mae: 1008.1291 - val_loss: 493.5655 - val_mse: 354352.3750 - val_mae: 493.5655\n",
            "Epoch 254/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 860.1745 - mse: 1224857.6250 - mae: 860.1745 - val_loss: 489.6523 - val_mse: 349865.3438 - val_mae: 489.6523\n",
            "Epoch 255/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1060.3486 - mse: 1615669.0000 - mae: 1060.3486 - val_loss: 491.5546 - val_mse: 351743.7188 - val_mae: 491.5546\n",
            "Epoch 256/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1030.9616 - mse: 1538090.6250 - mae: 1030.9615 - val_loss: 493.1566 - val_mse: 353304.1250 - val_mae: 493.1566\n",
            "Epoch 257/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 911.8546 - mse: 1141228.6250 - mae: 911.8545 - val_loss: 490.8798 - val_mse: 350558.1875 - val_mae: 490.8798\n",
            "Epoch 258/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 989.2454 - mse: 1454586.7500 - mae: 989.2453 - val_loss: 490.3839 - val_mse: 349872.8125 - val_mae: 490.3839\n",
            "Epoch 259/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1036.9934 - mse: 1519706.7500 - mae: 1036.9934 - val_loss: 490.3343 - val_mse: 349630.6562 - val_mae: 490.3343\n",
            "Epoch 260/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 972.1416 - mse: 1324739.0000 - mae: 972.1416 - val_loss: 490.6683 - val_mse: 349784.0625 - val_mae: 490.6683\n",
            "Epoch 261/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 914.8552 - mse: 1188827.3750 - mae: 914.8553 - val_loss: 487.1970 - val_mse: 345757.6562 - val_mae: 487.1970\n",
            "Epoch 262/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 997.8300 - mse: 1413784.7500 - mae: 997.8300 - val_loss: 485.3130 - val_mse: 343506.6562 - val_mae: 485.3130\n",
            "Epoch 263/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 894.1311 - mse: 1052651.1250 - mae: 894.1310 - val_loss: 482.6838 - val_mse: 340413.2812 - val_mae: 482.6838\n",
            "Epoch 264/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 898.7418 - mse: 1227026.1250 - mae: 898.7418 - val_loss: 479.1066 - val_mse: 336411.6875 - val_mae: 479.1066\n",
            "Epoch 265/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 941.3586 - mse: 1274222.6250 - mae: 941.3586 - val_loss: 478.8132 - val_mse: 335887.7500 - val_mae: 478.8132\n",
            "Epoch 266/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1050.0326 - mse: 1535014.6250 - mae: 1050.0326 - val_loss: 479.2704 - val_mse: 336183.5000 - val_mae: 479.2704\n",
            "Epoch 267/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 941.1513 - mse: 1358641.7500 - mae: 941.1512 - val_loss: 476.2809 - val_mse: 332837.9062 - val_mae: 476.2809\n",
            "Epoch 268/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1010.1394 - mse: 1436106.6250 - mae: 1010.1395 - val_loss: 477.2186 - val_mse: 333603.8750 - val_mae: 477.2186\n",
            "Epoch 269/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 955.4701 - mse: 1294134.0000 - mae: 955.4701 - val_loss: 477.4045 - val_mse: 333577.7812 - val_mae: 477.4045\n",
            "Epoch 270/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1030.3810 - mse: 1476945.1250 - mae: 1030.3810 - val_loss: 477.4771 - val_mse: 333474.2500 - val_mae: 477.4771\n",
            "Epoch 271/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 859.6381 - mse: 1059618.6250 - mae: 859.6381 - val_loss: 475.8748 - val_mse: 331534.2812 - val_mae: 475.8748\n",
            "Epoch 272/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 921.1109 - mse: 1307428.3750 - mae: 921.1110 - val_loss: 472.8169 - val_mse: 328125.0000 - val_mae: 472.8169\n",
            "Epoch 273/500\n",
            "20/20 [==============================] - 0s 922us/step - loss: 1045.7825 - mse: 1461634.7500 - mae: 1045.7825 - val_loss: 473.1561 - val_mse: 328315.5938 - val_mae: 473.1561\n",
            "Epoch 274/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1068.3250 - mse: 1542876.3750 - mae: 1068.3250 - val_loss: 473.8842 - val_mse: 328921.8125 - val_mae: 473.8842\n",
            "Epoch 275/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1009.8888 - mse: 1484335.6250 - mae: 1009.8888 - val_loss: 474.9708 - val_mse: 329890.5000 - val_mae: 474.9708\n",
            "Epoch 276/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 948.2610 - mse: 1348316.6250 - mae: 948.2610 - val_loss: 473.5282 - val_mse: 328209.3125 - val_mae: 473.5282\n",
            "Epoch 277/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 855.4215 - mse: 1003014.3750 - mae: 855.4215 - val_loss: 470.3980 - val_mse: 324705.8125 - val_mae: 470.3980\n",
            "Epoch 278/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 977.2103 - mse: 1345335.6250 - mae: 977.2103 - val_loss: 470.7158 - val_mse: 324792.8125 - val_mae: 470.7158\n",
            "Epoch 279/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1073.6661 - mse: 1577501.2500 - mae: 1073.6660 - val_loss: 473.0015 - val_mse: 326994.0625 - val_mae: 473.0015\n",
            "Epoch 280/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 889.2489 - mse: 1192431.0000 - mae: 889.2489 - val_loss: 469.5490 - val_mse: 323218.0625 - val_mae: 469.5490\n",
            "Epoch 281/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 914.5130 - mse: 1302555.1250 - mae: 914.5130 - val_loss: 467.7637 - val_mse: 321236.6562 - val_mae: 467.7637\n",
            "Epoch 282/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 979.1786 - mse: 1326769.1250 - mae: 979.1787 - val_loss: 467.1410 - val_mse: 320421.3750 - val_mae: 467.1410\n",
            "Epoch 283/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 939.8109 - mse: 1176831.2500 - mae: 939.8109 - val_loss: 465.5351 - val_mse: 318536.5312 - val_mae: 465.5351\n",
            "Epoch 284/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 955.5214 - mse: 1297946.2500 - mae: 955.5215 - val_loss: 464.9870 - val_mse: 317781.8125 - val_mae: 464.9870\n",
            "Epoch 285/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 828.5475 - mse: 1045141.1250 - mae: 828.5475 - val_loss: 461.6967 - val_mse: 314240.8438 - val_mae: 461.6967\n",
            "Epoch 286/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1029.7518 - mse: 1481704.2500 - mae: 1029.7518 - val_loss: 463.0839 - val_mse: 315431.5625 - val_mae: 463.0839\n",
            "Epoch 287/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1047.4230 - mse: 1477117.7500 - mae: 1047.4231 - val_loss: 462.2123 - val_mse: 314373.7812 - val_mae: 462.2123\n",
            "Epoch 288/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1015.9604 - mse: 1457515.2500 - mae: 1015.9603 - val_loss: 463.4078 - val_mse: 315415.9688 - val_mae: 463.4078\n",
            "Epoch 289/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 994.9726 - mse: 1446654.8750 - mae: 994.9727 - val_loss: 462.3337 - val_mse: 314171.3750 - val_mae: 462.3337\n",
            "Epoch 290/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 907.3747 - mse: 1139552.7500 - mae: 907.3747 - val_loss: 461.6384 - val_mse: 313254.6562 - val_mae: 461.6384\n",
            "Epoch 291/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 859.3994 - mse: 1079612.7500 - mae: 859.3994 - val_loss: 458.9899 - val_mse: 310340.8438 - val_mae: 458.9899\n",
            "Epoch 292/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 972.2620 - mse: 1297638.2500 - mae: 972.2620 - val_loss: 457.8648 - val_mse: 309031.3438 - val_mae: 457.8648\n",
            "Epoch 293/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1045.8837 - mse: 1446485.6250 - mae: 1045.8837 - val_loss: 459.5202 - val_mse: 310547.0938 - val_mae: 459.5202\n",
            "Epoch 294/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 975.5897 - mse: 1375196.0000 - mae: 975.5897 - val_loss: 458.5123 - val_mse: 309355.0938 - val_mae: 458.5123\n",
            "Epoch 295/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 934.1844 - mse: 1330069.0000 - mae: 934.1844 - val_loss: 458.3100 - val_mse: 309005.0625 - val_mae: 458.3100\n",
            "Epoch 296/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 865.3349 - mse: 1097112.3750 - mae: 865.3348 - val_loss: 456.9324 - val_mse: 307406.0000 - val_mae: 456.9324\n",
            "Epoch 297/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 929.2276 - mse: 1266270.7500 - mae: 929.2277 - val_loss: 456.7144 - val_mse: 307001.7188 - val_mae: 456.7144\n",
            "Epoch 298/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 936.6636 - mse: 1303686.0000 - mae: 936.6636 - val_loss: 456.5968 - val_mse: 306719.6875 - val_mae: 456.5968\n",
            "Epoch 299/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 941.1044 - mse: 1258709.7500 - mae: 941.1045 - val_loss: 456.4472 - val_mse: 306386.6875 - val_mae: 456.4472\n",
            "Epoch 300/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 942.0466 - mse: 1350308.0000 - mae: 942.0466 - val_loss: 456.5786 - val_mse: 306387.6562 - val_mae: 456.5786\n",
            "Epoch 301/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 911.8585 - mse: 1101675.7500 - mae: 911.8585 - val_loss: 454.5056 - val_mse: 304138.0625 - val_mae: 454.5056\n",
            "Epoch 302/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 936.2189 - mse: 1254791.2500 - mae: 936.2189 - val_loss: 454.2921 - val_mse: 303734.0312 - val_mae: 454.2921\n",
            "Epoch 303/500\n",
            "20/20 [==============================] - 0s 994us/step - loss: 896.6323 - mse: 1067815.1250 - mae: 896.6323 - val_loss: 452.2650 - val_mse: 301547.2500 - val_mae: 452.2650\n",
            "Epoch 304/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 949.0543 - mse: 1376140.3750 - mae: 949.0543 - val_loss: 451.0045 - val_mse: 300195.1562 - val_mae: 451.0045\n",
            "Epoch 305/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 919.0752 - mse: 1292201.6250 - mae: 919.0752 - val_loss: 446.7780 - val_mse: 295925.9375 - val_mae: 446.7780\n",
            "Epoch 306/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 986.6242 - mse: 1262203.6250 - mae: 986.6242 - val_loss: 447.4505 - val_mse: 296402.3125 - val_mae: 447.4505\n",
            "Epoch 307/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 987.8826 - mse: 1307058.3750 - mae: 987.8826 - val_loss: 448.2083 - val_mse: 296999.1562 - val_mae: 448.2083\n",
            "Epoch 308/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1023.7236 - mse: 1598125.7500 - mae: 1023.7236 - val_loss: 446.9937 - val_mse: 295709.8125 - val_mae: 446.9937\n",
            "Epoch 309/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1057.1877 - mse: 1498835.8750 - mae: 1057.1877 - val_loss: 447.3824 - val_mse: 295945.0625 - val_mae: 447.3824\n",
            "Epoch 310/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 966.0716 - mse: 1374864.7500 - mae: 966.0716 - val_loss: 447.3943 - val_mse: 295813.6875 - val_mae: 447.3943\n",
            "Epoch 311/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 941.6932 - mse: 1238253.2500 - mae: 941.6932 - val_loss: 447.2661 - val_mse: 295525.7500 - val_mae: 447.2661\n",
            "Epoch 312/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 790.9381 - mse: 966779.6250 - mae: 790.9381 - val_loss: 444.7527 - val_mse: 292895.5000 - val_mae: 444.7527\n",
            "Epoch 313/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 966.7889 - mse: 1359044.0000 - mae: 966.7889 - val_loss: 444.2733 - val_mse: 292296.0938 - val_mae: 444.2733\n",
            "Epoch 314/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 928.4120 - mse: 1189473.0000 - mae: 928.4119 - val_loss: 442.4415 - val_mse: 290370.0000 - val_mae: 442.4415\n",
            "Epoch 315/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1027.4488 - mse: 1462658.2500 - mae: 1027.4489 - val_loss: 442.4907 - val_mse: 290287.5625 - val_mae: 442.4907\n",
            "Epoch 316/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 990.0483 - mse: 1296201.8750 - mae: 990.0482 - val_loss: 443.4131 - val_mse: 291040.3125 - val_mae: 443.4131\n",
            "Epoch 317/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1037.4669 - mse: 1441881.5000 - mae: 1037.4669 - val_loss: 444.9832 - val_mse: 292408.5625 - val_mae: 444.9832\n",
            "Epoch 318/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 984.8563 - mse: 1413848.7500 - mae: 984.8563 - val_loss: 445.9971 - val_mse: 293273.8125 - val_mae: 445.9971\n",
            "Epoch 319/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 982.2838 - mse: 1277070.3750 - mae: 982.2838 - val_loss: 446.6123 - val_mse: 293709.9375 - val_mae: 446.6123\n",
            "Epoch 320/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1075.4319 - mse: 1632548.0000 - mae: 1075.4319 - val_loss: 448.9355 - val_mse: 295903.6875 - val_mae: 448.9355\n",
            "Epoch 321/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 905.2532 - mse: 1039701.0000 - mae: 905.2532 - val_loss: 446.8408 - val_mse: 293685.2188 - val_mae: 446.8408\n",
            "Epoch 322/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 964.8335 - mse: 1288745.1250 - mae: 964.8335 - val_loss: 445.7173 - val_mse: 292445.1562 - val_mae: 445.7173\n",
            "Epoch 323/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 956.1920 - mse: 1294332.1250 - mae: 956.1920 - val_loss: 443.5674 - val_mse: 290227.0625 - val_mae: 443.5674\n",
            "Epoch 324/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 943.4497 - mse: 1314593.7500 - mae: 943.4497 - val_loss: 443.7133 - val_mse: 290253.5938 - val_mae: 443.7133\n",
            "Epoch 325/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 906.0740 - mse: 1084241.6250 - mae: 906.0740 - val_loss: 443.0860 - val_mse: 289477.5625 - val_mae: 443.0860\n",
            "Epoch 326/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 951.8076 - mse: 1206862.7500 - mae: 951.8076 - val_loss: 443.3632 - val_mse: 289591.4375 - val_mae: 443.3632\n",
            "Epoch 327/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 881.6387 - mse: 1114483.6250 - mae: 881.6387 - val_loss: 441.1057 - val_mse: 287243.9375 - val_mae: 441.1057\n",
            "Epoch 328/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 952.0988 - mse: 1222924.6250 - mae: 952.0988 - val_loss: 439.8500 - val_mse: 285891.3438 - val_mae: 439.8500\n",
            "Epoch 329/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 963.1184 - mse: 1361195.7500 - mae: 963.1183 - val_loss: 438.7181 - val_mse: 284696.6875 - val_mae: 438.7181\n",
            "Epoch 330/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 922.9652 - mse: 1160422.8750 - mae: 922.9652 - val_loss: 437.9081 - val_mse: 283766.9375 - val_mae: 437.9081\n",
            "Epoch 331/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 954.9755 - mse: 1206695.8750 - mae: 954.9756 - val_loss: 438.0404 - val_mse: 283731.3125 - val_mae: 438.0404\n",
            "Epoch 332/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1025.5900 - mse: 1332229.3750 - mae: 1025.5901 - val_loss: 439.3604 - val_mse: 284856.0625 - val_mae: 439.3604\n",
            "Epoch 333/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 951.8192 - mse: 1323272.7500 - mae: 951.8192 - val_loss: 438.1273 - val_mse: 283539.8438 - val_mae: 438.1273\n",
            "Epoch 334/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 856.5732 - mse: 957494.8750 - mae: 856.5732 - val_loss: 436.7363 - val_mse: 282042.6875 - val_mae: 436.7363\n",
            "Epoch 335/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 920.4696 - mse: 1288099.7500 - mae: 920.4696 - val_loss: 436.2399 - val_mse: 281458.8750 - val_mae: 436.2399\n",
            "Epoch 336/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 902.7862 - mse: 1141022.6250 - mae: 902.7863 - val_loss: 435.5406 - val_mse: 280656.6875 - val_mae: 435.5406\n",
            "Epoch 337/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 971.4975 - mse: 1282157.2500 - mae: 971.4974 - val_loss: 436.1757 - val_mse: 281118.4062 - val_mae: 436.1757\n",
            "Epoch 338/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1048.0608 - mse: 1433129.2500 - mae: 1048.0608 - val_loss: 436.8434 - val_mse: 281632.3438 - val_mae: 436.8434\n",
            "Epoch 339/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 953.3302 - mse: 1212850.8750 - mae: 953.3303 - val_loss: 436.9581 - val_mse: 281609.6875 - val_mae: 436.9581\n",
            "Epoch 340/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 989.4849 - mse: 1326878.2500 - mae: 989.4849 - val_loss: 437.8205 - val_mse: 282337.9375 - val_mae: 437.8205\n",
            "Epoch 341/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 996.3641 - mse: 1420733.3750 - mae: 996.3641 - val_loss: 438.7738 - val_mse: 283146.1562 - val_mae: 438.7738\n",
            "Epoch 342/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1021.2187 - mse: 1368204.3750 - mae: 1021.2188 - val_loss: 440.0573 - val_mse: 284247.4062 - val_mae: 440.0573\n",
            "Epoch 343/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 965.8381 - mse: 1269327.7500 - mae: 965.8381 - val_loss: 438.3072 - val_mse: 282432.5312 - val_mae: 438.3072\n",
            "Epoch 344/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 961.8316 - mse: 1248718.0000 - mae: 961.8317 - val_loss: 438.5771 - val_mse: 282572.0625 - val_mae: 438.5771\n",
            "Epoch 345/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1023.5167 - mse: 1384256.1250 - mae: 1023.5166 - val_loss: 439.8909 - val_mse: 283734.1875 - val_mae: 439.8909\n",
            "Epoch 346/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1082.6022 - mse: 1523881.7500 - mae: 1082.6022 - val_loss: 439.9603 - val_mse: 283690.6562 - val_mae: 439.9603\n",
            "Epoch 347/500\n",
            "20/20 [==============================] - 0s 966us/step - loss: 939.9075 - mse: 1179408.7500 - mae: 939.9075 - val_loss: 440.0385 - val_mse: 283655.0625 - val_mae: 440.0385\n",
            "Epoch 348/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 890.9332 - mse: 1143898.3750 - mae: 890.9332 - val_loss: 439.1385 - val_mse: 282661.2500 - val_mae: 439.1385\n",
            "Epoch 349/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 943.0384 - mse: 1174514.3750 - mae: 943.0384 - val_loss: 439.1889 - val_mse: 282588.8125 - val_mae: 439.1889\n",
            "Epoch 350/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 908.0612 - mse: 1150040.0000 - mae: 908.0612 - val_loss: 437.3834 - val_mse: 280719.3438 - val_mae: 437.3834\n",
            "Epoch 351/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 960.0495 - mse: 1300927.6250 - mae: 960.0495 - val_loss: 436.2628 - val_mse: 279539.6875 - val_mae: 436.2628\n",
            "Epoch 352/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 854.1930 - mse: 968501.8750 - mae: 854.1930 - val_loss: 434.8673 - val_mse: 278058.7188 - val_mae: 434.8673\n",
            "Epoch 353/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 984.8654 - mse: 1270758.1250 - mae: 984.8654 - val_loss: 434.5071 - val_mse: 277603.8438 - val_mae: 434.5071\n",
            "Epoch 354/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1011.4338 - mse: 1337795.6250 - mae: 1011.4338 - val_loss: 435.6658 - val_mse: 278581.6562 - val_mae: 435.6658\n",
            "Epoch 355/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 949.7701 - mse: 1211772.7500 - mae: 949.7701 - val_loss: 434.6125 - val_mse: 277452.3125 - val_mae: 434.6125\n",
            "Epoch 356/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 918.8149 - mse: 1148998.6250 - mae: 918.8148 - val_loss: 434.1707 - val_mse: 276912.0625 - val_mae: 434.1707\n",
            "Epoch 357/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 935.6102 - mse: 1162228.0000 - mae: 935.6102 - val_loss: 434.0869 - val_mse: 276696.9062 - val_mae: 434.0869\n",
            "Epoch 358/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1042.6279 - mse: 1339647.2500 - mae: 1042.6279 - val_loss: 435.6652 - val_mse: 278084.5625 - val_mae: 435.6652\n",
            "Epoch 359/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 916.9456 - mse: 1326710.7500 - mae: 916.9457 - val_loss: 434.2843 - val_mse: 276691.6875 - val_mae: 434.2843\n",
            "Epoch 360/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 905.3625 - mse: 1046662.6250 - mae: 905.3625 - val_loss: 432.5148 - val_mse: 274878.0000 - val_mae: 432.5148\n",
            "Epoch 361/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 991.1100 - mse: 1326716.2500 - mae: 991.1100 - val_loss: 432.3351 - val_mse: 274609.5625 - val_mae: 432.3351\n",
            "Epoch 362/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 931.9300 - mse: 1217861.7500 - mae: 931.9301 - val_loss: 432.1166 - val_mse: 274301.2500 - val_mae: 432.1166\n",
            "Epoch 363/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 992.9333 - mse: 1254918.2500 - mae: 992.9334 - val_loss: 432.9093 - val_mse: 274922.8125 - val_mae: 432.9093\n",
            "Epoch 364/500\n",
            "20/20 [==============================] - 0s 998us/step - loss: 897.9859 - mse: 1247517.2500 - mae: 897.9860 - val_loss: 432.2997 - val_mse: 274251.9375 - val_mae: 432.2997\n",
            "Epoch 365/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 972.3425 - mse: 1262188.2500 - mae: 972.3426 - val_loss: 430.8613 - val_mse: 272774.1562 - val_mae: 430.8613\n",
            "Epoch 366/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 873.8014 - mse: 1069926.2500 - mae: 873.8014 - val_loss: 428.9310 - val_mse: 270851.6875 - val_mae: 428.9310\n",
            "Epoch 367/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 848.5497 - mse: 980499.8125 - mae: 848.5497 - val_loss: 426.3810 - val_mse: 268336.7500 - val_mae: 426.3810\n",
            "Epoch 368/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 947.7434 - mse: 1249658.7500 - mae: 947.7433 - val_loss: 425.2040 - val_mse: 267124.0000 - val_mae: 425.2040\n",
            "Epoch 369/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1000.6786 - mse: 1367655.6250 - mae: 1000.6785 - val_loss: 426.1695 - val_mse: 267910.9062 - val_mae: 426.1695\n",
            "Epoch 370/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 917.7733 - mse: 1162005.2500 - mae: 917.7733 - val_loss: 424.5662 - val_mse: 266314.1875 - val_mae: 424.5662\n",
            "Epoch 371/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 897.8658 - mse: 1257798.7500 - mae: 897.8658 - val_loss: 424.0885 - val_mse: 265785.1875 - val_mae: 424.0885\n",
            "Epoch 372/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1015.8809 - mse: 1379236.2500 - mae: 1015.8809 - val_loss: 425.1864 - val_mse: 266710.6875 - val_mae: 425.1864\n",
            "Epoch 373/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 922.1799 - mse: 1213869.2500 - mae: 922.1799 - val_loss: 423.6797 - val_mse: 265209.2188 - val_mae: 423.6797\n",
            "Epoch 374/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 902.2778 - mse: 1083515.2500 - mae: 902.2778 - val_loss: 423.0380 - val_mse: 264505.5938 - val_mae: 423.0380\n",
            "Epoch 375/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 917.1362 - mse: 1177253.6250 - mae: 917.1361 - val_loss: 421.5605 - val_mse: 263055.4062 - val_mae: 421.5605\n",
            "Epoch 376/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 983.7784 - mse: 1263026.2500 - mae: 983.7783 - val_loss: 420.1027 - val_mse: 261613.2031 - val_mae: 420.1027\n",
            "Epoch 377/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 920.5691 - mse: 1178015.6250 - mae: 920.5692 - val_loss: 417.4923 - val_mse: 259147.2969 - val_mae: 417.4923\n",
            "Epoch 378/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 985.2700 - mse: 1406006.6250 - mae: 985.2699 - val_loss: 415.7409 - val_mse: 257484.9531 - val_mae: 415.7409\n",
            "Epoch 379/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 901.9567 - mse: 1132817.5000 - mae: 901.9567 - val_loss: 413.8789 - val_mse: 255730.8438 - val_mae: 413.8789\n",
            "Epoch 380/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1085.9628 - mse: 1537138.1250 - mae: 1085.9626 - val_loss: 416.1520 - val_mse: 257683.7031 - val_mae: 416.1520\n",
            "Epoch 381/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1054.8673 - mse: 1392220.2500 - mae: 1054.8673 - val_loss: 417.7745 - val_mse: 259049.6719 - val_mae: 417.7745\n",
            "Epoch 382/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 928.0589 - mse: 1135876.2500 - mae: 928.0589 - val_loss: 417.7933 - val_mse: 258950.7500 - val_mae: 417.7933\n",
            "Epoch 383/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1009.2604 - mse: 1405786.3750 - mae: 1009.2604 - val_loss: 418.8309 - val_mse: 259813.0781 - val_mae: 418.8309\n",
            "Epoch 384/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 917.8737 - mse: 1227109.6250 - mae: 917.8737 - val_loss: 418.5966 - val_mse: 259522.9062 - val_mae: 418.5966\n",
            "Epoch 385/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 962.5397 - mse: 1430875.8750 - mae: 962.5397 - val_loss: 416.9325 - val_mse: 257947.4062 - val_mae: 416.9325\n",
            "Epoch 386/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 957.0182 - mse: 1297993.8750 - mae: 957.0182 - val_loss: 417.2198 - val_mse: 258110.7500 - val_mae: 417.2198\n",
            "Epoch 387/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 949.2805 - mse: 1197934.3750 - mae: 949.2806 - val_loss: 416.2896 - val_mse: 257169.5469 - val_mae: 416.2896\n",
            "Epoch 388/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 994.0298 - mse: 1508898.8750 - mae: 994.0299 - val_loss: 417.1527 - val_mse: 257886.4219 - val_mae: 417.1527\n",
            "Epoch 389/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 878.2157 - mse: 1178936.7500 - mae: 878.2157 - val_loss: 415.1979 - val_mse: 256043.5000 - val_mae: 415.1979\n",
            "Epoch 390/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 943.8553 - mse: 1137156.8750 - mae: 943.8553 - val_loss: 415.1797 - val_mse: 255912.7031 - val_mae: 415.1797\n",
            "Epoch 391/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 921.4840 - mse: 1085528.2500 - mae: 921.4840 - val_loss: 414.9419 - val_mse: 255583.1562 - val_mae: 414.9419\n",
            "Epoch 392/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 886.3518 - mse: 1279307.1250 - mae: 886.3517 - val_loss: 412.5413 - val_mse: 253360.2969 - val_mae: 412.5413\n",
            "Epoch 393/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1062.8120 - mse: 1432366.2500 - mae: 1062.8119 - val_loss: 412.9998 - val_mse: 253670.9062 - val_mae: 412.9998\n",
            "Epoch 394/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 949.7542 - mse: 1170574.6250 - mae: 949.7543 - val_loss: 413.0305 - val_mse: 253587.5000 - val_mae: 413.0305\n",
            "Epoch 395/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 998.2621 - mse: 1300365.5000 - mae: 998.2621 - val_loss: 413.8367 - val_mse: 254223.5938 - val_mae: 413.8367\n",
            "Epoch 396/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 947.4867 - mse: 1255607.3750 - mae: 947.4867 - val_loss: 413.9577 - val_mse: 254240.2031 - val_mae: 413.9577\n",
            "Epoch 397/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1000.3957 - mse: 1391359.0000 - mae: 1000.3957 - val_loss: 414.9118 - val_mse: 255007.9688 - val_mae: 414.9118\n",
            "Epoch 398/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1029.7310 - mse: 1367479.7500 - mae: 1029.7311 - val_loss: 416.0232 - val_mse: 255919.9531 - val_mae: 416.0232\n",
            "Epoch 399/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 874.4576 - mse: 1113509.1250 - mae: 874.4576 - val_loss: 412.1420 - val_mse: 252354.1250 - val_mae: 412.1420\n",
            "Epoch 400/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 963.6057 - mse: 1311979.5000 - mae: 963.6057 - val_loss: 412.4418 - val_mse: 252552.5469 - val_mae: 412.4418\n",
            "Epoch 401/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 937.3843 - mse: 1233882.6250 - mae: 937.3844 - val_loss: 412.4888 - val_mse: 252520.7500 - val_mae: 412.4888\n",
            "Epoch 402/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 934.6341 - mse: 1230864.6250 - mae: 934.6341 - val_loss: 409.9615 - val_mse: 250200.9062 - val_mae: 409.9615\n",
            "Epoch 403/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 786.0565 - mse: 893664.1875 - mae: 786.0565 - val_loss: 406.4955 - val_mse: 247046.5938 - val_mae: 406.4955\n",
            "Epoch 404/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 940.8062 - mse: 1218993.7500 - mae: 940.8063 - val_loss: 406.5803 - val_mse: 247034.1562 - val_mae: 406.5803\n",
            "Epoch 405/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1024.4423 - mse: 1496060.6250 - mae: 1024.4423 - val_loss: 406.6747 - val_mse: 247053.2500 - val_mae: 406.6747\n",
            "Epoch 406/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 940.3429 - mse: 1196170.6250 - mae: 940.3429 - val_loss: 405.5540 - val_mse: 245977.9219 - val_mae: 405.5540\n",
            "Epoch 407/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 957.6548 - mse: 1182043.6250 - mae: 957.6549 - val_loss: 405.8309 - val_mse: 246125.7031 - val_mae: 405.8309\n",
            "Epoch 408/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 957.3875 - mse: 1230504.0000 - mae: 957.3875 - val_loss: 406.1501 - val_mse: 246327.9062 - val_mae: 406.1501\n",
            "Epoch 409/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 852.4790 - mse: 987627.8125 - mae: 852.4790 - val_loss: 404.9942 - val_mse: 245214.9531 - val_mae: 404.9942\n",
            "Epoch 410/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 882.3019 - mse: 1197390.5000 - mae: 882.3019 - val_loss: 404.2005 - val_mse: 244453.5938 - val_mae: 404.2005\n",
            "Epoch 411/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 930.6159 - mse: 1167091.7500 - mae: 930.6158 - val_loss: 404.1124 - val_mse: 244286.0000 - val_mae: 404.1124\n",
            "Epoch 412/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 947.7104 - mse: 1262203.3750 - mae: 947.7104 - val_loss: 404.1486 - val_mse: 244229.8750 - val_mae: 404.1486\n",
            "Epoch 413/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1007.6772 - mse: 1402294.5000 - mae: 1007.6772 - val_loss: 403.8997 - val_mse: 243943.2969 - val_mae: 403.8997\n",
            "Epoch 414/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 837.4976 - mse: 1082934.0000 - mae: 837.4977 - val_loss: 401.6006 - val_mse: 241870.9219 - val_mae: 401.6006\n",
            "Epoch 415/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 907.5641 - mse: 987903.1875 - mae: 907.5641 - val_loss: 401.2576 - val_mse: 241468.5938 - val_mae: 401.2576\n",
            "Epoch 416/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 911.4944 - mse: 1241613.2500 - mae: 911.4944 - val_loss: 399.6687 - val_mse: 240026.0312 - val_mae: 399.6687\n",
            "Epoch 417/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 894.5927 - mse: 1185568.2500 - mae: 894.5927 - val_loss: 398.1235 - val_mse: 238636.0469 - val_mae: 398.1235\n",
            "Epoch 418/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1000.4104 - mse: 1264769.0000 - mae: 1000.4103 - val_loss: 399.0604 - val_mse: 239350.5000 - val_mae: 399.0604\n",
            "Epoch 419/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 922.7924 - mse: 1136777.3750 - mae: 922.7924 - val_loss: 397.5387 - val_mse: 237968.7031 - val_mae: 397.5387\n",
            "Epoch 420/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 961.3916 - mse: 1334737.6250 - mae: 961.3916 - val_loss: 397.8686 - val_mse: 238182.2812 - val_mae: 397.8686\n",
            "Epoch 421/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 872.0858 - mse: 960805.3750 - mae: 872.0858 - val_loss: 396.8894 - val_mse: 237241.7969 - val_mae: 396.8894\n",
            "Epoch 422/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1083.2664 - mse: 1423456.3750 - mae: 1083.2664 - val_loss: 398.9681 - val_mse: 238926.7969 - val_mae: 398.9681\n",
            "Epoch 423/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1073.7259 - mse: 1408606.2500 - mae: 1073.7258 - val_loss: 399.6071 - val_mse: 239391.1562 - val_mae: 399.6071\n",
            "Epoch 424/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 968.8162 - mse: 1158689.0000 - mae: 968.8161 - val_loss: 398.9029 - val_mse: 238687.4219 - val_mae: 398.9029\n",
            "Epoch 425/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1001.4474 - mse: 1275497.3750 - mae: 1001.4474 - val_loss: 399.6323 - val_mse: 239215.2500 - val_mae: 399.6323\n",
            "Epoch 426/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 944.9442 - mse: 1330761.0000 - mae: 944.9442 - val_loss: 398.5515 - val_mse: 238222.0000 - val_mae: 398.5515\n",
            "Epoch 427/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 948.7169 - mse: 1217896.8750 - mae: 948.7170 - val_loss: 397.4330 - val_mse: 237198.3438 - val_mae: 397.4330\n",
            "Epoch 428/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1035.7229 - mse: 1366553.0000 - mae: 1035.7229 - val_loss: 398.7192 - val_mse: 238221.1562 - val_mae: 398.7192\n",
            "Epoch 429/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 939.4140 - mse: 1236313.3750 - mae: 939.4141 - val_loss: 398.7837 - val_mse: 238206.4688 - val_mae: 398.7837\n",
            "Epoch 430/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 950.6489 - mse: 1186502.6250 - mae: 950.6488 - val_loss: 397.8784 - val_mse: 237336.4531 - val_mae: 397.8784\n",
            "Epoch 431/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 975.4852 - mse: 1360806.3750 - mae: 975.4852 - val_loss: 397.5042 - val_mse: 236952.5469 - val_mae: 397.5042\n",
            "Epoch 432/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 996.7910 - mse: 1441938.8750 - mae: 996.7910 - val_loss: 398.2549 - val_mse: 237531.2500 - val_mae: 398.2549\n",
            "Epoch 433/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 882.3025 - mse: 1146034.8750 - mae: 882.3024 - val_loss: 397.4555 - val_mse: 236774.7500 - val_mae: 397.4555\n",
            "Epoch 434/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 989.8179 - mse: 1321966.7500 - mae: 989.8180 - val_loss: 397.0788 - val_mse: 236389.7969 - val_mae: 397.0788\n",
            "Epoch 435/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 845.4420 - mse: 1122012.0000 - mae: 845.4420 - val_loss: 396.5067 - val_mse: 235829.6562 - val_mae: 396.5067\n",
            "Epoch 436/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 883.5601 - mse: 1105018.6250 - mae: 883.5602 - val_loss: 395.7639 - val_mse: 235119.0938 - val_mae: 395.7639\n",
            "Epoch 437/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 945.4252 - mse: 1180337.6250 - mae: 945.4252 - val_loss: 395.8879 - val_mse: 235130.3438 - val_mae: 395.8879\n",
            "Epoch 438/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 877.4522 - mse: 1113125.1250 - mae: 877.4521 - val_loss: 394.0042 - val_mse: 233470.9062 - val_mae: 394.0042\n",
            "Epoch 439/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1013.5915 - mse: 1381079.2500 - mae: 1013.5915 - val_loss: 392.7243 - val_mse: 232326.5312 - val_mae: 392.7243\n",
            "Epoch 440/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 948.1662 - mse: 1162463.7500 - mae: 948.1662 - val_loss: 392.8870 - val_mse: 232367.4219 - val_mae: 392.8870\n",
            "Epoch 441/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 839.9131 - mse: 1053230.0000 - mae: 839.9131 - val_loss: 389.2054 - val_mse: 229227.7031 - val_mae: 389.2054\n",
            "Epoch 442/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1022.7712 - mse: 1301609.7500 - mae: 1022.7713 - val_loss: 390.3163 - val_mse: 230073.5938 - val_mae: 390.3163\n",
            "Epoch 443/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 907.4398 - mse: 1179323.8750 - mae: 907.4398 - val_loss: 389.8703 - val_mse: 229621.7500 - val_mae: 389.8703\n",
            "Epoch 444/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 957.5968 - mse: 1181284.7500 - mae: 957.5968 - val_loss: 390.1470 - val_mse: 229769.3438 - val_mae: 390.1470\n",
            "Epoch 445/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 956.3979 - mse: 1168165.2500 - mae: 956.3979 - val_loss: 390.2916 - val_mse: 229810.4062 - val_mae: 390.2916\n",
            "Epoch 446/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1009.8468 - mse: 1373860.7500 - mae: 1009.8468 - val_loss: 391.2842 - val_mse: 230567.8750 - val_mae: 391.2842\n",
            "Epoch 447/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 882.1642 - mse: 1086320.6250 - mae: 882.1642 - val_loss: 388.1373 - val_mse: 227878.2031 - val_mae: 388.1373\n",
            "Epoch 448/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 992.5230 - mse: 1264834.3750 - mae: 992.5231 - val_loss: 387.8497 - val_mse: 227563.3750 - val_mae: 387.8497\n",
            "Epoch 449/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1004.2573 - mse: 1334885.0000 - mae: 1004.2573 - val_loss: 387.6808 - val_mse: 227349.9062 - val_mae: 387.6808\n",
            "Epoch 450/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 936.4448 - mse: 1276356.6250 - mae: 936.4448 - val_loss: 387.7486 - val_mse: 227342.7031 - val_mae: 387.7486\n",
            "Epoch 451/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 980.8122 - mse: 1269780.6250 - mae: 980.8122 - val_loss: 388.2646 - val_mse: 227699.7188 - val_mae: 388.2646\n",
            "Epoch 452/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 979.8620 - mse: 1343289.7500 - mae: 979.8619 - val_loss: 388.7906 - val_mse: 228071.4531 - val_mae: 388.7906\n",
            "Epoch 453/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 921.4942 - mse: 1201203.2500 - mae: 921.4943 - val_loss: 387.6756 - val_mse: 227085.5938 - val_mae: 387.6756\n",
            "Epoch 454/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 946.1029 - mse: 1333496.6250 - mae: 946.1029 - val_loss: 386.7412 - val_mse: 226267.4531 - val_mae: 386.7412\n",
            "Epoch 455/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1023.4402 - mse: 1415529.6250 - mae: 1023.4401 - val_loss: 387.9253 - val_mse: 227177.2969 - val_mae: 387.9253\n",
            "Epoch 456/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 979.3387 - mse: 1338429.3750 - mae: 979.3387 - val_loss: 387.3305 - val_mse: 226630.2031 - val_mae: 387.3305\n",
            "Epoch 457/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 962.2397 - mse: 1252462.3750 - mae: 962.2397 - val_loss: 386.6844 - val_mse: 226033.1562 - val_mae: 386.6844\n",
            "Epoch 458/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1017.8640 - mse: 1404616.8750 - mae: 1017.8641 - val_loss: 387.8138 - val_mse: 226897.5000 - val_mae: 387.8138\n",
            "Epoch 459/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 973.7880 - mse: 1286664.2500 - mae: 973.7880 - val_loss: 388.2017 - val_mse: 227154.9062 - val_mae: 388.2017\n",
            "Epoch 460/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 848.6852 - mse: 991960.1250 - mae: 848.6852 - val_loss: 386.4316 - val_mse: 225609.2969 - val_mae: 386.4316\n",
            "Epoch 461/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 995.0141 - mse: 1253147.3750 - mae: 995.0140 - val_loss: 387.1333 - val_mse: 226109.4219 - val_mae: 387.1333\n",
            "Epoch 462/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1037.4393 - mse: 1464568.7500 - mae: 1037.4392 - val_loss: 388.5230 - val_mse: 227204.9219 - val_mae: 388.5230\n",
            "Epoch 463/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 900.8390 - mse: 1030522.3750 - mae: 900.8390 - val_loss: 387.9356 - val_mse: 226631.5938 - val_mae: 387.9356\n",
            "Epoch 464/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 883.6236 - mse: 1020088.1875 - mae: 883.6237 - val_loss: 387.2667 - val_mse: 226003.7969 - val_mae: 387.2667\n",
            "Epoch 465/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 929.8498 - mse: 1206281.8750 - mae: 929.8498 - val_loss: 387.1924 - val_mse: 225874.7031 - val_mae: 387.1924\n",
            "Epoch 466/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 885.8263 - mse: 1086938.3750 - mae: 885.8263 - val_loss: 386.5651 - val_mse: 225279.5938 - val_mae: 386.5651\n",
            "Epoch 467/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1064.8870 - mse: 1427181.2500 - mae: 1064.8870 - val_loss: 387.2473 - val_mse: 225778.2500 - val_mae: 387.2473\n",
            "Epoch 468/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 961.1105 - mse: 1148816.7500 - mae: 961.1105 - val_loss: 386.3184 - val_mse: 224941.4531 - val_mae: 386.3184\n",
            "Epoch 469/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 915.4453 - mse: 1182685.0000 - mae: 915.4453 - val_loss: 383.7896 - val_mse: 222809.1250 - val_mae: 383.7896\n",
            "Epoch 470/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 878.1311 - mse: 1026230.3125 - mae: 878.1312 - val_loss: 381.8069 - val_mse: 221121.5469 - val_mae: 381.8069\n",
            "Epoch 471/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 847.2514 - mse: 1157083.1250 - mae: 847.2513 - val_loss: 379.6095 - val_mse: 219300.0000 - val_mae: 379.6095\n",
            "Epoch 472/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 927.3031 - mse: 1084380.8750 - mae: 927.3031 - val_loss: 379.5433 - val_mse: 219154.8750 - val_mae: 379.5433\n",
            "Epoch 473/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 958.0613 - mse: 1260340.8750 - mae: 958.0613 - val_loss: 378.5758 - val_mse: 218315.7031 - val_mae: 378.5758\n",
            "Epoch 474/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1036.0055 - mse: 1518983.2500 - mae: 1036.0055 - val_loss: 378.9806 - val_mse: 218587.9062 - val_mae: 378.9806\n",
            "Epoch 475/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 931.7618 - mse: 1356877.7500 - mae: 931.7618 - val_loss: 379.0484 - val_mse: 218590.5781 - val_mae: 379.0484\n",
            "Epoch 476/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 923.6909 - mse: 1173453.2500 - mae: 923.6908 - val_loss: 377.7013 - val_mse: 217455.7812 - val_mae: 377.7013\n",
            "Epoch 477/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 944.1162 - mse: 1220876.6250 - mae: 944.1162 - val_loss: 374.6590 - val_mse: 214986.9688 - val_mae: 374.6590\n",
            "Epoch 478/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 999.8333 - mse: 1288867.8750 - mae: 999.8333 - val_loss: 375.5320 - val_mse: 215603.5938 - val_mae: 375.5320\n",
            "Epoch 479/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 993.0160 - mse: 1429630.7500 - mae: 993.0160 - val_loss: 375.1436 - val_mse: 215236.9062 - val_mae: 375.1436\n",
            "Epoch 480/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 917.6238 - mse: 1158352.7500 - mae: 917.6238 - val_loss: 375.0083 - val_mse: 215065.0781 - val_mae: 375.0083\n",
            "Epoch 481/500\n",
            "20/20 [==============================] - 0s 955us/step - loss: 1053.2698 - mse: 1414230.0000 - mae: 1053.2698 - val_loss: 376.5512 - val_mse: 216219.2500 - val_mae: 376.5512\n",
            "Epoch 482/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1007.8164 - mse: 1362483.0000 - mae: 1007.8164 - val_loss: 376.9192 - val_mse: 216446.9219 - val_mae: 376.9192\n",
            "Epoch 483/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 941.3739 - mse: 1155893.0000 - mae: 941.3739 - val_loss: 375.7331 - val_mse: 215434.2188 - val_mae: 375.7331\n",
            "Epoch 484/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 940.9858 - mse: 1196280.5000 - mae: 940.9857 - val_loss: 375.7856 - val_mse: 215415.8750 - val_mae: 375.7856\n",
            "Epoch 485/500\n",
            "20/20 [==============================] - 0s 989us/step - loss: 1083.8307 - mse: 1469910.6250 - mae: 1083.8307 - val_loss: 377.8673 - val_mse: 217010.7188 - val_mae: 377.8673\n",
            "Epoch 486/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 881.1331 - mse: 1032069.3750 - mae: 881.1331 - val_loss: 377.2923 - val_mse: 216477.4531 - val_mae: 377.2923\n",
            "Epoch 487/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 924.1187 - mse: 1198566.7500 - mae: 924.1188 - val_loss: 375.9806 - val_mse: 215359.7188 - val_mae: 375.9806\n",
            "Epoch 488/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 880.3145 - mse: 1003441.3750 - mae: 880.3145 - val_loss: 375.4428 - val_mse: 214860.0938 - val_mae: 375.4428\n",
            "Epoch 489/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 831.7137 - mse: 940338.3125 - mae: 831.7137 - val_loss: 374.0106 - val_mse: 213651.7500 - val_mae: 374.0106\n",
            "Epoch 490/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 932.9686 - mse: 1188000.0000 - mae: 932.9686 - val_loss: 374.0458 - val_mse: 213618.5938 - val_mae: 374.0458\n",
            "Epoch 491/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 982.2972 - mse: 1238711.2500 - mae: 982.2972 - val_loss: 374.6534 - val_mse: 214020.3281 - val_mae: 374.6534\n",
            "Epoch 492/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 961.4581 - mse: 1303294.2500 - mae: 961.4580 - val_loss: 375.0404 - val_mse: 214268.8750 - val_mae: 375.0404\n",
            "Epoch 493/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 937.2286 - mse: 1218388.3750 - mae: 937.2286 - val_loss: 375.0502 - val_mse: 214225.9531 - val_mae: 375.0502\n",
            "Epoch 494/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1002.3593 - mse: 1246416.7500 - mae: 1002.3593 - val_loss: 375.8340 - val_mse: 214783.2031 - val_mae: 375.8340\n",
            "Epoch 495/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 970.1667 - mse: 1244398.2500 - mae: 970.1668 - val_loss: 375.3166 - val_mse: 214317.7812 - val_mae: 375.3166\n",
            "Epoch 496/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 896.9717 - mse: 1118352.2500 - mae: 896.9717 - val_loss: 373.8139 - val_mse: 213069.0469 - val_mae: 373.8139\n",
            "Epoch 497/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 977.2881 - mse: 1365617.3750 - mae: 977.2881 - val_loss: 372.0449 - val_mse: 211636.8281 - val_mae: 372.0449\n",
            "Epoch 498/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 995.0854 - mse: 1240937.3750 - mae: 995.0853 - val_loss: 371.6265 - val_mse: 211249.4688 - val_mae: 371.6265\n",
            "Epoch 499/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 931.6527 - mse: 1098727.2500 - mae: 931.6527 - val_loss: 370.5301 - val_mse: 210324.7812 - val_mae: 370.5301\n",
            "Epoch 500/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1009.8274 - mse: 1437839.1250 - mae: 1009.8273 - val_loss: 370.2624 - val_mse: 210068.8281 - val_mae: 370.2624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 30 samples, validate on 30 samples\n",
            "Epoch 1/500\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 3837.6970 - mse: 26728818.0000 - mae: 3837.6968 - val_loss: 6691.3388 - val_mse: 74243672.0000 - val_mae: 6691.3384\n",
            "Epoch 2/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3837.6564 - mse: 26728490.0000 - mae: 3837.6562 - val_loss: 6691.2957 - val_mse: 74243104.0000 - val_mae: 6691.2959\n",
            "Epoch 3/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3837.6070 - mse: 26728082.0000 - mae: 3837.6069 - val_loss: 6691.2022 - val_mse: 74242032.0000 - val_mae: 6691.2021\n",
            "Epoch 4/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3837.4816 - mse: 26727412.0000 - mae: 3837.4812 - val_loss: 6690.8032 - val_mse: 74238768.0000 - val_mae: 6690.8032\n",
            "Epoch 5/500\n",
            "30/30 [==============================] - 0s 974us/step - loss: 3836.7172 - mse: 26724126.0000 - mae: 3836.7173 - val_loss: 6689.0954 - val_mse: 74226640.0000 - val_mae: 6689.0952\n",
            "Epoch 6/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3834.5213 - mse: 26716234.0000 - mae: 3834.5212 - val_loss: 6685.9488 - val_mse: 74203600.0000 - val_mae: 6685.9487\n",
            "Epoch 7/500\n",
            "30/30 [==============================] - 0s 972us/step - loss: 3831.8602 - mse: 26705010.0000 - mae: 3831.8601 - val_loss: 6682.1249 - val_mse: 74172752.0000 - val_mae: 6682.1250\n",
            "Epoch 8/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3825.9264 - mse: 26685012.0000 - mae: 3825.9265 - val_loss: 6677.3252 - val_mse: 74131176.0000 - val_mae: 6677.3252\n",
            "Epoch 9/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3822.6366 - mse: 26661860.0000 - mae: 3822.6367 - val_loss: 6672.4013 - val_mse: 74080208.0000 - val_mae: 6672.4014\n",
            "Epoch 10/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3817.1978 - mse: 26637242.0000 - mae: 3817.1980 - val_loss: 6666.6870 - val_mse: 74021808.0000 - val_mae: 6666.6870\n",
            "Epoch 11/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3810.3761 - mse: 26591076.0000 - mae: 3810.3760 - val_loss: 6659.6830 - val_mse: 73945992.0000 - val_mae: 6659.6831\n",
            "Epoch 12/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3801.5984 - mse: 26540906.0000 - mae: 3801.5981 - val_loss: 6651.8972 - val_mse: 73862144.0000 - val_mae: 6651.8970\n",
            "Epoch 13/500\n",
            "30/30 [==============================] - 0s 919us/step - loss: 3799.5423 - mse: 26499636.0000 - mae: 3799.5422 - val_loss: 6644.0478 - val_mse: 73775008.0000 - val_mae: 6644.0479\n",
            "Epoch 14/500\n",
            "30/30 [==============================] - 0s 944us/step - loss: 3781.9084 - mse: 26410766.0000 - mae: 3781.9087 - val_loss: 6633.9646 - val_mse: 73666392.0000 - val_mae: 6633.9644\n",
            "Epoch 15/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3772.1262 - mse: 26338876.0000 - mae: 3772.1260 - val_loss: 6623.2923 - val_mse: 73546992.0000 - val_mae: 6623.2925\n",
            "Epoch 16/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3761.8072 - mse: 26266120.0000 - mae: 3761.8074 - val_loss: 6611.6724 - val_mse: 73415824.0000 - val_mae: 6611.6724\n",
            "Epoch 17/500\n",
            "30/30 [==============================] - 0s 933us/step - loss: 3752.8852 - mse: 26208714.0000 - mae: 3752.8855 - val_loss: 6599.6819 - val_mse: 73282072.0000 - val_mae: 6599.6816\n",
            "Epoch 18/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3739.5803 - mse: 26117556.0000 - mae: 3739.5803 - val_loss: 6586.9395 - val_mse: 73136744.0000 - val_mae: 6586.9395\n",
            "Epoch 19/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 3727.3867 - mse: 26029700.0000 - mae: 3727.3867 - val_loss: 6573.3925 - val_mse: 72979480.0000 - val_mae: 6573.3926\n",
            "Epoch 20/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3705.2601 - mse: 25865858.0000 - mae: 3705.2600 - val_loss: 6558.5182 - val_mse: 72804808.0000 - val_mae: 6558.5181\n",
            "Epoch 21/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3708.9356 - mse: 25909944.0000 - mae: 3708.9353 - val_loss: 6544.6593 - val_mse: 72644624.0000 - val_mae: 6544.6592\n",
            "Epoch 22/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3688.6433 - mse: 25770220.0000 - mae: 3688.6433 - val_loss: 6529.5522 - val_mse: 72467896.0000 - val_mae: 6529.5522\n",
            "Epoch 23/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3653.3822 - mse: 25501220.0000 - mae: 3653.3823 - val_loss: 6511.8856 - val_mse: 72256544.0000 - val_mae: 6511.8853\n",
            "Epoch 24/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3648.1822 - mse: 25448644.0000 - mae: 3648.1821 - val_loss: 6494.7709 - val_mse: 72052408.0000 - val_mae: 6494.7710\n",
            "Epoch 25/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3622.4790 - mse: 25271544.0000 - mae: 3622.4790 - val_loss: 6476.5129 - val_mse: 71835648.0000 - val_mae: 6476.5132\n",
            "Epoch 26/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3604.1598 - mse: 25199718.0000 - mae: 3604.1599 - val_loss: 6457.6094 - val_mse: 71610784.0000 - val_mae: 6457.6094\n",
            "Epoch 27/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3604.0624 - mse: 25125238.0000 - mae: 3604.0625 - val_loss: 6439.5274 - val_mse: 71392016.0000 - val_mae: 6439.5278\n",
            "Epoch 28/500\n",
            "30/30 [==============================] - 0s 992us/step - loss: 3561.2276 - mse: 24924270.0000 - mae: 3561.2275 - val_loss: 6419.2219 - val_mse: 71148128.0000 - val_mae: 6419.2217\n",
            "Epoch 29/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3570.4089 - mse: 24996386.0000 - mae: 3570.4089 - val_loss: 6400.1018 - val_mse: 70918232.0000 - val_mae: 6400.1016\n",
            "Epoch 30/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3543.1279 - mse: 24792174.0000 - mae: 3543.1279 - val_loss: 6383.1401 - val_mse: 70708672.0000 - val_mae: 6383.1401\n",
            "Epoch 31/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3531.5022 - mse: 24642802.0000 - mae: 3531.5024 - val_loss: 6362.4506 - val_mse: 70458888.0000 - val_mae: 6362.4507\n",
            "Epoch 32/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3488.5128 - mse: 24307974.0000 - mae: 3488.5127 - val_loss: 6342.8804 - val_mse: 70219464.0000 - val_mae: 6342.8809\n",
            "Epoch 33/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3486.1365 - mse: 24423128.0000 - mae: 3486.1365 - val_loss: 6325.0665 - val_mse: 69999336.0000 - val_mae: 6325.0669\n",
            "Epoch 34/500\n",
            "30/30 [==============================] - 0s 961us/step - loss: 3462.4667 - mse: 24177314.0000 - mae: 3462.4666 - val_loss: 6304.5047 - val_mse: 69747952.0000 - val_mae: 6304.5049\n",
            "Epoch 35/500\n",
            "30/30 [==============================] - 0s 957us/step - loss: 3432.0194 - mse: 23916374.0000 - mae: 3432.0193 - val_loss: 6283.9414 - val_mse: 69494400.0000 - val_mae: 6283.9419\n",
            "Epoch 36/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3482.9383 - mse: 24262844.0000 - mae: 3482.9382 - val_loss: 6270.0941 - val_mse: 69315424.0000 - val_mae: 6270.0938\n",
            "Epoch 37/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3428.2799 - mse: 23568540.0000 - mae: 3428.2798 - val_loss: 6252.1910 - val_mse: 69091272.0000 - val_mae: 6252.1909\n",
            "Epoch 38/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3397.6056 - mse: 23462476.0000 - mae: 3397.6057 - val_loss: 6232.0314 - val_mse: 68839984.0000 - val_mae: 6232.0312\n",
            "Epoch 39/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3402.0015 - mse: 23461678.0000 - mae: 3402.0015 - val_loss: 6214.3258 - val_mse: 68619352.0000 - val_mae: 6214.3257\n",
            "Epoch 40/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3406.1745 - mse: 23435940.0000 - mae: 3406.1746 - val_loss: 6194.9518 - val_mse: 68380400.0000 - val_mae: 6194.9521\n",
            "Epoch 41/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3393.3651 - mse: 23573248.0000 - mae: 3393.3650 - val_loss: 6175.1732 - val_mse: 68140072.0000 - val_mae: 6175.1729\n",
            "Epoch 42/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3378.0275 - mse: 23444142.0000 - mae: 3378.0273 - val_loss: 6155.3195 - val_mse: 67898544.0000 - val_mae: 6155.3198\n",
            "Epoch 43/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3327.0492 - mse: 23374042.0000 - mae: 3327.0491 - val_loss: 6134.3658 - val_mse: 67644048.0000 - val_mae: 6134.3657\n",
            "Epoch 44/500\n",
            "30/30 [==============================] - 0s 937us/step - loss: 3363.6643 - mse: 23270204.0000 - mae: 3363.6643 - val_loss: 6113.6000 - val_mse: 67393792.0000 - val_mae: 6113.6001\n",
            "Epoch 45/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3276.4718 - mse: 22373702.0000 - mae: 3276.4719 - val_loss: 6092.7748 - val_mse: 67135328.0000 - val_mae: 6092.7749\n",
            "Epoch 46/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3354.4341 - mse: 22769442.0000 - mae: 3354.4341 - val_loss: 6073.7530 - val_mse: 66901588.0000 - val_mae: 6073.7529\n",
            "Epoch 47/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3307.5842 - mse: 22546194.0000 - mae: 3307.5845 - val_loss: 6055.3848 - val_mse: 66675900.0000 - val_mae: 6055.3848\n",
            "Epoch 48/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3313.2866 - mse: 22538808.0000 - mae: 3313.2866 - val_loss: 6037.1684 - val_mse: 66452488.0000 - val_mae: 6037.1689\n",
            "Epoch 49/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3304.6583 - mse: 22419882.0000 - mae: 3304.6580 - val_loss: 6016.2168 - val_mse: 66199044.0000 - val_mae: 6016.2168\n",
            "Epoch 50/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3290.7816 - mse: 22176006.0000 - mae: 3290.7817 - val_loss: 5996.6880 - val_mse: 65957956.0000 - val_mae: 5996.6880\n",
            "Epoch 51/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3237.5966 - mse: 21791060.0000 - mae: 3237.5967 - val_loss: 5978.1569 - val_mse: 65729704.0000 - val_mae: 5978.1567\n",
            "Epoch 52/500\n",
            "30/30 [==============================] - 0s 995us/step - loss: 3208.7738 - mse: 21779486.0000 - mae: 3208.7739 - val_loss: 5953.9582 - val_mse: 65438868.0000 - val_mae: 5953.9585\n",
            "Epoch 53/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3213.9937 - mse: 21519476.0000 - mae: 3213.9937 - val_loss: 5932.2953 - val_mse: 65179156.0000 - val_mae: 5932.2954\n",
            "Epoch 54/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3200.8781 - mse: 21286186.0000 - mae: 3200.8782 - val_loss: 5912.6347 - val_mse: 64940976.0000 - val_mae: 5912.6343\n",
            "Epoch 55/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3194.2196 - mse: 21579406.0000 - mae: 3194.2195 - val_loss: 5894.1003 - val_mse: 64716052.0000 - val_mae: 5894.1001\n",
            "Epoch 56/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3195.3088 - mse: 21185994.0000 - mae: 3195.3088 - val_loss: 5871.8881 - val_mse: 64450860.0000 - val_mae: 5871.8882\n",
            "Epoch 57/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3231.1850 - mse: 21628130.0000 - mae: 3231.1848 - val_loss: 5855.2720 - val_mse: 64250676.0000 - val_mae: 5855.2720\n",
            "Epoch 58/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3224.0964 - mse: 21231810.0000 - mae: 3224.0964 - val_loss: 5837.2002 - val_mse: 64035056.0000 - val_mae: 5837.2002\n",
            "Epoch 59/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3264.1823 - mse: 21416718.0000 - mae: 3264.1824 - val_loss: 5820.3477 - val_mse: 63834444.0000 - val_mae: 5820.3481\n",
            "Epoch 60/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3197.9810 - mse: 21296454.0000 - mae: 3197.9807 - val_loss: 5802.9459 - val_mse: 63627672.0000 - val_mae: 5802.9458\n",
            "Epoch 61/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3202.4520 - mse: 21350330.0000 - mae: 3202.4521 - val_loss: 5790.6400 - val_mse: 63478060.0000 - val_mae: 5790.6401\n",
            "Epoch 62/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3101.6466 - mse: 20381234.0000 - mae: 3101.6467 - val_loss: 5769.1273 - val_mse: 63226096.0000 - val_mae: 5769.1270\n",
            "Epoch 63/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3122.4372 - mse: 20504954.0000 - mae: 3122.4373 - val_loss: 5750.6621 - val_mse: 63007908.0000 - val_mae: 5750.6621\n",
            "Epoch 64/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3115.3302 - mse: 20253154.0000 - mae: 3115.3303 - val_loss: 5730.6035 - val_mse: 62765440.0000 - val_mae: 5730.6035\n",
            "Epoch 65/500\n",
            "30/30 [==============================] - 0s 967us/step - loss: 3089.8578 - mse: 19971510.0000 - mae: 3089.8579 - val_loss: 5712.7453 - val_mse: 62541680.0000 - val_mae: 5712.7451\n",
            "Epoch 66/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3139.8371 - mse: 20368344.0000 - mae: 3139.8369 - val_loss: 5695.2226 - val_mse: 62323304.0000 - val_mae: 5695.2231\n",
            "Epoch 67/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3118.5680 - mse: 19714600.0000 - mae: 3118.5679 - val_loss: 5678.4471 - val_mse: 62113044.0000 - val_mae: 5678.4468\n",
            "Epoch 68/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3175.9250 - mse: 20536786.0000 - mae: 3175.9250 - val_loss: 5666.4420 - val_mse: 61960632.0000 - val_mae: 5666.4424\n",
            "Epoch 69/500\n",
            "30/30 [==============================] - 0s 956us/step - loss: 3111.8247 - mse: 19732398.0000 - mae: 3111.8247 - val_loss: 5651.8304 - val_mse: 61778228.0000 - val_mae: 5651.8301\n",
            "Epoch 70/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3096.3078 - mse: 19534478.0000 - mae: 3096.3079 - val_loss: 5636.8805 - val_mse: 61590872.0000 - val_mae: 5636.8809\n",
            "Epoch 71/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3122.9392 - mse: 19343100.0000 - mae: 3122.9390 - val_loss: 5622.8916 - val_mse: 61414996.0000 - val_mae: 5622.8916\n",
            "Epoch 72/500\n",
            "30/30 [==============================] - 0s 979us/step - loss: 3136.6152 - mse: 19905096.0000 - mae: 3136.6150 - val_loss: 5607.6984 - val_mse: 61227700.0000 - val_mae: 5607.6982\n",
            "Epoch 73/500\n",
            "30/30 [==============================] - 0s 899us/step - loss: 3093.7439 - mse: 19543390.0000 - mae: 3093.7437 - val_loss: 5593.1874 - val_mse: 61048004.0000 - val_mae: 5593.1875\n",
            "Epoch 74/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2946.8066 - mse: 18438690.0000 - mae: 2946.8064 - val_loss: 5571.0168 - val_mse: 60779504.0000 - val_mae: 5571.0166\n",
            "Epoch 75/500\n",
            "30/30 [==============================] - 0s 979us/step - loss: 3013.4563 - mse: 19182262.0000 - mae: 3013.4563 - val_loss: 5554.0610 - val_mse: 60566452.0000 - val_mae: 5554.0610\n",
            "Epoch 76/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3168.2934 - mse: 20498094.0000 - mae: 3168.2932 - val_loss: 5543.2065 - val_mse: 60424228.0000 - val_mae: 5543.2061\n",
            "Epoch 77/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 3089.8134 - mse: 19331286.0000 - mae: 3089.8135 - val_loss: 5529.6608 - val_mse: 60246132.0000 - val_mae: 5529.6606\n",
            "Epoch 78/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3035.4078 - mse: 18954654.0000 - mae: 3035.4077 - val_loss: 5514.4980 - val_mse: 60048968.0000 - val_mae: 5514.4980\n",
            "Epoch 79/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3118.9740 - mse: 19894086.0000 - mae: 3118.9739 - val_loss: 5502.3482 - val_mse: 59890696.0000 - val_mae: 5502.3481\n",
            "Epoch 80/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3210.8655 - mse: 19967606.0000 - mae: 3210.8657 - val_loss: 5493.6458 - val_mse: 59775800.0000 - val_mae: 5493.6460\n",
            "Epoch 81/500\n",
            "30/30 [==============================] - 0s 998us/step - loss: 3082.5327 - mse: 18343414.0000 - mae: 3082.5325 - val_loss: 5480.2551 - val_mse: 59601928.0000 - val_mae: 5480.2554\n",
            "Epoch 82/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3048.3402 - mse: 19024028.0000 - mae: 3048.3401 - val_loss: 5466.0259 - val_mse: 59419540.0000 - val_mae: 5466.0259\n",
            "Epoch 83/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3040.4585 - mse: 18822532.0000 - mae: 3040.4583 - val_loss: 5451.5808 - val_mse: 59235012.0000 - val_mae: 5451.5806\n",
            "Epoch 84/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3043.4500 - mse: 19403762.0000 - mae: 3043.4500 - val_loss: 5437.1373 - val_mse: 59052140.0000 - val_mae: 5437.1377\n",
            "Epoch 85/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3086.7974 - mse: 18860626.0000 - mae: 3086.7974 - val_loss: 5421.9794 - val_mse: 58859716.0000 - val_mae: 5421.9795\n",
            "Epoch 86/500\n",
            "30/30 [==============================] - 0s 983us/step - loss: 3124.2375 - mse: 19697088.0000 - mae: 3124.2375 - val_loss: 5411.2650 - val_mse: 58715044.0000 - val_mae: 5411.2651\n",
            "Epoch 87/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2984.7077 - mse: 18329566.0000 - mae: 2984.7075 - val_loss: 5396.3391 - val_mse: 58512688.0000 - val_mae: 5396.3389\n",
            "Epoch 88/500\n",
            "30/30 [==============================] - 0s 955us/step - loss: 2987.6348 - mse: 18340026.0000 - mae: 2987.6345 - val_loss: 5380.0788 - val_mse: 58293428.0000 - val_mae: 5380.0786\n",
            "Epoch 89/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3125.0840 - mse: 19111650.0000 - mae: 3125.0837 - val_loss: 5369.6889 - val_mse: 58151740.0000 - val_mae: 5369.6890\n",
            "Epoch 90/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2889.3305 - mse: 17687398.0000 - mae: 2889.3306 - val_loss: 5354.9747 - val_mse: 57918744.0000 - val_mae: 5354.9746\n",
            "Epoch 91/500\n",
            "30/30 [==============================] - 0s 1000us/step - loss: 2997.8612 - mse: 18294966.0000 - mae: 2997.8611 - val_loss: 5346.4559 - val_mse: 57780788.0000 - val_mae: 5346.4556\n",
            "Epoch 92/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2967.7117 - mse: 17562932.0000 - mae: 2967.7117 - val_loss: 5335.4250 - val_mse: 57583204.0000 - val_mae: 5335.4248\n",
            "Epoch 93/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3089.1745 - mse: 18291956.0000 - mae: 3089.1746 - val_loss: 5330.5977 - val_mse: 57485844.0000 - val_mae: 5330.5981\n",
            "Epoch 94/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3005.0661 - mse: 17768482.0000 - mae: 3005.0662 - val_loss: 5321.1776 - val_mse: 57299140.0000 - val_mae: 5321.1777\n",
            "Epoch 95/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2942.9357 - mse: 18501416.0000 - mae: 2942.9353 - val_loss: 5312.7918 - val_mse: 57133320.0000 - val_mae: 5312.7915\n",
            "Epoch 96/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2914.7634 - mse: 18171174.0000 - mae: 2914.7634 - val_loss: 5302.3991 - val_mse: 56924596.0000 - val_mae: 5302.3989\n",
            "Epoch 97/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2955.6396 - mse: 17626358.0000 - mae: 2955.6396 - val_loss: 5296.0324 - val_mse: 56781960.0000 - val_mae: 5296.0322\n",
            "Epoch 98/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3038.5352 - mse: 17547372.0000 - mae: 3038.5354 - val_loss: 5288.6798 - val_mse: 56617640.0000 - val_mae: 5288.6797\n",
            "Epoch 99/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2915.9051 - mse: 17250856.0000 - mae: 2915.9053 - val_loss: 5278.5477 - val_mse: 56392644.0000 - val_mae: 5278.5479\n",
            "Epoch 100/500\n",
            "30/30 [==============================] - 0s 997us/step - loss: 3035.1678 - mse: 17967070.0000 - mae: 3035.1677 - val_loss: 5271.0012 - val_mse: 56225232.0000 - val_mae: 5271.0010\n",
            "Epoch 101/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2892.8652 - mse: 18442210.0000 - mae: 2892.8650 - val_loss: 5263.5645 - val_mse: 56061140.0000 - val_mae: 5263.5645\n",
            "Epoch 102/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3019.3690 - mse: 17240882.0000 - mae: 3019.3687 - val_loss: 5258.2487 - val_mse: 55943544.0000 - val_mae: 5258.2490\n",
            "Epoch 103/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2906.1141 - mse: 17191190.0000 - mae: 2906.1140 - val_loss: 5251.5067 - val_mse: 55795328.0000 - val_mae: 5251.5068\n",
            "Epoch 104/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2942.4523 - mse: 17044014.0000 - mae: 2942.4524 - val_loss: 5247.3139 - val_mse: 55702524.0000 - val_mae: 5247.3140\n",
            "Epoch 105/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2897.1533 - mse: 17283538.0000 - mae: 2897.1531 - val_loss: 5240.3793 - val_mse: 55550744.0000 - val_mae: 5240.3794\n",
            "Epoch 106/500\n",
            "30/30 [==============================] - 0s 981us/step - loss: 2870.4459 - mse: 16540291.0000 - mae: 2870.4458 - val_loss: 5230.6075 - val_mse: 55337932.0000 - val_mae: 5230.6074\n",
            "Epoch 107/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2854.7620 - mse: 16275015.0000 - mae: 2854.7620 - val_loss: 5220.5716 - val_mse: 55120404.0000 - val_mae: 5220.5713\n",
            "Epoch 108/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2932.8753 - mse: 16958954.0000 - mae: 2932.8755 - val_loss: 5211.9688 - val_mse: 54934472.0000 - val_mae: 5211.9688\n",
            "Epoch 109/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2843.9197 - mse: 16239665.0000 - mae: 2843.9197 - val_loss: 5204.3823 - val_mse: 54770364.0000 - val_mae: 5204.3823\n",
            "Epoch 110/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 2904.4947 - mse: 16485590.0000 - mae: 2904.4949 - val_loss: 5195.7577 - val_mse: 54578396.0000 - val_mae: 5195.7578\n",
            "Epoch 111/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2945.0701 - mse: 16309939.0000 - mae: 2945.0701 - val_loss: 5190.2194 - val_mse: 54440728.0000 - val_mae: 5190.2192\n",
            "Epoch 112/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2855.2891 - mse: 16618245.0000 - mae: 2855.2891 - val_loss: 5181.2830 - val_mse: 54219132.0000 - val_mae: 5181.2827\n",
            "Epoch 113/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2952.2569 - mse: 16274992.0000 - mae: 2952.2568 - val_loss: 5174.1465 - val_mse: 54043036.0000 - val_mae: 5174.1465\n",
            "Epoch 114/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2802.3730 - mse: 16042349.0000 - mae: 2802.3728 - val_loss: 5171.0169 - val_mse: 53966376.0000 - val_mae: 5171.0166\n",
            "Epoch 115/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2701.4976 - mse: 15989268.0000 - mae: 2701.4973 - val_loss: 5164.5490 - val_mse: 53807624.0000 - val_mae: 5164.5488\n",
            "Epoch 116/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2820.2078 - mse: 16025194.0000 - mae: 2820.2078 - val_loss: 5157.7000 - val_mse: 53640036.0000 - val_mae: 5157.7002\n",
            "Epoch 117/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2730.7000 - mse: 15493646.0000 - mae: 2730.7000 - val_loss: 5147.9395 - val_mse: 53397136.0000 - val_mae: 5147.9395\n",
            "Epoch 118/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2968.4598 - mse: 16739107.0000 - mae: 2968.4600 - val_loss: 5143.9025 - val_mse: 53281128.0000 - val_mae: 5143.9028\n",
            "Epoch 119/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2891.2173 - mse: 16623398.0000 - mae: 2891.2173 - val_loss: 5142.3645 - val_mse: 53238836.0000 - val_mae: 5142.3647\n",
            "Epoch 120/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2904.3528 - mse: 16460015.0000 - mae: 2904.3525 - val_loss: 5137.5587 - val_mse: 53101056.0000 - val_mae: 5137.5581\n",
            "Epoch 121/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2837.3831 - mse: 17236774.0000 - mae: 2837.3831 - val_loss: 5133.5432 - val_mse: 52986320.0000 - val_mae: 5133.5435\n",
            "Epoch 122/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2884.0252 - mse: 15705474.0000 - mae: 2884.0251 - val_loss: 5128.4576 - val_mse: 52841500.0000 - val_mae: 5128.4575\n",
            "Epoch 123/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2929.2544 - mse: 16008148.0000 - mae: 2929.2542 - val_loss: 5125.4080 - val_mse: 52758828.0000 - val_mae: 5125.4082\n",
            "Epoch 124/500\n",
            "30/30 [==============================] - 0s 947us/step - loss: 2799.3362 - mse: 16125493.0000 - mae: 2799.3362 - val_loss: 5119.4684 - val_mse: 52589860.0000 - val_mae: 5119.4683\n",
            "Epoch 125/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2759.1198 - mse: 15679164.0000 - mae: 2759.1199 - val_loss: 5116.6850 - val_mse: 52518184.0000 - val_mae: 5116.6851\n",
            "Epoch 126/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2833.2295 - mse: 16324357.0000 - mae: 2833.2297 - val_loss: 5112.4045 - val_mse: 52403476.0000 - val_mae: 5112.4048\n",
            "Epoch 127/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2899.5305 - mse: 15848345.0000 - mae: 2899.5305 - val_loss: 5110.1157 - val_mse: 52356908.0000 - val_mae: 5110.1157\n",
            "Epoch 128/500\n",
            "30/30 [==============================] - 0s 964us/step - loss: 2818.3732 - mse: 16260339.0000 - mae: 2818.3735 - val_loss: 5106.8181 - val_mse: 52298664.0000 - val_mae: 5106.8184\n",
            "Epoch 129/500\n",
            "30/30 [==============================] - 0s 955us/step - loss: 2852.6702 - mse: 15729451.0000 - mae: 2852.6704 - val_loss: 5100.5112 - val_mse: 52206732.0000 - val_mae: 5100.5107\n",
            "Epoch 130/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2768.9137 - mse: 15726874.0000 - mae: 2768.9136 - val_loss: 5085.5438 - val_mse: 52238708.0000 - val_mae: 5085.5439\n",
            "Epoch 131/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2819.5493 - mse: 15617801.0000 - mae: 2819.5496 - val_loss: 5072.3870 - val_mse: 52142296.0000 - val_mae: 5072.3872\n",
            "Epoch 132/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2828.4209 - mse: 15735100.0000 - mae: 2828.4209 - val_loss: 5061.0475 - val_mse: 51952660.0000 - val_mae: 5061.0474\n",
            "Epoch 133/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2814.4590 - mse: 15989387.0000 - mae: 2814.4592 - val_loss: 5053.4362 - val_mse: 52058416.0000 - val_mae: 5053.4360\n",
            "Epoch 134/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2763.8135 - mse: 15567646.0000 - mae: 2763.8135 - val_loss: 5038.8360 - val_mse: 51755832.0000 - val_mae: 5038.8359\n",
            "Epoch 135/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2842.8128 - mse: 16035063.0000 - mae: 2842.8127 - val_loss: 5059.8966 - val_mse: 51958348.0000 - val_mae: 5059.8970\n",
            "Epoch 136/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2764.2046 - mse: 15232881.0000 - mae: 2764.2046 - val_loss: 5024.7435 - val_mse: 51643100.0000 - val_mae: 5024.7437\n",
            "Epoch 137/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2765.3436 - mse: 14814854.0000 - mae: 2765.3438 - val_loss: 5009.3693 - val_mse: 51413448.0000 - val_mae: 5009.3691\n",
            "Epoch 138/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2761.1531 - mse: 14857839.0000 - mae: 2761.1531 - val_loss: 4989.2760 - val_mse: 51135732.0000 - val_mae: 4989.2759\n",
            "Epoch 139/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2795.2703 - mse: 15825120.0000 - mae: 2795.2703 - val_loss: 4972.8954 - val_mse: 50873948.0000 - val_mae: 4972.8955\n",
            "Epoch 140/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2678.7431 - mse: 14724128.0000 - mae: 2678.7432 - val_loss: 4970.5944 - val_mse: 50795740.0000 - val_mae: 4970.5942\n",
            "Epoch 141/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2687.5252 - mse: 15086543.0000 - mae: 2687.5251 - val_loss: 4947.3646 - val_mse: 50471508.0000 - val_mae: 4947.3647\n",
            "Epoch 142/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2725.9466 - mse: 14813961.0000 - mae: 2725.9465 - val_loss: 4933.8162 - val_mse: 50194612.0000 - val_mae: 4933.8159\n",
            "Epoch 143/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2643.4950 - mse: 14235100.0000 - mae: 2643.4949 - val_loss: 4919.4914 - val_mse: 49937844.0000 - val_mae: 4919.4917\n",
            "Epoch 144/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2700.9681 - mse: 14363051.0000 - mae: 2700.9683 - val_loss: 4906.6220 - val_mse: 49758760.0000 - val_mae: 4906.6221\n",
            "Epoch 145/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2755.2278 - mse: 14816544.0000 - mae: 2755.2275 - val_loss: 4893.5434 - val_mse: 49543848.0000 - val_mae: 4893.5439\n",
            "Epoch 146/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2770.1564 - mse: 15144109.0000 - mae: 2770.1565 - val_loss: 4878.8651 - val_mse: 49361664.0000 - val_mae: 4878.8652\n",
            "Epoch 147/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2590.0395 - mse: 14000044.0000 - mae: 2590.0396 - val_loss: 4863.5253 - val_mse: 49084732.0000 - val_mae: 4863.5254\n",
            "Epoch 148/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2559.1361 - mse: 13012463.0000 - mae: 2559.1360 - val_loss: 4845.4536 - val_mse: 48840604.0000 - val_mae: 4845.4536\n",
            "Epoch 149/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2620.6472 - mse: 13174661.0000 - mae: 2620.6472 - val_loss: 4827.0775 - val_mse: 48623388.0000 - val_mae: 4827.0771\n",
            "Epoch 150/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2626.1883 - mse: 13829105.0000 - mae: 2626.1882 - val_loss: 4809.6873 - val_mse: 48370704.0000 - val_mae: 4809.6875\n",
            "Epoch 151/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2616.4454 - mse: 13685393.0000 - mae: 2616.4453 - val_loss: 4791.0791 - val_mse: 48144592.0000 - val_mae: 4791.0791\n",
            "Epoch 152/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2716.0706 - mse: 13827455.0000 - mae: 2716.0706 - val_loss: 4775.2213 - val_mse: 47891820.0000 - val_mae: 4775.2212\n",
            "Epoch 153/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2563.5499 - mse: 13075781.0000 - mae: 2563.5500 - val_loss: 4763.2527 - val_mse: 47565932.0000 - val_mae: 4763.2524\n",
            "Epoch 154/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2647.7054 - mse: 13332172.0000 - mae: 2647.7053 - val_loss: 4744.9091 - val_mse: 47363012.0000 - val_mae: 4744.9092\n",
            "Epoch 155/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2440.6155 - mse: 11887823.0000 - mae: 2440.6157 - val_loss: 4728.4578 - val_mse: 47051432.0000 - val_mae: 4728.4580\n",
            "Epoch 156/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2556.6898 - mse: 13532062.0000 - mae: 2556.6899 - val_loss: 4718.4328 - val_mse: 46773416.0000 - val_mae: 4718.4326\n",
            "Epoch 157/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2571.8232 - mse: 13267241.0000 - mae: 2571.8232 - val_loss: 4687.1142 - val_mse: 46613740.0000 - val_mae: 4687.1143\n",
            "Epoch 158/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2542.3639 - mse: 13082863.0000 - mae: 2542.3640 - val_loss: 4674.7221 - val_mse: 46334904.0000 - val_mae: 4674.7217\n",
            "Epoch 159/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2540.7646 - mse: 14017044.0000 - mae: 2540.7646 - val_loss: 4657.3374 - val_mse: 46073328.0000 - val_mae: 4657.3374\n",
            "Epoch 160/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2430.1436 - mse: 11302665.0000 - mae: 2430.1436 - val_loss: 4626.1543 - val_mse: 45847872.0000 - val_mae: 4626.1543\n",
            "Epoch 161/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2530.7973 - mse: 12959716.0000 - mae: 2530.7974 - val_loss: 4613.5868 - val_mse: 45538364.0000 - val_mae: 4613.5869\n",
            "Epoch 162/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2549.3245 - mse: 13456004.0000 - mae: 2549.3245 - val_loss: 4599.5200 - val_mse: 45278804.0000 - val_mae: 4599.5200\n",
            "Epoch 163/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2501.7668 - mse: 13218146.0000 - mae: 2501.7668 - val_loss: 4653.6513 - val_mse: 44973248.0000 - val_mae: 4653.6514\n",
            "Epoch 164/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2470.7432 - mse: 13059079.0000 - mae: 2470.7432 - val_loss: 4628.7767 - val_mse: 44707892.0000 - val_mae: 4628.7764\n",
            "Epoch 165/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2476.2074 - mse: 11970960.0000 - mae: 2476.2073 - val_loss: 4605.1656 - val_mse: 44448280.0000 - val_mae: 4605.1655\n",
            "Epoch 166/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2485.9257 - mse: 12604723.0000 - mae: 2485.9255 - val_loss: 4576.2068 - val_mse: 44190164.0000 - val_mae: 4576.2065\n",
            "Epoch 167/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2471.7614 - mse: 11679451.0000 - mae: 2471.7615 - val_loss: 4566.4700 - val_mse: 43926012.0000 - val_mae: 4566.4697\n",
            "Epoch 168/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2452.7043 - mse: 12252915.0000 - mae: 2452.7043 - val_loss: 4517.8375 - val_mse: 43659312.0000 - val_mae: 4517.8374\n",
            "Epoch 169/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2525.8364 - mse: 12489087.0000 - mae: 2525.8364 - val_loss: 4461.3657 - val_mse: 43433384.0000 - val_mae: 4461.3657\n",
            "Epoch 170/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2349.8636 - mse: 11772220.0000 - mae: 2349.8635 - val_loss: 4505.3240 - val_mse: 43145896.0000 - val_mae: 4505.3237\n",
            "Epoch 171/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2480.0879 - mse: 12168278.0000 - mae: 2480.0881 - val_loss: 4454.4805 - val_mse: 42888332.0000 - val_mae: 4454.4800\n",
            "Epoch 172/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2347.8488 - mse: 10961498.0000 - mae: 2347.8489 - val_loss: 4408.0451 - val_mse: 42607148.0000 - val_mae: 4408.0449\n",
            "Epoch 173/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2435.4751 - mse: 11684199.0000 - mae: 2435.4751 - val_loss: 4457.1712 - val_mse: 42358404.0000 - val_mae: 4457.1709\n",
            "Epoch 174/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2534.7047 - mse: 12468606.0000 - mae: 2534.7046 - val_loss: 4410.2097 - val_mse: 42144344.0000 - val_mae: 4410.2100\n",
            "Epoch 175/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2337.3812 - mse: 11547201.0000 - mae: 2337.3813 - val_loss: 4399.3318 - val_mse: 41865992.0000 - val_mae: 4399.3315\n",
            "Epoch 176/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2234.8272 - mse: 10248282.0000 - mae: 2234.8271 - val_loss: 4426.8610 - val_mse: 41592988.0000 - val_mae: 4426.8608\n",
            "Epoch 177/500\n",
            "30/30 [==============================] - 0s 966us/step - loss: 2465.7682 - mse: 12583520.0000 - mae: 2465.7683 - val_loss: 4323.3739 - val_mse: 41323008.0000 - val_mae: 4323.3740\n",
            "Epoch 178/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2461.5145 - mse: 12791701.0000 - mae: 2461.5146 - val_loss: 4416.1050 - val_mse: 41156336.0000 - val_mae: 4416.1045\n",
            "Epoch 179/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2478.8268 - mse: 12071351.0000 - mae: 2478.8269 - val_loss: 4394.8913 - val_mse: 40908460.0000 - val_mae: 4394.8911\n",
            "Epoch 180/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2254.1405 - mse: 9776188.0000 - mae: 2254.1406 - val_loss: 4331.6120 - val_mse: 40574540.0000 - val_mae: 4331.6118\n",
            "Epoch 181/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2199.0614 - mse: 10108542.0000 - mae: 2199.0615 - val_loss: 4342.8133 - val_mse: 40312748.0000 - val_mae: 4342.8130\n",
            "Epoch 182/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2323.5949 - mse: 11704534.0000 - mae: 2323.5947 - val_loss: 4350.9364 - val_mse: 40086152.0000 - val_mae: 4350.9365\n",
            "Epoch 183/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2093.3388 - mse: 8716454.0000 - mae: 2093.3389 - val_loss: 4355.2476 - val_mse: 39812780.0000 - val_mae: 4355.2476\n",
            "Epoch 184/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2296.3573 - mse: 10291919.0000 - mae: 2296.3572 - val_loss: 4315.6505 - val_mse: 39513664.0000 - val_mae: 4315.6504\n",
            "Epoch 185/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2240.9770 - mse: 10142272.0000 - mae: 2240.9771 - val_loss: 4279.2423 - val_mse: 39206844.0000 - val_mae: 4279.2422\n",
            "Epoch 186/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2198.8986 - mse: 10166864.0000 - mae: 2198.8987 - val_loss: 4267.8105 - val_mse: 38938800.0000 - val_mae: 4267.8105\n",
            "Epoch 187/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1995.1479 - mse: 9321202.0000 - mae: 1995.1479 - val_loss: 4322.1692 - val_mse: 38777500.0000 - val_mae: 4322.1694\n",
            "Epoch 188/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2112.7654 - mse: 10004096.0000 - mae: 2112.7654 - val_loss: 4210.9893 - val_mse: 38364868.0000 - val_mae: 4210.9897\n",
            "Epoch 189/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2176.7902 - mse: 9372739.0000 - mae: 2176.7903 - val_loss: 4142.8451 - val_mse: 38018804.0000 - val_mae: 4142.8452\n",
            "Epoch 190/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2115.8955 - mse: 9336462.0000 - mae: 2115.8955 - val_loss: 4230.6494 - val_mse: 37930652.0000 - val_mae: 4230.6494\n",
            "Epoch 191/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2156.4592 - mse: 10241661.0000 - mae: 2156.4592 - val_loss: 4186.0349 - val_mse: 37629000.0000 - val_mae: 4186.0347\n",
            "Epoch 192/500\n",
            "30/30 [==============================] - 0s 925us/step - loss: 2075.4642 - mse: 9449390.0000 - mae: 2075.4644 - val_loss: 4178.8134 - val_mse: 37368112.0000 - val_mae: 4178.8135\n",
            "Epoch 193/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2235.9744 - mse: 10177485.0000 - mae: 2235.9744 - val_loss: 3989.5131 - val_mse: 36894644.0000 - val_mae: 3989.5132\n",
            "Epoch 194/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2157.0720 - mse: 8807265.0000 - mae: 2157.0720 - val_loss: 4133.7928 - val_mse: 36845604.0000 - val_mae: 4133.7925\n",
            "Epoch 195/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2200.6020 - mse: 10447201.0000 - mae: 2200.6021 - val_loss: 4141.0769 - val_mse: 36653592.0000 - val_mae: 4141.0767\n",
            "Epoch 196/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1904.7784 - mse: 7398496.5000 - mae: 1904.7784 - val_loss: 4078.4450 - val_mse: 36272888.0000 - val_mae: 4078.4451\n",
            "Epoch 197/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1919.3487 - mse: 8848215.0000 - mae: 1919.3488 - val_loss: 4186.8391 - val_mse: 36309464.0000 - val_mae: 4186.8389\n",
            "Epoch 198/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2006.8906 - mse: 7835169.0000 - mae: 2006.8906 - val_loss: 4110.0001 - val_mse: 35869008.0000 - val_mae: 4110.0000\n",
            "Epoch 199/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 2036.5238 - mse: 8628785.0000 - mae: 2036.5238 - val_loss: 4183.2516 - val_mse: 35848696.0000 - val_mae: 4183.2515\n",
            "Epoch 200/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1711.7963 - mse: 5584607.0000 - mae: 1711.7964 - val_loss: 4142.6136 - val_mse: 35463952.0000 - val_mae: 4142.6138\n",
            "Epoch 201/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2033.8065 - mse: 9903409.0000 - mae: 2033.8064 - val_loss: 4091.3489 - val_mse: 35145740.0000 - val_mae: 4091.3489\n",
            "Epoch 202/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1923.1342 - mse: 8041171.0000 - mae: 1923.1342 - val_loss: 4082.1935 - val_mse: 34915752.0000 - val_mae: 4082.1936\n",
            "Epoch 203/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1865.3971 - mse: 7749743.0000 - mae: 1865.3970 - val_loss: 3911.1771 - val_mse: 34303768.0000 - val_mae: 3911.1770\n",
            "Epoch 204/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2024.4843 - mse: 8699435.0000 - mae: 2024.4844 - val_loss: 3833.4663 - val_mse: 33943720.0000 - val_mae: 3833.4663\n",
            "Epoch 205/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1867.0858 - mse: 7008064.0000 - mae: 1867.0857 - val_loss: 4011.5345 - val_mse: 34111776.0000 - val_mae: 4011.5344\n",
            "Epoch 206/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1957.0312 - mse: 8439350.0000 - mae: 1957.0312 - val_loss: 3877.1871 - val_mse: 33606836.0000 - val_mae: 3877.1873\n",
            "Epoch 207/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1832.7409 - mse: 6693115.5000 - mae: 1832.7408 - val_loss: 3845.6323 - val_mse: 33308540.0000 - val_mae: 3845.6323\n",
            "Epoch 208/500\n",
            "30/30 [==============================] - 0s 985us/step - loss: 1810.3493 - mse: 7392164.5000 - mae: 1810.3492 - val_loss: 3890.9920 - val_mse: 33158214.0000 - val_mae: 3890.9919\n",
            "Epoch 209/500\n",
            "30/30 [==============================] - 0s 953us/step - loss: 1622.7626 - mse: 5743656.0000 - mae: 1622.7626 - val_loss: 3845.5037 - val_mse: 32798142.0000 - val_mae: 3845.5037\n",
            "Epoch 210/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1659.9618 - mse: 6254815.0000 - mae: 1659.9617 - val_loss: 3917.1135 - val_mse: 32798508.0000 - val_mae: 3917.1135\n",
            "Epoch 211/500\n",
            "30/30 [==============================] - 0s 919us/step - loss: 1694.8955 - mse: 6125881.5000 - mae: 1694.8956 - val_loss: 3939.7422 - val_mse: 32653658.0000 - val_mae: 3939.7422\n",
            "Epoch 212/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1867.8381 - mse: 7889992.0000 - mae: 1867.8380 - val_loss: 3935.2996 - val_mse: 32440278.0000 - val_mae: 3935.2996\n",
            "Epoch 213/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1787.7450 - mse: 6498880.5000 - mae: 1787.7449 - val_loss: 3804.3255 - val_mse: 31884040.0000 - val_mae: 3804.3254\n",
            "Epoch 214/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1864.8345 - mse: 7852003.0000 - mae: 1864.8344 - val_loss: 3751.6849 - val_mse: 31574092.0000 - val_mae: 3751.6848\n",
            "Epoch 215/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1481.9936 - mse: 4802303.0000 - mae: 1481.9935 - val_loss: 3840.6027 - val_mse: 31544908.0000 - val_mae: 3840.6025\n",
            "Epoch 216/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1671.4448 - mse: 5943382.5000 - mae: 1671.4448 - val_loss: 3724.3375 - val_mse: 31042370.0000 - val_mae: 3724.3374\n",
            "Epoch 217/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1567.9809 - mse: 5373254.5000 - mae: 1567.9808 - val_loss: 3690.6105 - val_mse: 30725382.0000 - val_mae: 3690.6106\n",
            "Epoch 218/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1621.3009 - mse: 5894217.5000 - mae: 1621.3009 - val_loss: 3817.4856 - val_mse: 30832098.0000 - val_mae: 3817.4854\n",
            "Epoch 219/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1259.9112 - mse: 3678763.5000 - mae: 1259.9113 - val_loss: 3803.8159 - val_mse: 30536554.0000 - val_mae: 3803.8159\n",
            "Epoch 220/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1451.5627 - mse: 5395997.0000 - mae: 1451.5627 - val_loss: 3755.0503 - val_mse: 30184814.0000 - val_mae: 3755.0503\n",
            "Epoch 221/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1785.7225 - mse: 6267347.5000 - mae: 1785.7225 - val_loss: 3743.5773 - val_mse: 29927400.0000 - val_mae: 3743.5774\n",
            "Epoch 222/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1772.7165 - mse: 5726858.0000 - mae: 1772.7164 - val_loss: 3629.3458 - val_mse: 29432774.0000 - val_mae: 3629.3459\n",
            "Epoch 223/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1712.2365 - mse: 6435128.5000 - mae: 1712.2365 - val_loss: 3684.0858 - val_mse: 29405210.0000 - val_mae: 3684.0859\n",
            "Epoch 224/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1701.6918 - mse: 7379647.5000 - mae: 1701.6918 - val_loss: 3679.2898 - val_mse: 29188488.0000 - val_mae: 3679.2898\n",
            "Epoch 225/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1786.6577 - mse: 7062737.0000 - mae: 1786.6577 - val_loss: 3623.4111 - val_mse: 28825926.0000 - val_mae: 3623.4109\n",
            "Epoch 226/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1548.6304 - mse: 5489873.5000 - mae: 1548.6304 - val_loss: 3636.5908 - val_mse: 28633608.0000 - val_mae: 3636.5908\n",
            "Epoch 227/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1459.9254 - mse: 4678738.5000 - mae: 1459.9254 - val_loss: 3740.2429 - val_mse: 28712686.0000 - val_mae: 3740.2429\n",
            "Epoch 228/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 1713.4943 - mse: 6067273.5000 - mae: 1713.4943 - val_loss: 3632.3447 - val_mse: 28161522.0000 - val_mae: 3632.3447\n",
            "Epoch 229/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1432.7844 - mse: 4294659.0000 - mae: 1432.7844 - val_loss: 3747.7390 - val_mse: 28334586.0000 - val_mae: 3747.7390\n",
            "Epoch 230/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1529.0237 - mse: 5150309.5000 - mae: 1529.0237 - val_loss: 3907.4715 - val_mse: 28791674.0000 - val_mae: 3907.4714\n",
            "Epoch 231/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 1409.4943 - mse: 4288892.0000 - mae: 1409.4943 - val_loss: 3601.2623 - val_mse: 27431860.0000 - val_mae: 3601.2625\n",
            "Epoch 232/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1470.2001 - mse: 5079559.5000 - mae: 1470.2001 - val_loss: 3597.7635 - val_mse: 27273000.0000 - val_mae: 3597.7634\n",
            "Epoch 233/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 1307.2867 - mse: 4176734.5000 - mae: 1307.2867 - val_loss: 3625.5986 - val_mse: 27170578.0000 - val_mae: 3625.5984\n",
            "Epoch 234/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1517.3635 - mse: 6828949.0000 - mae: 1517.3635 - val_loss: 3674.1190 - val_mse: 27185182.0000 - val_mae: 3674.1187\n",
            "Epoch 235/500\n",
            "30/30 [==============================] - 0s 996us/step - loss: 1395.7187 - mse: 4319383.0000 - mae: 1395.7188 - val_loss: 3515.4744 - val_mse: 26464448.0000 - val_mae: 3515.4744\n",
            "Epoch 236/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1781.0212 - mse: 6614727.0000 - mae: 1781.0211 - val_loss: 3704.7028 - val_mse: 26890856.0000 - val_mae: 3704.7029\n",
            "Epoch 237/500\n",
            "30/30 [==============================] - 0s 944us/step - loss: 1376.9467 - mse: 4438851.0000 - mae: 1376.9468 - val_loss: 3736.1481 - val_mse: 26823160.0000 - val_mae: 3736.1479\n",
            "Epoch 238/500\n",
            "30/30 [==============================] - 0s 971us/step - loss: 1659.1005 - mse: 5618570.5000 - mae: 1659.1005 - val_loss: 3558.0831 - val_mse: 26090750.0000 - val_mae: 3558.0833\n",
            "Epoch 239/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 1577.8628 - mse: 6454826.0000 - mae: 1577.8628 - val_loss: 3829.1367 - val_mse: 26989706.0000 - val_mae: 3829.1367\n",
            "Epoch 240/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1588.6070 - mse: 4890874.5000 - mae: 1588.6071 - val_loss: 3523.7341 - val_mse: 25655586.0000 - val_mae: 3523.7341\n",
            "Epoch 241/500\n",
            "30/30 [==============================] - 0s 986us/step - loss: 1538.3236 - mse: 5886855.5000 - mae: 1538.3236 - val_loss: 3828.5111 - val_mse: 26693366.0000 - val_mae: 3828.5112\n",
            "Epoch 242/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1451.3166 - mse: 4711974.5000 - mae: 1451.3165 - val_loss: 3766.1173 - val_mse: 26227422.0000 - val_mae: 3766.1172\n",
            "Epoch 243/500\n",
            "30/30 [==============================] - 0s 981us/step - loss: 1246.5493 - mse: 3115448.0000 - mae: 1246.5492 - val_loss: 3765.9231 - val_mse: 26025058.0000 - val_mae: 3765.9231\n",
            "Epoch 244/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1811.3364 - mse: 8784410.0000 - mae: 1811.3363 - val_loss: 3726.1333 - val_mse: 25756478.0000 - val_mae: 3726.1333\n",
            "Epoch 245/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1230.8931 - mse: 3684748.2500 - mae: 1230.8929 - val_loss: 3808.1991 - val_mse: 25955684.0000 - val_mae: 3808.1990\n",
            "Epoch 246/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1732.6082 - mse: 6051150.5000 - mae: 1732.6080 - val_loss: 3541.0211 - val_mse: 24744546.0000 - val_mae: 3541.0210\n",
            "Epoch 247/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1178.0747 - mse: 2512772.0000 - mae: 1178.0747 - val_loss: 3568.9290 - val_mse: 24586374.0000 - val_mae: 3568.9290\n",
            "Epoch 248/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1258.1927 - mse: 4184808.0000 - mae: 1258.1927 - val_loss: 3520.3635 - val_mse: 24269068.0000 - val_mae: 3520.3635\n",
            "Epoch 249/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1328.2132 - mse: 4189648.0000 - mae: 1328.2131 - val_loss: 3585.0312 - val_mse: 24301622.0000 - val_mae: 3585.0312\n",
            "Epoch 250/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1388.0339 - mse: 4056018.2500 - mae: 1388.0339 - val_loss: 3719.0201 - val_mse: 24630802.0000 - val_mae: 3719.0200\n",
            "Epoch 251/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1020.4779 - mse: 2546280.0000 - mae: 1020.4780 - val_loss: 3693.2609 - val_mse: 24366634.0000 - val_mae: 3693.2610\n",
            "Epoch 252/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1015.4975 - mse: 2314659.7500 - mae: 1015.4975 - val_loss: 3493.9107 - val_mse: 23562322.0000 - val_mae: 3493.9106\n",
            "Epoch 253/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1000.9794 - mse: 2310501.2500 - mae: 1000.9795 - val_loss: 3849.4254 - val_mse: 24774592.0000 - val_mae: 3849.4255\n",
            "Epoch 254/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1709.4937 - mse: 6402510.0000 - mae: 1709.4938 - val_loss: 3725.8309 - val_mse: 24091734.0000 - val_mae: 3725.8311\n",
            "Epoch 255/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1177.7201 - mse: 3785801.0000 - mae: 1177.7201 - val_loss: 3620.7903 - val_mse: 23513838.0000 - val_mae: 3620.7900\n",
            "Epoch 256/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1413.9596 - mse: 4390218.5000 - mae: 1413.9596 - val_loss: 3802.3724 - val_mse: 24221196.0000 - val_mae: 3802.3723\n",
            "Epoch 257/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1083.9822 - mse: 2589212.0000 - mae: 1083.9822 - val_loss: 3523.7644 - val_mse: 23021248.0000 - val_mae: 3523.7646\n",
            "Epoch 258/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1133.7971 - mse: 2559077.0000 - mae: 1133.7971 - val_loss: 3736.7250 - val_mse: 23791958.0000 - val_mae: 3736.7251\n",
            "Epoch 259/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1348.2346 - mse: 3849429.7500 - mae: 1348.2346 - val_loss: 3786.7499 - val_mse: 23892790.0000 - val_mae: 3786.7500\n",
            "Epoch 260/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1087.9862 - mse: 2843167.2500 - mae: 1087.9861 - val_loss: 3631.8233 - val_mse: 23074130.0000 - val_mae: 3631.8232\n",
            "Epoch 261/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1101.6210 - mse: 2358668.5000 - mae: 1101.6210 - val_loss: 3639.7271 - val_mse: 22994258.0000 - val_mae: 3639.7271\n",
            "Epoch 262/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1327.1292 - mse: 4269010.5000 - mae: 1327.1293 - val_loss: 3647.0822 - val_mse: 22863630.0000 - val_mae: 3647.0823\n",
            "Epoch 263/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1320.9869 - mse: 4007672.5000 - mae: 1320.9869 - val_loss: 3613.7136 - val_mse: 22608596.0000 - val_mae: 3613.7136\n",
            "Epoch 264/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1445.1034 - mse: 4101883.5000 - mae: 1445.1034 - val_loss: 3572.1541 - val_mse: 22306052.0000 - val_mae: 3572.1541\n",
            "Epoch 265/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1169.4917 - mse: 3294508.7500 - mae: 1169.4917 - val_loss: 3703.9628 - val_mse: 22664250.0000 - val_mae: 3703.9631\n",
            "Epoch 266/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1188.2324 - mse: 3220151.5000 - mae: 1188.2323 - val_loss: 3895.2360 - val_mse: 23518464.0000 - val_mae: 3895.2358\n",
            "Epoch 267/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 1199.5224 - mse: 3460183.0000 - mae: 1199.5223 - val_loss: 3619.4128 - val_mse: 22007794.0000 - val_mae: 3619.4128\n",
            "Epoch 268/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1195.2357 - mse: 3383818.2500 - mae: 1195.2357 - val_loss: 3779.4204 - val_mse: 22676242.0000 - val_mae: 3779.4204\n",
            "Epoch 269/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 990.4595 - mse: 2010724.5000 - mae: 990.4595 - val_loss: 3488.5106 - val_mse: 21354816.0000 - val_mae: 3488.5107\n",
            "Epoch 270/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1090.4835 - mse: 3067388.7500 - mae: 1090.4835 - val_loss: 3733.8630 - val_mse: 22222688.0000 - val_mae: 3733.8630\n",
            "Epoch 271/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1197.3676 - mse: 2889046.5000 - mae: 1197.3676 - val_loss: 3631.3487 - val_mse: 21620256.0000 - val_mae: 3631.3486\n",
            "Epoch 272/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1307.9331 - mse: 4054930.5000 - mae: 1307.9331 - val_loss: 3699.7446 - val_mse: 21903566.0000 - val_mae: 3699.7446\n",
            "Epoch 273/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 966.7710 - mse: 2236612.5000 - mae: 966.7711 - val_loss: 3671.5852 - val_mse: 21727428.0000 - val_mae: 3671.5852\n",
            "Epoch 274/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1085.8307 - mse: 2366216.2500 - mae: 1085.8307 - val_loss: 3728.2895 - val_mse: 21889528.0000 - val_mae: 3728.2896\n",
            "Epoch 275/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 991.4121 - mse: 2064974.8750 - mae: 991.4121 - val_loss: 3516.5420 - val_mse: 20929978.0000 - val_mae: 3516.5422\n",
            "Epoch 276/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 978.1849 - mse: 2163098.7500 - mae: 978.1849 - val_loss: 3647.1906 - val_mse: 21349518.0000 - val_mae: 3647.1907\n",
            "Epoch 277/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1136.7063 - mse: 2515815.0000 - mae: 1136.7063 - val_loss: 3686.9974 - val_mse: 21446970.0000 - val_mae: 3686.9973\n",
            "Epoch 278/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1085.6093 - mse: 2365989.7500 - mae: 1085.6093 - val_loss: 3807.5261 - val_mse: 22036612.0000 - val_mae: 3807.5261\n",
            "Epoch 279/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1081.7368 - mse: 2996529.5000 - mae: 1081.7367 - val_loss: 3692.8078 - val_mse: 21425722.0000 - val_mae: 3692.8079\n",
            "Epoch 280/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1091.6713 - mse: 3323458.2500 - mae: 1091.6713 - val_loss: 3713.0834 - val_mse: 21417574.0000 - val_mae: 3713.0833\n",
            "Epoch 281/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 868.5127 - mse: 2539702.7500 - mae: 868.5127 - val_loss: 3630.1445 - val_mse: 20983866.0000 - val_mae: 3630.1445\n",
            "Epoch 282/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1247.1762 - mse: 3432082.5000 - mae: 1247.1761 - val_loss: 3717.2989 - val_mse: 21287226.0000 - val_mae: 3717.2991\n",
            "Epoch 283/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1069.7815 - mse: 3155901.0000 - mae: 1069.7815 - val_loss: 3752.0284 - val_mse: 21434568.0000 - val_mae: 3752.0283\n",
            "Epoch 284/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 994.2327 - mse: 1702355.5000 - mae: 994.2327 - val_loss: 3706.5259 - val_mse: 21088098.0000 - val_mae: 3706.5261\n",
            "Epoch 285/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1056.7107 - mse: 2825778.2500 - mae: 1056.7107 - val_loss: 3693.1798 - val_mse: 20902366.0000 - val_mae: 3693.1797\n",
            "Epoch 286/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1087.2581 - mse: 2257928.7500 - mae: 1087.2582 - val_loss: 3820.8855 - val_mse: 21413526.0000 - val_mae: 3820.8855\n",
            "Epoch 287/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1210.3527 - mse: 3070159.2500 - mae: 1210.3528 - val_loss: 3721.3472 - val_mse: 20869678.0000 - val_mae: 3721.3472\n",
            "Epoch 288/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1028.3972 - mse: 2313895.2500 - mae: 1028.3971 - val_loss: 3781.8872 - val_mse: 21088366.0000 - val_mae: 3781.8872\n",
            "Epoch 289/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 980.4147 - mse: 2188665.0000 - mae: 980.4147 - val_loss: 3715.7143 - val_mse: 20793322.0000 - val_mae: 3715.7144\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50 samples, validate on 40 samples\n",
            "Epoch 1/500\n",
            "50/50 [==============================] - 3s 56ms/step - loss: 12239.0578 - mse: 271849152.0000 - mae: 12239.0576 - val_loss: 11435.7804 - val_mse: 220774656.0000 - val_mae: 11435.7803\n",
            "Epoch 2/500\n",
            "50/50 [==============================] - 0s 939us/step - loss: 12239.0077 - mse: 271848096.0000 - mae: 12239.0078 - val_loss: 11435.7177 - val_mse: 220773424.0000 - val_mae: 11435.7178\n",
            "Epoch 3/500\n",
            "50/50 [==============================] - 0s 925us/step - loss: 12238.8955 - mse: 271846144.0000 - mae: 12238.8945 - val_loss: 11435.3456 - val_mse: 220768512.0000 - val_mae: 11435.3457\n",
            "Epoch 4/500\n",
            "50/50 [==============================] - 0s 938us/step - loss: 12237.8425 - mse: 271835520.0000 - mae: 12237.8428 - val_loss: 11432.8539 - val_mse: 220736176.0000 - val_mae: 11432.8535\n",
            "Epoch 5/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 12234.6762 - mse: 271790944.0000 - mae: 12234.6758 - val_loss: 11427.6125 - val_mse: 220662832.0000 - val_mae: 11427.6123\n",
            "Epoch 6/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 12228.0186 - mse: 271690048.0000 - mae: 12228.0186 - val_loss: 11420.1099 - val_mse: 220549504.0000 - val_mae: 11420.1104\n",
            "Epoch 7/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 12220.7316 - mse: 271549728.0000 - mae: 12220.7314 - val_loss: 11411.6197 - val_mse: 220409136.0000 - val_mae: 11411.6191\n",
            "Epoch 8/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 12210.1625 - mse: 271377984.0000 - mae: 12210.1621 - val_loss: 11401.3276 - val_mse: 220228688.0000 - val_mae: 11401.3271\n",
            "Epoch 9/500\n",
            "50/50 [==============================] - 0s 908us/step - loss: 12199.4578 - mse: 271127648.0000 - mae: 12199.4570 - val_loss: 11389.5945 - val_mse: 220014304.0000 - val_mae: 11389.5947\n",
            "Epoch 10/500\n",
            "50/50 [==============================] - 0s 945us/step - loss: 12187.8900 - mse: 270906112.0000 - mae: 12187.8896 - val_loss: 11376.4395 - val_mse: 219766656.0000 - val_mae: 11376.4395\n",
            "Epoch 11/500\n",
            "50/50 [==============================] - 0s 921us/step - loss: 12173.5883 - mse: 270604896.0000 - mae: 12173.5879 - val_loss: 11361.7160 - val_mse: 219486928.0000 - val_mae: 11361.7158\n",
            "Epoch 12/500\n",
            "50/50 [==============================] - 0s 899us/step - loss: 12153.2201 - mse: 270270432.0000 - mae: 12153.2197 - val_loss: 11344.6793 - val_mse: 219160400.0000 - val_mae: 11344.6797\n",
            "Epoch 13/500\n",
            "50/50 [==============================] - 0s 965us/step - loss: 12139.5703 - mse: 269900960.0000 - mae: 12139.5703 - val_loss: 11326.8553 - val_mse: 218807808.0000 - val_mae: 11326.8555\n",
            "Epoch 14/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 12129.0648 - mse: 269655360.0000 - mae: 12129.0654 - val_loss: 11308.6346 - val_mse: 218441216.0000 - val_mae: 11308.6348\n",
            "Epoch 15/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 12105.2015 - mse: 269074976.0000 - mae: 12105.2021 - val_loss: 11288.4294 - val_mse: 218034224.0000 - val_mae: 11288.4297\n",
            "Epoch 16/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 12093.7076 - mse: 268725504.0000 - mae: 12093.7070 - val_loss: 11267.9788 - val_mse: 217618144.0000 - val_mae: 11267.9785\n",
            "Epoch 17/500\n",
            "50/50 [==============================] - 0s 901us/step - loss: 12049.8510 - mse: 267856768.0000 - mae: 12049.8496 - val_loss: 11243.0547 - val_mse: 217109856.0000 - val_mae: 11243.0547\n",
            "Epoch 18/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 12021.7021 - mse: 267563376.0000 - mae: 12021.7012 - val_loss: 11216.8394 - val_mse: 216574976.0000 - val_mae: 11216.8398\n",
            "Epoch 19/500\n",
            "50/50 [==============================] - 0s 900us/step - loss: 12015.3103 - mse: 267251472.0000 - mae: 12015.3096 - val_loss: 11192.8358 - val_mse: 216080336.0000 - val_mae: 11192.8359\n",
            "Epoch 20/500\n",
            "50/50 [==============================] - 0s 924us/step - loss: 11975.7357 - mse: 266121712.0000 - mae: 11975.7354 - val_loss: 11163.9771 - val_mse: 215488208.0000 - val_mae: 11163.9775\n",
            "Epoch 21/500\n",
            "50/50 [==============================] - 0s 944us/step - loss: 11958.6668 - mse: 265951552.0000 - mae: 11958.6660 - val_loss: 11136.0750 - val_mse: 214908000.0000 - val_mae: 11136.0752\n",
            "Epoch 22/500\n",
            "50/50 [==============================] - 0s 918us/step - loss: 11935.4100 - mse: 265438416.0000 - mae: 11935.4102 - val_loss: 11107.3881 - val_mse: 214313728.0000 - val_mae: 11107.3877\n",
            "Epoch 23/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 11907.9424 - mse: 264308592.0000 - mae: 11907.9424 - val_loss: 11078.9605 - val_mse: 213714592.0000 - val_mae: 11078.9609\n",
            "Epoch 24/500\n",
            "50/50 [==============================] - 0s 870us/step - loss: 11852.8757 - mse: 263227072.0000 - mae: 11852.8760 - val_loss: 11046.7093 - val_mse: 213046880.0000 - val_mae: 11046.7090\n",
            "Epoch 25/500\n",
            "50/50 [==============================] - 0s 907us/step - loss: 11842.6195 - mse: 262816176.0000 - mae: 11842.6191 - val_loss: 11013.0431 - val_mse: 212347264.0000 - val_mae: 11013.0430\n",
            "Epoch 26/500\n",
            "50/50 [==============================] - 0s 887us/step - loss: 11818.0555 - mse: 262765216.0000 - mae: 11818.0547 - val_loss: 10980.3182 - val_mse: 211670432.0000 - val_mae: 10980.3184\n",
            "Epoch 27/500\n",
            "50/50 [==============================] - 0s 845us/step - loss: 11788.6785 - mse: 261337536.0000 - mae: 11788.6787 - val_loss: 10947.3544 - val_mse: 210977856.0000 - val_mae: 10947.3545\n",
            "Epoch 28/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 11767.4766 - mse: 260183904.0000 - mae: 11767.4766 - val_loss: 10914.1948 - val_mse: 210284176.0000 - val_mae: 10914.1943\n",
            "Epoch 29/500\n",
            "50/50 [==============================] - 0s 902us/step - loss: 11732.6051 - mse: 259950752.0000 - mae: 11732.6055 - val_loss: 10882.4362 - val_mse: 209622240.0000 - val_mae: 10882.4355\n",
            "Epoch 30/500\n",
            "50/50 [==============================] - 0s 847us/step - loss: 11770.7049 - mse: 259896816.0000 - mae: 11770.7051 - val_loss: 10855.8912 - val_mse: 209058016.0000 - val_mae: 10855.8916\n",
            "Epoch 31/500\n",
            "50/50 [==============================] - 0s 921us/step - loss: 11672.5417 - mse: 258617616.0000 - mae: 11672.5430 - val_loss: 10820.9340 - val_mse: 208335600.0000 - val_mae: 10820.9346\n",
            "Epoch 32/500\n",
            "50/50 [==============================] - 0s 914us/step - loss: 11667.9042 - mse: 257184480.0000 - mae: 11667.9033 - val_loss: 10785.7460 - val_mse: 207602336.0000 - val_mae: 10785.7461\n",
            "Epoch 33/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 11629.7704 - mse: 256855856.0000 - mae: 11629.7695 - val_loss: 10749.7093 - val_mse: 206850736.0000 - val_mae: 10749.7090\n",
            "Epoch 34/500\n",
            "50/50 [==============================] - 0s 906us/step - loss: 11586.4662 - mse: 256353728.0000 - mae: 11586.4658 - val_loss: 10712.6000 - val_mse: 206082848.0000 - val_mae: 10712.5996\n",
            "Epoch 35/500\n",
            "50/50 [==============================] - 0s 902us/step - loss: 11650.5157 - mse: 256619808.0000 - mae: 11650.5146 - val_loss: 10682.4944 - val_mse: 205457280.0000 - val_mae: 10682.4941\n",
            "Epoch 36/500\n",
            "50/50 [==============================] - 0s 958us/step - loss: 11560.3824 - mse: 254050064.0000 - mae: 11560.3828 - val_loss: 10644.6648 - val_mse: 204673296.0000 - val_mae: 10644.6650\n",
            "Epoch 37/500\n",
            "50/50 [==============================] - 0s 934us/step - loss: 11578.9964 - mse: 254817696.0000 - mae: 11578.9961 - val_loss: 10609.0036 - val_mse: 203938528.0000 - val_mae: 10609.0039\n",
            "Epoch 38/500\n",
            "50/50 [==============================] - 0s 872us/step - loss: 11541.6492 - mse: 252109456.0000 - mae: 11541.6504 - val_loss: 10576.6594 - val_mse: 203263088.0000 - val_mae: 10576.6592\n",
            "Epoch 39/500\n",
            "50/50 [==============================] - 0s 846us/step - loss: 11520.9627 - mse: 252724880.0000 - mae: 11520.9629 - val_loss: 10539.2939 - val_mse: 202492448.0000 - val_mae: 10539.2939\n",
            "Epoch 40/500\n",
            "50/50 [==============================] - 0s 934us/step - loss: 11484.5872 - mse: 252137936.0000 - mae: 11484.5879 - val_loss: 10504.2266 - val_mse: 201771296.0000 - val_mae: 10504.2266\n",
            "Epoch 41/500\n",
            "50/50 [==============================] - 0s 907us/step - loss: 11430.5414 - mse: 251222992.0000 - mae: 11430.5410 - val_loss: 10465.9386 - val_mse: 200949168.0000 - val_mae: 10465.9395\n",
            "Epoch 42/500\n",
            "50/50 [==============================] - 0s 897us/step - loss: 11447.0092 - mse: 249820304.0000 - mae: 11447.0098 - val_loss: 10431.2714 - val_mse: 200198912.0000 - val_mae: 10431.2715\n",
            "Epoch 43/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 11349.5963 - mse: 247167200.0000 - mae: 11349.5977 - val_loss: 10389.9459 - val_mse: 199313504.0000 - val_mae: 10389.9463\n",
            "Epoch 44/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 11370.9703 - mse: 247108912.0000 - mae: 11370.9717 - val_loss: 10350.5417 - val_mse: 198467056.0000 - val_mae: 10350.5420\n",
            "Epoch 45/500\n",
            "50/50 [==============================] - 0s 920us/step - loss: 11323.7706 - mse: 245481824.0000 - mae: 11323.7715 - val_loss: 10310.3374 - val_mse: 197582592.0000 - val_mae: 10310.3379\n",
            "Epoch 46/500\n",
            "50/50 [==============================] - 0s 871us/step - loss: 11307.0029 - mse: 246345728.0000 - mae: 11307.0029 - val_loss: 10268.8223 - val_mse: 196661200.0000 - val_mae: 10268.8223\n",
            "Epoch 47/500\n",
            "50/50 [==============================] - 0s 960us/step - loss: 11313.8605 - mse: 243719248.0000 - mae: 11313.8613 - val_loss: 10229.4833 - val_mse: 195781088.0000 - val_mae: 10229.4824\n",
            "Epoch 48/500\n",
            "50/50 [==============================] - 0s 898us/step - loss: 11216.2992 - mse: 241316544.0000 - mae: 11216.2988 - val_loss: 10185.9488 - val_mse: 194811552.0000 - val_mae: 10185.9482\n",
            "Epoch 49/500\n",
            "50/50 [==============================] - 0s 890us/step - loss: 11247.1141 - mse: 241954320.0000 - mae: 11247.1133 - val_loss: 10147.0136 - val_mse: 193896656.0000 - val_mae: 10147.0137\n",
            "Epoch 50/500\n",
            "50/50 [==============================] - 0s 880us/step - loss: 11282.3066 - mse: 243000112.0000 - mae: 11282.3076 - val_loss: 10113.3732 - val_mse: 193037136.0000 - val_mae: 10113.3730\n",
            "Epoch 51/500\n",
            "50/50 [==============================] - 0s 903us/step - loss: 11165.1713 - mse: 242371280.0000 - mae: 11165.1709 - val_loss: 10085.3473 - val_mse: 192210800.0000 - val_mae: 10085.3477\n",
            "Epoch 52/500\n",
            "50/50 [==============================] - 0s 967us/step - loss: 11143.7521 - mse: 239247152.0000 - mae: 11143.7529 - val_loss: 10053.3147 - val_mse: 191212064.0000 - val_mae: 10053.3145\n",
            "Epoch 53/500\n",
            "50/50 [==============================] - 0s 914us/step - loss: 11098.4191 - mse: 239458256.0000 - mae: 11098.4199 - val_loss: 10021.8350 - val_mse: 190178480.0000 - val_mae: 10021.8350\n",
            "Epoch 54/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 11087.3252 - mse: 238192912.0000 - mae: 11087.3262 - val_loss: 9993.0467 - val_mse: 189236816.0000 - val_mae: 9993.0469\n",
            "Epoch 55/500\n",
            "50/50 [==============================] - 0s 903us/step - loss: 11086.8582 - mse: 237257584.0000 - mae: 11086.8584 - val_loss: 9961.8168 - val_mse: 188220384.0000 - val_mae: 9961.8174\n",
            "Epoch 56/500\n",
            "50/50 [==============================] - 0s 916us/step - loss: 11063.6311 - mse: 235557152.0000 - mae: 11063.6309 - val_loss: 9935.4519 - val_mse: 187364384.0000 - val_mae: 9935.4512\n",
            "Epoch 57/500\n",
            "50/50 [==============================] - 0s 920us/step - loss: 10961.7871 - mse: 234518976.0000 - mae: 10961.7861 - val_loss: 9904.0993 - val_mse: 186269088.0000 - val_mae: 9904.0996\n",
            "Epoch 58/500\n",
            "50/50 [==============================] - 0s 881us/step - loss: 11066.5047 - mse: 237396416.0000 - mae: 11066.5049 - val_loss: 9881.1297 - val_mse: 185462400.0000 - val_mae: 9881.1299\n",
            "Epoch 59/500\n",
            "50/50 [==============================] - 0s 904us/step - loss: 10978.0871 - mse: 232457856.0000 - mae: 10978.0859 - val_loss: 9863.6804 - val_mse: 184827360.0000 - val_mae: 9863.6807\n",
            "Epoch 60/500\n",
            "50/50 [==============================] - 0s 914us/step - loss: 10995.5643 - mse: 234273584.0000 - mae: 10995.5654 - val_loss: 9840.4751 - val_mse: 183937392.0000 - val_mae: 9840.4746\n",
            "Epoch 61/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10908.2910 - mse: 231208400.0000 - mae: 10908.2910 - val_loss: 9817.8833 - val_mse: 183073696.0000 - val_mae: 9817.8828\n",
            "Epoch 62/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10958.0214 - mse: 234673344.0000 - mae: 10958.0215 - val_loss: 9796.0217 - val_mse: 182242944.0000 - val_mae: 9796.0215\n",
            "Epoch 63/500\n",
            "50/50 [==============================] - 0s 992us/step - loss: 10850.3328 - mse: 230945088.0000 - mae: 10850.3320 - val_loss: 9771.7505 - val_mse: 181325200.0000 - val_mae: 9771.7500\n",
            "Epoch 64/500\n",
            "50/50 [==============================] - 0s 981us/step - loss: 10877.2887 - mse: 227756320.0000 - mae: 10877.2891 - val_loss: 9744.4195 - val_mse: 180298240.0000 - val_mae: 9744.4199\n",
            "Epoch 65/500\n",
            "50/50 [==============================] - 0s 933us/step - loss: 10814.2901 - mse: 226690336.0000 - mae: 10814.2900 - val_loss: 9719.8298 - val_mse: 179376928.0000 - val_mae: 9719.8301\n",
            "Epoch 66/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10865.2548 - mse: 228751200.0000 - mae: 10865.2549 - val_loss: 9701.0051 - val_mse: 178672672.0000 - val_mae: 9701.0059\n",
            "Epoch 67/500\n",
            "50/50 [==============================] - 0s 944us/step - loss: 10869.9052 - mse: 227553088.0000 - mae: 10869.9053 - val_loss: 9682.4993 - val_mse: 177982416.0000 - val_mae: 9682.5000\n",
            "Epoch 68/500\n",
            "50/50 [==============================] - 0s 971us/step - loss: 10719.7842 - mse: 221214848.0000 - mae: 10719.7842 - val_loss: 9655.8881 - val_mse: 176998832.0000 - val_mae: 9655.8887\n",
            "Epoch 69/500\n",
            "50/50 [==============================] - 0s 910us/step - loss: 10798.7910 - mse: 224477104.0000 - mae: 10798.7910 - val_loss: 9631.7404 - val_mse: 176112080.0000 - val_mae: 9631.7402\n",
            "Epoch 70/500\n",
            "50/50 [==============================] - 0s 955us/step - loss: 10874.4627 - mse: 225699008.0000 - mae: 10874.4629 - val_loss: 9618.8139 - val_mse: 175633616.0000 - val_mae: 9618.8145\n",
            "Epoch 71/500\n",
            "50/50 [==============================] - 0s 935us/step - loss: 10786.8489 - mse: 225459392.0000 - mae: 10786.8486 - val_loss: 9598.6289 - val_mse: 174895504.0000 - val_mae: 9598.6279\n",
            "Epoch 72/500\n",
            "50/50 [==============================] - 0s 883us/step - loss: 10562.7159 - mse: 217298496.0000 - mae: 10562.7158 - val_loss: 9573.1544 - val_mse: 173970416.0000 - val_mae: 9573.1543\n",
            "Epoch 73/500\n",
            "50/50 [==============================] - 0s 886us/step - loss: 10778.4658 - mse: 222765136.0000 - mae: 10778.4658 - val_loss: 9552.8548 - val_mse: 173236192.0000 - val_mae: 9552.8545\n",
            "Epoch 74/500\n",
            "50/50 [==============================] - 0s 867us/step - loss: 10694.8228 - mse: 219334592.0000 - mae: 10694.8223 - val_loss: 9529.4073 - val_mse: 172391616.0000 - val_mae: 9529.4082\n",
            "Epoch 75/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10658.3172 - mse: 221664960.0000 - mae: 10658.3174 - val_loss: 9509.6147 - val_mse: 171682080.0000 - val_mae: 9509.6152\n",
            "Epoch 76/500\n",
            "50/50 [==============================] - 0s 895us/step - loss: 10706.2285 - mse: 222098528.0000 - mae: 10706.2275 - val_loss: 9485.6117 - val_mse: 170828304.0000 - val_mae: 9485.6113\n",
            "Epoch 77/500\n",
            "50/50 [==============================] - 0s 913us/step - loss: 10709.4329 - mse: 218941968.0000 - mae: 10709.4326 - val_loss: 9464.6740 - val_mse: 170085024.0000 - val_mae: 9464.6738\n",
            "Epoch 78/500\n",
            "50/50 [==============================] - 0s 919us/step - loss: 10695.3342 - mse: 218293056.0000 - mae: 10695.3350 - val_loss: 9443.5739 - val_mse: 169339984.0000 - val_mae: 9443.5742\n",
            "Epoch 79/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10741.3705 - mse: 218154112.0000 - mae: 10741.3701 - val_loss: 9425.9939 - val_mse: 168719952.0000 - val_mae: 9425.9941\n",
            "Epoch 80/500\n",
            "50/50 [==============================] - 0s 913us/step - loss: 10704.4030 - mse: 215394224.0000 - mae: 10704.4033 - val_loss: 9403.3947 - val_mse: 167927728.0000 - val_mae: 9403.3945\n",
            "Epoch 81/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10540.6637 - mse: 214477520.0000 - mae: 10540.6641 - val_loss: 9378.9869 - val_mse: 167079872.0000 - val_mae: 9378.9873\n",
            "Epoch 82/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10542.6890 - mse: 211367200.0000 - mae: 10542.6885 - val_loss: 9353.7408 - val_mse: 166207456.0000 - val_mae: 9353.7402\n",
            "Epoch 83/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10556.4973 - mse: 212931232.0000 - mae: 10556.4971 - val_loss: 9337.5944 - val_mse: 165647472.0000 - val_mae: 9337.5947\n",
            "Epoch 84/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10618.9883 - mse: 213435232.0000 - mae: 10618.9873 - val_loss: 9322.2203 - val_mse: 165117120.0000 - val_mae: 9322.2207\n",
            "Epoch 85/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10436.2388 - mse: 206466288.0000 - mae: 10436.2383 - val_loss: 9302.8163 - val_mse: 164450816.0000 - val_mae: 9302.8164\n",
            "Epoch 86/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10477.6086 - mse: 209828368.0000 - mae: 10477.6084 - val_loss: 9284.7133 - val_mse: 163833296.0000 - val_mae: 9284.7129\n",
            "Epoch 87/500\n",
            "50/50 [==============================] - 0s 979us/step - loss: 10677.9021 - mse: 214102400.0000 - mae: 10677.9023 - val_loss: 9273.7039 - val_mse: 163454768.0000 - val_mae: 9273.7041\n",
            "Epoch 88/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10381.4396 - mse: 209267040.0000 - mae: 10381.4395 - val_loss: 9247.2546 - val_mse: 162564400.0000 - val_mae: 9247.2549\n",
            "Epoch 89/500\n",
            "50/50 [==============================] - 0s 979us/step - loss: 10432.0990 - mse: 205062384.0000 - mae: 10432.0986 - val_loss: 9228.1832 - val_mse: 161921120.0000 - val_mae: 9228.1826\n",
            "Epoch 90/500\n",
            "50/50 [==============================] - 0s 891us/step - loss: 10455.7436 - mse: 206719024.0000 - mae: 10455.7441 - val_loss: 9209.6833 - val_mse: 161300784.0000 - val_mae: 9209.6826\n",
            "Epoch 91/500\n",
            "50/50 [==============================] - 0s 897us/step - loss: 10521.1435 - mse: 209047056.0000 - mae: 10521.1436 - val_loss: 9193.6833 - val_mse: 160765824.0000 - val_mae: 9193.6826\n",
            "Epoch 92/500\n",
            "50/50 [==============================] - 0s 898us/step - loss: 10487.2895 - mse: 205492352.0000 - mae: 10487.2900 - val_loss: 9178.8166 - val_mse: 160267744.0000 - val_mae: 9178.8164\n",
            "Epoch 93/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10534.9289 - mse: 210947936.0000 - mae: 10534.9297 - val_loss: 9163.3298 - val_mse: 159753824.0000 - val_mae: 9163.3301\n",
            "Epoch 94/500\n",
            "50/50 [==============================] - 0s 937us/step - loss: 10277.7938 - mse: 202152672.0000 - mae: 10277.7939 - val_loss: 9140.8037 - val_mse: 158992544.0000 - val_mae: 9140.8027\n",
            "Epoch 95/500\n",
            "50/50 [==============================] - 0s 905us/step - loss: 10523.6059 - mse: 204897984.0000 - mae: 10523.6064 - val_loss: 9126.7199 - val_mse: 158477520.0000 - val_mae: 9126.7207\n",
            "Epoch 96/500\n",
            "50/50 [==============================] - 0s 938us/step - loss: 10380.8111 - mse: 202989856.0000 - mae: 10380.8115 - val_loss: 9109.4196 - val_mse: 157849376.0000 - val_mae: 9109.4199\n",
            "Epoch 97/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10425.9359 - mse: 204387856.0000 - mae: 10425.9355 - val_loss: 9095.7083 - val_mse: 157352704.0000 - val_mae: 9095.7080\n",
            "Epoch 98/500\n",
            "50/50 [==============================] - 0s 908us/step - loss: 10360.0564 - mse: 204444544.0000 - mae: 10360.0566 - val_loss: 9076.2749 - val_mse: 156654864.0000 - val_mae: 9076.2754\n",
            "Epoch 99/500\n",
            "50/50 [==============================] - 0s 855us/step - loss: 10524.6475 - mse: 204696848.0000 - mae: 10524.6475 - val_loss: 9064.1891 - val_mse: 156219776.0000 - val_mae: 9064.1895\n",
            "Epoch 100/500\n",
            "50/50 [==============================] - 0s 885us/step - loss: 10629.1209 - mse: 206431728.0000 - mae: 10629.1211 - val_loss: 9053.8911 - val_mse: 155849552.0000 - val_mae: 9053.8906\n",
            "Epoch 101/500\n",
            "50/50 [==============================] - 0s 984us/step - loss: 10439.8549 - mse: 204063536.0000 - mae: 10439.8555 - val_loss: 9037.8661 - val_mse: 155279712.0000 - val_mae: 9037.8652\n",
            "Epoch 102/500\n",
            "50/50 [==============================] - 0s 946us/step - loss: 10502.5367 - mse: 200719776.0000 - mae: 10502.5361 - val_loss: 9027.9758 - val_mse: 154901568.0000 - val_mae: 9027.9746\n",
            "Epoch 103/500\n",
            "50/50 [==============================] - 0s 883us/step - loss: 10463.0615 - mse: 202939696.0000 - mae: 10463.0615 - val_loss: 9016.4745 - val_mse: 154444096.0000 - val_mae: 9016.4746\n",
            "Epoch 104/500\n",
            "50/50 [==============================] - 0s 899us/step - loss: 10397.7332 - mse: 200335200.0000 - mae: 10397.7324 - val_loss: 9003.7089 - val_mse: 153938480.0000 - val_mae: 9003.7090\n",
            "Epoch 105/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10467.9541 - mse: 200301008.0000 - mae: 10467.9541 - val_loss: 8992.1279 - val_mse: 153481376.0000 - val_mae: 8992.1279\n",
            "Epoch 106/500\n",
            "50/50 [==============================] - 0s 950us/step - loss: 10278.6362 - mse: 195943696.0000 - mae: 10278.6367 - val_loss: 8977.5959 - val_mse: 152910352.0000 - val_mae: 8977.5957\n",
            "Epoch 107/500\n",
            "50/50 [==============================] - 0s 943us/step - loss: 10598.8104 - mse: 201225968.0000 - mae: 10598.8096 - val_loss: 8970.0059 - val_mse: 152611424.0000 - val_mae: 8970.0059\n",
            "Epoch 108/500\n",
            "50/50 [==============================] - 0s 948us/step - loss: 10482.9889 - mse: 198931584.0000 - mae: 10482.9883 - val_loss: 8956.9293 - val_mse: 152101280.0000 - val_mae: 8956.9297\n",
            "Epoch 109/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10434.0247 - mse: 200175568.0000 - mae: 10434.0234 - val_loss: 8945.2012 - val_mse: 151645072.0000 - val_mae: 8945.2012\n",
            "Epoch 110/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10338.2971 - mse: 192528224.0000 - mae: 10338.2969 - val_loss: 8933.7798 - val_mse: 151201584.0000 - val_mae: 8933.7793\n",
            "Epoch 111/500\n",
            "50/50 [==============================] - 0s 844us/step - loss: 10172.6710 - mse: 192588240.0000 - mae: 10172.6709 - val_loss: 8918.7033 - val_mse: 150620448.0000 - val_mae: 8918.7031\n",
            "Epoch 112/500\n",
            "50/50 [==============================] - 0s 876us/step - loss: 10227.6631 - mse: 192229680.0000 - mae: 10227.6631 - val_loss: 8903.5034 - val_mse: 150037488.0000 - val_mae: 8903.5029\n",
            "Epoch 113/500\n",
            "50/50 [==============================] - 0s 940us/step - loss: 10100.8106 - mse: 191837552.0000 - mae: 10100.8105 - val_loss: 8888.4886 - val_mse: 149464160.0000 - val_mae: 8888.4893\n",
            "Epoch 114/500\n",
            "50/50 [==============================] - 0s 911us/step - loss: 10315.3589 - mse: 198076208.0000 - mae: 10315.3584 - val_loss: 8873.1763 - val_mse: 148883152.0000 - val_mae: 8873.1768\n",
            "Epoch 115/500\n",
            "50/50 [==============================] - 0s 891us/step - loss: 10163.5533 - mse: 191281504.0000 - mae: 10163.5537 - val_loss: 8859.8741 - val_mse: 148367264.0000 - val_mae: 8859.8740\n",
            "Epoch 116/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10340.3264 - mse: 198590048.0000 - mae: 10340.3262 - val_loss: 8848.8548 - val_mse: 147893760.0000 - val_mae: 8848.8545\n",
            "Epoch 117/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10224.8023 - mse: 190117984.0000 - mae: 10224.8018 - val_loss: 8835.6645 - val_mse: 147329472.0000 - val_mae: 8835.6641\n",
            "Epoch 118/500\n",
            "50/50 [==============================] - 0s 916us/step - loss: 10307.5283 - mse: 191691200.0000 - mae: 10307.5273 - val_loss: 8825.6659 - val_mse: 146903488.0000 - val_mae: 8825.6660\n",
            "Epoch 119/500\n",
            "50/50 [==============================] - 0s 986us/step - loss: 10234.3966 - mse: 190571600.0000 - mae: 10234.3965 - val_loss: 8814.4558 - val_mse: 146427856.0000 - val_mae: 8814.4561\n",
            "Epoch 120/500\n",
            "50/50 [==============================] - 0s 916us/step - loss: 10214.8732 - mse: 186643376.0000 - mae: 10214.8730 - val_loss: 8802.7915 - val_mse: 145935120.0000 - val_mae: 8802.7910\n",
            "Epoch 121/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10259.7786 - mse: 196020176.0000 - mae: 10259.7783 - val_loss: 8792.0743 - val_mse: 145484368.0000 - val_mae: 8792.0742\n",
            "Epoch 122/500\n",
            "50/50 [==============================] - 0s 963us/step - loss: 10212.3917 - mse: 190916704.0000 - mae: 10212.3916 - val_loss: 8778.2557 - val_mse: 144906128.0000 - val_mae: 8778.2559\n",
            "Epoch 123/500\n",
            "50/50 [==============================] - 0s 955us/step - loss: 10426.5689 - mse: 195563888.0000 - mae: 10426.5693 - val_loss: 8769.8538 - val_mse: 144555696.0000 - val_mae: 8769.8535\n",
            "Epoch 124/500\n",
            "50/50 [==============================] - 0s 946us/step - loss: 10115.6508 - mse: 185091520.0000 - mae: 10115.6514 - val_loss: 8757.5288 - val_mse: 144043936.0000 - val_mae: 8757.5293\n",
            "Epoch 125/500\n",
            "50/50 [==============================] - 0s 915us/step - loss: 9965.8675 - mse: 181528512.0000 - mae: 9965.8672 - val_loss: 8741.6392 - val_mse: 143388160.0000 - val_mae: 8741.6387\n",
            "Epoch 126/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10288.1570 - mse: 191818320.0000 - mae: 10288.1572 - val_loss: 8731.6249 - val_mse: 142976592.0000 - val_mae: 8731.6250\n",
            "Epoch 127/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 9957.4229 - mse: 184670112.0000 - mae: 9957.4229 - val_loss: 8717.4925 - val_mse: 142399120.0000 - val_mae: 8717.4922\n",
            "Epoch 128/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10318.6965 - mse: 192032272.0000 - mae: 10318.6963 - val_loss: 8707.9524 - val_mse: 142010848.0000 - val_mae: 8707.9521\n",
            "Epoch 129/500\n",
            "50/50 [==============================] - 0s 969us/step - loss: 10112.0838 - mse: 187825104.0000 - mae: 10112.0840 - val_loss: 8695.9431 - val_mse: 141524304.0000 - val_mae: 8695.9434\n",
            "Epoch 130/500\n",
            "50/50 [==============================] - 0s 921us/step - loss: 10261.8076 - mse: 191689232.0000 - mae: 10261.8076 - val_loss: 8684.4222 - val_mse: 141059808.0000 - val_mae: 8684.4219\n",
            "Epoch 131/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10287.0855 - mse: 188082416.0000 - mae: 10287.0859 - val_loss: 8674.3783 - val_mse: 140652976.0000 - val_mae: 8674.3779\n",
            "Epoch 132/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10224.8366 - mse: 189901392.0000 - mae: 10224.8369 - val_loss: 8665.0827 - val_mse: 140220160.0000 - val_mae: 8665.0830\n",
            "Epoch 133/500\n",
            "50/50 [==============================] - 0s 982us/step - loss: 10049.0176 - mse: 183583232.0000 - mae: 10049.0176 - val_loss: 8652.7920 - val_mse: 139650224.0000 - val_mae: 8652.7920\n",
            "Epoch 134/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10007.1307 - mse: 185175408.0000 - mae: 10007.1309 - val_loss: 8640.2135 - val_mse: 139070304.0000 - val_mae: 8640.2129\n",
            "Epoch 135/500\n",
            "50/50 [==============================] - 0s 987us/step - loss: 10014.6286 - mse: 186341936.0000 - mae: 10014.6289 - val_loss: 8626.9534 - val_mse: 138462656.0000 - val_mae: 8626.9531\n",
            "Epoch 136/500\n",
            "50/50 [==============================] - 0s 938us/step - loss: 10169.5344 - mse: 184080736.0000 - mae: 10169.5342 - val_loss: 8616.6444 - val_mse: 137993280.0000 - val_mae: 8616.6436\n",
            "Epoch 137/500\n",
            "50/50 [==============================] - 0s 914us/step - loss: 10449.9680 - mse: 182281632.0000 - mae: 10449.9678 - val_loss: 8610.0216 - val_mse: 137693664.0000 - val_mae: 8610.0215\n",
            "Epoch 138/500\n",
            "50/50 [==============================] - 0s 920us/step - loss: 10257.0875 - mse: 181865456.0000 - mae: 10257.0879 - val_loss: 8601.5091 - val_mse: 137309408.0000 - val_mae: 8601.5098\n",
            "Epoch 139/500\n",
            "50/50 [==============================] - 0s 911us/step - loss: 10238.9509 - mse: 185653120.0000 - mae: 10238.9512 - val_loss: 8592.8969 - val_mse: 136922064.0000 - val_mae: 8592.8965\n",
            "Epoch 140/500\n",
            "50/50 [==============================] - 0s 944us/step - loss: 10490.4092 - mse: 193884576.0000 - mae: 10490.4092 - val_loss: 8585.2594 - val_mse: 136580016.0000 - val_mae: 8585.2598\n",
            "Epoch 141/500\n",
            "50/50 [==============================] - 0s 917us/step - loss: 10074.9889 - mse: 184531744.0000 - mae: 10074.9883 - val_loss: 8575.0042 - val_mse: 136122432.0000 - val_mae: 8575.0039\n",
            "Epoch 142/500\n",
            "50/50 [==============================] - 0s 892us/step - loss: 10320.7094 - mse: 180374144.0000 - mae: 10320.7090 - val_loss: 8567.3763 - val_mse: 135784000.0000 - val_mae: 8567.3770\n",
            "Epoch 143/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10285.1173 - mse: 182453856.0000 - mae: 10285.1172 - val_loss: 8559.7916 - val_mse: 135425904.0000 - val_mae: 8559.7910\n",
            "Epoch 144/500\n",
            "50/50 [==============================] - 0s 916us/step - loss: 10173.4124 - mse: 180490688.0000 - mae: 10173.4121 - val_loss: 8552.0874 - val_mse: 135019360.0000 - val_mae: 8552.0879\n",
            "Epoch 145/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10103.2029 - mse: 181324192.0000 - mae: 10103.2021 - val_loss: 8543.9905 - val_mse: 134593952.0000 - val_mae: 8543.9902\n",
            "Epoch 146/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10304.8485 - mse: 183403600.0000 - mae: 10304.8486 - val_loss: 8536.7242 - val_mse: 134213696.0000 - val_mae: 8536.7246\n",
            "Epoch 147/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 9890.4261 - mse: 180385728.0000 - mae: 9890.4258 - val_loss: 8525.3385 - val_mse: 133619568.0000 - val_mae: 8525.3379\n",
            "Epoch 148/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10064.6858 - mse: 178870464.0000 - mae: 10064.6865 - val_loss: 8516.8749 - val_mse: 133181968.0000 - val_mae: 8516.8750\n",
            "Epoch 149/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 9942.5019 - mse: 176454528.0000 - mae: 9942.5020 - val_loss: 8509.9826 - val_mse: 132828072.0000 - val_mae: 8509.9824\n",
            "Epoch 150/500\n",
            "50/50 [==============================] - 0s 958us/step - loss: 9839.9969 - mse: 173843024.0000 - mae: 9839.9961 - val_loss: 8502.5082 - val_mse: 132416360.0000 - val_mae: 8502.5078\n",
            "Epoch 151/500\n",
            "50/50 [==============================] - 0s 999us/step - loss: 10205.8082 - mse: 177699120.0000 - mae: 10205.8076 - val_loss: 8496.8216 - val_mse: 132055632.0000 - val_mae: 8496.8223\n",
            "Epoch 152/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 9976.4092 - mse: 173982880.0000 - mae: 9976.4102 - val_loss: 8488.0970 - val_mse: 131501824.0000 - val_mae: 8488.0967\n",
            "Epoch 153/500\n",
            "50/50 [==============================] - 0s 965us/step - loss: 9987.5143 - mse: 175494928.0000 - mae: 9987.5137 - val_loss: 8482.7530 - val_mse: 131064464.0000 - val_mae: 8482.7529\n",
            "Epoch 154/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 9969.4219 - mse: 176608384.0000 - mae: 9969.4219 - val_loss: 8476.9022 - val_mse: 130574632.0000 - val_mae: 8476.9023\n",
            "Epoch 155/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10125.8676 - mse: 175811824.0000 - mae: 10125.8672 - val_loss: 8472.3352 - val_mse: 130201744.0000 - val_mae: 8472.3350\n",
            "Epoch 156/500\n",
            "50/50 [==============================] - 0s 860us/step - loss: 10230.6312 - mse: 177683040.0000 - mae: 10230.6309 - val_loss: 8468.1308 - val_mse: 129859560.0000 - val_mae: 8468.1309\n",
            "Epoch 157/500\n",
            "50/50 [==============================] - 0s 850us/step - loss: 10156.5448 - mse: 176279248.0000 - mae: 10156.5449 - val_loss: 8462.9590 - val_mse: 129456640.0000 - val_mae: 8462.9590\n",
            "Epoch 158/500\n",
            "50/50 [==============================] - 0s 914us/step - loss: 10173.3505 - mse: 175191280.0000 - mae: 10173.3506 - val_loss: 8458.3027 - val_mse: 129108368.0000 - val_mae: 8458.3027\n",
            "Epoch 159/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10189.2480 - mse: 180231488.0000 - mae: 10189.2490 - val_loss: 8452.7481 - val_mse: 128763952.0000 - val_mae: 8452.7480\n",
            "Epoch 160/500\n",
            "50/50 [==============================] - 0s 888us/step - loss: 10058.1551 - mse: 173307456.0000 - mae: 10058.1553 - val_loss: 8430.4386 - val_mse: 128282584.0000 - val_mae: 8430.4395\n",
            "Epoch 161/500\n",
            "50/50 [==============================] - 0s 924us/step - loss: 10204.7812 - mse: 173866720.0000 - mae: 10204.7812 - val_loss: 8374.6702 - val_mse: 127656680.0000 - val_mae: 8374.6699\n",
            "Epoch 162/500\n",
            "50/50 [==============================] - 0s 967us/step - loss: 9882.5745 - mse: 169703344.0000 - mae: 9882.5742 - val_loss: 8342.2496 - val_mse: 127025520.0000 - val_mae: 8342.2500\n",
            "Epoch 163/500\n",
            "50/50 [==============================] - 0s 907us/step - loss: 9895.2683 - mse: 170466160.0000 - mae: 9895.2686 - val_loss: 8287.4906 - val_mse: 126283536.0000 - val_mae: 8287.4902\n",
            "Epoch 164/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 9606.3232 - mse: 157668416.0000 - mae: 9606.3223 - val_loss: 8116.0555 - val_mse: 124558016.0000 - val_mae: 8116.0557\n",
            "Epoch 165/500\n",
            "50/50 [==============================] - 0s 929us/step - loss: 9449.6850 - mse: 170342912.0000 - mae: 9449.6855 - val_loss: 8145.4251 - val_mse: 129792080.0000 - val_mae: 8145.4248\n",
            "Epoch 166/500\n",
            "50/50 [==============================] - 0s 936us/step - loss: 9143.0679 - mse: 162267600.0000 - mae: 9143.0674 - val_loss: 7941.3058 - val_mse: 124745792.0000 - val_mae: 7941.3062\n",
            "Epoch 167/500\n",
            "50/50 [==============================] - 0s 882us/step - loss: 8977.1253 - mse: 160288224.0000 - mae: 8977.1250 - val_loss: 7870.1188 - val_mse: 126132120.0000 - val_mae: 7870.1187\n",
            "Epoch 168/500\n",
            "50/50 [==============================] - 0s 958us/step - loss: 9103.8517 - mse: 164277120.0000 - mae: 9103.8516 - val_loss: 7902.7435 - val_mse: 122140672.0000 - val_mae: 7902.7437\n",
            "Epoch 169/500\n",
            "50/50 [==============================] - 0s 962us/step - loss: 9006.6681 - mse: 167627984.0000 - mae: 9006.6680 - val_loss: 8387.0052 - val_mse: 122968304.0000 - val_mae: 8387.0059\n",
            "Epoch 170/500\n",
            "50/50 [==============================] - 0s 943us/step - loss: 9182.7451 - mse: 163406720.0000 - mae: 9182.7451 - val_loss: 8183.0957 - val_mse: 120927704.0000 - val_mae: 8183.0952\n",
            "Epoch 171/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 9032.1601 - mse: 165239600.0000 - mae: 9032.1602 - val_loss: 8334.3820 - val_mse: 120304400.0000 - val_mae: 8334.3818\n",
            "Epoch 172/500\n",
            "50/50 [==============================] - 0s 874us/step - loss: 8621.6299 - mse: 148900880.0000 - mae: 8621.6289 - val_loss: 8263.7870 - val_mse: 118594600.0000 - val_mae: 8263.7871\n",
            "Epoch 173/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 8687.5446 - mse: 150753344.0000 - mae: 8687.5439 - val_loss: 8056.5475 - val_mse: 115882096.0000 - val_mae: 8056.5479\n",
            "Epoch 174/500\n",
            "50/50 [==============================] - 0s 998us/step - loss: 8453.4918 - mse: 152373008.0000 - mae: 8453.4922 - val_loss: 8038.3311 - val_mse: 114636200.0000 - val_mae: 8038.3311\n",
            "Epoch 175/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 8188.1867 - mse: 141459472.0000 - mae: 8188.1860 - val_loss: 8131.4405 - val_mse: 114209176.0000 - val_mae: 8131.4404\n",
            "Epoch 176/500\n",
            "50/50 [==============================] - 0s 877us/step - loss: 8381.6421 - mse: 145130976.0000 - mae: 8381.6416 - val_loss: 8069.2376 - val_mse: 112659840.0000 - val_mae: 8069.2373\n",
            "Epoch 177/500\n",
            "50/50 [==============================] - 0s 831us/step - loss: 8436.1485 - mse: 149373248.0000 - mae: 8436.1484 - val_loss: 8067.4629 - val_mse: 111773352.0000 - val_mae: 8067.4624\n",
            "Epoch 178/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 8409.3628 - mse: 142802496.0000 - mae: 8409.3623 - val_loss: 7640.1119 - val_mse: 107645616.0000 - val_mae: 7640.1118\n",
            "Epoch 179/500\n",
            "50/50 [==============================] - 0s 960us/step - loss: 8320.7945 - mse: 145460432.0000 - mae: 8320.7939 - val_loss: 7954.7510 - val_mse: 108944920.0000 - val_mae: 7954.7515\n",
            "Epoch 180/500\n",
            "50/50 [==============================] - 0s 991us/step - loss: 8604.8742 - mse: 153217104.0000 - mae: 8604.8740 - val_loss: 7956.5212 - val_mse: 108229872.0000 - val_mae: 7956.5210\n",
            "Epoch 181/500\n",
            "50/50 [==============================] - 0s 914us/step - loss: 8544.8271 - mse: 148142432.0000 - mae: 8544.8271 - val_loss: 7758.4753 - val_mse: 105623800.0000 - val_mae: 7758.4751\n",
            "Epoch 182/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 8053.8253 - mse: 137142832.0000 - mae: 8053.8257 - val_loss: 7685.2050 - val_mse: 104046096.0000 - val_mae: 7685.2056\n",
            "Epoch 183/500\n",
            "50/50 [==============================] - 0s 926us/step - loss: 8107.8247 - mse: 141206752.0000 - mae: 8107.8252 - val_loss: 7587.4261 - val_mse: 102331152.0000 - val_mae: 7587.4268\n",
            "Epoch 184/500\n",
            "50/50 [==============================] - 0s 932us/step - loss: 7775.3072 - mse: 125983680.0000 - mae: 7775.3076 - val_loss: 7686.7227 - val_mse: 102235144.0000 - val_mae: 7686.7227\n",
            "Epoch 185/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 8107.1437 - mse: 140503568.0000 - mae: 8107.1436 - val_loss: 7592.6154 - val_mse: 100495824.0000 - val_mae: 7592.6157\n",
            "Epoch 186/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 8413.7100 - mse: 144084992.0000 - mae: 8413.7100 - val_loss: 7447.8344 - val_mse: 98486488.0000 - val_mae: 7447.8345\n",
            "Epoch 187/500\n",
            "50/50 [==============================] - 0s 916us/step - loss: 7985.1385 - mse: 140680480.0000 - mae: 7985.1387 - val_loss: 7419.1669 - val_mse: 97460976.0000 - val_mae: 7419.1670\n",
            "Epoch 188/500\n",
            "50/50 [==============================] - 0s 908us/step - loss: 7835.2567 - mse: 132583104.0000 - mae: 7835.2568 - val_loss: 7452.4258 - val_mse: 96955856.0000 - val_mae: 7452.4258\n",
            "Epoch 189/500\n",
            "50/50 [==============================] - 0s 946us/step - loss: 8043.8525 - mse: 138895136.0000 - mae: 8043.8525 - val_loss: 7289.1581 - val_mse: 94692160.0000 - val_mae: 7289.1577\n",
            "Epoch 190/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 7725.1184 - mse: 129181504.0000 - mae: 7725.1182 - val_loss: 7263.1292 - val_mse: 93699280.0000 - val_mae: 7263.1299\n",
            "Epoch 191/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 7456.7494 - mse: 117105176.0000 - mae: 7456.7500 - val_loss: 7222.6540 - val_mse: 92490936.0000 - val_mae: 7222.6548\n",
            "Epoch 192/500\n",
            "50/50 [==============================] - 0s 994us/step - loss: 7271.2822 - mse: 118202600.0000 - mae: 7271.2827 - val_loss: 7257.4944 - val_mse: 91979632.0000 - val_mae: 7257.4946\n",
            "Epoch 193/500\n",
            "50/50 [==============================] - 0s 885us/step - loss: 7530.2461 - mse: 127312856.0000 - mae: 7530.2456 - val_loss: 6990.5436 - val_mse: 88966856.0000 - val_mae: 6990.5439\n",
            "Epoch 194/500\n",
            "50/50 [==============================] - 0s 951us/step - loss: 7451.2532 - mse: 120238960.0000 - mae: 7451.2529 - val_loss: 7123.4186 - val_mse: 89313344.0000 - val_mae: 7123.4189\n",
            "Epoch 195/500\n",
            "50/50 [==============================] - 0s 953us/step - loss: 7514.9125 - mse: 125687272.0000 - mae: 7514.9126 - val_loss: 7047.9565 - val_mse: 87887824.0000 - val_mae: 7047.9561\n",
            "Epoch 196/500\n",
            "50/50 [==============================] - 0s 963us/step - loss: 7504.9667 - mse: 119136672.0000 - mae: 7504.9668 - val_loss: 7098.9006 - val_mse: 87857192.0000 - val_mae: 7098.9009\n",
            "Epoch 197/500\n",
            "50/50 [==============================] - 0s 911us/step - loss: 7366.2139 - mse: 120544104.0000 - mae: 7366.2129 - val_loss: 6920.8484 - val_mse: 85421200.0000 - val_mae: 6920.8486\n",
            "Epoch 198/500\n",
            "50/50 [==============================] - 0s 918us/step - loss: 6988.2795 - mse: 109279440.0000 - mae: 6988.2793 - val_loss: 6937.3169 - val_mse: 84942616.0000 - val_mae: 6937.3174\n",
            "Epoch 199/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6915.2822 - mse: 111601656.0000 - mae: 6915.2827 - val_loss: 6888.7433 - val_mse: 83853560.0000 - val_mae: 6888.7437\n",
            "Epoch 200/500\n",
            "50/50 [==============================] - 0s 930us/step - loss: 7205.6860 - mse: 112352048.0000 - mae: 7205.6855 - val_loss: 6833.4138 - val_mse: 82721072.0000 - val_mae: 6833.4141\n",
            "Epoch 201/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 7369.8714 - mse: 116141064.0000 - mae: 7369.8711 - val_loss: 6772.9761 - val_mse: 81499216.0000 - val_mae: 6772.9766\n",
            "Epoch 202/500\n",
            "50/50 [==============================] - 0s 911us/step - loss: 7292.2133 - mse: 123698968.0000 - mae: 7292.2139 - val_loss: 6726.3755 - val_mse: 80486712.0000 - val_mae: 6726.3760\n",
            "Epoch 203/500\n",
            "50/50 [==============================] - 0s 969us/step - loss: 6944.7738 - mse: 106873896.0000 - mae: 6944.7739 - val_loss: 6612.6248 - val_mse: 78760576.0000 - val_mae: 6612.6250\n",
            "Epoch 204/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6777.8828 - mse: 107085032.0000 - mae: 6777.8823 - val_loss: 6669.9844 - val_mse: 78716808.0000 - val_mae: 6669.9844\n",
            "Epoch 205/500\n",
            "50/50 [==============================] - 0s 995us/step - loss: 6982.9669 - mse: 107310304.0000 - mae: 6982.9663 - val_loss: 6570.9676 - val_mse: 77154320.0000 - val_mae: 6570.9673\n",
            "Epoch 206/500\n",
            "50/50 [==============================] - 0s 901us/step - loss: 7333.8384 - mse: 122272744.0000 - mae: 7333.8389 - val_loss: 6491.2976 - val_mse: 75828536.0000 - val_mae: 6491.2979\n",
            "Epoch 207/500\n",
            "50/50 [==============================] - 0s 812us/step - loss: 6988.2163 - mse: 109855680.0000 - mae: 6988.2163 - val_loss: 6428.2856 - val_mse: 74696800.0000 - val_mae: 6428.2856\n",
            "Epoch 208/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6869.8772 - mse: 105427528.0000 - mae: 6869.8774 - val_loss: 6403.6670 - val_mse: 74030656.0000 - val_mae: 6403.6670\n",
            "Epoch 209/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6959.3154 - mse: 106653760.0000 - mae: 6959.3154 - val_loss: 6381.0080 - val_mse: 73232456.0000 - val_mae: 6381.0078\n",
            "Epoch 210/500\n",
            "50/50 [==============================] - 0s 870us/step - loss: 6444.3532 - mse: 91482720.0000 - mae: 6444.3535 - val_loss: 6312.3078 - val_mse: 71988912.0000 - val_mae: 6312.3076\n",
            "Epoch 211/500\n",
            "50/50 [==============================] - 0s 873us/step - loss: 6299.3378 - mse: 88767784.0000 - mae: 6299.3379 - val_loss: 6256.9131 - val_mse: 70996528.0000 - val_mae: 6256.9131\n",
            "Epoch 212/500\n",
            "50/50 [==============================] - 0s 873us/step - loss: 7077.8022 - mse: 113091328.0000 - mae: 7077.8027 - val_loss: 6197.7450 - val_mse: 70030352.0000 - val_mae: 6197.7451\n",
            "Epoch 213/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6644.3979 - mse: 100002256.0000 - mae: 6644.3975 - val_loss: 6197.0439 - val_mse: 69590064.0000 - val_mae: 6197.0439\n",
            "Epoch 214/500\n",
            "50/50 [==============================] - 0s 907us/step - loss: 6679.6464 - mse: 101960856.0000 - mae: 6679.6465 - val_loss: 6113.0974 - val_mse: 68374000.0000 - val_mae: 6113.0972\n",
            "Epoch 215/500\n",
            "50/50 [==============================] - 0s 845us/step - loss: 6265.6777 - mse: 85732848.0000 - mae: 6265.6777 - val_loss: 5896.6701 - val_mse: 65915368.0000 - val_mae: 5896.6704\n",
            "Epoch 216/500\n",
            "50/50 [==============================] - 0s 909us/step - loss: 6439.5853 - mse: 89059368.0000 - mae: 6439.5854 - val_loss: 6057.1987 - val_mse: 66820288.0000 - val_mae: 6057.1982\n",
            "Epoch 217/500\n",
            "50/50 [==============================] - 0s 910us/step - loss: 6850.1016 - mse: 109931480.0000 - mae: 6850.1011 - val_loss: 5902.8319 - val_mse: 64982924.0000 - val_mae: 5902.8320\n",
            "Epoch 218/500\n",
            "50/50 [==============================] - 0s 904us/step - loss: 6524.3095 - mse: 95323544.0000 - mae: 6524.3086 - val_loss: 5934.8765 - val_mse: 64842328.0000 - val_mae: 5934.8765\n",
            "Epoch 219/500\n",
            "50/50 [==============================] - 0s 942us/step - loss: 6228.2015 - mse: 91269440.0000 - mae: 6228.2017 - val_loss: 5837.1819 - val_mse: 63346856.0000 - val_mae: 5837.1821\n",
            "Epoch 220/500\n",
            "50/50 [==============================] - 0s 947us/step - loss: 6643.2458 - mse: 96597552.0000 - mae: 6643.2461 - val_loss: 5881.4055 - val_mse: 63314368.0000 - val_mae: 5881.4053\n",
            "Epoch 221/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6321.4299 - mse: 91271416.0000 - mae: 6321.4302 - val_loss: 5732.9469 - val_mse: 61494324.0000 - val_mae: 5732.9468\n",
            "Epoch 222/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6141.9700 - mse: 81685480.0000 - mae: 6141.9702 - val_loss: 5646.4019 - val_mse: 60228520.0000 - val_mae: 5646.4014\n",
            "Epoch 223/500\n",
            "50/50 [==============================] - 0s 978us/step - loss: 6250.6143 - mse: 87135680.0000 - mae: 6250.6138 - val_loss: 5692.7278 - val_mse: 60142316.0000 - val_mae: 5692.7275\n",
            "Epoch 224/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5917.4527 - mse: 82274312.0000 - mae: 5917.4526 - val_loss: 5667.2728 - val_mse: 59335360.0000 - val_mae: 5667.2725\n",
            "Epoch 225/500\n",
            "50/50 [==============================] - 0s 961us/step - loss: 6176.4715 - mse: 83394480.0000 - mae: 6176.4712 - val_loss: 5586.2234 - val_mse: 58072236.0000 - val_mae: 5586.2236\n",
            "Epoch 226/500\n",
            "50/50 [==============================] - 0s 871us/step - loss: 5745.6804 - mse: 71700736.0000 - mae: 5745.6807 - val_loss: 5597.0001 - val_mse: 57734168.0000 - val_mae: 5597.0000\n",
            "Epoch 227/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5770.9966 - mse: 83445216.0000 - mae: 5770.9971 - val_loss: 5605.2809 - val_mse: 57282772.0000 - val_mae: 5605.2808\n",
            "Epoch 228/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6502.1261 - mse: 92602016.0000 - mae: 6502.1265 - val_loss: 5546.4513 - val_mse: 56276884.0000 - val_mae: 5546.4517\n",
            "Epoch 229/500\n",
            "50/50 [==============================] - 0s 936us/step - loss: 6123.6525 - mse: 82873128.0000 - mae: 6123.6523 - val_loss: 5446.5842 - val_mse: 54821964.0000 - val_mae: 5446.5845\n",
            "Epoch 230/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6380.9649 - mse: 82527168.0000 - mae: 6380.9648 - val_loss: 5464.7222 - val_mse: 54566728.0000 - val_mae: 5464.7217\n",
            "Epoch 231/500\n",
            "50/50 [==============================] - 0s 890us/step - loss: 6302.4618 - mse: 94037024.0000 - mae: 6302.4619 - val_loss: 5468.4471 - val_mse: 54260596.0000 - val_mae: 5468.4468\n",
            "Epoch 232/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6111.1511 - mse: 85508176.0000 - mae: 6111.1514 - val_loss: 5446.5274 - val_mse: 53679632.0000 - val_mae: 5446.5273\n",
            "Epoch 233/500\n",
            "50/50 [==============================] - 0s 967us/step - loss: 5879.5668 - mse: 79489400.0000 - mae: 5879.5669 - val_loss: 5384.0449 - val_mse: 52634556.0000 - val_mae: 5384.0449\n",
            "Epoch 234/500\n",
            "50/50 [==============================] - 0s 832us/step - loss: 6340.2707 - mse: 96947096.0000 - mae: 6340.2705 - val_loss: 5248.4954 - val_mse: 50986980.0000 - val_mae: 5248.4951\n",
            "Epoch 235/500\n",
            "50/50 [==============================] - 0s 840us/step - loss: 6173.5490 - mse: 86086912.0000 - mae: 6173.5488 - val_loss: 5237.9480 - val_mse: 50569864.0000 - val_mae: 5237.9482\n",
            "Epoch 236/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5812.0623 - mse: 83193464.0000 - mae: 5812.0620 - val_loss: 5297.0819 - val_mse: 50812376.0000 - val_mae: 5297.0820\n",
            "Epoch 237/500\n",
            "50/50 [==============================] - 0s 890us/step - loss: 6036.8903 - mse: 85142408.0000 - mae: 6036.8906 - val_loss: 5257.7669 - val_mse: 50029112.0000 - val_mae: 5257.7671\n",
            "Epoch 238/500\n",
            "50/50 [==============================] - 0s 865us/step - loss: 5691.3889 - mse: 70675896.0000 - mae: 5691.3887 - val_loss: 5217.7246 - val_mse: 49289056.0000 - val_mae: 5217.7241\n",
            "Epoch 239/500\n",
            "50/50 [==============================] - 0s 869us/step - loss: 5515.0575 - mse: 69969616.0000 - mae: 5515.0576 - val_loss: 5072.3595 - val_mse: 47562136.0000 - val_mae: 5072.3594\n",
            "Epoch 240/500\n",
            "50/50 [==============================] - 0s 877us/step - loss: 5519.5820 - mse: 72485784.0000 - mae: 5519.5820 - val_loss: 5095.6895 - val_mse: 47251252.0000 - val_mae: 5095.6899\n",
            "Epoch 241/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5637.8239 - mse: 75303240.0000 - mae: 5637.8237 - val_loss: 5153.9819 - val_mse: 47460680.0000 - val_mae: 5153.9819\n",
            "Epoch 242/500\n",
            "50/50 [==============================] - 0s 961us/step - loss: 6013.4480 - mse: 88381368.0000 - mae: 6013.4482 - val_loss: 5062.0652 - val_mse: 46238892.0000 - val_mae: 5062.0654\n",
            "Epoch 243/500\n",
            "50/50 [==============================] - 0s 858us/step - loss: 5351.2084 - mse: 66486900.0000 - mae: 5351.2080 - val_loss: 5002.2934 - val_mse: 45246924.0000 - val_mae: 5002.2935\n",
            "Epoch 244/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5775.6158 - mse: 72639824.0000 - mae: 5775.6157 - val_loss: 5047.1444 - val_mse: 45354808.0000 - val_mae: 5047.1445\n",
            "Epoch 245/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5532.0115 - mse: 68907672.0000 - mae: 5532.0112 - val_loss: 4954.9814 - val_mse: 44124892.0000 - val_mae: 4954.9814\n",
            "Epoch 246/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5359.7830 - mse: 66504316.0000 - mae: 5359.7832 - val_loss: 4986.5671 - val_mse: 44145456.0000 - val_mae: 4986.5674\n",
            "Epoch 247/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5387.0685 - mse: 70815008.0000 - mae: 5387.0688 - val_loss: 4923.1125 - val_mse: 43175680.0000 - val_mae: 4923.1123\n",
            "Epoch 248/500\n",
            "50/50 [==============================] - 0s 970us/step - loss: 5770.3694 - mse: 70690640.0000 - mae: 5770.3691 - val_loss: 4888.3757 - val_mse: 42587940.0000 - val_mae: 4888.3760\n",
            "Epoch 249/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5821.7127 - mse: 79110632.0000 - mae: 5821.7124 - val_loss: 4864.3874 - val_mse: 42150140.0000 - val_mae: 4864.3877\n",
            "Epoch 250/500\n",
            "50/50 [==============================] - 0s 953us/step - loss: 5126.5593 - mse: 59900596.0000 - mae: 5126.5596 - val_loss: 4820.0461 - val_mse: 41506424.0000 - val_mae: 4820.0459\n",
            "Epoch 251/500\n",
            "50/50 [==============================] - 0s 890us/step - loss: 5438.4153 - mse: 63855108.0000 - mae: 5438.4150 - val_loss: 4824.6978 - val_mse: 41357748.0000 - val_mae: 4824.6982\n",
            "Epoch 252/500\n",
            "50/50 [==============================] - 0s 938us/step - loss: 5095.0538 - mse: 52803696.0000 - mae: 5095.0537 - val_loss: 4795.3727 - val_mse: 40806816.0000 - val_mae: 4795.3726\n",
            "Epoch 253/500\n",
            "50/50 [==============================] - 0s 868us/step - loss: 5461.5037 - mse: 60825856.0000 - mae: 5461.5039 - val_loss: 4676.2568 - val_mse: 39246756.0000 - val_mae: 4676.2568\n",
            "Epoch 254/500\n",
            "50/50 [==============================] - 0s 952us/step - loss: 4965.8478 - mse: 56628412.0000 - mae: 4965.8477 - val_loss: 4687.5775 - val_mse: 39232900.0000 - val_mae: 4687.5771\n",
            "Epoch 255/500\n",
            "50/50 [==============================] - 0s 892us/step - loss: 5257.2763 - mse: 64774328.0000 - mae: 5257.2764 - val_loss: 4641.9145 - val_mse: 38629952.0000 - val_mae: 4641.9146\n",
            "Epoch 256/500\n",
            "50/50 [==============================] - 0s 879us/step - loss: 5126.0315 - mse: 60613416.0000 - mae: 5126.0312 - val_loss: 4649.0246 - val_mse: 38261288.0000 - val_mae: 4649.0244\n",
            "Epoch 257/500\n",
            "50/50 [==============================] - 0s 969us/step - loss: 5407.2350 - mse: 68076592.0000 - mae: 5407.2349 - val_loss: 4610.3720 - val_mse: 37670768.0000 - val_mae: 4610.3721\n",
            "Epoch 258/500\n",
            "50/50 [==============================] - 0s 892us/step - loss: 5070.1122 - mse: 54335476.0000 - mae: 5070.1123 - val_loss: 4559.0544 - val_mse: 37011432.0000 - val_mae: 4559.0547\n",
            "Epoch 259/500\n",
            "50/50 [==============================] - 0s 907us/step - loss: 4142.8600 - mse: 43479900.0000 - mae: 4142.8599 - val_loss: 4527.8658 - val_mse: 36462460.0000 - val_mae: 4527.8662\n",
            "Epoch 260/500\n",
            "50/50 [==============================] - 0s 951us/step - loss: 5114.4062 - mse: 55861200.0000 - mae: 5114.4062 - val_loss: 4430.3625 - val_mse: 35124108.0000 - val_mae: 4430.3623\n",
            "Epoch 261/500\n",
            "50/50 [==============================] - 0s 905us/step - loss: 5244.1212 - mse: 62981160.0000 - mae: 5244.1211 - val_loss: 4374.9992 - val_mse: 34459256.0000 - val_mae: 4374.9990\n",
            "Epoch 262/500\n",
            "50/50 [==============================] - 0s 954us/step - loss: 5029.1557 - mse: 53810976.0000 - mae: 5029.1558 - val_loss: 4393.5862 - val_mse: 34350932.0000 - val_mae: 4393.5864\n",
            "Epoch 263/500\n",
            "50/50 [==============================] - 0s 942us/step - loss: 4954.8442 - mse: 59433892.0000 - mae: 4954.8442 - val_loss: 4370.5536 - val_mse: 34033384.0000 - val_mae: 4370.5537\n",
            "Epoch 264/500\n",
            "50/50 [==============================] - 0s 899us/step - loss: 4779.7214 - mse: 56811048.0000 - mae: 4779.7217 - val_loss: 4389.5282 - val_mse: 34176984.0000 - val_mae: 4389.5283\n",
            "Epoch 265/500\n",
            "50/50 [==============================] - 0s 956us/step - loss: 4599.2691 - mse: 50602788.0000 - mae: 4599.2695 - val_loss: 4290.5492 - val_mse: 32801094.0000 - val_mae: 4290.5493\n",
            "Epoch 266/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5056.9268 - mse: 65457464.0000 - mae: 5056.9268 - val_loss: 4295.0218 - val_mse: 32618042.0000 - val_mae: 4295.0220\n",
            "Epoch 267/500\n",
            "50/50 [==============================] - 0s 951us/step - loss: 4207.8918 - mse: 41615504.0000 - mae: 4207.8921 - val_loss: 4233.4665 - val_mse: 31782314.0000 - val_mae: 4233.4663\n",
            "Epoch 268/500\n",
            "50/50 [==============================] - 0s 916us/step - loss: 4242.7148 - mse: 43328336.0000 - mae: 4242.7148 - val_loss: 4244.0464 - val_mse: 31546772.0000 - val_mae: 4244.0464\n",
            "Epoch 269/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4271.3351 - mse: 44729860.0000 - mae: 4271.3350 - val_loss: 4170.2296 - val_mse: 30207818.0000 - val_mae: 4170.2295\n",
            "Epoch 270/500\n",
            "50/50 [==============================] - 0s 913us/step - loss: 4650.0769 - mse: 54128772.0000 - mae: 4650.0767 - val_loss: 4218.2341 - val_mse: 30315164.0000 - val_mae: 4218.2339\n",
            "Epoch 271/500\n",
            "50/50 [==============================] - 0s 967us/step - loss: 4384.2249 - mse: 45668444.0000 - mae: 4384.2246 - val_loss: 4255.9641 - val_mse: 30387108.0000 - val_mae: 4255.9639\n",
            "Epoch 272/500\n",
            "50/50 [==============================] - 0s 886us/step - loss: 4234.8985 - mse: 38149668.0000 - mae: 4234.8984 - val_loss: 4287.2806 - val_mse: 30533012.0000 - val_mae: 4287.2803\n",
            "Epoch 273/500\n",
            "50/50 [==============================] - 0s 999us/step - loss: 4980.2554 - mse: 59991236.0000 - mae: 4980.2554 - val_loss: 4264.3230 - val_mse: 30184976.0000 - val_mae: 4264.3232\n",
            "Epoch 274/500\n",
            "50/50 [==============================] - 0s 882us/step - loss: 4439.1739 - mse: 50952316.0000 - mae: 4439.1738 - val_loss: 4297.1115 - val_mse: 30938464.0000 - val_mae: 4297.1118\n",
            "Epoch 275/500\n",
            "50/50 [==============================] - 0s 952us/step - loss: 4793.5530 - mse: 55038752.0000 - mae: 4793.5532 - val_loss: 4302.5651 - val_mse: 31035430.0000 - val_mae: 4302.5649\n",
            "Epoch 276/500\n",
            "50/50 [==============================] - 0s 888us/step - loss: 3645.3952 - mse: 30365730.0000 - mae: 3645.3953 - val_loss: 4226.3956 - val_mse: 29742758.0000 - val_mae: 4226.3955\n",
            "Epoch 277/500\n",
            "50/50 [==============================] - 0s 880us/step - loss: 4302.3093 - mse: 48045884.0000 - mae: 4302.3096 - val_loss: 4165.6163 - val_mse: 28720436.0000 - val_mae: 4165.6162\n",
            "Epoch 278/500\n",
            "50/50 [==============================] - 0s 992us/step - loss: 4825.7019 - mse: 57994240.0000 - mae: 4825.7021 - val_loss: 4230.5894 - val_mse: 29603812.0000 - val_mae: 4230.5894\n",
            "Epoch 279/500\n",
            "50/50 [==============================] - 0s 921us/step - loss: 4791.0422 - mse: 53136964.0000 - mae: 4791.0420 - val_loss: 4101.4863 - val_mse: 28306122.0000 - val_mae: 4101.4863\n",
            "Epoch 280/500\n",
            "50/50 [==============================] - 0s 861us/step - loss: 4911.4080 - mse: 60389740.0000 - mae: 4911.4077 - val_loss: 4196.9697 - val_mse: 28286564.0000 - val_mae: 4196.9697\n",
            "Epoch 281/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3652.7724 - mse: 30789650.0000 - mae: 3652.7725 - val_loss: 4081.0074 - val_mse: 26931110.0000 - val_mae: 4081.0073\n",
            "Epoch 282/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4943.6288 - mse: 54790388.0000 - mae: 4943.6289 - val_loss: 3998.4774 - val_mse: 26187696.0000 - val_mae: 3998.4773\n",
            "Epoch 283/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3925.0714 - mse: 38853876.0000 - mae: 3925.0713 - val_loss: 4145.2835 - val_mse: 26538816.0000 - val_mae: 4145.2837\n",
            "Epoch 284/500\n",
            "50/50 [==============================] - 0s 873us/step - loss: 4056.7596 - mse: 41291856.0000 - mae: 4056.7598 - val_loss: 3969.4356 - val_mse: 24236424.0000 - val_mae: 3969.4360\n",
            "Epoch 285/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4353.7560 - mse: 43259624.0000 - mae: 4353.7563 - val_loss: 4213.5302 - val_mse: 26425670.0000 - val_mae: 4213.5303\n",
            "Epoch 286/500\n",
            "50/50 [==============================] - 0s 942us/step - loss: 3951.3673 - mse: 34342864.0000 - mae: 3951.3674 - val_loss: 4146.2696 - val_mse: 25187036.0000 - val_mae: 4146.2695\n",
            "Epoch 287/500\n",
            "50/50 [==============================] - 0s 879us/step - loss: 3971.3559 - mse: 32879616.0000 - mae: 3971.3560 - val_loss: 4161.8908 - val_mse: 25214372.0000 - val_mae: 4161.8906\n",
            "Epoch 288/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4023.3278 - mse: 39844148.0000 - mae: 4023.3281 - val_loss: 4119.4533 - val_mse: 24537540.0000 - val_mae: 4119.4531\n",
            "Epoch 289/500\n",
            "50/50 [==============================] - 0s 815us/step - loss: 3360.2521 - mse: 27365520.0000 - mae: 3360.2522 - val_loss: 4138.2173 - val_mse: 24190964.0000 - val_mae: 4138.2173\n",
            "Epoch 290/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3885.3892 - mse: 33065974.0000 - mae: 3885.3892 - val_loss: 4251.5372 - val_mse: 25517792.0000 - val_mae: 4251.5376\n",
            "Epoch 291/500\n",
            "50/50 [==============================] - 0s 935us/step - loss: 4136.9855 - mse: 37767384.0000 - mae: 4136.9858 - val_loss: 4159.0000 - val_mse: 24202112.0000 - val_mae: 4159.0000\n",
            "Epoch 292/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3915.0865 - mse: 34414448.0000 - mae: 3915.0867 - val_loss: 4126.8431 - val_mse: 23706604.0000 - val_mae: 4126.8428\n",
            "Epoch 293/500\n",
            "50/50 [==============================] - 0s 761us/step - loss: 4092.2585 - mse: 37072348.0000 - mae: 4092.2581 - val_loss: 4065.3434 - val_mse: 22982116.0000 - val_mae: 4065.3433\n",
            "Epoch 294/500\n",
            "50/50 [==============================] - 0s 897us/step - loss: 3622.5552 - mse: 30954194.0000 - mae: 3622.5554 - val_loss: 4129.1247 - val_mse: 23588692.0000 - val_mae: 4129.1245\n",
            "Epoch 295/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3904.3054 - mse: 33184018.0000 - mae: 3904.3054 - val_loss: 4192.4559 - val_mse: 24200074.0000 - val_mae: 4192.4561\n",
            "Epoch 296/500\n",
            "50/50 [==============================] - 0s 957us/step - loss: 3849.6303 - mse: 37294152.0000 - mae: 3849.6299 - val_loss: 4108.9941 - val_mse: 23317964.0000 - val_mae: 4108.9941\n",
            "Epoch 297/500\n",
            "50/50 [==============================] - 0s 830us/step - loss: 4035.3405 - mse: 42345548.0000 - mae: 4035.3406 - val_loss: 4056.6238 - val_mse: 22346148.0000 - val_mae: 4056.6238\n",
            "Epoch 298/500\n",
            "50/50 [==============================] - 0s 859us/step - loss: 4713.9085 - mse: 50676768.0000 - mae: 4713.9087 - val_loss: 4112.4160 - val_mse: 22982128.0000 - val_mae: 4112.4160\n",
            "Epoch 299/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3261.3751 - mse: 24338786.0000 - mae: 3261.3750 - val_loss: 4096.2978 - val_mse: 23129692.0000 - val_mae: 4096.2979\n",
            "Epoch 300/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3518.9412 - mse: 29663692.0000 - mae: 3518.9412 - val_loss: 4012.7047 - val_mse: 21511444.0000 - val_mae: 4012.7046\n",
            "Epoch 301/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4231.8046 - mse: 41839948.0000 - mae: 4231.8047 - val_loss: 3968.6428 - val_mse: 21009076.0000 - val_mae: 3968.6431\n",
            "Epoch 302/500\n",
            "50/50 [==============================] - 0s 961us/step - loss: 4014.1025 - mse: 36360984.0000 - mae: 4014.1025 - val_loss: 4152.8293 - val_mse: 22482760.0000 - val_mae: 4152.8291\n",
            "Epoch 303/500\n",
            "50/50 [==============================] - 0s 937us/step - loss: 3686.0874 - mse: 28033320.0000 - mae: 3686.0874 - val_loss: 4130.6724 - val_mse: 21954352.0000 - val_mae: 4130.6724\n",
            "Epoch 304/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4226.6298 - mse: 36154992.0000 - mae: 4226.6299 - val_loss: 3995.7235 - val_mse: 20853112.0000 - val_mae: 3995.7234\n",
            "Epoch 305/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4289.1428 - mse: 44195256.0000 - mae: 4289.1426 - val_loss: 4044.8365 - val_mse: 21299964.0000 - val_mae: 4044.8367\n",
            "Epoch 306/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3769.5580 - mse: 31951224.0000 - mae: 3769.5579 - val_loss: 4117.1280 - val_mse: 21855270.0000 - val_mae: 4117.1279\n",
            "Epoch 307/500\n",
            "50/50 [==============================] - 0s 910us/step - loss: 3644.3942 - mse: 28190760.0000 - mae: 3644.3940 - val_loss: 4114.4384 - val_mse: 21967986.0000 - val_mae: 4114.4385\n",
            "Epoch 308/500\n",
            "50/50 [==============================] - 0s 943us/step - loss: 3715.1994 - mse: 32619870.0000 - mae: 3715.1995 - val_loss: 3944.1037 - val_mse: 19979862.0000 - val_mae: 3944.1040\n",
            "Epoch 309/500\n",
            "50/50 [==============================] - 0s 994us/step - loss: 3735.2542 - mse: 29732234.0000 - mae: 3735.2544 - val_loss: 4031.1201 - val_mse: 21100726.0000 - val_mae: 4031.1204\n",
            "Epoch 310/500\n",
            "50/50 [==============================] - 0s 958us/step - loss: 3487.2433 - mse: 29186932.0000 - mae: 3487.2432 - val_loss: 3893.9206 - val_mse: 19845452.0000 - val_mae: 3893.9204\n",
            "Epoch 311/500\n",
            "50/50 [==============================] - 0s 923us/step - loss: 4129.0323 - mse: 39548536.0000 - mae: 4129.0327 - val_loss: 3885.2938 - val_mse: 19753050.0000 - val_mae: 3885.2937\n",
            "Epoch 312/500\n",
            "50/50 [==============================] - 0s 943us/step - loss: 3617.3875 - mse: 29554232.0000 - mae: 3617.3875 - val_loss: 3889.2905 - val_mse: 19824960.0000 - val_mae: 3889.2905\n",
            "Epoch 313/500\n",
            "50/50 [==============================] - 0s 913us/step - loss: 3815.7845 - mse: 31105336.0000 - mae: 3815.7844 - val_loss: 3833.8538 - val_mse: 19689274.0000 - val_mae: 3833.8540\n",
            "Epoch 314/500\n",
            "50/50 [==============================] - 0s 920us/step - loss: 3282.0392 - mse: 25075062.0000 - mae: 3282.0393 - val_loss: 3689.5670 - val_mse: 18825924.0000 - val_mae: 3689.5671\n",
            "Epoch 315/500\n",
            "50/50 [==============================] - 0s 863us/step - loss: 3404.1333 - mse: 27152144.0000 - mae: 3404.1335 - val_loss: 3802.0699 - val_mse: 20143946.0000 - val_mae: 3802.0698\n",
            "Epoch 316/500\n",
            "50/50 [==============================] - 0s 872us/step - loss: 3411.2244 - mse: 31272382.0000 - mae: 3411.2244 - val_loss: 3537.0558 - val_mse: 17334694.0000 - val_mae: 3537.0559\n",
            "Epoch 317/500\n",
            "50/50 [==============================] - 0s 901us/step - loss: 3435.7492 - mse: 27275928.0000 - mae: 3435.7493 - val_loss: 3732.6394 - val_mse: 19195402.0000 - val_mae: 3732.6394\n",
            "Epoch 318/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3573.3488 - mse: 33032478.0000 - mae: 3573.3486 - val_loss: 3741.1765 - val_mse: 19098952.0000 - val_mae: 3741.1765\n",
            "Epoch 319/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4299.6451 - mse: 39532300.0000 - mae: 4299.6450 - val_loss: 3652.8882 - val_mse: 17717344.0000 - val_mae: 3652.8882\n",
            "Epoch 320/500\n",
            "50/50 [==============================] - 0s 887us/step - loss: 3100.2319 - mse: 19976056.0000 - mae: 3100.2319 - val_loss: 3702.9998 - val_mse: 18276212.0000 - val_mae: 3702.9995\n",
            "Epoch 321/500\n",
            "50/50 [==============================] - 0s 871us/step - loss: 4316.3278 - mse: 43081368.0000 - mae: 4316.3276 - val_loss: 3730.6884 - val_mse: 18736622.0000 - val_mae: 3730.6882\n",
            "Epoch 322/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3556.1745 - mse: 36820572.0000 - mae: 3556.1748 - val_loss: 3778.1813 - val_mse: 18614788.0000 - val_mae: 3778.1812\n",
            "Epoch 323/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4072.4904 - mse: 39749776.0000 - mae: 4072.4902 - val_loss: 3683.0354 - val_mse: 17731670.0000 - val_mae: 3683.0356\n",
            "Epoch 324/500\n",
            "50/50 [==============================] - 0s 921us/step - loss: 3480.0809 - mse: 29119966.0000 - mae: 3480.0811 - val_loss: 3664.3082 - val_mse: 17603590.0000 - val_mae: 3664.3081\n",
            "Epoch 325/500\n",
            "50/50 [==============================] - 0s 868us/step - loss: 3117.8942 - mse: 20299678.0000 - mae: 3117.8943 - val_loss: 3655.2656 - val_mse: 17621228.0000 - val_mae: 3655.2656\n",
            "Epoch 326/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3380.7438 - mse: 22770926.0000 - mae: 3380.7437 - val_loss: 3605.8409 - val_mse: 17814412.0000 - val_mae: 3605.8411\n",
            "Epoch 327/500\n",
            "50/50 [==============================] - 0s 885us/step - loss: 3084.2778 - mse: 17234078.0000 - mae: 3084.2778 - val_loss: 3591.9526 - val_mse: 17555394.0000 - val_mae: 3591.9524\n",
            "Epoch 328/500\n",
            "50/50 [==============================] - 0s 979us/step - loss: 3303.2536 - mse: 26433104.0000 - mae: 3303.2534 - val_loss: 3513.3456 - val_mse: 16054333.0000 - val_mae: 3513.3452\n",
            "Epoch 329/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3334.1050 - mse: 33576784.0000 - mae: 3334.1050 - val_loss: 3606.9984 - val_mse: 18169348.0000 - val_mae: 3606.9985\n",
            "Epoch 330/500\n",
            "50/50 [==============================] - 0s 970us/step - loss: 3326.7019 - mse: 23609402.0000 - mae: 3326.7017 - val_loss: 3552.2691 - val_mse: 17130220.0000 - val_mae: 3552.2690\n",
            "Epoch 331/500\n",
            "50/50 [==============================] - 0s 884us/step - loss: 3378.4348 - mse: 26417556.0000 - mae: 3378.4351 - val_loss: 3575.9589 - val_mse: 17673404.0000 - val_mae: 3575.9585\n",
            "Epoch 332/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3286.3704 - mse: 22920164.0000 - mae: 3286.3704 - val_loss: 3551.5731 - val_mse: 16576864.0000 - val_mae: 3551.5730\n",
            "Epoch 333/500\n",
            "50/50 [==============================] - 0s 946us/step - loss: 3407.0906 - mse: 27582556.0000 - mae: 3407.0906 - val_loss: 3552.6090 - val_mse: 16374491.0000 - val_mae: 3552.6089\n",
            "Epoch 334/500\n",
            "50/50 [==============================] - 0s 899us/step - loss: 2919.9589 - mse: 20973424.0000 - mae: 2919.9590 - val_loss: 3561.6382 - val_mse: 16654067.0000 - val_mae: 3561.6382\n",
            "Epoch 335/500\n",
            "50/50 [==============================] - 0s 882us/step - loss: 3578.8063 - mse: 30112384.0000 - mae: 3578.8062 - val_loss: 3503.8259 - val_mse: 16065858.0000 - val_mae: 3503.8257\n",
            "Epoch 336/500\n",
            "50/50 [==============================] - 0s 861us/step - loss: 3103.6981 - mse: 19704112.0000 - mae: 3103.6982 - val_loss: 3556.7781 - val_mse: 17312742.0000 - val_mae: 3556.7781\n",
            "Epoch 337/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3974.0244 - mse: 37563908.0000 - mae: 3974.0244 - val_loss: 3518.1550 - val_mse: 16528826.0000 - val_mae: 3518.1548\n",
            "Epoch 338/500\n",
            "50/50 [==============================] - 0s 956us/step - loss: 4398.2338 - mse: 45489664.0000 - mae: 4398.2339 - val_loss: 3504.1230 - val_mse: 16716536.0000 - val_mae: 3504.1230\n",
            "Epoch 339/500\n",
            "50/50 [==============================] - 0s 971us/step - loss: 4395.1910 - mse: 46056008.0000 - mae: 4395.1909 - val_loss: 3454.5902 - val_mse: 16095098.0000 - val_mae: 3454.5903\n",
            "Epoch 340/500\n",
            "50/50 [==============================] - 0s 883us/step - loss: 3665.8545 - mse: 27224386.0000 - mae: 3665.8545 - val_loss: 3419.7147 - val_mse: 15600568.0000 - val_mae: 3419.7148\n",
            "Epoch 341/500\n",
            "50/50 [==============================] - 0s 941us/step - loss: 3321.7755 - mse: 23929466.0000 - mae: 3321.7756 - val_loss: 3445.5548 - val_mse: 15980474.0000 - val_mae: 3445.5547\n",
            "Epoch 342/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3469.2014 - mse: 27892304.0000 - mae: 3469.2017 - val_loss: 3435.1703 - val_mse: 15786269.0000 - val_mae: 3435.1704\n",
            "Epoch 343/500\n",
            "50/50 [==============================] - 0s 926us/step - loss: 3237.9033 - mse: 23622424.0000 - mae: 3237.9033 - val_loss: 3440.6315 - val_mse: 15623562.0000 - val_mae: 3440.6316\n",
            "Epoch 344/500\n",
            "50/50 [==============================] - 0s 906us/step - loss: 3426.5877 - mse: 27508280.0000 - mae: 3426.5879 - val_loss: 3381.0067 - val_mse: 14912978.0000 - val_mae: 3381.0066\n",
            "Epoch 345/500\n",
            "50/50 [==============================] - 0s 962us/step - loss: 2943.5571 - mse: 16734886.0000 - mae: 2943.5569 - val_loss: 3421.9978 - val_mse: 15253120.0000 - val_mae: 3421.9976\n",
            "Epoch 346/500\n",
            "50/50 [==============================] - 0s 999us/step - loss: 3525.0763 - mse: 23815644.0000 - mae: 3525.0762 - val_loss: 3302.8989 - val_mse: 14321827.0000 - val_mae: 3302.8989\n",
            "Epoch 347/500\n",
            "50/50 [==============================] - 0s 979us/step - loss: 3405.5050 - mse: 27476104.0000 - mae: 3405.5049 - val_loss: 3359.9086 - val_mse: 15009806.0000 - val_mae: 3359.9087\n",
            "Epoch 348/500\n",
            "50/50 [==============================] - 0s 964us/step - loss: 3588.5035 - mse: 26377308.0000 - mae: 3588.5034 - val_loss: 3339.9875 - val_mse: 14644717.0000 - val_mae: 3339.9875\n",
            "Epoch 349/500\n",
            "50/50 [==============================] - 0s 929us/step - loss: 3463.5781 - mse: 27546118.0000 - mae: 3463.5781 - val_loss: 3429.3774 - val_mse: 15618330.0000 - val_mae: 3429.3774\n",
            "Epoch 350/500\n",
            "50/50 [==============================] - 0s 882us/step - loss: 2674.6107 - mse: 16459269.0000 - mae: 2674.6106 - val_loss: 3351.4229 - val_mse: 14512226.0000 - val_mae: 3351.4226\n",
            "Epoch 351/500\n",
            "50/50 [==============================] - 0s 923us/step - loss: 2788.1468 - mse: 18824208.0000 - mae: 2788.1465 - val_loss: 3383.8828 - val_mse: 15201210.0000 - val_mae: 3383.8828\n",
            "Epoch 352/500\n",
            "50/50 [==============================] - 0s 839us/step - loss: 3425.6142 - mse: 30015610.0000 - mae: 3425.6140 - val_loss: 3370.8854 - val_mse: 14951011.0000 - val_mae: 3370.8855\n",
            "Epoch 353/500\n",
            "50/50 [==============================] - 0s 897us/step - loss: 3337.7582 - mse: 27057922.0000 - mae: 3337.7581 - val_loss: 3362.3347 - val_mse: 15262595.0000 - val_mae: 3362.3345\n",
            "Epoch 354/500\n",
            "50/50 [==============================] - 0s 914us/step - loss: 2897.1724 - mse: 21609402.0000 - mae: 2897.1726 - val_loss: 3299.9392 - val_mse: 14731304.0000 - val_mae: 3299.9395\n",
            "Epoch 355/500\n",
            "50/50 [==============================] - 0s 913us/step - loss: 2972.7393 - mse: 18398984.0000 - mae: 2972.7393 - val_loss: 3242.4174 - val_mse: 14147294.0000 - val_mae: 3242.4175\n",
            "Epoch 356/500\n",
            "50/50 [==============================] - 0s 980us/step - loss: 3524.9303 - mse: 23996002.0000 - mae: 3524.9299 - val_loss: 3380.0472 - val_mse: 15728790.0000 - val_mae: 3380.0474\n",
            "Epoch 357/500\n",
            "50/50 [==============================] - 0s 974us/step - loss: 3009.3484 - mse: 19494580.0000 - mae: 3009.3484 - val_loss: 3337.3895 - val_mse: 15124294.0000 - val_mae: 3337.3894\n",
            "Epoch 358/500\n",
            "50/50 [==============================] - 0s 942us/step - loss: 2792.8115 - mse: 18246922.0000 - mae: 2792.8115 - val_loss: 3286.8347 - val_mse: 14468838.0000 - val_mae: 3286.8347\n",
            "Epoch 359/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3094.9809 - mse: 24416336.0000 - mae: 3094.9810 - val_loss: 3321.8428 - val_mse: 14928413.0000 - val_mae: 3321.8430\n",
            "Epoch 360/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2969.6029 - mse: 18730358.0000 - mae: 2969.6030 - val_loss: 3350.1404 - val_mse: 15468454.0000 - val_mae: 3350.1401\n",
            "Epoch 361/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3395.0260 - mse: 28487854.0000 - mae: 3395.0259 - val_loss: 3291.2736 - val_mse: 14863723.0000 - val_mae: 3291.2734\n",
            "Epoch 362/500\n",
            "50/50 [==============================] - 0s 923us/step - loss: 2721.5869 - mse: 17997850.0000 - mae: 2721.5869 - val_loss: 3327.0046 - val_mse: 15421970.0000 - val_mae: 3327.0046\n",
            "Epoch 363/500\n",
            "50/50 [==============================] - 0s 940us/step - loss: 3261.3157 - mse: 23987682.0000 - mae: 3261.3157 - val_loss: 3261.1868 - val_mse: 14648390.0000 - val_mae: 3261.1868\n",
            "Epoch 364/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3320.6885 - mse: 24046344.0000 - mae: 3320.6885 - val_loss: 3248.4974 - val_mse: 14450472.0000 - val_mae: 3248.4976\n",
            "Epoch 365/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3604.7106 - mse: 28911326.0000 - mae: 3604.7107 - val_loss: 3303.1941 - val_mse: 15320339.0000 - val_mae: 3303.1941\n",
            "Epoch 366/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2970.3317 - mse: 17529068.0000 - mae: 2970.3318 - val_loss: 3286.3230 - val_mse: 15147110.0000 - val_mae: 3286.3230\n",
            "Epoch 367/500\n",
            "50/50 [==============================] - 0s 922us/step - loss: 3205.7658 - mse: 20559790.0000 - mae: 3205.7656 - val_loss: 3256.9246 - val_mse: 14804806.0000 - val_mae: 3256.9246\n",
            "Epoch 368/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3771.8702 - mse: 35863916.0000 - mae: 3771.8706 - val_loss: 3182.4522 - val_mse: 13873512.0000 - val_mae: 3182.4519\n",
            "Epoch 369/500\n",
            "50/50 [==============================] - 0s 919us/step - loss: 3868.9035 - mse: 32392692.0000 - mae: 3868.9033 - val_loss: 3215.2823 - val_mse: 14207645.0000 - val_mae: 3215.2822\n",
            "Epoch 370/500\n",
            "50/50 [==============================] - 0s 963us/step - loss: 3290.5305 - mse: 24343016.0000 - mae: 3290.5303 - val_loss: 3118.6724 - val_mse: 13237923.0000 - val_mae: 3118.6724\n",
            "Epoch 371/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3349.3542 - mse: 25905804.0000 - mae: 3349.3545 - val_loss: 3208.1647 - val_mse: 14138086.0000 - val_mae: 3208.1646\n",
            "Epoch 372/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3038.2797 - mse: 26015976.0000 - mae: 3038.2798 - val_loss: 3205.8878 - val_mse: 14261042.0000 - val_mae: 3205.8879\n",
            "Epoch 373/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3492.4513 - mse: 23483420.0000 - mae: 3492.4512 - val_loss: 3150.0177 - val_mse: 13607370.0000 - val_mae: 3150.0178\n",
            "Epoch 374/500\n",
            "50/50 [==============================] - 0s 977us/step - loss: 3075.2779 - mse: 19698000.0000 - mae: 3075.2778 - val_loss: 3231.3670 - val_mse: 14633683.0000 - val_mae: 3231.3669\n",
            "Epoch 375/500\n",
            "50/50 [==============================] - 0s 884us/step - loss: 2830.8099 - mse: 18832916.0000 - mae: 2830.8101 - val_loss: 3264.5367 - val_mse: 15180310.0000 - val_mae: 3264.5366\n",
            "Epoch 376/500\n",
            "50/50 [==============================] - 0s 854us/step - loss: 3214.6193 - mse: 24930734.0000 - mae: 3214.6191 - val_loss: 3158.3976 - val_mse: 13811701.0000 - val_mae: 3158.3977\n",
            "Epoch 377/500\n",
            "50/50 [==============================] - 0s 952us/step - loss: 2707.5106 - mse: 20956770.0000 - mae: 2707.5107 - val_loss: 3184.2921 - val_mse: 14096450.0000 - val_mae: 3184.2922\n",
            "Epoch 378/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3394.0713 - mse: 26471490.0000 - mae: 3394.0713 - val_loss: 3177.5700 - val_mse: 13894150.0000 - val_mae: 3177.5698\n",
            "Epoch 379/500\n",
            "50/50 [==============================] - 0s 878us/step - loss: 2965.5206 - mse: 17741120.0000 - mae: 2965.5205 - val_loss: 3234.9234 - val_mse: 14583008.0000 - val_mae: 3234.9233\n",
            "Epoch 380/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2933.1372 - mse: 16110045.0000 - mae: 2933.1372 - val_loss: 3133.6665 - val_mse: 13337560.0000 - val_mae: 3133.6665\n",
            "Epoch 381/500\n",
            "50/50 [==============================] - 0s 978us/step - loss: 2607.5425 - mse: 16486865.0000 - mae: 2607.5427 - val_loss: 3178.2223 - val_mse: 13913878.0000 - val_mae: 3178.2222\n",
            "Epoch 382/500\n",
            "50/50 [==============================] - 0s 920us/step - loss: 3736.7953 - mse: 31868790.0000 - mae: 3736.7954 - val_loss: 3062.9676 - val_mse: 12802662.0000 - val_mae: 3062.9675\n",
            "Epoch 383/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3286.7626 - mse: 22048126.0000 - mae: 3286.7625 - val_loss: 3129.8645 - val_mse: 13518224.0000 - val_mae: 3129.8647\n",
            "Epoch 384/500\n",
            "50/50 [==============================] - 0s 894us/step - loss: 3098.4474 - mse: 22683878.0000 - mae: 3098.4473 - val_loss: 3090.7584 - val_mse: 13217590.0000 - val_mae: 3090.7585\n",
            "Epoch 385/500\n",
            "50/50 [==============================] - 0s 837us/step - loss: 3103.6966 - mse: 24273274.0000 - mae: 3103.6965 - val_loss: 3189.5390 - val_mse: 14376387.0000 - val_mae: 3189.5391\n",
            "Epoch 386/500\n",
            "50/50 [==============================] - 0s 839us/step - loss: 2801.3840 - mse: 17134944.0000 - mae: 2801.3838 - val_loss: 3095.0378 - val_mse: 13421307.0000 - val_mae: 3095.0378\n",
            "Epoch 387/500\n",
            "50/50 [==============================] - 0s 993us/step - loss: 2720.6360 - mse: 14783736.0000 - mae: 2720.6360 - val_loss: 3048.6229 - val_mse: 12995878.0000 - val_mae: 3048.6230\n",
            "Epoch 388/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3404.5090 - mse: 28612772.0000 - mae: 3404.5090 - val_loss: 2997.9495 - val_mse: 12528273.0000 - val_mae: 2997.9497\n",
            "Epoch 389/500\n",
            "50/50 [==============================] - 0s 857us/step - loss: 3723.8347 - mse: 28034488.0000 - mae: 3723.8345 - val_loss: 3160.3810 - val_mse: 14255653.0000 - val_mae: 3160.3811\n",
            "Epoch 390/500\n",
            "50/50 [==============================] - 0s 955us/step - loss: 2881.4768 - mse: 18089560.0000 - mae: 2881.4768 - val_loss: 3005.0873 - val_mse: 12715967.0000 - val_mae: 3005.0872\n",
            "Epoch 391/500\n",
            "50/50 [==============================] - 0s 922us/step - loss: 2695.3562 - mse: 17824066.0000 - mae: 2695.3562 - val_loss: 3119.9301 - val_mse: 13769328.0000 - val_mae: 3119.9302\n",
            "Epoch 392/500\n",
            "50/50 [==============================] - 0s 871us/step - loss: 3148.5595 - mse: 21616492.0000 - mae: 3148.5593 - val_loss: 3157.2908 - val_mse: 14130155.0000 - val_mae: 3157.2905\n",
            "Epoch 393/500\n",
            "50/50 [==============================] - 0s 898us/step - loss: 3300.7765 - mse: 25320570.0000 - mae: 3300.7764 - val_loss: 3092.1668 - val_mse: 13542861.0000 - val_mae: 3092.1667\n",
            "Epoch 394/500\n",
            "50/50 [==============================] - 0s 867us/step - loss: 3344.7586 - mse: 29567662.0000 - mae: 3344.7588 - val_loss: 3161.6199 - val_mse: 14327637.0000 - val_mae: 3161.6199\n",
            "Epoch 395/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2789.1608 - mse: 19144140.0000 - mae: 2789.1609 - val_loss: 3056.3838 - val_mse: 13087084.0000 - val_mae: 3056.3838\n",
            "Epoch 396/500\n",
            "50/50 [==============================] - 0s 929us/step - loss: 3638.6744 - mse: 26362588.0000 - mae: 3638.6743 - val_loss: 3060.1884 - val_mse: 13238459.0000 - val_mae: 3060.1885\n",
            "Epoch 397/500\n",
            "50/50 [==============================] - 0s 853us/step - loss: 3411.7638 - mse: 28597504.0000 - mae: 3411.7637 - val_loss: 3019.3433 - val_mse: 12988801.0000 - val_mae: 3019.3433\n",
            "Epoch 398/500\n",
            "50/50 [==============================] - 0s 880us/step - loss: 3457.4164 - mse: 25540270.0000 - mae: 3457.4165 - val_loss: 3077.3788 - val_mse: 13485336.0000 - val_mae: 3077.3787\n",
            "Epoch 399/500\n",
            "50/50 [==============================] - 0s 868us/step - loss: 4021.8734 - mse: 34704460.0000 - mae: 4021.8738 - val_loss: 3046.3967 - val_mse: 13318559.0000 - val_mae: 3046.3967\n",
            "Epoch 400/500\n",
            "50/50 [==============================] - 0s 986us/step - loss: 3636.9414 - mse: 33801980.0000 - mae: 3636.9412 - val_loss: 3115.1606 - val_mse: 14046578.0000 - val_mae: 3115.1606\n",
            "Epoch 401/500\n",
            "50/50 [==============================] - 0s 871us/step - loss: 3578.3773 - mse: 30503384.0000 - mae: 3578.3774 - val_loss: 3146.8661 - val_mse: 14533760.0000 - val_mae: 3146.8660\n",
            "Epoch 402/500\n",
            "50/50 [==============================] - 0s 898us/step - loss: 3528.2708 - mse: 25931060.0000 - mae: 3528.2705 - val_loss: 3087.9801 - val_mse: 13784549.0000 - val_mae: 3087.9800\n",
            "Epoch 403/500\n",
            "50/50 [==============================] - 0s 887us/step - loss: 2909.6216 - mse: 21627032.0000 - mae: 2909.6213 - val_loss: 3040.7881 - val_mse: 13233694.0000 - val_mae: 3040.7881\n",
            "Epoch 404/500\n",
            "50/50 [==============================] - 0s 926us/step - loss: 2737.4429 - mse: 15912442.0000 - mae: 2737.4429 - val_loss: 3108.2312 - val_mse: 14009430.0000 - val_mae: 3108.2312\n",
            "Epoch 405/500\n",
            "50/50 [==============================] - 0s 908us/step - loss: 2897.1497 - mse: 21056144.0000 - mae: 2897.1497 - val_loss: 3083.1190 - val_mse: 13639614.0000 - val_mae: 3083.1191\n",
            "Epoch 406/500\n",
            "50/50 [==============================] - 0s 843us/step - loss: 3086.5203 - mse: 19219776.0000 - mae: 3086.5203 - val_loss: 3089.6491 - val_mse: 13814462.0000 - val_mae: 3089.6492\n",
            "Epoch 407/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2750.7449 - mse: 14746659.0000 - mae: 2750.7451 - val_loss: 3109.6595 - val_mse: 14163106.0000 - val_mae: 3109.6594\n",
            "Epoch 408/500\n",
            "50/50 [==============================] - 0s 849us/step - loss: 3474.4664 - mse: 29637438.0000 - mae: 3474.4663 - val_loss: 3099.8081 - val_mse: 14038520.0000 - val_mae: 3099.8081\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60 samples, validate on 60 samples\n",
            "Epoch 1/500\n",
            "60/60 [==============================] - 3s 44ms/step - loss: 16327.4124 - mse: 452414752.0000 - mae: 16327.4121 - val_loss: 19914.6911 - val_mse: 603277952.0000 - val_mae: 19914.6914\n",
            "Epoch 2/500\n",
            "60/60 [==============================] - 0s 843us/step - loss: 16326.5387 - mse: 452402240.0000 - mae: 16326.5381 - val_loss: 19911.4342 - val_mse: 603205632.0000 - val_mae: 19911.4336\n",
            "Epoch 3/500\n",
            "60/60 [==============================] - 0s 871us/step - loss: 16319.1877 - mse: 452298144.0000 - mae: 16319.1875 - val_loss: 19900.6993 - val_mse: 602957440.0000 - val_mae: 19900.6992\n",
            "Epoch 4/500\n",
            "60/60 [==============================] - 0s 957us/step - loss: 16307.8991 - mse: 452096928.0000 - mae: 16307.9004 - val_loss: 19888.4605 - val_mse: 602669696.0000 - val_mae: 19888.4609\n",
            "Epoch 5/500\n",
            "60/60 [==============================] - 0s 915us/step - loss: 16297.1144 - mse: 451899072.0000 - mae: 16297.1143 - val_loss: 19876.0497 - val_mse: 602370496.0000 - val_mae: 19876.0508\n",
            "Epoch 6/500\n",
            "60/60 [==============================] - 0s 961us/step - loss: 16281.7331 - mse: 451659712.0000 - mae: 16281.7334 - val_loss: 19860.5691 - val_mse: 601985344.0000 - val_mae: 19860.5684\n",
            "Epoch 7/500\n",
            "60/60 [==============================] - 0s 914us/step - loss: 16263.0800 - mse: 451286912.0000 - mae: 16263.0801 - val_loss: 19840.8244 - val_mse: 601477952.0000 - val_mae: 19840.8242\n",
            "Epoch 8/500\n",
            "60/60 [==============================] - 0s 954us/step - loss: 16249.3123 - mse: 450977568.0000 - mae: 16249.3125 - val_loss: 19819.3692 - val_mse: 600906560.0000 - val_mae: 19819.3711\n",
            "Epoch 9/500\n",
            "60/60 [==============================] - 0s 938us/step - loss: 16226.4734 - mse: 450429376.0000 - mae: 16226.4727 - val_loss: 19794.5787 - val_mse: 600221312.0000 - val_mae: 19794.5801\n",
            "Epoch 10/500\n",
            "60/60 [==============================] - 0s 946us/step - loss: 16193.4152 - mse: 449878816.0000 - mae: 16193.4160 - val_loss: 19765.7891 - val_mse: 599412544.0000 - val_mae: 19765.7891\n",
            "Epoch 11/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 16167.1117 - mse: 448886048.0000 - mae: 16167.1123 - val_loss: 19734.2357 - val_mse: 598489024.0000 - val_mae: 19734.2383\n",
            "Epoch 12/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 16131.6130 - mse: 448283584.0000 - mae: 16131.6133 - val_loss: 19700.5966 - val_mse: 597502336.0000 - val_mae: 19700.5957\n",
            "Epoch 13/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 16087.3988 - mse: 447498656.0000 - mae: 16087.3994 - val_loss: 19663.6522 - val_mse: 596388160.0000 - val_mae: 19663.6523\n",
            "Epoch 14/500\n",
            "60/60 [==============================] - 0s 943us/step - loss: 16065.2954 - mse: 446512736.0000 - mae: 16065.2949 - val_loss: 19621.4688 - val_mse: 595088000.0000 - val_mae: 19621.4688\n",
            "Epoch 15/500\n",
            "60/60 [==============================] - 0s 996us/step - loss: 15979.2367 - mse: 444713376.0000 - mae: 15979.2373 - val_loss: 19574.0787 - val_mse: 593608448.0000 - val_mae: 19574.0801\n",
            "Epoch 16/500\n",
            "60/60 [==============================] - 0s 936us/step - loss: 15982.8534 - mse: 444124384.0000 - mae: 15982.8535 - val_loss: 19530.9638 - val_mse: 592231296.0000 - val_mae: 19530.9648\n",
            "Epoch 17/500\n",
            "60/60 [==============================] - 0s 932us/step - loss: 15911.3234 - mse: 442456928.0000 - mae: 15911.3232 - val_loss: 19482.9879 - val_mse: 590661248.0000 - val_mae: 19482.9883\n",
            "Epoch 18/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 15884.1284 - mse: 441363648.0000 - mae: 15884.1279 - val_loss: 19433.1937 - val_mse: 589025344.0000 - val_mae: 19433.1934\n",
            "Epoch 19/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 15870.8787 - mse: 440018112.0000 - mae: 15870.8789 - val_loss: 19389.6573 - val_mse: 587545664.0000 - val_mae: 19389.6582\n",
            "Epoch 20/500\n",
            "60/60 [==============================] - 0s 996us/step - loss: 15809.6838 - mse: 438714048.0000 - mae: 15809.6846 - val_loss: 19338.1667 - val_mse: 585811520.0000 - val_mae: 19338.1660\n",
            "Epoch 21/500\n",
            "60/60 [==============================] - 0s 872us/step - loss: 15783.9313 - mse: 437659488.0000 - mae: 15783.9316 - val_loss: 19280.6926 - val_mse: 583772352.0000 - val_mae: 19280.6914\n",
            "Epoch 22/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 15700.7303 - mse: 435046208.0000 - mae: 15700.7305 - val_loss: 19226.0942 - val_mse: 581851072.0000 - val_mae: 19226.0938\n",
            "Epoch 23/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 15740.7778 - mse: 434641312.0000 - mae: 15740.7764 - val_loss: 19180.1574 - val_mse: 580188480.0000 - val_mae: 19180.1582\n",
            "Epoch 24/500\n",
            "60/60 [==============================] - 0s 964us/step - loss: 15669.4219 - mse: 433111264.0000 - mae: 15669.4219 - val_loss: 19128.6006 - val_mse: 578370240.0000 - val_mae: 19128.5996\n",
            "Epoch 25/500\n",
            "60/60 [==============================] - 0s 895us/step - loss: 15602.3717 - mse: 430676800.0000 - mae: 15602.3730 - val_loss: 19069.9856 - val_mse: 576275392.0000 - val_mae: 19069.9863\n",
            "Epoch 26/500\n",
            "60/60 [==============================] - 0s 991us/step - loss: 15590.8302 - mse: 430420256.0000 - mae: 15590.8291 - val_loss: 19014.7703 - val_mse: 574254848.0000 - val_mae: 19014.7715\n",
            "Epoch 27/500\n",
            "60/60 [==============================] - 0s 876us/step - loss: 15576.4095 - mse: 429183200.0000 - mae: 15576.4092 - val_loss: 18960.1650 - val_mse: 572266880.0000 - val_mae: 18960.1641\n",
            "Epoch 28/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 15493.1733 - mse: 425459488.0000 - mae: 15493.1729 - val_loss: 18898.9032 - val_mse: 569960704.0000 - val_mae: 18898.9043\n",
            "Epoch 29/500\n",
            "60/60 [==============================] - 0s 871us/step - loss: 15461.9134 - mse: 422269280.0000 - mae: 15461.9150 - val_loss: 18834.2015 - val_mse: 567484736.0000 - val_mae: 18834.1992\n",
            "Epoch 30/500\n",
            "60/60 [==============================] - 0s 898us/step - loss: 15410.5321 - mse: 421587232.0000 - mae: 15410.5312 - val_loss: 18770.5041 - val_mse: 565008192.0000 - val_mae: 18770.5039\n",
            "Epoch 31/500\n",
            "60/60 [==============================] - 0s 832us/step - loss: 15354.3206 - mse: 418837664.0000 - mae: 15354.3213 - val_loss: 18706.0699 - val_mse: 562300544.0000 - val_mae: 18706.0703\n",
            "Epoch 32/500\n",
            "60/60 [==============================] - 0s 872us/step - loss: 15383.6331 - mse: 419951424.0000 - mae: 15383.6338 - val_loss: 18652.3657 - val_mse: 559729344.0000 - val_mae: 18652.3672\n",
            "Epoch 33/500\n",
            "60/60 [==============================] - 0s 835us/step - loss: 15218.2155 - mse: 413936512.0000 - mae: 15218.2158 - val_loss: 18589.5699 - val_mse: 556603200.0000 - val_mae: 18589.5703\n",
            "Epoch 34/500\n",
            "60/60 [==============================] - 0s 959us/step - loss: 15317.9528 - mse: 414726368.0000 - mae: 15317.9521 - val_loss: 18527.2822 - val_mse: 553357888.0000 - val_mae: 18527.2832\n",
            "Epoch 35/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 15214.0760 - mse: 410442592.0000 - mae: 15214.0771 - val_loss: 18452.5879 - val_mse: 549144512.0000 - val_mae: 18452.5879\n",
            "Epoch 36/500\n",
            "60/60 [==============================] - 0s 813us/step - loss: 15142.3161 - mse: 408526304.0000 - mae: 15142.3164 - val_loss: 18392.2969 - val_mse: 545871680.0000 - val_mae: 18392.2949\n",
            "Epoch 37/500\n",
            "60/60 [==============================] - 0s 908us/step - loss: 15060.6997 - mse: 406856992.0000 - mae: 15060.7002 - val_loss: 18330.4247 - val_mse: 542450304.0000 - val_mae: 18330.4258\n",
            "Epoch 38/500\n",
            "60/60 [==============================] - 0s 868us/step - loss: 15002.9880 - mse: 399442144.0000 - mae: 15002.9873 - val_loss: 18262.4283 - val_mse: 538640512.0000 - val_mae: 18262.4297\n",
            "Epoch 39/500\n",
            "60/60 [==============================] - 0s 911us/step - loss: 14916.6838 - mse: 400012960.0000 - mae: 14916.6846 - val_loss: 18188.7731 - val_mse: 534353760.0000 - val_mae: 18188.7734\n",
            "Epoch 40/500\n",
            "60/60 [==============================] - 0s 960us/step - loss: 14864.1188 - mse: 394149600.0000 - mae: 14864.1191 - val_loss: 18115.7221 - val_mse: 530112576.0000 - val_mae: 18115.7227\n",
            "Epoch 41/500\n",
            "60/60 [==============================] - 0s 806us/step - loss: 14832.1183 - mse: 395230368.0000 - mae: 14832.1182 - val_loss: 18039.8068 - val_mse: 525707776.0000 - val_mae: 18039.8066\n",
            "Epoch 42/500\n",
            "60/60 [==============================] - 0s 916us/step - loss: 14714.1638 - mse: 387154720.0000 - mae: 14714.1631 - val_loss: 17967.7620 - val_mse: 521619296.0000 - val_mae: 17967.7617\n",
            "Epoch 43/500\n",
            "60/60 [==============================] - 0s 831us/step - loss: 14644.3315 - mse: 386581216.0000 - mae: 14644.3320 - val_loss: 17915.2904 - val_mse: 519023712.0000 - val_mae: 17915.2910\n",
            "Epoch 44/500\n",
            "60/60 [==============================] - 0s 886us/step - loss: 14735.6003 - mse: 387295616.0000 - mae: 14735.6006 - val_loss: 17836.8066 - val_mse: 514445792.0000 - val_mae: 17836.8066\n",
            "Epoch 45/500\n",
            "60/60 [==============================] - 0s 942us/step - loss: 14559.6934 - mse: 379618528.0000 - mae: 14559.6934 - val_loss: 17767.4950 - val_mse: 510786528.0000 - val_mae: 17767.4961\n",
            "Epoch 46/500\n",
            "60/60 [==============================] - 0s 952us/step - loss: 14505.8014 - mse: 376743488.0000 - mae: 14505.8018 - val_loss: 17710.9309 - val_mse: 507893536.0000 - val_mae: 17710.9297\n",
            "Epoch 47/500\n",
            "60/60 [==============================] - 0s 976us/step - loss: 14336.1169 - mse: 367973312.0000 - mae: 14336.1162 - val_loss: 17624.7103 - val_mse: 503323360.0000 - val_mae: 17624.7090\n",
            "Epoch 48/500\n",
            "60/60 [==============================] - 0s 956us/step - loss: 14469.9917 - mse: 373528448.0000 - mae: 14469.9912 - val_loss: 17560.1040 - val_mse: 499935328.0000 - val_mae: 17560.1035\n",
            "Epoch 49/500\n",
            "60/60 [==============================] - 0s 866us/step - loss: 14280.4357 - mse: 365544352.0000 - mae: 14280.4365 - val_loss: 17502.5573 - val_mse: 496948864.0000 - val_mae: 17502.5586\n",
            "Epoch 50/500\n",
            "60/60 [==============================] - 0s 963us/step - loss: 14182.8076 - mse: 364550656.0000 - mae: 14182.8086 - val_loss: 17416.6359 - val_mse: 492759904.0000 - val_mae: 17416.6348\n",
            "Epoch 51/500\n",
            "60/60 [==============================] - 0s 915us/step - loss: 14211.4735 - mse: 363004480.0000 - mae: 14211.4727 - val_loss: 17464.5996 - val_mse: 490457440.0000 - val_mae: 17464.5996\n",
            "Epoch 52/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 14030.7480 - mse: 357798976.0000 - mae: 14030.7480 - val_loss: 17246.2997 - val_mse: 484332160.0000 - val_mae: 17246.3008\n",
            "Epoch 53/500\n",
            "60/60 [==============================] - 0s 829us/step - loss: 14008.7407 - mse: 355953536.0000 - mae: 14008.7412 - val_loss: 17247.6288 - val_mse: 481732672.0000 - val_mae: 17247.6289\n",
            "Epoch 54/500\n",
            "60/60 [==============================] - 0s 808us/step - loss: 13838.9775 - mse: 349457632.0000 - mae: 13838.9775 - val_loss: 17061.0346 - val_mse: 475267264.0000 - val_mae: 17061.0352\n",
            "Epoch 55/500\n",
            "60/60 [==============================] - 0s 825us/step - loss: 14060.9315 - mse: 349600224.0000 - mae: 14060.9326 - val_loss: 17006.7179 - val_mse: 472481984.0000 - val_mae: 17006.7188\n",
            "Epoch 56/500\n",
            "60/60 [==============================] - 0s 902us/step - loss: 13759.1139 - mse: 342901760.0000 - mae: 13759.1133 - val_loss: 16941.8224 - val_mse: 468503840.0000 - val_mae: 16941.8223\n",
            "Epoch 57/500\n",
            "60/60 [==============================] - 0s 818us/step - loss: 13725.9232 - mse: 342997312.0000 - mae: 13725.9229 - val_loss: 16819.3881 - val_mse: 463495104.0000 - val_mae: 16819.3887\n",
            "Epoch 58/500\n",
            "60/60 [==============================] - 0s 937us/step - loss: 13542.6305 - mse: 335023776.0000 - mae: 13542.6309 - val_loss: 16749.4364 - val_mse: 459370176.0000 - val_mae: 16749.4375\n",
            "Epoch 59/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 13581.7166 - mse: 337030144.0000 - mae: 13581.7168 - val_loss: 16752.9917 - val_mse: 455616576.0000 - val_mae: 16752.9922\n",
            "Epoch 60/500\n",
            "60/60 [==============================] - 0s 946us/step - loss: 13405.8428 - mse: 323914688.0000 - mae: 13405.8428 - val_loss: 16518.0647 - val_mse: 449587328.0000 - val_mae: 16518.0645\n",
            "Epoch 61/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 13588.9515 - mse: 332266432.0000 - mae: 13588.9512 - val_loss: 16435.6377 - val_mse: 445437824.0000 - val_mae: 16435.6367\n",
            "Epoch 62/500\n",
            "60/60 [==============================] - 0s 856us/step - loss: 13153.8060 - mse: 313195936.0000 - mae: 13153.8066 - val_loss: 16452.6823 - val_mse: 441326912.0000 - val_mae: 16452.6836\n",
            "Epoch 63/500\n",
            "60/60 [==============================] - 0s 997us/step - loss: 13316.1702 - mse: 325765056.0000 - mae: 13316.1699 - val_loss: 16402.0548 - val_mse: 436882016.0000 - val_mae: 16402.0547\n",
            "Epoch 64/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 13139.0185 - mse: 314924992.0000 - mae: 13139.0186 - val_loss: 16313.6874 - val_mse: 432131136.0000 - val_mae: 16313.6875\n",
            "Epoch 65/500\n",
            "60/60 [==============================] - 0s 955us/step - loss: 13152.6077 - mse: 315517792.0000 - mae: 13152.6074 - val_loss: 16554.9001 - val_mse: 429688960.0000 - val_mae: 16554.9004\n",
            "Epoch 66/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 12847.1628 - mse: 306139552.0000 - mae: 12847.1631 - val_loss: 16366.8043 - val_mse: 423885344.0000 - val_mae: 16366.8037\n",
            "Epoch 67/500\n",
            "60/60 [==============================] - 0s 988us/step - loss: 12777.5110 - mse: 295402944.0000 - mae: 12777.5107 - val_loss: 16064.5367 - val_mse: 417431840.0000 - val_mae: 16064.5361\n",
            "Epoch 68/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 12592.9676 - mse: 292703008.0000 - mae: 12592.9688 - val_loss: 15999.3433 - val_mse: 412489600.0000 - val_mae: 15999.3438\n",
            "Epoch 69/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 12674.8467 - mse: 300022368.0000 - mae: 12674.8457 - val_loss: 15800.1414 - val_mse: 406931872.0000 - val_mae: 15800.1416\n",
            "Epoch 70/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 12686.2251 - mse: 294280768.0000 - mae: 12686.2246 - val_loss: 15747.7484 - val_mse: 402199552.0000 - val_mae: 15747.7490\n",
            "Epoch 71/500\n",
            "60/60 [==============================] - 0s 929us/step - loss: 12458.0558 - mse: 289217248.0000 - mae: 12458.0566 - val_loss: 15762.0225 - val_mse: 397591360.0000 - val_mae: 15762.0225\n",
            "Epoch 72/500\n",
            "60/60 [==============================] - 0s 985us/step - loss: 11961.4532 - mse: 272452384.0000 - mae: 11961.4531 - val_loss: 15138.3924 - val_mse: 389468960.0000 - val_mae: 15138.3926\n",
            "Epoch 73/500\n",
            "60/60 [==============================] - 0s 927us/step - loss: 12368.2621 - mse: 283188096.0000 - mae: 12368.2627 - val_loss: 15091.2675 - val_mse: 384858720.0000 - val_mae: 15091.2676\n",
            "Epoch 74/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 11835.8475 - mse: 267664272.0000 - mae: 11835.8467 - val_loss: 15362.1652 - val_mse: 380949728.0000 - val_mae: 15362.1660\n",
            "Epoch 75/500\n",
            "60/60 [==============================] - 0s 911us/step - loss: 11807.8276 - mse: 264522240.0000 - mae: 11807.8271 - val_loss: 15341.4991 - val_mse: 375844672.0000 - val_mae: 15341.5000\n",
            "Epoch 76/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 11685.1678 - mse: 261132944.0000 - mae: 11685.1680 - val_loss: 14643.3820 - val_mse: 367369152.0000 - val_mae: 14643.3818\n",
            "Epoch 77/500\n",
            "60/60 [==============================] - 0s 954us/step - loss: 11693.4705 - mse: 257461680.0000 - mae: 11693.4697 - val_loss: 14600.0373 - val_mse: 362002464.0000 - val_mae: 14600.0371\n",
            "Epoch 78/500\n",
            "60/60 [==============================] - 0s 883us/step - loss: 11251.4381 - mse: 242319792.0000 - mae: 11251.4385 - val_loss: 14475.9990 - val_mse: 356067680.0000 - val_mae: 14475.9980\n",
            "Epoch 79/500\n",
            "60/60 [==============================] - 0s 890us/step - loss: 11466.5122 - mse: 252632688.0000 - mae: 11466.5127 - val_loss: 14731.6682 - val_mse: 352206112.0000 - val_mae: 14731.6689\n",
            "Epoch 80/500\n",
            "60/60 [==============================] - 0s 891us/step - loss: 11533.1746 - mse: 249841920.0000 - mae: 11533.1748 - val_loss: 14688.6541 - val_mse: 347214720.0000 - val_mae: 14688.6543\n",
            "Epoch 81/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 11015.4603 - mse: 237667024.0000 - mae: 11015.4600 - val_loss: 14605.9315 - val_mse: 342041248.0000 - val_mae: 14605.9316\n",
            "Epoch 82/500\n",
            "60/60 [==============================] - 0s 979us/step - loss: 11158.7255 - mse: 242763280.0000 - mae: 11158.7246 - val_loss: 14519.5598 - val_mse: 337176800.0000 - val_mae: 14519.5596\n",
            "Epoch 83/500\n",
            "60/60 [==============================] - 0s 959us/step - loss: 10997.2261 - mse: 237804592.0000 - mae: 10997.2256 - val_loss: 14440.2035 - val_mse: 332283872.0000 - val_mae: 14440.2041\n",
            "Epoch 84/500\n",
            "60/60 [==============================] - 0s 924us/step - loss: 11079.9608 - mse: 243242576.0000 - mae: 11079.9600 - val_loss: 13887.2803 - val_mse: 324638272.0000 - val_mae: 13887.2793\n",
            "Epoch 85/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 10456.0252 - mse: 212429840.0000 - mae: 10456.0254 - val_loss: 13783.6635 - val_mse: 318816800.0000 - val_mae: 13783.6631\n",
            "Epoch 86/500\n",
            "60/60 [==============================] - 0s 877us/step - loss: 10484.8322 - mse: 214604400.0000 - mae: 10484.8320 - val_loss: 13962.5175 - val_mse: 314723680.0000 - val_mae: 13962.5176\n",
            "Epoch 87/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 10126.7808 - mse: 209955776.0000 - mae: 10126.7812 - val_loss: 13191.5179 - val_mse: 306031968.0000 - val_mae: 13191.5176\n",
            "Epoch 88/500\n",
            "60/60 [==============================] - 0s 943us/step - loss: 10369.6948 - mse: 212702656.0000 - mae: 10369.6943 - val_loss: 13075.8948 - val_mse: 300614240.0000 - val_mae: 13075.8945\n",
            "Epoch 89/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 9960.5173 - mse: 193261344.0000 - mae: 9960.5176 - val_loss: 13183.7278 - val_mse: 296106528.0000 - val_mae: 13183.7285\n",
            "Epoch 90/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 10137.8467 - mse: 203940128.0000 - mae: 10137.8467 - val_loss: 13009.3166 - val_mse: 290677600.0000 - val_mae: 13009.3164\n",
            "Epoch 91/500\n",
            "60/60 [==============================] - 0s 885us/step - loss: 9607.3038 - mse: 183810080.0000 - mae: 9607.3027 - val_loss: 12882.8358 - val_mse: 284650880.0000 - val_mae: 12882.8350\n",
            "Epoch 92/500\n",
            "60/60 [==============================] - 0s 996us/step - loss: 9879.5094 - mse: 190934672.0000 - mae: 9879.5098 - val_loss: 13036.5717 - val_mse: 280834400.0000 - val_mae: 13036.5723\n",
            "Epoch 93/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 9522.8451 - mse: 190857664.0000 - mae: 9522.8457 - val_loss: 12885.7215 - val_mse: 275450912.0000 - val_mae: 12885.7207\n",
            "Epoch 94/500\n",
            "60/60 [==============================] - 0s 984us/step - loss: 9684.0075 - mse: 182594832.0000 - mae: 9684.0068 - val_loss: 12869.6736 - val_mse: 271045408.0000 - val_mae: 12869.6729\n",
            "Epoch 95/500\n",
            "60/60 [==============================] - 0s 978us/step - loss: 9689.3146 - mse: 196962240.0000 - mae: 9689.3145 - val_loss: 12268.2504 - val_mse: 263729936.0000 - val_mae: 12268.2500\n",
            "Epoch 96/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 9145.3105 - mse: 175250368.0000 - mae: 9145.3105 - val_loss: 12313.4420 - val_mse: 259306064.0000 - val_mae: 12313.4414\n",
            "Epoch 97/500\n",
            "60/60 [==============================] - 0s 965us/step - loss: 9530.6291 - mse: 180111248.0000 - mae: 9530.6289 - val_loss: 11836.9439 - val_mse: 252676560.0000 - val_mae: 11836.9434\n",
            "Epoch 98/500\n",
            "60/60 [==============================] - 0s 952us/step - loss: 9285.9858 - mse: 181099040.0000 - mae: 9285.9854 - val_loss: 12193.8117 - val_mse: 250077280.0000 - val_mae: 12193.8115\n",
            "Epoch 99/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 9468.1970 - mse: 185480864.0000 - mae: 9468.1982 - val_loss: 11678.4369 - val_mse: 243256384.0000 - val_mae: 11678.4375\n",
            "Epoch 100/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 8745.8556 - mse: 154289568.0000 - mae: 8745.8555 - val_loss: 11888.8942 - val_mse: 239742784.0000 - val_mae: 11888.8945\n",
            "Epoch 101/500\n",
            "60/60 [==============================] - 0s 899us/step - loss: 8751.4974 - mse: 157397056.0000 - mae: 8751.4971 - val_loss: 11712.2388 - val_mse: 234427632.0000 - val_mae: 11712.2383\n",
            "Epoch 102/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 9147.3138 - mse: 167356896.0000 - mae: 9147.3135 - val_loss: 11635.8764 - val_mse: 229965120.0000 - val_mae: 11635.8770\n",
            "Epoch 103/500\n",
            "60/60 [==============================] - 0s 993us/step - loss: 8921.4233 - mse: 175832608.0000 - mae: 8921.4238 - val_loss: 11177.4010 - val_mse: 224241104.0000 - val_mae: 11177.4014\n",
            "Epoch 104/500\n",
            "60/60 [==============================] - 0s 962us/step - loss: 9038.1317 - mse: 162618544.0000 - mae: 9038.1309 - val_loss: 11285.2540 - val_mse: 220803072.0000 - val_mae: 11285.2539\n",
            "Epoch 105/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 8829.7894 - mse: 160240896.0000 - mae: 8829.7900 - val_loss: 11185.9384 - val_mse: 216457072.0000 - val_mae: 11185.9375\n",
            "Epoch 106/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 8384.1286 - mse: 148761056.0000 - mae: 8384.1289 - val_loss: 11075.5917 - val_mse: 211718480.0000 - val_mae: 11075.5918\n",
            "Epoch 107/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 8310.4346 - mse: 154838816.0000 - mae: 8310.4346 - val_loss: 10640.0826 - val_mse: 205376016.0000 - val_mae: 10640.0820\n",
            "Epoch 108/500\n",
            "60/60 [==============================] - 0s 976us/step - loss: 8423.6624 - mse: 157199856.0000 - mae: 8423.6621 - val_loss: 10936.0036 - val_mse: 203117568.0000 - val_mae: 10936.0029\n",
            "Epoch 109/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 8576.4758 - mse: 145347776.0000 - mae: 8576.4756 - val_loss: 10529.8197 - val_mse: 197377568.0000 - val_mae: 10529.8213\n",
            "Epoch 110/500\n",
            "60/60 [==============================] - 0s 968us/step - loss: 8207.0487 - mse: 144510912.0000 - mae: 8207.0488 - val_loss: 10592.0372 - val_mse: 193995056.0000 - val_mae: 10592.0371\n",
            "Epoch 111/500\n",
            "60/60 [==============================] - 0s 848us/step - loss: 8003.1499 - mse: 140843568.0000 - mae: 8003.1494 - val_loss: 10498.1555 - val_mse: 189754608.0000 - val_mae: 10498.1562\n",
            "Epoch 112/500\n",
            "60/60 [==============================] - 0s 911us/step - loss: 7134.3776 - mse: 112188456.0000 - mae: 7134.3779 - val_loss: 10273.5650 - val_mse: 184819536.0000 - val_mae: 10273.5645\n",
            "Epoch 113/500\n",
            "60/60 [==============================] - 0s 950us/step - loss: 8016.0121 - mse: 139974992.0000 - mae: 8016.0112 - val_loss: 9881.5227 - val_mse: 179376848.0000 - val_mae: 9881.5225\n",
            "Epoch 114/500\n",
            "60/60 [==============================] - 0s 919us/step - loss: 7499.8108 - mse: 121645360.0000 - mae: 7499.8115 - val_loss: 9949.5769 - val_mse: 175966288.0000 - val_mae: 9949.5771\n",
            "Epoch 115/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 7431.6908 - mse: 117025752.0000 - mae: 7431.6904 - val_loss: 9799.4654 - val_mse: 171809376.0000 - val_mae: 9799.4658\n",
            "Epoch 116/500\n",
            "60/60 [==============================] - 0s 965us/step - loss: 7718.9590 - mse: 122536808.0000 - mae: 7718.9595 - val_loss: 9609.8029 - val_mse: 167637872.0000 - val_mae: 9609.8018\n",
            "Epoch 117/500\n",
            "60/60 [==============================] - 0s 960us/step - loss: 7543.2625 - mse: 130919496.0000 - mae: 7543.2632 - val_loss: 9636.5102 - val_mse: 165213600.0000 - val_mae: 9636.5107\n",
            "Epoch 118/500\n",
            "60/60 [==============================] - 0s 950us/step - loss: 7092.6454 - mse: 115904544.0000 - mae: 7092.6460 - val_loss: 9771.8642 - val_mse: 163194816.0000 - val_mae: 9771.8643\n",
            "Epoch 119/500\n",
            "60/60 [==============================] - 0s 908us/step - loss: 6684.9787 - mse: 105969032.0000 - mae: 6684.9790 - val_loss: 9475.1448 - val_mse: 158630304.0000 - val_mae: 9475.1445\n",
            "Epoch 120/500\n",
            "60/60 [==============================] - 0s 950us/step - loss: 6313.2175 - mse: 86983800.0000 - mae: 6313.2178 - val_loss: 9278.9161 - val_mse: 154323312.0000 - val_mae: 9278.9160\n",
            "Epoch 121/500\n",
            "60/60 [==============================] - 0s 902us/step - loss: 6151.5367 - mse: 88158816.0000 - mae: 6151.5366 - val_loss: 9124.5330 - val_mse: 150268480.0000 - val_mae: 9124.5332\n",
            "Epoch 122/500\n",
            "60/60 [==============================] - 0s 880us/step - loss: 7129.6967 - mse: 114104776.0000 - mae: 7129.6963 - val_loss: 8810.2759 - val_mse: 145894704.0000 - val_mae: 8810.2764\n",
            "Epoch 123/500\n",
            "60/60 [==============================] - 0s 917us/step - loss: 6551.1617 - mse: 84806480.0000 - mae: 6551.1621 - val_loss: 8871.9385 - val_mse: 142927856.0000 - val_mae: 8871.9385\n",
            "Epoch 124/500\n",
            "60/60 [==============================] - 0s 849us/step - loss: 6574.6228 - mse: 89812072.0000 - mae: 6574.6226 - val_loss: 8985.4124 - val_mse: 140503696.0000 - val_mae: 8985.4121\n",
            "Epoch 125/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 6626.9788 - mse: 94133968.0000 - mae: 6626.9790 - val_loss: 8858.9325 - val_mse: 136623504.0000 - val_mae: 8858.9326\n",
            "Epoch 126/500\n",
            "60/60 [==============================] - 0s 904us/step - loss: 6488.0032 - mse: 91160984.0000 - mae: 6488.0029 - val_loss: 8452.5062 - val_mse: 131842768.0000 - val_mae: 8452.5059\n",
            "Epoch 127/500\n",
            "60/60 [==============================] - 0s 880us/step - loss: 6190.0640 - mse: 90083080.0000 - mae: 6190.0640 - val_loss: 8488.3957 - val_mse: 129119952.0000 - val_mae: 8488.3955\n",
            "Epoch 128/500\n",
            "60/60 [==============================] - 0s 846us/step - loss: 6449.5264 - mse: 88071712.0000 - mae: 6449.5259 - val_loss: 8236.0495 - val_mse: 125631968.0000 - val_mae: 8236.0498\n",
            "Epoch 129/500\n",
            "60/60 [==============================] - 0s 969us/step - loss: 5616.1969 - mse: 68342608.0000 - mae: 5616.1968 - val_loss: 8229.4097 - val_mse: 122175368.0000 - val_mae: 8229.4102\n",
            "Epoch 130/500\n",
            "60/60 [==============================] - 0s 974us/step - loss: 6314.9983 - mse: 86245632.0000 - mae: 6314.9985 - val_loss: 8353.1171 - val_mse: 121126248.0000 - val_mae: 8353.1162\n",
            "Epoch 131/500\n",
            "60/60 [==============================] - 0s 945us/step - loss: 5871.5218 - mse: 81422456.0000 - mae: 5871.5220 - val_loss: 8007.2481 - val_mse: 116897408.0000 - val_mae: 8007.2480\n",
            "Epoch 132/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5721.7257 - mse: 75925104.0000 - mae: 5721.7261 - val_loss: 7657.3944 - val_mse: 113001712.0000 - val_mae: 7657.3950\n",
            "Epoch 133/500\n",
            "60/60 [==============================] - 0s 944us/step - loss: 5873.3699 - mse: 73055456.0000 - mae: 5873.3696 - val_loss: 7694.4729 - val_mse: 110949376.0000 - val_mae: 7694.4722\n",
            "Epoch 134/500\n",
            "60/60 [==============================] - 0s 885us/step - loss: 6344.6381 - mse: 95428752.0000 - mae: 6344.6382 - val_loss: 7937.5757 - val_mse: 110960160.0000 - val_mae: 7937.5757\n",
            "Epoch 135/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5505.7949 - mse: 73542688.0000 - mae: 5505.7949 - val_loss: 7594.1494 - val_mse: 107187920.0000 - val_mae: 7594.1489\n",
            "Epoch 136/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 6362.5649 - mse: 88807056.0000 - mae: 6362.5649 - val_loss: 7544.2958 - val_mse: 105857024.0000 - val_mae: 7544.2964\n",
            "Epoch 137/500\n",
            "60/60 [==============================] - 0s 882us/step - loss: 5332.5605 - mse: 70769816.0000 - mae: 5332.5605 - val_loss: 7402.0924 - val_mse: 103415608.0000 - val_mae: 7402.0923\n",
            "Epoch 138/500\n",
            "60/60 [==============================] - 0s 950us/step - loss: 5289.5386 - mse: 61371940.0000 - mae: 5289.5386 - val_loss: 7535.2868 - val_mse: 102096376.0000 - val_mae: 7535.2871\n",
            "Epoch 139/500\n",
            "60/60 [==============================] - 0s 975us/step - loss: 5634.1872 - mse: 75647280.0000 - mae: 5634.1865 - val_loss: 7408.4766 - val_mse: 100123440.0000 - val_mae: 7408.4766\n",
            "Epoch 140/500\n",
            "60/60 [==============================] - 0s 1000us/step - loss: 6127.1511 - mse: 82615048.0000 - mae: 6127.1504 - val_loss: 7590.1761 - val_mse: 100145192.0000 - val_mae: 7590.1763\n",
            "Epoch 141/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 6463.6620 - mse: 86664040.0000 - mae: 6463.6621 - val_loss: 7189.9473 - val_mse: 96176664.0000 - val_mae: 7189.9473\n",
            "Epoch 142/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5488.3676 - mse: 73791528.0000 - mae: 5488.3677 - val_loss: 7205.6370 - val_mse: 95340424.0000 - val_mae: 7205.6372\n",
            "Epoch 143/500\n",
            "60/60 [==============================] - 0s 939us/step - loss: 5412.0813 - mse: 71191232.0000 - mae: 5412.0811 - val_loss: 6894.8607 - val_mse: 91902856.0000 - val_mae: 6894.8604\n",
            "Epoch 144/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5474.5133 - mse: 64327980.0000 - mae: 5474.5132 - val_loss: 6771.0827 - val_mse: 90078392.0000 - val_mae: 6771.0830\n",
            "Epoch 145/500\n",
            "60/60 [==============================] - 0s 906us/step - loss: 5916.6979 - mse: 73338880.0000 - mae: 5916.6978 - val_loss: 6979.3139 - val_mse: 89364648.0000 - val_mae: 6979.3140\n",
            "Epoch 146/500\n",
            "60/60 [==============================] - 0s 864us/step - loss: 5522.0354 - mse: 59157640.0000 - mae: 5522.0356 - val_loss: 6860.8676 - val_mse: 87144656.0000 - val_mae: 6860.8677\n",
            "Epoch 147/500\n",
            "60/60 [==============================] - 0s 947us/step - loss: 5112.8986 - mse: 61815996.0000 - mae: 5112.8984 - val_loss: 6912.7953 - val_mse: 86421784.0000 - val_mae: 6912.7954\n",
            "Epoch 148/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5106.4412 - mse: 60913640.0000 - mae: 5106.4419 - val_loss: 6591.5550 - val_mse: 82996736.0000 - val_mae: 6591.5552\n",
            "Epoch 149/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5348.0573 - mse: 65640640.0000 - mae: 5348.0571 - val_loss: 6741.7811 - val_mse: 82303128.0000 - val_mae: 6741.7812\n",
            "Epoch 150/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5553.5127 - mse: 69536352.0000 - mae: 5553.5127 - val_loss: 6594.7489 - val_mse: 79980456.0000 - val_mae: 6594.7490\n",
            "Epoch 151/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4608.9968 - mse: 47954304.0000 - mae: 4608.9971 - val_loss: 6564.5019 - val_mse: 78656200.0000 - val_mae: 6564.5020\n",
            "Epoch 152/500\n",
            "60/60 [==============================] - 0s 976us/step - loss: 5188.8847 - mse: 62394652.0000 - mae: 5188.8843 - val_loss: 6242.8907 - val_mse: 75777280.0000 - val_mae: 6242.8906\n",
            "Epoch 153/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5272.7467 - mse: 58642076.0000 - mae: 5272.7466 - val_loss: 6286.6075 - val_mse: 74808624.0000 - val_mae: 6286.6074\n",
            "Epoch 154/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5568.7555 - mse: 69122176.0000 - mae: 5568.7554 - val_loss: 6020.0943 - val_mse: 72922200.0000 - val_mae: 6020.0942\n",
            "Epoch 155/500\n",
            "60/60 [==============================] - 0s 922us/step - loss: 4224.2124 - mse: 41133900.0000 - mae: 4224.2124 - val_loss: 6207.5757 - val_mse: 72968720.0000 - val_mae: 6207.5762\n",
            "Epoch 156/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5277.1694 - mse: 63936924.0000 - mae: 5277.1694 - val_loss: 6144.5059 - val_mse: 71690704.0000 - val_mae: 6144.5063\n",
            "Epoch 157/500\n",
            "60/60 [==============================] - 0s 866us/step - loss: 4761.5011 - mse: 46867268.0000 - mae: 4761.5010 - val_loss: 6085.7244 - val_mse: 69838808.0000 - val_mae: 6085.7246\n",
            "Epoch 158/500\n",
            "60/60 [==============================] - 0s 855us/step - loss: 4610.0498 - mse: 44460408.0000 - mae: 4610.0498 - val_loss: 5979.7460 - val_mse: 68642608.0000 - val_mae: 5979.7456\n",
            "Epoch 159/500\n",
            "60/60 [==============================] - 0s 844us/step - loss: 5198.1618 - mse: 55448268.0000 - mae: 5198.1621 - val_loss: 6238.4277 - val_mse: 69183112.0000 - val_mae: 6238.4282\n",
            "Epoch 160/500\n",
            "60/60 [==============================] - 0s 851us/step - loss: 5064.3269 - mse: 58240632.0000 - mae: 5064.3267 - val_loss: 5835.7920 - val_mse: 66301788.0000 - val_mae: 5835.7915\n",
            "Epoch 161/500\n",
            "60/60 [==============================] - 0s 828us/step - loss: 5003.8935 - mse: 52933020.0000 - mae: 5003.8936 - val_loss: 6110.6805 - val_mse: 67591488.0000 - val_mae: 6110.6807\n",
            "Epoch 162/500\n",
            "60/60 [==============================] - 0s 841us/step - loss: 4826.7761 - mse: 54739880.0000 - mae: 4826.7764 - val_loss: 5474.7442 - val_mse: 63414656.0000 - val_mae: 5474.7441\n",
            "Epoch 163/500\n",
            "60/60 [==============================] - 0s 824us/step - loss: 4207.4047 - mse: 36389524.0000 - mae: 4207.4048 - val_loss: 5886.1521 - val_mse: 64905352.0000 - val_mae: 5886.1519\n",
            "Epoch 164/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4544.7094 - mse: 50217540.0000 - mae: 4544.7095 - val_loss: 5726.4919 - val_mse: 63297400.0000 - val_mae: 5726.4917\n",
            "Epoch 165/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3868.5879 - mse: 37764584.0000 - mae: 3868.5881 - val_loss: 5611.9410 - val_mse: 62005340.0000 - val_mae: 5611.9409\n",
            "Epoch 166/500\n",
            "60/60 [==============================] - 0s 868us/step - loss: 4687.7850 - mse: 41157672.0000 - mae: 4687.7847 - val_loss: 5526.4146 - val_mse: 60599796.0000 - val_mae: 5526.4146\n",
            "Epoch 167/500\n",
            "60/60 [==============================] - 0s 825us/step - loss: 5178.3101 - mse: 63069944.0000 - mae: 5178.3105 - val_loss: 5565.5757 - val_mse: 60238420.0000 - val_mae: 5565.5757\n",
            "Epoch 168/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4566.2118 - mse: 42295244.0000 - mae: 4566.2114 - val_loss: 5785.4761 - val_mse: 60994124.0000 - val_mae: 5785.4761\n",
            "Epoch 169/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4801.9806 - mse: 47251560.0000 - mae: 4801.9810 - val_loss: 5385.1520 - val_mse: 58050620.0000 - val_mae: 5385.1519\n",
            "Epoch 170/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4984.4261 - mse: 54788760.0000 - mae: 4984.4263 - val_loss: 5461.4910 - val_mse: 57820420.0000 - val_mae: 5461.4907\n",
            "Epoch 171/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4587.7954 - mse: 51248108.0000 - mae: 4587.7959 - val_loss: 5632.1039 - val_mse: 58319592.0000 - val_mae: 5632.1040\n",
            "Epoch 172/500\n",
            "60/60 [==============================] - 0s 976us/step - loss: 4817.2607 - mse: 48071864.0000 - mae: 4817.2607 - val_loss: 5578.2423 - val_mse: 57324508.0000 - val_mae: 5578.2422\n",
            "Epoch 173/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4591.9601 - mse: 49216064.0000 - mae: 4591.9600 - val_loss: 5227.5393 - val_mse: 54676556.0000 - val_mae: 5227.5391\n",
            "Epoch 174/500\n",
            "60/60 [==============================] - 0s 990us/step - loss: 4522.5136 - mse: 44304164.0000 - mae: 4522.5137 - val_loss: 5299.2763 - val_mse: 54715644.0000 - val_mae: 5299.2759\n",
            "Epoch 175/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4478.3034 - mse: 51155284.0000 - mae: 4478.3037 - val_loss: 5420.5168 - val_mse: 55258864.0000 - val_mae: 5420.5166\n",
            "Epoch 176/500\n",
            "60/60 [==============================] - 0s 895us/step - loss: 4694.8070 - mse: 50589756.0000 - mae: 4694.8071 - val_loss: 4943.8990 - val_mse: 51751332.0000 - val_mae: 4943.8989\n",
            "Epoch 177/500\n",
            "60/60 [==============================] - 0s 924us/step - loss: 4849.5802 - mse: 51673052.0000 - mae: 4849.5801 - val_loss: 5157.3403 - val_mse: 52478548.0000 - val_mae: 5157.3408\n",
            "Epoch 178/500\n",
            "60/60 [==============================] - 0s 979us/step - loss: 4586.3781 - mse: 46218312.0000 - mae: 4586.3779 - val_loss: 5208.2694 - val_mse: 51993720.0000 - val_mae: 5208.2700\n",
            "Epoch 179/500\n",
            "60/60 [==============================] - 0s 971us/step - loss: 5208.4328 - mse: 58915748.0000 - mae: 5208.4331 - val_loss: 5057.7467 - val_mse: 51005116.0000 - val_mae: 5057.7471\n",
            "Epoch 180/500\n",
            "60/60 [==============================] - 0s 993us/step - loss: 5054.1772 - mse: 54543248.0000 - mae: 5054.1772 - val_loss: 4910.4629 - val_mse: 49484500.0000 - val_mae: 4910.4629\n",
            "Epoch 181/500\n",
            "60/60 [==============================] - 0s 950us/step - loss: 4392.6990 - mse: 41952696.0000 - mae: 4392.6987 - val_loss: 4884.0851 - val_mse: 48906404.0000 - val_mae: 4884.0850\n",
            "Epoch 182/500\n",
            "60/60 [==============================] - 0s 971us/step - loss: 5197.6882 - mse: 58248308.0000 - mae: 5197.6885 - val_loss: 5128.2081 - val_mse: 50382764.0000 - val_mae: 5128.2080\n",
            "Epoch 183/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4374.4176 - mse: 39220564.0000 - mae: 4374.4175 - val_loss: 5055.7681 - val_mse: 49651620.0000 - val_mae: 5055.7681\n",
            "Epoch 184/500\n",
            "60/60 [==============================] - 0s 917us/step - loss: 5030.9258 - mse: 56461620.0000 - mae: 5030.9263 - val_loss: 5052.2338 - val_mse: 49362116.0000 - val_mae: 5052.2339\n",
            "Epoch 185/500\n",
            "60/60 [==============================] - 0s 936us/step - loss: 4274.2731 - mse: 42397444.0000 - mae: 4274.2729 - val_loss: 5152.2680 - val_mse: 49583412.0000 - val_mae: 5152.2676\n",
            "Epoch 186/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4625.4659 - mse: 49905992.0000 - mae: 4625.4658 - val_loss: 4674.4678 - val_mse: 46541684.0000 - val_mae: 4674.4678\n",
            "Epoch 187/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5679.0228 - mse: 67779544.0000 - mae: 5679.0229 - val_loss: 4982.6273 - val_mse: 48059912.0000 - val_mae: 4982.6274\n",
            "Epoch 188/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5049.5372 - mse: 57741508.0000 - mae: 5049.5376 - val_loss: 5246.6720 - val_mse: 49575236.0000 - val_mae: 5246.6719\n",
            "Epoch 189/500\n",
            "60/60 [==============================] - 0s 924us/step - loss: 3990.3457 - mse: 36672980.0000 - mae: 3990.3455 - val_loss: 4736.8649 - val_mse: 46492560.0000 - val_mae: 4736.8647\n",
            "Epoch 190/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4198.2489 - mse: 41262316.0000 - mae: 4198.2490 - val_loss: 4948.3699 - val_mse: 47658640.0000 - val_mae: 4948.3696\n",
            "Epoch 191/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4974.1863 - mse: 52551432.0000 - mae: 4974.1865 - val_loss: 4906.8275 - val_mse: 47106772.0000 - val_mae: 4906.8276\n",
            "Epoch 192/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4061.4913 - mse: 33996800.0000 - mae: 4061.4912 - val_loss: 5261.7291 - val_mse: 48607516.0000 - val_mae: 5261.7290\n",
            "Epoch 193/500\n",
            "60/60 [==============================] - 0s 941us/step - loss: 4533.3844 - mse: 44220748.0000 - mae: 4533.3843 - val_loss: 4911.5067 - val_mse: 46556080.0000 - val_mae: 4911.5068\n",
            "Epoch 194/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4089.0285 - mse: 38701800.0000 - mae: 4089.0283 - val_loss: 4755.7827 - val_mse: 45349184.0000 - val_mae: 4755.7827\n",
            "Epoch 195/500\n",
            "60/60 [==============================] - 0s 930us/step - loss: 4814.4953 - mse: 51021464.0000 - mae: 4814.4951 - val_loss: 4751.4605 - val_mse: 45088000.0000 - val_mae: 4751.4604\n",
            "Epoch 196/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4513.9514 - mse: 48598268.0000 - mae: 4513.9517 - val_loss: 5064.5816 - val_mse: 46843352.0000 - val_mae: 5064.5815\n",
            "Epoch 197/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4561.1221 - mse: 46461812.0000 - mae: 4561.1221 - val_loss: 5036.1087 - val_mse: 46107888.0000 - val_mae: 5036.1089\n",
            "Epoch 198/500\n",
            "60/60 [==============================] - 0s 964us/step - loss: 4077.2209 - mse: 41832868.0000 - mae: 4077.2209 - val_loss: 5012.8002 - val_mse: 45692628.0000 - val_mae: 5012.7998\n",
            "Epoch 199/500\n",
            "60/60 [==============================] - 0s 916us/step - loss: 3918.5253 - mse: 38866148.0000 - mae: 3918.5251 - val_loss: 4812.0292 - val_mse: 44538896.0000 - val_mae: 4812.0293\n",
            "Epoch 200/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4311.0729 - mse: 44684964.0000 - mae: 4311.0728 - val_loss: 4586.8663 - val_mse: 43077368.0000 - val_mae: 4586.8662\n",
            "Epoch 201/500\n",
            "60/60 [==============================] - 0s 896us/step - loss: 4706.8049 - mse: 47378328.0000 - mae: 4706.8047 - val_loss: 5121.0809 - val_mse: 45541412.0000 - val_mae: 5121.0806\n",
            "Epoch 202/500\n",
            "60/60 [==============================] - 0s 876us/step - loss: 4395.8689 - mse: 46489008.0000 - mae: 4395.8687 - val_loss: 4996.1492 - val_mse: 44549504.0000 - val_mae: 4996.1489\n",
            "Epoch 203/500\n",
            "60/60 [==============================] - 0s 958us/step - loss: 3242.9189 - mse: 26172070.0000 - mae: 3242.9187 - val_loss: 4854.8289 - val_mse: 43674972.0000 - val_mae: 4854.8286\n",
            "Epoch 204/500\n",
            "60/60 [==============================] - 0s 995us/step - loss: 4631.2578 - mse: 46927792.0000 - mae: 4631.2578 - val_loss: 4689.1763 - val_mse: 42411260.0000 - val_mae: 4689.1763\n",
            "Epoch 205/500\n",
            "60/60 [==============================] - 0s 919us/step - loss: 3817.4745 - mse: 31061752.0000 - mae: 3817.4744 - val_loss: 4784.1892 - val_mse: 42767940.0000 - val_mae: 4784.1895\n",
            "Epoch 206/500\n",
            "60/60 [==============================] - 0s 970us/step - loss: 4158.4181 - mse: 40267220.0000 - mae: 4158.4185 - val_loss: 4958.5259 - val_mse: 43789748.0000 - val_mae: 4958.5259\n",
            "Epoch 207/500\n",
            "60/60 [==============================] - 0s 983us/step - loss: 4357.8858 - mse: 36758584.0000 - mae: 4357.8857 - val_loss: 5219.3010 - val_mse: 45410804.0000 - val_mae: 5219.3013\n",
            "Epoch 208/500\n",
            "60/60 [==============================] - 0s 970us/step - loss: 5126.5309 - mse: 52613236.0000 - mae: 5126.5312 - val_loss: 5206.9418 - val_mse: 45275056.0000 - val_mae: 5206.9419\n",
            "Epoch 209/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4460.6946 - mse: 40807000.0000 - mae: 4460.6948 - val_loss: 4755.4110 - val_mse: 41753616.0000 - val_mae: 4755.4111\n",
            "Epoch 210/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3911.5493 - mse: 32388896.0000 - mae: 3911.5496 - val_loss: 4657.1487 - val_mse: 40871340.0000 - val_mae: 4657.1489\n",
            "Epoch 211/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3754.4841 - mse: 35853268.0000 - mae: 3754.4841 - val_loss: 4832.0704 - val_mse: 41330876.0000 - val_mae: 4832.0703\n",
            "Epoch 212/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4706.8754 - mse: 51403624.0000 - mae: 4706.8755 - val_loss: 4536.3437 - val_mse: 39628412.0000 - val_mae: 4536.3438\n",
            "Epoch 213/500\n",
            "60/60 [==============================] - 0s 996us/step - loss: 3689.8436 - mse: 36414780.0000 - mae: 3689.8438 - val_loss: 4797.5764 - val_mse: 40750232.0000 - val_mae: 4797.5762\n",
            "Epoch 214/500\n",
            "60/60 [==============================] - 0s 979us/step - loss: 4528.6101 - mse: 46339944.0000 - mae: 4528.6099 - val_loss: 4766.2426 - val_mse: 40543444.0000 - val_mae: 4766.2427\n",
            "Epoch 215/500\n",
            "60/60 [==============================] - 0s 976us/step - loss: 4065.6653 - mse: 37848484.0000 - mae: 4065.6653 - val_loss: 4534.5242 - val_mse: 38570340.0000 - val_mae: 4534.5244\n",
            "Epoch 216/500\n",
            "60/60 [==============================] - 0s 943us/step - loss: 4125.9594 - mse: 33063840.0000 - mae: 4125.9595 - val_loss: 4611.4595 - val_mse: 38618116.0000 - val_mae: 4611.4595\n",
            "Epoch 217/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4139.8197 - mse: 36455096.0000 - mae: 4139.8198 - val_loss: 4555.5787 - val_mse: 37941084.0000 - val_mae: 4555.5786\n",
            "Epoch 218/500\n",
            "60/60 [==============================] - 0s 983us/step - loss: 4735.8975 - mse: 55012564.0000 - mae: 4735.8975 - val_loss: 4778.8065 - val_mse: 39066712.0000 - val_mae: 4778.8062\n",
            "Epoch 219/500\n",
            "60/60 [==============================] - 0s 996us/step - loss: 4416.8140 - mse: 39302392.0000 - mae: 4416.8140 - val_loss: 4634.8594 - val_mse: 38141848.0000 - val_mae: 4634.8594\n",
            "Epoch 220/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4704.6110 - mse: 48360804.0000 - mae: 4704.6104 - val_loss: 4665.1166 - val_mse: 38100488.0000 - val_mae: 4665.1167\n",
            "Epoch 221/500\n",
            "60/60 [==============================] - 0s 936us/step - loss: 4425.8754 - mse: 41716300.0000 - mae: 4425.8755 - val_loss: 4665.6794 - val_mse: 38072056.0000 - val_mae: 4665.6792\n",
            "Epoch 222/500\n",
            "60/60 [==============================] - 0s 996us/step - loss: 4481.0769 - mse: 45970064.0000 - mae: 4481.0771 - val_loss: 4655.2363 - val_mse: 37594632.0000 - val_mae: 4655.2363\n",
            "Epoch 223/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3633.2259 - mse: 27978296.0000 - mae: 3633.2258 - val_loss: 4262.4400 - val_mse: 35546084.0000 - val_mae: 4262.4399\n",
            "Epoch 224/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3931.8617 - mse: 36106744.0000 - mae: 3931.8618 - val_loss: 4416.3723 - val_mse: 36103004.0000 - val_mae: 4416.3726\n",
            "Epoch 225/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3648.0590 - mse: 26979320.0000 - mae: 3648.0588 - val_loss: 4683.1081 - val_mse: 37508388.0000 - val_mae: 4683.1084\n",
            "Epoch 226/500\n",
            "60/60 [==============================] - 0s 932us/step - loss: 3796.9090 - mse: 31577674.0000 - mae: 3796.9089 - val_loss: 4867.1998 - val_mse: 38511944.0000 - val_mae: 4867.2002\n",
            "Epoch 227/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4163.1677 - mse: 38711772.0000 - mae: 4163.1675 - val_loss: 4595.8490 - val_mse: 36200668.0000 - val_mae: 4595.8491\n",
            "Epoch 228/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5785.3233 - mse: 72131944.0000 - mae: 5785.3232 - val_loss: 4630.1706 - val_mse: 36486024.0000 - val_mae: 4630.1704\n",
            "Epoch 229/500\n",
            "60/60 [==============================] - 0s 940us/step - loss: 4216.4544 - mse: 40285692.0000 - mae: 4216.4546 - val_loss: 4636.1712 - val_mse: 36120720.0000 - val_mae: 4636.1714\n",
            "Epoch 230/500\n",
            "60/60 [==============================] - 0s 953us/step - loss: 3708.7221 - mse: 30307840.0000 - mae: 3708.7222 - val_loss: 4501.6295 - val_mse: 35082176.0000 - val_mae: 4501.6299\n",
            "Epoch 231/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4621.2400 - mse: 47724584.0000 - mae: 4621.2402 - val_loss: 4856.8951 - val_mse: 37756788.0000 - val_mae: 4856.8955\n",
            "Epoch 232/500\n",
            "60/60 [==============================] - 0s 869us/step - loss: 4239.4765 - mse: 33393070.0000 - mae: 4239.4766 - val_loss: 4731.6861 - val_mse: 36864360.0000 - val_mae: 4731.6865\n",
            "Epoch 233/500\n",
            "60/60 [==============================] - 0s 941us/step - loss: 3685.3270 - mse: 33861948.0000 - mae: 3685.3271 - val_loss: 4720.3854 - val_mse: 36660916.0000 - val_mae: 4720.3853\n",
            "Epoch 234/500\n",
            "60/60 [==============================] - 0s 888us/step - loss: 3654.9460 - mse: 29944576.0000 - mae: 3654.9463 - val_loss: 4829.4992 - val_mse: 37479748.0000 - val_mae: 4829.4990\n",
            "Epoch 235/500\n",
            "60/60 [==============================] - 0s 957us/step - loss: 3822.4714 - mse: 31562420.0000 - mae: 3822.4714 - val_loss: 4653.2023 - val_mse: 36095708.0000 - val_mae: 4653.2021\n",
            "Epoch 236/500\n",
            "60/60 [==============================] - 0s 865us/step - loss: 3423.3098 - mse: 30290888.0000 - mae: 3423.3098 - val_loss: 4663.8348 - val_mse: 35692328.0000 - val_mae: 4663.8350\n",
            "Epoch 237/500\n",
            "60/60 [==============================] - 0s 979us/step - loss: 4864.6654 - mse: 57880240.0000 - mae: 4864.6655 - val_loss: 4464.2316 - val_mse: 34238432.0000 - val_mae: 4464.2319\n",
            "Epoch 238/500\n",
            "60/60 [==============================] - 0s 896us/step - loss: 3939.4076 - mse: 28289490.0000 - mae: 3939.4075 - val_loss: 4701.8181 - val_mse: 35776180.0000 - val_mae: 4701.8179\n",
            "Epoch 239/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3607.0481 - mse: 26457842.0000 - mae: 3607.0479 - val_loss: 4604.9861 - val_mse: 34933576.0000 - val_mae: 4604.9858\n",
            "Epoch 240/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4237.1783 - mse: 36895568.0000 - mae: 4237.1782 - val_loss: 4585.3738 - val_mse: 34645408.0000 - val_mae: 4585.3735\n",
            "Epoch 241/500\n",
            "60/60 [==============================] - 0s 912us/step - loss: 4363.3247 - mse: 46974360.0000 - mae: 4363.3247 - val_loss: 4486.6583 - val_mse: 33651716.0000 - val_mae: 4486.6582\n",
            "Epoch 242/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3705.9231 - mse: 32562748.0000 - mae: 3705.9231 - val_loss: 4633.7095 - val_mse: 34359184.0000 - val_mae: 4633.7095\n",
            "Epoch 243/500\n",
            "60/60 [==============================] - 0s 978us/step - loss: 4053.2854 - mse: 32660550.0000 - mae: 4053.2854 - val_loss: 4495.9283 - val_mse: 33347520.0000 - val_mae: 4495.9282\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 80 samples, validate on 70 samples\n",
            "Epoch 1/500\n",
            "80/80 [==============================] - 2s 27ms/step - loss: 22886.3989 - mse: 795738368.0000 - mae: 22886.3984 - val_loss: 23427.2409 - val_mse: 801285120.0000 - val_mae: 23427.2402\n",
            "Epoch 2/500\n",
            "80/80 [==============================] - 0s 783us/step - loss: 22883.2197 - mse: 795654784.0000 - mae: 22883.2227 - val_loss: 23419.4479 - val_mse: 801046080.0000 - val_mae: 23419.4473\n",
            "Epoch 3/500\n",
            "80/80 [==============================] - 0s 784us/step - loss: 22872.6138 - mse: 795308928.0000 - mae: 22872.6133 - val_loss: 23402.8792 - val_mse: 800394304.0000 - val_mae: 23402.8789\n",
            "Epoch 4/500\n",
            "80/80 [==============================] - 0s 887us/step - loss: 22855.1138 - mse: 794721920.0000 - mae: 22855.1133 - val_loss: 23381.1119 - val_mse: 799437824.0000 - val_mae: 23381.1113\n",
            "Epoch 5/500\n",
            "80/80 [==============================] - 0s 893us/step - loss: 22830.1003 - mse: 793348608.0000 - mae: 22830.0996 - val_loss: 23352.8524 - val_mse: 798154752.0000 - val_mae: 23352.8516\n",
            "Epoch 6/500\n",
            "80/80 [==============================] - 0s 789us/step - loss: 22802.0649 - mse: 792534144.0000 - mae: 22802.0664 - val_loss: 23321.1515 - val_mse: 796725056.0000 - val_mae: 23321.1504\n",
            "Epoch 7/500\n",
            "80/80 [==============================] - 0s 814us/step - loss: 22770.7004 - mse: 790964416.0000 - mae: 22770.7012 - val_loss: 23285.2612 - val_mse: 795091584.0000 - val_mae: 23285.2598\n",
            "Epoch 8/500\n",
            "80/80 [==============================] - 0s 901us/step - loss: 22730.2456 - mse: 788798336.0000 - mae: 22730.2441 - val_loss: 23243.7615 - val_mse: 793194688.0000 - val_mae: 23243.7598\n",
            "Epoch 9/500\n",
            "80/80 [==============================] - 0s 874us/step - loss: 22679.3300 - mse: 786493248.0000 - mae: 22679.3320 - val_loss: 23196.5626 - val_mse: 791038912.0000 - val_mae: 23196.5645\n",
            "Epoch 10/500\n",
            "80/80 [==============================] - 0s 841us/step - loss: 22636.0758 - mse: 784813952.0000 - mae: 22636.0762 - val_loss: 23145.2494 - val_mse: 788698048.0000 - val_mae: 23145.2500\n",
            "Epoch 11/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 22585.9521 - mse: 783220032.0000 - mae: 22585.9512 - val_loss: 23087.9994 - val_mse: 786093184.0000 - val_mae: 23087.9980\n",
            "Epoch 12/500\n",
            "80/80 [==============================] - 0s 792us/step - loss: 22538.7532 - mse: 780493248.0000 - mae: 22538.7539 - val_loss: 23027.8171 - val_mse: 783351104.0000 - val_mae: 23027.8184\n",
            "Epoch 13/500\n",
            "80/80 [==============================] - 0s 868us/step - loss: 22443.6200 - mse: 775694144.0000 - mae: 22443.6211 - val_loss: 22956.8117 - val_mse: 780116160.0000 - val_mae: 22956.8105\n",
            "Epoch 14/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 22404.5685 - mse: 773940480.0000 - mae: 22404.5684 - val_loss: 22889.9044 - val_mse: 777070656.0000 - val_mae: 22889.9043\n",
            "Epoch 15/500\n",
            "80/80 [==============================] - 0s 772us/step - loss: 22355.4348 - mse: 770692992.0000 - mae: 22355.4375 - val_loss: 22822.7033 - val_mse: 774015296.0000 - val_mae: 22822.7031\n",
            "Epoch 16/500\n",
            "80/80 [==============================] - 0s 795us/step - loss: 22263.6063 - mse: 766402112.0000 - mae: 22263.6055 - val_loss: 22747.2890 - val_mse: 770597120.0000 - val_mae: 22747.2891\n",
            "Epoch 17/500\n",
            "80/80 [==============================] - 0s 895us/step - loss: 22209.2341 - mse: 763950976.0000 - mae: 22209.2363 - val_loss: 22672.5672 - val_mse: 767217408.0000 - val_mae: 22672.5684\n",
            "Epoch 18/500\n",
            "80/80 [==============================] - 0s 767us/step - loss: 22187.4370 - mse: 762554880.0000 - mae: 22187.4375 - val_loss: 22599.6837 - val_mse: 763930240.0000 - val_mae: 22599.6836\n",
            "Epoch 19/500\n",
            "80/80 [==============================] - 0s 890us/step - loss: 22089.3599 - mse: 758130560.0000 - mae: 22089.3594 - val_loss: 22520.3895 - val_mse: 760362496.0000 - val_mae: 22520.3906\n",
            "Epoch 20/500\n",
            "80/80 [==============================] - 0s 821us/step - loss: 22053.1685 - mse: 754616704.0000 - mae: 22053.1680 - val_loss: 22439.7112 - val_mse: 756703360.0000 - val_mae: 22439.7109\n",
            "Epoch 21/500\n",
            "80/80 [==============================] - 0s 758us/step - loss: 21961.8344 - mse: 751122432.0000 - mae: 21961.8320 - val_loss: 22353.2785 - val_mse: 752728448.0000 - val_mae: 22353.2793\n",
            "Epoch 22/500\n",
            "80/80 [==============================] - 0s 973us/step - loss: 21934.7533 - mse: 748299072.0000 - mae: 21934.7539 - val_loss: 22266.3183 - val_mse: 748715136.0000 - val_mae: 22266.3184\n",
            "Epoch 23/500\n",
            "80/80 [==============================] - 0s 953us/step - loss: 21882.7456 - mse: 743570048.0000 - mae: 21882.7461 - val_loss: 22180.1052 - val_mse: 744658048.0000 - val_mae: 22180.1055\n",
            "Epoch 24/500\n",
            "80/80 [==============================] - 0s 818us/step - loss: 21786.5159 - mse: 740967360.0000 - mae: 21786.5156 - val_loss: 22090.3266 - val_mse: 740388096.0000 - val_mae: 22090.3262\n",
            "Epoch 25/500\n",
            "80/80 [==============================] - 0s 990us/step - loss: 21777.8890 - mse: 736548672.0000 - mae: 21777.8906 - val_loss: 22010.3059 - val_mse: 736184640.0000 - val_mae: 22010.3047\n",
            "Epoch 26/500\n",
            "80/80 [==============================] - 0s 837us/step - loss: 21693.6501 - mse: 727599232.0000 - mae: 21693.6504 - val_loss: 21930.6685 - val_mse: 731695104.0000 - val_mae: 21930.6680\n",
            "Epoch 27/500\n",
            "80/80 [==============================] - 0s 882us/step - loss: 21505.0975 - mse: 725609984.0000 - mae: 21505.0977 - val_loss: 21843.2209 - val_mse: 726757056.0000 - val_mae: 21843.2207\n",
            "Epoch 28/500\n",
            "80/80 [==============================] - 0s 844us/step - loss: 21416.9492 - mse: 719926656.0000 - mae: 21416.9492 - val_loss: 21754.8462 - val_mse: 721643712.0000 - val_mae: 21754.8457\n",
            "Epoch 29/500\n",
            "80/80 [==============================] - 0s 863us/step - loss: 21401.8240 - mse: 716773248.0000 - mae: 21401.8242 - val_loss: 21677.3137 - val_mse: 717004928.0000 - val_mae: 21677.3125\n",
            "Epoch 30/500\n",
            "80/80 [==============================] - 0s 820us/step - loss: 21315.5735 - mse: 714673472.0000 - mae: 21315.5742 - val_loss: 21597.7881 - val_mse: 712223872.0000 - val_mae: 21597.7871\n",
            "Epoch 31/500\n",
            "80/80 [==============================] - 0s 828us/step - loss: 21289.4094 - mse: 707073408.0000 - mae: 21289.4102 - val_loss: 21524.6466 - val_mse: 707844160.0000 - val_mae: 21524.6465\n",
            "Epoch 32/500\n",
            "80/80 [==============================] - 0s 921us/step - loss: 21112.4343 - mse: 701168960.0000 - mae: 21112.4336 - val_loss: 21446.4019 - val_mse: 703184640.0000 - val_mae: 21446.4023\n",
            "Epoch 33/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 21020.2437 - mse: 695578240.0000 - mae: 21020.2441 - val_loss: 21359.8905 - val_mse: 698062016.0000 - val_mae: 21359.8887\n",
            "Epoch 34/500\n",
            "80/80 [==============================] - 0s 840us/step - loss: 21078.8577 - mse: 692017856.0000 - mae: 21078.8555 - val_loss: 21285.7970 - val_mse: 693693696.0000 - val_mae: 21285.7969\n",
            "Epoch 35/500\n",
            "80/80 [==============================] - 0s 884us/step - loss: 20953.6476 - mse: 689192192.0000 - mae: 20953.6465 - val_loss: 21213.7083 - val_mse: 689464384.0000 - val_mae: 21213.7070\n",
            "Epoch 36/500\n",
            "80/80 [==============================] - 0s 782us/step - loss: 20879.0409 - mse: 679093888.0000 - mae: 20879.0410 - val_loss: 21130.9429 - val_mse: 684634688.0000 - val_mae: 21130.9434\n",
            "Epoch 37/500\n",
            "80/80 [==============================] - 0s 818us/step - loss: 20885.4592 - mse: 687079040.0000 - mae: 20885.4570 - val_loss: 21047.2679 - val_mse: 679782016.0000 - val_mae: 21047.2676\n",
            "Epoch 38/500\n",
            "80/80 [==============================] - 0s 925us/step - loss: 20673.5586 - mse: 668400192.0000 - mae: 20673.5586 - val_loss: 20959.9158 - val_mse: 674742848.0000 - val_mae: 20959.9180\n",
            "Epoch 39/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 20734.9502 - mse: 673463040.0000 - mae: 20734.9492 - val_loss: 20874.1499 - val_mse: 669826112.0000 - val_mae: 20874.1504\n",
            "Epoch 40/500\n",
            "80/80 [==============================] - 0s 843us/step - loss: 20671.3387 - mse: 669632000.0000 - mae: 20671.3398 - val_loss: 20792.5712 - val_mse: 665174336.0000 - val_mae: 20792.5723\n",
            "Epoch 41/500\n",
            "80/80 [==============================] - 0s 872us/step - loss: 20642.7019 - mse: 661945216.0000 - mae: 20642.7012 - val_loss: 20711.7852 - val_mse: 660593536.0000 - val_mae: 20711.7852\n",
            "Epoch 42/500\n",
            "80/80 [==============================] - 0s 811us/step - loss: 20422.5089 - mse: 659621056.0000 - mae: 20422.5098 - val_loss: 20624.9358 - val_mse: 655521024.0000 - val_mae: 20624.9355\n",
            "Epoch 43/500\n",
            "80/80 [==============================] - 0s 844us/step - loss: 20414.3348 - mse: 657121984.0000 - mae: 20414.3340 - val_loss: 20538.3334 - val_mse: 650468672.0000 - val_mae: 20538.3320\n",
            "Epoch 44/500\n",
            "80/80 [==============================] - 0s 850us/step - loss: 20375.8904 - mse: 656021824.0000 - mae: 20375.8906 - val_loss: 20457.1648 - val_mse: 645576192.0000 - val_mae: 20457.1641\n",
            "Epoch 45/500\n",
            "80/80 [==============================] - 0s 800us/step - loss: 20161.2444 - mse: 631432448.0000 - mae: 20161.2441 - val_loss: 20369.4560 - val_mse: 640304384.0000 - val_mae: 20369.4570\n",
            "Epoch 46/500\n",
            "80/80 [==============================] - 0s 874us/step - loss: 20173.4565 - mse: 628030272.0000 - mae: 20173.4570 - val_loss: 20284.9057 - val_mse: 635255552.0000 - val_mae: 20284.9062\n",
            "Epoch 47/500\n",
            "80/80 [==============================] - 0s 797us/step - loss: 20094.9503 - mse: 633542336.0000 - mae: 20094.9492 - val_loss: 20196.1019 - val_mse: 629835392.0000 - val_mae: 20196.1035\n",
            "Epoch 48/500\n",
            "80/80 [==============================] - 0s 796us/step - loss: 19919.2938 - mse: 616505920.0000 - mae: 19919.2930 - val_loss: 20107.9090 - val_mse: 624408064.0000 - val_mae: 20107.9082\n",
            "Epoch 49/500\n",
            "80/80 [==============================] - 0s 847us/step - loss: 19931.7389 - mse: 611311744.0000 - mae: 19931.7383 - val_loss: 20021.0625 - val_mse: 619101504.0000 - val_mae: 20021.0605\n",
            "Epoch 50/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 20090.9049 - mse: 625025920.0000 - mae: 20090.9062 - val_loss: 19948.1753 - val_mse: 614676864.0000 - val_mae: 19948.1758\n",
            "Epoch 51/500\n",
            "80/80 [==============================] - 0s 856us/step - loss: 19789.3197 - mse: 608300416.0000 - mae: 19789.3184 - val_loss: 19858.1394 - val_mse: 609070656.0000 - val_mae: 19858.1387\n",
            "Epoch 52/500\n",
            "80/80 [==============================] - 0s 997us/step - loss: 19730.5079 - mse: 601967488.0000 - mae: 19730.5098 - val_loss: 19767.3859 - val_mse: 603385536.0000 - val_mae: 19767.3848\n",
            "Epoch 53/500\n",
            "80/80 [==============================] - 0s 774us/step - loss: 19653.4242 - mse: 599142400.0000 - mae: 19653.4238 - val_loss: 19678.9631 - val_mse: 597831744.0000 - val_mae: 19678.9629\n",
            "Epoch 54/500\n",
            "80/80 [==============================] - 0s 963us/step - loss: 19278.9141 - mse: 574348416.0000 - mae: 19278.9121 - val_loss: 19583.1440 - val_mse: 591633024.0000 - val_mae: 19583.1445\n",
            "Epoch 55/500\n",
            "80/80 [==============================] - 0s 923us/step - loss: 19523.4363 - mse: 583544192.0000 - mae: 19523.4355 - val_loss: 19506.4900 - val_mse: 586458944.0000 - val_mae: 19506.4902\n",
            "Epoch 56/500\n",
            "80/80 [==============================] - 0s 832us/step - loss: 19511.1714 - mse: 576747328.0000 - mae: 19511.1738 - val_loss: 19429.7489 - val_mse: 581032768.0000 - val_mae: 19429.7500\n",
            "Epoch 57/500\n",
            "80/80 [==============================] - 0s 839us/step - loss: 19264.3953 - mse: 576509312.0000 - mae: 19264.3945 - val_loss: 19347.5408 - val_mse: 575221312.0000 - val_mae: 19347.5410\n",
            "Epoch 58/500\n",
            "80/80 [==============================] - 0s 841us/step - loss: 19220.3669 - mse: 565639232.0000 - mae: 19220.3672 - val_loss: 19270.5492 - val_mse: 569467840.0000 - val_mae: 19270.5508\n",
            "Epoch 59/500\n",
            "80/80 [==============================] - 0s 742us/step - loss: 19051.1991 - mse: 566673152.0000 - mae: 19051.1992 - val_loss: 19203.3080 - val_mse: 564258496.0000 - val_mae: 19203.3066\n",
            "Epoch 60/500\n",
            "80/80 [==============================] - 0s 838us/step - loss: 19159.5262 - mse: 565626560.0000 - mae: 19159.5273 - val_loss: 19138.3348 - val_mse: 559266176.0000 - val_mae: 19138.3359\n",
            "Epoch 61/500\n",
            "80/80 [==============================] - 0s 884us/step - loss: 19211.5454 - mse: 558976448.0000 - mae: 19211.5469 - val_loss: 19070.3190 - val_mse: 554083264.0000 - val_mae: 19070.3184\n",
            "Epoch 62/500\n",
            "80/80 [==============================] - 0s 807us/step - loss: 18847.5209 - mse: 546621888.0000 - mae: 18847.5215 - val_loss: 18999.8784 - val_mse: 548761600.0000 - val_mae: 18999.8789\n",
            "Epoch 63/500\n",
            "80/80 [==============================] - 0s 793us/step - loss: 19101.3026 - mse: 546943808.0000 - mae: 19101.3027 - val_loss: 18929.4644 - val_mse: 543318912.0000 - val_mae: 18929.4648\n",
            "Epoch 64/500\n",
            "80/80 [==============================] - 0s 809us/step - loss: 19336.5458 - mse: 561530816.0000 - mae: 19336.5469 - val_loss: 18875.2325 - val_mse: 539025280.0000 - val_mae: 18875.2324\n",
            "Epoch 65/500\n",
            "80/80 [==============================] - 0s 983us/step - loss: 18812.4952 - mse: 529629600.0000 - mae: 18812.4961 - val_loss: 18810.1404 - val_mse: 533913952.0000 - val_mae: 18810.1406\n",
            "Epoch 66/500\n",
            "80/80 [==============================] - 0s 892us/step - loss: 18863.7704 - mse: 535451072.0000 - mae: 18863.7695 - val_loss: 18746.3987 - val_mse: 528952928.0000 - val_mae: 18746.4004\n",
            "Epoch 67/500\n",
            "80/80 [==============================] - 0s 802us/step - loss: 18336.6570 - mse: 512741792.0000 - mae: 18336.6582 - val_loss: 18672.9107 - val_mse: 523288000.0000 - val_mae: 18672.9102\n",
            "Epoch 68/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 18606.2528 - mse: 530826144.0000 - mae: 18606.2539 - val_loss: 18609.4226 - val_mse: 518441792.0000 - val_mae: 18609.4258\n",
            "Epoch 69/500\n",
            "80/80 [==============================] - 0s 829us/step - loss: 18789.2963 - mse: 521923424.0000 - mae: 18789.2949 - val_loss: 18558.0211 - val_mse: 514552224.0000 - val_mae: 18558.0215\n",
            "Epoch 70/500\n",
            "80/80 [==============================] - 0s 826us/step - loss: 19016.8086 - mse: 538785344.0000 - mae: 19016.8086 - val_loss: 18507.6723 - val_mse: 510768864.0000 - val_mae: 18507.6719\n",
            "Epoch 71/500\n",
            "80/80 [==============================] - 0s 915us/step - loss: 18268.6005 - mse: 502153056.0000 - mae: 18268.5996 - val_loss: 18447.5251 - val_mse: 506285088.0000 - val_mae: 18447.5254\n",
            "Epoch 72/500\n",
            "80/80 [==============================] - 0s 884us/step - loss: 18308.5259 - mse: 506052768.0000 - mae: 18308.5254 - val_loss: 18383.8612 - val_mse: 501580992.0000 - val_mae: 18383.8613\n",
            "Epoch 73/500\n",
            "80/80 [==============================] - 0s 802us/step - loss: 18503.5497 - mse: 516358144.0000 - mae: 18503.5508 - val_loss: 18323.5112 - val_mse: 497162528.0000 - val_mae: 18323.5098\n",
            "Epoch 74/500\n",
            "80/80 [==============================] - 0s 805us/step - loss: 18244.3470 - mse: 494619968.0000 - mae: 18244.3477 - val_loss: 18264.1142 - val_mse: 492853408.0000 - val_mae: 18264.1152\n",
            "Epoch 75/500\n",
            "80/80 [==============================] - 0s 739us/step - loss: 18563.5023 - mse: 504951808.0000 - mae: 18563.5020 - val_loss: 18203.1745 - val_mse: 488472128.0000 - val_mae: 18203.1758\n",
            "Epoch 76/500\n",
            "80/80 [==============================] - 0s 779us/step - loss: 18598.1819 - mse: 508317696.0000 - mae: 18598.1836 - val_loss: 18151.4582 - val_mse: 484789280.0000 - val_mae: 18151.4590\n",
            "Epoch 77/500\n",
            "80/80 [==============================] - 0s 861us/step - loss: 18431.5673 - mse: 497741408.0000 - mae: 18431.5664 - val_loss: 18096.9911 - val_mse: 480958912.0000 - val_mae: 18096.9922\n",
            "Epoch 78/500\n",
            "80/80 [==============================] - 0s 819us/step - loss: 18257.5571 - mse: 487899712.0000 - mae: 18257.5566 - val_loss: 18026.4488 - val_mse: 476091072.0000 - val_mae: 18026.4473\n",
            "Epoch 79/500\n",
            "80/80 [==============================] - 0s 792us/step - loss: 18015.7042 - mse: 489119232.0000 - mae: 18015.7051 - val_loss: 17390.7223 - val_mse: 467655904.0000 - val_mae: 17390.7207\n",
            "Epoch 80/500\n",
            "80/80 [==============================] - 0s 978us/step - loss: 17153.2316 - mse: 467783168.0000 - mae: 17153.2305 - val_loss: 17350.3753 - val_mse: 460250624.0000 - val_mae: 17350.3750\n",
            "Epoch 81/500\n",
            "80/80 [==============================] - 0s 783us/step - loss: 17188.8220 - mse: 467936864.0000 - mae: 17188.8242 - val_loss: 16833.0464 - val_mse: 451009216.0000 - val_mae: 16833.0469\n",
            "Epoch 82/500\n",
            "80/80 [==============================] - 0s 843us/step - loss: 16915.4801 - mse: 460141920.0000 - mae: 16915.4805 - val_loss: 16894.4837 - val_mse: 444093952.0000 - val_mae: 16894.4844\n",
            "Epoch 83/500\n",
            "80/80 [==============================] - 0s 841us/step - loss: 16831.3400 - mse: 461405856.0000 - mae: 16831.3398 - val_loss: 16557.4896 - val_mse: 435427840.0000 - val_mae: 16557.4902\n",
            "Epoch 84/500\n",
            "80/80 [==============================] - 0s 852us/step - loss: 16497.2004 - mse: 443723616.0000 - mae: 16497.1992 - val_loss: 16240.3506 - val_mse: 427736576.0000 - val_mae: 16240.3496\n",
            "Epoch 85/500\n",
            "80/80 [==============================] - 0s 862us/step - loss: 16334.4584 - mse: 425348800.0000 - mae: 16334.4580 - val_loss: 15993.4716 - val_mse: 418566656.0000 - val_mae: 15993.4717\n",
            "Epoch 86/500\n",
            "80/80 [==============================] - 0s 882us/step - loss: 15791.7953 - mse: 413563872.0000 - mae: 15791.7939 - val_loss: 15794.4038 - val_mse: 409709056.0000 - val_mae: 15794.4033\n",
            "Epoch 87/500\n",
            "80/80 [==============================] - 0s 860us/step - loss: 16029.6046 - mse: 409299136.0000 - mae: 16029.6035 - val_loss: 16041.5805 - val_mse: 405563104.0000 - val_mae: 16041.5801\n",
            "Epoch 88/500\n",
            "80/80 [==============================] - 0s 804us/step - loss: 15640.9073 - mse: 396220672.0000 - mae: 15640.9062 - val_loss: 15374.9831 - val_mse: 393008448.0000 - val_mae: 15374.9844\n",
            "Epoch 89/500\n",
            "80/80 [==============================] - 0s 805us/step - loss: 15359.1548 - mse: 388232800.0000 - mae: 15359.1562 - val_loss: 15518.6286 - val_mse: 386106176.0000 - val_mae: 15518.6289\n",
            "Epoch 90/500\n",
            "80/80 [==============================] - 0s 781us/step - loss: 15327.0879 - mse: 377871392.0000 - mae: 15327.0889 - val_loss: 15018.7720 - val_mse: 376686816.0000 - val_mae: 15018.7715\n",
            "Epoch 91/500\n",
            "80/80 [==============================] - 0s 852us/step - loss: 15268.0513 - mse: 402318848.0000 - mae: 15268.0518 - val_loss: 15090.0643 - val_mse: 369473344.0000 - val_mae: 15090.0645\n",
            "Epoch 92/500\n",
            "80/80 [==============================] - 0s 799us/step - loss: 15652.4421 - mse: 399204640.0000 - mae: 15652.4424 - val_loss: 14556.2371 - val_mse: 360169120.0000 - val_mae: 14556.2373\n",
            "Epoch 93/500\n",
            "80/80 [==============================] - 0s 808us/step - loss: 14319.2278 - mse: 350521184.0000 - mae: 14319.2285 - val_loss: 14364.1454 - val_mse: 351299104.0000 - val_mae: 14364.1445\n",
            "Epoch 94/500\n",
            "80/80 [==============================] - 0s 888us/step - loss: 14340.6404 - mse: 348770912.0000 - mae: 14340.6387 - val_loss: 14097.8717 - val_mse: 342143104.0000 - val_mae: 14097.8711\n",
            "Epoch 95/500\n",
            "80/80 [==============================] - 0s 906us/step - loss: 14379.7599 - mse: 346623552.0000 - mae: 14379.7607 - val_loss: 14506.3080 - val_mse: 337102208.0000 - val_mae: 14506.3076\n",
            "Epoch 96/500\n",
            "80/80 [==============================] - 0s 880us/step - loss: 14208.0059 - mse: 333796672.0000 - mae: 14208.0059 - val_loss: 13668.8209 - val_mse: 325248672.0000 - val_mae: 13668.8203\n",
            "Epoch 97/500\n",
            "80/80 [==============================] - 0s 853us/step - loss: 13655.4446 - mse: 320975136.0000 - mae: 13655.4434 - val_loss: 13670.0403 - val_mse: 317236064.0000 - val_mae: 13670.0410\n",
            "Epoch 98/500\n",
            "80/80 [==============================] - 0s 800us/step - loss: 13841.3630 - mse: 337816160.0000 - mae: 13841.3623 - val_loss: 13199.2417 - val_mse: 308187232.0000 - val_mae: 13199.2412\n",
            "Epoch 99/500\n",
            "80/80 [==============================] - 0s 861us/step - loss: 13052.9722 - mse: 297696320.0000 - mae: 13052.9727 - val_loss: 13121.9756 - val_mse: 299629664.0000 - val_mae: 13121.9756\n",
            "Epoch 100/500\n",
            "80/80 [==============================] - 0s 845us/step - loss: 13360.1233 - mse: 313004416.0000 - mae: 13360.1230 - val_loss: 12719.6903 - val_mse: 290560416.0000 - val_mae: 12719.6904\n",
            "Epoch 101/500\n",
            "80/80 [==============================] - 0s 771us/step - loss: 13115.4788 - mse: 316706400.0000 - mae: 13115.4795 - val_loss: 12545.5987 - val_mse: 283341312.0000 - val_mae: 12545.5986\n",
            "Epoch 102/500\n",
            "80/80 [==============================] - 0s 773us/step - loss: 12526.6099 - mse: 274469920.0000 - mae: 12526.6104 - val_loss: 12304.4072 - val_mse: 274740896.0000 - val_mae: 12304.4072\n",
            "Epoch 103/500\n",
            "80/80 [==============================] - 0s 870us/step - loss: 12609.7938 - mse: 288945600.0000 - mae: 12609.7939 - val_loss: 12101.0506 - val_mse: 266765632.0000 - val_mae: 12101.0498\n",
            "Epoch 104/500\n",
            "80/80 [==============================] - 0s 863us/step - loss: 12690.0049 - mse: 283803680.0000 - mae: 12690.0049 - val_loss: 11890.3137 - val_mse: 259165792.0000 - val_mae: 11890.3145\n",
            "Epoch 105/500\n",
            "80/80 [==============================] - 0s 799us/step - loss: 12287.7631 - mse: 272583648.0000 - mae: 12287.7637 - val_loss: 11668.1248 - val_mse: 250660864.0000 - val_mae: 11668.1250\n",
            "Epoch 106/500\n",
            "80/80 [==============================] - 0s 861us/step - loss: 11900.4616 - mse: 268450624.0000 - mae: 11900.4619 - val_loss: 12255.4291 - val_mse: 246739648.0000 - val_mae: 12255.4287\n",
            "Epoch 107/500\n",
            "80/80 [==============================] - 0s 752us/step - loss: 11642.5721 - mse: 257749456.0000 - mae: 11642.5723 - val_loss: 11440.4461 - val_mse: 236207360.0000 - val_mae: 11440.4463\n",
            "Epoch 108/500\n",
            "80/80 [==============================] - 0s 856us/step - loss: 10880.8801 - mse: 206330848.0000 - mae: 10880.8799 - val_loss: 11057.9897 - val_mse: 227493120.0000 - val_mae: 11057.9893\n",
            "Epoch 109/500\n",
            "80/80 [==============================] - 0s 883us/step - loss: 11478.4698 - mse: 244387024.0000 - mae: 11478.4697 - val_loss: 11019.6501 - val_mse: 220653536.0000 - val_mae: 11019.6504\n",
            "Epoch 110/500\n",
            "80/80 [==============================] - 0s 825us/step - loss: 10664.9926 - mse: 221309472.0000 - mae: 10664.9922 - val_loss: 12050.7814 - val_mse: 221475952.0000 - val_mae: 12050.7812\n",
            "Epoch 111/500\n",
            "80/80 [==============================] - 0s 862us/step - loss: 9805.4592 - mse: 193588768.0000 - mae: 9805.4590 - val_loss: 10554.6155 - val_mse: 205997328.0000 - val_mae: 10554.6152\n",
            "Epoch 112/500\n",
            "80/80 [==============================] - 0s 764us/step - loss: 10743.7255 - mse: 224256080.0000 - mae: 10743.7246 - val_loss: 11303.9962 - val_mse: 202965920.0000 - val_mae: 11303.9961\n",
            "Epoch 113/500\n",
            "80/80 [==============================] - 0s 826us/step - loss: 11155.0724 - mse: 223004704.0000 - mae: 11155.0723 - val_loss: 10169.5276 - val_mse: 192082304.0000 - val_mae: 10169.5273\n",
            "Epoch 114/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 9747.3385 - mse: 179190352.0000 - mae: 9747.3379 - val_loss: 10734.4775 - val_mse: 188491184.0000 - val_mae: 10734.4766\n",
            "Epoch 115/500\n",
            "80/80 [==============================] - 0s 926us/step - loss: 9961.2987 - mse: 182918304.0000 - mae: 9961.2988 - val_loss: 10139.5797 - val_mse: 179495872.0000 - val_mae: 10139.5801\n",
            "Epoch 116/500\n",
            "80/80 [==============================] - 0s 960us/step - loss: 9260.9639 - mse: 162286752.0000 - mae: 9260.9639 - val_loss: 10682.1717 - val_mse: 178229776.0000 - val_mae: 10682.1719\n",
            "Epoch 117/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 9349.9951 - mse: 170216704.0000 - mae: 9349.9951 - val_loss: 9606.4456 - val_mse: 167336016.0000 - val_mae: 9606.4453\n",
            "Epoch 118/500\n",
            "80/80 [==============================] - 0s 891us/step - loss: 9162.5660 - mse: 160015840.0000 - mae: 9162.5654 - val_loss: 9343.3482 - val_mse: 160957120.0000 - val_mae: 9343.3486\n",
            "Epoch 119/500\n",
            "80/80 [==============================] - 0s 898us/step - loss: 8988.7905 - mse: 162968960.0000 - mae: 8988.7910 - val_loss: 9266.9386 - val_mse: 156486064.0000 - val_mae: 9266.9385\n",
            "Epoch 120/500\n",
            "80/80 [==============================] - 0s 794us/step - loss: 8996.4972 - mse: 162091936.0000 - mae: 8996.4980 - val_loss: 9138.2913 - val_mse: 151141024.0000 - val_mae: 9138.2910\n",
            "Epoch 121/500\n",
            "80/80 [==============================] - 0s 848us/step - loss: 9407.9677 - mse: 169406032.0000 - mae: 9407.9668 - val_loss: 8931.8470 - val_mse: 145954576.0000 - val_mae: 8931.8467\n",
            "Epoch 122/500\n",
            "80/80 [==============================] - 0s 936us/step - loss: 10039.7916 - mse: 199376384.0000 - mae: 10039.7920 - val_loss: 8775.2585 - val_mse: 140679264.0000 - val_mae: 8775.2578\n",
            "Epoch 123/500\n",
            "80/80 [==============================] - 0s 839us/step - loss: 8797.5995 - mse: 154248160.0000 - mae: 8797.5996 - val_loss: 9144.3736 - val_mse: 139763648.0000 - val_mae: 9144.3740\n",
            "Epoch 124/500\n",
            "80/80 [==============================] - 0s 810us/step - loss: 8528.5833 - mse: 144006816.0000 - mae: 8528.5830 - val_loss: 8451.7670 - val_mse: 131814592.0000 - val_mae: 8451.7666\n",
            "Epoch 125/500\n",
            "80/80 [==============================] - 0s 844us/step - loss: 7959.5802 - mse: 133115696.0000 - mae: 7959.5806 - val_loss: 8316.1156 - val_mse: 127561536.0000 - val_mae: 8316.1162\n",
            "Epoch 126/500\n",
            "80/80 [==============================] - 0s 854us/step - loss: 7537.0555 - mse: 116560664.0000 - mae: 7537.0557 - val_loss: 8308.5087 - val_mse: 124879232.0000 - val_mae: 8308.5088\n",
            "Epoch 127/500\n",
            "80/80 [==============================] - 0s 756us/step - loss: 8596.5241 - mse: 139823648.0000 - mae: 8596.5244 - val_loss: 8008.3323 - val_mse: 119607256.0000 - val_mae: 8008.3320\n",
            "Epoch 128/500\n",
            "80/80 [==============================] - 0s 795us/step - loss: 8979.5629 - mse: 155495472.0000 - mae: 8979.5625 - val_loss: 8383.4804 - val_mse: 119060320.0000 - val_mae: 8383.4805\n",
            "Epoch 129/500\n",
            "80/80 [==============================] - 0s 851us/step - loss: 8156.4756 - mse: 138058256.0000 - mae: 8156.4756 - val_loss: 8612.1244 - val_mse: 117711664.0000 - val_mae: 8612.1240\n",
            "Epoch 130/500\n",
            "80/80 [==============================] - 0s 770us/step - loss: 8392.6009 - mse: 131687016.0000 - mae: 8392.6006 - val_loss: 7631.7289 - val_mse: 108798744.0000 - val_mae: 7631.7295\n",
            "Epoch 131/500\n",
            "80/80 [==============================] - 0s 730us/step - loss: 8404.7238 - mse: 144615248.0000 - mae: 8404.7236 - val_loss: 7503.4280 - val_mse: 105020008.0000 - val_mae: 7503.4287\n",
            "Epoch 132/500\n",
            "80/80 [==============================] - 0s 790us/step - loss: 7683.3278 - mse: 130335360.0000 - mae: 7683.3281 - val_loss: 7294.0196 - val_mse: 99982776.0000 - val_mae: 7294.0195\n",
            "Epoch 133/500\n",
            "80/80 [==============================] - 0s 850us/step - loss: 8209.6680 - mse: 129737968.0000 - mae: 8209.6680 - val_loss: 7203.4316 - val_mse: 97296264.0000 - val_mae: 7203.4312\n",
            "Epoch 134/500\n",
            "80/80 [==============================] - 0s 809us/step - loss: 7241.0098 - mse: 109354456.0000 - mae: 7241.0093 - val_loss: 7842.9772 - val_mse: 99826192.0000 - val_mae: 7842.9766\n",
            "Epoch 135/500\n",
            "80/80 [==============================] - 0s 805us/step - loss: 7410.8641 - mse: 106797232.0000 - mae: 7410.8643 - val_loss: 6951.7947 - val_mse: 90979168.0000 - val_mae: 6951.7944\n",
            "Epoch 136/500\n",
            "80/80 [==============================] - 0s 790us/step - loss: 8856.5042 - mse: 148072272.0000 - mae: 8856.5049 - val_loss: 6981.3336 - val_mse: 89720424.0000 - val_mae: 6981.3335\n",
            "Epoch 137/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 7798.4610 - mse: 119187376.0000 - mae: 7798.4609 - val_loss: 6757.1102 - val_mse: 85347672.0000 - val_mae: 6757.1104\n",
            "Epoch 138/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 7183.8401 - mse: 105788656.0000 - mae: 7183.8398 - val_loss: 6946.2208 - val_mse: 85550400.0000 - val_mae: 6946.2207\n",
            "Epoch 139/500\n",
            "80/80 [==============================] - 0s 837us/step - loss: 8177.9502 - mse: 131679760.0000 - mae: 8177.9507 - val_loss: 6561.2050 - val_mse: 80001800.0000 - val_mae: 6561.2051\n",
            "Epoch 140/500\n",
            "80/80 [==============================] - 0s 824us/step - loss: 7427.8141 - mse: 111210320.0000 - mae: 7427.8149 - val_loss: 7020.8078 - val_mse: 82758888.0000 - val_mae: 7020.8081\n",
            "Epoch 141/500\n",
            "80/80 [==============================] - 0s 811us/step - loss: 7859.1705 - mse: 112436560.0000 - mae: 7859.1709 - val_loss: 6433.6055 - val_mse: 75981816.0000 - val_mae: 6433.6055\n",
            "Epoch 142/500\n",
            "80/80 [==============================] - 0s 861us/step - loss: 7179.4770 - mse: 101805400.0000 - mae: 7179.4766 - val_loss: 6500.9751 - val_mse: 75428616.0000 - val_mae: 6500.9751\n",
            "Epoch 143/500\n",
            "80/80 [==============================] - 0s 853us/step - loss: 7490.8649 - mse: 112050192.0000 - mae: 7490.8647 - val_loss: 6444.6571 - val_mse: 73116584.0000 - val_mae: 6444.6572\n",
            "Epoch 144/500\n",
            "80/80 [==============================] - 0s 807us/step - loss: 6704.1970 - mse: 105467256.0000 - mae: 6704.1968 - val_loss: 6458.1250 - val_mse: 72442336.0000 - val_mae: 6458.1250\n",
            "Epoch 145/500\n",
            "80/80 [==============================] - 0s 729us/step - loss: 6877.0078 - mse: 111277632.0000 - mae: 6877.0078 - val_loss: 7415.1829 - val_mse: 78640728.0000 - val_mae: 7415.1831\n",
            "Epoch 146/500\n",
            "80/80 [==============================] - 0s 816us/step - loss: 7511.0345 - mse: 111316696.0000 - mae: 7511.0342 - val_loss: 6170.4773 - val_mse: 67250120.0000 - val_mae: 6170.4775\n",
            "Epoch 147/500\n",
            "80/80 [==============================] - 0s 889us/step - loss: 7013.1828 - mse: 90129488.0000 - mae: 7013.1826 - val_loss: 5978.6657 - val_mse: 64633716.0000 - val_mae: 5978.6655\n",
            "Epoch 148/500\n",
            "80/80 [==============================] - 0s 801us/step - loss: 6840.5741 - mse: 92351344.0000 - mae: 6840.5742 - val_loss: 6226.9930 - val_mse: 66101336.0000 - val_mae: 6226.9927\n",
            "Epoch 149/500\n",
            "80/80 [==============================] - 0s 925us/step - loss: 6288.0150 - mse: 83193072.0000 - mae: 6288.0146 - val_loss: 6081.2199 - val_mse: 63432016.0000 - val_mae: 6081.2197\n",
            "Epoch 150/500\n",
            "80/80 [==============================] - 0s 858us/step - loss: 7675.1062 - mse: 116584256.0000 - mae: 7675.1064 - val_loss: 6011.2582 - val_mse: 62109228.0000 - val_mae: 6011.2578\n",
            "Epoch 151/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6298.5446 - mse: 90571440.0000 - mae: 6298.5444 - val_loss: 6078.1783 - val_mse: 62480164.0000 - val_mae: 6078.1787\n",
            "Epoch 152/500\n",
            "80/80 [==============================] - 0s 788us/step - loss: 6770.2269 - mse: 93366464.0000 - mae: 6770.2266 - val_loss: 6121.7421 - val_mse: 61839024.0000 - val_mae: 6121.7422\n",
            "Epoch 153/500\n",
            "80/80 [==============================] - 0s 811us/step - loss: 6372.9258 - mse: 89800400.0000 - mae: 6372.9258 - val_loss: 6095.5801 - val_mse: 60658128.0000 - val_mae: 6095.5806\n",
            "Epoch 154/500\n",
            "80/80 [==============================] - 0s 813us/step - loss: 6198.9005 - mse: 72794320.0000 - mae: 6198.9004 - val_loss: 5863.7587 - val_mse: 57074484.0000 - val_mae: 5863.7578\n",
            "Epoch 155/500\n",
            "80/80 [==============================] - 0s 791us/step - loss: 6406.7773 - mse: 84930232.0000 - mae: 6406.7773 - val_loss: 5801.0004 - val_mse: 55880144.0000 - val_mae: 5801.0000\n",
            "Epoch 156/500\n",
            "80/80 [==============================] - 0s 811us/step - loss: 7245.0127 - mse: 110703528.0000 - mae: 7245.0127 - val_loss: 5844.5782 - val_mse: 56174380.0000 - val_mae: 5844.5776\n",
            "Epoch 157/500\n",
            "80/80 [==============================] - 0s 866us/step - loss: 7458.9603 - mse: 117897624.0000 - mae: 7458.9600 - val_loss: 5692.6932 - val_mse: 53848208.0000 - val_mae: 5692.6929\n",
            "Epoch 158/500\n",
            "80/80 [==============================] - 0s 791us/step - loss: 5837.7313 - mse: 79788176.0000 - mae: 5837.7314 - val_loss: 5681.8938 - val_mse: 53216792.0000 - val_mae: 5681.8936\n",
            "Epoch 159/500\n",
            "80/80 [==============================] - 0s 838us/step - loss: 5956.3656 - mse: 72768776.0000 - mae: 5956.3657 - val_loss: 5636.1634 - val_mse: 52133872.0000 - val_mae: 5636.1636\n",
            "Epoch 160/500\n",
            "80/80 [==============================] - 0s 852us/step - loss: 6346.7038 - mse: 90562200.0000 - mae: 6346.7041 - val_loss: 6377.6765 - val_mse: 57134584.0000 - val_mae: 6377.6768\n",
            "Epoch 161/500\n",
            "80/80 [==============================] - 0s 900us/step - loss: 5852.1716 - mse: 67310928.0000 - mae: 5852.1719 - val_loss: 5587.5023 - val_mse: 50800780.0000 - val_mae: 5587.5024\n",
            "Epoch 162/500\n",
            "80/80 [==============================] - 0s 772us/step - loss: 6944.0733 - mse: 91339728.0000 - mae: 6944.0732 - val_loss: 5504.4588 - val_mse: 49546600.0000 - val_mae: 5504.4590\n",
            "Epoch 163/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6253.0519 - mse: 74547440.0000 - mae: 6253.0518 - val_loss: 5549.1430 - val_mse: 49585724.0000 - val_mae: 5549.1431\n",
            "Epoch 164/500\n",
            "80/80 [==============================] - 0s 845us/step - loss: 6500.3229 - mse: 79047952.0000 - mae: 6500.3232 - val_loss: 5292.4908 - val_mse: 46291120.0000 - val_mae: 5292.4912\n",
            "Epoch 165/500\n",
            "80/80 [==============================] - 0s 931us/step - loss: 6469.4942 - mse: 86810768.0000 - mae: 6469.4946 - val_loss: 5232.2351 - val_mse: 45355924.0000 - val_mae: 5232.2354\n",
            "Epoch 166/500\n",
            "80/80 [==============================] - 0s 776us/step - loss: 5888.1710 - mse: 72439424.0000 - mae: 5888.1709 - val_loss: 5041.5684 - val_mse: 43105664.0000 - val_mae: 5041.5684\n",
            "Epoch 167/500\n",
            "80/80 [==============================] - 0s 890us/step - loss: 6152.9453 - mse: 84503664.0000 - mae: 6152.9453 - val_loss: 4833.7403 - val_mse: 41499808.0000 - val_mae: 4833.7402\n",
            "Epoch 168/500\n",
            "80/80 [==============================] - 0s 907us/step - loss: 5686.6267 - mse: 64261108.0000 - mae: 5686.6260 - val_loss: 5110.3716 - val_mse: 43120812.0000 - val_mae: 5110.3716\n",
            "Epoch 169/500\n",
            "80/80 [==============================] - 0s 806us/step - loss: 6263.7543 - mse: 76802128.0000 - mae: 6263.7544 - val_loss: 4876.2487 - val_mse: 40843724.0000 - val_mae: 4876.2485\n",
            "Epoch 170/500\n",
            "80/80 [==============================] - 0s 859us/step - loss: 6546.3551 - mse: 92893296.0000 - mae: 6546.3550 - val_loss: 4837.4698 - val_mse: 40363632.0000 - val_mae: 4837.4697\n",
            "Epoch 171/500\n",
            "80/80 [==============================] - 0s 815us/step - loss: 6070.4184 - mse: 70008872.0000 - mae: 6070.4180 - val_loss: 4739.1890 - val_mse: 39080740.0000 - val_mae: 4739.1895\n",
            "Epoch 172/500\n",
            "80/80 [==============================] - 0s 797us/step - loss: 6195.8260 - mse: 74597360.0000 - mae: 6195.8262 - val_loss: 4679.4408 - val_mse: 38312220.0000 - val_mae: 4679.4409\n",
            "Epoch 173/500\n",
            "80/80 [==============================] - 0s 808us/step - loss: 5721.4921 - mse: 69438200.0000 - mae: 5721.4922 - val_loss: 4526.1682 - val_mse: 36913428.0000 - val_mae: 4526.1685\n",
            "Epoch 174/500\n",
            "80/80 [==============================] - 0s 840us/step - loss: 5854.2536 - mse: 68643088.0000 - mae: 5854.2534 - val_loss: 4460.2612 - val_mse: 36103016.0000 - val_mae: 4460.2617\n",
            "Epoch 175/500\n",
            "80/80 [==============================] - 0s 847us/step - loss: 6628.9687 - mse: 85535144.0000 - mae: 6628.9688 - val_loss: 4510.6970 - val_mse: 36232000.0000 - val_mae: 4510.6973\n",
            "Epoch 176/500\n",
            "80/80 [==============================] - 0s 866us/step - loss: 6882.1036 - mse: 90012912.0000 - mae: 6882.1040 - val_loss: 4301.5981 - val_mse: 34593760.0000 - val_mae: 4301.5981\n",
            "Epoch 177/500\n",
            "80/80 [==============================] - 0s 777us/step - loss: 6100.3539 - mse: 67509480.0000 - mae: 6100.3540 - val_loss: 4417.6724 - val_mse: 34928896.0000 - val_mae: 4417.6724\n",
            "Epoch 178/500\n",
            "80/80 [==============================] - 0s 786us/step - loss: 6338.7801 - mse: 80587200.0000 - mae: 6338.7803 - val_loss: 4406.0003 - val_mse: 34413460.0000 - val_mae: 4406.0000\n",
            "Epoch 179/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5266.0708 - mse: 56219124.0000 - mae: 5266.0708 - val_loss: 4280.3184 - val_mse: 33357582.0000 - val_mae: 4280.3184\n",
            "Epoch 180/500\n",
            "80/80 [==============================] - 0s 905us/step - loss: 6020.8036 - mse: 71108192.0000 - mae: 6020.8037 - val_loss: 4342.3697 - val_mse: 33384836.0000 - val_mae: 4342.3696\n",
            "Epoch 181/500\n",
            "80/80 [==============================] - 0s 831us/step - loss: 5940.2216 - mse: 69822160.0000 - mae: 5940.2217 - val_loss: 4102.0780 - val_mse: 31677074.0000 - val_mae: 4102.0776\n",
            "Epoch 182/500\n",
            "80/80 [==============================] - 0s 893us/step - loss: 5914.1485 - mse: 74418880.0000 - mae: 5914.1484 - val_loss: 4037.5085 - val_mse: 30726242.0000 - val_mae: 4037.5085\n",
            "Epoch 183/500\n",
            "80/80 [==============================] - 0s 928us/step - loss: 6121.8677 - mse: 74743328.0000 - mae: 6121.8677 - val_loss: 4768.5009 - val_mse: 34501044.0000 - val_mae: 4768.5010\n",
            "Epoch 184/500\n",
            "80/80 [==============================] - 0s 913us/step - loss: 4733.9056 - mse: 52932444.0000 - mae: 4733.9053 - val_loss: 4470.6555 - val_mse: 32632094.0000 - val_mae: 4470.6553\n",
            "Epoch 185/500\n",
            "80/80 [==============================] - 0s 853us/step - loss: 6494.9624 - mse: 86063896.0000 - mae: 6494.9624 - val_loss: 4585.6518 - val_mse: 32853684.0000 - val_mae: 4585.6519\n",
            "Epoch 186/500\n",
            "80/80 [==============================] - 0s 944us/step - loss: 5733.8332 - mse: 60432588.0000 - mae: 5733.8330 - val_loss: 4047.5029 - val_mse: 29567478.0000 - val_mae: 4047.5027\n",
            "Epoch 187/500\n",
            "80/80 [==============================] - 0s 891us/step - loss: 6180.3951 - mse: 85832536.0000 - mae: 6180.3950 - val_loss: 4256.7909 - val_mse: 30747180.0000 - val_mae: 4256.7910\n",
            "Epoch 188/500\n",
            "80/80 [==============================] - 0s 905us/step - loss: 5260.1409 - mse: 58799796.0000 - mae: 5260.1411 - val_loss: 3868.3762 - val_mse: 28414046.0000 - val_mae: 3868.3762\n",
            "Epoch 189/500\n",
            "80/80 [==============================] - 0s 959us/step - loss: 5182.2137 - mse: 58518420.0000 - mae: 5182.2134 - val_loss: 3848.8971 - val_mse: 28244318.0000 - val_mae: 3848.8972\n",
            "Epoch 190/500\n",
            "80/80 [==============================] - 0s 843us/step - loss: 5770.4753 - mse: 63413192.0000 - mae: 5770.4751 - val_loss: 3806.5028 - val_mse: 27621602.0000 - val_mae: 3806.5027\n",
            "Epoch 191/500\n",
            "80/80 [==============================] - 0s 781us/step - loss: 5317.3523 - mse: 54463124.0000 - mae: 5317.3525 - val_loss: 3900.9329 - val_mse: 27981110.0000 - val_mae: 3900.9331\n",
            "Epoch 192/500\n",
            "80/80 [==============================] - 0s 904us/step - loss: 6046.7915 - mse: 72361264.0000 - mae: 6046.7915 - val_loss: 3902.2532 - val_mse: 27585754.0000 - val_mae: 3902.2532\n",
            "Epoch 193/500\n",
            "80/80 [==============================] - 0s 884us/step - loss: 5704.9091 - mse: 74774648.0000 - mae: 5704.9087 - val_loss: 3933.8406 - val_mse: 27729794.0000 - val_mae: 3933.8406\n",
            "Epoch 194/500\n",
            "80/80 [==============================] - 0s 806us/step - loss: 5732.5821 - mse: 62610836.0000 - mae: 5732.5820 - val_loss: 4171.3272 - val_mse: 29025920.0000 - val_mae: 4171.3271\n",
            "Epoch 195/500\n",
            "80/80 [==============================] - 0s 815us/step - loss: 5259.4503 - mse: 58985844.0000 - mae: 5259.4502 - val_loss: 3777.3330 - val_mse: 26222672.0000 - val_mae: 3777.3330\n",
            "Epoch 196/500\n",
            "80/80 [==============================] - 0s 917us/step - loss: 5430.7983 - mse: 61482516.0000 - mae: 5430.7979 - val_loss: 3839.8187 - val_mse: 26443780.0000 - val_mae: 3839.8188\n",
            "Epoch 197/500\n",
            "80/80 [==============================] - 0s 881us/step - loss: 6322.9902 - mse: 80562256.0000 - mae: 6322.9897 - val_loss: 3633.5156 - val_mse: 24737048.0000 - val_mae: 3633.5156\n",
            "Epoch 198/500\n",
            "80/80 [==============================] - 0s 913us/step - loss: 5465.4731 - mse: 61875400.0000 - mae: 5465.4731 - val_loss: 3591.1258 - val_mse: 24498226.0000 - val_mae: 3591.1260\n",
            "Epoch 199/500\n",
            "80/80 [==============================] - 0s 861us/step - loss: 5478.6812 - mse: 67959040.0000 - mae: 5478.6812 - val_loss: 3784.3265 - val_mse: 25484742.0000 - val_mae: 3784.3269\n",
            "Epoch 200/500\n",
            "80/80 [==============================] - 0s 852us/step - loss: 5069.3373 - mse: 51040896.0000 - mae: 5069.3369 - val_loss: 3782.9568 - val_mse: 25433654.0000 - val_mae: 3782.9570\n",
            "Epoch 201/500\n",
            "80/80 [==============================] - 0s 888us/step - loss: 5218.8576 - mse: 56200524.0000 - mae: 5218.8574 - val_loss: 3687.0237 - val_mse: 24815648.0000 - val_mae: 3687.0237\n",
            "Epoch 202/500\n",
            "80/80 [==============================] - 0s 804us/step - loss: 6394.8633 - mse: 79258128.0000 - mae: 6394.8633 - val_loss: 3479.6015 - val_mse: 23080034.0000 - val_mae: 3479.6016\n",
            "Epoch 203/500\n",
            "80/80 [==============================] - 0s 814us/step - loss: 5587.1271 - mse: 57781320.0000 - mae: 5587.1279 - val_loss: 3474.2334 - val_mse: 23213042.0000 - val_mae: 3474.2332\n",
            "Epoch 204/500\n",
            "80/80 [==============================] - 0s 884us/step - loss: 5280.7127 - mse: 49653648.0000 - mae: 5280.7124 - val_loss: 3660.3313 - val_mse: 24308982.0000 - val_mae: 3660.3313\n",
            "Epoch 205/500\n",
            "80/80 [==============================] - 0s 823us/step - loss: 5312.5095 - mse: 50931084.0000 - mae: 5312.5093 - val_loss: 3727.3547 - val_mse: 24556014.0000 - val_mae: 3727.3547\n",
            "Epoch 206/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5394.4467 - mse: 53758764.0000 - mae: 5394.4463 - val_loss: 3410.7012 - val_mse: 22332288.0000 - val_mae: 3410.7012\n",
            "Epoch 207/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5744.3195 - mse: 63697480.0000 - mae: 5744.3193 - val_loss: 3407.3402 - val_mse: 22036152.0000 - val_mae: 3407.3401\n",
            "Epoch 208/500\n",
            "80/80 [==============================] - 0s 828us/step - loss: 6605.8286 - mse: 83346448.0000 - mae: 6605.8291 - val_loss: 3568.9260 - val_mse: 22917512.0000 - val_mae: 3568.9258\n",
            "Epoch 209/500\n",
            "80/80 [==============================] - 0s 775us/step - loss: 5482.0344 - mse: 64638228.0000 - mae: 5482.0347 - val_loss: 3316.1715 - val_mse: 21265532.0000 - val_mae: 3316.1714\n",
            "Epoch 210/500\n",
            "80/80 [==============================] - 0s 877us/step - loss: 5566.7511 - mse: 66597656.0000 - mae: 5566.7510 - val_loss: 3431.4870 - val_mse: 21803598.0000 - val_mae: 3431.4868\n",
            "Epoch 211/500\n",
            "80/80 [==============================] - 0s 845us/step - loss: 5310.0733 - mse: 54009228.0000 - mae: 5310.0732 - val_loss: 3208.5670 - val_mse: 20508186.0000 - val_mae: 3208.5669\n",
            "Epoch 212/500\n",
            "80/80 [==============================] - 0s 829us/step - loss: 4603.4541 - mse: 48821104.0000 - mae: 4603.4541 - val_loss: 3150.8580 - val_mse: 20071706.0000 - val_mae: 3150.8582\n",
            "Epoch 213/500\n",
            "80/80 [==============================] - 0s 794us/step - loss: 6103.2520 - mse: 79652776.0000 - mae: 6103.2524 - val_loss: 3130.9104 - val_mse: 20121884.0000 - val_mae: 3130.9104\n",
            "Epoch 214/500\n",
            "80/80 [==============================] - 0s 955us/step - loss: 5945.5613 - mse: 67804960.0000 - mae: 5945.5615 - val_loss: 3140.6674 - val_mse: 20176120.0000 - val_mae: 3140.6675\n",
            "Epoch 215/500\n",
            "80/80 [==============================] - 0s 865us/step - loss: 5699.9509 - mse: 63963084.0000 - mae: 5699.9512 - val_loss: 3450.3504 - val_mse: 21098058.0000 - val_mae: 3450.3503\n",
            "Epoch 216/500\n",
            "80/80 [==============================] - 0s 863us/step - loss: 4833.2997 - mse: 46665040.0000 - mae: 4833.2998 - val_loss: 3126.2656 - val_mse: 19245378.0000 - val_mae: 3126.2656\n",
            "Epoch 217/500\n",
            "80/80 [==============================] - 0s 950us/step - loss: 5449.6242 - mse: 63618272.0000 - mae: 5449.6240 - val_loss: 3135.2011 - val_mse: 19578848.0000 - val_mae: 3135.2012\n",
            "Epoch 218/500\n",
            "80/80 [==============================] - 0s 892us/step - loss: 4818.0475 - mse: 53820104.0000 - mae: 4818.0479 - val_loss: 3110.2525 - val_mse: 19355038.0000 - val_mae: 3110.2527\n",
            "Epoch 219/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6258.9501 - mse: 84651864.0000 - mae: 6258.9502 - val_loss: 3077.9111 - val_mse: 19115140.0000 - val_mae: 3077.9111\n",
            "Epoch 220/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5140.7061 - mse: 54365568.0000 - mae: 5140.7061 - val_loss: 3288.9763 - val_mse: 20418714.0000 - val_mae: 3288.9761\n",
            "Epoch 221/500\n",
            "80/80 [==============================] - 0s 912us/step - loss: 4693.7992 - mse: 42885792.0000 - mae: 4693.7993 - val_loss: 3252.2696 - val_mse: 20551774.0000 - val_mae: 3252.2695\n",
            "Epoch 222/500\n",
            "80/80 [==============================] - 0s 906us/step - loss: 5256.4801 - mse: 55851372.0000 - mae: 5256.4795 - val_loss: 3138.4326 - val_mse: 20281062.0000 - val_mae: 3138.4326\n",
            "Epoch 223/500\n",
            "80/80 [==============================] - 0s 931us/step - loss: 5583.4460 - mse: 67897984.0000 - mae: 5583.4463 - val_loss: 3064.7931 - val_mse: 19871686.0000 - val_mae: 3064.7930\n",
            "Epoch 224/500\n",
            "80/80 [==============================] - 0s 832us/step - loss: 5501.8379 - mse: 60363352.0000 - mae: 5501.8374 - val_loss: 3168.6120 - val_mse: 20568368.0000 - val_mae: 3168.6121\n",
            "Epoch 225/500\n",
            "80/80 [==============================] - 0s 804us/step - loss: 5380.8714 - mse: 54206964.0000 - mae: 5380.8711 - val_loss: 3112.6788 - val_mse: 20147808.0000 - val_mae: 3112.6790\n",
            "Epoch 226/500\n",
            "80/80 [==============================] - 0s 894us/step - loss: 5385.1709 - mse: 63950232.0000 - mae: 5385.1709 - val_loss: 3136.0861 - val_mse: 20184012.0000 - val_mae: 3136.0862\n",
            "Epoch 227/500\n",
            "80/80 [==============================] - 0s 858us/step - loss: 5325.8830 - mse: 58700152.0000 - mae: 5325.8833 - val_loss: 3345.7085 - val_mse: 20563186.0000 - val_mae: 3345.7085\n",
            "Epoch 228/500\n",
            "80/80 [==============================] - 0s 862us/step - loss: 4394.2578 - mse: 38087708.0000 - mae: 4394.2578 - val_loss: 2967.5428 - val_mse: 18861050.0000 - val_mae: 2967.5430\n",
            "Epoch 229/500\n",
            "80/80 [==============================] - 0s 902us/step - loss: 5410.6834 - mse: 59500812.0000 - mae: 5410.6836 - val_loss: 3513.7770 - val_mse: 20568578.0000 - val_mae: 3513.7771\n",
            "Epoch 230/500\n",
            "80/80 [==============================] - 0s 877us/step - loss: 4208.1328 - mse: 39997416.0000 - mae: 4208.1333 - val_loss: 2965.5450 - val_mse: 17794680.0000 - val_mae: 2965.5452\n",
            "Epoch 231/500\n",
            "80/80 [==============================] - 0s 829us/step - loss: 5370.9137 - mse: 56733196.0000 - mae: 5370.9131 - val_loss: 3124.0627 - val_mse: 18672176.0000 - val_mae: 3124.0627\n",
            "Epoch 232/500\n",
            "80/80 [==============================] - 0s 932us/step - loss: 4983.6367 - mse: 50079256.0000 - mae: 4983.6367 - val_loss: 2962.6246 - val_mse: 18099222.0000 - val_mae: 2962.6245\n",
            "Epoch 233/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5930.5437 - mse: 80072752.0000 - mae: 5930.5439 - val_loss: 3050.0200 - val_mse: 18269974.0000 - val_mae: 3050.0200\n",
            "Epoch 234/500\n",
            "80/80 [==============================] - 0s 930us/step - loss: 5943.9508 - mse: 80547816.0000 - mae: 5943.9507 - val_loss: 3245.4775 - val_mse: 18919256.0000 - val_mae: 3245.4775\n",
            "Epoch 235/500\n",
            "80/80 [==============================] - 0s 916us/step - loss: 5409.5936 - mse: 59056328.0000 - mae: 5409.5938 - val_loss: 2863.6015 - val_mse: 17287314.0000 - val_mae: 2863.6016\n",
            "Epoch 236/500\n",
            "80/80 [==============================] - 0s 845us/step - loss: 4743.9437 - mse: 52943636.0000 - mae: 4743.9438 - val_loss: 2824.1197 - val_mse: 16870966.0000 - val_mae: 2824.1196\n",
            "Epoch 237/500\n",
            "80/80 [==============================] - 0s 919us/step - loss: 5218.5450 - mse: 50241856.0000 - mae: 5218.5454 - val_loss: 2845.5104 - val_mse: 16600258.0000 - val_mae: 2845.5105\n",
            "Epoch 238/500\n",
            "80/80 [==============================] - 0s 930us/step - loss: 4726.4365 - mse: 43042140.0000 - mae: 4726.4365 - val_loss: 2859.3154 - val_mse: 16143826.0000 - val_mae: 2859.3154\n",
            "Epoch 239/500\n",
            "80/80 [==============================] - 0s 821us/step - loss: 4526.9037 - mse: 53333912.0000 - mae: 4526.9038 - val_loss: 2951.7316 - val_mse: 17213604.0000 - val_mae: 2951.7317\n",
            "Epoch 240/500\n",
            "80/80 [==============================] - 0s 828us/step - loss: 4671.6647 - mse: 55521880.0000 - mae: 4671.6650 - val_loss: 2890.2395 - val_mse: 17169020.0000 - val_mae: 2890.2395\n",
            "Epoch 241/500\n",
            "80/80 [==============================] - 0s 841us/step - loss: 5250.9766 - mse: 52362112.0000 - mae: 5250.9771 - val_loss: 2872.6673 - val_mse: 16498739.0000 - val_mae: 2872.6672\n",
            "Epoch 242/500\n",
            "80/80 [==============================] - 0s 769us/step - loss: 5395.3641 - mse: 58368556.0000 - mae: 5395.3643 - val_loss: 2933.9001 - val_mse: 16811390.0000 - val_mae: 2933.8999\n",
            "Epoch 243/500\n",
            "80/80 [==============================] - 0s 807us/step - loss: 5117.6182 - mse: 50761556.0000 - mae: 5117.6182 - val_loss: 2932.8117 - val_mse: 16566005.0000 - val_mae: 2932.8115\n",
            "Epoch 244/500\n",
            "80/80 [==============================] - 0s 812us/step - loss: 5374.9734 - mse: 64111636.0000 - mae: 5374.9736 - val_loss: 2842.9267 - val_mse: 16302475.0000 - val_mae: 2842.9268\n",
            "Epoch 245/500\n",
            "80/80 [==============================] - 0s 857us/step - loss: 4603.9575 - mse: 45145080.0000 - mae: 4603.9575 - val_loss: 2988.4096 - val_mse: 17083544.0000 - val_mae: 2988.4097\n",
            "Epoch 246/500\n",
            "80/80 [==============================] - 0s 955us/step - loss: 5036.3182 - mse: 61018664.0000 - mae: 5036.3184 - val_loss: 2845.7174 - val_mse: 16653608.0000 - val_mae: 2845.7175\n",
            "Epoch 247/500\n",
            "80/80 [==============================] - 0s 884us/step - loss: 5185.4312 - mse: 57846312.0000 - mae: 5185.4312 - val_loss: 2918.5049 - val_mse: 17052990.0000 - val_mae: 2918.5049\n",
            "Epoch 248/500\n",
            "80/80 [==============================] - 0s 793us/step - loss: 4766.3154 - mse: 42576600.0000 - mae: 4766.3154 - val_loss: 2974.2745 - val_mse: 17306262.0000 - val_mae: 2974.2747\n",
            "Epoch 249/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4760.1692 - mse: 44544564.0000 - mae: 4760.1689 - val_loss: 2933.3901 - val_mse: 17678512.0000 - val_mae: 2933.3899\n",
            "Epoch 250/500\n",
            "80/80 [==============================] - 0s 832us/step - loss: 5028.2270 - mse: 56127948.0000 - mae: 5028.2271 - val_loss: 2855.1755 - val_mse: 17742072.0000 - val_mae: 2855.1755\n",
            "Epoch 251/500\n",
            "80/80 [==============================] - 0s 852us/step - loss: 4850.9494 - mse: 48354688.0000 - mae: 4850.9492 - val_loss: 2892.4576 - val_mse: 17589998.0000 - val_mae: 2892.4575\n",
            "Epoch 252/500\n",
            "80/80 [==============================] - 0s 804us/step - loss: 5111.7124 - mse: 56172180.0000 - mae: 5111.7124 - val_loss: 2773.3258 - val_mse: 16579178.0000 - val_mae: 2773.3259\n",
            "Epoch 253/500\n",
            "80/80 [==============================] - 0s 879us/step - loss: 5330.9614 - mse: 64110772.0000 - mae: 5330.9619 - val_loss: 2829.2419 - val_mse: 16214129.0000 - val_mae: 2829.2419\n",
            "Epoch 254/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5103.5842 - mse: 55406796.0000 - mae: 5103.5845 - val_loss: 3156.6626 - val_mse: 17731446.0000 - val_mae: 3156.6626\n",
            "Epoch 255/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5170.3145 - mse: 50367832.0000 - mae: 5170.3145 - val_loss: 2733.9610 - val_mse: 15181674.0000 - val_mae: 2733.9609\n",
            "Epoch 256/500\n",
            "80/80 [==============================] - 0s 909us/step - loss: 5923.0164 - mse: 63020648.0000 - mae: 5923.0166 - val_loss: 2723.4777 - val_mse: 14830541.0000 - val_mae: 2723.4778\n",
            "Epoch 257/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5652.3443 - mse: 62080024.0000 - mae: 5652.3442 - val_loss: 2814.8998 - val_mse: 14746853.0000 - val_mae: 2814.8997\n",
            "Epoch 258/500\n",
            "80/80 [==============================] - 0s 897us/step - loss: 6106.6048 - mse: 79117584.0000 - mae: 6106.6045 - val_loss: 2793.5470 - val_mse: 14598956.0000 - val_mae: 2793.5469\n",
            "Epoch 259/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4448.8346 - mse: 36349876.0000 - mae: 4448.8350 - val_loss: 2708.4231 - val_mse: 14224053.0000 - val_mae: 2708.4231\n",
            "Epoch 260/500\n",
            "80/80 [==============================] - 0s 858us/step - loss: 4670.4851 - mse: 39671772.0000 - mae: 4670.4854 - val_loss: 2650.8929 - val_mse: 13848220.0000 - val_mae: 2650.8928\n",
            "Epoch 261/500\n",
            "80/80 [==============================] - 0s 820us/step - loss: 5232.3225 - mse: 54672236.0000 - mae: 5232.3228 - val_loss: 3233.2009 - val_mse: 16138650.0000 - val_mae: 3233.2009\n",
            "Epoch 262/500\n",
            "80/80 [==============================] - 0s 844us/step - loss: 4786.9078 - mse: 47128160.0000 - mae: 4786.9082 - val_loss: 2615.6983 - val_mse: 13877045.0000 - val_mae: 2615.6982\n",
            "Epoch 263/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5855.8591 - mse: 72118192.0000 - mae: 5855.8594 - val_loss: 3031.6181 - val_mse: 15805586.0000 - val_mae: 3031.6179\n",
            "Epoch 264/500\n",
            "80/80 [==============================] - 0s 918us/step - loss: 5048.2357 - mse: 53881472.0000 - mae: 5048.2354 - val_loss: 2701.7058 - val_mse: 14231319.0000 - val_mae: 2701.7058\n",
            "Epoch 265/500\n",
            "80/80 [==============================] - 0s 880us/step - loss: 5547.5486 - mse: 57991128.0000 - mae: 5547.5488 - val_loss: 2685.6232 - val_mse: 13896082.0000 - val_mae: 2685.6233\n",
            "Epoch 266/500\n",
            "80/80 [==============================] - 0s 905us/step - loss: 4877.3244 - mse: 45672568.0000 - mae: 4877.3247 - val_loss: 2684.9424 - val_mse: 14845883.0000 - val_mae: 2684.9424\n",
            "Epoch 267/500\n",
            "80/80 [==============================] - 0s 979us/step - loss: 4888.1128 - mse: 46537252.0000 - mae: 4888.1133 - val_loss: 2633.5626 - val_mse: 13991960.0000 - val_mae: 2633.5625\n",
            "Epoch 268/500\n",
            "80/80 [==============================] - 0s 871us/step - loss: 4557.6985 - mse: 46566656.0000 - mae: 4557.6982 - val_loss: 3382.1894 - val_mse: 17325780.0000 - val_mae: 3382.1892\n",
            "Epoch 269/500\n",
            "80/80 [==============================] - 0s 874us/step - loss: 5071.2210 - mse: 57416332.0000 - mae: 5071.2212 - val_loss: 2714.1182 - val_mse: 14296581.0000 - val_mae: 2714.1184\n",
            "Epoch 270/500\n",
            "80/80 [==============================] - 0s 892us/step - loss: 5150.4019 - mse: 50366988.0000 - mae: 5150.4014 - val_loss: 3192.8887 - val_mse: 15886018.0000 - val_mae: 3192.8889\n",
            "Epoch 271/500\n",
            "80/80 [==============================] - 0s 855us/step - loss: 5270.1673 - mse: 58321108.0000 - mae: 5270.1670 - val_loss: 3321.2151 - val_mse: 16630109.0000 - val_mae: 3321.2148\n",
            "Epoch 272/500\n",
            "80/80 [==============================] - 0s 864us/step - loss: 4709.8100 - mse: 46299968.0000 - mae: 4709.8096 - val_loss: 2683.6351 - val_mse: 13526782.0000 - val_mae: 2683.6350\n",
            "Epoch 273/500\n",
            "80/80 [==============================] - 0s 949us/step - loss: 5162.7556 - mse: 57921240.0000 - mae: 5162.7554 - val_loss: 2643.2196 - val_mse: 13041827.0000 - val_mae: 2643.2197\n",
            "Epoch 274/500\n",
            "80/80 [==============================] - 0s 857us/step - loss: 4496.4445 - mse: 38625148.0000 - mae: 4496.4443 - val_loss: 2662.0093 - val_mse: 12925532.0000 - val_mae: 2662.0093\n",
            "Epoch 275/500\n",
            "80/80 [==============================] - 0s 851us/step - loss: 4322.0323 - mse: 38263560.0000 - mae: 4322.0322 - val_loss: 2445.3410 - val_mse: 11979232.0000 - val_mae: 2445.3411\n",
            "Epoch 276/500\n",
            "80/80 [==============================] - 0s 859us/step - loss: 4957.3282 - mse: 51443232.0000 - mae: 4957.3281 - val_loss: 2912.3017 - val_mse: 13808567.0000 - val_mae: 2912.3018\n",
            "Epoch 277/500\n",
            "80/80 [==============================] - 0s 815us/step - loss: 4956.4931 - mse: 56763552.0000 - mae: 4956.4932 - val_loss: 2559.2208 - val_mse: 12286027.0000 - val_mae: 2559.2207\n",
            "Epoch 278/500\n",
            "80/80 [==============================] - 0s 800us/step - loss: 5185.1006 - mse: 52977948.0000 - mae: 5185.1006 - val_loss: 2654.7279 - val_mse: 12735214.0000 - val_mae: 2654.7278\n",
            "Epoch 279/500\n",
            "80/80 [==============================] - 0s 798us/step - loss: 4257.4771 - mse: 30060374.0000 - mae: 4257.4771 - val_loss: 2415.3505 - val_mse: 11825912.0000 - val_mae: 2415.3503\n",
            "Epoch 280/500\n",
            "80/80 [==============================] - 0s 847us/step - loss: 5105.5590 - mse: 55253216.0000 - mae: 5105.5596 - val_loss: 2638.0756 - val_mse: 12649068.0000 - val_mae: 2638.0757\n",
            "Epoch 281/500\n",
            "80/80 [==============================] - 0s 810us/step - loss: 5137.2817 - mse: 59497432.0000 - mae: 5137.2812 - val_loss: 3007.4774 - val_mse: 14220997.0000 - val_mae: 3007.4773\n",
            "Epoch 282/500\n",
            "80/80 [==============================] - 0s 791us/step - loss: 5464.6647 - mse: 60065224.0000 - mae: 5464.6646 - val_loss: 2905.0772 - val_mse: 13770455.0000 - val_mae: 2905.0771\n",
            "Epoch 283/500\n",
            "80/80 [==============================] - 0s 820us/step - loss: 4836.0709 - mse: 57037112.0000 - mae: 4836.0703 - val_loss: 2429.5527 - val_mse: 11947563.0000 - val_mae: 2429.5527\n",
            "Epoch 284/500\n",
            "80/80 [==============================] - 0s 885us/step - loss: 5077.6456 - mse: 55131852.0000 - mae: 5077.6455 - val_loss: 2745.1124 - val_mse: 13098334.0000 - val_mae: 2745.1125\n",
            "Epoch 285/500\n",
            "80/80 [==============================] - 0s 806us/step - loss: 5753.1845 - mse: 66769664.0000 - mae: 5753.1846 - val_loss: 2899.8761 - val_mse: 13785541.0000 - val_mae: 2899.8762\n",
            "Epoch 286/500\n",
            "80/80 [==============================] - 0s 851us/step - loss: 5655.8328 - mse: 64784792.0000 - mae: 5655.8325 - val_loss: 2383.5648 - val_mse: 11729093.0000 - val_mae: 2383.5647\n",
            "Epoch 287/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4912.7383 - mse: 47475624.0000 - mae: 4912.7383 - val_loss: 2584.9441 - val_mse: 12406674.0000 - val_mae: 2584.9441\n",
            "Epoch 288/500\n",
            "80/80 [==============================] - 0s 855us/step - loss: 4969.2920 - mse: 51980888.0000 - mae: 4969.2920 - val_loss: 2829.2717 - val_mse: 13342949.0000 - val_mae: 2829.2720\n",
            "Epoch 289/500\n",
            "80/80 [==============================] - 0s 858us/step - loss: 4164.4672 - mse: 40208172.0000 - mae: 4164.4673 - val_loss: 3712.3986 - val_mse: 18923524.0000 - val_mae: 3712.3987\n",
            "Epoch 290/500\n",
            "80/80 [==============================] - 0s 808us/step - loss: 4617.8116 - mse: 43714696.0000 - mae: 4617.8115 - val_loss: 2503.9725 - val_mse: 12138951.0000 - val_mae: 2503.9727\n",
            "Epoch 291/500\n",
            "80/80 [==============================] - 0s 985us/step - loss: 4747.6320 - mse: 52101716.0000 - mae: 4747.6318 - val_loss: 2685.9363 - val_mse: 12972586.0000 - val_mae: 2685.9363\n",
            "Epoch 292/500\n",
            "80/80 [==============================] - 0s 869us/step - loss: 4521.5937 - mse: 46644360.0000 - mae: 4521.5933 - val_loss: 2493.4148 - val_mse: 12327388.0000 - val_mae: 2493.4148\n",
            "Epoch 293/500\n",
            "80/80 [==============================] - 0s 737us/step - loss: 4236.8254 - mse: 34496076.0000 - mae: 4236.8252 - val_loss: 2489.1078 - val_mse: 12151120.0000 - val_mae: 2489.1079\n",
            "Epoch 294/500\n",
            "80/80 [==============================] - 0s 876us/step - loss: 4754.8280 - mse: 43878648.0000 - mae: 4754.8281 - val_loss: 2531.5398 - val_mse: 12358015.0000 - val_mae: 2531.5398\n",
            "Epoch 295/500\n",
            "80/80 [==============================] - 0s 908us/step - loss: 4681.5604 - mse: 43337840.0000 - mae: 4681.5601 - val_loss: 2502.0648 - val_mse: 12223910.0000 - val_mae: 2502.0647\n",
            "Epoch 296/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4082.3583 - mse: 35624976.0000 - mae: 4082.3582 - val_loss: 2697.1733 - val_mse: 12988437.0000 - val_mae: 2697.1731\n",
            "Epoch 297/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4808.1248 - mse: 47642000.0000 - mae: 4808.1250 - val_loss: 2845.1248 - val_mse: 13517406.0000 - val_mae: 2845.1245\n",
            "Epoch 298/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4352.1279 - mse: 43651816.0000 - mae: 4352.1279 - val_loss: 3343.8124 - val_mse: 16210443.0000 - val_mae: 3343.8123\n",
            "Epoch 299/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4854.3777 - mse: 50791768.0000 - mae: 4854.3774 - val_loss: 2433.1350 - val_mse: 11750364.0000 - val_mae: 2433.1350\n",
            "Epoch 300/500\n",
            "80/80 [==============================] - 0s 856us/step - loss: 4982.5933 - mse: 48085204.0000 - mae: 4982.5933 - val_loss: 2892.3041 - val_mse: 13691293.0000 - val_mae: 2892.3040\n",
            "Epoch 301/500\n",
            "80/80 [==============================] - 0s 933us/step - loss: 4862.4619 - mse: 46962036.0000 - mae: 4862.4619 - val_loss: 2570.5968 - val_mse: 12507504.0000 - val_mae: 2570.5969\n",
            "Epoch 302/500\n",
            "80/80 [==============================] - 0s 836us/step - loss: 5560.4703 - mse: 66571464.0000 - mae: 5560.4702 - val_loss: 2519.2697 - val_mse: 12205600.0000 - val_mae: 2519.2695\n",
            "Epoch 303/500\n",
            "80/80 [==============================] - 0s 898us/step - loss: 5123.9516 - mse: 52903656.0000 - mae: 5123.9517 - val_loss: 2648.6063 - val_mse: 12986019.0000 - val_mae: 2648.6062\n",
            "Epoch 304/500\n",
            "80/80 [==============================] - 0s 765us/step - loss: 4171.0632 - mse: 38114432.0000 - mae: 4171.0630 - val_loss: 2759.3863 - val_mse: 13572958.0000 - val_mae: 2759.3862\n",
            "Epoch 305/500\n",
            "80/80 [==============================] - 0s 904us/step - loss: 3819.9606 - mse: 28996774.0000 - mae: 3819.9609 - val_loss: 2723.6410 - val_mse: 13567308.0000 - val_mae: 2723.6411\n",
            "Epoch 306/500\n",
            "80/80 [==============================] - 0s 784us/step - loss: 4353.2164 - mse: 43673344.0000 - mae: 4353.2163 - val_loss: 2692.1234 - val_mse: 13280709.0000 - val_mae: 2692.1235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 90 samples, validate on 90 samples\n",
            "Epoch 1/500\n",
            "90/90 [==============================] - 2s 20ms/step - loss: 24196.0433 - mse: 842675840.0000 - mae: 24196.0449 - val_loss: 25722.4181 - val_mse: 882281792.0000 - val_mae: 25722.4160\n",
            "Epoch 2/500\n",
            "90/90 [==============================] - 0s 838us/step - loss: 24191.3414 - mse: 842511232.0000 - mae: 24191.3418 - val_loss: 25709.4839 - val_mse: 881781440.0000 - val_mae: 25709.4863\n",
            "Epoch 3/500\n",
            "90/90 [==============================] - 0s 808us/step - loss: 24170.9672 - mse: 841750208.0000 - mae: 24170.9668 - val_loss: 25682.0917 - val_mse: 880528704.0000 - val_mae: 25682.0918\n",
            "Epoch 4/500\n",
            "90/90 [==============================] - 0s 783us/step - loss: 24132.4384 - mse: 840050432.0000 - mae: 24132.4395 - val_loss: 25640.8423 - val_mse: 878543104.0000 - val_mae: 25640.8438\n",
            "Epoch 5/500\n",
            "90/90 [==============================] - 0s 742us/step - loss: 24089.2678 - mse: 838103744.0000 - mae: 24089.2695 - val_loss: 25587.3051 - val_mse: 875894272.0000 - val_mae: 25587.3047\n",
            "Epoch 6/500\n",
            "90/90 [==============================] - 0s 753us/step - loss: 24034.3242 - mse: 835716160.0000 - mae: 24034.3242 - val_loss: 25523.4684 - val_mse: 872694144.0000 - val_mae: 25523.4668\n",
            "Epoch 7/500\n",
            "90/90 [==============================] - 0s 741us/step - loss: 23967.7454 - mse: 832639616.0000 - mae: 23967.7441 - val_loss: 25449.4880 - val_mse: 868969600.0000 - val_mae: 25449.4863\n",
            "Epoch 8/500\n",
            "90/90 [==============================] - 0s 763us/step - loss: 23879.8906 - mse: 828213760.0000 - mae: 23879.8926 - val_loss: 25371.6219 - val_mse: 865042624.0000 - val_mae: 25371.6230\n",
            "Epoch 9/500\n",
            "90/90 [==============================] - 0s 798us/step - loss: 23817.1640 - mse: 824921664.0000 - mae: 23817.1641 - val_loss: 25283.0941 - val_mse: 860584704.0000 - val_mae: 25283.0918\n",
            "Epoch 10/500\n",
            "90/90 [==============================] - 0s 789us/step - loss: 23724.5503 - mse: 820995520.0000 - mae: 23724.5508 - val_loss: 25187.4752 - val_mse: 855779264.0000 - val_mae: 25187.4746\n",
            "Epoch 11/500\n",
            "90/90 [==============================] - 0s 793us/step - loss: 23620.5869 - mse: 814590912.0000 - mae: 23620.5859 - val_loss: 25085.0973 - val_mse: 850642880.0000 - val_mae: 25085.0977\n",
            "Epoch 12/500\n",
            "90/90 [==============================] - 0s 764us/step - loss: 23553.9655 - mse: 811135552.0000 - mae: 23553.9668 - val_loss: 24986.0574 - val_mse: 845690368.0000 - val_mae: 24986.0586\n",
            "Epoch 13/500\n",
            "90/90 [==============================] - 0s 740us/step - loss: 23495.1115 - mse: 807767232.0000 - mae: 23495.1113 - val_loss: 24881.5257 - val_mse: 840486208.0000 - val_mae: 24881.5254\n",
            "Epoch 14/500\n",
            "90/90 [==============================] - 0s 906us/step - loss: 23382.3398 - mse: 801259776.0000 - mae: 23382.3398 - val_loss: 24770.1699 - val_mse: 834955200.0000 - val_mae: 24770.1660\n",
            "Epoch 15/500\n",
            "90/90 [==============================] - 0s 827us/step - loss: 23303.1326 - mse: 796367104.0000 - mae: 23303.1309 - val_loss: 24653.2471 - val_mse: 829046528.0000 - val_mae: 24653.2480\n",
            "Epoch 16/500\n",
            "90/90 [==============================] - 0s 799us/step - loss: 23226.8003 - mse: 790538304.0000 - mae: 23226.8008 - val_loss: 24534.5784 - val_mse: 823008384.0000 - val_mae: 24534.5781\n",
            "Epoch 17/500\n",
            "90/90 [==============================] - 0s 880us/step - loss: 23083.4090 - mse: 785307072.0000 - mae: 23083.4062 - val_loss: 24405.0158 - val_mse: 816339968.0000 - val_mae: 24405.0176\n",
            "Epoch 18/500\n",
            "90/90 [==============================] - 0s 823us/step - loss: 23031.3498 - mse: 779515200.0000 - mae: 23031.3477 - val_loss: 24284.9290 - val_mse: 809654976.0000 - val_mae: 24284.9297\n",
            "Epoch 19/500\n",
            "90/90 [==============================] - 0s 777us/step - loss: 22910.0430 - mse: 773451328.0000 - mae: 22910.0410 - val_loss: 24169.2179 - val_mse: 802846656.0000 - val_mae: 24169.2168\n",
            "Epoch 20/500\n",
            "90/90 [==============================] - 0s 819us/step - loss: 22792.3129 - mse: 766089280.0000 - mae: 22792.3125 - val_loss: 24048.0618 - val_mse: 795665536.0000 - val_mae: 24048.0605\n",
            "Epoch 21/500\n",
            "90/90 [==============================] - 0s 797us/step - loss: 22660.4659 - mse: 757697280.0000 - mae: 22660.4668 - val_loss: 23924.7047 - val_mse: 788149504.0000 - val_mae: 23924.7031\n",
            "Epoch 22/500\n",
            "90/90 [==============================] - 0s 728us/step - loss: 22592.6118 - mse: 753516928.0000 - mae: 22592.6113 - val_loss: 23809.2608 - val_mse: 781100928.0000 - val_mae: 23809.2617\n",
            "Epoch 23/500\n",
            "90/90 [==============================] - 0s 807us/step - loss: 22437.8691 - mse: 744095552.0000 - mae: 22437.8691 - val_loss: 23683.8048 - val_mse: 773491584.0000 - val_mae: 23683.8027\n",
            "Epoch 24/500\n",
            "90/90 [==============================] - 0s 740us/step - loss: 22270.1589 - mse: 734378112.0000 - mae: 22270.1602 - val_loss: 23552.7853 - val_mse: 765599552.0000 - val_mae: 23552.7832\n",
            "Epoch 25/500\n",
            "90/90 [==============================] - 0s 730us/step - loss: 22141.2237 - mse: 727490240.0000 - mae: 22141.2266 - val_loss: 23424.7006 - val_mse: 757938816.0000 - val_mae: 23424.6992\n",
            "Epoch 26/500\n",
            "90/90 [==============================] - 0s 753us/step - loss: 22119.7941 - mse: 728433728.0000 - mae: 22119.7930 - val_loss: 23303.3454 - val_mse: 750727424.0000 - val_mae: 23303.3438\n",
            "Epoch 27/500\n",
            "90/90 [==============================] - 0s 814us/step - loss: 22005.7160 - mse: 716493952.0000 - mae: 22005.7148 - val_loss: 23171.0572 - val_mse: 742922176.0000 - val_mae: 23171.0547\n",
            "Epoch 28/500\n",
            "90/90 [==============================] - 0s 744us/step - loss: 21891.3073 - mse: 704809024.0000 - mae: 21891.3066 - val_loss: 23042.6506 - val_mse: 735394496.0000 - val_mae: 23042.6504\n",
            "Epoch 29/500\n",
            "90/90 [==============================] - 0s 758us/step - loss: 21791.5920 - mse: 706548480.0000 - mae: 21791.5938 - val_loss: 22918.2552 - val_mse: 728151168.0000 - val_mae: 22918.2559\n",
            "Epoch 30/500\n",
            "90/90 [==============================] - 0s 734us/step - loss: 21709.1680 - mse: 692829632.0000 - mae: 21709.1660 - val_loss: 22780.7762 - val_mse: 720028032.0000 - val_mae: 22780.7773\n",
            "Epoch 31/500\n",
            "90/90 [==============================] - 0s 713us/step - loss: 21487.2143 - mse: 682545088.0000 - mae: 21487.2148 - val_loss: 22641.0313 - val_mse: 711691264.0000 - val_mae: 22641.0332\n",
            "Epoch 32/500\n",
            "90/90 [==============================] - 0s 751us/step - loss: 21492.8743 - mse: 679998592.0000 - mae: 21492.8730 - val_loss: 22508.6279 - val_mse: 703715136.0000 - val_mae: 22508.6270\n",
            "Epoch 33/500\n",
            "90/90 [==============================] - 0s 878us/step - loss: 21417.0867 - mse: 672245824.0000 - mae: 21417.0859 - val_loss: 22374.1568 - val_mse: 695672320.0000 - val_mae: 22374.1562\n",
            "Epoch 34/500\n",
            "90/90 [==============================] - 0s 734us/step - loss: 21330.0595 - mse: 665824384.0000 - mae: 21330.0605 - val_loss: 22244.4098 - val_mse: 687750016.0000 - val_mae: 22244.4102\n",
            "Epoch 35/500\n",
            "90/90 [==============================] - 0s 721us/step - loss: 21256.6826 - mse: 665565888.0000 - mae: 21256.6797 - val_loss: 22113.6334 - val_mse: 679833152.0000 - val_mae: 22113.6328\n",
            "Epoch 36/500\n",
            "90/90 [==============================] - 0s 801us/step - loss: 21342.1782 - mse: 660035584.0000 - mae: 21342.1777 - val_loss: 21989.9073 - val_mse: 672386176.0000 - val_mae: 21989.9062\n",
            "Epoch 37/500\n",
            "90/90 [==============================] - 0s 779us/step - loss: 20934.3861 - mse: 642922816.0000 - mae: 20934.3867 - val_loss: 21847.9156 - val_mse: 663667072.0000 - val_mae: 21847.9141\n",
            "Epoch 38/500\n",
            "90/90 [==============================] - 0s 803us/step - loss: 20766.5699 - mse: 629097472.0000 - mae: 20766.5703 - val_loss: 21703.2103 - val_mse: 654786496.0000 - val_mae: 21703.2090\n",
            "Epoch 39/500\n",
            "90/90 [==============================] - 0s 922us/step - loss: 20724.1669 - mse: 629685888.0000 - mae: 20724.1660 - val_loss: 21559.7819 - val_mse: 645804160.0000 - val_mae: 21559.7832\n",
            "Epoch 40/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 20682.1914 - mse: 630054656.0000 - mae: 20682.1895 - val_loss: 21433.9413 - val_mse: 637572608.0000 - val_mae: 21433.9414\n",
            "Epoch 41/500\n",
            "90/90 [==============================] - 0s 962us/step - loss: 20507.7853 - mse: 613348480.0000 - mae: 20507.7852 - val_loss: 21298.6952 - val_mse: 628654336.0000 - val_mae: 21298.6953\n",
            "Epoch 42/500\n",
            "90/90 [==============================] - 0s 850us/step - loss: 20550.1853 - mse: 611832576.0000 - mae: 20550.1855 - val_loss: 21181.0100 - val_mse: 620591168.0000 - val_mae: 21181.0098\n",
            "Epoch 43/500\n",
            "90/90 [==============================] - 0s 757us/step - loss: 20322.1347 - mse: 599106432.0000 - mae: 20322.1367 - val_loss: 21064.3439 - val_mse: 612561472.0000 - val_mae: 21064.3438\n",
            "Epoch 44/500\n",
            "90/90 [==============================] - 0s 801us/step - loss: 20095.8053 - mse: 583757568.0000 - mae: 20095.8047 - val_loss: 20935.1059 - val_mse: 603759936.0000 - val_mae: 20935.1055\n",
            "Epoch 45/500\n",
            "90/90 [==============================] - 0s 750us/step - loss: 20009.9054 - mse: 580917824.0000 - mae: 20009.9062 - val_loss: 20817.5674 - val_mse: 595784256.0000 - val_mae: 20817.5664\n",
            "Epoch 46/500\n",
            "90/90 [==============================] - 0s 792us/step - loss: 20116.8672 - mse: 584004608.0000 - mae: 20116.8672 - val_loss: 20703.5125 - val_mse: 587876992.0000 - val_mae: 20703.5117\n",
            "Epoch 47/500\n",
            "90/90 [==============================] - 0s 746us/step - loss: 19905.0513 - mse: 569950912.0000 - mae: 19905.0527 - val_loss: 20583.6431 - val_mse: 579657024.0000 - val_mae: 20583.6426\n",
            "Epoch 48/500\n",
            "90/90 [==============================] - 0s 785us/step - loss: 19778.9060 - mse: 556294080.0000 - mae: 19778.9062 - val_loss: 20461.5479 - val_mse: 571379584.0000 - val_mae: 20461.5469\n",
            "Epoch 49/500\n",
            "90/90 [==============================] - 0s 823us/step - loss: 19289.2802 - mse: 534779808.0000 - mae: 19289.2812 - val_loss: 20322.6075 - val_mse: 562076544.0000 - val_mae: 20322.6074\n",
            "Epoch 50/500\n",
            "90/90 [==============================] - 0s 820us/step - loss: 19633.4359 - mse: 558215744.0000 - mae: 19633.4355 - val_loss: 20186.1601 - val_mse: 553061696.0000 - val_mae: 20186.1602\n",
            "Epoch 51/500\n",
            "90/90 [==============================] - 0s 778us/step - loss: 19413.3865 - mse: 542308032.0000 - mae: 19413.3867 - val_loss: 20067.8863 - val_mse: 545345664.0000 - val_mae: 20067.8867\n",
            "Epoch 52/500\n",
            "90/90 [==============================] - 0s 795us/step - loss: 18965.4669 - mse: 512328032.0000 - mae: 18965.4668 - val_loss: 19936.5484 - val_mse: 536886848.0000 - val_mae: 19936.5488\n",
            "Epoch 53/500\n",
            "90/90 [==============================] - 0s 743us/step - loss: 19058.3447 - mse: 515014560.0000 - mae: 19058.3438 - val_loss: 19803.5094 - val_mse: 528436064.0000 - val_mae: 19803.5117\n",
            "Epoch 54/500\n",
            "90/90 [==============================] - 0s 767us/step - loss: 19171.0239 - mse: 520349408.0000 - mae: 19171.0234 - val_loss: 19687.1092 - val_mse: 521141824.0000 - val_mae: 19687.1113\n",
            "Epoch 55/500\n",
            "90/90 [==============================] - 0s 836us/step - loss: 19129.3190 - mse: 513501312.0000 - mae: 19129.3203 - val_loss: 19566.7227 - val_mse: 513683872.0000 - val_mae: 19566.7207\n",
            "Epoch 56/500\n",
            "90/90 [==============================] - 0s 768us/step - loss: 18933.0630 - mse: 510800064.0000 - mae: 18933.0645 - val_loss: 19451.8757 - val_mse: 506681696.0000 - val_mae: 19451.8750\n",
            "Epoch 57/500\n",
            "90/90 [==============================] - 0s 827us/step - loss: 18744.1975 - mse: 494142752.0000 - mae: 18744.1973 - val_loss: 19329.8509 - val_mse: 499364032.0000 - val_mae: 19329.8516\n",
            "Epoch 58/500\n",
            "90/90 [==============================] - 0s 764us/step - loss: 18808.9618 - mse: 492495200.0000 - mae: 18808.9629 - val_loss: 19217.1821 - val_mse: 492818336.0000 - val_mae: 19217.1836\n",
            "Epoch 59/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 18627.8849 - mse: 479860928.0000 - mae: 18627.8867 - val_loss: 19067.9014 - val_mse: 486342016.0000 - val_mae: 19067.9023\n",
            "Epoch 60/500\n",
            "90/90 [==============================] - 0s 889us/step - loss: 18664.1561 - mse: 486425344.0000 - mae: 18664.1562 - val_loss: 18745.9449 - val_mse: 476310496.0000 - val_mae: 18745.9453\n",
            "Epoch 61/500\n",
            "90/90 [==============================] - 0s 833us/step - loss: 18147.2753 - mse: 475945856.0000 - mae: 18147.2754 - val_loss: 18417.0393 - val_mse: 465846944.0000 - val_mae: 18417.0410\n",
            "Epoch 62/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 17687.3446 - mse: 455590176.0000 - mae: 17687.3457 - val_loss: 18074.5104 - val_mse: 455010336.0000 - val_mae: 18074.5117\n",
            "Epoch 63/500\n",
            "90/90 [==============================] - 0s 739us/step - loss: 17435.4741 - mse: 439322656.0000 - mae: 17435.4746 - val_loss: 17938.7115 - val_mse: 449213408.0000 - val_mae: 17938.7109\n",
            "Epoch 64/500\n",
            "90/90 [==============================] - 0s 982us/step - loss: 17078.9906 - mse: 431358624.0000 - mae: 17078.9922 - val_loss: 17427.9246 - val_mse: 430966208.0000 - val_mae: 17427.9258\n",
            "Epoch 65/500\n",
            "90/90 [==============================] - 0s 813us/step - loss: 16988.8786 - mse: 427157600.0000 - mae: 16988.8809 - val_loss: 17290.6483 - val_mse: 422223456.0000 - val_mae: 17290.6465\n",
            "Epoch 66/500\n",
            "90/90 [==============================] - 0s 877us/step - loss: 16975.6368 - mse: 429036928.0000 - mae: 16975.6367 - val_loss: 17134.1420 - val_mse: 412927104.0000 - val_mae: 17134.1426\n",
            "Epoch 67/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 16149.9536 - mse: 400567968.0000 - mae: 16149.9541 - val_loss: 16635.8802 - val_mse: 396356736.0000 - val_mae: 16635.8809\n",
            "Epoch 68/500\n",
            "90/90 [==============================] - 0s 765us/step - loss: 16206.6926 - mse: 391427104.0000 - mae: 16206.6914 - val_loss: 16444.8453 - val_mse: 382529760.0000 - val_mae: 16444.8438\n",
            "Epoch 69/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 15519.9368 - mse: 358047616.0000 - mae: 15519.9375 - val_loss: 16636.7584 - val_mse: 379177088.0000 - val_mae: 16636.7578\n",
            "Epoch 70/500\n",
            "90/90 [==============================] - 0s 839us/step - loss: 15546.7199 - mse: 372286496.0000 - mae: 15546.7197 - val_loss: 16038.3652 - val_mse: 359019104.0000 - val_mae: 16038.3652\n",
            "Epoch 71/500\n",
            "90/90 [==============================] - 0s 971us/step - loss: 15553.1234 - mse: 364828064.0000 - mae: 15553.1250 - val_loss: 15384.3575 - val_mse: 345901760.0000 - val_mae: 15384.3584\n",
            "Epoch 72/500\n",
            "90/90 [==============================] - 0s 860us/step - loss: 14595.4452 - mse: 339165792.0000 - mae: 14595.4463 - val_loss: 15465.0963 - val_mse: 338999040.0000 - val_mae: 15465.0977\n",
            "Epoch 73/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 15185.9485 - mse: 361908416.0000 - mae: 15185.9482 - val_loss: 14812.6156 - val_mse: 322806400.0000 - val_mae: 14812.6152\n",
            "Epoch 74/500\n",
            "90/90 [==============================] - 0s 791us/step - loss: 14434.5173 - mse: 328877312.0000 - mae: 14434.5176 - val_loss: 14449.5980 - val_mse: 310097312.0000 - val_mae: 14449.5986\n",
            "Epoch 75/500\n",
            "90/90 [==============================] - 0s 772us/step - loss: 14307.2567 - mse: 321214176.0000 - mae: 14307.2568 - val_loss: 14192.9965 - val_mse: 297986784.0000 - val_mae: 14192.9971\n",
            "Epoch 76/500\n",
            "90/90 [==============================] - 0s 774us/step - loss: 14354.9961 - mse: 329515680.0000 - mae: 14354.9961 - val_loss: 13873.1319 - val_mse: 287766592.0000 - val_mae: 13873.1318\n",
            "Epoch 77/500\n",
            "90/90 [==============================] - 0s 885us/step - loss: 13048.6466 - mse: 274424096.0000 - mae: 13048.6455 - val_loss: 13528.0004 - val_mse: 274945056.0000 - val_mae: 13528.0000\n",
            "Epoch 78/500\n",
            "90/90 [==============================] - 0s 781us/step - loss: 12905.7711 - mse: 279885696.0000 - mae: 12905.7705 - val_loss: 13241.3610 - val_mse: 263936736.0000 - val_mae: 13241.3613\n",
            "Epoch 79/500\n",
            "90/90 [==============================] - 0s 804us/step - loss: 13108.2198 - mse: 282441312.0000 - mae: 13108.2207 - val_loss: 12882.7255 - val_mse: 252209632.0000 - val_mae: 12882.7266\n",
            "Epoch 80/500\n",
            "90/90 [==============================] - 0s 962us/step - loss: 12694.8775 - mse: 268864000.0000 - mae: 12694.8760 - val_loss: 12576.3953 - val_mse: 241274304.0000 - val_mae: 12576.3945\n",
            "Epoch 81/500\n",
            "90/90 [==============================] - 0s 800us/step - loss: 11998.9523 - mse: 254907056.0000 - mae: 11998.9531 - val_loss: 12642.1696 - val_mse: 233167760.0000 - val_mae: 12642.1699\n",
            "Epoch 82/500\n",
            "90/90 [==============================] - 0s 880us/step - loss: 11744.1628 - mse: 216749472.0000 - mae: 11744.1621 - val_loss: 11747.2774 - val_mse: 218233760.0000 - val_mae: 11747.2773\n",
            "Epoch 83/500\n",
            "90/90 [==============================] - 0s 796us/step - loss: 11288.2988 - mse: 215180448.0000 - mae: 11288.2988 - val_loss: 11396.4340 - val_mse: 207604128.0000 - val_mae: 11396.4336\n",
            "Epoch 84/500\n",
            "90/90 [==============================] - 0s 742us/step - loss: 11688.8977 - mse: 223269136.0000 - mae: 11688.8975 - val_loss: 11407.4578 - val_mse: 199460016.0000 - val_mae: 11407.4590\n",
            "Epoch 85/500\n",
            "90/90 [==============================] - 0s 816us/step - loss: 10715.5945 - mse: 202892816.0000 - mae: 10715.5947 - val_loss: 11187.8649 - val_mse: 190578000.0000 - val_mae: 11187.8643\n",
            "Epoch 86/500\n",
            "90/90 [==============================] - 0s 771us/step - loss: 10357.7673 - mse: 185061584.0000 - mae: 10357.7666 - val_loss: 10373.7482 - val_mse: 177665824.0000 - val_mae: 10373.7490\n",
            "Epoch 87/500\n",
            "90/90 [==============================] - 0s 797us/step - loss: 10093.6929 - mse: 169758848.0000 - mae: 10093.6924 - val_loss: 10107.1285 - val_mse: 167897184.0000 - val_mae: 10107.1289\n",
            "Epoch 88/500\n",
            "90/90 [==============================] - 0s 931us/step - loss: 10148.9049 - mse: 186022304.0000 - mae: 10148.9053 - val_loss: 10064.2017 - val_mse: 160391824.0000 - val_mae: 10064.2021\n",
            "Epoch 89/500\n",
            "90/90 [==============================] - 0s 756us/step - loss: 9185.5007 - mse: 169132448.0000 - mae: 9185.5010 - val_loss: 9431.0156 - val_mse: 150957824.0000 - val_mae: 9431.0156\n",
            "Epoch 90/500\n",
            "90/90 [==============================] - 0s 849us/step - loss: 9379.9250 - mse: 162033040.0000 - mae: 9379.9248 - val_loss: 9045.7184 - val_mse: 141750336.0000 - val_mae: 9045.7188\n",
            "Epoch 91/500\n",
            "90/90 [==============================] - 0s 839us/step - loss: 9102.5977 - mse: 151736800.0000 - mae: 9102.5986 - val_loss: 9392.0590 - val_mse: 137454528.0000 - val_mae: 9392.0586\n",
            "Epoch 92/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 8389.2121 - mse: 127570488.0000 - mae: 8389.2129 - val_loss: 8411.2927 - val_mse: 125654984.0000 - val_mae: 8411.2930\n",
            "Epoch 93/500\n",
            "90/90 [==============================] - 0s 868us/step - loss: 9285.5506 - mse: 155547840.0000 - mae: 9285.5498 - val_loss: 8132.7963 - val_mse: 118813648.0000 - val_mae: 8132.7964\n",
            "Epoch 94/500\n",
            "90/90 [==============================] - 0s 761us/step - loss: 8168.5595 - mse: 127776072.0000 - mae: 8168.5596 - val_loss: 9056.6401 - val_mse: 121455672.0000 - val_mae: 9056.6396\n",
            "Epoch 95/500\n",
            "90/90 [==============================] - 0s 827us/step - loss: 8342.5228 - mse: 126805080.0000 - mae: 8342.5225 - val_loss: 7712.1179 - val_mse: 107695592.0000 - val_mae: 7712.1182\n",
            "Epoch 96/500\n",
            "90/90 [==============================] - 0s 814us/step - loss: 8568.2220 - mse: 146293520.0000 - mae: 8568.2227 - val_loss: 7487.0194 - val_mse: 102367776.0000 - val_mae: 7487.0200\n",
            "Epoch 97/500\n",
            "90/90 [==============================] - 0s 825us/step - loss: 8430.2549 - mse: 124219912.0000 - mae: 8430.2559 - val_loss: 7890.9353 - val_mse: 100056408.0000 - val_mae: 7890.9360\n",
            "Epoch 98/500\n",
            "90/90 [==============================] - 0s 867us/step - loss: 7057.3798 - mse: 99095920.0000 - mae: 7057.3799 - val_loss: 7133.9419 - val_mse: 92639048.0000 - val_mae: 7133.9424\n",
            "Epoch 99/500\n",
            "90/90 [==============================] - 0s 749us/step - loss: 7968.6855 - mse: 117854456.0000 - mae: 7968.6860 - val_loss: 6751.7107 - val_mse: 87293096.0000 - val_mae: 6751.7104\n",
            "Epoch 100/500\n",
            "90/90 [==============================] - 0s 778us/step - loss: 7373.7937 - mse: 103235960.0000 - mae: 7373.7930 - val_loss: 6795.4408 - val_mse: 83879528.0000 - val_mae: 6795.4409\n",
            "Epoch 101/500\n",
            "90/90 [==============================] - 0s 986us/step - loss: 7492.4798 - mse: 116441576.0000 - mae: 7492.4800 - val_loss: 6345.7306 - val_mse: 78018848.0000 - val_mae: 6345.7305\n",
            "Epoch 102/500\n",
            "90/90 [==============================] - 0s 846us/step - loss: 8142.2086 - mse: 129234432.0000 - mae: 8142.2090 - val_loss: 7048.0665 - val_mse: 79900680.0000 - val_mae: 7048.0669\n",
            "Epoch 103/500\n",
            "90/90 [==============================] - 0s 776us/step - loss: 7200.9692 - mse: 106365200.0000 - mae: 7200.9692 - val_loss: 6020.7887 - val_mse: 70892200.0000 - val_mae: 6020.7891\n",
            "Epoch 104/500\n",
            "90/90 [==============================] - 0s 840us/step - loss: 6638.6706 - mse: 84916608.0000 - mae: 6638.6709 - val_loss: 6378.5782 - val_mse: 70045888.0000 - val_mae: 6378.5786\n",
            "Epoch 105/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 6615.8838 - mse: 88455688.0000 - mae: 6615.8833 - val_loss: 5846.7730 - val_mse: 64707856.0000 - val_mae: 5846.7734\n",
            "Epoch 106/500\n",
            "90/90 [==============================] - 0s 872us/step - loss: 6180.8586 - mse: 73097832.0000 - mae: 6180.8584 - val_loss: 5838.8291 - val_mse: 63124992.0000 - val_mae: 5838.8291\n",
            "Epoch 107/500\n",
            "90/90 [==============================] - 0s 729us/step - loss: 7112.4186 - mse: 98844992.0000 - mae: 7112.4180 - val_loss: 5556.0299 - val_mse: 60091328.0000 - val_mae: 5556.0298\n",
            "Epoch 108/500\n",
            "90/90 [==============================] - 0s 801us/step - loss: 7045.7187 - mse: 105570952.0000 - mae: 7045.7188 - val_loss: 5460.3127 - val_mse: 57798412.0000 - val_mae: 5460.3130\n",
            "Epoch 109/500\n",
            "90/90 [==============================] - 0s 828us/step - loss: 7697.7851 - mse: 114307576.0000 - mae: 7697.7847 - val_loss: 5716.7227 - val_mse: 57106396.0000 - val_mae: 5716.7231\n",
            "Epoch 110/500\n",
            "90/90 [==============================] - 0s 745us/step - loss: 6427.4557 - mse: 74263400.0000 - mae: 6427.4556 - val_loss: 5385.9851 - val_mse: 54005368.0000 - val_mae: 5385.9849\n",
            "Epoch 111/500\n",
            "90/90 [==============================] - 0s 816us/step - loss: 7595.6054 - mse: 103846024.0000 - mae: 7595.6055 - val_loss: 5270.9618 - val_mse: 52438504.0000 - val_mae: 5270.9619\n",
            "Epoch 112/500\n",
            "90/90 [==============================] - 0s 810us/step - loss: 6729.5039 - mse: 88428296.0000 - mae: 6729.5044 - val_loss: 5127.9910 - val_mse: 49992148.0000 - val_mae: 5127.9912\n",
            "Epoch 113/500\n",
            "90/90 [==============================] - 0s 783us/step - loss: 6451.5652 - mse: 81125240.0000 - mae: 6451.5654 - val_loss: 5363.5946 - val_mse: 49329944.0000 - val_mae: 5363.5947\n",
            "Epoch 114/500\n",
            "90/90 [==============================] - 0s 814us/step - loss: 7219.2805 - mse: 102713584.0000 - mae: 7219.2808 - val_loss: 5288.3081 - val_mse: 48181520.0000 - val_mae: 5288.3081\n",
            "Epoch 115/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 6108.8567 - mse: 73987560.0000 - mae: 6108.8569 - val_loss: 4949.9031 - val_mse: 44951236.0000 - val_mae: 4949.9033\n",
            "Epoch 116/500\n",
            "90/90 [==============================] - 0s 819us/step - loss: 6311.4115 - mse: 77888048.0000 - mae: 6311.4116 - val_loss: 4803.4927 - val_mse: 43316544.0000 - val_mae: 4803.4927\n",
            "Epoch 117/500\n",
            "90/90 [==============================] - 0s 846us/step - loss: 6953.8092 - mse: 86311792.0000 - mae: 6953.8096 - val_loss: 5226.7909 - val_mse: 44602764.0000 - val_mae: 5226.7910\n",
            "Epoch 118/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 6734.4456 - mse: 79398136.0000 - mae: 6734.4458 - val_loss: 5027.9293 - val_mse: 43192236.0000 - val_mae: 5027.9297\n",
            "Epoch 119/500\n",
            "90/90 [==============================] - 0s 885us/step - loss: 7161.0405 - mse: 101850008.0000 - mae: 7161.0400 - val_loss: 4666.6787 - val_mse: 40669476.0000 - val_mae: 4666.6787\n",
            "Epoch 120/500\n",
            "90/90 [==============================] - 0s 824us/step - loss: 5556.6532 - mse: 61736356.0000 - mae: 5556.6533 - val_loss: 4650.6243 - val_mse: 39473936.0000 - val_mae: 4650.6240\n",
            "Epoch 121/500\n",
            "90/90 [==============================] - 0s 870us/step - loss: 6340.2358 - mse: 77550424.0000 - mae: 6340.2354 - val_loss: 5015.0186 - val_mse: 40352572.0000 - val_mae: 5015.0186\n",
            "Epoch 122/500\n",
            "90/90 [==============================] - 0s 759us/step - loss: 6241.6554 - mse: 78581144.0000 - mae: 6241.6558 - val_loss: 4851.1539 - val_mse: 38784792.0000 - val_mae: 4851.1543\n",
            "Epoch 123/500\n",
            "90/90 [==============================] - 0s 858us/step - loss: 5712.5088 - mse: 73861096.0000 - mae: 5712.5088 - val_loss: 4949.8809 - val_mse: 39302820.0000 - val_mae: 4949.8804\n",
            "Epoch 124/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5653.5473 - mse: 62649304.0000 - mae: 5653.5474 - val_loss: 4420.2899 - val_mse: 35827796.0000 - val_mae: 4420.2900\n",
            "Epoch 125/500\n",
            "90/90 [==============================] - 0s 919us/step - loss: 5462.0183 - mse: 66544868.0000 - mae: 5462.0181 - val_loss: 4352.2978 - val_mse: 34689904.0000 - val_mae: 4352.2979\n",
            "Epoch 126/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5740.4004 - mse: 70879792.0000 - mae: 5740.4004 - val_loss: 4406.3090 - val_mse: 34348264.0000 - val_mae: 4406.3091\n",
            "Epoch 127/500\n",
            "90/90 [==============================] - 0s 846us/step - loss: 6616.4597 - mse: 86637520.0000 - mae: 6616.4600 - val_loss: 4900.9187 - val_mse: 37049596.0000 - val_mae: 4900.9189\n",
            "Epoch 128/500\n",
            "90/90 [==============================] - 0s 951us/step - loss: 6136.4070 - mse: 65420232.0000 - mae: 6136.4067 - val_loss: 4277.8321 - val_mse: 32936726.0000 - val_mae: 4277.8320\n",
            "Epoch 129/500\n",
            "90/90 [==============================] - 0s 809us/step - loss: 6359.8014 - mse: 82946624.0000 - mae: 6359.8013 - val_loss: 4405.1542 - val_mse: 33173792.0000 - val_mae: 4405.1543\n",
            "Epoch 130/500\n",
            "90/90 [==============================] - 0s 894us/step - loss: 5471.0016 - mse: 67371960.0000 - mae: 5471.0015 - val_loss: 4526.1210 - val_mse: 33535528.0000 - val_mae: 4526.1211\n",
            "Epoch 131/500\n",
            "90/90 [==============================] - 0s 814us/step - loss: 5973.8896 - mse: 71992944.0000 - mae: 5973.8896 - val_loss: 4633.2729 - val_mse: 33901948.0000 - val_mae: 4633.2729\n",
            "Epoch 132/500\n",
            "90/90 [==============================] - 0s 834us/step - loss: 5450.0610 - mse: 53636660.0000 - mae: 5450.0605 - val_loss: 4106.0914 - val_mse: 30491016.0000 - val_mae: 4106.0913\n",
            "Epoch 133/500\n",
            "90/90 [==============================] - 0s 775us/step - loss: 5432.1376 - mse: 60017796.0000 - mae: 5432.1377 - val_loss: 4093.4856 - val_mse: 29815406.0000 - val_mae: 4093.4854\n",
            "Epoch 134/500\n",
            "90/90 [==============================] - 0s 883us/step - loss: 5769.8382 - mse: 67343104.0000 - mae: 5769.8384 - val_loss: 4210.7766 - val_mse: 30295668.0000 - val_mae: 4210.7769\n",
            "Epoch 135/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5828.9695 - mse: 64422388.0000 - mae: 5828.9692 - val_loss: 4589.7532 - val_mse: 32444954.0000 - val_mae: 4589.7534\n",
            "Epoch 136/500\n",
            "90/90 [==============================] - 0s 856us/step - loss: 5231.4631 - mse: 57380772.0000 - mae: 5231.4634 - val_loss: 4140.4296 - val_mse: 29317246.0000 - val_mae: 4140.4292\n",
            "Epoch 137/500\n",
            "90/90 [==============================] - 0s 837us/step - loss: 5862.9023 - mse: 85084072.0000 - mae: 5862.9019 - val_loss: 4043.6671 - val_mse: 28506854.0000 - val_mae: 4043.6675\n",
            "Epoch 138/500\n",
            "90/90 [==============================] - 0s 865us/step - loss: 6578.4878 - mse: 89663560.0000 - mae: 6578.4873 - val_loss: 4099.8676 - val_mse: 28608228.0000 - val_mae: 4099.8672\n",
            "Epoch 139/500\n",
            "90/90 [==============================] - 0s 754us/step - loss: 6021.0535 - mse: 77117192.0000 - mae: 6021.0542 - val_loss: 4091.6620 - val_mse: 28138542.0000 - val_mae: 4091.6621\n",
            "Epoch 140/500\n",
            "90/90 [==============================] - 0s 842us/step - loss: 5646.7097 - mse: 63409148.0000 - mae: 5646.7100 - val_loss: 3931.1578 - val_mse: 26983950.0000 - val_mae: 3931.1580\n",
            "Epoch 141/500\n",
            "90/90 [==============================] - 0s 954us/step - loss: 5853.1504 - mse: 59228224.0000 - mae: 5853.1499 - val_loss: 3903.7627 - val_mse: 26263740.0000 - val_mae: 3903.7625\n",
            "Epoch 142/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5746.4629 - mse: 69459576.0000 - mae: 5746.4634 - val_loss: 4060.5785 - val_mse: 27353582.0000 - val_mae: 4060.5784\n",
            "Epoch 143/500\n",
            "90/90 [==============================] - 0s 833us/step - loss: 5509.2759 - mse: 57785752.0000 - mae: 5509.2759 - val_loss: 4103.8036 - val_mse: 27703626.0000 - val_mae: 4103.8037\n",
            "Epoch 144/500\n",
            "90/90 [==============================] - 0s 912us/step - loss: 5717.1882 - mse: 64992460.0000 - mae: 5717.1880 - val_loss: 3791.3680 - val_mse: 25796940.0000 - val_mae: 3791.3682\n",
            "Epoch 145/500\n",
            "90/90 [==============================] - 0s 833us/step - loss: 5887.3062 - mse: 73060544.0000 - mae: 5887.3062 - val_loss: 4313.7902 - val_mse: 27865720.0000 - val_mae: 4313.7900\n",
            "Epoch 146/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 4943.4168 - mse: 45765840.0000 - mae: 4943.4170 - val_loss: 3859.3043 - val_mse: 25700728.0000 - val_mae: 3859.3044\n",
            "Epoch 147/500\n",
            "90/90 [==============================] - 0s 831us/step - loss: 5412.8349 - mse: 57768552.0000 - mae: 5412.8350 - val_loss: 3821.3807 - val_mse: 25941280.0000 - val_mae: 3821.3806\n",
            "Epoch 148/500\n",
            "90/90 [==============================] - 0s 826us/step - loss: 4402.0866 - mse: 40412028.0000 - mae: 4402.0864 - val_loss: 3796.6993 - val_mse: 25669456.0000 - val_mae: 3796.6992\n",
            "Epoch 149/500\n",
            "90/90 [==============================] - 0s 830us/step - loss: 5818.8653 - mse: 62719256.0000 - mae: 5818.8657 - val_loss: 3852.1392 - val_mse: 25428316.0000 - val_mae: 3852.1389\n",
            "Epoch 150/500\n",
            "90/90 [==============================] - 0s 793us/step - loss: 5444.1082 - mse: 54565700.0000 - mae: 5444.1084 - val_loss: 3808.0960 - val_mse: 25081012.0000 - val_mae: 3808.0959\n",
            "Epoch 151/500\n",
            "90/90 [==============================] - 0s 799us/step - loss: 5351.2135 - mse: 62689684.0000 - mae: 5351.2134 - val_loss: 3703.8361 - val_mse: 24144216.0000 - val_mae: 3703.8362\n",
            "Epoch 152/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5750.0604 - mse: 60744976.0000 - mae: 5750.0601 - val_loss: 3898.3405 - val_mse: 25259230.0000 - val_mae: 3898.3406\n",
            "Epoch 153/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5211.1941 - mse: 61471676.0000 - mae: 5211.1943 - val_loss: 3843.3533 - val_mse: 24463460.0000 - val_mae: 3843.3535\n",
            "Epoch 154/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5549.2380 - mse: 58404000.0000 - mae: 5549.2378 - val_loss: 3920.2698 - val_mse: 24937696.0000 - val_mae: 3920.2698\n",
            "Epoch 155/500\n",
            "90/90 [==============================] - 0s 920us/step - loss: 5647.4222 - mse: 67475936.0000 - mae: 5647.4224 - val_loss: 3421.2009 - val_mse: 21629320.0000 - val_mae: 3421.2007\n",
            "Epoch 156/500\n",
            "90/90 [==============================] - 0s 852us/step - loss: 5968.5094 - mse: 66945024.0000 - mae: 5968.5088 - val_loss: 3454.5402 - val_mse: 21480710.0000 - val_mae: 3454.5400\n",
            "Epoch 157/500\n",
            "90/90 [==============================] - 0s 833us/step - loss: 5416.7288 - mse: 54353032.0000 - mae: 5416.7285 - val_loss: 3720.7532 - val_mse: 23123658.0000 - val_mae: 3720.7532\n",
            "Epoch 158/500\n",
            "90/90 [==============================] - 0s 823us/step - loss: 5543.4830 - mse: 55844044.0000 - mae: 5543.4834 - val_loss: 3427.6164 - val_mse: 21608028.0000 - val_mae: 3427.6162\n",
            "Epoch 159/500\n",
            "90/90 [==============================] - 0s 804us/step - loss: 6453.9596 - mse: 78606584.0000 - mae: 6453.9600 - val_loss: 3786.3410 - val_mse: 24455736.0000 - val_mae: 3786.3411\n",
            "Epoch 160/500\n",
            "90/90 [==============================] - 0s 875us/step - loss: 5281.1117 - mse: 52614708.0000 - mae: 5281.1113 - val_loss: 4011.3642 - val_mse: 25358626.0000 - val_mae: 4011.3638\n",
            "Epoch 161/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5640.4419 - mse: 65277952.0000 - mae: 5640.4419 - val_loss: 3569.3143 - val_mse: 22189156.0000 - val_mae: 3569.3142\n",
            "Epoch 162/500\n",
            "90/90 [==============================] - 0s 999us/step - loss: 5600.8071 - mse: 62614996.0000 - mae: 5600.8076 - val_loss: 3600.5757 - val_mse: 22371148.0000 - val_mae: 3600.5757\n",
            "Epoch 163/500\n",
            "90/90 [==============================] - 0s 897us/step - loss: 5277.0435 - mse: 54474900.0000 - mae: 5277.0435 - val_loss: 3273.9346 - val_mse: 20186410.0000 - val_mae: 3273.9348\n",
            "Epoch 164/500\n",
            "90/90 [==============================] - 0s 849us/step - loss: 5601.0490 - mse: 65582320.0000 - mae: 5601.0493 - val_loss: 3543.8607 - val_mse: 21713746.0000 - val_mae: 3543.8608\n",
            "Epoch 165/500\n",
            "90/90 [==============================] - 0s 965us/step - loss: 5343.7007 - mse: 60579584.0000 - mae: 5343.7012 - val_loss: 3430.4344 - val_mse: 20884852.0000 - val_mae: 3430.4343\n",
            "Epoch 166/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 4908.9083 - mse: 48842820.0000 - mae: 4908.9082 - val_loss: 3188.1336 - val_mse: 19533610.0000 - val_mae: 3188.1333\n",
            "Epoch 167/500\n",
            "90/90 [==============================] - 0s 966us/step - loss: 5008.4031 - mse: 41872776.0000 - mae: 5008.4033 - val_loss: 3436.8468 - val_mse: 20631160.0000 - val_mae: 3436.8469\n",
            "Epoch 168/500\n",
            "90/90 [==============================] - 0s 849us/step - loss: 5134.9511 - mse: 47976732.0000 - mae: 5134.9512 - val_loss: 3208.0120 - val_mse: 19383874.0000 - val_mae: 3208.0117\n",
            "Epoch 169/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5735.5184 - mse: 64687668.0000 - mae: 5735.5181 - val_loss: 3355.9390 - val_mse: 20176820.0000 - val_mae: 3355.9390\n",
            "Epoch 170/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 6208.6318 - mse: 69212800.0000 - mae: 6208.6318 - val_loss: 3307.3585 - val_mse: 19902012.0000 - val_mae: 3307.3584\n",
            "Epoch 171/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5405.0341 - mse: 65187096.0000 - mae: 5405.0342 - val_loss: 3587.8263 - val_mse: 21580652.0000 - val_mae: 3587.8264\n",
            "Epoch 172/500\n",
            "90/90 [==============================] - 0s 926us/step - loss: 5551.1791 - mse: 62567760.0000 - mae: 5551.1792 - val_loss: 3187.5439 - val_mse: 19152702.0000 - val_mae: 3187.5442\n",
            "Epoch 173/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5510.9431 - mse: 59771552.0000 - mae: 5510.9438 - val_loss: 3147.9951 - val_mse: 18953014.0000 - val_mae: 3147.9951\n",
            "Epoch 174/500\n",
            "90/90 [==============================] - 0s 894us/step - loss: 5612.3601 - mse: 55706492.0000 - mae: 5612.3599 - val_loss: 3190.6546 - val_mse: 19183886.0000 - val_mae: 3190.6548\n",
            "Epoch 175/500\n",
            "90/90 [==============================] - 0s 860us/step - loss: 4200.5312 - mse: 31741212.0000 - mae: 4200.5312 - val_loss: 3124.6426 - val_mse: 18714444.0000 - val_mae: 3124.6428\n",
            "Epoch 176/500\n",
            "90/90 [==============================] - 0s 964us/step - loss: 5162.3912 - mse: 54588608.0000 - mae: 5162.3911 - val_loss: 3149.3268 - val_mse: 18911194.0000 - val_mae: 3149.3267\n",
            "Epoch 177/500\n",
            "90/90 [==============================] - 0s 960us/step - loss: 4778.3273 - mse: 43169312.0000 - mae: 4778.3276 - val_loss: 3176.7167 - val_mse: 19072182.0000 - val_mae: 3176.7166\n",
            "Epoch 178/500\n",
            "90/90 [==============================] - 0s 864us/step - loss: 5266.2884 - mse: 59916420.0000 - mae: 5266.2881 - val_loss: 3111.9042 - val_mse: 18790150.0000 - val_mae: 3111.9041\n",
            "Epoch 179/500\n",
            "90/90 [==============================] - 0s 972us/step - loss: 5551.2830 - mse: 59888636.0000 - mae: 5551.2827 - val_loss: 3098.1108 - val_mse: 18715170.0000 - val_mae: 3098.1108\n",
            "Epoch 180/500\n",
            "90/90 [==============================] - 0s 781us/step - loss: 4597.2475 - mse: 38312752.0000 - mae: 4597.2476 - val_loss: 3219.8821 - val_mse: 19267822.0000 - val_mae: 3219.8818\n",
            "Epoch 181/500\n",
            "90/90 [==============================] - 0s 808us/step - loss: 4620.3184 - mse: 47016880.0000 - mae: 4620.3179 - val_loss: 3178.2451 - val_mse: 19190398.0000 - val_mae: 3178.2451\n",
            "Epoch 182/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5370.2444 - mse: 55145308.0000 - mae: 5370.2446 - val_loss: 3096.0327 - val_mse: 18870122.0000 - val_mae: 3096.0327\n",
            "Epoch 183/500\n",
            "90/90 [==============================] - 0s 873us/step - loss: 4902.8206 - mse: 47258272.0000 - mae: 4902.8203 - val_loss: 3192.9460 - val_mse: 19633608.0000 - val_mae: 3192.9458\n",
            "Epoch 184/500\n",
            "90/90 [==============================] - 0s 804us/step - loss: 4986.6281 - mse: 48276912.0000 - mae: 4986.6284 - val_loss: 3247.0574 - val_mse: 19996884.0000 - val_mae: 3247.0574\n",
            "Epoch 185/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5431.7828 - mse: 54336488.0000 - mae: 5431.7827 - val_loss: 3442.1600 - val_mse: 21945386.0000 - val_mae: 3442.1602\n",
            "Epoch 186/500\n",
            "90/90 [==============================] - 0s 812us/step - loss: 5492.0624 - mse: 54657944.0000 - mae: 5492.0630 - val_loss: 3381.2479 - val_mse: 20908578.0000 - val_mae: 3381.2478\n",
            "Epoch 187/500\n",
            "90/90 [==============================] - 0s 811us/step - loss: 5922.8512 - mse: 64141096.0000 - mae: 5922.8516 - val_loss: 3252.2406 - val_mse: 20055944.0000 - val_mae: 3252.2410\n",
            "Epoch 188/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5311.2354 - mse: 56666736.0000 - mae: 5311.2354 - val_loss: 3230.0797 - val_mse: 20189894.0000 - val_mae: 3230.0798\n",
            "Epoch 189/500\n",
            "90/90 [==============================] - 0s 830us/step - loss: 6011.0800 - mse: 66193532.0000 - mae: 6011.0801 - val_loss: 3257.6894 - val_mse: 20530666.0000 - val_mae: 3257.6897\n",
            "Epoch 190/500\n",
            "90/90 [==============================] - 0s 957us/step - loss: 4908.0017 - mse: 50535504.0000 - mae: 4908.0020 - val_loss: 3667.0279 - val_mse: 24065450.0000 - val_mae: 3667.0278\n",
            "Epoch 191/500\n",
            "90/90 [==============================] - 0s 848us/step - loss: 4738.3444 - mse: 48144192.0000 - mae: 4738.3442 - val_loss: 3610.8511 - val_mse: 23461624.0000 - val_mae: 3610.8513\n",
            "Epoch 192/500\n",
            "90/90 [==============================] - 0s 807us/step - loss: 5165.3988 - mse: 53330728.0000 - mae: 5165.3984 - val_loss: 3303.0198 - val_mse: 20681352.0000 - val_mae: 3303.0198\n",
            "Epoch 193/500\n",
            "90/90 [==============================] - 0s 878us/step - loss: 5279.2689 - mse: 52865252.0000 - mae: 5279.2686 - val_loss: 3597.3525 - val_mse: 23122538.0000 - val_mae: 3597.3528\n",
            "Epoch 194/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5705.3350 - mse: 59514016.0000 - mae: 5705.3350 - val_loss: 3211.3530 - val_mse: 20318604.0000 - val_mae: 3211.3528\n",
            "Epoch 195/500\n",
            "90/90 [==============================] - 0s 944us/step - loss: 5863.9651 - mse: 65985856.0000 - mae: 5863.9653 - val_loss: 3233.8333 - val_mse: 20263516.0000 - val_mae: 3233.8333\n",
            "Epoch 196/500\n",
            "90/90 [==============================] - 0s 907us/step - loss: 6082.0541 - mse: 68311464.0000 - mae: 6082.0542 - val_loss: 3093.5259 - val_mse: 18911422.0000 - val_mae: 3093.5256\n",
            "Epoch 197/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5980.0696 - mse: 67760152.0000 - mae: 5980.0693 - val_loss: 3197.7044 - val_mse: 19078074.0000 - val_mae: 3197.7041\n",
            "Epoch 198/500\n",
            "90/90 [==============================] - 0s 825us/step - loss: 4623.4457 - mse: 49896512.0000 - mae: 4623.4458 - val_loss: 3076.5894 - val_mse: 18823868.0000 - val_mae: 3076.5896\n",
            "Epoch 199/500\n",
            "90/90 [==============================] - 0s 819us/step - loss: 5134.5688 - mse: 49571244.0000 - mae: 5134.5693 - val_loss: 3135.2859 - val_mse: 19334098.0000 - val_mae: 3135.2861\n",
            "Epoch 200/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 6557.8892 - mse: 91261472.0000 - mae: 6557.8896 - val_loss: 3105.7569 - val_mse: 19041186.0000 - val_mae: 3105.7566\n",
            "Epoch 201/500\n",
            "90/90 [==============================] - 0s 840us/step - loss: 5499.9411 - mse: 56090520.0000 - mae: 5499.9414 - val_loss: 3346.8381 - val_mse: 20270734.0000 - val_mae: 3346.8381\n",
            "Epoch 202/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5534.4033 - mse: 64063384.0000 - mae: 5534.4033 - val_loss: 3163.1021 - val_mse: 19230716.0000 - val_mae: 3163.1021\n",
            "Epoch 203/500\n",
            "90/90 [==============================] - 0s 820us/step - loss: 5015.7553 - mse: 45699860.0000 - mae: 5015.7554 - val_loss: 3213.0439 - val_mse: 19755750.0000 - val_mae: 3213.0437\n",
            "Epoch 204/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5425.1187 - mse: 67413872.0000 - mae: 5425.1187 - val_loss: 3099.5764 - val_mse: 19097168.0000 - val_mae: 3099.5764\n",
            "Epoch 205/500\n",
            "90/90 [==============================] - 0s 856us/step - loss: 5589.7213 - mse: 65326512.0000 - mae: 5589.7212 - val_loss: 3129.0391 - val_mse: 19442462.0000 - val_mae: 3129.0388\n",
            "Epoch 206/500\n",
            "90/90 [==============================] - 0s 795us/step - loss: 5108.2451 - mse: 51300476.0000 - mae: 5108.2451 - val_loss: 3182.5888 - val_mse: 20036446.0000 - val_mae: 3182.5889\n",
            "Epoch 207/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 4875.8188 - mse: 43050936.0000 - mae: 4875.8184 - val_loss: 3443.2510 - val_mse: 21644778.0000 - val_mae: 3443.2507\n",
            "Epoch 208/500\n",
            "90/90 [==============================] - 0s 979us/step - loss: 5453.0025 - mse: 55363520.0000 - mae: 5453.0024 - val_loss: 3508.8152 - val_mse: 22585322.0000 - val_mae: 3508.8152\n",
            "Epoch 209/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 4965.4189 - mse: 53460444.0000 - mae: 4965.4189 - val_loss: 3307.1096 - val_mse: 21268130.0000 - val_mae: 3307.1096\n",
            "Epoch 210/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5153.6245 - mse: 47417264.0000 - mae: 5153.6245 - val_loss: 3387.4051 - val_mse: 21666202.0000 - val_mae: 3387.4048\n",
            "Epoch 211/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5985.8325 - mse: 76337592.0000 - mae: 5985.8320 - val_loss: 3392.1570 - val_mse: 21818076.0000 - val_mae: 3392.1570\n",
            "Epoch 212/500\n",
            "90/90 [==============================] - 0s 853us/step - loss: 5111.0396 - mse: 46161564.0000 - mae: 5111.0396 - val_loss: 3207.2815 - val_mse: 20131312.0000 - val_mae: 3207.2815\n",
            "Epoch 213/500\n",
            "90/90 [==============================] - 0s 827us/step - loss: 4792.3350 - mse: 49287344.0000 - mae: 4792.3350 - val_loss: 3224.4352 - val_mse: 20096470.0000 - val_mae: 3224.4353\n",
            "Epoch 214/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5271.0793 - mse: 57348792.0000 - mae: 5271.0796 - val_loss: 3172.9998 - val_mse: 19768178.0000 - val_mae: 3172.9998\n",
            "Epoch 215/500\n",
            "90/90 [==============================] - 0s 2ms/step - loss: 5040.6695 - mse: 52545524.0000 - mae: 5040.6694 - val_loss: 3332.3950 - val_mse: 20649738.0000 - val_mae: 3332.3950\n",
            "Epoch 216/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5379.1117 - mse: 59036540.0000 - mae: 5379.1123 - val_loss: 3265.8447 - val_mse: 20028764.0000 - val_mae: 3265.8447\n",
            "Epoch 217/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 4878.5154 - mse: 46616684.0000 - mae: 4878.5156 - val_loss: 3106.7124 - val_mse: 19045138.0000 - val_mae: 3106.7124\n",
            "Epoch 218/500\n",
            "90/90 [==============================] - 0s 865us/step - loss: 5540.1614 - mse: 57127724.0000 - mae: 5540.1616 - val_loss: 3041.8326 - val_mse: 18495644.0000 - val_mae: 3041.8325\n",
            "Epoch 219/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 6300.3209 - mse: 80378584.0000 - mae: 6300.3208 - val_loss: 3180.8743 - val_mse: 19281096.0000 - val_mae: 3180.8740\n",
            "Epoch 220/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 4985.6582 - mse: 48818104.0000 - mae: 4985.6587 - val_loss: 3036.6371 - val_mse: 18432274.0000 - val_mae: 3036.6372\n",
            "Epoch 221/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5164.3657 - mse: 54715000.0000 - mae: 5164.3657 - val_loss: 3535.3113 - val_mse: 21403252.0000 - val_mae: 3535.3110\n",
            "Epoch 222/500\n",
            "90/90 [==============================] - 0s 860us/step - loss: 5236.2091 - mse: 45371724.0000 - mae: 5236.2085 - val_loss: 3093.3636 - val_mse: 18684736.0000 - val_mae: 3093.3638\n",
            "Epoch 223/500\n",
            "90/90 [==============================] - 0s 821us/step - loss: 5183.1520 - mse: 48749192.0000 - mae: 5183.1519 - val_loss: 3465.1097 - val_mse: 20892312.0000 - val_mae: 3465.1094\n",
            "Epoch 224/500\n",
            "90/90 [==============================] - 0s 826us/step - loss: 5861.6212 - mse: 60953800.0000 - mae: 5861.6206 - val_loss: 2996.3266 - val_mse: 18001658.0000 - val_mae: 2996.3264\n",
            "Epoch 225/500\n",
            "90/90 [==============================] - 0s 944us/step - loss: 5120.0415 - mse: 52359372.0000 - mae: 5120.0415 - val_loss: 3076.5016 - val_mse: 18512036.0000 - val_mae: 3076.5017\n",
            "Epoch 226/500\n",
            "90/90 [==============================] - 0s 843us/step - loss: 6398.5347 - mse: 77431744.0000 - mae: 6398.5347 - val_loss: 3084.4345 - val_mse: 18561154.0000 - val_mae: 3084.4343\n",
            "Epoch 227/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5214.8921 - mse: 48704624.0000 - mae: 5214.8926 - val_loss: 3127.5754 - val_mse: 19299140.0000 - val_mae: 3127.5754\n",
            "Epoch 228/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5234.3415 - mse: 50198476.0000 - mae: 5234.3413 - val_loss: 3328.2229 - val_mse: 19991056.0000 - val_mae: 3328.2229\n",
            "Epoch 229/500\n",
            "90/90 [==============================] - 0s 950us/step - loss: 5829.4634 - mse: 71962984.0000 - mae: 5829.4639 - val_loss: 2961.8871 - val_mse: 17657588.0000 - val_mae: 2961.8867\n",
            "Epoch 230/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 4811.7747 - mse: 49299728.0000 - mae: 4811.7744 - val_loss: 2960.9771 - val_mse: 17545508.0000 - val_mae: 2960.9771\n",
            "Epoch 231/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 4971.4897 - mse: 51453816.0000 - mae: 4971.4897 - val_loss: 2922.4280 - val_mse: 17257520.0000 - val_mae: 2922.4277\n",
            "Epoch 232/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5235.6445 - mse: 61308980.0000 - mae: 5235.6445 - val_loss: 3000.4681 - val_mse: 18017732.0000 - val_mae: 3000.4680\n",
            "Epoch 233/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 4978.6485 - mse: 51232096.0000 - mae: 4978.6484 - val_loss: 2970.8929 - val_mse: 17790126.0000 - val_mae: 2970.8931\n",
            "Epoch 234/500\n",
            "90/90 [==============================] - 0s 935us/step - loss: 5287.6828 - mse: 53883996.0000 - mae: 5287.6831 - val_loss: 3098.3239 - val_mse: 18438510.0000 - val_mae: 3098.3240\n",
            "Epoch 235/500\n",
            "90/90 [==============================] - 0s 998us/step - loss: 5103.5576 - mse: 51202504.0000 - mae: 5103.5576 - val_loss: 3196.7339 - val_mse: 19165116.0000 - val_mae: 3196.7341\n",
            "Epoch 236/500\n",
            "90/90 [==============================] - 0s 935us/step - loss: 5428.0453 - mse: 54390236.0000 - mae: 5428.0459 - val_loss: 3056.7627 - val_mse: 18273402.0000 - val_mae: 3056.7629\n",
            "Epoch 237/500\n",
            "90/90 [==============================] - 0s 966us/step - loss: 4840.5078 - mse: 49137264.0000 - mae: 4840.5078 - val_loss: 2979.8195 - val_mse: 17802936.0000 - val_mae: 2979.8193\n",
            "Epoch 238/500\n",
            "90/90 [==============================] - 0s 878us/step - loss: 4995.0835 - mse: 44385736.0000 - mae: 4995.0835 - val_loss: 2999.9800 - val_mse: 18127796.0000 - val_mae: 2999.9800\n",
            "Epoch 239/500\n",
            "90/90 [==============================] - 0s 975us/step - loss: 5272.3646 - mse: 60885544.0000 - mae: 5272.3647 - val_loss: 3013.5938 - val_mse: 18457366.0000 - val_mae: 3013.5938\n",
            "Epoch 240/500\n",
            "90/90 [==============================] - 0s 891us/step - loss: 4556.2675 - mse: 44152048.0000 - mae: 4556.2676 - val_loss: 3488.1420 - val_mse: 21306406.0000 - val_mae: 3488.1421\n",
            "Epoch 241/500\n",
            "90/90 [==============================] - 0s 989us/step - loss: 5527.7883 - mse: 59074536.0000 - mae: 5527.7881 - val_loss: 2990.6576 - val_mse: 17986966.0000 - val_mae: 2990.6577\n",
            "Epoch 242/500\n",
            "90/90 [==============================] - 0s 845us/step - loss: 5019.8524 - mse: 54813436.0000 - mae: 5019.8521 - val_loss: 3000.7838 - val_mse: 18072292.0000 - val_mae: 3000.7837\n",
            "Epoch 243/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5755.2643 - mse: 68247816.0000 - mae: 5755.2637 - val_loss: 2971.2323 - val_mse: 17770872.0000 - val_mae: 2971.2322\n",
            "Epoch 244/500\n",
            "90/90 [==============================] - 0s 819us/step - loss: 4966.0117 - mse: 49757332.0000 - mae: 4966.0112 - val_loss: 2970.3585 - val_mse: 17670324.0000 - val_mae: 2970.3584\n",
            "Epoch 245/500\n",
            "90/90 [==============================] - 0s 951us/step - loss: 5396.3166 - mse: 56565056.0000 - mae: 5396.3169 - val_loss: 2948.3845 - val_mse: 17523860.0000 - val_mae: 2948.3848\n",
            "Epoch 246/500\n",
            "90/90 [==============================] - 0s 767us/step - loss: 6032.7087 - mse: 74111704.0000 - mae: 6032.7085 - val_loss: 2952.5390 - val_mse: 17563050.0000 - val_mae: 2952.5388\n",
            "Epoch 247/500\n",
            "90/90 [==============================] - 0s 803us/step - loss: 5423.5641 - mse: 56832364.0000 - mae: 5423.5645 - val_loss: 3059.5634 - val_mse: 18407762.0000 - val_mae: 3059.5632\n",
            "Epoch 248/500\n",
            "90/90 [==============================] - 0s 820us/step - loss: 4890.4816 - mse: 45309224.0000 - mae: 4890.4814 - val_loss: 3254.9753 - val_mse: 19489744.0000 - val_mae: 3254.9753\n",
            "Epoch 249/500\n",
            "90/90 [==============================] - 0s 762us/step - loss: 4882.9169 - mse: 56158920.0000 - mae: 4882.9175 - val_loss: 3082.4287 - val_mse: 18311368.0000 - val_mae: 3082.4287\n",
            "Epoch 250/500\n",
            "90/90 [==============================] - 0s 848us/step - loss: 4551.6739 - mse: 41537256.0000 - mae: 4551.6738 - val_loss: 2993.1641 - val_mse: 17810976.0000 - val_mae: 2993.1638\n",
            "Epoch 251/500\n",
            "90/90 [==============================] - 0s 808us/step - loss: 4497.1074 - mse: 40677980.0000 - mae: 4497.1074 - val_loss: 2921.1678 - val_mse: 17397296.0000 - val_mae: 2921.1680\n",
            "Epoch 252/500\n",
            "90/90 [==============================] - 0s 841us/step - loss: 4995.5163 - mse: 50172320.0000 - mae: 4995.5161 - val_loss: 2941.7619 - val_mse: 17667192.0000 - val_mae: 2941.7617\n",
            "Epoch 253/500\n",
            "90/90 [==============================] - 0s 834us/step - loss: 5400.6464 - mse: 55688196.0000 - mae: 5400.6465 - val_loss: 2945.4493 - val_mse: 17689326.0000 - val_mae: 2945.4492\n",
            "Epoch 254/500\n",
            "90/90 [==============================] - 0s 796us/step - loss: 5140.4090 - mse: 50394760.0000 - mae: 5140.4092 - val_loss: 3284.2520 - val_mse: 19704206.0000 - val_mae: 3284.2522\n",
            "Epoch 255/500\n",
            "90/90 [==============================] - 0s 942us/step - loss: 5112.3069 - mse: 61908656.0000 - mae: 5112.3066 - val_loss: 2933.5738 - val_mse: 17671174.0000 - val_mae: 2933.5740\n",
            "Epoch 256/500\n",
            "90/90 [==============================] - 0s 770us/step - loss: 5187.0698 - mse: 52666384.0000 - mae: 5187.0693 - val_loss: 3165.1903 - val_mse: 19423254.0000 - val_mae: 3165.1902\n",
            "Epoch 257/500\n",
            "90/90 [==============================] - 0s 961us/step - loss: 4759.3029 - mse: 45840036.0000 - mae: 4759.3027 - val_loss: 3065.7867 - val_mse: 18859136.0000 - val_mae: 3065.7869\n",
            "Epoch 258/500\n",
            "90/90 [==============================] - 0s 992us/step - loss: 5803.8638 - mse: 74944528.0000 - mae: 5803.8638 - val_loss: 2954.5324 - val_mse: 17935668.0000 - val_mae: 2954.5327\n",
            "Epoch 259/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 4681.8746 - mse: 51581048.0000 - mae: 4681.8745 - val_loss: 2948.0049 - val_mse: 17720846.0000 - val_mae: 2948.0049\n",
            "Epoch 260/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5506.3322 - mse: 59236388.0000 - mae: 5506.3320 - val_loss: 2912.5874 - val_mse: 17363584.0000 - val_mae: 2912.5874\n",
            "Epoch 261/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5249.6982 - mse: 61327800.0000 - mae: 5249.6982 - val_loss: 2943.7521 - val_mse: 17630992.0000 - val_mae: 2943.7522\n",
            "Epoch 262/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5257.2034 - mse: 53243528.0000 - mae: 5257.2031 - val_loss: 3010.3003 - val_mse: 17923582.0000 - val_mae: 3010.3000\n",
            "Epoch 263/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5570.7463 - mse: 60336784.0000 - mae: 5570.7466 - val_loss: 2959.3572 - val_mse: 17689026.0000 - val_mae: 2959.3572\n",
            "Epoch 264/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5942.6327 - mse: 79039320.0000 - mae: 5942.6318 - val_loss: 3272.3033 - val_mse: 19383946.0000 - val_mae: 3272.3035\n",
            "Epoch 265/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5515.3311 - mse: 60983416.0000 - mae: 5515.3311 - val_loss: 3189.6541 - val_mse: 19038832.0000 - val_mae: 3189.6541\n",
            "Epoch 266/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5094.9159 - mse: 52681784.0000 - mae: 5094.9160 - val_loss: 2909.5423 - val_mse: 17305494.0000 - val_mae: 2909.5422\n",
            "Epoch 267/500\n",
            "90/90 [==============================] - 0s 883us/step - loss: 5208.4916 - mse: 49203052.0000 - mae: 5208.4917 - val_loss: 2920.7196 - val_mse: 17351110.0000 - val_mae: 2920.7195\n",
            "Epoch 268/500\n",
            "90/90 [==============================] - 0s 978us/step - loss: 4325.1656 - mse: 39663728.0000 - mae: 4325.1655 - val_loss: 2926.7151 - val_mse: 17490232.0000 - val_mae: 2926.7153\n",
            "Epoch 269/500\n",
            "90/90 [==============================] - 0s 863us/step - loss: 4820.0569 - mse: 48739528.0000 - mae: 4820.0571 - val_loss: 2932.5278 - val_mse: 17614922.0000 - val_mae: 2932.5278\n",
            "Epoch 270/500\n",
            "90/90 [==============================] - 0s 867us/step - loss: 5128.5328 - mse: 58374560.0000 - mae: 5128.5332 - val_loss: 2943.8238 - val_mse: 17624530.0000 - val_mae: 2943.8240\n",
            "Epoch 271/500\n",
            "90/90 [==============================] - 0s 856us/step - loss: 5200.1201 - mse: 57907336.0000 - mae: 5200.1206 - val_loss: 2929.3613 - val_mse: 17567216.0000 - val_mae: 2929.3616\n",
            "Epoch 272/500\n",
            "90/90 [==============================] - 0s 891us/step - loss: 4500.8750 - mse: 36656800.0000 - mae: 4500.8750 - val_loss: 2952.5841 - val_mse: 17646268.0000 - val_mae: 2952.5840\n",
            "Epoch 273/500\n",
            "90/90 [==============================] - 0s 866us/step - loss: 6394.4913 - mse: 89389464.0000 - mae: 6394.4917 - val_loss: 2938.0792 - val_mse: 17612236.0000 - val_mae: 2938.0791\n",
            "Epoch 274/500\n",
            "90/90 [==============================] - 0s 826us/step - loss: 4999.4081 - mse: 49559052.0000 - mae: 4999.4082 - val_loss: 3016.2458 - val_mse: 18121460.0000 - val_mae: 3016.2458\n",
            "Epoch 275/500\n",
            "90/90 [==============================] - 0s 839us/step - loss: 5217.0355 - mse: 55566528.0000 - mae: 5217.0361 - val_loss: 3055.0974 - val_mse: 18607628.0000 - val_mae: 3055.0972\n",
            "Epoch 276/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 4732.7131 - mse: 42849000.0000 - mae: 4732.7134 - val_loss: 2995.4861 - val_mse: 18435488.0000 - val_mae: 2995.4861\n",
            "Epoch 277/500\n",
            "90/90 [==============================] - 0s 873us/step - loss: 5795.5851 - mse: 58345324.0000 - mae: 5795.5850 - val_loss: 2986.5250 - val_mse: 18253016.0000 - val_mae: 2986.5249\n",
            "Epoch 278/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5584.5495 - mse: 61490584.0000 - mae: 5584.5498 - val_loss: 2968.6699 - val_mse: 18097156.0000 - val_mae: 2968.6702\n",
            "Epoch 279/500\n",
            "90/90 [==============================] - 0s 857us/step - loss: 4868.5102 - mse: 49240412.0000 - mae: 4868.5103 - val_loss: 2972.3681 - val_mse: 18254672.0000 - val_mae: 2972.3682\n",
            "Epoch 280/500\n",
            "90/90 [==============================] - 0s 788us/step - loss: 4724.4088 - mse: 43642476.0000 - mae: 4724.4082 - val_loss: 3201.6615 - val_mse: 20276892.0000 - val_mae: 3201.6614\n",
            "Epoch 281/500\n",
            "90/90 [==============================] - 0s 802us/step - loss: 5179.9840 - mse: 51579892.0000 - mae: 5179.9839 - val_loss: 3509.8547 - val_mse: 21759410.0000 - val_mae: 3509.8550\n",
            "Epoch 282/500\n",
            "90/90 [==============================] - 0s 953us/step - loss: 5282.1729 - mse: 54712284.0000 - mae: 5282.1733 - val_loss: 3032.5868 - val_mse: 18898242.0000 - val_mae: 3032.5869\n",
            "Epoch 283/500\n",
            "90/90 [==============================] - 0s 803us/step - loss: 5226.0267 - mse: 53018136.0000 - mae: 5226.0264 - val_loss: 3049.4399 - val_mse: 18696062.0000 - val_mae: 3049.4399\n",
            "Epoch 284/500\n",
            "90/90 [==============================] - 0s 791us/step - loss: 5326.7133 - mse: 57041876.0000 - mae: 5326.7134 - val_loss: 2964.8635 - val_mse: 18229424.0000 - val_mae: 2964.8635\n",
            "Epoch 285/500\n",
            "90/90 [==============================] - 0s 843us/step - loss: 4984.2160 - mse: 48368784.0000 - mae: 4984.2158 - val_loss: 3023.0557 - val_mse: 18404722.0000 - val_mae: 3023.0557\n",
            "Epoch 286/500\n",
            "90/90 [==============================] - 0s 822us/step - loss: 5156.1477 - mse: 51668400.0000 - mae: 5156.1475 - val_loss: 3012.9661 - val_mse: 18586504.0000 - val_mae: 3012.9661\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 20 samples, validate on 10 samples\n",
            "Epoch 1/500\n",
            "20/20 [==============================] - 2s 101ms/step - loss: 2131.0349 - mse: 8338605.5000 - mae: 2131.0347 - val_loss: 1255.1714 - val_mse: 1816702.7500 - val_mae: 1255.1714\n",
            "Epoch 2/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2130.9966 - mse: 8338453.0000 - mae: 2130.9966 - val_loss: 1255.1370 - val_mse: 1816617.0000 - val_mae: 1255.1370\n",
            "Epoch 3/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2130.9617 - mse: 8338322.5000 - mae: 2130.9617 - val_loss: 1255.1022 - val_mse: 1816531.6250 - val_mae: 1255.1022\n",
            "Epoch 4/500\n",
            "20/20 [==============================] - 0s 915us/step - loss: 2130.9192 - mse: 8338119.0000 - mae: 2130.9192 - val_loss: 1255.0614 - val_mse: 1816433.0000 - val_mae: 1255.0614\n",
            "Epoch 5/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2130.8720 - mse: 8337958.5000 - mae: 2130.8721 - val_loss: 1255.0109 - val_mse: 1816316.6250 - val_mae: 1255.0109\n",
            "Epoch 6/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 2130.7860 - mse: 8337681.0000 - mae: 2130.7859 - val_loss: 1254.9332 - val_mse: 1816146.7500 - val_mae: 1254.9332\n",
            "Epoch 7/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2130.6476 - mse: 8337296.0000 - mae: 2130.6477 - val_loss: 1254.8070 - val_mse: 1815886.0000 - val_mae: 1254.8070\n",
            "Epoch 8/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2130.1521 - mse: 8336086.5000 - mae: 2130.1519 - val_loss: 1254.4924 - val_mse: 1815282.7500 - val_mae: 1254.4924\n",
            "Epoch 9/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2128.7944 - mse: 8333664.0000 - mae: 2128.7944 - val_loss: 1253.6975 - val_mse: 1813834.6250 - val_mae: 1253.6975\n",
            "Epoch 10/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2125.1724 - mse: 8325796.0000 - mae: 2125.1724 - val_loss: 1251.7180 - val_mse: 1810322.7500 - val_mae: 1251.7180\n",
            "Epoch 11/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2119.0536 - mse: 8317290.5000 - mae: 2119.0535 - val_loss: 1248.4962 - val_mse: 1804643.3750 - val_mae: 1248.4962\n",
            "Epoch 12/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 2109.4745 - mse: 8296876.0000 - mae: 2109.4746 - val_loss: 1243.9153 - val_mse: 1796596.7500 - val_mae: 1243.9153\n",
            "Epoch 13/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2101.0231 - mse: 8291086.5000 - mae: 2101.0229 - val_loss: 1238.7076 - val_mse: 1787505.0000 - val_mae: 1238.7076\n",
            "Epoch 14/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2093.3336 - mse: 8245251.0000 - mae: 2093.3335 - val_loss: 1232.7880 - val_mse: 1777272.3750 - val_mae: 1232.7880\n",
            "Epoch 15/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2081.1324 - mse: 8235021.5000 - mae: 2081.1323 - val_loss: 1225.9490 - val_mse: 1765570.3750 - val_mae: 1225.9490\n",
            "Epoch 16/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2061.0959 - mse: 8203473.0000 - mae: 2061.0959 - val_loss: 1220.0044 - val_mse: 1755473.0000 - val_mae: 1220.0044\n",
            "Epoch 17/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2058.6998 - mse: 8197232.0000 - mae: 2058.7000 - val_loss: 1212.1663 - val_mse: 1742326.3750 - val_mae: 1212.1663\n",
            "Epoch 18/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2043.5503 - mse: 8146633.5000 - mae: 2043.5504 - val_loss: 1205.6791 - val_mse: 1731539.6250 - val_mae: 1205.6791\n",
            "Epoch 19/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2039.3141 - mse: 8138017.5000 - mae: 2039.3141 - val_loss: 1199.1809 - val_mse: 1720851.6250 - val_mae: 1199.1809\n",
            "Epoch 20/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2049.9047 - mse: 8160763.0000 - mae: 2049.9048 - val_loss: 1192.5055 - val_mse: 1709942.3750 - val_mae: 1192.5055\n",
            "Epoch 21/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 2032.1030 - mse: 8127212.0000 - mae: 2032.1029 - val_loss: 1184.5122 - val_mse: 1697069.3750 - val_mae: 1184.5122\n",
            "Epoch 22/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2017.4401 - mse: 8018997.0000 - mae: 2017.4402 - val_loss: 1179.8077 - val_mse: 1689502.2500 - val_mae: 1179.8077\n",
            "Epoch 23/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2010.0984 - mse: 8032408.0000 - mae: 2010.0984 - val_loss: 1172.1882 - val_mse: 1677465.2500 - val_mae: 1172.1882\n",
            "Epoch 24/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2015.9373 - mse: 8020982.5000 - mae: 2015.9371 - val_loss: 1165.8435 - val_mse: 1667413.0000 - val_mae: 1165.8435\n",
            "Epoch 25/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2007.4243 - mse: 8014188.0000 - mae: 2007.4242 - val_loss: 1158.8391 - val_mse: 1656434.5000 - val_mae: 1158.8391\n",
            "Epoch 26/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2004.3293 - mse: 7996261.5000 - mae: 2004.3293 - val_loss: 1154.9795 - val_mse: 1650347.6250 - val_mae: 1154.9795\n",
            "Epoch 27/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1997.5369 - mse: 7942323.0000 - mae: 1997.5369 - val_loss: 1149.0867 - val_mse: 1641239.7500 - val_mae: 1149.0867\n",
            "Epoch 28/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1995.4741 - mse: 7962665.5000 - mae: 1995.4742 - val_loss: 1141.6735 - val_mse: 1629905.5000 - val_mae: 1141.6735\n",
            "Epoch 29/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1990.9409 - mse: 7919983.0000 - mae: 1990.9410 - val_loss: 1136.3005 - val_mse: 1621624.5000 - val_mae: 1136.3005\n",
            "Epoch 30/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1974.2482 - mse: 7879475.0000 - mae: 1974.2480 - val_loss: 1130.6606 - val_mse: 1613148.5000 - val_mae: 1130.6606\n",
            "Epoch 31/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1989.6720 - mse: 7890609.5000 - mae: 1989.6721 - val_loss: 1128.2906 - val_mse: 1609296.7500 - val_mae: 1128.2906\n",
            "Epoch 32/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1986.7047 - mse: 7889723.0000 - mae: 1986.7047 - val_loss: 1125.9597 - val_mse: 1605469.2500 - val_mae: 1125.9597\n",
            "Epoch 33/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 2000.8147 - mse: 7911848.0000 - mae: 2000.8148 - val_loss: 1119.5449 - val_mse: 1595925.0000 - val_mae: 1119.5449\n",
            "Epoch 34/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1977.8508 - mse: 7826410.5000 - mae: 1977.8508 - val_loss: 1113.0236 - val_mse: 1586270.2500 - val_mae: 1113.0236\n",
            "Epoch 35/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1993.2989 - mse: 7871666.5000 - mae: 1993.2988 - val_loss: 1109.4485 - val_mse: 1580867.5000 - val_mae: 1109.4485\n",
            "Epoch 36/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1981.5184 - mse: 7875180.0000 - mae: 1981.5183 - val_loss: 1109.0231 - val_mse: 1579644.6250 - val_mae: 1109.0231\n",
            "Epoch 37/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1944.1901 - mse: 7691249.0000 - mae: 1944.1902 - val_loss: 1102.2737 - val_mse: 1569655.0000 - val_mae: 1102.2737\n",
            "Epoch 38/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1944.4551 - mse: 7667648.0000 - mae: 1944.4551 - val_loss: 1099.6453 - val_mse: 1565214.1250 - val_mae: 1099.6453\n",
            "Epoch 39/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1942.8650 - mse: 7617900.0000 - mae: 1942.8649 - val_loss: 1094.4568 - val_mse: 1557209.6250 - val_mae: 1094.4568\n",
            "Epoch 40/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1957.2861 - mse: 7679073.5000 - mae: 1957.2861 - val_loss: 1089.7944 - val_mse: 1550142.7500 - val_mae: 1089.7944\n",
            "Epoch 41/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1949.2989 - mse: 7642829.0000 - mae: 1949.2988 - val_loss: 1087.8931 - val_mse: 1546852.3750 - val_mae: 1087.8931\n",
            "Epoch 42/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1921.4645 - mse: 7566402.5000 - mae: 1921.4645 - val_loss: 1080.2162 - val_mse: 1535717.7500 - val_mae: 1080.2162\n",
            "Epoch 43/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1952.0425 - mse: 7598498.5000 - mae: 1952.0426 - val_loss: 1076.5017 - val_mse: 1529870.3750 - val_mae: 1076.5017\n",
            "Epoch 44/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1910.6854 - mse: 7369091.0000 - mae: 1910.6853 - val_loss: 1070.5255 - val_mse: 1520954.3750 - val_mae: 1070.5255\n",
            "Epoch 45/500\n",
            "20/20 [==============================] - 0s 977us/step - loss: 1943.7556 - mse: 7565737.0000 - mae: 1943.7556 - val_loss: 1068.7336 - val_mse: 1517796.2500 - val_mae: 1068.7336\n",
            "Epoch 46/500\n",
            "20/20 [==============================] - 0s 977us/step - loss: 1944.9623 - mse: 7561865.5000 - mae: 1944.9625 - val_loss: 1066.8817 - val_mse: 1514461.6250 - val_mae: 1066.8817\n",
            "Epoch 47/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1949.5470 - mse: 7587697.5000 - mae: 1949.5469 - val_loss: 1065.5349 - val_mse: 1511837.6250 - val_mae: 1065.5349\n",
            "Epoch 48/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1941.9867 - mse: 7504309.0000 - mae: 1941.9867 - val_loss: 1061.4905 - val_mse: 1505576.6250 - val_mae: 1061.4905\n",
            "Epoch 49/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1936.8669 - mse: 7472507.0000 - mae: 1936.8668 - val_loss: 1054.3546 - val_mse: 1495242.3750 - val_mae: 1054.3546\n",
            "Epoch 50/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1929.3702 - mse: 7369771.0000 - mae: 1929.3704 - val_loss: 1054.1447 - val_mse: 1493799.6250 - val_mae: 1054.1447\n",
            "Epoch 51/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1916.7495 - mse: 7440040.0000 - mae: 1916.7494 - val_loss: 1051.6071 - val_mse: 1489396.5000 - val_mae: 1051.6071\n",
            "Epoch 52/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1953.8533 - mse: 7439761.5000 - mae: 1953.8533 - val_loss: 1053.4417 - val_mse: 1490310.7500 - val_mae: 1053.4417\n",
            "Epoch 53/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1969.0145 - mse: 7465179.0000 - mae: 1969.0144 - val_loss: 1054.4929 - val_mse: 1490079.0000 - val_mae: 1054.4929\n",
            "Epoch 54/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1933.3477 - mse: 7421990.5000 - mae: 1933.3477 - val_loss: 1051.4518 - val_mse: 1484565.8750 - val_mae: 1051.4518\n",
            "Epoch 55/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1895.3429 - mse: 7245180.0000 - mae: 1895.3430 - val_loss: 1051.2598 - val_mse: 1482401.3750 - val_mae: 1051.2598\n",
            "Epoch 56/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1913.7646 - mse: 7291499.0000 - mae: 1913.7646 - val_loss: 1055.2318 - val_mse: 1485568.2500 - val_mae: 1055.2318\n",
            "Epoch 57/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1919.3837 - mse: 7343823.0000 - mae: 1919.3835 - val_loss: 1058.0986 - val_mse: 1487160.7500 - val_mae: 1058.0986\n",
            "Epoch 58/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1903.9494 - mse: 7190877.5000 - mae: 1903.9495 - val_loss: 1058.5420 - val_mse: 1484506.6250 - val_mae: 1058.5420\n",
            "Epoch 59/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1889.7988 - mse: 7235992.0000 - mae: 1889.7988 - val_loss: 1059.7383 - val_mse: 1482277.3750 - val_mae: 1059.7383\n",
            "Epoch 60/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1876.7255 - mse: 7109724.0000 - mae: 1876.7256 - val_loss: 1059.2604 - val_mse: 1477485.7500 - val_mae: 1059.2604\n",
            "Epoch 61/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1887.7161 - mse: 7156309.0000 - mae: 1887.7161 - val_loss: 1066.8831 - val_mse: 1482362.7500 - val_mae: 1066.8831\n",
            "Epoch 62/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1860.8387 - mse: 7187293.0000 - mae: 1860.8386 - val_loss: 1068.3708 - val_mse: 1480661.7500 - val_mae: 1068.3708\n",
            "Epoch 63/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1893.0392 - mse: 7269821.0000 - mae: 1893.0393 - val_loss: 1066.7793 - val_mse: 1475342.8750 - val_mae: 1066.7793\n",
            "Epoch 64/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1857.3177 - mse: 7134352.0000 - mae: 1857.3176 - val_loss: 1059.5442 - val_mse: 1463014.7500 - val_mae: 1059.5442\n",
            "Epoch 65/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1888.4120 - mse: 7219759.0000 - mae: 1888.4121 - val_loss: 1051.0046 - val_mse: 1451741.7500 - val_mae: 1051.0046\n",
            "Epoch 66/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1909.3076 - mse: 7410228.0000 - mae: 1909.3076 - val_loss: 1057.5188 - val_mse: 1455416.2500 - val_mae: 1057.5188\n",
            "Epoch 67/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1878.0652 - mse: 7210176.0000 - mae: 1878.0652 - val_loss: 1063.0911 - val_mse: 1458651.8750 - val_mae: 1063.0911\n",
            "Epoch 68/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1854.8668 - mse: 7088271.0000 - mae: 1854.8668 - val_loss: 1055.6959 - val_mse: 1447406.3750 - val_mae: 1055.6959\n",
            "Epoch 69/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1834.1588 - mse: 7044268.0000 - mae: 1834.1588 - val_loss: 1043.7610 - val_mse: 1429996.2500 - val_mae: 1043.7610\n",
            "Epoch 70/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1826.3040 - mse: 6969333.5000 - mae: 1826.3040 - val_loss: 1036.9485 - val_mse: 1419459.6250 - val_mae: 1036.9485\n",
            "Epoch 71/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1849.1052 - mse: 6912605.5000 - mae: 1849.1052 - val_loss: 1039.1051 - val_mse: 1417617.7500 - val_mae: 1039.1051\n",
            "Epoch 72/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1836.8157 - mse: 7104624.0000 - mae: 1836.8157 - val_loss: 1028.5453 - val_mse: 1403118.1250 - val_mae: 1028.5453\n",
            "Epoch 73/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1831.1575 - mse: 7094410.5000 - mae: 1831.1575 - val_loss: 1031.9313 - val_mse: 1401638.2500 - val_mae: 1031.9313\n",
            "Epoch 74/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1810.2614 - mse: 6844541.0000 - mae: 1810.2614 - val_loss: 1025.2483 - val_mse: 1391200.5000 - val_mae: 1025.2483\n",
            "Epoch 75/500\n",
            "20/20 [==============================] - 0s 975us/step - loss: 1825.6440 - mse: 6864223.0000 - mae: 1825.6439 - val_loss: 1021.2859 - val_mse: 1382846.7500 - val_mae: 1021.2859\n",
            "Epoch 76/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1819.5161 - mse: 6833255.0000 - mae: 1819.5160 - val_loss: 1020.6795 - val_mse: 1378821.1250 - val_mae: 1020.6795\n",
            "Epoch 77/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1821.7632 - mse: 6910150.5000 - mae: 1821.7633 - val_loss: 1016.4914 - val_mse: 1371127.7500 - val_mae: 1016.4914\n",
            "Epoch 78/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1803.8695 - mse: 6826039.0000 - mae: 1803.8695 - val_loss: 1005.1346 - val_mse: 1353370.7500 - val_mae: 1005.1346\n",
            "Epoch 79/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1867.8112 - mse: 7099329.0000 - mae: 1867.8112 - val_loss: 1009.6717 - val_mse: 1354363.7500 - val_mae: 1009.6717\n",
            "Epoch 80/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1784.9531 - mse: 6637005.0000 - mae: 1784.9531 - val_loss: 997.6564 - val_mse: 1337897.2500 - val_mae: 997.6564\n",
            "Epoch 81/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1799.0641 - mse: 6735702.5000 - mae: 1799.0641 - val_loss: 983.7057 - val_mse: 1318970.7500 - val_mae: 983.7057\n",
            "Epoch 82/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1801.6959 - mse: 6642126.0000 - mae: 1801.6959 - val_loss: 983.6461 - val_mse: 1313048.3750 - val_mae: 983.6461\n",
            "Epoch 83/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1786.5193 - mse: 6612403.5000 - mae: 1786.5192 - val_loss: 971.8611 - val_mse: 1297956.2500 - val_mae: 971.8611\n",
            "Epoch 84/500\n",
            "20/20 [==============================] - 0s 950us/step - loss: 1803.0897 - mse: 6542202.5000 - mae: 1803.0896 - val_loss: 971.2819 - val_mse: 1292073.6250 - val_mae: 971.2819\n",
            "Epoch 85/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1804.7177 - mse: 6505005.5000 - mae: 1804.7175 - val_loss: 962.4877 - val_mse: 1280360.6250 - val_mae: 962.4877\n",
            "Epoch 86/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1770.0742 - mse: 6512980.0000 - mae: 1770.0742 - val_loss: 963.8307 - val_mse: 1275867.1250 - val_mae: 963.8307\n",
            "Epoch 87/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1766.7145 - mse: 6391252.5000 - mae: 1766.7145 - val_loss: 963.6878 - val_mse: 1268608.2500 - val_mae: 963.6878\n",
            "Epoch 88/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1777.2131 - mse: 6471189.0000 - mae: 1777.2131 - val_loss: 952.7516 - val_mse: 1252614.1250 - val_mae: 952.7516\n",
            "Epoch 89/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1789.3246 - mse: 6393330.5000 - mae: 1789.3246 - val_loss: 946.7580 - val_mse: 1243067.3750 - val_mae: 946.7580\n",
            "Epoch 90/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1760.7895 - mse: 6327365.5000 - mae: 1760.7894 - val_loss: 948.6288 - val_mse: 1238537.0000 - val_mae: 948.6288\n",
            "Epoch 91/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1712.5244 - mse: 6155879.5000 - mae: 1712.5242 - val_loss: 952.4928 - val_mse: 1235321.8750 - val_mae: 952.4928\n",
            "Epoch 92/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1789.8616 - mse: 6440117.0000 - mae: 1789.8617 - val_loss: 940.8379 - val_mse: 1221118.6250 - val_mae: 940.8379\n",
            "Epoch 93/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1757.1395 - mse: 6270288.0000 - mae: 1757.1394 - val_loss: 929.4191 - val_mse: 1205533.6250 - val_mae: 929.4191\n",
            "Epoch 94/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1704.6476 - mse: 6082660.5000 - mae: 1704.6477 - val_loss: 923.3513 - val_mse: 1192753.1250 - val_mae: 923.3513\n",
            "Epoch 95/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1809.9196 - mse: 6485750.5000 - mae: 1809.9196 - val_loss: 924.4396 - val_mse: 1188385.1250 - val_mae: 924.4396\n",
            "Epoch 96/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1700.9691 - mse: 6144820.0000 - mae: 1700.9691 - val_loss: 919.4906 - val_mse: 1178037.7500 - val_mae: 919.4906\n",
            "Epoch 97/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1742.8405 - mse: 6142669.0000 - mae: 1742.8406 - val_loss: 918.7529 - val_mse: 1171196.3750 - val_mae: 918.7529\n",
            "Epoch 98/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1713.3499 - mse: 6003882.5000 - mae: 1713.3500 - val_loss: 903.5947 - val_mse: 1153712.1250 - val_mae: 903.5947\n",
            "Epoch 99/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1732.0311 - mse: 5978933.0000 - mae: 1732.0310 - val_loss: 912.3340 - val_mse: 1153443.2500 - val_mae: 912.3340\n",
            "Epoch 100/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1803.6100 - mse: 6377098.0000 - mae: 1803.6100 - val_loss: 907.7426 - val_mse: 1145931.2500 - val_mae: 907.7426\n",
            "Epoch 101/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1755.8851 - mse: 6288955.0000 - mae: 1755.8851 - val_loss: 899.6223 - val_mse: 1134837.2500 - val_mae: 899.6223\n",
            "Epoch 102/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1656.6277 - mse: 5568519.0000 - mae: 1656.6277 - val_loss: 891.3652 - val_mse: 1120449.2500 - val_mae: 891.3652\n",
            "Epoch 103/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1723.3309 - mse: 6121129.5000 - mae: 1723.3308 - val_loss: 897.6322 - val_mse: 1118624.2500 - val_mae: 897.6322\n",
            "Epoch 104/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1706.1177 - mse: 6023792.0000 - mae: 1706.1176 - val_loss: 886.9229 - val_mse: 1103461.2500 - val_mae: 886.9229\n",
            "Epoch 105/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1638.6771 - mse: 5647685.0000 - mae: 1638.6770 - val_loss: 878.4388 - val_mse: 1086835.7500 - val_mae: 878.4388\n",
            "Epoch 106/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1646.8652 - mse: 5635181.5000 - mae: 1646.8652 - val_loss: 872.9084 - val_mse: 1074534.7500 - val_mae: 872.9084\n",
            "Epoch 107/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1646.4312 - mse: 5607071.0000 - mae: 1646.4313 - val_loss: 866.1808 - val_mse: 1061404.7500 - val_mae: 866.1808\n",
            "Epoch 108/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1660.5874 - mse: 5702908.5000 - mae: 1660.5873 - val_loss: 864.6906 - val_mse: 1051719.3750 - val_mae: 864.6906\n",
            "Epoch 109/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1691.6133 - mse: 5804129.5000 - mae: 1691.6133 - val_loss: 855.3841 - val_mse: 1036280.3750 - val_mae: 855.3841\n",
            "Epoch 110/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1704.2966 - mse: 6044901.5000 - mae: 1704.2965 - val_loss: 851.5502 - val_mse: 1026452.8125 - val_mae: 851.5502\n",
            "Epoch 111/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1659.9188 - mse: 5660873.5000 - mae: 1659.9187 - val_loss: 841.3374 - val_mse: 1010309.6250 - val_mae: 841.3374\n",
            "Epoch 112/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1618.5214 - mse: 5406746.5000 - mae: 1618.5215 - val_loss: 836.0312 - val_mse: 996422.1875 - val_mae: 836.0312\n",
            "Epoch 113/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1612.4843 - mse: 5412529.0000 - mae: 1612.4843 - val_loss: 831.7842 - val_mse: 983085.3750 - val_mae: 831.7842\n",
            "Epoch 114/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1570.9437 - mse: 5221740.5000 - mae: 1570.9437 - val_loss: 811.8915 - val_mse: 959829.3750 - val_mae: 811.8915\n",
            "Epoch 115/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1583.0471 - mse: 5129142.0000 - mae: 1583.0471 - val_loss: 809.1705 - val_mse: 947418.6875 - val_mae: 809.1705\n",
            "Epoch 116/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1572.1677 - mse: 5257448.0000 - mae: 1572.1677 - val_loss: 804.5226 - val_mse: 934602.1875 - val_mae: 804.5226\n",
            "Epoch 117/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1510.2888 - mse: 4797065.0000 - mae: 1510.2888 - val_loss: 803.5104 - val_mse: 922185.3750 - val_mae: 803.5104\n",
            "Epoch 118/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1603.6336 - mse: 5337757.5000 - mae: 1603.6335 - val_loss: 809.6277 - val_mse: 917673.6875 - val_mae: 809.6277\n",
            "Epoch 119/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1605.9087 - mse: 5383968.5000 - mae: 1605.9088 - val_loss: 797.5164 - val_mse: 899490.0000 - val_mae: 797.5164\n",
            "Epoch 120/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1558.1031 - mse: 5152077.5000 - mae: 1558.1031 - val_loss: 781.9263 - val_mse: 877152.1250 - val_mae: 781.9263\n",
            "Epoch 121/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1529.3845 - mse: 4956992.0000 - mae: 1529.3845 - val_loss: 772.2015 - val_mse: 860935.3750 - val_mae: 772.2015\n",
            "Epoch 122/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1430.5115 - mse: 4440704.0000 - mae: 1430.5115 - val_loss: 764.6824 - val_mse: 842858.6250 - val_mae: 764.6824\n",
            "Epoch 123/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1448.5650 - mse: 4687104.0000 - mae: 1448.5651 - val_loss: 754.0109 - val_mse: 824191.5625 - val_mae: 754.0109\n",
            "Epoch 124/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1464.5826 - mse: 4747790.5000 - mae: 1464.5826 - val_loss: 742.5096 - val_mse: 803798.6250 - val_mae: 742.5096\n",
            "Epoch 125/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1421.7976 - mse: 4391708.0000 - mae: 1421.7976 - val_loss: 728.1463 - val_mse: 779816.6875 - val_mae: 728.1463\n",
            "Epoch 126/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1450.9684 - mse: 4869150.5000 - mae: 1450.9684 - val_loss: 727.9735 - val_mse: 769234.7500 - val_mae: 727.9735\n",
            "Epoch 127/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1455.3116 - mse: 4442781.0000 - mae: 1455.3116 - val_loss: 720.7462 - val_mse: 752656.0625 - val_mae: 720.7462\n",
            "Epoch 128/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1381.8454 - mse: 4198928.0000 - mae: 1381.8455 - val_loss: 710.9874 - val_mse: 733975.4375 - val_mae: 710.9874\n",
            "Epoch 129/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1445.8555 - mse: 4527019.0000 - mae: 1445.8555 - val_loss: 706.0813 - val_mse: 720228.3750 - val_mae: 706.0813\n",
            "Epoch 130/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1386.0568 - mse: 4289908.0000 - mae: 1386.0569 - val_loss: 693.8843 - val_mse: 699600.6875 - val_mae: 693.8843\n",
            "Epoch 131/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1377.4420 - mse: 4147607.5000 - mae: 1377.4420 - val_loss: 673.1852 - val_mse: 676563.3125 - val_mae: 673.1852\n",
            "Epoch 132/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1420.2645 - mse: 4480826.5000 - mae: 1420.2644 - val_loss: 654.1100 - val_mse: 654398.6250 - val_mae: 654.1100\n",
            "Epoch 133/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1459.2855 - mse: 4719221.0000 - mae: 1459.2855 - val_loss: 635.9676 - val_mse: 635038.0000 - val_mae: 635.9676\n",
            "Epoch 134/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1360.1480 - mse: 4037078.7500 - mae: 1360.1481 - val_loss: 650.8589 - val_mse: 627949.8125 - val_mae: 650.8589\n",
            "Epoch 135/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1352.7741 - mse: 4173223.2500 - mae: 1352.7742 - val_loss: 624.0709 - val_mse: 602271.3750 - val_mae: 624.0709\n",
            "Epoch 136/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1316.2013 - mse: 4014092.7500 - mae: 1316.2013 - val_loss: 597.2996 - val_mse: 575452.2500 - val_mae: 597.2996\n",
            "Epoch 137/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1354.4899 - mse: 4380026.5000 - mae: 1354.4900 - val_loss: 589.6742 - val_mse: 560627.1250 - val_mae: 589.6742\n",
            "Epoch 138/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1320.7085 - mse: 3866757.5000 - mae: 1320.7085 - val_loss: 598.3834 - val_mse: 549569.4375 - val_mae: 598.3834\n",
            "Epoch 139/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1222.1853 - mse: 3329858.5000 - mae: 1222.1853 - val_loss: 574.1876 - val_mse: 523741.1875 - val_mae: 574.1876\n",
            "Epoch 140/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1285.4298 - mse: 3747814.7500 - mae: 1285.4298 - val_loss: 570.9885 - val_mse: 510650.1875 - val_mae: 570.9885\n",
            "Epoch 141/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1257.2721 - mse: 3588629.5000 - mae: 1257.2721 - val_loss: 550.5490 - val_mse: 487173.5000 - val_mae: 550.5490\n",
            "Epoch 142/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1156.5042 - mse: 3191788.2500 - mae: 1156.5042 - val_loss: 538.1206 - val_mse: 467991.3125 - val_mae: 538.1206\n",
            "Epoch 143/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1209.0312 - mse: 3543246.0000 - mae: 1209.0312 - val_loss: 526.6696 - val_mse: 451315.0625 - val_mae: 526.6696\n",
            "Epoch 144/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1176.3713 - mse: 3624793.5000 - mae: 1176.3713 - val_loss: 498.1849 - val_mse: 426335.1875 - val_mae: 498.1849\n",
            "Epoch 145/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1145.7062 - mse: 3237810.0000 - mae: 1145.7063 - val_loss: 491.4185 - val_mse: 413804.8438 - val_mae: 491.4185\n",
            "Epoch 146/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1236.8373 - mse: 3387944.0000 - mae: 1236.8373 - val_loss: 477.5666 - val_mse: 396344.0625 - val_mae: 477.5666\n",
            "Epoch 147/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1230.8828 - mse: 3791838.5000 - mae: 1230.8828 - val_loss: 459.4476 - val_mse: 378299.0000 - val_mae: 459.4476\n",
            "Epoch 148/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1147.3467 - mse: 2970922.2500 - mae: 1147.3467 - val_loss: 443.9173 - val_mse: 360609.8438 - val_mae: 443.9173\n",
            "Epoch 149/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1050.5362 - mse: 2585265.5000 - mae: 1050.5361 - val_loss: 427.9927 - val_mse: 342116.6250 - val_mae: 427.9927\n",
            "Epoch 150/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1169.4661 - mse: 3101609.0000 - mae: 1169.4661 - val_loss: 416.7133 - val_mse: 326336.9688 - val_mae: 416.7133\n",
            "Epoch 151/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1101.1055 - mse: 2782737.2500 - mae: 1101.1055 - val_loss: 403.5887 - val_mse: 310899.3125 - val_mae: 403.5887\n",
            "Epoch 152/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1099.6332 - mse: 2917985.5000 - mae: 1099.6332 - val_loss: 410.9761 - val_mse: 301977.2500 - val_mae: 410.9761\n",
            "Epoch 153/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1073.9202 - mse: 2657495.5000 - mae: 1073.9202 - val_loss: 396.8173 - val_mse: 285385.0000 - val_mae: 396.8173\n",
            "Epoch 154/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 986.5545 - mse: 2316933.5000 - mae: 986.5545 - val_loss: 385.3720 - val_mse: 268514.9375 - val_mae: 385.3720\n",
            "Epoch 155/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1068.9279 - mse: 2479100.5000 - mae: 1068.9280 - val_loss: 375.4346 - val_mse: 255219.7969 - val_mae: 375.4346\n",
            "Epoch 156/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 981.4662 - mse: 2544749.5000 - mae: 981.4662 - val_loss: 365.4968 - val_mse: 246009.0781 - val_mae: 365.4968\n",
            "Epoch 157/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1040.9785 - mse: 2585004.5000 - mae: 1040.9785 - val_loss: 361.3218 - val_mse: 234466.4531 - val_mae: 361.3218\n",
            "Epoch 158/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 963.6789 - mse: 2448631.0000 - mae: 963.6788 - val_loss: 356.3783 - val_mse: 224729.1719 - val_mae: 356.3783\n",
            "Epoch 159/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 841.2951 - mse: 1883367.7500 - mae: 841.2951 - val_loss: 346.9877 - val_mse: 214947.6719 - val_mae: 346.9877\n",
            "Epoch 160/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1009.5040 - mse: 2710442.5000 - mae: 1009.5040 - val_loss: 345.9034 - val_mse: 205575.0938 - val_mae: 345.9034\n",
            "Epoch 161/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 987.5430 - mse: 2682861.5000 - mae: 987.5430 - val_loss: 324.3218 - val_mse: 192085.0938 - val_mae: 324.3218\n",
            "Epoch 162/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 743.4764 - mse: 1619504.2500 - mae: 743.4764 - val_loss: 313.9588 - val_mse: 179570.4219 - val_mae: 313.9588\n",
            "Epoch 163/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 987.5341 - mse: 2590931.5000 - mae: 987.5342 - val_loss: 306.9155 - val_mse: 169690.2031 - val_mae: 306.9155\n",
            "Epoch 164/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1020.2950 - mse: 2301250.0000 - mae: 1020.2950 - val_loss: 298.1353 - val_mse: 161138.0938 - val_mae: 298.1353\n",
            "Epoch 165/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 872.6970 - mse: 1983704.2500 - mae: 872.6969 - val_loss: 296.9496 - val_mse: 156795.2812 - val_mae: 296.9496\n",
            "Epoch 166/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 1034.8459 - mse: 2759470.0000 - mae: 1034.8459 - val_loss: 297.5179 - val_mse: 152461.8750 - val_mae: 297.5179\n",
            "Epoch 167/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 926.3755 - mse: 2429073.2500 - mae: 926.3755 - val_loss: 289.1359 - val_mse: 145253.5156 - val_mae: 289.1359\n",
            "Epoch 168/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 834.7395 - mse: 1855794.2500 - mae: 834.7394 - val_loss: 283.5998 - val_mse: 136952.4844 - val_mae: 283.5998\n",
            "Epoch 169/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 789.8589 - mse: 1773259.7500 - mae: 789.8589 - val_loss: 276.8792 - val_mse: 130485.8281 - val_mae: 276.8792\n",
            "Epoch 170/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 841.7013 - mse: 1777481.2500 - mae: 841.7013 - val_loss: 271.1950 - val_mse: 123505.0000 - val_mae: 271.1950\n",
            "Epoch 171/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 734.9501 - mse: 1444882.7500 - mae: 734.9501 - val_loss: 273.8706 - val_mse: 124306.9609 - val_mae: 273.8706\n",
            "Epoch 172/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 762.9066 - mse: 1887921.6250 - mae: 762.9066 - val_loss: 266.5145 - val_mse: 116890.1875 - val_mae: 266.5145\n",
            "Epoch 173/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 756.0723 - mse: 1977799.6250 - mae: 756.0723 - val_loss: 269.5939 - val_mse: 120686.8516 - val_mae: 269.5939\n",
            "Epoch 174/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 846.0384 - mse: 1717830.3750 - mae: 846.0384 - val_loss: 257.5458 - val_mse: 109878.8594 - val_mae: 257.5458\n",
            "Epoch 175/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 886.2752 - mse: 2064784.7500 - mae: 886.2752 - val_loss: 241.8292 - val_mse: 99028.6797 - val_mae: 241.8292\n",
            "Epoch 176/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 834.2820 - mse: 1759442.2500 - mae: 834.2820 - val_loss: 244.9776 - val_mse: 100209.1719 - val_mae: 244.9776\n",
            "Epoch 177/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 849.7587 - mse: 2230775.2500 - mae: 849.7587 - val_loss: 242.0130 - val_mse: 98558.3984 - val_mae: 242.0130\n",
            "Epoch 178/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 704.8997 - mse: 1322215.3750 - mae: 704.8997 - val_loss: 237.9858 - val_mse: 95992.8281 - val_mae: 237.9858\n",
            "Epoch 179/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 815.1356 - mse: 1618309.0000 - mae: 815.1356 - val_loss: 235.4180 - val_mse: 96707.4688 - val_mae: 235.4180\n",
            "Epoch 180/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 791.7977 - mse: 1561746.7500 - mae: 791.7977 - val_loss: 239.4358 - val_mse: 100592.4219 - val_mae: 239.4358\n",
            "Epoch 181/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 607.1843 - mse: 1161300.3750 - mae: 607.1843 - val_loss: 226.1414 - val_mse: 84336.4922 - val_mae: 226.1414\n",
            "Epoch 182/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 787.3282 - mse: 1323286.2500 - mae: 787.3282 - val_loss: 234.5271 - val_mse: 90289.2344 - val_mae: 234.5271\n",
            "Epoch 183/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 835.0899 - mse: 1876763.7500 - mae: 835.0898 - val_loss: 227.3849 - val_mse: 81115.8125 - val_mae: 227.3849\n",
            "Epoch 184/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 785.1770 - mse: 1500237.6250 - mae: 785.1771 - val_loss: 231.7676 - val_mse: 81885.2969 - val_mae: 231.7676\n",
            "Epoch 185/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 708.5660 - mse: 1235011.1250 - mae: 708.5660 - val_loss: 235.1280 - val_mse: 81882.7734 - val_mae: 235.1280\n",
            "Epoch 186/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 778.7216 - mse: 1385209.7500 - mae: 778.7216 - val_loss: 236.8412 - val_mse: 82004.6719 - val_mae: 236.8412\n",
            "Epoch 187/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 544.1711 - mse: 772266.1250 - mae: 544.1711 - val_loss: 237.7116 - val_mse: 82448.6719 - val_mae: 237.7116\n",
            "Epoch 188/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 650.5155 - mse: 1343904.2500 - mae: 650.5155 - val_loss: 230.6251 - val_mse: 76818.2109 - val_mae: 230.6251\n",
            "Epoch 189/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 579.3592 - mse: 1045994.8750 - mae: 579.3593 - val_loss: 235.3368 - val_mse: 80008.0938 - val_mae: 235.3368\n",
            "Epoch 190/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 730.1006 - mse: 1354694.2500 - mae: 730.1006 - val_loss: 233.6183 - val_mse: 78122.2656 - val_mae: 233.6183\n",
            "Epoch 191/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 632.8069 - mse: 1228204.7500 - mae: 632.8069 - val_loss: 241.4417 - val_mse: 84875.9375 - val_mae: 241.4417\n",
            "Epoch 192/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 635.0246 - mse: 1041585.8125 - mae: 635.0246 - val_loss: 235.8251 - val_mse: 77861.6719 - val_mae: 235.8251\n",
            "Epoch 193/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 710.5088 - mse: 1130370.3750 - mae: 710.5088 - val_loss: 238.8896 - val_mse: 80144.2031 - val_mae: 238.8896\n",
            "Epoch 194/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 522.4889 - mse: 869282.6250 - mae: 522.4889 - val_loss: 232.0909 - val_mse: 73993.0938 - val_mae: 232.0909\n",
            "Epoch 195/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 583.1897 - mse: 1032956.0000 - mae: 583.1898 - val_loss: 234.3887 - val_mse: 75928.0000 - val_mae: 234.3887\n",
            "Epoch 196/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 681.1620 - mse: 1228445.3750 - mae: 681.1620 - val_loss: 236.3817 - val_mse: 78347.4219 - val_mae: 236.3817\n",
            "Epoch 197/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 869.1589 - mse: 2056296.2500 - mae: 869.1590 - val_loss: 234.5644 - val_mse: 76675.9844 - val_mae: 234.5644\n",
            "Epoch 198/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 640.8050 - mse: 1137209.3750 - mae: 640.8050 - val_loss: 245.4009 - val_mse: 85970.1484 - val_mae: 245.4009\n",
            "Epoch 199/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 763.7619 - mse: 1526871.7500 - mae: 763.7619 - val_loss: 239.9962 - val_mse: 80299.0938 - val_mae: 239.9962\n",
            "Epoch 200/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 805.6181 - mse: 1595455.3750 - mae: 805.6180 - val_loss: 234.9091 - val_mse: 75855.9609 - val_mae: 234.9091\n",
            "Epoch 201/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 562.8760 - mse: 810177.0000 - mae: 562.8760 - val_loss: 236.2028 - val_mse: 77383.0703 - val_mae: 236.2028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 30 samples, validate on 30 samples\n",
            "Epoch 1/500\n",
            "30/30 [==============================] - 3s 90ms/step - loss: 4854.1213 - mse: 41046792.0000 - mae: 4854.1216 - val_loss: 5867.4036 - val_mse: 47030696.0000 - val_mae: 5867.4038\n",
            "Epoch 2/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4854.0915 - mse: 41046512.0000 - mae: 4854.0918 - val_loss: 5867.3759 - val_mse: 47030380.0000 - val_mae: 5867.3760\n",
            "Epoch 3/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4854.0647 - mse: 41046228.0000 - mae: 4854.0645 - val_loss: 5867.3360 - val_mse: 47029948.0000 - val_mae: 5867.3359\n",
            "Epoch 4/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4854.0176 - mse: 41045936.0000 - mae: 4854.0176 - val_loss: 5867.2072 - val_mse: 47028820.0000 - val_mae: 5867.2075\n",
            "Epoch 5/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4853.8451 - mse: 41045168.0000 - mae: 4853.8452 - val_loss: 5866.6168 - val_mse: 47024024.0000 - val_mae: 5866.6167\n",
            "Epoch 6/500\n",
            "30/30 [==============================] - 0s 1000us/step - loss: 4853.0814 - mse: 41041164.0000 - mae: 4853.0811 - val_loss: 5864.7734 - val_mse: 47009280.0000 - val_mae: 5864.7734\n",
            "Epoch 7/500\n",
            "30/30 [==============================] - 0s 1000us/step - loss: 4851.2861 - mse: 41034236.0000 - mae: 4851.2861 - val_loss: 5861.7500 - val_mse: 46985104.0000 - val_mae: 5861.7500\n",
            "Epoch 8/500\n",
            "30/30 [==============================] - 0s 938us/step - loss: 4847.4654 - mse: 41013480.0000 - mae: 4847.4658 - val_loss: 5857.4533 - val_mse: 46950768.0000 - val_mae: 5857.4531\n",
            "Epoch 9/500\n",
            "30/30 [==============================] - 0s 928us/step - loss: 4841.5999 - mse: 40989760.0000 - mae: 4841.6001 - val_loss: 5851.2931 - val_mse: 46901560.0000 - val_mae: 5851.2925\n",
            "Epoch 10/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4835.3861 - mse: 40954376.0000 - mae: 4835.3862 - val_loss: 5845.6219 - val_mse: 46856080.0000 - val_mae: 5845.6221\n",
            "Epoch 11/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4832.1182 - mse: 40940672.0000 - mae: 4832.1177 - val_loss: 5840.6623 - val_mse: 46816152.0000 - val_mae: 5840.6626\n",
            "Epoch 12/500\n",
            "30/30 [==============================] - 0s 994us/step - loss: 4826.6494 - mse: 40908700.0000 - mae: 4826.6494 - val_loss: 5835.5556 - val_mse: 46774672.0000 - val_mae: 5835.5557\n",
            "Epoch 13/500\n",
            "30/30 [==============================] - 0s 987us/step - loss: 4821.2321 - mse: 40869076.0000 - mae: 4821.2324 - val_loss: 5830.2218 - val_mse: 46730904.0000 - val_mae: 5830.2217\n",
            "Epoch 14/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4813.2106 - mse: 40817520.0000 - mae: 4813.2104 - val_loss: 5824.4326 - val_mse: 46683024.0000 - val_mae: 5824.4321\n",
            "Epoch 15/500\n",
            "30/30 [==============================] - 0s 923us/step - loss: 4807.0042 - mse: 40819088.0000 - mae: 4807.0044 - val_loss: 5818.4195 - val_mse: 46633020.0000 - val_mae: 5818.4194\n",
            "Epoch 16/500\n",
            "30/30 [==============================] - 0s 952us/step - loss: 4803.7619 - mse: 40787664.0000 - mae: 4803.7612 - val_loss: 5812.5382 - val_mse: 46583896.0000 - val_mae: 5812.5381\n",
            "Epoch 17/500\n",
            "30/30 [==============================] - 0s 936us/step - loss: 4791.1951 - mse: 40720528.0000 - mae: 4791.1948 - val_loss: 5805.7545 - val_mse: 46526388.0000 - val_mae: 5805.7544\n",
            "Epoch 18/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4789.4893 - mse: 40699484.0000 - mae: 4789.4897 - val_loss: 5799.2585 - val_mse: 46471032.0000 - val_mae: 5799.2583\n",
            "Epoch 19/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4786.6232 - mse: 40637176.0000 - mae: 4786.6230 - val_loss: 5792.3423 - val_mse: 46411056.0000 - val_mae: 5792.3423\n",
            "Epoch 20/500\n",
            "30/30 [==============================] - 0s 895us/step - loss: 4768.4049 - mse: 40548208.0000 - mae: 4768.4053 - val_loss: 5784.2437 - val_mse: 46340928.0000 - val_mae: 5784.2437\n",
            "Epoch 21/500\n",
            "30/30 [==============================] - 0s 913us/step - loss: 4771.6420 - mse: 40559192.0000 - mae: 4771.6416 - val_loss: 5775.9249 - val_mse: 46268520.0000 - val_mae: 5775.9248\n",
            "Epoch 22/500\n",
            "30/30 [==============================] - 0s 940us/step - loss: 4755.4907 - mse: 40507268.0000 - mae: 4755.4907 - val_loss: 5767.5974 - val_mse: 46195124.0000 - val_mae: 5767.5972\n",
            "Epoch 23/500\n",
            "30/30 [==============================] - 0s 951us/step - loss: 4748.3298 - mse: 40400980.0000 - mae: 4748.3296 - val_loss: 5758.1086 - val_mse: 46111320.0000 - val_mae: 5758.1084\n",
            "Epoch 24/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4735.4834 - mse: 40352152.0000 - mae: 4735.4834 - val_loss: 5748.1685 - val_mse: 46023000.0000 - val_mae: 5748.1689\n",
            "Epoch 25/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4733.7638 - mse: 40282956.0000 - mae: 4733.7637 - val_loss: 5736.9584 - val_mse: 45921408.0000 - val_mae: 5736.9585\n",
            "Epoch 26/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4734.4023 - mse: 40271912.0000 - mae: 4734.4019 - val_loss: 5728.7818 - val_mse: 45843372.0000 - val_mae: 5728.7817\n",
            "Epoch 27/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 4716.8454 - mse: 40133424.0000 - mae: 4716.8452 - val_loss: 5719.3733 - val_mse: 45755068.0000 - val_mae: 5719.3730\n",
            "Epoch 28/500\n",
            "30/30 [==============================] - 0s 945us/step - loss: 4714.6844 - mse: 40040832.0000 - mae: 4714.6846 - val_loss: 5709.5115 - val_mse: 45660760.0000 - val_mae: 5709.5112\n",
            "Epoch 29/500\n",
            "30/30 [==============================] - 0s 940us/step - loss: 4698.3407 - mse: 39987960.0000 - mae: 4698.3408 - val_loss: 5697.7270 - val_mse: 45550084.0000 - val_mae: 5697.7271\n",
            "Epoch 30/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4704.5451 - mse: 40025992.0000 - mae: 4704.5449 - val_loss: 5688.6624 - val_mse: 45463272.0000 - val_mae: 5688.6626\n",
            "Epoch 31/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4702.6660 - mse: 39888168.0000 - mae: 4702.6660 - val_loss: 5678.5021 - val_mse: 45366108.0000 - val_mae: 5678.5020\n",
            "Epoch 32/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4689.1170 - mse: 39765556.0000 - mae: 4689.1172 - val_loss: 5666.8184 - val_mse: 45255972.0000 - val_mae: 5666.8184\n",
            "Epoch 33/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4681.0407 - mse: 39590128.0000 - mae: 4681.0405 - val_loss: 5655.0430 - val_mse: 45144016.0000 - val_mae: 5655.0425\n",
            "Epoch 34/500\n",
            "30/30 [==============================] - 0s 958us/step - loss: 4663.1519 - mse: 39516996.0000 - mae: 4663.1519 - val_loss: 5641.7049 - val_mse: 45017940.0000 - val_mae: 5641.7051\n",
            "Epoch 35/500\n",
            "30/30 [==============================] - 0s 940us/step - loss: 4660.2790 - mse: 39524956.0000 - mae: 4660.2793 - val_loss: 5631.3245 - val_mse: 44918264.0000 - val_mae: 5631.3247\n",
            "Epoch 36/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4649.9499 - mse: 39502916.0000 - mae: 4649.9502 - val_loss: 5619.9850 - val_mse: 44808952.0000 - val_mae: 5619.9849\n",
            "Epoch 37/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4676.7204 - mse: 39542784.0000 - mae: 4676.7202 - val_loss: 5609.2260 - val_mse: 44704356.0000 - val_mae: 5609.2261\n",
            "Epoch 38/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4649.1266 - mse: 39262192.0000 - mae: 4649.1265 - val_loss: 5599.9880 - val_mse: 44612028.0000 - val_mae: 5599.9878\n",
            "Epoch 39/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4656.7301 - mse: 39284884.0000 - mae: 4656.7300 - val_loss: 5589.6662 - val_mse: 44512148.0000 - val_mae: 5589.6665\n",
            "Epoch 40/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4655.4024 - mse: 39296812.0000 - mae: 4655.4028 - val_loss: 5581.0528 - val_mse: 44426512.0000 - val_mae: 5581.0527\n",
            "Epoch 41/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4646.2192 - mse: 39038476.0000 - mae: 4646.2192 - val_loss: 5569.7025 - val_mse: 44311480.0000 - val_mae: 5569.7026\n",
            "Epoch 42/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4616.9652 - mse: 38768072.0000 - mae: 4616.9658 - val_loss: 5558.5480 - val_mse: 44200368.0000 - val_mae: 5558.5479\n",
            "Epoch 43/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4623.0133 - mse: 38817768.0000 - mae: 4623.0132 - val_loss: 5547.3763 - val_mse: 44089036.0000 - val_mae: 5547.3760\n",
            "Epoch 44/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4640.0424 - mse: 38760512.0000 - mae: 4640.0425 - val_loss: 5536.7935 - val_mse: 43979888.0000 - val_mae: 5536.7939\n",
            "Epoch 45/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4589.8934 - mse: 38653704.0000 - mae: 4589.8936 - val_loss: 5525.8115 - val_mse: 43867940.0000 - val_mae: 5525.8115\n",
            "Epoch 46/500\n",
            "30/30 [==============================] - 0s 964us/step - loss: 4618.9162 - mse: 38614676.0000 - mae: 4618.9160 - val_loss: 5519.0691 - val_mse: 43795404.0000 - val_mae: 5519.0688\n",
            "Epoch 47/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4595.3826 - mse: 38637996.0000 - mae: 4595.3823 - val_loss: 5508.3366 - val_mse: 43687300.0000 - val_mae: 5508.3364\n",
            "Epoch 48/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4595.3154 - mse: 38237488.0000 - mae: 4595.3154 - val_loss: 5499.4920 - val_mse: 43592692.0000 - val_mae: 5499.4922\n",
            "Epoch 49/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4603.9159 - mse: 38268068.0000 - mae: 4603.9155 - val_loss: 5486.8795 - val_mse: 43464364.0000 - val_mae: 5486.8799\n",
            "Epoch 50/500\n",
            "30/30 [==============================] - 0s 980us/step - loss: 4580.6901 - mse: 38234532.0000 - mae: 4580.6904 - val_loss: 5474.7048 - val_mse: 43340848.0000 - val_mae: 5474.7051\n",
            "Epoch 51/500\n",
            "30/30 [==============================] - 0s 933us/step - loss: 4601.0373 - mse: 38265020.0000 - mae: 4601.0376 - val_loss: 5463.7941 - val_mse: 43224700.0000 - val_mae: 5463.7944\n",
            "Epoch 52/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4576.4759 - mse: 38233900.0000 - mae: 4576.4761 - val_loss: 5453.9450 - val_mse: 43119540.0000 - val_mae: 5453.9448\n",
            "Epoch 53/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4624.3729 - mse: 38324248.0000 - mae: 4624.3730 - val_loss: 5449.8160 - val_mse: 43063792.0000 - val_mae: 5449.8154\n",
            "Epoch 54/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4578.6912 - mse: 37821012.0000 - mae: 4578.6909 - val_loss: 5439.4623 - val_mse: 42950400.0000 - val_mae: 5439.4624\n",
            "Epoch 55/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4550.6621 - mse: 37612032.0000 - mae: 4550.6621 - val_loss: 5423.3112 - val_mse: 42781092.0000 - val_mae: 5423.3115\n",
            "Epoch 56/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4523.7913 - mse: 37235420.0000 - mae: 4523.7915 - val_loss: 5409.5889 - val_mse: 42637832.0000 - val_mae: 5409.5889\n",
            "Epoch 57/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4557.1898 - mse: 37094540.0000 - mae: 4557.1895 - val_loss: 5397.4565 - val_mse: 42505064.0000 - val_mae: 5397.4561\n",
            "Epoch 58/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4518.3491 - mse: 37009188.0000 - mae: 4518.3491 - val_loss: 5385.5717 - val_mse: 42379112.0000 - val_mae: 5385.5718\n",
            "Epoch 59/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4514.6398 - mse: 37157576.0000 - mae: 4514.6396 - val_loss: 5371.6533 - val_mse: 42239556.0000 - val_mae: 5371.6533\n",
            "Epoch 60/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4518.3071 - mse: 36798532.0000 - mae: 4518.3071 - val_loss: 5358.2090 - val_mse: 42100804.0000 - val_mae: 5358.2090\n",
            "Epoch 61/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4521.6805 - mse: 36973096.0000 - mae: 4521.6802 - val_loss: 5347.1579 - val_mse: 41982444.0000 - val_mae: 5347.1582\n",
            "Epoch 62/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4533.6771 - mse: 36569436.0000 - mae: 4533.6772 - val_loss: 5337.4594 - val_mse: 41871060.0000 - val_mae: 5337.4595\n",
            "Epoch 63/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4525.4698 - mse: 36524780.0000 - mae: 4525.4697 - val_loss: 5327.3835 - val_mse: 41757756.0000 - val_mae: 5327.3833\n",
            "Epoch 64/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4487.7732 - mse: 36181128.0000 - mae: 4487.7729 - val_loss: 5315.0607 - val_mse: 41622188.0000 - val_mae: 5315.0605\n",
            "Epoch 65/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4512.9129 - mse: 36389592.0000 - mae: 4512.9126 - val_loss: 5301.7501 - val_mse: 41474984.0000 - val_mae: 5301.7500\n",
            "Epoch 66/500\n",
            "30/30 [==============================] - 0s 993us/step - loss: 4505.9495 - mse: 36298520.0000 - mae: 4505.9497 - val_loss: 5290.4596 - val_mse: 41347108.0000 - val_mae: 5290.4595\n",
            "Epoch 67/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4480.7754 - mse: 36328384.0000 - mae: 4480.7749 - val_loss: 5278.2528 - val_mse: 41210096.0000 - val_mae: 5278.2529\n",
            "Epoch 68/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4500.4412 - mse: 36296980.0000 - mae: 4500.4419 - val_loss: 5266.7390 - val_mse: 41080180.0000 - val_mae: 5266.7388\n",
            "Epoch 69/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4471.1064 - mse: 35628532.0000 - mae: 4471.1064 - val_loss: 5253.0110 - val_mse: 40916068.0000 - val_mae: 5253.0107\n",
            "Epoch 70/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4466.9667 - mse: 35711540.0000 - mae: 4466.9668 - val_loss: 5240.3460 - val_mse: 40762632.0000 - val_mae: 5240.3457\n",
            "Epoch 71/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4489.6594 - mse: 35746340.0000 - mae: 4489.6592 - val_loss: 5229.1018 - val_mse: 40625896.0000 - val_mae: 5229.1021\n",
            "Epoch 72/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4492.6787 - mse: 35504764.0000 - mae: 4492.6787 - val_loss: 5218.2542 - val_mse: 40493692.0000 - val_mae: 5218.2544\n",
            "Epoch 73/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4479.9390 - mse: 35350396.0000 - mae: 4479.9390 - val_loss: 5206.2177 - val_mse: 40346472.0000 - val_mae: 5206.2178\n",
            "Epoch 74/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4528.2925 - mse: 35136936.0000 - mae: 4528.2925 - val_loss: 5197.2773 - val_mse: 40235644.0000 - val_mae: 5197.2769\n",
            "Epoch 75/500\n",
            "30/30 [==============================] - 0s 867us/step - loss: 4474.5430 - mse: 35154500.0000 - mae: 4474.5425 - val_loss: 5185.8681 - val_mse: 40100992.0000 - val_mae: 5185.8677\n",
            "Epoch 76/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4382.9539 - mse: 34466776.0000 - mae: 4382.9541 - val_loss: 5169.8726 - val_mse: 39915928.0000 - val_mae: 5169.8726\n",
            "Epoch 77/500\n",
            "30/30 [==============================] - 0s 954us/step - loss: 4420.2801 - mse: 34388244.0000 - mae: 4420.2803 - val_loss: 5156.8726 - val_mse: 39752192.0000 - val_mae: 5156.8726\n",
            "Epoch 78/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4376.8910 - mse: 33877884.0000 - mae: 4376.8911 - val_loss: 5142.0425 - val_mse: 39568244.0000 - val_mae: 5142.0425\n",
            "Epoch 79/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4355.7601 - mse: 33399540.0000 - mae: 4355.7603 - val_loss: 5126.2894 - val_mse: 39372764.0000 - val_mae: 5126.2896\n",
            "Epoch 80/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4378.8568 - mse: 33678540.0000 - mae: 4378.8569 - val_loss: 5112.0072 - val_mse: 39197168.0000 - val_mae: 5112.0073\n",
            "Epoch 81/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4354.4938 - mse: 33197970.0000 - mae: 4354.4941 - val_loss: 5096.6162 - val_mse: 39008172.0000 - val_mae: 5096.6162\n",
            "Epoch 82/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 4309.2334 - mse: 33452774.0000 - mae: 4309.2334 - val_loss: 5079.8344 - val_mse: 38805044.0000 - val_mae: 5079.8345\n",
            "Epoch 83/500\n",
            "30/30 [==============================] - 0s 980us/step - loss: 4421.3777 - mse: 33659312.0000 - mae: 4421.3774 - val_loss: 5068.5106 - val_mse: 38660284.0000 - val_mae: 5068.5103\n",
            "Epoch 84/500\n",
            "30/30 [==============================] - 0s 926us/step - loss: 4417.7203 - mse: 33895408.0000 - mae: 4417.7197 - val_loss: 5058.0083 - val_mse: 38520080.0000 - val_mae: 5058.0083\n",
            "Epoch 85/500\n",
            "30/30 [==============================] - 0s 970us/step - loss: 4374.0986 - mse: 33204348.0000 - mae: 4374.0986 - val_loss: 5045.6225 - val_mse: 38355932.0000 - val_mae: 5045.6230\n",
            "Epoch 86/500\n",
            "30/30 [==============================] - 0s 918us/step - loss: 4376.5972 - mse: 33233818.0000 - mae: 4376.5967 - val_loss: 5033.0773 - val_mse: 38190064.0000 - val_mae: 5033.0771\n",
            "Epoch 87/500\n",
            "30/30 [==============================] - 0s 897us/step - loss: 4286.1691 - mse: 31592576.0000 - mae: 4286.1694 - val_loss: 5017.1685 - val_mse: 37980748.0000 - val_mae: 5017.1689\n",
            "Epoch 88/500\n",
            "30/30 [==============================] - 0s 964us/step - loss: 4348.6164 - mse: 32951740.0000 - mae: 4348.6167 - val_loss: 5003.9949 - val_mse: 37808240.0000 - val_mae: 5003.9946\n",
            "Epoch 89/500\n",
            "30/30 [==============================] - 0s 992us/step - loss: 4379.8241 - mse: 32696786.0000 - mae: 4379.8237 - val_loss: 4992.2119 - val_mse: 37653880.0000 - val_mae: 4992.2119\n",
            "Epoch 90/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4384.6589 - mse: 32976972.0000 - mae: 4384.6587 - val_loss: 4981.5534 - val_mse: 37500772.0000 - val_mae: 4981.5532\n",
            "Epoch 91/500\n",
            "30/30 [==============================] - 0s 982us/step - loss: 4306.9653 - mse: 31909048.0000 - mae: 4306.9653 - val_loss: 4968.3013 - val_mse: 37310260.0000 - val_mae: 4968.3013\n",
            "Epoch 92/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4318.4255 - mse: 31853920.0000 - mae: 4318.4253 - val_loss: 4955.7578 - val_mse: 37130308.0000 - val_mae: 4955.7578\n",
            "Epoch 93/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4338.1891 - mse: 31612514.0000 - mae: 4338.1890 - val_loss: 4944.0560 - val_mse: 36962868.0000 - val_mae: 4944.0557\n",
            "Epoch 94/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4340.5488 - mse: 32436034.0000 - mae: 4340.5488 - val_loss: 4932.3750 - val_mse: 36797144.0000 - val_mae: 4932.3750\n",
            "Epoch 95/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4258.1719 - mse: 31563178.0000 - mae: 4258.1719 - val_loss: 4918.0449 - val_mse: 36595472.0000 - val_mae: 4918.0449\n",
            "Epoch 96/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4280.3426 - mse: 31169288.0000 - mae: 4280.3423 - val_loss: 4903.4581 - val_mse: 36386904.0000 - val_mae: 4903.4585\n",
            "Epoch 97/500\n",
            "30/30 [==============================] - 0s 953us/step - loss: 4207.8379 - mse: 30540860.0000 - mae: 4207.8379 - val_loss: 4889.2478 - val_mse: 36163916.0000 - val_mae: 4889.2480\n",
            "Epoch 98/500\n",
            "30/30 [==============================] - 0s 976us/step - loss: 4272.5443 - mse: 30989178.0000 - mae: 4272.5444 - val_loss: 4877.2564 - val_mse: 35976380.0000 - val_mae: 4877.2563\n",
            "Epoch 99/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4260.9881 - mse: 31123542.0000 - mae: 4260.9878 - val_loss: 4865.0724 - val_mse: 35787272.0000 - val_mae: 4865.0723\n",
            "Epoch 100/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4262.1018 - mse: 30117366.0000 - mae: 4262.1021 - val_loss: 4852.7610 - val_mse: 35596636.0000 - val_mae: 4852.7607\n",
            "Epoch 101/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4279.1210 - mse: 30552282.0000 - mae: 4279.1211 - val_loss: 4841.1235 - val_mse: 35417516.0000 - val_mae: 4841.1235\n",
            "Epoch 102/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4313.3135 - mse: 31263624.0000 - mae: 4313.3135 - val_loss: 4830.9096 - val_mse: 35260904.0000 - val_mae: 4830.9092\n",
            "Epoch 103/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4141.1318 - mse: 28847748.0000 - mae: 4141.1318 - val_loss: 4815.4572 - val_mse: 35025136.0000 - val_mae: 4815.4575\n",
            "Epoch 104/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4254.4972 - mse: 30910634.0000 - mae: 4254.4971 - val_loss: 4803.3444 - val_mse: 34841600.0000 - val_mae: 4803.3442\n",
            "Epoch 105/500\n",
            "30/30 [==============================] - 0s 979us/step - loss: 4218.6458 - mse: 30617470.0000 - mae: 4218.6460 - val_loss: 4791.5725 - val_mse: 34646156.0000 - val_mae: 4791.5723\n",
            "Epoch 106/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 4207.2189 - mse: 30098906.0000 - mae: 4207.2188 - val_loss: 4780.1931 - val_mse: 34451496.0000 - val_mae: 4780.1929\n",
            "Epoch 107/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4242.2037 - mse: 30341940.0000 - mae: 4242.2036 - val_loss: 4769.6330 - val_mse: 34271808.0000 - val_mae: 4769.6333\n",
            "Epoch 108/500\n",
            "30/30 [==============================] - 0s 965us/step - loss: 4228.1870 - mse: 29966818.0000 - mae: 4228.1870 - val_loss: 4758.4334 - val_mse: 34082220.0000 - val_mae: 4758.4331\n",
            "Epoch 109/500\n",
            "30/30 [==============================] - 0s 995us/step - loss: 4224.9552 - mse: 29752734.0000 - mae: 4224.9551 - val_loss: 4747.2763 - val_mse: 33894328.0000 - val_mae: 4747.2764\n",
            "Epoch 110/500\n",
            "30/30 [==============================] - 0s 923us/step - loss: 4280.1428 - mse: 29295044.0000 - mae: 4280.1426 - val_loss: 4737.3362 - val_mse: 33727732.0000 - val_mae: 4737.3364\n",
            "Epoch 111/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4101.5311 - mse: 28888690.0000 - mae: 4101.5308 - val_loss: 4723.8268 - val_mse: 33502702.0000 - val_mae: 4723.8271\n",
            "Epoch 112/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4219.5028 - mse: 30064468.0000 - mae: 4219.5024 - val_loss: 4712.8615 - val_mse: 33321074.0000 - val_mae: 4712.8613\n",
            "Epoch 113/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4203.8422 - mse: 28648916.0000 - mae: 4203.8423 - val_loss: 4701.4645 - val_mse: 33133270.0000 - val_mae: 4701.4644\n",
            "Epoch 114/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4167.0580 - mse: 28863924.0000 - mae: 4167.0581 - val_loss: 4690.2054 - val_mse: 32938504.0000 - val_mae: 4690.2051\n",
            "Epoch 115/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4077.8215 - mse: 28227634.0000 - mae: 4077.8213 - val_loss: 4678.3015 - val_mse: 32713510.0000 - val_mae: 4678.3018\n",
            "Epoch 116/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3936.7731 - mse: 26634680.0000 - mae: 3936.7732 - val_loss: 4663.9137 - val_mse: 32443186.0000 - val_mae: 4663.9136\n",
            "Epoch 117/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4083.8481 - mse: 27647608.0000 - mae: 4083.8481 - val_loss: 4652.5005 - val_mse: 32230874.0000 - val_mae: 4652.5005\n",
            "Epoch 118/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4200.9123 - mse: 28221942.0000 - mae: 4200.9121 - val_loss: 4643.1098 - val_mse: 32057568.0000 - val_mae: 4643.1099\n",
            "Epoch 119/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4050.8811 - mse: 27731520.0000 - mae: 4050.8811 - val_loss: 4631.0798 - val_mse: 31836184.0000 - val_mae: 4631.0796\n",
            "Epoch 120/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3962.0431 - mse: 25670238.0000 - mae: 3962.0432 - val_loss: 4617.4109 - val_mse: 31586770.0000 - val_mae: 4617.4111\n",
            "Epoch 121/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3979.5601 - mse: 26334598.0000 - mae: 3979.5598 - val_loss: 4604.3946 - val_mse: 31350794.0000 - val_mae: 4604.3950\n",
            "Epoch 122/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3998.1363 - mse: 26210428.0000 - mae: 3998.1365 - val_loss: 4594.3719 - val_mse: 31171122.0000 - val_mae: 4594.3721\n",
            "Epoch 123/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4002.7132 - mse: 26499694.0000 - mae: 4002.7131 - val_loss: 4581.8926 - val_mse: 30947948.0000 - val_mae: 4581.8926\n",
            "Epoch 124/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4086.7508 - mse: 26643938.0000 - mae: 4086.7510 - val_loss: 4572.4339 - val_mse: 30752536.0000 - val_mae: 4572.4341\n",
            "Epoch 125/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4095.5778 - mse: 27068906.0000 - mae: 4095.5779 - val_loss: 4565.7487 - val_mse: 30613410.0000 - val_mae: 4565.7490\n",
            "Epoch 126/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 3936.7360 - mse: 24822076.0000 - mae: 3936.7358 - val_loss: 4554.1613 - val_mse: 30371822.0000 - val_mae: 4554.1616\n",
            "Epoch 127/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 4042.1305 - mse: 25925660.0000 - mae: 4042.1301 - val_loss: 4546.4963 - val_mse: 30215028.0000 - val_mae: 4546.4966\n",
            "Epoch 128/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 4073.3871 - mse: 26821400.0000 - mae: 4073.3872 - val_loss: 4537.2918 - val_mse: 30026240.0000 - val_mae: 4537.2915\n",
            "Epoch 129/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4040.8512 - mse: 26277564.0000 - mae: 4040.8511 - val_loss: 4527.6181 - val_mse: 29829612.0000 - val_mae: 4527.6177\n",
            "Epoch 130/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3921.8179 - mse: 25642048.0000 - mae: 3921.8176 - val_loss: 4516.4862 - val_mse: 29603836.0000 - val_mae: 4516.4863\n",
            "Epoch 131/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4043.7694 - mse: 25771434.0000 - mae: 4043.7693 - val_loss: 4506.9388 - val_mse: 29412874.0000 - val_mae: 4506.9385\n",
            "Epoch 132/500\n",
            "30/30 [==============================] - 0s 982us/step - loss: 4035.5676 - mse: 26662238.0000 - mae: 4035.5676 - val_loss: 4499.6986 - val_mse: 29269042.0000 - val_mae: 4499.6982\n",
            "Epoch 133/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4122.4736 - mse: 26658194.0000 - mae: 4122.4736 - val_loss: 4491.4533 - val_mse: 29105284.0000 - val_mae: 4491.4531\n",
            "Epoch 134/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3914.2785 - mse: 24669450.0000 - mae: 3914.2783 - val_loss: 4482.5844 - val_mse: 28888150.0000 - val_mae: 4482.5845\n",
            "Epoch 135/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3911.7727 - mse: 24413018.0000 - mae: 3911.7729 - val_loss: 4473.7959 - val_mse: 28674520.0000 - val_mae: 4473.7959\n",
            "Epoch 136/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4015.1512 - mse: 26559782.0000 - mae: 4015.1511 - val_loss: 4466.2757 - val_mse: 28492776.0000 - val_mae: 4466.2754\n",
            "Epoch 137/500\n",
            "30/30 [==============================] - 0s 985us/step - loss: 4004.1312 - mse: 25528994.0000 - mae: 4004.1313 - val_loss: 4458.5423 - val_mse: 28307680.0000 - val_mae: 4458.5420\n",
            "Epoch 138/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4201.8854 - mse: 26856232.0000 - mae: 4201.8853 - val_loss: 4452.7850 - val_mse: 28172892.0000 - val_mae: 4452.7847\n",
            "Epoch 139/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3683.1860 - mse: 23257732.0000 - mae: 3683.1860 - val_loss: 4443.3031 - val_mse: 27949256.0000 - val_mae: 4443.3032\n",
            "Epoch 140/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3959.0203 - mse: 24006702.0000 - mae: 3959.0203 - val_loss: 4435.0243 - val_mse: 27756620.0000 - val_mae: 4435.0244\n",
            "Epoch 141/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4065.5658 - mse: 25772888.0000 - mae: 4065.5657 - val_loss: 4430.3363 - val_mse: 27650466.0000 - val_mae: 4430.3364\n",
            "Epoch 142/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3933.3574 - mse: 24307478.0000 - mae: 3933.3572 - val_loss: 4425.5285 - val_mse: 27542638.0000 - val_mae: 4425.5288\n",
            "Epoch 143/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3879.3014 - mse: 24216170.0000 - mae: 3879.3015 - val_loss: 4418.8843 - val_mse: 27391236.0000 - val_mae: 4418.8843\n",
            "Epoch 144/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3834.3055 - mse: 23018124.0000 - mae: 3834.3052 - val_loss: 4409.4564 - val_mse: 27176702.0000 - val_mae: 4409.4561\n",
            "Epoch 145/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3917.1759 - mse: 24210518.0000 - mae: 3917.1758 - val_loss: 4404.6262 - val_mse: 27071002.0000 - val_mae: 4404.6260\n",
            "Epoch 146/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3822.4396 - mse: 22274090.0000 - mae: 3822.4397 - val_loss: 4398.8643 - val_mse: 26944956.0000 - val_mae: 4398.8647\n",
            "Epoch 147/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3862.1823 - mse: 23500344.0000 - mae: 3862.1824 - val_loss: 4391.9231 - val_mse: 26790412.0000 - val_mae: 4391.9229\n",
            "Epoch 148/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3855.2546 - mse: 24784630.0000 - mae: 3855.2546 - val_loss: 4383.2950 - val_mse: 26598058.0000 - val_mae: 4383.2949\n",
            "Epoch 149/500\n",
            "30/30 [==============================] - 0s 952us/step - loss: 3804.1486 - mse: 22406934.0000 - mae: 3804.1484 - val_loss: 4377.5380 - val_mse: 26473674.0000 - val_mae: 4377.5381\n",
            "Epoch 150/500\n",
            "30/30 [==============================] - 0s 970us/step - loss: 3701.1940 - mse: 22475610.0000 - mae: 3701.1938 - val_loss: 4367.5792 - val_mse: 26255078.0000 - val_mae: 4367.5791\n",
            "Epoch 151/500\n",
            "30/30 [==============================] - 0s 988us/step - loss: 3731.0347 - mse: 22193094.0000 - mae: 3731.0347 - val_loss: 4359.3409 - val_mse: 26077140.0000 - val_mae: 4359.3408\n",
            "Epoch 152/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3634.4709 - mse: 20669154.0000 - mae: 3634.4709 - val_loss: 4350.5225 - val_mse: 25888526.0000 - val_mae: 4350.5225\n",
            "Epoch 153/500\n",
            "30/30 [==============================] - 0s 950us/step - loss: 3737.9576 - mse: 22223146.0000 - mae: 3737.9578 - val_loss: 4344.6227 - val_mse: 25765066.0000 - val_mae: 4344.6230\n",
            "Epoch 154/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3935.9574 - mse: 24649940.0000 - mae: 3935.9575 - val_loss: 4340.1117 - val_mse: 25672426.0000 - val_mae: 4340.1118\n",
            "Epoch 155/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4007.6601 - mse: 24432274.0000 - mae: 4007.6602 - val_loss: 4334.6798 - val_mse: 25560076.0000 - val_mae: 4334.6797\n",
            "Epoch 156/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4051.0360 - mse: 24374148.0000 - mae: 4051.0359 - val_loss: 4328.1621 - val_mse: 25425128.0000 - val_mae: 4328.1621\n",
            "Epoch 157/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3764.8090 - mse: 21533980.0000 - mae: 3764.8088 - val_loss: 4320.3899 - val_mse: 25265228.0000 - val_mae: 4320.3896\n",
            "Epoch 158/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3579.9539 - mse: 20589966.0000 - mae: 3579.9539 - val_loss: 4314.5830 - val_mse: 25149024.0000 - val_mae: 4314.5830\n",
            "Epoch 159/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3449.8163 - mse: 20140518.0000 - mae: 3449.8162 - val_loss: 4302.5972 - val_mse: 24903894.0000 - val_mae: 4302.5972\n",
            "Epoch 160/500\n",
            "30/30 [==============================] - 0s 933us/step - loss: 4031.3398 - mse: 23935426.0000 - mae: 4031.3396 - val_loss: 4298.9622 - val_mse: 24833884.0000 - val_mae: 4298.9624\n",
            "Epoch 161/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 3673.4293 - mse: 20210168.0000 - mae: 3673.4292 - val_loss: 4293.0367 - val_mse: 24717524.0000 - val_mae: 4293.0366\n",
            "Epoch 162/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3899.3552 - mse: 22634410.0000 - mae: 3899.3555 - val_loss: 4286.7107 - val_mse: 24593720.0000 - val_mae: 4286.7104\n",
            "Epoch 163/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3752.2618 - mse: 22777038.0000 - mae: 3752.2617 - val_loss: 4279.6838 - val_mse: 24455752.0000 - val_mae: 4279.6841\n",
            "Epoch 164/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3698.0653 - mse: 20388792.0000 - mae: 3698.0652 - val_loss: 4270.0625 - val_mse: 24268350.0000 - val_mae: 4270.0625\n",
            "Epoch 165/500\n",
            "30/30 [==============================] - 0s 962us/step - loss: 3865.0293 - mse: 22182442.0000 - mae: 3865.0291 - val_loss: 4264.1007 - val_mse: 24154282.0000 - val_mae: 4264.1006\n",
            "Epoch 166/500\n",
            "30/30 [==============================] - 0s 923us/step - loss: 3818.7534 - mse: 22847886.0000 - mae: 3818.7537 - val_loss: 4260.8885 - val_mse: 24094836.0000 - val_mae: 4260.8887\n",
            "Epoch 167/500\n",
            "30/30 [==============================] - 0s 916us/step - loss: 3814.1294 - mse: 22209924.0000 - mae: 3814.1294 - val_loss: 4257.5500 - val_mse: 24033618.0000 - val_mae: 4257.5498\n",
            "Epoch 168/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3936.5609 - mse: 21282188.0000 - mae: 3936.5610 - val_loss: 4251.3611 - val_mse: 23918332.0000 - val_mae: 4251.3613\n",
            "Epoch 169/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3746.9069 - mse: 20858816.0000 - mae: 3746.9067 - val_loss: 4247.2729 - val_mse: 23844294.0000 - val_mae: 4247.2729\n",
            "Epoch 170/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3810.5199 - mse: 21752334.0000 - mae: 3810.5198 - val_loss: 4242.1757 - val_mse: 23749362.0000 - val_mae: 4242.1758\n",
            "Epoch 171/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3704.0767 - mse: 19679816.0000 - mae: 3704.0767 - val_loss: 4234.4618 - val_mse: 23606026.0000 - val_mae: 4234.4619\n",
            "Epoch 172/500\n",
            "30/30 [==============================] - 0s 991us/step - loss: 3990.7263 - mse: 23045468.0000 - mae: 3990.7263 - val_loss: 4233.0646 - val_mse: 23583036.0000 - val_mae: 4233.0645\n",
            "Epoch 173/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3840.7253 - mse: 21944666.0000 - mae: 3840.7253 - val_loss: 4228.6917 - val_mse: 23504468.0000 - val_mae: 4228.6919\n",
            "Epoch 174/500\n",
            "30/30 [==============================] - 0s 954us/step - loss: 3765.1160 - mse: 21336318.0000 - mae: 3765.1160 - val_loss: 4223.7445 - val_mse: 23415400.0000 - val_mae: 4223.7446\n",
            "Epoch 175/500\n",
            "30/30 [==============================] - 0s 936us/step - loss: 3893.0744 - mse: 21802642.0000 - mae: 3893.0742 - val_loss: 4218.9807 - val_mse: 23330342.0000 - val_mae: 4218.9810\n",
            "Epoch 176/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3871.2711 - mse: 22560326.0000 - mae: 3871.2710 - val_loss: 4218.6999 - val_mse: 23328436.0000 - val_mae: 4218.7002\n",
            "Epoch 177/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3793.2699 - mse: 21668546.0000 - mae: 3793.2698 - val_loss: 4215.3965 - val_mse: 23269944.0000 - val_mae: 4215.3965\n",
            "Epoch 178/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3491.6771 - mse: 19835830.0000 - mae: 3491.6770 - val_loss: 4210.4644 - val_mse: 23181790.0000 - val_mae: 4210.4644\n",
            "Epoch 179/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3366.8407 - mse: 19363460.0000 - mae: 3366.8406 - val_loss: 4203.6981 - val_mse: 23059846.0000 - val_mae: 4203.6978\n",
            "Epoch 180/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3653.6112 - mse: 20447048.0000 - mae: 3653.6116 - val_loss: 4197.3556 - val_mse: 22946596.0000 - val_mae: 4197.3560\n",
            "Epoch 181/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4008.9449 - mse: 22410484.0000 - mae: 4008.9448 - val_loss: 4199.9719 - val_mse: 22997964.0000 - val_mae: 4199.9717\n",
            "Epoch 182/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3787.7734 - mse: 22592910.0000 - mae: 3787.7734 - val_loss: 4194.1593 - val_mse: 22894696.0000 - val_mae: 4194.1592\n",
            "Epoch 183/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3459.4857 - mse: 19253184.0000 - mae: 3459.4858 - val_loss: 4184.5681 - val_mse: 22724364.0000 - val_mae: 4184.5684\n",
            "Epoch 184/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3678.2646 - mse: 20161856.0000 - mae: 3678.2646 - val_loss: 4180.5570 - val_mse: 22655534.0000 - val_mae: 4180.5571\n",
            "Epoch 185/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3743.8258 - mse: 20731570.0000 - mae: 3743.8257 - val_loss: 4178.8644 - val_mse: 22627876.0000 - val_mae: 4178.8643\n",
            "Epoch 186/500\n",
            "30/30 [==============================] - 0s 930us/step - loss: 3783.3167 - mse: 21570530.0000 - mae: 3783.3167 - val_loss: 4175.5966 - val_mse: 22572354.0000 - val_mae: 4175.5967\n",
            "Epoch 187/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3682.9434 - mse: 21172126.0000 - mae: 3682.9431 - val_loss: 4169.6711 - val_mse: 22469738.0000 - val_mae: 4169.6709\n",
            "Epoch 188/500\n",
            "30/30 [==============================] - 0s 939us/step - loss: 3841.1982 - mse: 21637630.0000 - mae: 3841.1982 - val_loss: 4163.8998 - val_mse: 22370876.0000 - val_mae: 4163.8999\n",
            "Epoch 189/500\n",
            "30/30 [==============================] - 0s 891us/step - loss: 3886.7827 - mse: 22965078.0000 - mae: 3886.7827 - val_loss: 4161.6732 - val_mse: 22334052.0000 - val_mae: 4161.6733\n",
            "Epoch 190/500\n",
            "30/30 [==============================] - 0s 910us/step - loss: 3818.4045 - mse: 21667148.0000 - mae: 3818.4045 - val_loss: 4158.5807 - val_mse: 22282548.0000 - val_mae: 4158.5806\n",
            "Epoch 191/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 3720.3700 - mse: 20852702.0000 - mae: 3720.3699 - val_loss: 4153.7551 - val_mse: 22201342.0000 - val_mae: 4153.7549\n",
            "Epoch 192/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3874.2783 - mse: 21516758.0000 - mae: 3874.2781 - val_loss: 4149.3372 - val_mse: 22127704.0000 - val_mae: 4149.3374\n",
            "Epoch 193/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3784.1567 - mse: 22260808.0000 - mae: 3784.1567 - val_loss: 4146.0001 - val_mse: 22072530.0000 - val_mae: 4146.0000\n",
            "Epoch 194/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3869.0579 - mse: 21831456.0000 - mae: 3869.0579 - val_loss: 4140.0515 - val_mse: 21973278.0000 - val_mae: 4140.0518\n",
            "Epoch 195/500\n",
            "30/30 [==============================] - 0s 949us/step - loss: 3826.8138 - mse: 21285024.0000 - mae: 3826.8135 - val_loss: 4135.0990 - val_mse: 21892156.0000 - val_mae: 4135.0991\n",
            "Epoch 196/500\n",
            "30/30 [==============================] - 0s 961us/step - loss: 3833.1680 - mse: 22357244.0000 - mae: 3833.1680 - val_loss: 4129.7190 - val_mse: 21803622.0000 - val_mae: 4129.7192\n",
            "Epoch 197/500\n",
            "30/30 [==============================] - 0s 993us/step - loss: 3900.7048 - mse: 20876812.0000 - mae: 3900.7048 - val_loss: 4127.4995 - val_mse: 21768584.0000 - val_mae: 4127.4995\n",
            "Epoch 198/500\n",
            "30/30 [==============================] - 0s 932us/step - loss: 3826.6326 - mse: 22051610.0000 - mae: 3826.6326 - val_loss: 4124.6876 - val_mse: 21723548.0000 - val_mae: 4124.6875\n",
            "Epoch 199/500\n",
            "30/30 [==============================] - 0s 979us/step - loss: 3689.7633 - mse: 20803182.0000 - mae: 3689.7632 - val_loss: 4121.3746 - val_mse: 21670698.0000 - val_mae: 4121.3745\n",
            "Epoch 200/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3729.8128 - mse: 21157324.0000 - mae: 3729.8130 - val_loss: 4120.5756 - val_mse: 21659578.0000 - val_mae: 4120.5757\n",
            "Epoch 201/500\n",
            "30/30 [==============================] - 0s 978us/step - loss: 3837.8706 - mse: 21909278.0000 - mae: 3837.8706 - val_loss: 4116.5662 - val_mse: 21595182.0000 - val_mae: 4116.5659\n",
            "Epoch 202/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3849.6165 - mse: 20506342.0000 - mae: 3849.6162 - val_loss: 4119.9857 - val_mse: 21653980.0000 - val_mae: 4119.9858\n",
            "Epoch 203/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3506.6488 - mse: 19337852.0000 - mae: 3506.6489 - val_loss: 4111.5232 - val_mse: 21516392.0000 - val_mae: 4111.5229\n",
            "Epoch 204/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3653.9412 - mse: 19756834.0000 - mae: 3653.9414 - val_loss: 4106.7000 - val_mse: 21439806.0000 - val_mae: 4106.7002\n",
            "Epoch 205/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3767.9664 - mse: 21019034.0000 - mae: 3767.9663 - val_loss: 4107.6859 - val_mse: 21457560.0000 - val_mae: 4107.6860\n",
            "Epoch 206/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3448.2686 - mse: 18325926.0000 - mae: 3448.2688 - val_loss: 4103.7050 - val_mse: 21394676.0000 - val_mae: 4103.7051\n",
            "Epoch 207/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3650.3787 - mse: 18984290.0000 - mae: 3650.3787 - val_loss: 4099.3713 - val_mse: 21326336.0000 - val_mae: 4099.3716\n",
            "Epoch 208/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3653.7402 - mse: 21502132.0000 - mae: 3653.7400 - val_loss: 4093.8709 - val_mse: 21239330.0000 - val_mae: 4093.8708\n",
            "Epoch 209/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3639.6759 - mse: 20576572.0000 - mae: 3639.6760 - val_loss: 4091.5991 - val_mse: 21204676.0000 - val_mae: 4091.5991\n",
            "Epoch 210/500\n",
            "30/30 [==============================] - 0s 977us/step - loss: 3589.8918 - mse: 19256578.0000 - mae: 3589.8918 - val_loss: 4086.1661 - val_mse: 21119862.0000 - val_mae: 4086.1663\n",
            "Epoch 211/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3779.9884 - mse: 21647326.0000 - mae: 3779.9885 - val_loss: 4087.0537 - val_mse: 21135526.0000 - val_mae: 4087.0537\n",
            "Epoch 212/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3834.6602 - mse: 22393094.0000 - mae: 3834.6602 - val_loss: 4084.1614 - val_mse: 21091164.0000 - val_mae: 4084.1614\n",
            "Epoch 213/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3449.4845 - mse: 18111526.0000 - mae: 3449.4844 - val_loss: 4080.5555 - val_mse: 21036014.0000 - val_mae: 4080.5554\n",
            "Epoch 214/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3386.2795 - mse: 18793684.0000 - mae: 3386.2795 - val_loss: 4080.3464 - val_mse: 21034330.0000 - val_mae: 4080.3464\n",
            "Epoch 215/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3555.8880 - mse: 19666048.0000 - mae: 3555.8879 - val_loss: 4078.1078 - val_mse: 21000702.0000 - val_mae: 4078.1079\n",
            "Epoch 216/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3691.1496 - mse: 20366714.0000 - mae: 3691.1494 - val_loss: 4075.8825 - val_mse: 20967420.0000 - val_mae: 4075.8826\n",
            "Epoch 217/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3639.7680 - mse: 19619294.0000 - mae: 3639.7681 - val_loss: 4072.6878 - val_mse: 20918996.0000 - val_mae: 4072.6880\n",
            "Epoch 218/500\n",
            "30/30 [==============================] - 0s 929us/step - loss: 3632.9154 - mse: 20063394.0000 - mae: 3632.9153 - val_loss: 4068.5026 - val_mse: 20855040.0000 - val_mae: 4068.5027\n",
            "Epoch 219/500\n",
            "30/30 [==============================] - 0s 933us/step - loss: 3597.3201 - mse: 20038272.0000 - mae: 3597.3201 - val_loss: 4066.9203 - val_mse: 20831934.0000 - val_mae: 4066.9204\n",
            "Epoch 220/500\n",
            "30/30 [==============================] - 0s 892us/step - loss: 3887.7857 - mse: 21137514.0000 - mae: 3887.7854 - val_loss: 4066.5817 - val_mse: 20828236.0000 - val_mae: 4066.5818\n",
            "Epoch 221/500\n",
            "30/30 [==============================] - 0s 969us/step - loss: 3602.3837 - mse: 18141238.0000 - mae: 3602.3838 - val_loss: 4061.7380 - val_mse: 20755016.0000 - val_mae: 4061.7380\n",
            "Epoch 222/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3695.3164 - mse: 20044998.0000 - mae: 3695.3164 - val_loss: 4059.8075 - val_mse: 20726866.0000 - val_mae: 4059.8076\n",
            "Epoch 223/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3881.4137 - mse: 21844836.0000 - mae: 3881.4136 - val_loss: 4058.2706 - val_mse: 20704726.0000 - val_mae: 4058.2705\n",
            "Epoch 224/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3823.8352 - mse: 21785474.0000 - mae: 3823.8354 - val_loss: 4058.3135 - val_mse: 20706956.0000 - val_mae: 4058.3135\n",
            "Epoch 225/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3748.2628 - mse: 20793792.0000 - mae: 3748.2627 - val_loss: 4059.0389 - val_mse: 20719662.0000 - val_mae: 4059.0388\n",
            "Epoch 226/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3792.2686 - mse: 20904498.0000 - mae: 3792.2688 - val_loss: 4057.6598 - val_mse: 20700268.0000 - val_mae: 4057.6599\n",
            "Epoch 227/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3685.5726 - mse: 19491070.0000 - mae: 3685.5725 - val_loss: 4054.2048 - val_mse: 20649076.0000 - val_mae: 4054.2046\n",
            "Epoch 228/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3464.6141 - mse: 17444564.0000 - mae: 3464.6140 - val_loss: 4050.7959 - val_mse: 20599086.0000 - val_mae: 4050.7959\n",
            "Epoch 229/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3671.5138 - mse: 19957624.0000 - mae: 3671.5139 - val_loss: 4048.0168 - val_mse: 20558892.0000 - val_mae: 4048.0168\n",
            "Epoch 230/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3816.5841 - mse: 22313902.0000 - mae: 3816.5837 - val_loss: 4043.0667 - val_mse: 20485640.0000 - val_mae: 4043.0667\n",
            "Epoch 231/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3596.5910 - mse: 19437510.0000 - mae: 3596.5908 - val_loss: 4040.7712 - val_mse: 20452838.0000 - val_mae: 4040.7712\n",
            "Epoch 232/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3867.1895 - mse: 21284078.0000 - mae: 3867.1897 - val_loss: 4037.6166 - val_mse: 20407406.0000 - val_mae: 4037.6167\n",
            "Epoch 233/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 3877.0302 - mse: 21897322.0000 - mae: 3877.0303 - val_loss: 4036.7145 - val_mse: 20395788.0000 - val_mae: 4036.7146\n",
            "Epoch 234/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3434.7411 - mse: 17981038.0000 - mae: 3434.7412 - val_loss: 4029.2152 - val_mse: 20285882.0000 - val_mae: 4029.2151\n",
            "Epoch 235/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3442.4523 - mse: 17935898.0000 - mae: 3442.4521 - val_loss: 4024.0601 - val_mse: 20210646.0000 - val_mae: 4024.0601\n",
            "Epoch 236/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3720.7201 - mse: 20497956.0000 - mae: 3720.7202 - val_loss: 4022.6479 - val_mse: 20191716.0000 - val_mae: 4022.6479\n",
            "Epoch 237/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3689.5540 - mse: 19381346.0000 - mae: 3689.5540 - val_loss: 4018.6299 - val_mse: 20133868.0000 - val_mae: 4018.6299\n",
            "Epoch 238/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3542.5378 - mse: 18077378.0000 - mae: 3542.5378 - val_loss: 4013.4417 - val_mse: 20059446.0000 - val_mae: 4013.4417\n",
            "Epoch 239/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3656.7755 - mse: 17919746.0000 - mae: 3656.7756 - val_loss: 4006.7023 - val_mse: 19962322.0000 - val_mae: 4006.7024\n",
            "Epoch 240/500\n",
            "30/30 [==============================] - 0s 991us/step - loss: 3818.2320 - mse: 20047500.0000 - mae: 3818.2319 - val_loss: 4004.4578 - val_mse: 19930486.0000 - val_mae: 4004.4578\n",
            "Epoch 241/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3838.3454 - mse: 22370412.0000 - mae: 3838.3452 - val_loss: 4003.6041 - val_mse: 19920948.0000 - val_mae: 4003.6042\n",
            "Epoch 242/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3743.1270 - mse: 20572626.0000 - mae: 3743.1272 - val_loss: 4005.0266 - val_mse: 19942430.0000 - val_mae: 4005.0266\n",
            "Epoch 243/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3562.7463 - mse: 18950024.0000 - mae: 3562.7463 - val_loss: 3999.0765 - val_mse: 19857954.0000 - val_mae: 3999.0767\n",
            "Epoch 244/500\n",
            "30/30 [==============================] - 0s 928us/step - loss: 3564.1257 - mse: 18821250.0000 - mae: 3564.1257 - val_loss: 3991.5344 - val_mse: 19751038.0000 - val_mae: 3991.5344\n",
            "Epoch 245/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3570.8368 - mse: 18474588.0000 - mae: 3570.8369 - val_loss: 3984.5581 - val_mse: 19650466.0000 - val_mae: 3984.5583\n",
            "Epoch 246/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3610.0841 - mse: 19514802.0000 - mae: 3610.0845 - val_loss: 3980.1571 - val_mse: 19589290.0000 - val_mae: 3980.1572\n",
            "Epoch 247/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3901.1979 - mse: 21965844.0000 - mae: 3901.1980 - val_loss: 3976.8868 - val_mse: 19545248.0000 - val_mae: 3976.8870\n",
            "Epoch 248/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3584.0706 - mse: 18926146.0000 - mae: 3584.0706 - val_loss: 3966.2856 - val_mse: 19400652.0000 - val_mae: 3966.2854\n",
            "Epoch 249/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3732.3097 - mse: 20757908.0000 - mae: 3732.3098 - val_loss: 3962.3746 - val_mse: 19349398.0000 - val_mae: 3962.3748\n",
            "Epoch 250/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3543.5725 - mse: 18659486.0000 - mae: 3543.5725 - val_loss: 3955.9540 - val_mse: 19261250.0000 - val_mae: 3955.9541\n",
            "Epoch 251/500\n",
            "30/30 [==============================] - 0s 938us/step - loss: 3774.8730 - mse: 20083098.0000 - mae: 3774.8728 - val_loss: 3951.8508 - val_mse: 19205682.0000 - val_mae: 3951.8508\n",
            "Epoch 252/500\n",
            "30/30 [==============================] - 0s 972us/step - loss: 3459.5046 - mse: 17800170.0000 - mae: 3459.5044 - val_loss: 3942.7827 - val_mse: 19082134.0000 - val_mae: 3942.7827\n",
            "Epoch 253/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3690.9466 - mse: 19118116.0000 - mae: 3690.9465 - val_loss: 3942.2248 - val_mse: 19074274.0000 - val_mae: 3942.2249\n",
            "Epoch 254/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3568.8142 - mse: 19164150.0000 - mae: 3568.8140 - val_loss: 3936.3349 - val_mse: 18994650.0000 - val_mae: 3936.3350\n",
            "Epoch 255/500\n",
            "30/30 [==============================] - 0s 960us/step - loss: 3448.7444 - mse: 17802362.0000 - mae: 3448.7444 - val_loss: 3925.8660 - val_mse: 18853102.0000 - val_mae: 3925.8660\n",
            "Epoch 256/500\n",
            "30/30 [==============================] - 0s 980us/step - loss: 3644.8913 - mse: 19642708.0000 - mae: 3644.8911 - val_loss: 3917.8807 - val_mse: 18745544.0000 - val_mae: 3917.8806\n",
            "Epoch 257/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3713.2224 - mse: 19350072.0000 - mae: 3713.2224 - val_loss: 3915.3608 - val_mse: 18711676.0000 - val_mae: 3915.3608\n",
            "Epoch 258/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3537.8617 - mse: 18553148.0000 - mae: 3537.8618 - val_loss: 3907.5303 - val_mse: 18608758.0000 - val_mae: 3907.5303\n",
            "Epoch 259/500\n",
            "30/30 [==============================] - 0s 974us/step - loss: 3628.6860 - mse: 20980440.0000 - mae: 3628.6863 - val_loss: 3904.4597 - val_mse: 18568550.0000 - val_mae: 3904.4600\n",
            "Epoch 260/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3856.0591 - mse: 20492588.0000 - mae: 3856.0591 - val_loss: 3901.3923 - val_mse: 18527958.0000 - val_mae: 3901.3921\n",
            "Epoch 261/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3564.1528 - mse: 18650342.0000 - mae: 3564.1528 - val_loss: 3900.1766 - val_mse: 18510846.0000 - val_mae: 3900.1765\n",
            "Epoch 262/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3676.9531 - mse: 20605850.0000 - mae: 3676.9531 - val_loss: 3905.7655 - val_mse: 18583556.0000 - val_mae: 3905.7656\n",
            "Epoch 263/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3472.3605 - mse: 17932660.0000 - mae: 3472.3604 - val_loss: 3899.8889 - val_mse: 18504764.0000 - val_mae: 3899.8889\n",
            "Epoch 264/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3513.9283 - mse: 17964878.0000 - mae: 3513.9285 - val_loss: 3891.5793 - val_mse: 18396334.0000 - val_mae: 3891.5791\n",
            "Epoch 265/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3422.1743 - mse: 16722617.0000 - mae: 3422.1743 - val_loss: 3888.9200 - val_mse: 18361206.0000 - val_mae: 3888.9197\n",
            "Epoch 266/500\n",
            "30/30 [==============================] - 0s 995us/step - loss: 3597.1059 - mse: 18714594.0000 - mae: 3597.1057 - val_loss: 3883.0007 - val_mse: 18283076.0000 - val_mae: 3883.0005\n",
            "Epoch 267/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3556.8300 - mse: 18111840.0000 - mae: 3556.8296 - val_loss: 3874.4278 - val_mse: 18173084.0000 - val_mae: 3874.4275\n",
            "Epoch 268/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3590.5312 - mse: 17820820.0000 - mae: 3590.5312 - val_loss: 3868.7000 - val_mse: 18098944.0000 - val_mae: 3868.7000\n",
            "Epoch 269/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3604.3921 - mse: 19006844.0000 - mae: 3604.3921 - val_loss: 3866.1257 - val_mse: 18065420.0000 - val_mae: 3866.1255\n",
            "Epoch 270/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3600.6098 - mse: 18970108.0000 - mae: 3600.6096 - val_loss: 3865.2585 - val_mse: 18053422.0000 - val_mae: 3865.2585\n",
            "Epoch 271/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3450.0977 - mse: 16826342.0000 - mae: 3450.0977 - val_loss: 3849.9163 - val_mse: 17858842.0000 - val_mae: 3849.9163\n",
            "Epoch 272/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3524.9303 - mse: 18162990.0000 - mae: 3524.9304 - val_loss: 3848.0454 - val_mse: 17834878.0000 - val_mae: 3848.0454\n",
            "Epoch 273/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3495.5291 - mse: 17438058.0000 - mae: 3495.5291 - val_loss: 3840.9735 - val_mse: 17746300.0000 - val_mae: 3840.9734\n",
            "Epoch 274/500\n",
            "30/30 [==============================] - 0s 970us/step - loss: 3491.0993 - mse: 16784404.0000 - mae: 3491.0991 - val_loss: 3848.2926 - val_mse: 17837600.0000 - val_mae: 3848.2927\n",
            "Epoch 275/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3449.6910 - mse: 17753780.0000 - mae: 3449.6909 - val_loss: 3844.8455 - val_mse: 17794116.0000 - val_mae: 3844.8452\n",
            "Epoch 276/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 3499.2159 - mse: 17641272.0000 - mae: 3499.2158 - val_loss: 3841.0724 - val_mse: 17746782.0000 - val_mae: 3841.0725\n",
            "Epoch 277/500\n",
            "30/30 [==============================] - 0s 985us/step - loss: 3718.2796 - mse: 19283838.0000 - mae: 3718.2798 - val_loss: 3838.7916 - val_mse: 17719208.0000 - val_mae: 3838.7917\n",
            "Epoch 278/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3785.2015 - mse: 20648190.0000 - mae: 3785.2017 - val_loss: 3837.2795 - val_mse: 17700228.0000 - val_mae: 3837.2795\n",
            "Epoch 279/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3688.9823 - mse: 18509666.0000 - mae: 3688.9822 - val_loss: 3843.7856 - val_mse: 17782584.0000 - val_mae: 3843.7856\n",
            "Epoch 280/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 3717.1977 - mse: 20419264.0000 - mae: 3717.1978 - val_loss: 3840.1311 - val_mse: 17738228.0000 - val_mae: 3840.1313\n",
            "Epoch 281/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3605.5156 - mse: 19198426.0000 - mae: 3605.5156 - val_loss: 3845.0380 - val_mse: 17802216.0000 - val_mae: 3845.0381\n",
            "Epoch 282/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3504.4415 - mse: 18015474.0000 - mae: 3504.4417 - val_loss: 3837.3032 - val_mse: 17706436.0000 - val_mae: 3837.3032\n",
            "Epoch 283/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3632.1649 - mse: 19211712.0000 - mae: 3632.1648 - val_loss: 3836.3083 - val_mse: 17695814.0000 - val_mae: 3836.3083\n",
            "Epoch 284/500\n",
            "30/30 [==============================] - 0s 978us/step - loss: 3350.3791 - mse: 16870082.0000 - mae: 3350.3792 - val_loss: 3834.2145 - val_mse: 17673694.0000 - val_mae: 3834.2146\n",
            "Epoch 285/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3530.1316 - mse: 18170396.0000 - mae: 3530.1313 - val_loss: 3842.2489 - val_mse: 17780810.0000 - val_mae: 3842.2490\n",
            "Epoch 286/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 3643.5116 - mse: 18617398.0000 - mae: 3643.5115 - val_loss: 3854.0050 - val_mse: 17946560.0000 - val_mae: 3854.0051\n",
            "Epoch 287/500\n",
            "30/30 [==============================] - 0s 999us/step - loss: 3740.2262 - mse: 20358154.0000 - mae: 3740.2261 - val_loss: 3858.4781 - val_mse: 18042340.0000 - val_mae: 3858.4780\n",
            "Epoch 288/500\n",
            "30/30 [==============================] - 0s 966us/step - loss: 3839.3302 - mse: 20586524.0000 - mae: 3839.3303 - val_loss: 3862.7009 - val_mse: 18212806.0000 - val_mae: 3862.7009\n",
            "Epoch 289/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3128.6405 - mse: 14620902.0000 - mae: 3128.6406 - val_loss: 3842.1433 - val_mse: 18386158.0000 - val_mae: 3842.1433\n",
            "Epoch 290/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3447.6395 - mse: 18400576.0000 - mae: 3447.6396 - val_loss: 3814.8567 - val_mse: 19801654.0000 - val_mae: 3814.8567\n",
            "Epoch 291/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 3070.7668 - mse: 17001862.0000 - mae: 3070.7666 - val_loss: 4403.5027 - val_mse: 26206728.0000 - val_mae: 4403.5029\n",
            "Epoch 292/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2397.0642 - mse: 12184922.0000 - mae: 2397.0640 - val_loss: 4473.9814 - val_mse: 26710430.0000 - val_mae: 4473.9814\n",
            "Epoch 293/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2620.6665 - mse: 15682542.0000 - mae: 2620.6665 - val_loss: 4659.8176 - val_mse: 28309960.0000 - val_mae: 4659.8179\n",
            "Epoch 294/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2622.8308 - mse: 15270270.0000 - mae: 2622.8308 - val_loss: 4384.9517 - val_mse: 25902178.0000 - val_mae: 4384.9517\n",
            "Epoch 295/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2524.2875 - mse: 14443798.0000 - mae: 2524.2876 - val_loss: 4528.6334 - val_mse: 27024616.0000 - val_mae: 4528.6333\n",
            "Epoch 296/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2104.7920 - mse: 11082885.0000 - mae: 2104.7920 - val_loss: 4621.9464 - val_mse: 27868170.0000 - val_mae: 4621.9463\n",
            "Epoch 297/500\n",
            "30/30 [==============================] - 0s 965us/step - loss: 2356.6905 - mse: 13331390.0000 - mae: 2356.6904 - val_loss: 4462.5551 - val_mse: 26423544.0000 - val_mae: 4462.5552\n",
            "Epoch 298/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2461.2545 - mse: 14586027.0000 - mae: 2461.2546 - val_loss: 4726.2713 - val_mse: 29011562.0000 - val_mae: 4726.2715\n",
            "Epoch 299/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2342.9613 - mse: 13035100.0000 - mae: 2342.9612 - val_loss: 4685.7133 - val_mse: 28501240.0000 - val_mae: 4685.7134\n",
            "Epoch 300/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2423.1998 - mse: 13307757.0000 - mae: 2423.2000 - val_loss: 4544.4913 - val_mse: 27057506.0000 - val_mae: 4544.4917\n",
            "Epoch 301/500\n",
            "30/30 [==============================] - 0s 956us/step - loss: 2392.4597 - mse: 13637914.0000 - mae: 2392.4597 - val_loss: 4672.7485 - val_mse: 28348594.0000 - val_mae: 4672.7485\n",
            "Epoch 302/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2297.6160 - mse: 13312897.0000 - mae: 2297.6162 - val_loss: 4646.6539 - val_mse: 28036562.0000 - val_mae: 4646.6538\n",
            "Epoch 303/500\n",
            "30/30 [==============================] - 0s 999us/step - loss: 2236.3931 - mse: 12230785.0000 - mae: 2236.3933 - val_loss: 4546.5714 - val_mse: 27025396.0000 - val_mae: 4546.5713\n",
            "Epoch 304/500\n",
            "30/30 [==============================] - 0s 945us/step - loss: 2252.6819 - mse: 11075963.0000 - mae: 2252.6819 - val_loss: 4630.4638 - val_mse: 27856298.0000 - val_mae: 4630.4634\n",
            "Epoch 305/500\n",
            "30/30 [==============================] - 0s 999us/step - loss: 2177.4231 - mse: 11483539.0000 - mae: 2177.4231 - val_loss: 4498.3656 - val_mse: 26526178.0000 - val_mae: 4498.3657\n",
            "Epoch 306/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2256.7090 - mse: 12229156.0000 - mae: 2256.7092 - val_loss: 4538.7434 - val_mse: 26877536.0000 - val_mae: 4538.7432\n",
            "Epoch 307/500\n",
            "30/30 [==============================] - 0s 966us/step - loss: 2383.1529 - mse: 13209922.0000 - mae: 2383.1528 - val_loss: 4617.1905 - val_mse: 27700128.0000 - val_mae: 4617.1904\n",
            "Epoch 308/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2222.0213 - mse: 11344497.0000 - mae: 2222.0210 - val_loss: 4471.3491 - val_mse: 26223628.0000 - val_mae: 4471.3491\n",
            "Epoch 309/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2294.8064 - mse: 11744922.0000 - mae: 2294.8062 - val_loss: 4562.6155 - val_mse: 27097132.0000 - val_mae: 4562.6157\n",
            "Epoch 310/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 2208.5265 - mse: 11878366.0000 - mae: 2208.5266 - val_loss: 4567.1020 - val_mse: 27141384.0000 - val_mae: 4567.1021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50 samples, validate on 40 samples\n",
            "Epoch 1/500\n",
            "50/50 [==============================] - 3s 55ms/step - loss: 6315.8818 - mse: 53640888.0000 - mae: 6315.8818 - val_loss: 6601.7226 - val_mse: 54654408.0000 - val_mae: 6601.7227\n",
            "Epoch 2/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6315.7949 - mse: 53639876.0000 - mae: 6315.7949 - val_loss: 6601.4771 - val_mse: 54651560.0000 - val_mae: 6601.4766\n",
            "Epoch 3/500\n",
            "50/50 [==============================] - 0s 851us/step - loss: 6314.6587 - mse: 53629492.0000 - mae: 6314.6587 - val_loss: 6598.5108 - val_mse: 54617172.0000 - val_mae: 6598.5107\n",
            "Epoch 4/500\n",
            "50/50 [==============================] - 0s 859us/step - loss: 6310.6294 - mse: 53581972.0000 - mae: 6310.6294 - val_loss: 6593.3402 - val_mse: 54556160.0000 - val_mae: 6593.3408\n",
            "Epoch 5/500\n",
            "50/50 [==============================] - 0s 891us/step - loss: 6303.8449 - mse: 53502972.0000 - mae: 6303.8452 - val_loss: 6587.8848 - val_mse: 54491508.0000 - val_mae: 6587.8843\n",
            "Epoch 6/500\n",
            "50/50 [==============================] - 0s 900us/step - loss: 6300.5520 - mse: 53439408.0000 - mae: 6300.5518 - val_loss: 6583.1236 - val_mse: 54434728.0000 - val_mae: 6583.1235\n",
            "Epoch 7/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6295.7695 - mse: 53404404.0000 - mae: 6295.7695 - val_loss: 6578.0820 - val_mse: 54374232.0000 - val_mae: 6578.0820\n",
            "Epoch 8/500\n",
            "50/50 [==============================] - 0s 900us/step - loss: 6287.9303 - mse: 53314344.0000 - mae: 6287.9302 - val_loss: 6572.0319 - val_mse: 54301176.0000 - val_mae: 6572.0312\n",
            "Epoch 9/500\n",
            "50/50 [==============================] - 0s 950us/step - loss: 6282.8195 - mse: 53244216.0000 - mae: 6282.8198 - val_loss: 6565.5688 - val_mse: 54222452.0000 - val_mae: 6565.5688\n",
            "Epoch 10/500\n",
            "50/50 [==============================] - 0s 940us/step - loss: 6274.6544 - mse: 53153772.0000 - mae: 6274.6548 - val_loss: 6558.5326 - val_mse: 54136408.0000 - val_mae: 6558.5327\n",
            "Epoch 11/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6271.4778 - mse: 53124168.0000 - mae: 6271.4775 - val_loss: 6551.3928 - val_mse: 54048088.0000 - val_mae: 6551.3926\n",
            "Epoch 12/500\n",
            "50/50 [==============================] - 0s 908us/step - loss: 6257.6052 - mse: 52962236.0000 - mae: 6257.6050 - val_loss: 6543.0192 - val_mse: 53944972.0000 - val_mae: 6543.0190\n",
            "Epoch 13/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6250.5504 - mse: 52852560.0000 - mae: 6250.5498 - val_loss: 6534.6187 - val_mse: 53841516.0000 - val_mae: 6534.6187\n",
            "Epoch 14/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6241.0340 - mse: 52775608.0000 - mae: 6241.0337 - val_loss: 6525.6870 - val_mse: 53731480.0000 - val_mae: 6525.6865\n",
            "Epoch 15/500\n",
            "50/50 [==============================] - 0s 889us/step - loss: 6228.1053 - mse: 52556780.0000 - mae: 6228.1050 - val_loss: 6516.2932 - val_mse: 53614976.0000 - val_mae: 6516.2930\n",
            "Epoch 16/500\n",
            "50/50 [==============================] - 0s 937us/step - loss: 6231.9784 - mse: 52589200.0000 - mae: 6231.9785 - val_loss: 6507.7548 - val_mse: 53508948.0000 - val_mae: 6507.7549\n",
            "Epoch 17/500\n",
            "50/50 [==============================] - 0s 876us/step - loss: 6216.6352 - mse: 52402152.0000 - mae: 6216.6348 - val_loss: 6498.1352 - val_mse: 53389552.0000 - val_mae: 6498.1353\n",
            "Epoch 18/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6200.4214 - mse: 52241604.0000 - mae: 6200.4214 - val_loss: 6487.4935 - val_mse: 53257432.0000 - val_mae: 6487.4937\n",
            "Epoch 19/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6190.4858 - mse: 52120588.0000 - mae: 6190.4863 - val_loss: 6476.9062 - val_mse: 53125384.0000 - val_mae: 6476.9062\n",
            "Epoch 20/500\n",
            "50/50 [==============================] - 0s 945us/step - loss: 6190.8855 - mse: 52098856.0000 - mae: 6190.8857 - val_loss: 6467.4244 - val_mse: 53007028.0000 - val_mae: 6467.4243\n",
            "Epoch 21/500\n",
            "50/50 [==============================] - 0s 917us/step - loss: 6185.4382 - mse: 51993076.0000 - mae: 6185.4380 - val_loss: 6457.0892 - val_mse: 52878776.0000 - val_mae: 6457.0889\n",
            "Epoch 22/500\n",
            "50/50 [==============================] - 0s 954us/step - loss: 6170.8924 - mse: 51849836.0000 - mae: 6170.8926 - val_loss: 6446.4322 - val_mse: 52745892.0000 - val_mae: 6446.4326\n",
            "Epoch 23/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6160.9275 - mse: 51735164.0000 - mae: 6160.9277 - val_loss: 6435.3677 - val_mse: 52607640.0000 - val_mae: 6435.3672\n",
            "Epoch 24/500\n",
            "50/50 [==============================] - 0s 826us/step - loss: 6148.6836 - mse: 51522052.0000 - mae: 6148.6836 - val_loss: 6423.5673 - val_mse: 52460932.0000 - val_mae: 6423.5674\n",
            "Epoch 25/500\n",
            "50/50 [==============================] - 0s 875us/step - loss: 6141.7130 - mse: 51468668.0000 - mae: 6141.7124 - val_loss: 6411.5889 - val_mse: 52312000.0000 - val_mae: 6411.5889\n",
            "Epoch 26/500\n",
            "50/50 [==============================] - 0s 921us/step - loss: 6112.4814 - mse: 51063920.0000 - mae: 6112.4814 - val_loss: 6397.9430 - val_mse: 52143272.0000 - val_mae: 6397.9429\n",
            "Epoch 27/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6127.2766 - mse: 51272132.0000 - mae: 6127.2764 - val_loss: 6386.4884 - val_mse: 52000520.0000 - val_mae: 6386.4883\n",
            "Epoch 28/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6100.6569 - mse: 50780520.0000 - mae: 6100.6567 - val_loss: 6372.2406 - val_mse: 51824184.0000 - val_mae: 6372.2407\n",
            "Epoch 29/500\n",
            "50/50 [==============================] - 0s 951us/step - loss: 6113.0610 - mse: 51011000.0000 - mae: 6113.0610 - val_loss: 6359.6542 - val_mse: 51669096.0000 - val_mae: 6359.6543\n",
            "Epoch 30/500\n",
            "50/50 [==============================] - 0s 921us/step - loss: 6068.0373 - mse: 50317236.0000 - mae: 6068.0376 - val_loss: 6343.6234 - val_mse: 51471412.0000 - val_mae: 6343.6235\n",
            "Epoch 31/500\n",
            "50/50 [==============================] - 0s 971us/step - loss: 6068.0710 - mse: 50353780.0000 - mae: 6068.0713 - val_loss: 6330.8065 - val_mse: 51313080.0000 - val_mae: 6330.8066\n",
            "Epoch 32/500\n",
            "50/50 [==============================] - 0s 828us/step - loss: 6057.6991 - mse: 50122984.0000 - mae: 6057.6992 - val_loss: 6316.6214 - val_mse: 51138472.0000 - val_mae: 6316.6221\n",
            "Epoch 33/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6059.5723 - mse: 50085244.0000 - mae: 6059.5723 - val_loss: 6301.3633 - val_mse: 50951092.0000 - val_mae: 6301.3633\n",
            "Epoch 34/500\n",
            "50/50 [==============================] - 0s 887us/step - loss: 6042.1657 - mse: 49917372.0000 - mae: 6042.1655 - val_loss: 6287.6488 - val_mse: 50781940.0000 - val_mae: 6287.6484\n",
            "Epoch 35/500\n",
            "50/50 [==============================] - 0s 925us/step - loss: 6044.7895 - mse: 49943756.0000 - mae: 6044.7896 - val_loss: 6274.0639 - val_mse: 50615084.0000 - val_mae: 6274.0635\n",
            "Epoch 36/500\n",
            "50/50 [==============================] - 0s 933us/step - loss: 6004.5854 - mse: 49351372.0000 - mae: 6004.5854 - val_loss: 6257.0060 - val_mse: 50407052.0000 - val_mae: 6257.0059\n",
            "Epoch 37/500\n",
            "50/50 [==============================] - 0s 850us/step - loss: 6028.3752 - mse: 49661512.0000 - mae: 6028.3750 - val_loss: 6242.8964 - val_mse: 50234880.0000 - val_mae: 6242.8965\n",
            "Epoch 38/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5990.7643 - mse: 49141008.0000 - mae: 5990.7637 - val_loss: 6226.8794 - val_mse: 50040120.0000 - val_mae: 6226.8799\n",
            "Epoch 39/500\n",
            "50/50 [==============================] - 0s 957us/step - loss: 5997.0923 - mse: 49240432.0000 - mae: 5997.0918 - val_loss: 6211.6490 - val_mse: 49855060.0000 - val_mae: 6211.6494\n",
            "Epoch 40/500\n",
            "50/50 [==============================] - 0s 975us/step - loss: 5999.5831 - mse: 49067196.0000 - mae: 5999.5830 - val_loss: 6196.1381 - val_mse: 49666488.0000 - val_mae: 6196.1377\n",
            "Epoch 41/500\n",
            "50/50 [==============================] - 0s 952us/step - loss: 5961.1485 - mse: 48697548.0000 - mae: 5961.1489 - val_loss: 6181.8237 - val_mse: 49493172.0000 - val_mae: 6181.8237\n",
            "Epoch 42/500\n",
            "50/50 [==============================] - 0s 894us/step - loss: 5990.4745 - mse: 48921816.0000 - mae: 5990.4746 - val_loss: 6168.0734 - val_mse: 49326632.0000 - val_mae: 6168.0732\n",
            "Epoch 43/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5934.6709 - mse: 48016620.0000 - mae: 5934.6714 - val_loss: 6151.4693 - val_mse: 49126344.0000 - val_mae: 6151.4697\n",
            "Epoch 44/500\n",
            "50/50 [==============================] - 0s 898us/step - loss: 5920.2129 - mse: 47890944.0000 - mae: 5920.2124 - val_loss: 6134.4043 - val_mse: 48921268.0000 - val_mae: 6134.4043\n",
            "Epoch 45/500\n",
            "50/50 [==============================] - 0s 837us/step - loss: 5943.4384 - mse: 47939516.0000 - mae: 5943.4380 - val_loss: 6119.4283 - val_mse: 48740844.0000 - val_mae: 6119.4287\n",
            "Epoch 46/500\n",
            "50/50 [==============================] - 0s 866us/step - loss: 5923.8540 - mse: 47587784.0000 - mae: 5923.8535 - val_loss: 6101.4776 - val_mse: 48525576.0000 - val_mae: 6101.4775\n",
            "Epoch 47/500\n",
            "50/50 [==============================] - 0s 867us/step - loss: 5912.4845 - mse: 47364304.0000 - mae: 5912.4844 - val_loss: 6085.2132 - val_mse: 48331064.0000 - val_mae: 6085.2134\n",
            "Epoch 48/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5925.5953 - mse: 47544360.0000 - mae: 5925.5952 - val_loss: 6068.8687 - val_mse: 48136376.0000 - val_mae: 6068.8682\n",
            "Epoch 49/500\n",
            "50/50 [==============================] - 0s 873us/step - loss: 5919.3069 - mse: 47545168.0000 - mae: 5919.3066 - val_loss: 6053.3254 - val_mse: 47950776.0000 - val_mae: 6053.3252\n",
            "Epoch 50/500\n",
            "50/50 [==============================] - 0s 822us/step - loss: 5892.4738 - mse: 47154412.0000 - mae: 5892.4736 - val_loss: 6036.6070 - val_mse: 47752364.0000 - val_mae: 6036.6069\n",
            "Epoch 51/500\n",
            "50/50 [==============================] - 0s 849us/step - loss: 5845.1365 - mse: 46311004.0000 - mae: 5845.1362 - val_loss: 6018.2682 - val_mse: 47526780.0000 - val_mae: 6018.2686\n",
            "Epoch 52/500\n",
            "50/50 [==============================] - 0s 845us/step - loss: 5872.6927 - mse: 46739644.0000 - mae: 5872.6924 - val_loss: 6001.7301 - val_mse: 47321144.0000 - val_mae: 6001.7305\n",
            "Epoch 53/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5834.3435 - mse: 46399248.0000 - mae: 5834.3438 - val_loss: 5982.4441 - val_mse: 47081816.0000 - val_mae: 5982.4438\n",
            "Epoch 54/500\n",
            "50/50 [==============================] - 0s 906us/step - loss: 5843.0984 - mse: 46328588.0000 - mae: 5843.0986 - val_loss: 5964.6948 - val_mse: 46861880.0000 - val_mae: 5964.6943\n",
            "Epoch 55/500\n",
            "50/50 [==============================] - 0s 997us/step - loss: 5828.4790 - mse: 46105456.0000 - mae: 5828.4785 - val_loss: 5946.6695 - val_mse: 46637448.0000 - val_mae: 5946.6694\n",
            "Epoch 56/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5778.9003 - mse: 45447776.0000 - mae: 5778.8999 - val_loss: 5925.1831 - val_mse: 46358708.0000 - val_mae: 5925.1826\n",
            "Epoch 57/500\n",
            "50/50 [==============================] - 0s 805us/step - loss: 5805.7122 - mse: 45492568.0000 - mae: 5805.7124 - val_loss: 5907.4162 - val_mse: 46128312.0000 - val_mae: 5907.4165\n",
            "Epoch 58/500\n",
            "50/50 [==============================] - 0s 828us/step - loss: 5751.1383 - mse: 44798424.0000 - mae: 5751.1382 - val_loss: 5886.5605 - val_mse: 45858592.0000 - val_mae: 5886.5601\n",
            "Epoch 59/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5770.7104 - mse: 44923188.0000 - mae: 5770.7100 - val_loss: 5867.6907 - val_mse: 45615456.0000 - val_mae: 5867.6904\n",
            "Epoch 60/500\n",
            "50/50 [==============================] - 0s 970us/step - loss: 5769.0129 - mse: 44989588.0000 - mae: 5769.0132 - val_loss: 5848.1543 - val_mse: 45363608.0000 - val_mae: 5848.1538\n",
            "Epoch 61/500\n",
            "50/50 [==============================] - 0s 850us/step - loss: 5739.1214 - mse: 44772864.0000 - mae: 5739.1211 - val_loss: 5827.9229 - val_mse: 45089376.0000 - val_mae: 5827.9229\n",
            "Epoch 62/500\n",
            "50/50 [==============================] - 0s 918us/step - loss: 5770.0391 - mse: 44626728.0000 - mae: 5770.0386 - val_loss: 5810.5673 - val_mse: 44854740.0000 - val_mae: 5810.5674\n",
            "Epoch 63/500\n",
            "50/50 [==============================] - 0s 884us/step - loss: 5693.4685 - mse: 43996676.0000 - mae: 5693.4683 - val_loss: 5790.4335 - val_mse: 44583392.0000 - val_mae: 5790.4331\n",
            "Epoch 64/500\n",
            "50/50 [==============================] - 0s 844us/step - loss: 5737.3383 - mse: 44394608.0000 - mae: 5737.3379 - val_loss: 5771.9444 - val_mse: 44335252.0000 - val_mae: 5771.9443\n",
            "Epoch 65/500\n",
            "50/50 [==============================] - 0s 820us/step - loss: 5725.7543 - mse: 44151900.0000 - mae: 5725.7544 - val_loss: 5753.3852 - val_mse: 44087276.0000 - val_mae: 5753.3853\n",
            "Epoch 66/500\n",
            "50/50 [==============================] - 0s 908us/step - loss: 5746.7912 - mse: 44034520.0000 - mae: 5746.7910 - val_loss: 5736.2597 - val_mse: 43851224.0000 - val_mae: 5736.2593\n",
            "Epoch 67/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5672.4041 - mse: 43089100.0000 - mae: 5672.4038 - val_loss: 5717.4459 - val_mse: 43585472.0000 - val_mae: 5717.4463\n",
            "Epoch 68/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5691.7256 - mse: 43409488.0000 - mae: 5691.7251 - val_loss: 5699.2669 - val_mse: 43329396.0000 - val_mae: 5699.2671\n",
            "Epoch 69/500\n",
            "50/50 [==============================] - 0s 933us/step - loss: 5679.7334 - mse: 43590936.0000 - mae: 5679.7329 - val_loss: 5680.9265 - val_mse: 43072580.0000 - val_mae: 5680.9268\n",
            "Epoch 70/500\n",
            "50/50 [==============================] - 0s 935us/step - loss: 5598.1401 - mse: 42174160.0000 - mae: 5598.1401 - val_loss: 5660.0064 - val_mse: 42781024.0000 - val_mae: 5660.0063\n",
            "Epoch 71/500\n",
            "50/50 [==============================] - 0s 928us/step - loss: 5595.8618 - mse: 41885736.0000 - mae: 5595.8618 - val_loss: 5639.8711 - val_mse: 42490400.0000 - val_mae: 5639.8711\n",
            "Epoch 72/500\n",
            "50/50 [==============================] - 0s 837us/step - loss: 5557.2173 - mse: 41192468.0000 - mae: 5557.2173 - val_loss: 5619.3915 - val_mse: 42187676.0000 - val_mae: 5619.3916\n",
            "Epoch 73/500\n",
            "50/50 [==============================] - 0s 870us/step - loss: 5612.9684 - mse: 42143704.0000 - mae: 5612.9683 - val_loss: 5600.9053 - val_mse: 41915732.0000 - val_mae: 5600.9053\n",
            "Epoch 74/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5583.5540 - mse: 40762864.0000 - mae: 5583.5537 - val_loss: 5581.5310 - val_mse: 41631832.0000 - val_mae: 5581.5312\n",
            "Epoch 75/500\n",
            "50/50 [==============================] - 0s 895us/step - loss: 5604.8961 - mse: 41325840.0000 - mae: 5604.8965 - val_loss: 5563.1527 - val_mse: 41363892.0000 - val_mae: 5563.1528\n",
            "Epoch 76/500\n",
            "50/50 [==============================] - 0s 953us/step - loss: 5538.4797 - mse: 40523688.0000 - mae: 5538.4800 - val_loss: 5543.2849 - val_mse: 41068948.0000 - val_mae: 5543.2852\n",
            "Epoch 77/500\n",
            "50/50 [==============================] - 0s 881us/step - loss: 5493.9712 - mse: 39881576.0000 - mae: 5493.9712 - val_loss: 5523.5282 - val_mse: 40762568.0000 - val_mae: 5523.5283\n",
            "Epoch 78/500\n",
            "50/50 [==============================] - 0s 887us/step - loss: 5479.5816 - mse: 40231320.0000 - mae: 5479.5820 - val_loss: 5503.5293 - val_mse: 40454132.0000 - val_mae: 5503.5293\n",
            "Epoch 79/500\n",
            "50/50 [==============================] - 0s 864us/step - loss: 5521.4195 - mse: 40725264.0000 - mae: 5521.4194 - val_loss: 5484.7538 - val_mse: 40166060.0000 - val_mae: 5484.7534\n",
            "Epoch 80/500\n",
            "50/50 [==============================] - 0s 931us/step - loss: 5443.5064 - mse: 39314800.0000 - mae: 5443.5063 - val_loss: 5463.9320 - val_mse: 39848332.0000 - val_mae: 5463.9321\n",
            "Epoch 81/500\n",
            "50/50 [==============================] - 0s 898us/step - loss: 5440.9249 - mse: 38803280.0000 - mae: 5440.9248 - val_loss: 5443.4884 - val_mse: 39538156.0000 - val_mae: 5443.4883\n",
            "Epoch 82/500\n",
            "50/50 [==============================] - 0s 971us/step - loss: 5444.6423 - mse: 39179904.0000 - mae: 5444.6426 - val_loss: 5423.2705 - val_mse: 39233148.0000 - val_mae: 5423.2705\n",
            "Epoch 83/500\n",
            "50/50 [==============================] - 0s 909us/step - loss: 5461.2175 - mse: 39421748.0000 - mae: 5461.2173 - val_loss: 5404.6616 - val_mse: 38937144.0000 - val_mae: 5404.6616\n",
            "Epoch 84/500\n",
            "50/50 [==============================] - 0s 940us/step - loss: 5298.2619 - mse: 37230140.0000 - mae: 5298.2617 - val_loss: 5382.5878 - val_mse: 38581476.0000 - val_mae: 5382.5874\n",
            "Epoch 85/500\n",
            "50/50 [==============================] - 0s 881us/step - loss: 5407.0124 - mse: 38667136.0000 - mae: 5407.0127 - val_loss: 5363.1993 - val_mse: 38271116.0000 - val_mae: 5363.1992\n",
            "Epoch 86/500\n",
            "50/50 [==============================] - 0s 925us/step - loss: 5406.3396 - mse: 38731056.0000 - mae: 5406.3394 - val_loss: 5343.9343 - val_mse: 37964552.0000 - val_mae: 5343.9346\n",
            "Epoch 87/500\n",
            "50/50 [==============================] - 0s 904us/step - loss: 5283.5382 - mse: 36937128.0000 - mae: 5283.5381 - val_loss: 5322.1122 - val_mse: 37619448.0000 - val_mae: 5322.1123\n",
            "Epoch 88/500\n",
            "50/50 [==============================] - 0s 986us/step - loss: 5335.1393 - mse: 37584892.0000 - mae: 5335.1392 - val_loss: 5301.6255 - val_mse: 37297652.0000 - val_mae: 5301.6260\n",
            "Epoch 89/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5332.5500 - mse: 37148212.0000 - mae: 5332.5498 - val_loss: 5281.3138 - val_mse: 36979688.0000 - val_mae: 5281.3140\n",
            "Epoch 90/500\n",
            "50/50 [==============================] - 0s 935us/step - loss: 5336.8233 - mse: 37550540.0000 - mae: 5336.8232 - val_loss: 5262.8578 - val_mse: 36668532.0000 - val_mae: 5262.8579\n",
            "Epoch 91/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5277.0530 - mse: 36479656.0000 - mae: 5277.0532 - val_loss: 5243.3315 - val_mse: 36341356.0000 - val_mae: 5243.3311\n",
            "Epoch 92/500\n",
            "50/50 [==============================] - 0s 942us/step - loss: 5220.4112 - mse: 35461872.0000 - mae: 5220.4111 - val_loss: 5222.7631 - val_mse: 35999192.0000 - val_mae: 5222.7627\n",
            "Epoch 93/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5087.4267 - mse: 33976692.0000 - mae: 5087.4268 - val_loss: 5199.7632 - val_mse: 35619376.0000 - val_mae: 5199.7632\n",
            "Epoch 94/500\n",
            "50/50 [==============================] - 0s 863us/step - loss: 5337.6018 - mse: 36619040.0000 - mae: 5337.6021 - val_loss: 5181.8992 - val_mse: 35326748.0000 - val_mae: 5181.8994\n",
            "Epoch 95/500\n",
            "50/50 [==============================] - 0s 992us/step - loss: 5237.1680 - mse: 35620028.0000 - mae: 5237.1680 - val_loss: 5162.1611 - val_mse: 35005512.0000 - val_mae: 5162.1611\n",
            "Epoch 96/500\n",
            "50/50 [==============================] - 0s 930us/step - loss: 5115.9152 - mse: 34524616.0000 - mae: 5115.9155 - val_loss: 5140.0938 - val_mse: 34648936.0000 - val_mae: 5140.0938\n",
            "Epoch 97/500\n",
            "50/50 [==============================] - 0s 892us/step - loss: 5064.9798 - mse: 33986104.0000 - mae: 5064.9795 - val_loss: 5117.8183 - val_mse: 34281440.0000 - val_mae: 5117.8184\n",
            "Epoch 98/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5000.7750 - mse: 32797736.0000 - mae: 5000.7749 - val_loss: 5097.6877 - val_mse: 33931172.0000 - val_mae: 5097.6880\n",
            "Epoch 99/500\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 5079.7876 - mse: 33222290.0000 - mae: 5079.7876 - val_loss: 5079.2052 - val_mse: 33612184.0000 - val_mae: 5079.2051\n",
            "Epoch 100/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5140.7837 - mse: 34339628.0000 - mae: 5140.7837 - val_loss: 5060.1631 - val_mse: 33285772.0000 - val_mae: 5060.1631\n",
            "Epoch 101/500\n",
            "50/50 [==============================] - 0s 991us/step - loss: 5157.3295 - mse: 34727556.0000 - mae: 5157.3296 - val_loss: 5041.6043 - val_mse: 32970144.0000 - val_mae: 5041.6045\n",
            "Epoch 102/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5266.1547 - mse: 35626700.0000 - mae: 5266.1543 - val_loss: 5026.7395 - val_mse: 32719860.0000 - val_mae: 5026.7393\n",
            "Epoch 103/500\n",
            "50/50 [==============================] - 0s 997us/step - loss: 4992.0762 - mse: 32972032.0000 - mae: 4992.0762 - val_loss: 5007.5823 - val_mse: 32398640.0000 - val_mae: 5007.5825\n",
            "Epoch 104/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4995.3532 - mse: 31680778.0000 - mae: 4995.3535 - val_loss: 4988.6017 - val_mse: 32076506.0000 - val_mae: 4988.6016\n",
            "Epoch 105/500\n",
            "50/50 [==============================] - 0s 868us/step - loss: 5042.7970 - mse: 32926910.0000 - mae: 5042.7974 - val_loss: 4971.7866 - val_mse: 31770522.0000 - val_mae: 4971.7866\n",
            "Epoch 106/500\n",
            "50/50 [==============================] - 0s 938us/step - loss: 5002.0921 - mse: 32397580.0000 - mae: 5002.0923 - val_loss: 4952.9489 - val_mse: 31430170.0000 - val_mae: 4952.9487\n",
            "Epoch 107/500\n",
            "50/50 [==============================] - 0s 915us/step - loss: 4913.4700 - mse: 30919186.0000 - mae: 4913.4702 - val_loss: 4932.9590 - val_mse: 31072026.0000 - val_mae: 4932.9590\n",
            "Epoch 108/500\n",
            "50/50 [==============================] - 0s 959us/step - loss: 4884.9504 - mse: 31315632.0000 - mae: 4884.9502 - val_loss: 4912.7060 - val_mse: 30712676.0000 - val_mae: 4912.7061\n",
            "Epoch 109/500\n",
            "50/50 [==============================] - 0s 952us/step - loss: 5012.7706 - mse: 31754910.0000 - mae: 5012.7705 - val_loss: 4895.9279 - val_mse: 30418144.0000 - val_mae: 4895.9282\n",
            "Epoch 110/500\n",
            "50/50 [==============================] - 0s 897us/step - loss: 4631.8042 - mse: 27545196.0000 - mae: 4631.8042 - val_loss: 4873.5249 - val_mse: 30027692.0000 - val_mae: 4873.5249\n",
            "Epoch 111/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4922.3353 - mse: 31206692.0000 - mae: 4922.3350 - val_loss: 4853.9769 - val_mse: 29690604.0000 - val_mae: 4853.9771\n",
            "Epoch 112/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4889.9376 - mse: 30671482.0000 - mae: 4889.9375 - val_loss: 4835.5245 - val_mse: 29375500.0000 - val_mae: 4835.5244\n",
            "Epoch 113/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4745.3835 - mse: 29051340.0000 - mae: 4745.3833 - val_loss: 4815.2363 - val_mse: 29032224.0000 - val_mae: 4815.2363\n",
            "Epoch 114/500\n",
            "50/50 [==============================] - 0s 941us/step - loss: 4885.1635 - mse: 30393232.0000 - mae: 4885.1636 - val_loss: 4798.5630 - val_mse: 28752832.0000 - val_mae: 4798.5630\n",
            "Epoch 115/500\n",
            "50/50 [==============================] - 0s 980us/step - loss: 4876.9268 - mse: 30648274.0000 - mae: 4876.9268 - val_loss: 4780.6656 - val_mse: 28455860.0000 - val_mae: 4780.6655\n",
            "Epoch 116/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4843.5897 - mse: 30017308.0000 - mae: 4843.5898 - val_loss: 4761.8701 - val_mse: 28146662.0000 - val_mae: 4761.8701\n",
            "Epoch 117/500\n",
            "50/50 [==============================] - 0s 987us/step - loss: 4833.4361 - mse: 29767014.0000 - mae: 4833.4360 - val_loss: 4743.1150 - val_mse: 27840822.0000 - val_mae: 4743.1147\n",
            "Epoch 118/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4758.9440 - mse: 28895938.0000 - mae: 4758.9438 - val_loss: 4724.8848 - val_mse: 27546496.0000 - val_mae: 4724.8848\n",
            "Epoch 119/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4604.0863 - mse: 27577182.0000 - mae: 4604.0864 - val_loss: 4705.1030 - val_mse: 27230154.0000 - val_mae: 4705.1030\n",
            "Epoch 120/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4562.4256 - mse: 26659420.0000 - mae: 4562.4253 - val_loss: 4682.6060 - val_mse: 26874070.0000 - val_mae: 4682.6064\n",
            "Epoch 121/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4577.4597 - mse: 26631772.0000 - mae: 4577.4595 - val_loss: 4658.9037 - val_mse: 26503486.0000 - val_mae: 4658.9038\n",
            "Epoch 122/500\n",
            "50/50 [==============================] - 0s 871us/step - loss: 4666.9715 - mse: 27275244.0000 - mae: 4666.9712 - val_loss: 4639.7626 - val_mse: 26207990.0000 - val_mae: 4639.7627\n",
            "Epoch 123/500\n",
            "50/50 [==============================] - 0s 865us/step - loss: 4739.5933 - mse: 28128950.0000 - mae: 4739.5933 - val_loss: 4621.5596 - val_mse: 25929804.0000 - val_mae: 4621.5596\n",
            "Epoch 124/500\n",
            "50/50 [==============================] - 0s 906us/step - loss: 4675.7141 - mse: 27315482.0000 - mae: 4675.7139 - val_loss: 4601.1639 - val_mse: 25620940.0000 - val_mae: 4601.1636\n",
            "Epoch 125/500\n",
            "50/50 [==============================] - 0s 980us/step - loss: 4559.8158 - mse: 25750436.0000 - mae: 4559.8154 - val_loss: 4582.5640 - val_mse: 25342406.0000 - val_mae: 4582.5640\n",
            "Epoch 126/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4467.8801 - mse: 25099218.0000 - mae: 4467.8804 - val_loss: 4561.4850 - val_mse: 25030188.0000 - val_mae: 4561.4854\n",
            "Epoch 127/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4573.7310 - mse: 26949304.0000 - mae: 4573.7310 - val_loss: 4540.2988 - val_mse: 24720158.0000 - val_mae: 4540.2988\n",
            "Epoch 128/500\n",
            "50/50 [==============================] - 0s 914us/step - loss: 4467.3491 - mse: 25422356.0000 - mae: 4467.3496 - val_loss: 4521.3745 - val_mse: 24446676.0000 - val_mae: 4521.3745\n",
            "Epoch 129/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4442.6262 - mse: 24787304.0000 - mae: 4442.6265 - val_loss: 4501.4951 - val_mse: 24162208.0000 - val_mae: 4501.4951\n",
            "Epoch 130/500\n",
            "50/50 [==============================] - 0s 948us/step - loss: 4437.0629 - mse: 24994420.0000 - mae: 4437.0630 - val_loss: 4482.8059 - val_mse: 23898010.0000 - val_mae: 4482.8057\n",
            "Epoch 131/500\n",
            "50/50 [==============================] - 0s 948us/step - loss: 4630.0018 - mse: 26443970.0000 - mae: 4630.0020 - val_loss: 4463.8029 - val_mse: 23632068.0000 - val_mae: 4463.8027\n",
            "Epoch 132/500\n",
            "50/50 [==============================] - 0s 975us/step - loss: 4437.6382 - mse: 24565230.0000 - mae: 4437.6382 - val_loss: 4442.0720 - val_mse: 23331716.0000 - val_mae: 4442.0718\n",
            "Epoch 133/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4517.6362 - mse: 25911832.0000 - mae: 4517.6362 - val_loss: 4423.7780 - val_mse: 23082042.0000 - val_mae: 4423.7783\n",
            "Epoch 134/500\n",
            "50/50 [==============================] - 0s 951us/step - loss: 4566.6453 - mse: 25826288.0000 - mae: 4566.6455 - val_loss: 4407.9868 - val_mse: 22869076.0000 - val_mae: 4407.9868\n",
            "Epoch 135/500\n",
            "50/50 [==============================] - 0s 899us/step - loss: 4525.6276 - mse: 25981356.0000 - mae: 4525.6274 - val_loss: 4391.9010 - val_mse: 22654118.0000 - val_mae: 4391.9009\n",
            "Epoch 136/500\n",
            "50/50 [==============================] - 0s 946us/step - loss: 4490.4589 - mse: 25234286.0000 - mae: 4490.4590 - val_loss: 4373.1411 - val_mse: 22405574.0000 - val_mae: 4373.1411\n",
            "Epoch 137/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4538.0705 - mse: 24629242.0000 - mae: 4538.0703 - val_loss: 4353.5792 - val_mse: 22149554.0000 - val_mae: 4353.5791\n",
            "Epoch 138/500\n",
            "50/50 [==============================] - 0s 902us/step - loss: 4235.1382 - mse: 21930554.0000 - mae: 4235.1382 - val_loss: 4333.5292 - val_mse: 21890508.0000 - val_mae: 4333.5288\n",
            "Epoch 139/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4272.6556 - mse: 22702940.0000 - mae: 4272.6558 - val_loss: 4313.0809 - val_mse: 21629680.0000 - val_mae: 4313.0811\n",
            "Epoch 140/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4362.3623 - mse: 23523106.0000 - mae: 4362.3623 - val_loss: 4300.4030 - val_mse: 21470226.0000 - val_mae: 4300.4033\n",
            "Epoch 141/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4099.6037 - mse: 21012360.0000 - mae: 4099.6035 - val_loss: 4276.7357 - val_mse: 21174932.0000 - val_mae: 4276.7358\n",
            "Epoch 142/500\n",
            "50/50 [==============================] - 0s 999us/step - loss: 4274.9695 - mse: 23449560.0000 - mae: 4274.9697 - val_loss: 4258.0663 - val_mse: 20945264.0000 - val_mae: 4258.0664\n",
            "Epoch 143/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4066.3434 - mse: 20293564.0000 - mae: 4066.3438 - val_loss: 4236.3765 - val_mse: 20682110.0000 - val_mae: 4236.3765\n",
            "Epoch 144/500\n",
            "50/50 [==============================] - 0s 919us/step - loss: 4348.8127 - mse: 23592510.0000 - mae: 4348.8125 - val_loss: 4222.5398 - val_mse: 20516436.0000 - val_mae: 4222.5400\n",
            "Epoch 145/500\n",
            "50/50 [==============================] - 0s 890us/step - loss: 4522.4058 - mse: 24276614.0000 - mae: 4522.4058 - val_loss: 4210.5859 - val_mse: 20374722.0000 - val_mae: 4210.5859\n",
            "Epoch 146/500\n",
            "50/50 [==============================] - 0s 948us/step - loss: 4279.2570 - mse: 22294902.0000 - mae: 4279.2568 - val_loss: 4191.9215 - val_mse: 20155244.0000 - val_mae: 4191.9219\n",
            "Epoch 147/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4128.1489 - mse: 21226582.0000 - mae: 4128.1489 - val_loss: 4170.9772 - val_mse: 19912266.0000 - val_mae: 4170.9775\n",
            "Epoch 148/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4214.5559 - mse: 22332480.0000 - mae: 4214.5557 - val_loss: 4151.2786 - val_mse: 19686970.0000 - val_mae: 4151.2783\n",
            "Epoch 149/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4066.2164 - mse: 20447668.0000 - mae: 4066.2163 - val_loss: 4128.0718 - val_mse: 19425532.0000 - val_mae: 4128.0718\n",
            "Epoch 150/500\n",
            "50/50 [==============================] - 0s 812us/step - loss: 4314.6356 - mse: 22123988.0000 - mae: 4314.6357 - val_loss: 4109.8115 - val_mse: 19223022.0000 - val_mae: 4109.8115\n",
            "Epoch 151/500\n",
            "50/50 [==============================] - 0s 884us/step - loss: 4022.5229 - mse: 20357820.0000 - mae: 4022.5227 - val_loss: 4093.9993 - val_mse: 19049930.0000 - val_mae: 4093.9993\n",
            "Epoch 152/500\n",
            "50/50 [==============================] - 0s 872us/step - loss: 4280.2843 - mse: 22026348.0000 - mae: 4280.2842 - val_loss: 4077.2737 - val_mse: 18868958.0000 - val_mae: 4077.2734\n",
            "Epoch 153/500\n",
            "50/50 [==============================] - 0s 892us/step - loss: 3954.8529 - mse: 19425590.0000 - mae: 3954.8528 - val_loss: 4059.5176 - val_mse: 18679248.0000 - val_mae: 4059.5171\n",
            "Epoch 154/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4358.5171 - mse: 23232540.0000 - mae: 4358.5171 - val_loss: 4042.1380 - val_mse: 18495984.0000 - val_mae: 4042.1382\n",
            "Epoch 155/500\n",
            "50/50 [==============================] - 0s 908us/step - loss: 4214.3562 - mse: 22077104.0000 - mae: 4214.3564 - val_loss: 4025.0807 - val_mse: 18318532.0000 - val_mae: 4025.0806\n",
            "Epoch 156/500\n",
            "50/50 [==============================] - 0s 956us/step - loss: 4126.2844 - mse: 21098648.0000 - mae: 4126.2842 - val_loss: 4009.6616 - val_mse: 18160124.0000 - val_mae: 4009.6616\n",
            "Epoch 157/500\n",
            "50/50 [==============================] - 0s 984us/step - loss: 3842.1528 - mse: 18211162.0000 - mae: 3842.1526 - val_loss: 3985.6739 - val_mse: 17917516.0000 - val_mae: 3985.6738\n",
            "Epoch 158/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4115.1821 - mse: 20828920.0000 - mae: 4115.1821 - val_loss: 3963.4731 - val_mse: 17697160.0000 - val_mae: 3963.4731\n",
            "Epoch 159/500\n",
            "50/50 [==============================] - 0s 961us/step - loss: 4037.0348 - mse: 20019032.0000 - mae: 4037.0347 - val_loss: 3946.8790 - val_mse: 17534994.0000 - val_mae: 3946.8789\n",
            "Epoch 160/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4009.9363 - mse: 19594800.0000 - mae: 4009.9360 - val_loss: 3929.7559 - val_mse: 17370010.0000 - val_mae: 3929.7559\n",
            "Epoch 161/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4027.2552 - mse: 19101352.0000 - mae: 4027.2549 - val_loss: 3912.5853 - val_mse: 17206924.0000 - val_mae: 3912.5852\n",
            "Epoch 162/500\n",
            "50/50 [==============================] - 0s 911us/step - loss: 4084.1064 - mse: 19630948.0000 - mae: 4084.1062 - val_loss: 3898.9553 - val_mse: 17079090.0000 - val_mae: 3898.9556\n",
            "Epoch 163/500\n",
            "50/50 [==============================] - 0s 876us/step - loss: 4097.9833 - mse: 20703714.0000 - mae: 4097.9829 - val_loss: 3886.1548 - val_mse: 16960296.0000 - val_mae: 3886.1548\n",
            "Epoch 164/500\n",
            "50/50 [==============================] - 0s 949us/step - loss: 4006.0402 - mse: 20304420.0000 - mae: 4006.0403 - val_loss: 3864.7916 - val_mse: 16765347.0000 - val_mae: 3864.7917\n",
            "Epoch 165/500\n",
            "50/50 [==============================] - 0s 945us/step - loss: 3971.0070 - mse: 19364674.0000 - mae: 3971.0068 - val_loss: 3849.4720 - val_mse: 16627661.0000 - val_mae: 3849.4719\n",
            "Epoch 166/500\n",
            "50/50 [==============================] - 0s 874us/step - loss: 3912.0729 - mse: 19018266.0000 - mae: 3912.0732 - val_loss: 3829.1724 - val_mse: 16448256.0000 - val_mae: 3829.1726\n",
            "Epoch 167/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3830.1437 - mse: 16889902.0000 - mae: 3830.1436 - val_loss: 3809.7184 - val_mse: 16279354.0000 - val_mae: 3809.7183\n",
            "Epoch 168/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4181.0521 - mse: 21144238.0000 - mae: 4181.0522 - val_loss: 3793.1617 - val_mse: 16138058.0000 - val_mae: 3793.1616\n",
            "Epoch 169/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3923.4748 - mse: 18452420.0000 - mae: 3923.4746 - val_loss: 3772.9269 - val_mse: 15968310.0000 - val_mae: 3772.9270\n",
            "Epoch 170/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3710.5248 - mse: 17374024.0000 - mae: 3710.5247 - val_loss: 3752.6371 - val_mse: 15801573.0000 - val_mae: 3752.6372\n",
            "Epoch 171/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3784.4555 - mse: 18225874.0000 - mae: 3784.4553 - val_loss: 3736.1871 - val_mse: 15668650.0000 - val_mae: 3736.1870\n",
            "Epoch 172/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4013.4106 - mse: 19892856.0000 - mae: 4013.4106 - val_loss: 3725.7008 - val_mse: 15584634.0000 - val_mae: 3725.7007\n",
            "Epoch 173/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3845.7416 - mse: 18615808.0000 - mae: 3845.7415 - val_loss: 3705.1900 - val_mse: 15423795.0000 - val_mae: 3705.1899\n",
            "Epoch 174/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3970.6565 - mse: 18825104.0000 - mae: 3970.6565 - val_loss: 3688.0784 - val_mse: 15292056.0000 - val_mae: 3688.0786\n",
            "Epoch 175/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4066.6189 - mse: 20273734.0000 - mae: 4066.6187 - val_loss: 3678.9512 - val_mse: 15222440.0000 - val_mae: 3678.9512\n",
            "Epoch 176/500\n",
            "50/50 [==============================] - 0s 894us/step - loss: 3866.5881 - mse: 18410974.0000 - mae: 3866.5881 - val_loss: 3663.1854 - val_mse: 15104317.0000 - val_mae: 3663.1851\n",
            "Epoch 177/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4150.1964 - mse: 20702918.0000 - mae: 4150.1968 - val_loss: 3646.2323 - val_mse: 14979658.0000 - val_mae: 3646.2319\n",
            "Epoch 178/500\n",
            "50/50 [==============================] - 0s 941us/step - loss: 3869.5164 - mse: 18437824.0000 - mae: 3869.5164 - val_loss: 3630.5669 - val_mse: 14866485.0000 - val_mae: 3630.5671\n",
            "Epoch 179/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3893.3710 - mse: 18332166.0000 - mae: 3893.3713 - val_loss: 3614.9691 - val_mse: 14755566.0000 - val_mae: 3614.9692\n",
            "Epoch 180/500\n",
            "50/50 [==============================] - 0s 958us/step - loss: 3658.7055 - mse: 16181896.0000 - mae: 3658.7056 - val_loss: 3599.6026 - val_mse: 14648208.0000 - val_mae: 3599.6028\n",
            "Epoch 181/500\n",
            "50/50 [==============================] - 0s 941us/step - loss: 3715.5538 - mse: 16754829.0000 - mae: 3715.5540 - val_loss: 3584.3699 - val_mse: 14543510.0000 - val_mae: 3584.3699\n",
            "Epoch 182/500\n",
            "50/50 [==============================] - 0s 877us/step - loss: 3973.9970 - mse: 20125836.0000 - mae: 3973.9968 - val_loss: 3571.8395 - val_mse: 14458352.0000 - val_mae: 3571.8394\n",
            "Epoch 183/500\n",
            "50/50 [==============================] - 0s 912us/step - loss: 4084.5101 - mse: 20231908.0000 - mae: 4084.5100 - val_loss: 3562.7664 - val_mse: 14397272.0000 - val_mae: 3562.7664\n",
            "Epoch 184/500\n",
            "50/50 [==============================] - 0s 988us/step - loss: 3829.9379 - mse: 17924766.0000 - mae: 3829.9382 - val_loss: 3547.2021 - val_mse: 14294979.0000 - val_mae: 3547.2024\n",
            "Epoch 185/500\n",
            "50/50 [==============================] - 0s 977us/step - loss: 3661.3488 - mse: 16769037.0000 - mae: 3661.3486 - val_loss: 3531.4781 - val_mse: 14193472.0000 - val_mae: 3531.4780\n",
            "Epoch 186/500\n",
            "50/50 [==============================] - 0s 970us/step - loss: 3731.4023 - mse: 17267520.0000 - mae: 3731.4026 - val_loss: 3510.2055 - val_mse: 14060090.0000 - val_mae: 3510.2056\n",
            "Epoch 187/500\n",
            "50/50 [==============================] - 0s 980us/step - loss: 3434.4734 - mse: 15887846.0000 - mae: 3434.4731 - val_loss: 3493.9294 - val_mse: 13959835.0000 - val_mae: 3493.9292\n",
            "Epoch 188/500\n",
            "50/50 [==============================] - 0s 893us/step - loss: 4053.1355 - mse: 20286818.0000 - mae: 4053.1357 - val_loss: 3484.4873 - val_mse: 13902218.0000 - val_mae: 3484.4875\n",
            "Epoch 189/500\n",
            "50/50 [==============================] - 0s 889us/step - loss: 3685.6324 - mse: 17049730.0000 - mae: 3685.6326 - val_loss: 3471.3784 - val_mse: 13824062.0000 - val_mae: 3471.3782\n",
            "Epoch 190/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4092.8639 - mse: 20618316.0000 - mae: 4092.8640 - val_loss: 3458.0294 - val_mse: 13745656.0000 - val_mae: 3458.0293\n",
            "Epoch 191/500\n",
            "50/50 [==============================] - 0s 918us/step - loss: 3724.1769 - mse: 16755238.0000 - mae: 3724.1772 - val_loss: 3447.2209 - val_mse: 13682813.0000 - val_mae: 3447.2212\n",
            "Epoch 192/500\n",
            "50/50 [==============================] - 0s 863us/step - loss: 3800.8930 - mse: 17803524.0000 - mae: 3800.8928 - val_loss: 3435.5932 - val_mse: 13616621.0000 - val_mae: 3435.5933\n",
            "Epoch 193/500\n",
            "50/50 [==============================] - 0s 954us/step - loss: 3900.6287 - mse: 18727564.0000 - mae: 3900.6287 - val_loss: 3422.3952 - val_mse: 13543061.0000 - val_mae: 3422.3953\n",
            "Epoch 194/500\n",
            "50/50 [==============================] - 0s 922us/step - loss: 3680.4071 - mse: 16572616.0000 - mae: 3680.4072 - val_loss: 3408.9498 - val_mse: 13469459.0000 - val_mae: 3408.9500\n",
            "Epoch 195/500\n",
            "50/50 [==============================] - 0s 890us/step - loss: 3768.9606 - mse: 17686824.0000 - mae: 3768.9607 - val_loss: 3392.5038 - val_mse: 13381506.0000 - val_mae: 3392.5039\n",
            "Epoch 196/500\n",
            "50/50 [==============================] - 0s 922us/step - loss: 3755.5323 - mse: 18128020.0000 - mae: 3755.5325 - val_loss: 3378.5266 - val_mse: 13308360.0000 - val_mae: 3378.5266\n",
            "Epoch 197/500\n",
            "50/50 [==============================] - 0s 904us/step - loss: 3656.6920 - mse: 17369978.0000 - mae: 3656.6919 - val_loss: 3364.3687 - val_mse: 13235790.0000 - val_mae: 3364.3687\n",
            "Epoch 198/500\n",
            "50/50 [==============================] - 0s 945us/step - loss: 3655.0867 - mse: 15593384.0000 - mae: 3655.0869 - val_loss: 3349.0884 - val_mse: 13159454.0000 - val_mae: 3349.0884\n",
            "Epoch 199/500\n",
            "50/50 [==============================] - 0s 932us/step - loss: 3701.5709 - mse: 16565897.0000 - mae: 3701.5713 - val_loss: 3332.1883 - val_mse: 13074834.0000 - val_mae: 3332.1882\n",
            "Epoch 200/500\n",
            "50/50 [==============================] - 0s 942us/step - loss: 3908.5185 - mse: 19006604.0000 - mae: 3908.5186 - val_loss: 3324.5542 - val_mse: 13033947.0000 - val_mae: 3324.5540\n",
            "Epoch 201/500\n",
            "50/50 [==============================] - 0s 928us/step - loss: 3334.3313 - mse: 13640324.0000 - mae: 3334.3313 - val_loss: 3307.4299 - val_mse: 12945904.0000 - val_mae: 3307.4297\n",
            "Epoch 202/500\n",
            "50/50 [==============================] - 0s 914us/step - loss: 3700.4090 - mse: 16541524.0000 - mae: 3700.4092 - val_loss: 3293.9337 - val_mse: 12878246.0000 - val_mae: 3293.9336\n",
            "Epoch 203/500\n",
            "50/50 [==============================] - 0s 994us/step - loss: 3643.0582 - mse: 16962634.0000 - mae: 3643.0581 - val_loss: 3278.4251 - val_mse: 12802809.0000 - val_mae: 3278.4250\n",
            "Epoch 204/500\n",
            "50/50 [==============================] - 0s 969us/step - loss: 3670.8570 - mse: 17336688.0000 - mae: 3670.8569 - val_loss: 3270.1429 - val_mse: 12760906.0000 - val_mae: 3270.1431\n",
            "Epoch 205/500\n",
            "50/50 [==============================] - 0s 973us/step - loss: 3486.4785 - mse: 15830977.0000 - mae: 3486.4785 - val_loss: 3260.2458 - val_mse: 12705986.0000 - val_mae: 3260.2456\n",
            "Epoch 206/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3838.5446 - mse: 18094886.0000 - mae: 3838.5447 - val_loss: 3251.9985 - val_mse: 12657610.0000 - val_mae: 3251.9985\n",
            "Epoch 207/500\n",
            "50/50 [==============================] - 0s 979us/step - loss: 3737.9729 - mse: 16862786.0000 - mae: 3737.9731 - val_loss: 3245.9355 - val_mse: 12622528.0000 - val_mae: 3245.9355\n",
            "Epoch 208/500\n",
            "50/50 [==============================] - 0s 924us/step - loss: 3906.6162 - mse: 18638320.0000 - mae: 3906.6162 - val_loss: 3239.8371 - val_mse: 12587783.0000 - val_mae: 3239.8372\n",
            "Epoch 209/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3852.8941 - mse: 18849518.0000 - mae: 3852.8943 - val_loss: 3230.0697 - val_mse: 12533946.0000 - val_mae: 3230.0698\n",
            "Epoch 210/500\n",
            "50/50 [==============================] - 0s 926us/step - loss: 3507.8880 - mse: 15976398.0000 - mae: 3507.8877 - val_loss: 3218.4114 - val_mse: 12472029.0000 - val_mae: 3218.4114\n",
            "Epoch 211/500\n",
            "50/50 [==============================] - 0s 921us/step - loss: 3683.2425 - mse: 17982424.0000 - mae: 3683.2422 - val_loss: 3207.7914 - val_mse: 12417290.0000 - val_mae: 3207.7915\n",
            "Epoch 212/500\n",
            "50/50 [==============================] - 0s 986us/step - loss: 3698.8616 - mse: 17935032.0000 - mae: 3698.8616 - val_loss: 3198.9491 - val_mse: 12373048.0000 - val_mae: 3198.9490\n",
            "Epoch 213/500\n",
            "50/50 [==============================] - 0s 936us/step - loss: 3593.8280 - mse: 16759049.0000 - mae: 3593.8281 - val_loss: 3188.1010 - val_mse: 12320860.0000 - val_mae: 3188.1011\n",
            "Epoch 214/500\n",
            "50/50 [==============================] - 0s 978us/step - loss: 3733.6552 - mse: 17909368.0000 - mae: 3733.6553 - val_loss: 3179.6440 - val_mse: 12280982.0000 - val_mae: 3179.6440\n",
            "Epoch 215/500\n",
            "50/50 [==============================] - 0s 972us/step - loss: 3974.9273 - mse: 20109636.0000 - mae: 3974.9275 - val_loss: 3175.4987 - val_mse: 12261172.0000 - val_mae: 3175.4988\n",
            "Epoch 216/500\n",
            "50/50 [==============================] - 0s 958us/step - loss: 3702.6046 - mse: 18574836.0000 - mae: 3702.6047 - val_loss: 3173.4596 - val_mse: 12250830.0000 - val_mae: 3173.4595\n",
            "Epoch 217/500\n",
            "50/50 [==============================] - 0s 925us/step - loss: 3589.6494 - mse: 15198725.0000 - mae: 3589.6494 - val_loss: 3166.2649 - val_mse: 12215235.0000 - val_mae: 3166.2649\n",
            "Epoch 218/500\n",
            "50/50 [==============================] - 0s 891us/step - loss: 4026.7375 - mse: 21622610.0000 - mae: 4026.7378 - val_loss: 3161.0512 - val_mse: 12188439.0000 - val_mae: 3161.0513\n",
            "Epoch 219/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3684.9615 - mse: 18281562.0000 - mae: 3684.9612 - val_loss: 3155.7318 - val_mse: 12161670.0000 - val_mae: 3155.7319\n",
            "Epoch 220/500\n",
            "50/50 [==============================] - 0s 973us/step - loss: 3562.6604 - mse: 16171181.0000 - mae: 3562.6606 - val_loss: 3148.1158 - val_mse: 12124878.0000 - val_mae: 3148.1157\n",
            "Epoch 221/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3909.1601 - mse: 18294686.0000 - mae: 3909.1599 - val_loss: 3140.8461 - val_mse: 12090955.0000 - val_mae: 3140.8462\n",
            "Epoch 222/500\n",
            "50/50 [==============================] - 0s 960us/step - loss: 3611.7304 - mse: 18685958.0000 - mae: 3611.7307 - val_loss: 3131.8645 - val_mse: 12050733.0000 - val_mae: 3131.8647\n",
            "Epoch 223/500\n",
            "50/50 [==============================] - 0s 929us/step - loss: 3531.2200 - mse: 16819748.0000 - mae: 3531.2200 - val_loss: 3123.5631 - val_mse: 12015115.0000 - val_mae: 3123.5630\n",
            "Epoch 224/500\n",
            "50/50 [==============================] - 0s 967us/step - loss: 3699.9448 - mse: 18587118.0000 - mae: 3699.9451 - val_loss: 3118.1259 - val_mse: 11992069.0000 - val_mae: 3118.1257\n",
            "Epoch 225/500\n",
            "50/50 [==============================] - 0s 943us/step - loss: 3637.8043 - mse: 17325872.0000 - mae: 3637.8044 - val_loss: 3109.4546 - val_mse: 11957665.0000 - val_mae: 3109.4546\n",
            "Epoch 226/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3645.6608 - mse: 17740062.0000 - mae: 3645.6606 - val_loss: 3102.0795 - val_mse: 11929483.0000 - val_mae: 3102.0796\n",
            "Epoch 227/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3584.9942 - mse: 17209312.0000 - mae: 3584.9944 - val_loss: 3098.7155 - val_mse: 11915126.0000 - val_mae: 3098.7153\n",
            "Epoch 228/500\n",
            "50/50 [==============================] - 0s 996us/step - loss: 3521.1018 - mse: 15871461.0000 - mae: 3521.1018 - val_loss: 3095.2761 - val_mse: 11899994.0000 - val_mae: 3095.2759\n",
            "Epoch 229/500\n",
            "50/50 [==============================] - 0s 942us/step - loss: 3452.6023 - mse: 16175782.0000 - mae: 3452.6025 - val_loss: 3092.4095 - val_mse: 11887242.0000 - val_mae: 3092.4094\n",
            "Epoch 230/500\n",
            "50/50 [==============================] - 0s 934us/step - loss: 3741.0491 - mse: 19141154.0000 - mae: 3741.0491 - val_loss: 3089.8333 - val_mse: 11876251.0000 - val_mae: 3089.8335\n",
            "Epoch 231/500\n",
            "50/50 [==============================] - 0s 896us/step - loss: 3608.4047 - mse: 17074696.0000 - mae: 3608.4048 - val_loss: 3082.1884 - val_mse: 11845982.0000 - val_mae: 3082.1882\n",
            "Epoch 232/500\n",
            "50/50 [==============================] - 0s 963us/step - loss: 3878.4797 - mse: 19616674.0000 - mae: 3878.4797 - val_loss: 3079.7476 - val_mse: 11836184.0000 - val_mae: 3079.7476\n",
            "Epoch 233/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3674.6848 - mse: 17428070.0000 - mae: 3674.6846 - val_loss: 3073.9161 - val_mse: 11814517.0000 - val_mae: 3073.9160\n",
            "Epoch 234/500\n",
            "50/50 [==============================] - 0s 992us/step - loss: 3561.2463 - mse: 17392768.0000 - mae: 3561.2463 - val_loss: 3068.0104 - val_mse: 11793954.0000 - val_mae: 3068.0105\n",
            "Epoch 235/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3491.2159 - mse: 15279976.0000 - mae: 3491.2158 - val_loss: 3062.4918 - val_mse: 11775369.0000 - val_mae: 3062.4917\n",
            "Epoch 236/500\n",
            "50/50 [==============================] - 0s 949us/step - loss: 3369.4938 - mse: 16119863.0000 - mae: 3369.4937 - val_loss: 3058.4948 - val_mse: 11762893.0000 - val_mae: 3058.4946\n",
            "Epoch 237/500\n",
            "50/50 [==============================] - 0s 896us/step - loss: 3486.0815 - mse: 16214301.0000 - mae: 3486.0815 - val_loss: 3055.0291 - val_mse: 11752390.0000 - val_mae: 3055.0291\n",
            "Epoch 238/500\n",
            "50/50 [==============================] - 0s 885us/step - loss: 3807.0875 - mse: 19773172.0000 - mae: 3807.0879 - val_loss: 3051.1677 - val_mse: 11740817.0000 - val_mae: 3051.1675\n",
            "Epoch 239/500\n",
            "50/50 [==============================] - 0s 931us/step - loss: 3772.7344 - mse: 18664144.0000 - mae: 3772.7344 - val_loss: 3050.0966 - val_mse: 11736526.0000 - val_mae: 3050.0964\n",
            "Epoch 240/500\n",
            "50/50 [==============================] - 0s 910us/step - loss: 3620.3101 - mse: 18055158.0000 - mae: 3620.3101 - val_loss: 3047.2036 - val_mse: 11727770.0000 - val_mae: 3047.2036\n",
            "Epoch 241/500\n",
            "50/50 [==============================] - 0s 883us/step - loss: 3569.8996 - mse: 17258278.0000 - mae: 3569.8994 - val_loss: 3044.1167 - val_mse: 11719390.0000 - val_mae: 3044.1167\n",
            "Epoch 242/500\n",
            "50/50 [==============================] - 0s 990us/step - loss: 3472.2104 - mse: 16689736.0000 - mae: 3472.2102 - val_loss: 3040.6855 - val_mse: 11709549.0000 - val_mae: 3040.6855\n",
            "Epoch 243/500\n",
            "50/50 [==============================] - 0s 947us/step - loss: 3404.0671 - mse: 17043992.0000 - mae: 3404.0671 - val_loss: 3039.4583 - val_mse: 11705413.0000 - val_mae: 3039.4585\n",
            "Epoch 244/500\n",
            "50/50 [==============================] - 0s 901us/step - loss: 3726.5248 - mse: 19907138.0000 - mae: 3726.5249 - val_loss: 3035.2393 - val_mse: 11694192.0000 - val_mae: 3035.2393\n",
            "Epoch 245/500\n",
            "50/50 [==============================] - 0s 943us/step - loss: 3371.6249 - mse: 15093622.0000 - mae: 3371.6250 - val_loss: 3035.5802 - val_mse: 11694792.0000 - val_mae: 3035.5801\n",
            "Epoch 246/500\n",
            "50/50 [==============================] - 0s 898us/step - loss: 3301.5637 - mse: 15261788.0000 - mae: 3301.5635 - val_loss: 3031.8721 - val_mse: 11685375.0000 - val_mae: 3031.8723\n",
            "Epoch 247/500\n",
            "50/50 [==============================] - 0s 943us/step - loss: 3359.6248 - mse: 15553539.0000 - mae: 3359.6250 - val_loss: 3024.0889 - val_mse: 11668632.0000 - val_mae: 3024.0889\n",
            "Epoch 248/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3749.5318 - mse: 17204262.0000 - mae: 3749.5320 - val_loss: 3020.9978 - val_mse: 11662005.0000 - val_mae: 3020.9980\n",
            "Epoch 249/500\n",
            "50/50 [==============================] - 0s 959us/step - loss: 3719.5196 - mse: 18893424.0000 - mae: 3719.5198 - val_loss: 3014.3953 - val_mse: 11650298.0000 - val_mae: 3014.3953\n",
            "Epoch 250/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3668.4738 - mse: 17255104.0000 - mae: 3668.4734 - val_loss: 3009.3018 - val_mse: 11642084.0000 - val_mae: 3009.3015\n",
            "Epoch 251/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3771.8789 - mse: 18254684.0000 - mae: 3771.8787 - val_loss: 3006.4673 - val_mse: 11636528.0000 - val_mae: 3006.4673\n",
            "Epoch 252/500\n",
            "50/50 [==============================] - 0s 908us/step - loss: 3460.7191 - mse: 16902242.0000 - mae: 3460.7190 - val_loss: 3005.4028 - val_mse: 11634395.0000 - val_mae: 3005.4028\n",
            "Epoch 253/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3667.8277 - mse: 18641378.0000 - mae: 3667.8274 - val_loss: 3002.7569 - val_mse: 11629919.0000 - val_mae: 3002.7571\n",
            "Epoch 254/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3923.9874 - mse: 19979504.0000 - mae: 3923.9873 - val_loss: 3001.3152 - val_mse: 11627092.0000 - val_mae: 3001.3152\n",
            "Epoch 255/500\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 3411.9368 - mse: 16434816.0000 - mae: 3411.9368 - val_loss: 2995.7622 - val_mse: 11619478.0000 - val_mae: 2995.7622\n",
            "Epoch 256/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3219.2173 - mse: 13466578.0000 - mae: 3219.2173 - val_loss: 2990.4749 - val_mse: 11613635.0000 - val_mae: 2990.4749\n",
            "Epoch 257/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3583.7890 - mse: 18888856.0000 - mae: 3583.7891 - val_loss: 2990.1323 - val_mse: 11612166.0000 - val_mae: 2990.1323\n",
            "Epoch 258/500\n",
            "50/50 [==============================] - 0s 913us/step - loss: 3198.9091 - mse: 14695988.0000 - mae: 3198.9094 - val_loss: 2988.6719 - val_mse: 11609586.0000 - val_mae: 2988.6719\n",
            "Epoch 259/500\n",
            "50/50 [==============================] - 0s 854us/step - loss: 3534.9489 - mse: 16394912.0000 - mae: 3534.9487 - val_loss: 2985.6079 - val_mse: 11606098.0000 - val_mae: 2985.6079\n",
            "Epoch 260/500\n",
            "50/50 [==============================] - 0s 897us/step - loss: 3568.8413 - mse: 20064986.0000 - mae: 3568.8413 - val_loss: 2984.4871 - val_mse: 11604322.0000 - val_mae: 2984.4871\n",
            "Epoch 261/500\n",
            "50/50 [==============================] - 0s 858us/step - loss: 3370.4214 - mse: 14776353.0000 - mae: 3370.4211 - val_loss: 2980.7663 - val_mse: 11600166.0000 - val_mae: 2980.7661\n",
            "Epoch 262/500\n",
            "50/50 [==============================] - 0s 913us/step - loss: 3163.7370 - mse: 14900262.0000 - mae: 3163.7368 - val_loss: 2977.1706 - val_mse: 11597020.0000 - val_mae: 2977.1704\n",
            "Epoch 263/500\n",
            "50/50 [==============================] - 0s 960us/step - loss: 3674.0192 - mse: 17881516.0000 - mae: 3674.0193 - val_loss: 2975.7351 - val_mse: 11595110.0000 - val_mae: 2975.7351\n",
            "Epoch 264/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3489.3355 - mse: 17280424.0000 - mae: 3489.3357 - val_loss: 2977.6820 - val_mse: 11594697.0000 - val_mae: 2977.6821\n",
            "Epoch 265/500\n",
            "50/50 [==============================] - 0s 935us/step - loss: 3388.2580 - mse: 16031291.0000 - mae: 3388.2578 - val_loss: 2972.5815 - val_mse: 11592042.0000 - val_mae: 2972.5815\n",
            "Epoch 266/500\n",
            "50/50 [==============================] - 0s 917us/step - loss: 3819.1191 - mse: 19835268.0000 - mae: 3819.1191 - val_loss: 2976.3600 - val_mse: 11591568.0000 - val_mae: 2976.3599\n",
            "Epoch 267/500\n",
            "50/50 [==============================] - 0s 889us/step - loss: 3372.2870 - mse: 15473134.0000 - mae: 3372.2869 - val_loss: 2973.3792 - val_mse: 11589299.0000 - val_mae: 2973.3794\n",
            "Epoch 268/500\n",
            "50/50 [==============================] - 0s 928us/step - loss: 3064.9578 - mse: 14357432.0000 - mae: 3064.9580 - val_loss: 2973.3341 - val_mse: 11587422.0000 - val_mae: 2973.3340\n",
            "Epoch 269/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3053.4403 - mse: 13549984.0000 - mae: 3053.4404 - val_loss: 2972.3842 - val_mse: 11585442.0000 - val_mae: 2972.3843\n",
            "Epoch 270/500\n",
            "50/50 [==============================] - 0s 866us/step - loss: 3594.0487 - mse: 19138628.0000 - mae: 3594.0488 - val_loss: 2972.0819 - val_mse: 11584407.0000 - val_mae: 2972.0818\n",
            "Epoch 271/500\n",
            "50/50 [==============================] - 0s 980us/step - loss: 3326.6619 - mse: 15793741.0000 - mae: 3326.6619 - val_loss: 2968.2108 - val_mse: 11583443.0000 - val_mae: 2968.2109\n",
            "Epoch 272/500\n",
            "50/50 [==============================] - 0s 855us/step - loss: 3207.3547 - mse: 13250048.0000 - mae: 3207.3550 - val_loss: 2966.6092 - val_mse: 11582175.0000 - val_mae: 2966.6091\n",
            "Epoch 273/500\n",
            "50/50 [==============================] - 0s 996us/step - loss: 3639.3551 - mse: 19765438.0000 - mae: 3639.3552 - val_loss: 2962.6734 - val_mse: 11580883.0000 - val_mae: 2962.6733\n",
            "Epoch 274/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3292.5616 - mse: 15361324.0000 - mae: 3292.5615 - val_loss: 2962.6020 - val_mse: 11580525.0000 - val_mae: 2962.6021\n",
            "Epoch 275/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3380.3900 - mse: 15221376.0000 - mae: 3380.3899 - val_loss: 2961.0579 - val_mse: 11579683.0000 - val_mae: 2961.0579\n",
            "Epoch 276/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3585.6811 - mse: 19823438.0000 - mae: 3585.6812 - val_loss: 2958.7824 - val_mse: 11579250.0000 - val_mae: 2958.7822\n",
            "Epoch 277/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3387.7092 - mse: 16839324.0000 - mae: 3387.7090 - val_loss: 2956.4265 - val_mse: 11579709.0000 - val_mae: 2956.4265\n",
            "Epoch 278/500\n",
            "50/50 [==============================] - 0s 991us/step - loss: 3836.2914 - mse: 19090488.0000 - mae: 3836.2913 - val_loss: 2953.2302 - val_mse: 11579698.0000 - val_mae: 2953.2300\n",
            "Epoch 279/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3557.7456 - mse: 17975930.0000 - mae: 3557.7454 - val_loss: 2952.0803 - val_mse: 11578517.0000 - val_mae: 2952.0803\n",
            "Epoch 280/500\n",
            "50/50 [==============================] - 0s 954us/step - loss: 3448.8548 - mse: 16786118.0000 - mae: 3448.8547 - val_loss: 2951.9809 - val_mse: 11578054.0000 - val_mae: 2951.9810\n",
            "Epoch 281/500\n",
            "50/50 [==============================] - 0s 965us/step - loss: 3822.8305 - mse: 19882068.0000 - mae: 3822.8306 - val_loss: 2950.4778 - val_mse: 11573787.0000 - val_mae: 2950.4778\n",
            "Epoch 282/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3453.7045 - mse: 17657722.0000 - mae: 3453.7043 - val_loss: 2948.8310 - val_mse: 11572334.0000 - val_mae: 2948.8313\n",
            "Epoch 283/500\n",
            "50/50 [==============================] - 0s 963us/step - loss: 3388.2190 - mse: 17526540.0000 - mae: 3388.2188 - val_loss: 2948.7736 - val_mse: 11564762.0000 - val_mae: 2948.7737\n",
            "Epoch 284/500\n",
            "50/50 [==============================] - 0s 949us/step - loss: 3511.2441 - mse: 17180034.0000 - mae: 3511.2441 - val_loss: 2946.3196 - val_mse: 11546464.0000 - val_mae: 2946.3198\n",
            "Epoch 285/500\n",
            "50/50 [==============================] - 0s 911us/step - loss: 3488.7115 - mse: 16669792.0000 - mae: 3488.7117 - val_loss: 2936.7116 - val_mse: 11476934.0000 - val_mae: 2936.7117\n",
            "Epoch 286/500\n",
            "50/50 [==============================] - 0s 927us/step - loss: 3235.3396 - mse: 13970555.0000 - mae: 3235.3396 - val_loss: 2936.5672 - val_mse: 11370522.0000 - val_mae: 2936.5669\n",
            "Epoch 287/500\n",
            "50/50 [==============================] - 0s 945us/step - loss: 3267.0744 - mse: 15467288.0000 - mae: 3267.0745 - val_loss: 2906.3468 - val_mse: 11143423.0000 - val_mae: 2906.3469\n",
            "Epoch 288/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3164.1816 - mse: 14360768.0000 - mae: 3164.1816 - val_loss: 2880.4619 - val_mse: 10842931.0000 - val_mae: 2880.4619\n",
            "Epoch 289/500\n",
            "50/50 [==============================] - 0s 947us/step - loss: 3028.0201 - mse: 13906387.0000 - mae: 3028.0200 - val_loss: 2844.5811 - val_mse: 10466758.0000 - val_mae: 2844.5813\n",
            "Epoch 290/500\n",
            "50/50 [==============================] - 0s 895us/step - loss: 3044.0157 - mse: 14101750.0000 - mae: 3044.0156 - val_loss: 2807.6988 - val_mse: 9912634.0000 - val_mae: 2807.6987\n",
            "Epoch 291/500\n",
            "50/50 [==============================] - 0s 885us/step - loss: 2936.3438 - mse: 11990728.0000 - mae: 2936.3438 - val_loss: 2805.0466 - val_mse: 9466288.0000 - val_mae: 2805.0466\n",
            "Epoch 292/500\n",
            "50/50 [==============================] - 0s 838us/step - loss: 3377.2544 - mse: 16751200.0000 - mae: 3377.2546 - val_loss: 2991.9513 - val_mse: 10956942.0000 - val_mae: 2991.9514\n",
            "Epoch 293/500\n",
            "50/50 [==============================] - 0s 901us/step - loss: 3307.7567 - mse: 16501204.0000 - mae: 3307.7566 - val_loss: 2935.4549 - val_mse: 10244438.0000 - val_mae: 2935.4548\n",
            "Epoch 294/500\n",
            "50/50 [==============================] - 0s 985us/step - loss: 2941.3169 - mse: 13786112.0000 - mae: 2941.3169 - val_loss: 2847.4153 - val_mse: 9092701.0000 - val_mae: 2847.4150\n",
            "Epoch 295/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2975.1972 - mse: 13243933.0000 - mae: 2975.1973 - val_loss: 2807.5718 - val_mse: 8811760.0000 - val_mae: 2807.5718\n",
            "Epoch 296/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2900.1795 - mse: 12971776.0000 - mae: 2900.1797 - val_loss: 2816.1064 - val_mse: 8946819.0000 - val_mae: 2816.1062\n",
            "Epoch 297/500\n",
            "50/50 [==============================] - 0s 974us/step - loss: 2626.4036 - mse: 10671212.0000 - mae: 2626.4038 - val_loss: 2750.7110 - val_mse: 8424346.0000 - val_mae: 2750.7109\n",
            "Epoch 298/500\n",
            "50/50 [==============================] - 0s 946us/step - loss: 3052.1825 - mse: 13038135.0000 - mae: 3052.1826 - val_loss: 2738.5529 - val_mse: 8315614.5000 - val_mae: 2738.5532\n",
            "Epoch 299/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3006.0470 - mse: 12933637.0000 - mae: 3006.0469 - val_loss: 2705.7025 - val_mse: 8185770.5000 - val_mae: 2705.7026\n",
            "Epoch 300/500\n",
            "50/50 [==============================] - 0s 809us/step - loss: 3008.1983 - mse: 13403278.0000 - mae: 3008.1982 - val_loss: 2686.9349 - val_mse: 8052507.0000 - val_mae: 2686.9351\n",
            "Epoch 301/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2852.8558 - mse: 11860529.0000 - mae: 2852.8560 - val_loss: 2683.0853 - val_mse: 7989026.5000 - val_mae: 2683.0854\n",
            "Epoch 302/500\n",
            "50/50 [==============================] - 0s 976us/step - loss: 2877.9923 - mse: 12835876.0000 - mae: 2877.9924 - val_loss: 2687.0955 - val_mse: 8032439.0000 - val_mae: 2687.0955\n",
            "Epoch 303/500\n",
            "50/50 [==============================] - 0s 923us/step - loss: 2938.5171 - mse: 12905164.0000 - mae: 2938.5171 - val_loss: 2668.6526 - val_mse: 7949283.0000 - val_mae: 2668.6528\n",
            "Epoch 304/500\n",
            "50/50 [==============================] - 0s 959us/step - loss: 2937.0499 - mse: 13598346.0000 - mae: 2937.0500 - val_loss: 2659.0963 - val_mse: 8239401.0000 - val_mae: 2659.0962\n",
            "Epoch 305/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2983.6522 - mse: 12797426.0000 - mae: 2983.6521 - val_loss: 2613.2167 - val_mse: 7550869.0000 - val_mae: 2613.2168\n",
            "Epoch 306/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2714.0657 - mse: 11125530.0000 - mae: 2714.0657 - val_loss: 2583.9580 - val_mse: 7409720.0000 - val_mae: 2583.9583\n",
            "Epoch 307/500\n",
            "50/50 [==============================] - 0s 884us/step - loss: 2982.1549 - mse: 13371191.0000 - mae: 2982.1550 - val_loss: 2562.0128 - val_mse: 7370457.5000 - val_mae: 2562.0129\n",
            "Epoch 308/500\n",
            "50/50 [==============================] - 0s 856us/step - loss: 3044.4812 - mse: 12956838.0000 - mae: 3044.4812 - val_loss: 2549.0090 - val_mse: 7197305.5000 - val_mae: 2549.0090\n",
            "Epoch 309/500\n",
            "50/50 [==============================] - 0s 945us/step - loss: 2941.5218 - mse: 12448268.0000 - mae: 2941.5215 - val_loss: 2518.4655 - val_mse: 7062718.5000 - val_mae: 2518.4653\n",
            "Epoch 310/500\n",
            "50/50 [==============================] - 0s 920us/step - loss: 2958.6372 - mse: 13152302.0000 - mae: 2958.6372 - val_loss: 2505.4360 - val_mse: 7028447.0000 - val_mae: 2505.4360\n",
            "Epoch 311/500\n",
            "50/50 [==============================] - 0s 998us/step - loss: 2753.4595 - mse: 11448157.0000 - mae: 2753.4597 - val_loss: 2472.8664 - val_mse: 7077555.0000 - val_mae: 2472.8662\n",
            "Epoch 312/500\n",
            "50/50 [==============================] - 0s 874us/step - loss: 2806.9645 - mse: 12543542.0000 - mae: 2806.9644 - val_loss: 2456.6274 - val_mse: 6815664.0000 - val_mae: 2456.6274\n",
            "Epoch 313/500\n",
            "50/50 [==============================] - 0s 884us/step - loss: 2743.4173 - mse: 11278336.0000 - mae: 2743.4172 - val_loss: 2439.1493 - val_mse: 6651267.0000 - val_mae: 2439.1492\n",
            "Epoch 314/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2987.8483 - mse: 12915119.0000 - mae: 2987.8484 - val_loss: 2427.6673 - val_mse: 6471332.0000 - val_mae: 2427.6675\n",
            "Epoch 315/500\n",
            "50/50 [==============================] - 0s 915us/step - loss: 2972.6834 - mse: 13845391.0000 - mae: 2972.6833 - val_loss: 2400.5243 - val_mse: 6544952.0000 - val_mae: 2400.5244\n",
            "Epoch 316/500\n",
            "50/50 [==============================] - 0s 919us/step - loss: 2656.1053 - mse: 10526016.0000 - mae: 2656.1052 - val_loss: 2372.4543 - val_mse: 6546021.5000 - val_mae: 2372.4543\n",
            "Epoch 317/500\n",
            "50/50 [==============================] - 0s 920us/step - loss: 2674.2947 - mse: 10495287.0000 - mae: 2674.2944 - val_loss: 2340.2563 - val_mse: 6205859.0000 - val_mae: 2340.2563\n",
            "Epoch 318/500\n",
            "50/50 [==============================] - 0s 988us/step - loss: 2768.5104 - mse: 11637571.0000 - mae: 2768.5107 - val_loss: 2315.6398 - val_mse: 6127058.5000 - val_mae: 2315.6399\n",
            "Epoch 319/500\n",
            "50/50 [==============================] - 0s 938us/step - loss: 2921.5748 - mse: 11290601.0000 - mae: 2921.5750 - val_loss: 2293.3951 - val_mse: 5955754.5000 - val_mae: 2293.3950\n",
            "Epoch 320/500\n",
            "50/50 [==============================] - 0s 925us/step - loss: 2629.6132 - mse: 9609401.0000 - mae: 2629.6130 - val_loss: 2277.3524 - val_mse: 5816285.0000 - val_mae: 2277.3523\n",
            "Epoch 321/500\n",
            "50/50 [==============================] - 0s 896us/step - loss: 2679.3333 - mse: 9976467.0000 - mae: 2679.3330 - val_loss: 2262.4670 - val_mse: 6801876.0000 - val_mae: 2262.4670\n",
            "Epoch 322/500\n",
            "50/50 [==============================] - 0s 888us/step - loss: 2929.9520 - mse: 11234909.0000 - mae: 2929.9519 - val_loss: 2241.3459 - val_mse: 6048875.5000 - val_mae: 2241.3459\n",
            "Epoch 323/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2689.9570 - mse: 11016769.0000 - mae: 2689.9568 - val_loss: 2233.9472 - val_mse: 6591613.5000 - val_mae: 2233.9473\n",
            "Epoch 324/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2855.3863 - mse: 11303706.0000 - mae: 2855.3862 - val_loss: 2200.3627 - val_mse: 5357421.0000 - val_mae: 2200.3628\n",
            "Epoch 325/500\n",
            "50/50 [==============================] - 0s 945us/step - loss: 2811.8793 - mse: 11199098.0000 - mae: 2811.8794 - val_loss: 2207.3463 - val_mse: 6420093.0000 - val_mae: 2207.3462\n",
            "Epoch 326/500\n",
            "50/50 [==============================] - 0s 865us/step - loss: 2692.8552 - mse: 10678906.0000 - mae: 2692.8552 - val_loss: 2178.2776 - val_mse: 5411516.0000 - val_mae: 2178.2776\n",
            "Epoch 327/500\n",
            "50/50 [==============================] - 0s 946us/step - loss: 2437.9126 - mse: 9441584.0000 - mae: 2437.9126 - val_loss: 2178.9840 - val_mse: 5812591.5000 - val_mae: 2178.9841\n",
            "Epoch 328/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 3045.8281 - mse: 14230723.0000 - mae: 3045.8281 - val_loss: 2165.8494 - val_mse: 6072823.5000 - val_mae: 2165.8494\n",
            "Epoch 329/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2352.1658 - mse: 8217886.5000 - mae: 2352.1658 - val_loss: 2135.0992 - val_mse: 5062427.0000 - val_mae: 2135.0991\n",
            "Epoch 330/500\n",
            "50/50 [==============================] - 0s 880us/step - loss: 2548.1788 - mse: 10272220.0000 - mae: 2548.1787 - val_loss: 2120.8462 - val_mse: 5123637.0000 - val_mae: 2120.8462\n",
            "Epoch 331/500\n",
            "50/50 [==============================] - 0s 900us/step - loss: 2239.3461 - mse: 7246517.0000 - mae: 2239.3462 - val_loss: 2117.5078 - val_mse: 5661823.0000 - val_mae: 2117.5078\n",
            "Epoch 332/500\n",
            "50/50 [==============================] - 0s 993us/step - loss: 2322.9033 - mse: 8302910.0000 - mae: 2322.9033 - val_loss: 2083.4480 - val_mse: 4860693.0000 - val_mae: 2083.4480\n",
            "Epoch 333/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2653.3252 - mse: 11313716.0000 - mae: 2653.3252 - val_loss: 2086.5738 - val_mse: 5730193.5000 - val_mae: 2086.5737\n",
            "Epoch 334/500\n",
            "50/50 [==============================] - 0s 908us/step - loss: 2350.8059 - mse: 8488250.0000 - mae: 2350.8059 - val_loss: 2061.0284 - val_mse: 4969352.0000 - val_mae: 2061.0286\n",
            "Epoch 335/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2538.3914 - mse: 9837948.0000 - mae: 2538.3914 - val_loss: 2074.0516 - val_mse: 6276367.0000 - val_mae: 2074.0515\n",
            "Epoch 336/500\n",
            "50/50 [==============================] - 0s 857us/step - loss: 2926.3851 - mse: 12959314.0000 - mae: 2926.3850 - val_loss: 2093.4717 - val_mse: 6804173.0000 - val_mae: 2093.4717\n",
            "Epoch 337/500\n",
            "50/50 [==============================] - 0s 810us/step - loss: 2606.3825 - mse: 10083502.0000 - mae: 2606.3826 - val_loss: 2051.7226 - val_mse: 5925352.5000 - val_mae: 2051.7227\n",
            "Epoch 338/500\n",
            "50/50 [==============================] - 0s 924us/step - loss: 2419.2262 - mse: 8786354.0000 - mae: 2419.2261 - val_loss: 2074.4557 - val_mse: 6822963.0000 - val_mae: 2074.4556\n",
            "Epoch 339/500\n",
            "50/50 [==============================] - 0s 933us/step - loss: 2700.6272 - mse: 10243540.0000 - mae: 2700.6272 - val_loss: 2016.0416 - val_mse: 5109721.5000 - val_mae: 2016.0416\n",
            "Epoch 340/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2676.4238 - mse: 10839976.0000 - mae: 2676.4238 - val_loss: 2010.1108 - val_mse: 4710272.0000 - val_mae: 2010.1110\n",
            "Epoch 341/500\n",
            "50/50 [==============================] - 0s 928us/step - loss: 2588.5780 - mse: 9395667.0000 - mae: 2588.5781 - val_loss: 2014.6684 - val_mse: 5081850.0000 - val_mae: 2014.6686\n",
            "Epoch 342/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2411.3421 - mse: 8749614.0000 - mae: 2411.3423 - val_loss: 2011.2888 - val_mse: 5023717.0000 - val_mae: 2011.2887\n",
            "Epoch 343/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2474.6389 - mse: 8087764.5000 - mae: 2474.6389 - val_loss: 1999.4263 - val_mse: 4651399.5000 - val_mae: 1999.4264\n",
            "Epoch 344/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2893.0477 - mse: 12403295.0000 - mae: 2893.0476 - val_loss: 2008.9721 - val_mse: 5832860.0000 - val_mae: 2008.9720\n",
            "Epoch 345/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2492.9866 - mse: 9253736.0000 - mae: 2492.9868 - val_loss: 2001.4057 - val_mse: 5585887.0000 - val_mae: 2001.4056\n",
            "Epoch 346/500\n",
            "50/50 [==============================] - 0s 963us/step - loss: 2455.3885 - mse: 9041146.0000 - mae: 2455.3884 - val_loss: 1995.8830 - val_mse: 5734169.0000 - val_mae: 1995.8831\n",
            "Epoch 347/500\n",
            "50/50 [==============================] - 0s 952us/step - loss: 2378.0725 - mse: 8416668.0000 - mae: 2378.0723 - val_loss: 1986.3312 - val_mse: 6198131.0000 - val_mae: 1986.3313\n",
            "Epoch 348/500\n",
            "50/50 [==============================] - 0s 911us/step - loss: 2439.2540 - mse: 8241263.5000 - mae: 2439.2542 - val_loss: 1936.0177 - val_mse: 4690587.0000 - val_mae: 1936.0176\n",
            "Epoch 349/500\n",
            "50/50 [==============================] - 0s 934us/step - loss: 2577.6992 - mse: 9944596.0000 - mae: 2577.6992 - val_loss: 1940.5621 - val_mse: 5420490.0000 - val_mae: 1940.5621\n",
            "Epoch 350/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2651.5946 - mse: 11104119.0000 - mae: 2651.5947 - val_loss: 1941.9535 - val_mse: 5618996.5000 - val_mae: 1941.9535\n",
            "Epoch 351/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2470.7313 - mse: 9143717.0000 - mae: 2470.7312 - val_loss: 1935.7306 - val_mse: 5431173.5000 - val_mae: 1935.7307\n",
            "Epoch 352/500\n",
            "50/50 [==============================] - 0s 801us/step - loss: 2287.9433 - mse: 8846583.0000 - mae: 2287.9434 - val_loss: 1940.0553 - val_mse: 6136396.5000 - val_mae: 1940.0553\n",
            "Epoch 353/500\n",
            "50/50 [==============================] - 0s 995us/step - loss: 2419.2958 - mse: 8953591.0000 - mae: 2419.2959 - val_loss: 1935.5843 - val_mse: 6050416.5000 - val_mae: 1935.5844\n",
            "Epoch 354/500\n",
            "50/50 [==============================] - 0s 848us/step - loss: 2762.4879 - mse: 11052197.0000 - mae: 2762.4878 - val_loss: 1924.7488 - val_mse: 5308747.5000 - val_mae: 1924.7488\n",
            "Epoch 355/500\n",
            "50/50 [==============================] - 0s 883us/step - loss: 2450.9618 - mse: 8941837.0000 - mae: 2450.9619 - val_loss: 1880.8970 - val_mse: 4193238.7500 - val_mae: 1880.8969\n",
            "Epoch 356/500\n",
            "50/50 [==============================] - 0s 912us/step - loss: 2680.1780 - mse: 11708774.0000 - mae: 2680.1782 - val_loss: 1915.0859 - val_mse: 5683721.0000 - val_mae: 1915.0859\n",
            "Epoch 357/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2265.9038 - mse: 7720807.5000 - mae: 2265.9038 - val_loss: 1892.3077 - val_mse: 4911209.0000 - val_mae: 1892.3079\n",
            "Epoch 358/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2833.5034 - mse: 11584881.0000 - mae: 2833.5032 - val_loss: 1883.0086 - val_mse: 5505531.0000 - val_mae: 1883.0085\n",
            "Epoch 359/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2212.9305 - mse: 7173851.0000 - mae: 2212.9307 - val_loss: 1861.0837 - val_mse: 5271024.5000 - val_mae: 1861.0836\n",
            "Epoch 360/500\n",
            "50/50 [==============================] - 0s 846us/step - loss: 2095.1433 - mse: 7068723.0000 - mae: 2095.1436 - val_loss: 1929.1966 - val_mse: 6579313.0000 - val_mae: 1929.1967\n",
            "Epoch 361/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2706.7020 - mse: 10211720.0000 - mae: 2706.7021 - val_loss: 1845.6232 - val_mse: 5035997.0000 - val_mae: 1845.6233\n",
            "Epoch 362/500\n",
            "50/50 [==============================] - 0s 864us/step - loss: 2471.4256 - mse: 8945066.0000 - mae: 2471.4255 - val_loss: 1871.1676 - val_mse: 6027242.5000 - val_mae: 1871.1676\n",
            "Epoch 363/500\n",
            "50/50 [==============================] - 0s 890us/step - loss: 2627.5749 - mse: 9412237.0000 - mae: 2627.5750 - val_loss: 1847.1025 - val_mse: 5574141.0000 - val_mae: 1847.1025\n",
            "Epoch 364/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2408.6477 - mse: 8525268.0000 - mae: 2408.6477 - val_loss: 1864.9756 - val_mse: 6094458.5000 - val_mae: 1864.9756\n",
            "Epoch 365/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2644.0554 - mse: 10172691.0000 - mae: 2644.0557 - val_loss: 1820.5622 - val_mse: 5342517.0000 - val_mae: 1820.5621\n",
            "Epoch 366/500\n",
            "50/50 [==============================] - 0s 925us/step - loss: 2375.9922 - mse: 8957549.0000 - mae: 2375.9924 - val_loss: 1804.1851 - val_mse: 5572168.0000 - val_mae: 1804.1852\n",
            "Epoch 367/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2240.1159 - mse: 6782978.5000 - mae: 2240.1160 - val_loss: 1826.9061 - val_mse: 6062031.0000 - val_mae: 1826.9060\n",
            "Epoch 368/500\n",
            "50/50 [==============================] - 0s 882us/step - loss: 2175.4643 - mse: 7670574.5000 - mae: 2175.4644 - val_loss: 1983.8351 - val_mse: 7466385.0000 - val_mae: 1983.8352\n",
            "Epoch 369/500\n",
            "50/50 [==============================] - 0s 866us/step - loss: 2684.4294 - mse: 9150619.0000 - mae: 2684.4294 - val_loss: 1763.5870 - val_mse: 4877511.0000 - val_mae: 1763.5869\n",
            "Epoch 370/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2493.2116 - mse: 9832054.0000 - mae: 2493.2117 - val_loss: 1774.8985 - val_mse: 5710819.5000 - val_mae: 1774.8984\n",
            "Epoch 371/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2621.8352 - mse: 9597094.0000 - mae: 2621.8352 - val_loss: 1758.7513 - val_mse: 5096361.0000 - val_mae: 1758.7513\n",
            "Epoch 372/500\n",
            "50/50 [==============================] - 0s 937us/step - loss: 2380.4902 - mse: 8199674.0000 - mae: 2380.4902 - val_loss: 1825.9138 - val_mse: 6336685.0000 - val_mae: 1825.9138\n",
            "Epoch 373/500\n",
            "50/50 [==============================] - 0s 972us/step - loss: 2271.0304 - mse: 7896811.5000 - mae: 2271.0303 - val_loss: 1885.0458 - val_mse: 6849743.0000 - val_mae: 1885.0457\n",
            "Epoch 374/500\n",
            "50/50 [==============================] - 0s 920us/step - loss: 2526.9573 - mse: 10434830.0000 - mae: 2526.9570 - val_loss: 1732.1256 - val_mse: 4053572.5000 - val_mae: 1732.1256\n",
            "Epoch 375/500\n",
            "50/50 [==============================] - 0s 932us/step - loss: 1981.5875 - mse: 6250032.5000 - mae: 1981.5875 - val_loss: 1855.7187 - val_mse: 6610289.0000 - val_mae: 1855.7188\n",
            "Epoch 376/500\n",
            "50/50 [==============================] - 0s 907us/step - loss: 2454.0000 - mse: 8384909.0000 - mae: 2454.0000 - val_loss: 1752.7439 - val_mse: 5155419.0000 - val_mae: 1752.7439\n",
            "Epoch 377/500\n",
            "50/50 [==============================] - 0s 961us/step - loss: 2103.2844 - mse: 6481181.5000 - mae: 2103.2844 - val_loss: 1840.4363 - val_mse: 6506645.5000 - val_mae: 1840.4363\n",
            "Epoch 378/500\n",
            "50/50 [==============================] - 0s 960us/step - loss: 2665.8491 - mse: 9315310.0000 - mae: 2665.8494 - val_loss: 1763.7024 - val_mse: 5598623.0000 - val_mae: 1763.7025\n",
            "Epoch 379/500\n",
            "50/50 [==============================] - 0s 877us/step - loss: 2254.1443 - mse: 6835806.5000 - mae: 2254.1443 - val_loss: 1734.5212 - val_mse: 4799318.0000 - val_mae: 1734.5212\n",
            "Epoch 380/500\n",
            "50/50 [==============================] - 0s 958us/step - loss: 1976.2716 - mse: 6058330.0000 - mae: 1976.2716 - val_loss: 1757.7938 - val_mse: 5721077.0000 - val_mae: 1757.7937\n",
            "Epoch 381/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2251.7587 - mse: 7804426.0000 - mae: 2251.7588 - val_loss: 1879.7540 - val_mse: 6914894.5000 - val_mae: 1879.7539\n",
            "Epoch 382/500\n",
            "50/50 [==============================] - 0s 848us/step - loss: 2354.8915 - mse: 7140552.5000 - mae: 2354.8916 - val_loss: 1762.8545 - val_mse: 5932528.5000 - val_mae: 1762.8545\n",
            "Epoch 383/500\n",
            "50/50 [==============================] - 0s 983us/step - loss: 2630.8843 - mse: 10506693.0000 - mae: 2630.8843 - val_loss: 1706.8244 - val_mse: 4619983.0000 - val_mae: 1706.8245\n",
            "Epoch 384/500\n",
            "50/50 [==============================] - 0s 905us/step - loss: 2684.8266 - mse: 10510193.0000 - mae: 2684.8267 - val_loss: 1714.2554 - val_mse: 5136354.5000 - val_mae: 1714.2555\n",
            "Epoch 385/500\n",
            "50/50 [==============================] - 0s 897us/step - loss: 2448.6519 - mse: 8919648.0000 - mae: 2448.6521 - val_loss: 1698.8681 - val_mse: 4799779.5000 - val_mae: 1698.8679\n",
            "Epoch 386/500\n",
            "50/50 [==============================] - 0s 854us/step - loss: 2182.1006 - mse: 7642381.0000 - mae: 2182.1006 - val_loss: 1700.3062 - val_mse: 5227577.0000 - val_mae: 1700.3060\n",
            "Epoch 387/500\n",
            "50/50 [==============================] - 0s 910us/step - loss: 2234.6364 - mse: 7359488.5000 - mae: 2234.6362 - val_loss: 1713.1506 - val_mse: 5745306.0000 - val_mae: 1713.1506\n",
            "Epoch 388/500\n",
            "50/50 [==============================] - 0s 952us/step - loss: 2139.2382 - mse: 6366144.5000 - mae: 2139.2380 - val_loss: 1704.8734 - val_mse: 5574265.5000 - val_mae: 1704.8734\n",
            "Epoch 389/500\n",
            "50/50 [==============================] - 0s 903us/step - loss: 2435.1154 - mse: 8200106.0000 - mae: 2435.1155 - val_loss: 1718.3647 - val_mse: 5733889.0000 - val_mae: 1718.3646\n",
            "Epoch 390/500\n",
            "50/50 [==============================] - 0s 907us/step - loss: 2484.8395 - mse: 8219118.5000 - mae: 2484.8394 - val_loss: 1767.4710 - val_mse: 6208440.0000 - val_mae: 1767.4711\n",
            "Epoch 391/500\n",
            "50/50 [==============================] - 0s 851us/step - loss: 2252.4153 - mse: 8276165.0000 - mae: 2252.4155 - val_loss: 1763.8403 - val_mse: 6255954.0000 - val_mae: 1763.8402\n",
            "Epoch 392/500\n",
            "50/50 [==============================] - 0s 897us/step - loss: 2519.5618 - mse: 8943093.0000 - mae: 2519.5618 - val_loss: 1679.2917 - val_mse: 5252980.5000 - val_mae: 1679.2917\n",
            "Epoch 393/500\n",
            "50/50 [==============================] - 0s 883us/step - loss: 2614.6130 - mse: 9506394.0000 - mae: 2614.6130 - val_loss: 1774.2005 - val_mse: 6397741.0000 - val_mae: 1774.2006\n",
            "Epoch 394/500\n",
            "50/50 [==============================] - 0s 996us/step - loss: 2327.9569 - mse: 7791101.5000 - mae: 2327.9568 - val_loss: 1715.1100 - val_mse: 5889355.5000 - val_mae: 1715.1100\n",
            "Epoch 395/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2802.2587 - mse: 11008940.0000 - mae: 2802.2585 - val_loss: 1766.2284 - val_mse: 6367739.0000 - val_mae: 1766.2285\n",
            "Epoch 396/500\n",
            "50/50 [==============================] - 0s 921us/step - loss: 2101.7226 - mse: 6439572.0000 - mae: 2101.7227 - val_loss: 1705.0834 - val_mse: 5927322.5000 - val_mae: 1705.0834\n",
            "Epoch 397/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2056.4115 - mse: 6855385.0000 - mae: 2056.4116 - val_loss: 1787.8139 - val_mse: 6656039.5000 - val_mae: 1787.8138\n",
            "Epoch 398/500\n",
            "50/50 [==============================] - 0s 976us/step - loss: 2575.3160 - mse: 9626287.0000 - mae: 2575.3159 - val_loss: 1742.7750 - val_mse: 6356950.5000 - val_mae: 1742.7750\n",
            "Epoch 399/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2336.4460 - mse: 7705031.0000 - mae: 2336.4460 - val_loss: 1642.4605 - val_mse: 5269609.5000 - val_mae: 1642.4603\n",
            "Epoch 400/500\n",
            "50/50 [==============================] - 0s 901us/step - loss: 2144.9144 - mse: 6805636.0000 - mae: 2144.9143 - val_loss: 1726.3287 - val_mse: 6205610.0000 - val_mae: 1726.3287\n",
            "Epoch 401/500\n",
            "50/50 [==============================] - 0s 891us/step - loss: 2101.2706 - mse: 6542275.0000 - mae: 2101.2705 - val_loss: 1623.8374 - val_mse: 5046079.0000 - val_mae: 1623.8374\n",
            "Epoch 402/500\n",
            "50/50 [==============================] - 0s 927us/step - loss: 2647.7339 - mse: 9982057.0000 - mae: 2647.7341 - val_loss: 1746.1790 - val_mse: 6414343.0000 - val_mae: 1746.1790\n",
            "Epoch 403/500\n",
            "50/50 [==============================] - 0s 917us/step - loss: 2393.0108 - mse: 8125092.5000 - mae: 2393.0107 - val_loss: 1622.6124 - val_mse: 5037365.0000 - val_mae: 1622.6125\n",
            "Epoch 404/500\n",
            "50/50 [==============================] - 0s 900us/step - loss: 2504.1395 - mse: 9090397.0000 - mae: 2504.1396 - val_loss: 1775.9041 - val_mse: 6757262.5000 - val_mae: 1775.9039\n",
            "Epoch 405/500\n",
            "50/50 [==============================] - 0s 969us/step - loss: 2381.6874 - mse: 8711143.0000 - mae: 2381.6873 - val_loss: 1619.6118 - val_mse: 5127260.5000 - val_mae: 1619.6117\n",
            "Epoch 406/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2446.8555 - mse: 9316256.0000 - mae: 2446.8552 - val_loss: 1663.6871 - val_mse: 5722184.0000 - val_mae: 1663.6871\n",
            "Epoch 407/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2289.6324 - mse: 7475392.5000 - mae: 2289.6323 - val_loss: 1804.3308 - val_mse: 6924038.5000 - val_mae: 1804.3307\n",
            "Epoch 408/500\n",
            "50/50 [==============================] - 0s 993us/step - loss: 2174.1650 - mse: 7677880.5000 - mae: 2174.1650 - val_loss: 1637.2518 - val_mse: 5529787.0000 - val_mae: 1637.2517\n",
            "Epoch 409/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2174.3767 - mse: 6307489.5000 - mae: 2174.3767 - val_loss: 1788.5140 - val_mse: 6914984.0000 - val_mae: 1788.5140\n",
            "Epoch 410/500\n",
            "50/50 [==============================] - 0s 967us/step - loss: 2579.4045 - mse: 9921361.0000 - mae: 2579.4045 - val_loss: 1592.3177 - val_mse: 4860561.0000 - val_mae: 1592.3176\n",
            "Epoch 411/500\n",
            "50/50 [==============================] - 0s 852us/step - loss: 2477.8961 - mse: 9907420.0000 - mae: 2477.8960 - val_loss: 1761.5639 - val_mse: 6772990.5000 - val_mae: 1761.5638\n",
            "Epoch 412/500\n",
            "50/50 [==============================] - 0s 801us/step - loss: 2091.2167 - mse: 7497006.0000 - mae: 2091.2168 - val_loss: 1564.2372 - val_mse: 4821718.5000 - val_mae: 1564.2371\n",
            "Epoch 413/500\n",
            "50/50 [==============================] - 0s 921us/step - loss: 2581.7412 - mse: 9239752.0000 - mae: 2581.7412 - val_loss: 1621.4910 - val_mse: 5732029.5000 - val_mae: 1621.4910\n",
            "Epoch 414/500\n",
            "50/50 [==============================] - 0s 949us/step - loss: 2264.6847 - mse: 8322481.5000 - mae: 2264.6846 - val_loss: 1871.5411 - val_mse: 7770349.0000 - val_mae: 1871.5410\n",
            "Epoch 415/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2105.1394 - mse: 6832694.0000 - mae: 2105.1394 - val_loss: 1639.3972 - val_mse: 5725315.5000 - val_mae: 1639.3972\n",
            "Epoch 416/500\n",
            "50/50 [==============================] - 0s 978us/step - loss: 2251.6504 - mse: 8023815.5000 - mae: 2251.6504 - val_loss: 1580.4057 - val_mse: 5255777.0000 - val_mae: 1580.4058\n",
            "Epoch 417/500\n",
            "50/50 [==============================] - 0s 948us/step - loss: 2430.4678 - mse: 8199193.5000 - mae: 2430.4680 - val_loss: 1564.9220 - val_mse: 4942115.0000 - val_mae: 1564.9221\n",
            "Epoch 418/500\n",
            "50/50 [==============================] - 0s 927us/step - loss: 2359.6069 - mse: 7598553.0000 - mae: 2359.6069 - val_loss: 1609.4832 - val_mse: 5768419.0000 - val_mae: 1609.4832\n",
            "Epoch 419/500\n",
            "50/50 [==============================] - 0s 930us/step - loss: 2346.1113 - mse: 7940487.5000 - mae: 2346.1113 - val_loss: 1589.3865 - val_mse: 5458744.0000 - val_mae: 1589.3865\n",
            "Epoch 420/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2238.4345 - mse: 7058731.0000 - mae: 2238.4346 - val_loss: 1884.8286 - val_mse: 7941861.5000 - val_mae: 1884.8285\n",
            "Epoch 421/500\n",
            "50/50 [==============================] - 0s 922us/step - loss: 2180.4987 - mse: 7502355.0000 - mae: 2180.4988 - val_loss: 1675.6767 - val_mse: 6178871.5000 - val_mae: 1675.6768\n",
            "Epoch 422/500\n",
            "50/50 [==============================] - 0s 897us/step - loss: 2255.6856 - mse: 7097410.0000 - mae: 2255.6855 - val_loss: 1678.8160 - val_mse: 6213728.0000 - val_mae: 1678.8160\n",
            "Epoch 423/500\n",
            "50/50 [==============================] - 0s 950us/step - loss: 1927.6730 - mse: 5804151.0000 - mae: 1927.6730 - val_loss: 1628.7155 - val_mse: 5483197.5000 - val_mae: 1628.7156\n",
            "Epoch 424/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2407.8641 - mse: 8282119.0000 - mae: 2407.8640 - val_loss: 1914.9783 - val_mse: 7793773.0000 - val_mae: 1914.9781\n",
            "Epoch 425/500\n",
            "50/50 [==============================] - 0s 932us/step - loss: 2191.4432 - mse: 6484243.0000 - mae: 2191.4434 - val_loss: 1847.0581 - val_mse: 7400969.5000 - val_mae: 1847.0582\n",
            "Epoch 426/500\n",
            "50/50 [==============================] - 0s 934us/step - loss: 2373.2898 - mse: 7374025.5000 - mae: 2373.2898 - val_loss: 1813.1294 - val_mse: 7117213.0000 - val_mae: 1813.1295\n",
            "Epoch 427/500\n",
            "50/50 [==============================] - 0s 941us/step - loss: 2355.7550 - mse: 7658461.5000 - mae: 2355.7549 - val_loss: 1783.1776 - val_mse: 6786885.0000 - val_mae: 1783.1775\n",
            "Epoch 428/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2283.1620 - mse: 7545784.5000 - mae: 2283.1621 - val_loss: 1718.8175 - val_mse: 6228827.5000 - val_mae: 1718.8176\n",
            "Epoch 429/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2293.9628 - mse: 7773817.0000 - mae: 2293.9629 - val_loss: 1665.0253 - val_mse: 5737161.0000 - val_mae: 1665.0254\n",
            "Epoch 430/500\n",
            "50/50 [==============================] - 0s 932us/step - loss: 2199.6436 - mse: 6993284.0000 - mae: 2199.6438 - val_loss: 1845.3703 - val_mse: 7323865.5000 - val_mae: 1845.3704\n",
            "Epoch 431/500\n",
            "50/50 [==============================] - 0s 904us/step - loss: 2205.1832 - mse: 6679756.0000 - mae: 2205.1831 - val_loss: 1619.6873 - val_mse: 5197536.0000 - val_mae: 1619.6873\n",
            "Epoch 432/500\n",
            "50/50 [==============================] - 0s 935us/step - loss: 2421.0860 - mse: 9041351.0000 - mae: 2421.0859 - val_loss: 1775.3523 - val_mse: 6737946.5000 - val_mae: 1775.3523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60 samples, validate on 60 samples\n",
            "Epoch 1/500\n",
            "60/60 [==============================] - 3s 46ms/step - loss: 6364.4595 - mse: 51977388.0000 - mae: 6364.4595 - val_loss: 6630.1674 - val_mse: 51409664.0000 - val_mae: 6630.1670\n",
            "Epoch 2/500\n",
            "60/60 [==============================] - 0s 919us/step - loss: 6364.1415 - mse: 51973188.0000 - mae: 6364.1416 - val_loss: 6628.9444 - val_mse: 51394452.0000 - val_mae: 6628.9448\n",
            "Epoch 3/500\n",
            "60/60 [==============================] - 0s 967us/step - loss: 6361.1744 - mse: 51937820.0000 - mae: 6361.1743 - val_loss: 6622.8305 - val_mse: 51316096.0000 - val_mae: 6622.8306\n",
            "Epoch 4/500\n",
            "60/60 [==============================] - 0s 908us/step - loss: 6354.3673 - mse: 51844100.0000 - mae: 6354.3667 - val_loss: 6613.9244 - val_mse: 51199208.0000 - val_mae: 6613.9238\n",
            "Epoch 5/500\n",
            "60/60 [==============================] - 0s 907us/step - loss: 6344.1682 - mse: 51731120.0000 - mae: 6344.1685 - val_loss: 6601.8948 - val_mse: 51040676.0000 - val_mae: 6601.8950\n",
            "Epoch 6/500\n",
            "60/60 [==============================] - 0s 856us/step - loss: 6327.2194 - mse: 51509332.0000 - mae: 6327.2197 - val_loss: 6585.4977 - val_mse: 50824068.0000 - val_mae: 6585.4980\n",
            "Epoch 7/500\n",
            "60/60 [==============================] - 0s 995us/step - loss: 6313.2668 - mse: 51371500.0000 - mae: 6313.2666 - val_loss: 6567.0154 - val_mse: 50580520.0000 - val_mae: 6567.0156\n",
            "Epoch 8/500\n",
            "60/60 [==============================] - 0s 838us/step - loss: 6291.7839 - mse: 51082164.0000 - mae: 6291.7837 - val_loss: 6545.5218 - val_mse: 50298008.0000 - val_mae: 6545.5220\n",
            "Epoch 9/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 6263.6196 - mse: 50715640.0000 - mae: 6263.6196 - val_loss: 6520.2157 - val_mse: 49966628.0000 - val_mae: 6520.2158\n",
            "Epoch 10/500\n",
            "60/60 [==============================] - 0s 929us/step - loss: 6237.5695 - mse: 50373640.0000 - mae: 6237.5698 - val_loss: 6492.5656 - val_mse: 49605696.0000 - val_mae: 6492.5654\n",
            "Epoch 11/500\n",
            "60/60 [==============================] - 0s 907us/step - loss: 6206.3351 - mse: 49938552.0000 - mae: 6206.3350 - val_loss: 6463.3840 - val_mse: 49226000.0000 - val_mae: 6463.3843\n",
            "Epoch 12/500\n",
            "60/60 [==============================] - 0s 945us/step - loss: 6182.3994 - mse: 49657408.0000 - mae: 6182.3994 - val_loss: 6432.5923 - val_mse: 48827400.0000 - val_mae: 6432.5923\n",
            "Epoch 13/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 6153.6813 - mse: 49186916.0000 - mae: 6153.6816 - val_loss: 6399.4918 - val_mse: 48400920.0000 - val_mae: 6399.4917\n",
            "Epoch 14/500\n",
            "60/60 [==============================] - 0s 989us/step - loss: 6135.3092 - mse: 48910428.0000 - mae: 6135.3096 - val_loss: 6366.7236 - val_mse: 47981612.0000 - val_mae: 6366.7236\n",
            "Epoch 15/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 6104.9810 - mse: 48441668.0000 - mae: 6104.9810 - val_loss: 6334.5297 - val_mse: 47571760.0000 - val_mae: 6334.5298\n",
            "Epoch 16/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 6076.0266 - mse: 47993176.0000 - mae: 6076.0269 - val_loss: 6297.1080 - val_mse: 47097976.0000 - val_mae: 6297.1084\n",
            "Epoch 17/500\n",
            "60/60 [==============================] - 0s 888us/step - loss: 6054.0902 - mse: 47663192.0000 - mae: 6054.0903 - val_loss: 6260.6743 - val_mse: 46639700.0000 - val_mae: 6260.6738\n",
            "Epoch 18/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 6040.4238 - mse: 47449692.0000 - mae: 6040.4238 - val_loss: 6224.8853 - val_mse: 46192244.0000 - val_mae: 6224.8853\n",
            "Epoch 19/500\n",
            "60/60 [==============================] - 0s 960us/step - loss: 6013.0794 - mse: 47099372.0000 - mae: 6013.0796 - val_loss: 6185.2644 - val_mse: 45700032.0000 - val_mae: 6185.2646\n",
            "Epoch 20/500\n",
            "60/60 [==============================] - 0s 898us/step - loss: 5948.7887 - mse: 45900980.0000 - mae: 5948.7886 - val_loss: 6141.1349 - val_mse: 45155628.0000 - val_mae: 6141.1343\n",
            "Epoch 21/500\n",
            "60/60 [==============================] - 0s 926us/step - loss: 5939.1644 - mse: 45874732.0000 - mae: 5939.1641 - val_loss: 6095.1418 - val_mse: 44592396.0000 - val_mae: 6095.1416\n",
            "Epoch 22/500\n",
            "60/60 [==============================] - 0s 875us/step - loss: 5897.6879 - mse: 45213304.0000 - mae: 5897.6875 - val_loss: 6045.6359 - val_mse: 43990984.0000 - val_mae: 6045.6357\n",
            "Epoch 23/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5912.7693 - mse: 45350376.0000 - mae: 5912.7686 - val_loss: 6004.5658 - val_mse: 43479536.0000 - val_mae: 6004.5654\n",
            "Epoch 24/500\n",
            "60/60 [==============================] - 0s 858us/step - loss: 5858.6605 - mse: 44352444.0000 - mae: 5858.6606 - val_loss: 5958.6504 - val_mse: 42910656.0000 - val_mae: 5958.6504\n",
            "Epoch 25/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5820.4301 - mse: 43556492.0000 - mae: 5820.4302 - val_loss: 5911.9643 - val_mse: 42316796.0000 - val_mae: 5911.9644\n",
            "Epoch 26/500\n",
            "60/60 [==============================] - 0s 871us/step - loss: 5760.8807 - mse: 42717416.0000 - mae: 5760.8809 - val_loss: 5861.5130 - val_mse: 41680144.0000 - val_mae: 5861.5132\n",
            "Epoch 27/500\n",
            "60/60 [==============================] - 0s 909us/step - loss: 5738.6838 - mse: 42440888.0000 - mae: 5738.6831 - val_loss: 5811.2145 - val_mse: 41032372.0000 - val_mae: 5811.2144\n",
            "Epoch 28/500\n",
            "60/60 [==============================] - 0s 925us/step - loss: 5717.4458 - mse: 41999480.0000 - mae: 5717.4458 - val_loss: 5760.3331 - val_mse: 40378752.0000 - val_mae: 5760.3330\n",
            "Epoch 29/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5676.2454 - mse: 41295080.0000 - mae: 5676.2446 - val_loss: 5709.2611 - val_mse: 39714460.0000 - val_mae: 5709.2607\n",
            "Epoch 30/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5595.8936 - mse: 40252612.0000 - mae: 5595.8936 - val_loss: 5652.3776 - val_mse: 38970624.0000 - val_mae: 5652.3774\n",
            "Epoch 31/500\n",
            "60/60 [==============================] - 0s 835us/step - loss: 5567.5417 - mse: 39388176.0000 - mae: 5567.5415 - val_loss: 5593.8308 - val_mse: 38193240.0000 - val_mae: 5593.8311\n",
            "Epoch 32/500\n",
            "60/60 [==============================] - 0s 940us/step - loss: 5535.7622 - mse: 39214960.0000 - mae: 5535.7622 - val_loss: 5535.7372 - val_mse: 37421740.0000 - val_mae: 5535.7368\n",
            "Epoch 33/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5475.5101 - mse: 38589060.0000 - mae: 5475.5103 - val_loss: 5475.3244 - val_mse: 36605988.0000 - val_mae: 5475.3252\n",
            "Epoch 34/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5471.5785 - mse: 37727500.0000 - mae: 5471.5786 - val_loss: 5415.5041 - val_mse: 35800884.0000 - val_mae: 5415.5044\n",
            "Epoch 35/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5394.6654 - mse: 36928260.0000 - mae: 5394.6655 - val_loss: 5352.5385 - val_mse: 34957720.0000 - val_mae: 5352.5391\n",
            "Epoch 36/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5329.2707 - mse: 36230840.0000 - mae: 5329.2710 - val_loss: 5289.2327 - val_mse: 34093608.0000 - val_mae: 5289.2329\n",
            "Epoch 37/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 5234.7606 - mse: 34610164.0000 - mae: 5234.7603 - val_loss: 5220.4210 - val_mse: 33170382.0000 - val_mae: 5220.4209\n",
            "Epoch 38/500\n",
            "60/60 [==============================] - 0s 979us/step - loss: 5128.0124 - mse: 32777890.0000 - mae: 5128.0127 - val_loss: 5149.2786 - val_mse: 32201622.0000 - val_mae: 5149.2788\n",
            "Epoch 39/500\n",
            "60/60 [==============================] - 0s 836us/step - loss: 5094.3125 - mse: 33130198.0000 - mae: 5094.3125 - val_loss: 5078.8296 - val_mse: 31249990.0000 - val_mae: 5078.8296\n",
            "Epoch 40/500\n",
            "60/60 [==============================] - 0s 916us/step - loss: 5043.0890 - mse: 32068308.0000 - mae: 5043.0884 - val_loss: 5008.4822 - val_mse: 30318374.0000 - val_mae: 5008.4824\n",
            "Epoch 41/500\n",
            "60/60 [==============================] - 0s 966us/step - loss: 5102.1235 - mse: 32479172.0000 - mae: 5102.1235 - val_loss: 4945.2473 - val_mse: 29461166.0000 - val_mae: 4945.2476\n",
            "Epoch 42/500\n",
            "60/60 [==============================] - 0s 898us/step - loss: 5022.8363 - mse: 31019714.0000 - mae: 5022.8364 - val_loss: 4879.2504 - val_mse: 28580690.0000 - val_mae: 4879.2505\n",
            "Epoch 43/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4861.5523 - mse: 29670120.0000 - mae: 4861.5522 - val_loss: 4811.9382 - val_mse: 27687522.0000 - val_mae: 4811.9380\n",
            "Epoch 44/500\n",
            "60/60 [==============================] - 0s 957us/step - loss: 4757.2065 - mse: 28532390.0000 - mae: 4757.2061 - val_loss: 4740.1750 - val_mse: 26725114.0000 - val_mae: 4740.1748\n",
            "Epoch 45/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4757.5343 - mse: 28285812.0000 - mae: 4757.5342 - val_loss: 4670.4294 - val_mse: 25811498.0000 - val_mae: 4670.4292\n",
            "Epoch 46/500\n",
            "60/60 [==============================] - 0s 969us/step - loss: 4601.0391 - mse: 26652234.0000 - mae: 4601.0396 - val_loss: 4595.2861 - val_mse: 24854628.0000 - val_mae: 4595.2866\n",
            "Epoch 47/500\n",
            "60/60 [==============================] - 0s 845us/step - loss: 4649.0672 - mse: 27164020.0000 - mae: 4649.0674 - val_loss: 4531.3231 - val_mse: 24061138.0000 - val_mae: 4531.3232\n",
            "Epoch 48/500\n",
            "60/60 [==============================] - 0s 903us/step - loss: 4579.6178 - mse: 25920994.0000 - mae: 4579.6177 - val_loss: 4460.9387 - val_mse: 23207334.0000 - val_mae: 4460.9385\n",
            "Epoch 49/500\n",
            "60/60 [==============================] - 0s 874us/step - loss: 4415.4185 - mse: 24081088.0000 - mae: 4415.4189 - val_loss: 4381.2074 - val_mse: 22268118.0000 - val_mae: 4381.2075\n",
            "Epoch 50/500\n",
            "60/60 [==============================] - 0s 862us/step - loss: 4372.0386 - mse: 23589742.0000 - mae: 4372.0386 - val_loss: 4309.4438 - val_mse: 21448684.0000 - val_mae: 4309.4438\n",
            "Epoch 51/500\n",
            "60/60 [==============================] - 0s 902us/step - loss: 4326.5633 - mse: 23398034.0000 - mae: 4326.5635 - val_loss: 4240.7656 - val_mse: 20686186.0000 - val_mae: 4240.7656\n",
            "Epoch 52/500\n",
            "60/60 [==============================] - 0s 882us/step - loss: 4436.3390 - mse: 24874462.0000 - mae: 4436.3384 - val_loss: 4172.9144 - val_mse: 19954758.0000 - val_mae: 4172.9146\n",
            "Epoch 53/500\n",
            "60/60 [==============================] - 0s 960us/step - loss: 4385.8013 - mse: 23596702.0000 - mae: 4385.8018 - val_loss: 4108.9902 - val_mse: 19285170.0000 - val_mae: 4108.9902\n",
            "Epoch 54/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 4304.4782 - mse: 23204604.0000 - mae: 4304.4780 - val_loss: 4047.7867 - val_mse: 18661166.0000 - val_mae: 4047.7866\n",
            "Epoch 55/500\n",
            "60/60 [==============================] - 0s 902us/step - loss: 3969.5934 - mse: 19805602.0000 - mae: 3969.5935 - val_loss: 3985.9985 - val_mse: 18050190.0000 - val_mae: 3985.9985\n",
            "Epoch 56/500\n",
            "60/60 [==============================] - 0s 929us/step - loss: 4054.9340 - mse: 20775722.0000 - mae: 4054.9338 - val_loss: 3917.7740 - val_mse: 17392984.0000 - val_mae: 3917.7739\n",
            "Epoch 57/500\n",
            "60/60 [==============================] - 0s 855us/step - loss: 4213.7719 - mse: 21643866.0000 - mae: 4213.7720 - val_loss: 3857.4414 - val_mse: 16830972.0000 - val_mae: 3857.4414\n",
            "Epoch 58/500\n",
            "60/60 [==============================] - 0s 914us/step - loss: 3793.1306 - mse: 17848908.0000 - mae: 3793.1304 - val_loss: 3777.6481 - val_mse: 16110162.0000 - val_mae: 3777.6482\n",
            "Epoch 59/500\n",
            "60/60 [==============================] - 0s 869us/step - loss: 3801.0885 - mse: 17713196.0000 - mae: 3801.0886 - val_loss: 3702.8614 - val_mse: 15461218.0000 - val_mae: 3702.8616\n",
            "Epoch 60/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3677.5013 - mse: 16754242.0000 - mae: 3677.5012 - val_loss: 3614.0223 - val_mse: 14722535.0000 - val_mae: 3614.0225\n",
            "Epoch 61/500\n",
            "60/60 [==============================] - 0s 906us/step - loss: 3914.2909 - mse: 19098176.0000 - mae: 3914.2905 - val_loss: 3544.5671 - val_mse: 14172212.0000 - val_mae: 3544.5671\n",
            "Epoch 62/500\n",
            "60/60 [==============================] - 0s 845us/step - loss: 3737.2041 - mse: 17156498.0000 - mae: 3737.2041 - val_loss: 3473.4683 - val_mse: 13630308.0000 - val_mae: 3473.4685\n",
            "Epoch 63/500\n",
            "60/60 [==============================] - 0s 870us/step - loss: 3598.7902 - mse: 16129920.0000 - mae: 3598.7900 - val_loss: 3403.4573 - val_mse: 13119793.0000 - val_mae: 3403.4573\n",
            "Epoch 64/500\n",
            "60/60 [==============================] - 0s 763us/step - loss: 3578.8566 - mse: 16258699.0000 - mae: 3578.8567 - val_loss: 3329.8923 - val_mse: 12606683.0000 - val_mae: 3329.8923\n",
            "Epoch 65/500\n",
            "60/60 [==============================] - 0s 815us/step - loss: 3623.3421 - mse: 16721591.0000 - mae: 3623.3423 - val_loss: 3263.7370 - val_mse: 12167277.0000 - val_mae: 3263.7371\n",
            "Epoch 66/500\n",
            "60/60 [==============================] - 0s 790us/step - loss: 3561.2414 - mse: 15500201.0000 - mae: 3561.2412 - val_loss: 3200.3472 - val_mse: 11764172.0000 - val_mae: 3200.3472\n",
            "Epoch 67/500\n",
            "60/60 [==============================] - 0s 866us/step - loss: 3424.6362 - mse: 15031840.0000 - mae: 3424.6362 - val_loss: 3120.9270 - val_mse: 11283651.0000 - val_mae: 3120.9270\n",
            "Epoch 68/500\n",
            "60/60 [==============================] - 0s 844us/step - loss: 3213.4616 - mse: 13050190.0000 - mae: 3213.4617 - val_loss: 3032.5899 - val_mse: 10781587.0000 - val_mae: 3032.5898\n",
            "Epoch 69/500\n",
            "60/60 [==============================] - 0s 817us/step - loss: 3687.7109 - mse: 17184934.0000 - mae: 3687.7104 - val_loss: 2978.8079 - val_mse: 10496596.0000 - val_mae: 2978.8079\n",
            "Epoch 70/500\n",
            "60/60 [==============================] - 0s 861us/step - loss: 3758.1974 - mse: 17366602.0000 - mae: 3758.1978 - val_loss: 2931.1604 - val_mse: 10253084.0000 - val_mae: 2931.1602\n",
            "Epoch 71/500\n",
            "60/60 [==============================] - 0s 913us/step - loss: 3292.2305 - mse: 13861357.0000 - mae: 3292.2307 - val_loss: 2864.1542 - val_mse: 9913495.0000 - val_mae: 2864.1538\n",
            "Epoch 72/500\n",
            "60/60 [==============================] - 0s 857us/step - loss: 3271.0273 - mse: 15842070.0000 - mae: 3271.0273 - val_loss: 3201.5065 - val_mse: 14740230.0000 - val_mae: 3201.5066\n",
            "Epoch 73/500\n",
            "60/60 [==============================] - 0s 882us/step - loss: 3274.4345 - mse: 15125947.0000 - mae: 3274.4343 - val_loss: 2815.5524 - val_mse: 11423230.0000 - val_mae: 2815.5520\n",
            "Epoch 74/500\n",
            "60/60 [==============================] - 0s 893us/step - loss: 3096.2483 - mse: 13926074.0000 - mae: 3096.2483 - val_loss: 2621.3382 - val_mse: 8639842.0000 - val_mae: 2621.3384\n",
            "Epoch 75/500\n",
            "60/60 [==============================] - 0s 967us/step - loss: 2845.0195 - mse: 11778092.0000 - mae: 2845.0195 - val_loss: 2507.8627 - val_mse: 7781798.5000 - val_mae: 2507.8625\n",
            "Epoch 76/500\n",
            "60/60 [==============================] - 0s 893us/step - loss: 2945.0055 - mse: 12058382.0000 - mae: 2945.0054 - val_loss: 2459.8651 - val_mse: 7802679.0000 - val_mae: 2459.8650\n",
            "Epoch 77/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3067.2257 - mse: 13592789.0000 - mae: 3067.2256 - val_loss: 2350.0520 - val_mse: 7088438.0000 - val_mae: 2350.0520\n",
            "Epoch 78/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2822.0155 - mse: 12476730.0000 - mae: 2822.0154 - val_loss: 2290.4520 - val_mse: 8166464.5000 - val_mae: 2290.4521\n",
            "Epoch 79/500\n",
            "60/60 [==============================] - 0s 898us/step - loss: 2905.1857 - mse: 11518855.0000 - mae: 2905.1860 - val_loss: 2374.4989 - val_mse: 9744503.0000 - val_mae: 2374.4990\n",
            "Epoch 80/500\n",
            "60/60 [==============================] - 0s 966us/step - loss: 2622.2120 - mse: 10261039.0000 - mae: 2622.2119 - val_loss: 2148.3165 - val_mse: 6565702.0000 - val_mae: 2148.3164\n",
            "Epoch 81/500\n",
            "60/60 [==============================] - 0s 922us/step - loss: 2881.9451 - mse: 12443268.0000 - mae: 2881.9448 - val_loss: 2306.5633 - val_mse: 9676166.0000 - val_mae: 2306.5632\n",
            "Epoch 82/500\n",
            "60/60 [==============================] - 0s 888us/step - loss: 2673.3743 - mse: 11140284.0000 - mae: 2673.3743 - val_loss: 2166.6031 - val_mse: 8724363.0000 - val_mae: 2166.6030\n",
            "Epoch 83/500\n",
            "60/60 [==============================] - 0s 872us/step - loss: 2789.2738 - mse: 12514688.0000 - mae: 2789.2739 - val_loss: 2024.5196 - val_mse: 7616815.5000 - val_mae: 2024.5198\n",
            "Epoch 84/500\n",
            "60/60 [==============================] - 0s 875us/step - loss: 2951.5715 - mse: 12111465.0000 - mae: 2951.5715 - val_loss: 2184.8388 - val_mse: 9250673.0000 - val_mae: 2184.8389\n",
            "Epoch 85/500\n",
            "60/60 [==============================] - 0s 883us/step - loss: 2539.9591 - mse: 9559270.0000 - mae: 2539.9592 - val_loss: 1892.7798 - val_mse: 6306303.5000 - val_mae: 1892.7798\n",
            "Epoch 86/500\n",
            "60/60 [==============================] - 0s 942us/step - loss: 2527.3514 - mse: 9539209.0000 - mae: 2527.3513 - val_loss: 1828.3056 - val_mse: 5285875.5000 - val_mae: 1828.3055\n",
            "Epoch 87/500\n",
            "60/60 [==============================] - 0s 929us/step - loss: 2496.8369 - mse: 9800316.0000 - mae: 2496.8367 - val_loss: 1991.7440 - val_mse: 8117310.0000 - val_mae: 1991.7440\n",
            "Epoch 88/500\n",
            "60/60 [==============================] - 0s 911us/step - loss: 2486.1526 - mse: 10109216.0000 - mae: 2486.1526 - val_loss: 2147.3754 - val_mse: 9362608.0000 - val_mae: 2147.3755\n",
            "Epoch 89/500\n",
            "60/60 [==============================] - 0s 897us/step - loss: 2374.3228 - mse: 8696970.0000 - mae: 2374.3230 - val_loss: 1739.0247 - val_mse: 5242434.0000 - val_mae: 1739.0248\n",
            "Epoch 90/500\n",
            "60/60 [==============================] - 0s 909us/step - loss: 2231.8818 - mse: 8303001.5000 - mae: 2231.8818 - val_loss: 1739.9227 - val_mse: 5760954.0000 - val_mae: 1739.9226\n",
            "Epoch 91/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2319.8134 - mse: 8888861.0000 - mae: 2319.8135 - val_loss: 1687.3707 - val_mse: 4557104.0000 - val_mae: 1687.3707\n",
            "Epoch 92/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2444.6931 - mse: 9083436.0000 - mae: 2444.6931 - val_loss: 1737.1943 - val_mse: 5932610.5000 - val_mae: 1737.1945\n",
            "Epoch 93/500\n",
            "60/60 [==============================] - 0s 938us/step - loss: 2387.2613 - mse: 8821254.0000 - mae: 2387.2612 - val_loss: 1757.6188 - val_mse: 6215332.5000 - val_mae: 1757.6189\n",
            "Epoch 94/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2545.5619 - mse: 8905727.0000 - mae: 2545.5620 - val_loss: 1696.2177 - val_mse: 5563936.0000 - val_mae: 1696.2175\n",
            "Epoch 95/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2693.2453 - mse: 10133118.0000 - mae: 2693.2454 - val_loss: 2001.7982 - val_mse: 8217175.5000 - val_mae: 2001.7982\n",
            "Epoch 96/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2397.2657 - mse: 8851657.0000 - mae: 2397.2659 - val_loss: 1647.8549 - val_mse: 4790198.0000 - val_mae: 1647.8549\n",
            "Epoch 97/500\n",
            "60/60 [==============================] - 0s 964us/step - loss: 2542.9688 - mse: 9607443.0000 - mae: 2542.9688 - val_loss: 1713.2608 - val_mse: 5799521.0000 - val_mae: 1713.2610\n",
            "Epoch 98/500\n",
            "60/60 [==============================] - 0s 873us/step - loss: 2286.7208 - mse: 8407041.0000 - mae: 2286.7209 - val_loss: 1972.5907 - val_mse: 7912264.0000 - val_mae: 1972.5906\n",
            "Epoch 99/500\n",
            "60/60 [==============================] - 0s 868us/step - loss: 2287.3881 - mse: 8276234.5000 - mae: 2287.3879 - val_loss: 1892.6749 - val_mse: 7302820.5000 - val_mae: 1892.6750\n",
            "Epoch 100/500\n",
            "60/60 [==============================] - 0s 893us/step - loss: 2422.6630 - mse: 9025826.0000 - mae: 2422.6631 - val_loss: 2128.1360 - val_mse: 9375568.0000 - val_mae: 2128.1360\n",
            "Epoch 101/500\n",
            "60/60 [==============================] - 0s 985us/step - loss: 2441.7673 - mse: 8986309.0000 - mae: 2441.7671 - val_loss: 1822.4401 - val_mse: 7261804.5000 - val_mae: 1822.4401\n",
            "Epoch 102/500\n",
            "60/60 [==============================] - 0s 893us/step - loss: 2306.9791 - mse: 8000813.5000 - mae: 2306.9792 - val_loss: 1708.7326 - val_mse: 6325848.5000 - val_mae: 1708.7325\n",
            "Epoch 103/500\n",
            "60/60 [==============================] - 0s 850us/step - loss: 2386.8478 - mse: 8996869.0000 - mae: 2386.8477 - val_loss: 1963.4079 - val_mse: 8424439.0000 - val_mae: 1963.4081\n",
            "Epoch 104/500\n",
            "60/60 [==============================] - 0s 850us/step - loss: 2259.9490 - mse: 7520578.5000 - mae: 2259.9490 - val_loss: 1546.9627 - val_mse: 4951648.0000 - val_mae: 1546.9628\n",
            "Epoch 105/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2249.2570 - mse: 8013170.0000 - mae: 2249.2571 - val_loss: 2017.7666 - val_mse: 8846789.0000 - val_mae: 2017.7665\n",
            "Epoch 106/500\n",
            "60/60 [==============================] - 0s 846us/step - loss: 2164.4157 - mse: 7031051.5000 - mae: 2164.4158 - val_loss: 1648.2739 - val_mse: 5765212.5000 - val_mae: 1648.2739\n",
            "Epoch 107/500\n",
            "60/60 [==============================] - 0s 865us/step - loss: 2301.5931 - mse: 7703287.5000 - mae: 2301.5933 - val_loss: 2032.8413 - val_mse: 8960548.0000 - val_mae: 2032.8416\n",
            "Epoch 108/500\n",
            "60/60 [==============================] - 0s 893us/step - loss: 2283.2080 - mse: 8035351.5000 - mae: 2283.2080 - val_loss: 2028.8662 - val_mse: 8921733.0000 - val_mae: 2028.8665\n",
            "Epoch 109/500\n",
            "60/60 [==============================] - 0s 905us/step - loss: 2376.7170 - mse: 8076903.0000 - mae: 2376.7170 - val_loss: 1700.9638 - val_mse: 6182665.5000 - val_mae: 1700.9637\n",
            "Epoch 110/500\n",
            "60/60 [==============================] - 0s 870us/step - loss: 2366.5687 - mse: 8339759.5000 - mae: 2366.5688 - val_loss: 1895.0650 - val_mse: 7674670.5000 - val_mae: 1895.0649\n",
            "Epoch 111/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2227.1441 - mse: 7191587.5000 - mae: 2227.1440 - val_loss: 2012.7025 - val_mse: 8820945.0000 - val_mae: 2012.7025\n",
            "Epoch 112/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2334.1014 - mse: 8325374.0000 - mae: 2334.1016 - val_loss: 2013.9663 - val_mse: 9109247.0000 - val_mae: 2013.9664\n",
            "Epoch 113/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2088.6156 - mse: 7378241.0000 - mae: 2088.6157 - val_loss: 1655.7516 - val_mse: 5967273.0000 - val_mae: 1655.7516\n",
            "Epoch 114/500\n",
            "60/60 [==============================] - 0s 970us/step - loss: 2387.6993 - mse: 7792401.5000 - mae: 2387.6992 - val_loss: 1907.8317 - val_mse: 8129630.5000 - val_mae: 1907.8319\n",
            "Epoch 115/500\n",
            "60/60 [==============================] - 0s 898us/step - loss: 2172.3948 - mse: 6379151.5000 - mae: 2172.3948 - val_loss: 1890.3456 - val_mse: 7974889.0000 - val_mae: 1890.3456\n",
            "Epoch 116/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2172.3029 - mse: 6703691.5000 - mae: 2172.3030 - val_loss: 1892.5015 - val_mse: 7924498.0000 - val_mae: 1892.5013\n",
            "Epoch 117/500\n",
            "60/60 [==============================] - 0s 870us/step - loss: 2087.7968 - mse: 7330548.5000 - mae: 2087.7969 - val_loss: 2028.7257 - val_mse: 9076272.0000 - val_mae: 2028.7257\n",
            "Epoch 118/500\n",
            "60/60 [==============================] - 0s 833us/step - loss: 2373.6726 - mse: 9214791.0000 - mae: 2373.6726 - val_loss: 1994.0606 - val_mse: 8956299.0000 - val_mae: 1994.0607\n",
            "Epoch 119/500\n",
            "60/60 [==============================] - 0s 892us/step - loss: 2254.3467 - mse: 8406030.0000 - mae: 2254.3467 - val_loss: 1929.8283 - val_mse: 8407213.0000 - val_mae: 1929.8284\n",
            "Epoch 120/500\n",
            "60/60 [==============================] - 0s 841us/step - loss: 2226.8785 - mse: 7411617.5000 - mae: 2226.8784 - val_loss: 1909.5642 - val_mse: 8222994.0000 - val_mae: 1909.5641\n",
            "Epoch 121/500\n",
            "60/60 [==============================] - 0s 840us/step - loss: 2279.3936 - mse: 7386759.5000 - mae: 2279.3938 - val_loss: 1975.7962 - val_mse: 8764050.0000 - val_mae: 1975.7964\n",
            "Epoch 122/500\n",
            "60/60 [==============================] - 0s 840us/step - loss: 1855.6727 - mse: 5327051.0000 - mae: 1855.6726 - val_loss: 1689.3168 - val_mse: 6384223.0000 - val_mae: 1689.3168\n",
            "Epoch 123/500\n",
            "60/60 [==============================] - 0s 990us/step - loss: 2142.6844 - mse: 6902879.0000 - mae: 2142.6843 - val_loss: 1848.4906 - val_mse: 7679715.5000 - val_mae: 1848.4905\n",
            "Epoch 124/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2195.3843 - mse: 7111192.0000 - mae: 2195.3843 - val_loss: 1974.0454 - val_mse: 8895937.0000 - val_mae: 1974.0454\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 80 samples, validate on 70 samples\n",
            "Epoch 1/500\n",
            "80/80 [==============================] - 2s 28ms/step - loss: 6107.8432 - mse: 46142368.0000 - mae: 6107.8428 - val_loss: 6436.8198 - val_mse: 48043236.0000 - val_mae: 6436.8198\n",
            "Epoch 2/500\n",
            "80/80 [==============================] - 0s 933us/step - loss: 6104.7318 - mse: 46106728.0000 - mae: 6104.7314 - val_loss: 6427.7040 - val_mse: 47924652.0000 - val_mae: 6427.7036\n",
            "Epoch 3/500\n",
            "80/80 [==============================] - 0s 842us/step - loss: 6088.8512 - mse: 45910248.0000 - mae: 6088.8516 - val_loss: 6403.2186 - val_mse: 47603528.0000 - val_mae: 6403.2188\n",
            "Epoch 4/500\n",
            "80/80 [==============================] - 0s 821us/step - loss: 6056.9495 - mse: 45507192.0000 - mae: 6056.9497 - val_loss: 6360.6671 - val_mse: 47052184.0000 - val_mae: 6360.6670\n",
            "Epoch 5/500\n",
            "80/80 [==============================] - 0s 898us/step - loss: 6012.7267 - mse: 44942052.0000 - mae: 6012.7266 - val_loss: 6309.2317 - val_mse: 46393296.0000 - val_mae: 6309.2319\n",
            "Epoch 6/500\n",
            "80/80 [==============================] - 0s 852us/step - loss: 5955.2362 - mse: 44145332.0000 - mae: 5955.2363 - val_loss: 6250.5291 - val_mse: 45649720.0000 - val_mae: 6250.5288\n",
            "Epoch 7/500\n",
            "80/80 [==============================] - 0s 872us/step - loss: 5921.1530 - mse: 43660520.0000 - mae: 5921.1528 - val_loss: 6193.4652 - val_mse: 44935600.0000 - val_mae: 6193.4648\n",
            "Epoch 8/500\n",
            "80/80 [==============================] - 0s 878us/step - loss: 5855.9155 - mse: 42862712.0000 - mae: 5855.9150 - val_loss: 6121.2015 - val_mse: 44040680.0000 - val_mae: 6121.2012\n",
            "Epoch 9/500\n",
            "80/80 [==============================] - 0s 846us/step - loss: 5819.3427 - mse: 42247260.0000 - mae: 5819.3428 - val_loss: 6050.9715 - val_mse: 43181500.0000 - val_mae: 6050.9712\n",
            "Epoch 10/500\n",
            "80/80 [==============================] - 0s 899us/step - loss: 5745.1713 - mse: 41189344.0000 - mae: 5745.1709 - val_loss: 5965.9662 - val_mse: 42155184.0000 - val_mae: 5965.9663\n",
            "Epoch 11/500\n",
            "80/80 [==============================] - 0s 899us/step - loss: 5656.8893 - mse: 40040612.0000 - mae: 5656.8887 - val_loss: 5870.0131 - val_mse: 41010116.0000 - val_mae: 5870.0132\n",
            "Epoch 12/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5606.0494 - mse: 39261240.0000 - mae: 5606.0493 - val_loss: 5776.0847 - val_mse: 39871344.0000 - val_mae: 5776.0850\n",
            "Epoch 13/500\n",
            "80/80 [==============================] - 0s 992us/step - loss: 5500.0184 - mse: 37702976.0000 - mae: 5500.0186 - val_loss: 5671.2171 - val_mse: 38586228.0000 - val_mae: 5671.2168\n",
            "Epoch 14/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5433.3321 - mse: 36700248.0000 - mae: 5433.3325 - val_loss: 5566.1264 - val_mse: 37288024.0000 - val_mae: 5566.1260\n",
            "Epoch 15/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5357.1634 - mse: 35909912.0000 - mae: 5357.1631 - val_loss: 5461.2064 - val_mse: 35979636.0000 - val_mae: 5461.2065\n",
            "Epoch 16/500\n",
            "80/80 [==============================] - 0s 903us/step - loss: 5213.1897 - mse: 33780904.0000 - mae: 5213.1895 - val_loss: 5345.0548 - val_mse: 34523416.0000 - val_mae: 5345.0547\n",
            "Epoch 17/500\n",
            "80/80 [==============================] - 0s 929us/step - loss: 5182.7808 - mse: 33251194.0000 - mae: 5182.7803 - val_loss: 5230.5065 - val_mse: 33088694.0000 - val_mae: 5230.5068\n",
            "Epoch 18/500\n",
            "80/80 [==============================] - 0s 907us/step - loss: 5032.1840 - mse: 31445280.0000 - mae: 5032.1841 - val_loss: 5106.6063 - val_mse: 31549550.0000 - val_mae: 5106.6064\n",
            "Epoch 19/500\n",
            "80/80 [==============================] - 0s 879us/step - loss: 4983.6829 - mse: 30747148.0000 - mae: 4983.6826 - val_loss: 4987.3954 - val_mse: 30080816.0000 - val_mae: 4987.3955\n",
            "Epoch 20/500\n",
            "80/80 [==============================] - 0s 840us/step - loss: 4852.0536 - mse: 29198640.0000 - mae: 4852.0537 - val_loss: 4862.6889 - val_mse: 28548402.0000 - val_mae: 4862.6890\n",
            "Epoch 21/500\n",
            "80/80 [==============================] - 0s 894us/step - loss: 4676.0798 - mse: 27348532.0000 - mae: 4676.0796 - val_loss: 4728.7001 - val_mse: 26962012.0000 - val_mae: 4728.7007\n",
            "Epoch 22/500\n",
            "80/80 [==============================] - 0s 785us/step - loss: 4630.8508 - mse: 26424232.0000 - mae: 4630.8506 - val_loss: 4595.3251 - val_mse: 25406788.0000 - val_mae: 4595.3257\n",
            "Epoch 23/500\n",
            "80/80 [==============================] - 0s 815us/step - loss: 4373.8683 - mse: 23847776.0000 - mae: 4373.8682 - val_loss: 4457.8806 - val_mse: 23799106.0000 - val_mae: 4457.8804\n",
            "Epoch 24/500\n",
            "80/80 [==============================] - 0s 862us/step - loss: 4281.0036 - mse: 22998164.0000 - mae: 4281.0034 - val_loss: 4315.6155 - val_mse: 22193462.0000 - val_mae: 4315.6157\n",
            "Epoch 25/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4188.1770 - mse: 21981020.0000 - mae: 4188.1768 - val_loss: 4171.1322 - val_mse: 20639310.0000 - val_mae: 4171.1323\n",
            "Epoch 26/500\n",
            "80/80 [==============================] - 0s 981us/step - loss: 3989.6675 - mse: 20480092.0000 - mae: 3989.6675 - val_loss: 4034.4463 - val_mse: 19244832.0000 - val_mae: 4034.4465\n",
            "Epoch 27/500\n",
            "80/80 [==============================] - 0s 944us/step - loss: 3750.0872 - mse: 18976820.0000 - mae: 3750.0874 - val_loss: 3880.5280 - val_mse: 17758060.0000 - val_mae: 3880.5276\n",
            "Epoch 28/500\n",
            "80/80 [==============================] - 0s 811us/step - loss: 3667.5301 - mse: 17504916.0000 - mae: 3667.5300 - val_loss: 3737.1614 - val_mse: 16457552.0000 - val_mae: 3737.1611\n",
            "Epoch 29/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 3436.4054 - mse: 15561373.0000 - mae: 3436.4055 - val_loss: 3580.5164 - val_mse: 15127610.0000 - val_mae: 3580.5166\n",
            "Epoch 30/500\n",
            "80/80 [==============================] - 0s 890us/step - loss: 3346.4097 - mse: 15334568.0000 - mae: 3346.4097 - val_loss: 3424.9075 - val_mse: 13902000.0000 - val_mae: 3424.9075\n",
            "Epoch 31/500\n",
            "80/80 [==============================] - 0s 857us/step - loss: 3309.2371 - mse: 14939168.0000 - mae: 3309.2371 - val_loss: 3307.2494 - val_mse: 13038903.0000 - val_mae: 3307.2490\n",
            "Epoch 32/500\n",
            "80/80 [==============================] - 0s 890us/step - loss: 2955.5950 - mse: 11988502.0000 - mae: 2955.5950 - val_loss: 3129.9451 - val_mse: 11843045.0000 - val_mae: 3129.9448\n",
            "Epoch 33/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 3073.1572 - mse: 13258798.0000 - mae: 3073.1575 - val_loss: 2988.5455 - val_mse: 10977892.0000 - val_mae: 2988.5457\n",
            "Epoch 34/500\n",
            "80/80 [==============================] - 0s 882us/step - loss: 2984.1869 - mse: 13067031.0000 - mae: 2984.1870 - val_loss: 2869.0090 - val_mse: 10305513.0000 - val_mae: 2869.0090\n",
            "Epoch 35/500\n",
            "80/80 [==============================] - 0s 898us/step - loss: 3017.2458 - mse: 12824718.0000 - mae: 3017.2458 - val_loss: 2768.2597 - val_mse: 9780807.0000 - val_mae: 2768.2598\n",
            "Epoch 36/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2855.2108 - mse: 11216066.0000 - mae: 2855.2112 - val_loss: 2691.4016 - val_mse: 9405534.0000 - val_mae: 2691.4016\n",
            "Epoch 37/500\n",
            "80/80 [==============================] - 0s 900us/step - loss: 2473.7449 - mse: 9278771.0000 - mae: 2473.7449 - val_loss: 2558.3623 - val_mse: 8811570.0000 - val_mae: 2558.3623\n",
            "Epoch 38/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2674.7474 - mse: 10200597.0000 - mae: 2674.7476 - val_loss: 2444.2094 - val_mse: 8355969.0000 - val_mae: 2444.2092\n",
            "Epoch 39/500\n",
            "80/80 [==============================] - 0s 957us/step - loss: 2630.4601 - mse: 10674860.0000 - mae: 2630.4602 - val_loss: 2401.9456 - val_mse: 8198850.5000 - val_mae: 2401.9458\n",
            "Epoch 40/500\n",
            "80/80 [==============================] - 0s 850us/step - loss: 2958.6422 - mse: 12341737.0000 - mae: 2958.6423 - val_loss: 2345.1474 - val_mse: 7995759.5000 - val_mae: 2345.1472\n",
            "Epoch 41/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2875.4034 - mse: 11631737.0000 - mae: 2875.4033 - val_loss: 2272.9327 - val_mse: 7663251.5000 - val_mae: 2272.9326\n",
            "Epoch 42/500\n",
            "80/80 [==============================] - 0s 867us/step - loss: 2636.6788 - mse: 11086202.0000 - mae: 2636.6787 - val_loss: 2240.6980 - val_mse: 7331639.5000 - val_mae: 2240.6980\n",
            "Epoch 43/500\n",
            "80/80 [==============================] - 0s 912us/step - loss: 2629.4724 - mse: 10706340.0000 - mae: 2629.4724 - val_loss: 2262.0276 - val_mse: 7450082.5000 - val_mae: 2262.0273\n",
            "Epoch 44/500\n",
            "80/80 [==============================] - 0s 940us/step - loss: 2509.5583 - mse: 10289050.0000 - mae: 2509.5581 - val_loss: 2280.2046 - val_mse: 8043038.0000 - val_mae: 2280.2046\n",
            "Epoch 45/500\n",
            "80/80 [==============================] - 0s 822us/step - loss: 2363.8009 - mse: 9371882.0000 - mae: 2363.8010 - val_loss: 2266.7995 - val_mse: 8856895.0000 - val_mae: 2266.7996\n",
            "Epoch 46/500\n",
            "80/80 [==============================] - 0s 990us/step - loss: 2604.8653 - mse: 10451130.0000 - mae: 2604.8652 - val_loss: 2180.4614 - val_mse: 7169127.0000 - val_mae: 2180.4614\n",
            "Epoch 47/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2406.6108 - mse: 10280235.0000 - mae: 2406.6108 - val_loss: 2170.0927 - val_mse: 7252503.0000 - val_mae: 2170.0928\n",
            "Epoch 48/500\n",
            "80/80 [==============================] - 0s 881us/step - loss: 2455.0700 - mse: 10215828.0000 - mae: 2455.0698 - val_loss: 2207.6596 - val_mse: 8573481.0000 - val_mae: 2207.6599\n",
            "Epoch 49/500\n",
            "80/80 [==============================] - 0s 833us/step - loss: 2569.4955 - mse: 11038153.0000 - mae: 2569.4956 - val_loss: 2186.0615 - val_mse: 8627917.0000 - val_mae: 2186.0613\n",
            "Epoch 50/500\n",
            "80/80 [==============================] - 0s 905us/step - loss: 2501.0864 - mse: 10060925.0000 - mae: 2501.0864 - val_loss: 2119.2870 - val_mse: 7701702.5000 - val_mae: 2119.2871\n",
            "Epoch 51/500\n",
            "80/80 [==============================] - 0s 968us/step - loss: 2361.2800 - mse: 8907267.0000 - mae: 2361.2800 - val_loss: 2031.5928 - val_mse: 6492451.0000 - val_mae: 2031.5931\n",
            "Epoch 52/500\n",
            "80/80 [==============================] - 0s 827us/step - loss: 2396.3320 - mse: 9034074.0000 - mae: 2396.3320 - val_loss: 2024.2868 - val_mse: 6642380.0000 - val_mae: 2024.2871\n",
            "Epoch 53/500\n",
            "80/80 [==============================] - 0s 829us/step - loss: 2168.0816 - mse: 8332231.0000 - mae: 2168.0815 - val_loss: 1977.5659 - val_mse: 6121017.0000 - val_mae: 1977.5657\n",
            "Epoch 54/500\n",
            "80/80 [==============================] - 0s 933us/step - loss: 2293.8852 - mse: 8748707.0000 - mae: 2293.8853 - val_loss: 1914.9470 - val_mse: 5568540.0000 - val_mae: 1914.9469\n",
            "Epoch 55/500\n",
            "80/80 [==============================] - 0s 951us/step - loss: 2363.3987 - mse: 8721170.0000 - mae: 2363.3987 - val_loss: 2013.5839 - val_mse: 8053933.0000 - val_mae: 2013.5841\n",
            "Epoch 56/500\n",
            "80/80 [==============================] - 0s 926us/step - loss: 2355.2026 - mse: 9543046.0000 - mae: 2355.2026 - val_loss: 2246.9266 - val_mse: 10687685.0000 - val_mae: 2246.9263\n",
            "Epoch 57/500\n",
            "80/80 [==============================] - 0s 854us/step - loss: 2270.3311 - mse: 9702754.0000 - mae: 2270.3311 - val_loss: 1791.8686 - val_mse: 5968976.0000 - val_mae: 1791.8687\n",
            "Epoch 58/500\n",
            "80/80 [==============================] - 0s 816us/step - loss: 2165.8829 - mse: 8461982.0000 - mae: 2165.8828 - val_loss: 1788.7967 - val_mse: 4914028.0000 - val_mae: 1788.7966\n",
            "Epoch 59/500\n",
            "80/80 [==============================] - 0s 946us/step - loss: 2374.9802 - mse: 9392298.0000 - mae: 2374.9802 - val_loss: 1718.4276 - val_mse: 5412689.0000 - val_mae: 1718.4279\n",
            "Epoch 60/500\n",
            "80/80 [==============================] - 0s 913us/step - loss: 2204.7871 - mse: 8509523.0000 - mae: 2204.7871 - val_loss: 1700.9070 - val_mse: 5212403.0000 - val_mae: 1700.9070\n",
            "Epoch 61/500\n",
            "80/80 [==============================] - 0s 884us/step - loss: 2213.9688 - mse: 8827278.0000 - mae: 2213.9688 - val_loss: 1750.8403 - val_mse: 6886524.5000 - val_mae: 1750.8405\n",
            "Epoch 62/500\n",
            "80/80 [==============================] - 0s 909us/step - loss: 2276.0874 - mse: 8992551.0000 - mae: 2276.0874 - val_loss: 1913.6485 - val_mse: 8041339.5000 - val_mae: 1913.6484\n",
            "Epoch 63/500\n",
            "80/80 [==============================] - 0s 862us/step - loss: 2228.3402 - mse: 8451078.0000 - mae: 2228.3403 - val_loss: 1657.9093 - val_mse: 5153821.0000 - val_mae: 1657.9094\n",
            "Epoch 64/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2124.6392 - mse: 7803707.0000 - mae: 2124.6392 - val_loss: 1591.4131 - val_mse: 5109605.0000 - val_mae: 1591.4132\n",
            "Epoch 65/500\n",
            "80/80 [==============================] - 0s 880us/step - loss: 2140.4484 - mse: 7041606.5000 - mae: 2140.4485 - val_loss: 1619.6548 - val_mse: 5430202.0000 - val_mae: 1619.6547\n",
            "Epoch 66/500\n",
            "80/80 [==============================] - 0s 836us/step - loss: 2090.4578 - mse: 8040679.0000 - mae: 2090.4575 - val_loss: 1671.7208 - val_mse: 6136130.5000 - val_mae: 1671.7208\n",
            "Epoch 67/500\n",
            "80/80 [==============================] - 0s 815us/step - loss: 2070.0379 - mse: 7872137.5000 - mae: 2070.0381 - val_loss: 1687.2540 - val_mse: 6512050.5000 - val_mae: 1687.2539\n",
            "Epoch 68/500\n",
            "80/80 [==============================] - 0s 989us/step - loss: 2224.2343 - mse: 8348306.5000 - mae: 2224.2344 - val_loss: 1662.8851 - val_mse: 5906011.5000 - val_mae: 1662.8851\n",
            "Epoch 69/500\n",
            "80/80 [==============================] - 0s 958us/step - loss: 2074.8539 - mse: 7366533.0000 - mae: 2074.8540 - val_loss: 1572.0691 - val_mse: 5202455.0000 - val_mae: 1572.0690\n",
            "Epoch 70/500\n",
            "80/80 [==============================] - 0s 865us/step - loss: 2072.5022 - mse: 7820026.5000 - mae: 2072.5022 - val_loss: 1717.2975 - val_mse: 7408898.5000 - val_mae: 1717.2975\n",
            "Epoch 71/500\n",
            "80/80 [==============================] - 0s 882us/step - loss: 2028.8156 - mse: 7489597.5000 - mae: 2028.8157 - val_loss: 1534.6454 - val_mse: 5526727.0000 - val_mae: 1534.6454\n",
            "Epoch 72/500\n",
            "80/80 [==============================] - 0s 898us/step - loss: 2083.4343 - mse: 7094853.5000 - mae: 2083.4343 - val_loss: 1549.4629 - val_mse: 5022320.0000 - val_mae: 1549.4629\n",
            "Epoch 73/500\n",
            "80/80 [==============================] - 0s 845us/step - loss: 2198.4837 - mse: 7975596.0000 - mae: 2198.4836 - val_loss: 1914.9193 - val_mse: 8787189.0000 - val_mae: 1914.9192\n",
            "Epoch 74/500\n",
            "80/80 [==============================] - 0s 879us/step - loss: 2179.6301 - mse: 8399973.0000 - mae: 2179.6299 - val_loss: 1744.2489 - val_mse: 6730579.0000 - val_mae: 1744.2489\n",
            "Epoch 75/500\n",
            "80/80 [==============================] - 0s 904us/step - loss: 2158.9932 - mse: 7786549.0000 - mae: 2158.9929 - val_loss: 1772.3363 - val_mse: 6808036.5000 - val_mae: 1772.3362\n",
            "Epoch 76/500\n",
            "80/80 [==============================] - 0s 857us/step - loss: 2123.2636 - mse: 7563755.0000 - mae: 2123.2637 - val_loss: 1546.9054 - val_mse: 5170957.0000 - val_mae: 1546.9055\n",
            "Epoch 77/500\n",
            "80/80 [==============================] - 0s 942us/step - loss: 1943.6616 - mse: 6559783.0000 - mae: 1943.6615 - val_loss: 1569.7179 - val_mse: 5630711.0000 - val_mae: 1569.7179\n",
            "Epoch 78/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2030.0581 - mse: 7087956.0000 - mae: 2030.0580 - val_loss: 1842.0335 - val_mse: 7941755.5000 - val_mae: 1842.0334\n",
            "Epoch 79/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1948.3642 - mse: 6863061.0000 - mae: 1948.3640 - val_loss: 1745.7913 - val_mse: 6989430.5000 - val_mae: 1745.7914\n",
            "Epoch 80/500\n",
            "80/80 [==============================] - 0s 847us/step - loss: 2176.3423 - mse: 8050443.0000 - mae: 2176.3423 - val_loss: 1591.2277 - val_mse: 4975902.5000 - val_mae: 1591.2277\n",
            "Epoch 81/500\n",
            "80/80 [==============================] - 0s 831us/step - loss: 1999.2119 - mse: 7155954.5000 - mae: 1999.2117 - val_loss: 1516.0489 - val_mse: 4834879.0000 - val_mae: 1516.0488\n",
            "Epoch 82/500\n",
            "80/80 [==============================] - 0s 847us/step - loss: 2031.8345 - mse: 7462917.5000 - mae: 2031.8346 - val_loss: 1922.2419 - val_mse: 8765165.0000 - val_mae: 1922.2419\n",
            "Epoch 83/500\n",
            "80/80 [==============================] - 0s 813us/step - loss: 1779.7160 - mse: 5789295.0000 - mae: 1779.7161 - val_loss: 1945.3682 - val_mse: 8373831.5000 - val_mae: 1945.3680\n",
            "Epoch 84/500\n",
            "80/80 [==============================] - 0s 900us/step - loss: 2027.2017 - mse: 7171055.0000 - mae: 2027.2015 - val_loss: 1724.1681 - val_mse: 6859619.5000 - val_mae: 1724.1681\n",
            "Epoch 85/500\n",
            "80/80 [==============================] - 0s 940us/step - loss: 1921.0247 - mse: 6633370.0000 - mae: 1921.0247 - val_loss: 1734.6147 - val_mse: 6614786.0000 - val_mae: 1734.6147\n",
            "Epoch 86/500\n",
            "80/80 [==============================] - 0s 898us/step - loss: 2049.8392 - mse: 7316141.5000 - mae: 2049.8391 - val_loss: 1519.2639 - val_mse: 5031624.5000 - val_mae: 1519.2638\n",
            "Epoch 87/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1975.1619 - mse: 6977587.0000 - mae: 1975.1619 - val_loss: 1769.0565 - val_mse: 7284866.5000 - val_mae: 1769.0566\n",
            "Epoch 88/500\n",
            "80/80 [==============================] - 0s 938us/step - loss: 1765.4109 - mse: 5215948.5000 - mae: 1765.4109 - val_loss: 1898.7533 - val_mse: 8542222.0000 - val_mae: 1898.7533\n",
            "Epoch 89/500\n",
            "80/80 [==============================] - 0s 881us/step - loss: 2090.0101 - mse: 7897961.5000 - mae: 2090.0100 - val_loss: 1901.2441 - val_mse: 8659584.0000 - val_mae: 1901.2441\n",
            "Epoch 90/500\n",
            "80/80 [==============================] - 0s 994us/step - loss: 2019.3744 - mse: 7001908.0000 - mae: 2019.3744 - val_loss: 1481.2811 - val_mse: 5056319.0000 - val_mae: 1481.2811\n",
            "Epoch 91/500\n",
            "80/80 [==============================] - 0s 872us/step - loss: 1991.4070 - mse: 6848915.0000 - mae: 1991.4069 - val_loss: 1885.7639 - val_mse: 8317128.0000 - val_mae: 1885.7638\n",
            "Epoch 92/500\n",
            "80/80 [==============================] - 0s 822us/step - loss: 2076.4174 - mse: 7154157.0000 - mae: 2076.4175 - val_loss: 1617.5922 - val_mse: 6154243.5000 - val_mae: 1617.5920\n",
            "Epoch 93/500\n",
            "80/80 [==============================] - 0s 867us/step - loss: 1878.2372 - mse: 6372970.0000 - mae: 1878.2371 - val_loss: 1638.6510 - val_mse: 5972424.5000 - val_mae: 1638.6511\n",
            "Epoch 94/500\n",
            "80/80 [==============================] - 0s 809us/step - loss: 2133.4775 - mse: 7786694.5000 - mae: 2133.4773 - val_loss: 1667.8436 - val_mse: 6480816.5000 - val_mae: 1667.8438\n",
            "Epoch 95/500\n",
            "80/80 [==============================] - 0s 887us/step - loss: 2092.4861 - mse: 7467145.5000 - mae: 2092.4861 - val_loss: 1868.4338 - val_mse: 8024490.0000 - val_mae: 1868.4338\n",
            "Epoch 96/500\n",
            "80/80 [==============================] - 0s 807us/step - loss: 2128.5770 - mse: 6988543.0000 - mae: 2128.5769 - val_loss: 2002.9940 - val_mse: 9024031.0000 - val_mae: 2002.9938\n",
            "Epoch 97/500\n",
            "80/80 [==============================] - 0s 892us/step - loss: 1964.1064 - mse: 7872765.0000 - mae: 1964.1062 - val_loss: 1728.8224 - val_mse: 6877585.0000 - val_mae: 1728.8224\n",
            "Epoch 98/500\n",
            "80/80 [==============================] - 0s 872us/step - loss: 1876.6288 - mse: 6085689.0000 - mae: 1876.6287 - val_loss: 1785.9134 - val_mse: 7258352.5000 - val_mae: 1785.9133\n",
            "Epoch 99/500\n",
            "80/80 [==============================] - 0s 925us/step - loss: 2062.3246 - mse: 7029235.0000 - mae: 2062.3247 - val_loss: 1878.1396 - val_mse: 7893003.0000 - val_mae: 1878.1395\n",
            "Epoch 100/500\n",
            "80/80 [==============================] - 0s 974us/step - loss: 1812.7864 - mse: 5691488.5000 - mae: 1812.7865 - val_loss: 1688.4873 - val_mse: 6577736.5000 - val_mae: 1688.4874\n",
            "Epoch 101/500\n",
            "80/80 [==============================] - 0s 815us/step - loss: 2001.9966 - mse: 6629314.5000 - mae: 2001.9967 - val_loss: 1870.9969 - val_mse: 8030280.0000 - val_mae: 1870.9968\n",
            "Epoch 102/500\n",
            "80/80 [==============================] - 0s 841us/step - loss: 2282.7890 - mse: 8470022.0000 - mae: 2282.7891 - val_loss: 1786.5479 - val_mse: 7330306.5000 - val_mae: 1786.5480\n",
            "Epoch 103/500\n",
            "80/80 [==============================] - 0s 819us/step - loss: 1994.9259 - mse: 5641368.0000 - mae: 1994.9260 - val_loss: 1782.3995 - val_mse: 7525875.0000 - val_mae: 1782.3994\n",
            "Epoch 104/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1779.2297 - mse: 6072402.0000 - mae: 1779.2297 - val_loss: 1874.4960 - val_mse: 8337040.5000 - val_mae: 1874.4960\n",
            "Epoch 105/500\n",
            "80/80 [==============================] - 0s 895us/step - loss: 1949.0621 - mse: 6922116.0000 - mae: 1949.0621 - val_loss: 1893.9085 - val_mse: 8399088.0000 - val_mae: 1893.9084\n",
            "Epoch 106/500\n",
            "80/80 [==============================] - 0s 902us/step - loss: 2076.8070 - mse: 7019119.0000 - mae: 2076.8071 - val_loss: 1819.2233 - val_mse: 7566450.5000 - val_mae: 1819.2234\n",
            "Epoch 107/500\n",
            "80/80 [==============================] - 0s 881us/step - loss: 1900.5593 - mse: 5846187.0000 - mae: 1900.5593 - val_loss: 1898.6171 - val_mse: 8305277.5000 - val_mae: 1898.6172\n",
            "Epoch 108/500\n",
            "80/80 [==============================] - 0s 891us/step - loss: 1695.2041 - mse: 4703280.0000 - mae: 1695.2041 - val_loss: 1818.9238 - val_mse: 7931960.0000 - val_mae: 1818.9238\n",
            "Epoch 109/500\n",
            "80/80 [==============================] - 0s 803us/step - loss: 1936.6365 - mse: 6436490.5000 - mae: 1936.6365 - val_loss: 1805.9461 - val_mse: 7925273.5000 - val_mae: 1805.9462\n",
            "Epoch 110/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1954.5476 - mse: 6418115.0000 - mae: 1954.5476 - val_loss: 1884.8319 - val_mse: 8725863.0000 - val_mae: 1884.8319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 90 samples, validate on 90 samples\n",
            "Epoch 1/500\n",
            "90/90 [==============================] - 2s 20ms/step - loss: 5993.4473 - mse: 43882684.0000 - mae: 5993.4473 - val_loss: 6163.6641 - val_mse: 43432948.0000 - val_mae: 6163.6641\n",
            "Epoch 2/500\n",
            "90/90 [==============================] - 0s 798us/step - loss: 5992.1033 - mse: 43866216.0000 - mae: 5992.1035 - val_loss: 6159.1347 - val_mse: 43377300.0000 - val_mae: 6159.1348\n",
            "Epoch 3/500\n",
            "90/90 [==============================] - 0s 838us/step - loss: 5979.9320 - mse: 43725544.0000 - mae: 5979.9321 - val_loss: 6136.5203 - val_mse: 43098888.0000 - val_mae: 6136.5200\n",
            "Epoch 4/500\n",
            "90/90 [==============================] - 0s 835us/step - loss: 5948.1702 - mse: 43317800.0000 - mae: 5948.1709 - val_loss: 6100.6574 - val_mse: 42658860.0000 - val_mae: 6100.6577\n",
            "Epoch 5/500\n",
            "90/90 [==============================] - 0s 807us/step - loss: 5906.1573 - mse: 42832400.0000 - mae: 5906.1567 - val_loss: 6051.9726 - val_mse: 42066348.0000 - val_mae: 6051.9731\n",
            "Epoch 6/500\n",
            "90/90 [==============================] - 0s 866us/step - loss: 5860.6833 - mse: 42273364.0000 - mae: 5860.6831 - val_loss: 5991.0868 - val_mse: 41331804.0000 - val_mae: 5991.0869\n",
            "Epoch 7/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5792.2132 - mse: 41418232.0000 - mae: 5792.2134 - val_loss: 5919.0180 - val_mse: 40472676.0000 - val_mae: 5919.0181\n",
            "Epoch 8/500\n",
            "90/90 [==============================] - 0s 835us/step - loss: 5733.4510 - mse: 40647128.0000 - mae: 5733.4512 - val_loss: 5840.6810 - val_mse: 39550948.0000 - val_mae: 5840.6812\n",
            "Epoch 9/500\n",
            "90/90 [==============================] - 0s 763us/step - loss: 5641.7544 - mse: 39554020.0000 - mae: 5641.7544 - val_loss: 5747.3115 - val_mse: 38468324.0000 - val_mae: 5747.3110\n",
            "Epoch 10/500\n",
            "90/90 [==============================] - 0s 946us/step - loss: 5572.6280 - mse: 38519448.0000 - mae: 5572.6279 - val_loss: 5655.4789 - val_mse: 37420992.0000 - val_mae: 5655.4790\n",
            "Epoch 11/500\n",
            "90/90 [==============================] - 0s 839us/step - loss: 5498.1737 - mse: 37644904.0000 - mae: 5498.1738 - val_loss: 5548.7842 - val_mse: 36224772.0000 - val_mae: 5548.7842\n",
            "Epoch 12/500\n",
            "90/90 [==============================] - 0s 937us/step - loss: 5409.2549 - mse: 36354184.0000 - mae: 5409.2554 - val_loss: 5434.0278 - val_mse: 34930488.0000 - val_mae: 5434.0273\n",
            "Epoch 13/500\n",
            "90/90 [==============================] - 0s 915us/step - loss: 5322.5369 - mse: 35023904.0000 - mae: 5322.5366 - val_loss: 5314.6715 - val_mse: 33579684.0000 - val_mae: 5314.6719\n",
            "Epoch 14/500\n",
            "90/90 [==============================] - 0s 838us/step - loss: 5249.3568 - mse: 33977064.0000 - mae: 5249.3564 - val_loss: 5194.9664 - val_mse: 32226702.0000 - val_mae: 5194.9668\n",
            "Epoch 15/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 5124.5771 - mse: 32388690.0000 - mae: 5124.5771 - val_loss: 5065.1898 - val_mse: 30759362.0000 - val_mae: 5065.1895\n",
            "Epoch 16/500\n",
            "90/90 [==============================] - 0s 800us/step - loss: 4941.7278 - mse: 30459370.0000 - mae: 4941.7275 - val_loss: 4919.9690 - val_mse: 29124206.0000 - val_mae: 4919.9692\n",
            "Epoch 17/500\n",
            "90/90 [==============================] - 0s 983us/step - loss: 4796.6289 - mse: 28756094.0000 - mae: 4796.6289 - val_loss: 4768.4567 - val_mse: 27438928.0000 - val_mae: 4768.4565\n",
            "Epoch 18/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 4570.3289 - mse: 26574072.0000 - mae: 4570.3291 - val_loss: 4607.7531 - val_mse: 25682922.0000 - val_mae: 4607.7529\n",
            "Epoch 19/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 4528.8221 - mse: 25713514.0000 - mae: 4528.8218 - val_loss: 4450.0280 - val_mse: 23993898.0000 - val_mae: 4450.0278\n",
            "Epoch 20/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 4443.1268 - mse: 24854940.0000 - mae: 4443.1270 - val_loss: 4294.3305 - val_mse: 22367416.0000 - val_mae: 4294.3301\n",
            "Epoch 21/500\n",
            "90/90 [==============================] - 0s 871us/step - loss: 4256.3093 - mse: 23143966.0000 - mae: 4256.3096 - val_loss: 4136.2195 - val_mse: 20760662.0000 - val_mae: 4136.2192\n",
            "Epoch 22/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 4039.9692 - mse: 21185012.0000 - mae: 4039.9690 - val_loss: 3972.9681 - val_mse: 19162616.0000 - val_mae: 3972.9680\n",
            "Epoch 23/500\n",
            "90/90 [==============================] - 0s 954us/step - loss: 3964.0794 - mse: 20460442.0000 - mae: 3964.0796 - val_loss: 3809.7385 - val_mse: 17652700.0000 - val_mae: 3809.7383\n",
            "Epoch 24/500\n",
            "90/90 [==============================] - 0s 927us/step - loss: 3769.0304 - mse: 18544318.0000 - mae: 3769.0305 - val_loss: 3639.1348 - val_mse: 16168114.0000 - val_mae: 3639.1348\n",
            "Epoch 25/500\n",
            "90/90 [==============================] - 0s 924us/step - loss: 3535.6003 - mse: 16503132.0000 - mae: 3535.6001 - val_loss: 3452.2588 - val_mse: 14651971.0000 - val_mae: 3452.2588\n",
            "Epoch 26/500\n",
            "90/90 [==============================] - 0s 804us/step - loss: 3384.6575 - mse: 14772389.0000 - mae: 3384.6577 - val_loss: 3260.8367 - val_mse: 13218495.0000 - val_mae: 3260.8364\n",
            "Epoch 27/500\n",
            "90/90 [==============================] - 0s 750us/step - loss: 3267.9093 - mse: 15254086.0000 - mae: 3267.9094 - val_loss: 3072.9387 - val_mse: 11929761.0000 - val_mae: 3072.9390\n",
            "Epoch 28/500\n",
            "90/90 [==============================] - 0s 895us/step - loss: 3136.3162 - mse: 14418061.0000 - mae: 3136.3159 - val_loss: 2903.8419 - val_mse: 10869342.0000 - val_mae: 2903.8420\n",
            "Epoch 29/500\n",
            "90/90 [==============================] - 0s 969us/step - loss: 2936.2331 - mse: 12935337.0000 - mae: 2936.2329 - val_loss: 2732.1536 - val_mse: 9889538.0000 - val_mae: 2732.1538\n",
            "Epoch 30/500\n",
            "90/90 [==============================] - 0s 809us/step - loss: 2835.3686 - mse: 12281064.0000 - mae: 2835.3687 - val_loss: 2560.1735 - val_mse: 9006333.0000 - val_mae: 2560.1736\n",
            "Epoch 31/500\n",
            "90/90 [==============================] - 0s 850us/step - loss: 2890.3218 - mse: 11617255.0000 - mae: 2890.3218 - val_loss: 2416.5601 - val_mse: 8342597.0000 - val_mae: 2416.5601\n",
            "Epoch 32/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 2800.6689 - mse: 11787777.0000 - mae: 2800.6687 - val_loss: 2317.1616 - val_mse: 7923850.5000 - val_mae: 2317.1616\n",
            "Epoch 33/500\n",
            "90/90 [==============================] - 0s 835us/step - loss: 2613.6156 - mse: 10412902.0000 - mae: 2613.6157 - val_loss: 2214.9209 - val_mse: 7526612.5000 - val_mae: 2214.9209\n",
            "Epoch 34/500\n",
            "90/90 [==============================] - 0s 864us/step - loss: 2631.1987 - mse: 10596958.0000 - mae: 2631.1987 - val_loss: 2094.8554 - val_mse: 7047192.0000 - val_mae: 2094.8555\n",
            "Epoch 35/500\n",
            "90/90 [==============================] - 0s 887us/step - loss: 2585.4684 - mse: 10306889.0000 - mae: 2585.4685 - val_loss: 2055.7322 - val_mse: 6873512.0000 - val_mae: 2055.7324\n",
            "Epoch 36/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 2481.0739 - mse: 8934858.0000 - mae: 2481.0740 - val_loss: 1995.5406 - val_mse: 6625366.0000 - val_mae: 1995.5406\n",
            "Epoch 37/500\n",
            "90/90 [==============================] - 0s 792us/step - loss: 2456.6623 - mse: 9828019.0000 - mae: 2456.6624 - val_loss: 1941.2096 - val_mse: 6417117.0000 - val_mae: 1941.2094\n",
            "Epoch 38/500\n",
            "90/90 [==============================] - 0s 840us/step - loss: 2632.1670 - mse: 10940997.0000 - mae: 2632.1670 - val_loss: 1923.0364 - val_mse: 6345858.0000 - val_mae: 1923.0365\n",
            "Epoch 39/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 2438.7889 - mse: 9973514.0000 - mae: 2438.7888 - val_loss: 1927.2818 - val_mse: 6362173.0000 - val_mae: 1927.2820\n",
            "Epoch 40/500\n",
            "90/90 [==============================] - 0s 967us/step - loss: 2496.0923 - mse: 10345607.0000 - mae: 2496.0923 - val_loss: 1901.9238 - val_mse: 6261801.0000 - val_mae: 1901.9240\n",
            "Epoch 41/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 2471.7532 - mse: 10065398.0000 - mae: 2471.7534 - val_loss: 1854.6238 - val_mse: 6068461.5000 - val_mae: 1854.6237\n",
            "Epoch 42/500\n",
            "90/90 [==============================] - 0s 857us/step - loss: 2375.3348 - mse: 9133829.0000 - mae: 2375.3350 - val_loss: 1839.9721 - val_mse: 5995739.5000 - val_mae: 1839.9720\n",
            "Epoch 43/500\n",
            "90/90 [==============================] - 0s 888us/step - loss: 2342.7805 - mse: 9250271.0000 - mae: 2342.7805 - val_loss: 1812.5966 - val_mse: 5854121.0000 - val_mae: 1812.5966\n",
            "Epoch 44/500\n",
            "90/90 [==============================] - 0s 895us/step - loss: 2529.3029 - mse: 10156028.0000 - mae: 2529.3027 - val_loss: 1806.8423 - val_mse: 5821347.5000 - val_mae: 1806.8424\n",
            "Epoch 45/500\n",
            "90/90 [==============================] - 0s 828us/step - loss: 2427.4603 - mse: 9325790.0000 - mae: 2427.4604 - val_loss: 1802.6205 - val_mse: 5796105.0000 - val_mae: 1802.6204\n",
            "Epoch 46/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 2438.4215 - mse: 9764726.0000 - mae: 2438.4216 - val_loss: 1799.7044 - val_mse: 5778966.0000 - val_mae: 1799.7043\n",
            "Epoch 47/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 2521.3296 - mse: 10422070.0000 - mae: 2521.3296 - val_loss: 1798.7244 - val_mse: 5773398.5000 - val_mae: 1798.7246\n",
            "Epoch 48/500\n",
            "90/90 [==============================] - 0s 885us/step - loss: 2694.8355 - mse: 11174062.0000 - mae: 2694.8354 - val_loss: 1798.0607 - val_mse: 5769723.5000 - val_mae: 1798.0609\n",
            "Epoch 49/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 2505.2346 - mse: 9485185.0000 - mae: 2505.2346 - val_loss: 1802.1440 - val_mse: 5793935.0000 - val_mae: 1802.1440\n",
            "Epoch 50/500\n",
            "90/90 [==============================] - 0s 990us/step - loss: 2481.0979 - mse: 10116439.0000 - mae: 2481.0979 - val_loss: 1798.2962 - val_mse: 5771353.5000 - val_mae: 1798.2961\n",
            "Epoch 51/500\n",
            "90/90 [==============================] - 0s 901us/step - loss: 2450.9565 - mse: 9983589.0000 - mae: 2450.9565 - val_loss: 1788.0097 - val_mse: 5714348.5000 - val_mae: 1788.0098\n",
            "Epoch 52/500\n",
            "90/90 [==============================] - 0s 855us/step - loss: 2401.2858 - mse: 9606299.0000 - mae: 2401.2856 - val_loss: 1780.9576 - val_mse: 5670225.5000 - val_mae: 1780.9576\n",
            "Epoch 53/500\n",
            "90/90 [==============================] - 0s 844us/step - loss: 2546.6608 - mse: 10201035.0000 - mae: 2546.6609 - val_loss: 1785.4221 - val_mse: 5699961.0000 - val_mae: 1785.4219\n",
            "Epoch 54/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 2233.3796 - mse: 8201282.0000 - mae: 2233.3796 - val_loss: 1776.8441 - val_mse: 5641981.0000 - val_mae: 1776.8442\n",
            "Epoch 55/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 2492.5881 - mse: 10210458.0000 - mae: 2492.5881 - val_loss: 1777.4868 - val_mse: 5646450.0000 - val_mae: 1777.4868\n",
            "Epoch 56/500\n",
            "90/90 [==============================] - 0s 831us/step - loss: 2638.6670 - mse: 11243798.0000 - mae: 2638.6670 - val_loss: 1774.6814 - val_mse: 5627268.5000 - val_mae: 1774.6814\n",
            "Epoch 57/500\n",
            "90/90 [==============================] - 0s 790us/step - loss: 2094.0525 - mse: 7822393.0000 - mae: 2094.0525 - val_loss: 1777.5859 - val_mse: 5647707.5000 - val_mae: 1777.5859\n",
            "Epoch 58/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 2344.8212 - mse: 8449286.0000 - mae: 2344.8213 - val_loss: 1777.8800 - val_mse: 5649223.5000 - val_mae: 1777.8800\n",
            "Epoch 59/500\n",
            "90/90 [==============================] - 0s 883us/step - loss: 2576.3137 - mse: 10264787.0000 - mae: 2576.3137 - val_loss: 1776.1401 - val_mse: 5634941.5000 - val_mae: 1776.1399\n",
            "Epoch 60/500\n",
            "90/90 [==============================] - 0s 912us/step - loss: 2495.5460 - mse: 9142626.0000 - mae: 2495.5459 - val_loss: 1774.1569 - val_mse: 5621289.5000 - val_mae: 1774.1567\n",
            "Epoch 61/500\n",
            "90/90 [==============================] - 0s 828us/step - loss: 2313.5926 - mse: 8933063.0000 - mae: 2313.5925 - val_loss: 1772.7855 - val_mse: 5617599.0000 - val_mae: 1772.7854\n",
            "Epoch 62/500\n",
            "90/90 [==============================] - 0s 869us/step - loss: 2289.1606 - mse: 8569111.0000 - mae: 2289.1606 - val_loss: 1768.5661 - val_mse: 5592913.5000 - val_mae: 1768.5659\n",
            "Epoch 63/500\n",
            "90/90 [==============================] - 0s 824us/step - loss: 2317.4772 - mse: 8932201.0000 - mae: 2317.4773 - val_loss: 1768.0503 - val_mse: 5583852.5000 - val_mae: 1768.0503\n",
            "Epoch 64/500\n",
            "90/90 [==============================] - 0s 763us/step - loss: 2575.2949 - mse: 10071951.0000 - mae: 2575.2947 - val_loss: 1768.6468 - val_mse: 5570123.5000 - val_mae: 1768.6469\n",
            "Epoch 65/500\n",
            "90/90 [==============================] - 0s 852us/step - loss: 2407.7512 - mse: 10270103.0000 - mae: 2407.7512 - val_loss: 1775.6199 - val_mse: 5511646.5000 - val_mae: 1775.6201\n",
            "Epoch 66/500\n",
            "90/90 [==============================] - 0s 838us/step - loss: 2367.3939 - mse: 8835140.0000 - mae: 2367.3940 - val_loss: 1859.8812 - val_mse: 5473954.0000 - val_mae: 1859.8811\n",
            "Epoch 67/500\n",
            "90/90 [==============================] - 0s 784us/step - loss: 2387.9357 - mse: 9448239.0000 - mae: 2387.9358 - val_loss: 2160.4999 - val_mse: 7236736.0000 - val_mae: 2160.4998\n",
            "Epoch 68/500\n",
            "90/90 [==============================] - 0s 931us/step - loss: 2102.0707 - mse: 8124955.0000 - mae: 2102.0706 - val_loss: 1916.8702 - val_mse: 6084519.0000 - val_mae: 1916.8701\n",
            "Epoch 69/500\n",
            "90/90 [==============================] - 0s 786us/step - loss: 2250.1818 - mse: 8949434.0000 - mae: 2250.1819 - val_loss: 1948.4376 - val_mse: 6265767.0000 - val_mae: 1948.4375\n",
            "Epoch 70/500\n",
            "90/90 [==============================] - 0s 758us/step - loss: 2155.4942 - mse: 8793015.0000 - mae: 2155.4941 - val_loss: 1951.3121 - val_mse: 6733081.5000 - val_mae: 1951.3121\n",
            "Epoch 71/500\n",
            "90/90 [==============================] - 0s 934us/step - loss: 2232.5719 - mse: 8656075.0000 - mae: 2232.5718 - val_loss: 1837.2501 - val_mse: 6042982.5000 - val_mae: 1837.2500\n",
            "Epoch 72/500\n",
            "90/90 [==============================] - 0s 761us/step - loss: 2180.1289 - mse: 8508377.0000 - mae: 2180.1289 - val_loss: 1817.9498 - val_mse: 5828745.0000 - val_mae: 1817.9497\n",
            "Epoch 73/500\n",
            "90/90 [==============================] - 0s 807us/step - loss: 2179.7947 - mse: 8799902.0000 - mae: 2179.7947 - val_loss: 1870.5375 - val_mse: 5888786.0000 - val_mae: 1870.5375\n",
            "Epoch 74/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 2112.8311 - mse: 8570544.0000 - mae: 2112.8311 - val_loss: 1837.3879 - val_mse: 6641421.5000 - val_mae: 1837.3882\n",
            "Epoch 75/500\n",
            "90/90 [==============================] - 0s 960us/step - loss: 1964.0329 - mse: 7182375.0000 - mae: 1964.0330 - val_loss: 1840.9919 - val_mse: 5962171.0000 - val_mae: 1840.9921\n",
            "Epoch 76/500\n",
            "90/90 [==============================] - 0s 841us/step - loss: 2011.9323 - mse: 7451360.5000 - mae: 2011.9321 - val_loss: 1837.0205 - val_mse: 6292810.5000 - val_mae: 1837.0205\n",
            "Epoch 77/500\n",
            "90/90 [==============================] - 0s 920us/step - loss: 1936.8419 - mse: 6639148.0000 - mae: 1936.8420 - val_loss: 1555.4273 - val_mse: 4205180.5000 - val_mae: 1555.4275\n",
            "Epoch 78/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 2192.9467 - mse: 7996739.5000 - mae: 2192.9468 - val_loss: 1668.5062 - val_mse: 5497559.0000 - val_mae: 1668.5062\n",
            "Epoch 79/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 2043.9292 - mse: 6837764.5000 - mae: 2043.9292 - val_loss: 1522.2860 - val_mse: 4136602.7500 - val_mae: 1522.2859\n",
            "Epoch 80/500\n",
            "90/90 [==============================] - 0s 889us/step - loss: 2172.6890 - mse: 8091607.0000 - mae: 2172.6890 - val_loss: 1520.5204 - val_mse: 4029917.5000 - val_mae: 1520.5205\n",
            "Epoch 81/500\n",
            "90/90 [==============================] - 0s 882us/step - loss: 2029.0792 - mse: 7526409.0000 - mae: 2029.0792 - val_loss: 1523.0305 - val_mse: 4998365.0000 - val_mae: 1523.0305\n",
            "Epoch 82/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1907.8358 - mse: 7239579.0000 - mae: 1907.8359 - val_loss: 1528.6728 - val_mse: 4207090.0000 - val_mae: 1528.6727\n",
            "Epoch 83/500\n",
            "90/90 [==============================] - 0s 901us/step - loss: 2093.9872 - mse: 7202617.0000 - mae: 2093.9873 - val_loss: 1475.1755 - val_mse: 4375696.0000 - val_mae: 1475.1755\n",
            "Epoch 84/500\n",
            "90/90 [==============================] - 0s 793us/step - loss: 1959.5001 - mse: 7190881.5000 - mae: 1959.5001 - val_loss: 1449.7588 - val_mse: 3700561.5000 - val_mae: 1449.7587\n",
            "Epoch 85/500\n",
            "90/90 [==============================] - 0s 806us/step - loss: 2099.4970 - mse: 7779680.0000 - mae: 2099.4971 - val_loss: 1382.1628 - val_mse: 3431086.2500 - val_mae: 1382.1627\n",
            "Epoch 86/500\n",
            "90/90 [==============================] - 0s 854us/step - loss: 2007.8405 - mse: 6803976.0000 - mae: 2007.8403 - val_loss: 1401.9747 - val_mse: 3868928.0000 - val_mae: 1401.9745\n",
            "Epoch 87/500\n",
            "90/90 [==============================] - 0s 800us/step - loss: 1995.0903 - mse: 7585572.5000 - mae: 1995.0903 - val_loss: 1453.4444 - val_mse: 3768033.5000 - val_mae: 1453.4443\n",
            "Epoch 88/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 2044.9038 - mse: 7401613.0000 - mae: 2044.9037 - val_loss: 1542.5368 - val_mse: 4089166.2500 - val_mae: 1542.5369\n",
            "Epoch 89/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1999.9122 - mse: 7520953.0000 - mae: 1999.9121 - val_loss: 1431.4591 - val_mse: 3682439.0000 - val_mae: 1431.4592\n",
            "Epoch 90/500\n",
            "90/90 [==============================] - 0s 806us/step - loss: 1981.3682 - mse: 6836830.5000 - mae: 1981.3680 - val_loss: 1482.0262 - val_mse: 4290288.5000 - val_mae: 1482.0262\n",
            "Epoch 91/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1988.8929 - mse: 7122545.0000 - mae: 1988.8928 - val_loss: 1547.7783 - val_mse: 4031141.0000 - val_mae: 1547.7783\n",
            "Epoch 92/500\n",
            "90/90 [==============================] - 0s 829us/step - loss: 1802.9595 - mse: 6657411.0000 - mae: 1802.9594 - val_loss: 1401.9752 - val_mse: 3399536.7500 - val_mae: 1401.9751\n",
            "Epoch 93/500\n",
            "90/90 [==============================] - 0s 858us/step - loss: 2040.9675 - mse: 7159093.5000 - mae: 2040.9675 - val_loss: 1394.1916 - val_mse: 3501660.0000 - val_mae: 1394.1915\n",
            "Epoch 94/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1959.5120 - mse: 7220253.0000 - mae: 1959.5122 - val_loss: 1403.9167 - val_mse: 3413007.0000 - val_mae: 1403.9166\n",
            "Epoch 95/500\n",
            "90/90 [==============================] - 0s 796us/step - loss: 2110.2327 - mse: 6968511.5000 - mae: 2110.2329 - val_loss: 1461.1728 - val_mse: 3691974.7500 - val_mae: 1461.1730\n",
            "Epoch 96/500\n",
            "90/90 [==============================] - 0s 848us/step - loss: 1921.5852 - mse: 6939357.0000 - mae: 1921.5854 - val_loss: 1325.1126 - val_mse: 3190010.7500 - val_mae: 1325.1128\n",
            "Epoch 97/500\n",
            "90/90 [==============================] - 0s 775us/step - loss: 1931.5286 - mse: 6701170.5000 - mae: 1931.5287 - val_loss: 1460.8445 - val_mse: 4627986.5000 - val_mae: 1460.8446\n",
            "Epoch 98/500\n",
            "90/90 [==============================] - 0s 840us/step - loss: 2032.1989 - mse: 7862360.0000 - mae: 2032.1990 - val_loss: 1368.7464 - val_mse: 3777701.7500 - val_mae: 1368.7463\n",
            "Epoch 99/500\n",
            "90/90 [==============================] - 0s 946us/step - loss: 2014.4597 - mse: 7044798.5000 - mae: 2014.4597 - val_loss: 1457.2987 - val_mse: 3533903.0000 - val_mae: 1457.2988\n",
            "Epoch 100/500\n",
            "90/90 [==============================] - 0s 969us/step - loss: 1964.6505 - mse: 6927862.0000 - mae: 1964.6505 - val_loss: 1312.6209 - val_mse: 3571154.7500 - val_mae: 1312.6208\n",
            "Epoch 101/500\n",
            "90/90 [==============================] - 0s 918us/step - loss: 1878.1197 - mse: 6608215.5000 - mae: 1878.1196 - val_loss: 1283.7921 - val_mse: 3192747.7500 - val_mae: 1283.7922\n",
            "Epoch 102/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1856.7571 - mse: 6628565.5000 - mae: 1856.7571 - val_loss: 1313.2692 - val_mse: 3818391.7500 - val_mae: 1313.2692\n",
            "Epoch 103/500\n",
            "90/90 [==============================] - 0s 958us/step - loss: 1893.3831 - mse: 6513512.0000 - mae: 1893.3832 - val_loss: 1297.8906 - val_mse: 3320876.0000 - val_mae: 1297.8906\n",
            "Epoch 104/500\n",
            "90/90 [==============================] - 0s 999us/step - loss: 1857.5861 - mse: 6202655.5000 - mae: 1857.5861 - val_loss: 1411.9484 - val_mse: 3251005.2500 - val_mae: 1411.9485\n",
            "Epoch 105/500\n",
            "90/90 [==============================] - 0s 796us/step - loss: 1809.3636 - mse: 5989247.5000 - mae: 1809.3635 - val_loss: 1337.8517 - val_mse: 3268643.2500 - val_mae: 1337.8517\n",
            "Epoch 106/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1785.5513 - mse: 6116741.0000 - mae: 1785.5514 - val_loss: 1386.3959 - val_mse: 4038358.5000 - val_mae: 1386.3959\n",
            "Epoch 107/500\n",
            "90/90 [==============================] - 0s 775us/step - loss: 1829.7802 - mse: 6310284.0000 - mae: 1829.7802 - val_loss: 1460.8181 - val_mse: 3484372.2500 - val_mae: 1460.8181\n",
            "Epoch 108/500\n",
            "90/90 [==============================] - 0s 788us/step - loss: 1825.8332 - mse: 6046683.5000 - mae: 1825.8331 - val_loss: 1401.9053 - val_mse: 3836246.0000 - val_mae: 1401.9053\n",
            "Epoch 109/500\n",
            "90/90 [==============================] - 0s 868us/step - loss: 1663.3118 - mse: 5097425.5000 - mae: 1663.3118 - val_loss: 1286.5788 - val_mse: 2866717.5000 - val_mae: 1286.5789\n",
            "Epoch 110/500\n",
            "90/90 [==============================] - 0s 819us/step - loss: 1826.4208 - mse: 6356836.5000 - mae: 1826.4207 - val_loss: 1375.6029 - val_mse: 3186327.5000 - val_mae: 1375.6029\n",
            "Epoch 111/500\n",
            "90/90 [==============================] - 0s 858us/step - loss: 1834.0532 - mse: 5924605.0000 - mae: 1834.0531 - val_loss: 1314.6489 - val_mse: 3379218.2500 - val_mae: 1314.6489\n",
            "Epoch 112/500\n",
            "90/90 [==============================] - 0s 790us/step - loss: 1845.0431 - mse: 6059095.5000 - mae: 1845.0432 - val_loss: 1586.9065 - val_mse: 4850212.5000 - val_mae: 1586.9064\n",
            "Epoch 113/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1868.8899 - mse: 6065978.5000 - mae: 1868.8899 - val_loss: 1323.6807 - val_mse: 3054813.5000 - val_mae: 1323.6808\n",
            "Epoch 114/500\n",
            "90/90 [==============================] - 0s 951us/step - loss: 1759.4629 - mse: 6290389.5000 - mae: 1759.4629 - val_loss: 1281.5356 - val_mse: 3374556.7500 - val_mae: 1281.5356\n",
            "Epoch 115/500\n",
            "90/90 [==============================] - 0s 872us/step - loss: 1970.7203 - mse: 7732124.5000 - mae: 1970.7201 - val_loss: 1326.8235 - val_mse: 3125050.7500 - val_mae: 1326.8236\n",
            "Epoch 116/500\n",
            "90/90 [==============================] - 0s 992us/step - loss: 1578.2091 - mse: 4792622.0000 - mae: 1578.2090 - val_loss: 1273.3268 - val_mse: 2892858.2500 - val_mae: 1273.3268\n",
            "Epoch 117/500\n",
            "90/90 [==============================] - 0s 815us/step - loss: 1860.7332 - mse: 6282244.5000 - mae: 1860.7333 - val_loss: 1356.7648 - val_mse: 3585926.5000 - val_mae: 1356.7649\n",
            "Epoch 118/500\n",
            "90/90 [==============================] - 0s 997us/step - loss: 1890.0610 - mse: 6540843.5000 - mae: 1890.0612 - val_loss: 1272.5435 - val_mse: 3001328.0000 - val_mae: 1272.5435\n",
            "Epoch 119/500\n",
            "90/90 [==============================] - 0s 846us/step - loss: 1912.0223 - mse: 6045753.5000 - mae: 1912.0223 - val_loss: 1292.1349 - val_mse: 2973649.7500 - val_mae: 1292.1349\n",
            "Epoch 120/500\n",
            "90/90 [==============================] - 0s 792us/step - loss: 1756.4586 - mse: 5372806.5000 - mae: 1756.4585 - val_loss: 1246.5497 - val_mse: 2710982.7500 - val_mae: 1246.5497\n",
            "Epoch 121/500\n",
            "90/90 [==============================] - 0s 868us/step - loss: 1752.1988 - mse: 5147591.0000 - mae: 1752.1990 - val_loss: 1266.2824 - val_mse: 2762544.2500 - val_mae: 1266.2823\n",
            "Epoch 122/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1952.6370 - mse: 6338836.0000 - mae: 1952.6370 - val_loss: 1549.4858 - val_mse: 5598871.0000 - val_mae: 1549.4860\n",
            "Epoch 123/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1781.5008 - mse: 5674700.5000 - mae: 1781.5007 - val_loss: 1315.6934 - val_mse: 3870777.0000 - val_mae: 1315.6934\n",
            "Epoch 124/500\n",
            "90/90 [==============================] - 0s 852us/step - loss: 1672.5636 - mse: 5076775.0000 - mae: 1672.5636 - val_loss: 1284.2598 - val_mse: 2771323.0000 - val_mae: 1284.2598\n",
            "Epoch 125/500\n",
            "90/90 [==============================] - 0s 743us/step - loss: 1811.1170 - mse: 5474951.0000 - mae: 1811.1171 - val_loss: 1408.0368 - val_mse: 4763862.5000 - val_mae: 1408.0370\n",
            "Epoch 126/500\n",
            "90/90 [==============================] - 0s 792us/step - loss: 1878.8988 - mse: 6648042.5000 - mae: 1878.8988 - val_loss: 1238.6332 - val_mse: 2689467.7500 - val_mae: 1238.6332\n",
            "Epoch 127/500\n",
            "90/90 [==============================] - 0s 765us/step - loss: 1936.0516 - mse: 6465719.5000 - mae: 1936.0514 - val_loss: 1329.0645 - val_mse: 3915925.0000 - val_mae: 1329.0645\n",
            "Epoch 128/500\n",
            "90/90 [==============================] - 0s 868us/step - loss: 2013.4944 - mse: 6803366.5000 - mae: 2013.4944 - val_loss: 1297.5818 - val_mse: 3734962.5000 - val_mae: 1297.5817\n",
            "Epoch 129/500\n",
            "90/90 [==============================] - 0s 833us/step - loss: 1807.1351 - mse: 5443242.0000 - mae: 1807.1353 - val_loss: 1281.2513 - val_mse: 2861171.2500 - val_mae: 1281.2512\n",
            "Epoch 130/500\n",
            "90/90 [==============================] - 0s 932us/step - loss: 1893.8257 - mse: 5729912.0000 - mae: 1893.8257 - val_loss: 1456.8630 - val_mse: 5220802.5000 - val_mae: 1456.8630\n",
            "Epoch 131/500\n",
            "90/90 [==============================] - 0s 848us/step - loss: 1782.5451 - mse: 5390333.0000 - mae: 1782.5452 - val_loss: 1312.7196 - val_mse: 3675044.2500 - val_mae: 1312.7195\n",
            "Epoch 132/500\n",
            "90/90 [==============================] - 0s 844us/step - loss: 1825.2397 - mse: 5679042.0000 - mae: 1825.2397 - val_loss: 1263.7693 - val_mse: 3018885.0000 - val_mae: 1263.7693\n",
            "Epoch 133/500\n",
            "90/90 [==============================] - 0s 781us/step - loss: 1669.5552 - mse: 5110141.5000 - mae: 1669.5554 - val_loss: 1189.8455 - val_mse: 2531528.5000 - val_mae: 1189.8455\n",
            "Epoch 134/500\n",
            "90/90 [==============================] - 0s 817us/step - loss: 1825.4248 - mse: 6167054.0000 - mae: 1825.4248 - val_loss: 1186.2678 - val_mse: 3107429.7500 - val_mae: 1186.2678\n",
            "Epoch 135/500\n",
            "90/90 [==============================] - 0s 765us/step - loss: 1966.7143 - mse: 6721738.0000 - mae: 1966.7142 - val_loss: 1177.1360 - val_mse: 2594764.0000 - val_mae: 1177.1360\n",
            "Epoch 136/500\n",
            "90/90 [==============================] - 0s 774us/step - loss: 1750.7281 - mse: 5189768.0000 - mae: 1750.7279 - val_loss: 1238.6733 - val_mse: 3148181.2500 - val_mae: 1238.6732\n",
            "Epoch 137/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 2088.5473 - mse: 7251072.0000 - mae: 2088.5471 - val_loss: 1301.9003 - val_mse: 2892632.2500 - val_mae: 1301.9004\n",
            "Epoch 138/500\n",
            "90/90 [==============================] - 0s 877us/step - loss: 1779.9066 - mse: 5267338.0000 - mae: 1779.9066 - val_loss: 1195.8747 - val_mse: 2799320.5000 - val_mae: 1195.8746\n",
            "Epoch 139/500\n",
            "90/90 [==============================] - 0s 832us/step - loss: 1677.2983 - mse: 4971268.5000 - mae: 1677.2982 - val_loss: 1173.3493 - val_mse: 2630309.2500 - val_mae: 1173.3492\n",
            "Epoch 140/500\n",
            "90/90 [==============================] - 0s 926us/step - loss: 1834.8762 - mse: 6082688.5000 - mae: 1834.8762 - val_loss: 1167.3923 - val_mse: 3126934.0000 - val_mae: 1167.3923\n",
            "Epoch 141/500\n",
            "90/90 [==============================] - 0s 841us/step - loss: 1804.5460 - mse: 5902613.5000 - mae: 1804.5460 - val_loss: 1148.3549 - val_mse: 2532551.5000 - val_mae: 1148.3549\n",
            "Epoch 142/500\n",
            "90/90 [==============================] - 0s 814us/step - loss: 1719.2289 - mse: 5025274.5000 - mae: 1719.2290 - val_loss: 1149.3331 - val_mse: 2464845.7500 - val_mae: 1149.3331\n",
            "Epoch 143/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1870.4373 - mse: 6262464.0000 - mae: 1870.4374 - val_loss: 1205.7666 - val_mse: 3385124.0000 - val_mae: 1205.7667\n",
            "Epoch 144/500\n",
            "90/90 [==============================] - 0s 948us/step - loss: 1792.2622 - mse: 5166125.5000 - mae: 1792.2622 - val_loss: 1205.4943 - val_mse: 2517629.7500 - val_mae: 1205.4943\n",
            "Epoch 145/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1962.8112 - mse: 6483524.5000 - mae: 1962.8112 - val_loss: 1194.0575 - val_mse: 2820748.5000 - val_mae: 1194.0576\n",
            "Epoch 146/500\n",
            "90/90 [==============================] - 0s 863us/step - loss: 1754.4824 - mse: 5717429.5000 - mae: 1754.4824 - val_loss: 1206.4766 - val_mse: 2657159.2500 - val_mae: 1206.4766\n",
            "Epoch 147/500\n",
            "90/90 [==============================] - 0s 918us/step - loss: 1691.4391 - mse: 4960756.5000 - mae: 1691.4391 - val_loss: 1222.9575 - val_mse: 3663350.5000 - val_mae: 1222.9576\n",
            "Epoch 148/500\n",
            "90/90 [==============================] - 0s 854us/step - loss: 1903.9640 - mse: 6099575.0000 - mae: 1903.9641 - val_loss: 1176.8132 - val_mse: 2783780.5000 - val_mae: 1176.8134\n",
            "Epoch 149/500\n",
            "90/90 [==============================] - 0s 814us/step - loss: 2015.6601 - mse: 6549293.0000 - mae: 2015.6600 - val_loss: 1228.4436 - val_mse: 3049146.2500 - val_mae: 1228.4437\n",
            "Epoch 150/500\n",
            "90/90 [==============================] - 0s 846us/step - loss: 1681.2706 - mse: 4631688.0000 - mae: 1681.2706 - val_loss: 1181.0257 - val_mse: 2380737.0000 - val_mae: 1181.0256\n",
            "Epoch 151/500\n",
            "90/90 [==============================] - 0s 776us/step - loss: 1675.2368 - mse: 4835949.0000 - mae: 1675.2366 - val_loss: 1254.1673 - val_mse: 2725730.7500 - val_mae: 1254.1675\n",
            "Epoch 152/500\n",
            "90/90 [==============================] - 0s 771us/step - loss: 1837.5277 - mse: 5518871.0000 - mae: 1837.5278 - val_loss: 1251.3406 - val_mse: 2677791.7500 - val_mae: 1251.3406\n",
            "Epoch 153/500\n",
            "90/90 [==============================] - 0s 778us/step - loss: 1760.7848 - mse: 5517520.5000 - mae: 1760.7847 - val_loss: 1185.5006 - val_mse: 2408244.0000 - val_mae: 1185.5007\n",
            "Epoch 154/500\n",
            "90/90 [==============================] - 0s 909us/step - loss: 1839.6049 - mse: 5386974.0000 - mae: 1839.6050 - val_loss: 1232.2065 - val_mse: 3624036.2500 - val_mae: 1232.2065\n",
            "Epoch 155/500\n",
            "90/90 [==============================] - 0s 791us/step - loss: 1647.2134 - mse: 4868486.5000 - mae: 1647.2131 - val_loss: 1123.0911 - val_mse: 2554227.7500 - val_mae: 1123.0912\n",
            "Epoch 156/500\n",
            "90/90 [==============================] - 0s 796us/step - loss: 1685.6815 - mse: 4522897.0000 - mae: 1685.6814 - val_loss: 1146.2519 - val_mse: 3010997.2500 - val_mae: 1146.2520\n",
            "Epoch 157/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1760.0914 - mse: 5330712.0000 - mae: 1760.0913 - val_loss: 1133.0646 - val_mse: 2546176.2500 - val_mae: 1133.0645\n",
            "Epoch 158/500\n",
            "90/90 [==============================] - 0s 814us/step - loss: 1826.5716 - mse: 5727049.0000 - mae: 1826.5715 - val_loss: 1208.6992 - val_mse: 3158253.5000 - val_mae: 1208.6991\n",
            "Epoch 159/500\n",
            "90/90 [==============================] - 0s 833us/step - loss: 1826.3661 - mse: 5638980.5000 - mae: 1826.3661 - val_loss: 1121.2325 - val_mse: 2335663.5000 - val_mae: 1121.2324\n",
            "Epoch 160/500\n",
            "90/90 [==============================] - 0s 874us/step - loss: 1786.8586 - mse: 5645492.5000 - mae: 1786.8586 - val_loss: 1149.1333 - val_mse: 3332372.2500 - val_mae: 1149.1333\n",
            "Epoch 161/500\n",
            "90/90 [==============================] - 0s 868us/step - loss: 1834.4945 - mse: 6195984.5000 - mae: 1834.4944 - val_loss: 1128.7474 - val_mse: 2603370.7500 - val_mae: 1128.7474\n",
            "Epoch 162/500\n",
            "90/90 [==============================] - 0s 990us/step - loss: 1741.7335 - mse: 5183094.5000 - mae: 1741.7333 - val_loss: 1338.6424 - val_mse: 4481398.0000 - val_mae: 1338.6423\n",
            "Epoch 163/500\n",
            "90/90 [==============================] - 0s 828us/step - loss: 1813.1871 - mse: 6184455.0000 - mae: 1813.1871 - val_loss: 1221.0189 - val_mse: 3386489.2500 - val_mae: 1221.0190\n",
            "Epoch 164/500\n",
            "90/90 [==============================] - 0s 768us/step - loss: 1754.9545 - mse: 5299473.5000 - mae: 1754.9545 - val_loss: 1195.7731 - val_mse: 2621798.5000 - val_mae: 1195.7731\n",
            "Epoch 165/500\n",
            "90/90 [==============================] - 0s 802us/step - loss: 1812.2804 - mse: 5478264.5000 - mae: 1812.2804 - val_loss: 1220.0527 - val_mse: 2391058.2500 - val_mae: 1220.0527\n",
            "Epoch 166/500\n",
            "90/90 [==============================] - 0s 809us/step - loss: 1769.4435 - mse: 4987294.5000 - mae: 1769.4434 - val_loss: 1304.7332 - val_mse: 3859845.0000 - val_mae: 1304.7332\n",
            "Epoch 167/500\n",
            "90/90 [==============================] - 0s 986us/step - loss: 1569.0398 - mse: 4243120.0000 - mae: 1569.0398 - val_loss: 1171.7008 - val_mse: 3212136.5000 - val_mae: 1171.7009\n",
            "Epoch 168/500\n",
            "90/90 [==============================] - 0s 838us/step - loss: 1844.7635 - mse: 5626571.5000 - mae: 1844.7634 - val_loss: 1192.2649 - val_mse: 3340591.7500 - val_mae: 1192.2648\n",
            "Epoch 169/500\n",
            "90/90 [==============================] - 0s 786us/step - loss: 1787.6424 - mse: 5289830.5000 - mae: 1787.6423 - val_loss: 1209.3802 - val_mse: 3069133.2500 - val_mae: 1209.3800\n",
            "Epoch 170/500\n",
            "90/90 [==============================] - 0s 765us/step - loss: 1801.7488 - mse: 5490175.0000 - mae: 1801.7488 - val_loss: 1217.4416 - val_mse: 2536753.2500 - val_mae: 1217.4417\n",
            "Epoch 171/500\n",
            "90/90 [==============================] - 0s 791us/step - loss: 1457.1772 - mse: 3864245.0000 - mae: 1457.1771 - val_loss: 1332.1389 - val_mse: 4388259.0000 - val_mae: 1332.1387\n",
            "Epoch 172/500\n",
            "90/90 [==============================] - 0s 853us/step - loss: 1787.1150 - mse: 5194756.0000 - mae: 1787.1150 - val_loss: 1164.7747 - val_mse: 2915892.2500 - val_mae: 1164.7748\n",
            "Epoch 173/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1619.4050 - mse: 4764287.0000 - mae: 1619.4050 - val_loss: 1093.8917 - val_mse: 2127462.5000 - val_mae: 1093.8917\n",
            "Epoch 174/500\n",
            "90/90 [==============================] - 0s 775us/step - loss: 1808.4883 - mse: 5706264.0000 - mae: 1808.4882 - val_loss: 1148.0726 - val_mse: 2394132.5000 - val_mae: 1148.0725\n",
            "Epoch 175/500\n",
            "90/90 [==============================] - 0s 851us/step - loss: 1814.2573 - mse: 5823825.5000 - mae: 1814.2573 - val_loss: 1113.9977 - val_mse: 2212802.2500 - val_mae: 1113.9977\n",
            "Epoch 176/500\n",
            "90/90 [==============================] - 0s 825us/step - loss: 1809.1660 - mse: 5485638.0000 - mae: 1809.1660 - val_loss: 1128.8138 - val_mse: 2403031.2500 - val_mae: 1128.8138\n",
            "Epoch 177/500\n",
            "90/90 [==============================] - 0s 843us/step - loss: 1676.4126 - mse: 4692959.5000 - mae: 1676.4125 - val_loss: 1124.8363 - val_mse: 3335413.2500 - val_mae: 1124.8362\n",
            "Epoch 178/500\n",
            "90/90 [==============================] - 0s 818us/step - loss: 1697.3544 - mse: 4925927.0000 - mae: 1697.3544 - val_loss: 1100.4035 - val_mse: 2430539.7500 - val_mae: 1100.4037\n",
            "Epoch 179/500\n",
            "90/90 [==============================] - 0s 860us/step - loss: 1694.4154 - mse: 5251415.0000 - mae: 1694.4153 - val_loss: 1172.5818 - val_mse: 2810451.5000 - val_mae: 1172.5819\n",
            "Epoch 180/500\n",
            "90/90 [==============================] - 0s 847us/step - loss: 2048.8669 - mse: 6971818.5000 - mae: 2048.8669 - val_loss: 1310.4601 - val_mse: 3966232.2500 - val_mae: 1310.4601\n",
            "Epoch 181/500\n",
            "90/90 [==============================] - 0s 872us/step - loss: 1844.5285 - mse: 5750262.5000 - mae: 1844.5284 - val_loss: 1194.7069 - val_mse: 2390808.5000 - val_mae: 1194.7068\n",
            "Epoch 182/500\n",
            "90/90 [==============================] - 0s 757us/step - loss: 1812.8733 - mse: 5239231.5000 - mae: 1812.8733 - val_loss: 1100.6707 - val_mse: 2651472.2500 - val_mae: 1100.6707\n",
            "Epoch 183/500\n",
            "90/90 [==============================] - 0s 812us/step - loss: 1697.9166 - mse: 5056603.5000 - mae: 1697.9169 - val_loss: 1093.6898 - val_mse: 2422959.0000 - val_mae: 1093.6897\n",
            "Epoch 184/500\n",
            "90/90 [==============================] - 0s 853us/step - loss: 1841.7684 - mse: 5391897.0000 - mae: 1841.7686 - val_loss: 1110.5513 - val_mse: 2839510.5000 - val_mae: 1110.5513\n",
            "Epoch 185/500\n",
            "90/90 [==============================] - 0s 766us/step - loss: 1869.6650 - mse: 5568462.0000 - mae: 1869.6649 - val_loss: 1093.3237 - val_mse: 2893254.5000 - val_mae: 1093.3237\n",
            "Epoch 186/500\n",
            "90/90 [==============================] - 0s 925us/step - loss: 1839.5072 - mse: 6205334.0000 - mae: 1839.5073 - val_loss: 1055.0448 - val_mse: 2197421.7500 - val_mae: 1055.0447\n",
            "Epoch 187/500\n",
            "90/90 [==============================] - 0s 882us/step - loss: 1670.8875 - mse: 5000446.5000 - mae: 1670.8875 - val_loss: 1219.7877 - val_mse: 3982115.5000 - val_mae: 1219.7877\n",
            "Epoch 188/500\n",
            "90/90 [==============================] - 0s 887us/step - loss: 1643.5942 - mse: 5349623.0000 - mae: 1643.5941 - val_loss: 1144.9341 - val_mse: 2270914.7500 - val_mae: 1144.9341\n",
            "Epoch 189/500\n",
            "90/90 [==============================] - 0s 851us/step - loss: 1622.0475 - mse: 4379241.0000 - mae: 1622.0476 - val_loss: 1235.8480 - val_mse: 2667202.5000 - val_mae: 1235.8480\n",
            "Epoch 190/500\n",
            "90/90 [==============================] - 0s 816us/step - loss: 1756.2607 - mse: 4913187.0000 - mae: 1756.2607 - val_loss: 1179.6396 - val_mse: 2947656.2500 - val_mae: 1179.6396\n",
            "Epoch 191/500\n",
            "90/90 [==============================] - 0s 826us/step - loss: 1655.1891 - mse: 4211459.5000 - mae: 1655.1891 - val_loss: 1154.5765 - val_mse: 2755967.2500 - val_mae: 1154.5765\n",
            "Epoch 192/500\n",
            "90/90 [==============================] - 0s 928us/step - loss: 1670.5950 - mse: 4677611.5000 - mae: 1670.5950 - val_loss: 1180.6749 - val_mse: 3058011.0000 - val_mae: 1180.6748\n",
            "Epoch 193/500\n",
            "90/90 [==============================] - 0s 780us/step - loss: 1692.3886 - mse: 4501589.0000 - mae: 1692.3887 - val_loss: 1162.6605 - val_mse: 2477454.2500 - val_mae: 1162.6605\n",
            "Epoch 194/500\n",
            "90/90 [==============================] - 0s 773us/step - loss: 1681.7780 - mse: 4772415.5000 - mae: 1681.7778 - val_loss: 1180.9710 - val_mse: 2599173.7500 - val_mae: 1180.9709\n",
            "Epoch 195/500\n",
            "90/90 [==============================] - 0s 858us/step - loss: 1798.7559 - mse: 5347100.0000 - mae: 1798.7559 - val_loss: 1222.5035 - val_mse: 3143662.5000 - val_mae: 1222.5034\n",
            "Epoch 196/500\n",
            "90/90 [==============================] - 0s 781us/step - loss: 1762.6672 - mse: 5023307.5000 - mae: 1762.6672 - val_loss: 1163.4444 - val_mse: 2200345.0000 - val_mae: 1163.4445\n",
            "Epoch 197/500\n",
            "90/90 [==============================] - 0s 905us/step - loss: 1960.5858 - mse: 5936487.0000 - mae: 1960.5858 - val_loss: 1129.3142 - val_mse: 3048077.7500 - val_mae: 1129.3142\n",
            "Epoch 198/500\n",
            "90/90 [==============================] - 0s 881us/step - loss: 1559.9674 - mse: 4512791.0000 - mae: 1559.9674 - val_loss: 1060.1900 - val_mse: 2399146.7500 - val_mae: 1060.1901\n",
            "Epoch 199/500\n",
            "90/90 [==============================] - 0s 843us/step - loss: 1619.3325 - mse: 4449934.0000 - mae: 1619.3325 - val_loss: 1369.2945 - val_mse: 5252600.0000 - val_mae: 1369.2943\n",
            "Epoch 200/500\n",
            "90/90 [==============================] - 0s 779us/step - loss: 1700.5940 - mse: 4973811.0000 - mae: 1700.5939 - val_loss: 1068.9927 - val_mse: 2535864.0000 - val_mae: 1068.9927\n",
            "Epoch 201/500\n",
            "90/90 [==============================] - 0s 835us/step - loss: 1698.2020 - mse: 5036613.0000 - mae: 1698.2019 - val_loss: 1199.0306 - val_mse: 3275875.2500 - val_mae: 1199.0305\n",
            "Epoch 202/500\n",
            "90/90 [==============================] - 0s 768us/step - loss: 1725.3914 - mse: 5043701.0000 - mae: 1725.3914 - val_loss: 1157.2087 - val_mse: 3507222.5000 - val_mae: 1157.2087\n",
            "Epoch 203/500\n",
            "90/90 [==============================] - 0s 748us/step - loss: 1754.9876 - mse: 4849772.5000 - mae: 1754.9875 - val_loss: 1114.8322 - val_mse: 2278044.2500 - val_mae: 1114.8323\n",
            "Epoch 204/500\n",
            "90/90 [==============================] - 0s 800us/step - loss: 1828.5581 - mse: 5606221.0000 - mae: 1828.5581 - val_loss: 1412.6656 - val_mse: 5670593.0000 - val_mae: 1412.6656\n",
            "Epoch 205/500\n",
            "90/90 [==============================] - 0s 912us/step - loss: 1767.3821 - mse: 5646043.5000 - mae: 1767.3823 - val_loss: 1083.0057 - val_mse: 2026444.1250 - val_mae: 1083.0056\n",
            "Epoch 206/500\n",
            "90/90 [==============================] - 0s 868us/step - loss: 1771.9044 - mse: 5540722.0000 - mae: 1771.9043 - val_loss: 1093.8235 - val_mse: 2451910.7500 - val_mae: 1093.8235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im2-1jnV8T_W",
        "outputId": "e2efb930-f027-46af-824d-08f80de95c3b"
      },
      "source": [
        "# Save the running time for each country\n",
        "for i in range(len(countries)):\n",
        "  save_runtime(results_runtime_static[i], path=exp1_runtime_path, country = countries[i], static_learner=True)\n",
        "\n",
        "display_runtime_per_country(results_runtime_static, countries)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_____________Running Time for United_States_of_America________________\n",
            "   PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge   LSTM\n",
            "0            30         0.133             0.039      0.003         0.002          0.031 19.122\n",
            "1            60         0.222             0.094      0.016         0.005          0.006 23.774\n",
            "2            90         0.150             0.101      0.017         0.004          0.005 18.444\n",
            "3           120         0.160             0.134      0.019         0.005          0.004 21.028\n",
            "4           150         0.173             0.170      0.024         0.006          0.004 30.600\n",
            "5           180         0.192             0.204      0.031         0.006          0.005 22.750\n",
            "\n",
            "\n",
            "_____________Running Time for India________________\n",
            "   PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge   LSTM\n",
            "0            30         0.115             0.030      0.002         0.002          0.003  8.926\n",
            "1            60         0.186             0.058      0.005         0.002          0.004 16.521\n",
            "2            90         0.182             0.097      0.013         0.004          0.005 14.201\n",
            "3           120         0.148             0.119      0.018         0.004          0.006 24.162\n",
            "4           150         0.168             0.154      0.023         0.005          0.004 17.268\n",
            "5           180         0.178             0.186      0.028         0.006          0.006 34.851\n",
            "\n",
            "\n",
            "_____________Running Time for Brazil________________\n",
            "   PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge   LSTM\n",
            "0            30         0.121             0.033      0.003         0.003          0.004 20.510\n",
            "1            60         0.208             0.074      0.010         0.003          0.009 18.640\n",
            "2            90         0.236             0.127      0.018         0.005          0.005 28.072\n",
            "3           120         0.254             0.171      0.027         0.006          0.009 21.580\n",
            "4           150         0.163             0.161      0.023         0.005          0.005 27.036\n",
            "5           180         0.186             0.191      0.028         0.006          0.006 28.217\n",
            "\n",
            "\n",
            "_____________Running Time for Russia________________\n",
            "   PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge   LSTM\n",
            "0            30         0.113             0.027      0.002         0.002          0.003 10.539\n",
            "1            60         0.209             0.073      0.008         0.004          0.006 17.884\n",
            "2            90         0.231             0.116      0.017         0.005          0.005 29.275\n",
            "3           120         0.249             0.171      0.026         0.007          0.012 13.745\n",
            "4           150         0.267             0.222      0.036         0.013          0.011 13.795\n",
            "5           180         0.184             0.194      0.027         0.006          0.005 21.388\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NlVmdHK37Bk",
        "outputId": "19cd24fe-acab-4367-8550-1c877753558a"
      },
      "source": [
        "countrywise_error_scores_static = calc_save_err_metric_countrywise(countries, error_metrics, results_static, max_of_pretrain_per_country, max_cases_per_country, path=exp1_path, static_learner=True, transpose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________United_States_of_America____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting   LinearSVR  DecisionTree  BayesianRidge      LSTM\n",
            "0                      MAE        30.000      4093.543          3867.215 1006480.785      4560.880      99068.637  2276.213\n",
            "1                      MAE        60.000      2042.897          1590.215 2886240.906      1845.367       1676.672  9492.547\n",
            "2                      MAE        90.000     28342.025         25233.845  153275.689     25486.720      26921.287 34087.460\n",
            "3                      MAE       120.000      8530.576          6842.980   71920.223      6927.423      10656.728 38005.649\n",
            "4                      MAE       150.000      5149.964          5649.100   54311.707      8471.413       4885.341 10392.261\n",
            "5                      MAE       180.000      7393.098          7508.353   62839.832      7452.797      31479.673 14947.489\n",
            "mean                   NaN       105.000      9258.684          8448.618  705844.857      9124.100      29114.723 18200.270\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_States_of_America____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge  LSTM\n",
            "0                     MAPE        30.000         0.174             0.165     42.852         0.193          4.188 0.085\n",
            "1                     MAPE        60.000         0.086             0.065    120.951         0.075          0.068 0.385\n",
            "2                     MAPE        90.000         0.507             0.444      3.307         0.453          0.479 0.620\n",
            "3                     MAPE       120.000         0.156             0.133      1.354         0.127          0.186 0.656\n",
            "4                     MAPE       150.000         0.134             0.142      1.367         0.211          0.122 0.258\n",
            "5                     MAPE       180.000         0.141             0.143      1.287         0.142          0.651 0.274\n",
            "mean                   NaN       105.000         0.200             0.182     28.520         0.200          0.949 0.380\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_States_of_America____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting   LinearSVR  DecisionTree  BayesianRidge      LSTM\n",
            "0                     RMSE        30.000      4744.386          4609.011 1271395.761      5284.493     113821.065  2910.893\n",
            "1                     RMSE        60.000      2504.816          2314.059 3558130.736      2600.921       2376.807 13575.947\n",
            "2                     RMSE        90.000     30471.877         27734.113  209418.840     27890.611      29160.952 36028.012\n",
            "3                     RMSE       120.000      9865.211          8583.636   81623.132      8256.524      11244.922 41453.027\n",
            "4                     RMSE       150.000      5666.527          6106.212   63430.163      9177.109       6597.735 12100.775\n",
            "5                     RMSE       180.000      9571.022          9741.810   74241.247      9703.350      32886.465 23434.808\n",
            "mean                   NaN       105.000     10470.640          9848.140  876373.313     10485.501      32681.324 21583.910\n",
            "\n",
            "\n",
            "\n",
            "_________________________________India____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge      LSTM\n",
            "0                      MAE        30.000      2219.274          2119.419  49018.158      2133.470      30825.818  2716.518\n",
            "1                      MAE        60.000      4698.939          4000.698  36792.906      4106.457       3116.148  9333.160\n",
            "2                      MAE        90.000     13901.470         10974.902  18090.459     10935.703       5446.482 20098.803\n",
            "3                      MAE       120.000     20994.592         13996.480  20186.811     13912.277       8424.858 45226.456\n",
            "4                      MAE       150.000     24361.284         18253.243 139414.138     18587.637      57737.370 58065.662\n",
            "5                      MAE       180.000     13069.371         16135.610 103960.188     16010.930     132002.377 31493.082\n",
            "mean                   NaN       105.000     13207.488         10913.392  61243.777     10947.746      39592.175 27822.280\n",
            "\n",
            "\n",
            "\n",
            "_________________________________India____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge  LSTM\n",
            "0                     MAPE        30.000         0.553             0.520     11.097         0.523          6.853 0.697\n",
            "1                     MAPE        60.000         0.429             0.358      3.388         0.368          0.267 0.919\n",
            "2                     MAPE        90.000         0.486             0.366      0.702         0.364          0.163 0.745\n",
            "3                     MAPE       120.000         0.360             0.236      0.351         0.234          0.151 0.797\n",
            "4                     MAPE       150.000         0.285             0.211      1.681         0.215          0.670 0.715\n",
            "5                     MAPE       180.000         0.200             0.243      1.424         0.241          1.807 0.389\n",
            "mean                   NaN       105.000         0.385             0.322      3.108         0.324          1.652 0.710\n",
            "\n",
            "\n",
            "\n",
            "_________________________________India____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge      LSTM\n",
            "0                     RMSE        30.000      2630.341          2545.014  65093.625      2558.767      41864.652  3178.205\n",
            "1                     RMSE        60.000      5293.839          4674.987  41330.425      4775.740       4158.752  9670.935\n",
            "2                     RMSE        90.000     15952.494         13441.233  21063.042     13428.463       8319.709 21435.312\n",
            "3                     RMSE       120.000     21886.902         15248.913  23216.691     15225.473       9315.392 45381.186\n",
            "4                     RMSE       150.000     25867.027         20216.601 140951.480     20521.481      65572.720 58481.194\n",
            "5                     RMSE       180.000     16011.392         18700.994 113242.863     18517.183     154810.268 43311.243\n",
            "mean                   NaN       105.000     14606.999         12471.290  67483.021     12504.518      47340.249 30243.013\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Brazil____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge      LSTM\n",
            "0                      MAE        30.000      6198.427          5951.848  56259.985      5912.117      38991.391  7924.319\n",
            "1                      MAE        60.000     10139.263          9648.610  92391.027      9114.667      16350.385 18383.257\n",
            "2                      MAE        90.000      9908.548          3683.370 135009.764      3689.307      44414.595 19606.535\n",
            "3                      MAE       120.000      6664.261          6097.986 158413.612      5062.437      71334.490 18332.620\n",
            "4                      MAE       150.000      6559.735          8215.268 570801.345      8163.937       7106.082 13894.585\n",
            "5                      MAE       180.000     12751.262          6183.065  99863.363      6207.103      10983.856 15008.832\n",
            "mean                   NaN       105.000      8703.583          6630.025 185456.516      6358.261      31530.133 15525.025\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Brazil____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge  LSTM\n",
            "0                     MAPE        30.000         0.609             0.576      5.351         0.570          3.686 0.843\n",
            "1                     MAPE        60.000         0.388             0.368      3.420         0.346          0.600 0.724\n",
            "2                     MAPE        90.000         0.265             0.097      3.619         0.097          1.187 0.528\n",
            "3                     MAPE       120.000         0.149             0.138      3.772         0.117          1.694 0.424\n",
            "4                     MAPE       150.000         0.216             0.263     17.444         0.257          0.210 0.410\n",
            "5                     MAPE       180.000         0.533             0.259      4.019         0.263          0.480 0.626\n",
            "mean                   NaN       105.000         0.360             0.283      6.271         0.275          1.310 0.592\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Brazil____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge      LSTM\n",
            "0                     RMSE        30.000      7389.526          7180.106  70827.009      7147.987      48803.931  8906.726\n",
            "1                     RMSE        60.000     10881.681         10411.376 114873.300      9918.555      21044.106 18822.145\n",
            "2                     RMSE        90.000     10249.724          4279.658 181344.671      4379.845      69068.995 20650.406\n",
            "3                     RMSE       120.000      7470.251          6602.394 178632.735      6037.715      75779.258 18838.202\n",
            "4                     RMSE       150.000      7922.796          9277.860 618638.522      9193.457       8096.932 19050.229\n",
            "5                     RMSE       180.000     13088.589          6807.651 118368.077      7160.208      13608.999 16434.490\n",
            "mean                   NaN       105.000      9500.428          7426.508 213780.719      7306.294      39400.370 17117.033\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Russia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge     LSTM\n",
            "0                      MAE        30.000      4304.059          4175.457 231128.120      4013.103     236842.524 6894.299\n",
            "1                      MAE        60.000      1292.715           437.030 672933.764       422.583     428814.041 6311.923\n",
            "2                      MAE        90.000      2440.714          1137.614 150570.792      1278.357       5003.118 2612.105\n",
            "3                      MAE       120.000      1716.750           971.733 252095.954      1767.440        536.541 2770.321\n",
            "4                      MAE       150.000      1148.643           440.703   2369.570       385.327        380.042 1883.404\n",
            "5                      MAE       180.000      4630.360          4034.664   3679.111      3991.427       4904.962 5759.184\n",
            "mean                   NaN       105.000      2588.873          1866.200 218796.219      1976.373     112746.871 4371.873\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Russia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge  LSTM\n",
            "0                     MAPE        30.000         0.460             0.439     24.177         0.425         24.786 0.761\n",
            "1                     MAPE        60.000         0.152             0.052     80.347         0.051         51.288 0.735\n",
            "2                     MAPE        90.000         0.371             0.175     23.297         0.196          0.754 0.395\n",
            "3                     MAPE       120.000         0.328             0.188     47.419         0.343          0.103 0.532\n",
            "4                     MAPE       150.000         0.227             0.084      0.457         0.071          0.076 0.364\n",
            "5                     MAPE       180.000         0.393             0.336      0.430         0.330          0.425 0.538\n",
            "mean                   NaN       105.000         0.322             0.212     29.355         0.236         12.905 0.554\n",
            "\n",
            "\n",
            "\n",
            "_________________________________Russia____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge     LSTM\n",
            "0                     RMSE        30.000      4639.528          4599.301 353045.034      4470.403     359797.672 7160.711\n",
            "1                     RMSE        60.000      1335.239           548.602 834417.839       538.820     539707.175 6614.185\n",
            "2                     RMSE        90.000      2468.802          1209.699 173987.197      1344.469       5163.417 2917.064\n",
            "3                     RMSE       120.000      1762.492          1056.620 259942.864      1998.025        591.084 3018.977\n",
            "4                     RMSE       150.000      1183.815           488.516   3039.042       526.882        443.774 1983.292\n",
            "5                     RMSE       180.000      5490.130          4898.934   4572.613      4914.731       5655.956 6686.649\n",
            "mean                   NaN       105.000      2813.334          2133.612 271500.765      2298.888     151893.180 4730.146\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "vU9yFzgj_IHX",
        "outputId": "890f5dd0-ba1e-480d-fb97-a2a248c09f23"
      },
      "source": [
        "summary_table_countrywise_static = get_summary_table_countrywise(countrywise_error_scores_static, ['MAPE'], static_learner=True)\n",
        "\n",
        "# Saving the transposed matrix\n",
        "save_summary_table(summary_table_countrywise_static, exp1_summary_path, country=True, static_learner=True, alternate_batch=False, transpose=True)\n",
        "\n",
        "summary_table_countrywise_static"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RandomForest</th>\n",
              "      <th>GradientBoosting</th>\n",
              "      <th>LinearSVR</th>\n",
              "      <th>DecisionTree</th>\n",
              "      <th>BayesianRidge</th>\n",
              "      <th>LSTM</th>\n",
              "      <th>EvaluationMeasurement</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Country(MAPE)</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>United_States_of_America</th>\n",
              "      <td>0.200</td>\n",
              "      <td>0.182</td>\n",
              "      <td>28.520</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.949</td>\n",
              "      <td>0.380</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>India</th>\n",
              "      <td>0.385</td>\n",
              "      <td>0.322</td>\n",
              "      <td>3.108</td>\n",
              "      <td>0.324</td>\n",
              "      <td>1.652</td>\n",
              "      <td>0.710</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Brazil</th>\n",
              "      <td>0.360</td>\n",
              "      <td>0.283</td>\n",
              "      <td>6.271</td>\n",
              "      <td>0.275</td>\n",
              "      <td>1.310</td>\n",
              "      <td>0.592</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Russia</th>\n",
              "      <td>0.322</td>\n",
              "      <td>0.212</td>\n",
              "      <td>29.355</td>\n",
              "      <td>0.236</td>\n",
              "      <td>12.905</td>\n",
              "      <td>0.554</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          RandomForest  ...  EvaluationMeasurement\n",
              "Country(MAPE)                           ...                       \n",
              "United_States_of_America         0.200  ...                   MAPE\n",
              "India                            0.385  ...                   MAPE\n",
              "Brazil                           0.360  ...                   MAPE\n",
              "Russia                           0.322  ...                   MAPE\n",
              "\n",
              "[4 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "cTMQFGlMfCkT",
        "outputId": "dd89f04c-cc56-4386-d1fd-73af5aacabe8"
      },
      "source": [
        "sum_static_countrywise_mean = get_sum_table_combined_mean(countrywise_error_scores_static,results_runtime_static,static_learner=True)\n",
        "save_combined_summary_table(sum_static_countrywise_mean, exp1_summary_path, static_learner=True, transpose=True) \n",
        "sum_static_countrywise_mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RandomForest</th>\n",
              "      <th>GradientBoosting</th>\n",
              "      <th>LinearSVR</th>\n",
              "      <th>DecisionTree</th>\n",
              "      <th>BayesianRidge</th>\n",
              "      <th>LSTM</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Metric</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MAE</th>\n",
              "      <td>8439.657</td>\n",
              "      <td>6964.559</td>\n",
              "      <td>292835.342</td>\n",
              "      <td>7101.620</td>\n",
              "      <td>53245.976</td>\n",
              "      <td>16479.862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MAPE</th>\n",
              "      <td>0.317</td>\n",
              "      <td>0.250</td>\n",
              "      <td>16.813</td>\n",
              "      <td>0.259</td>\n",
              "      <td>4.204</td>\n",
              "      <td>0.559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RMSE</th>\n",
              "      <td>9347.850</td>\n",
              "      <td>7969.888</td>\n",
              "      <td>357284.455</td>\n",
              "      <td>8148.800</td>\n",
              "      <td>67828.781</td>\n",
              "      <td>18418.526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time(sec)</th>\n",
              "      <td>0.185</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.007</td>\n",
              "      <td>20.930</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           RandomForest  GradientBoosting  ...  BayesianRidge      LSTM\n",
              "Metric                                     ...                         \n",
              "MAE            8439.657          6964.559  ...      53245.976 16479.862\n",
              "MAPE              0.317             0.250  ...          4.204     0.559\n",
              "RMSE           9347.850          7969.888  ...      67828.781 18418.526\n",
              "Time(sec)         0.185             0.123  ...          0.007    20.930\n",
              "\n",
              "[4 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIMr3U1QAc63"
      },
      "source": [
        "## Significance tests for Experiment 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "VlZxtco1AbsI",
        "outputId": "424ccfb9-daa7-4362-a192-4d1d601ae506"
      },
      "source": [
        "## EXP1\n",
        "# Significance results for Experiment 1\n",
        "err_metric_for_significance = 'MAPE'\n",
        "significance_thresh = 0.01\n",
        "\n",
        "# Concatenating a population of all results (as in boxplot) for experiment 1\n",
        "concated_df = pd.concat([summary_table_countrywise_incremental.transpose(), summary_table_countrywise_static.transpose()]).transpose().drop(columns=['EvaluationMeasurement'], axis=1)\n",
        "concated_df\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "      <th>RandomForest</th>\n",
              "      <th>GradientBoosting</th>\n",
              "      <th>LinearSVR</th>\n",
              "      <th>DecisionTree</th>\n",
              "      <th>BayesianRidge</th>\n",
              "      <th>LSTM</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Country(MAPE)</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>United_States_of_America</th>\n",
              "      <td>12.173</td>\n",
              "      <td>16.213</td>\n",
              "      <td>20.855</td>\n",
              "      <td>97.783</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.182</td>\n",
              "      <td>28.520</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.949</td>\n",
              "      <td>0.380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>India</th>\n",
              "      <td>1.319</td>\n",
              "      <td>1.161</td>\n",
              "      <td>1.721</td>\n",
              "      <td>6.632</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.322</td>\n",
              "      <td>3.108</td>\n",
              "      <td>0.324</td>\n",
              "      <td>1.652</td>\n",
              "      <td>0.710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Brazil</th>\n",
              "      <td>2.434</td>\n",
              "      <td>2.816</td>\n",
              "      <td>0.949</td>\n",
              "      <td>10.542</td>\n",
              "      <td>0.360</td>\n",
              "      <td>0.283</td>\n",
              "      <td>6.271</td>\n",
              "      <td>0.275</td>\n",
              "      <td>1.310</td>\n",
              "      <td>0.592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Russia</th>\n",
              "      <td>7.548</td>\n",
              "      <td>7.201</td>\n",
              "      <td>8.489</td>\n",
              "      <td>62.896</td>\n",
              "      <td>0.322</td>\n",
              "      <td>0.212</td>\n",
              "      <td>29.355</td>\n",
              "      <td>0.236</td>\n",
              "      <td>12.905</td>\n",
              "      <td>0.554</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         HT_Reg HAT_Reg  ... BayesianRidge  LSTM\n",
              "Country(MAPE)                            ...                    \n",
              "United_States_of_America 12.173  16.213  ...         0.949 0.380\n",
              "India                     1.319   1.161  ...         1.652 0.710\n",
              "Brazil                    2.434   2.816  ...         1.310 0.592\n",
              "Russia                    7.548   7.201  ...        12.905 0.554\n",
              "\n",
              "[4 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hyz23f1PMrpY",
        "outputId": "8bde1fa4-c948-456b-f8b7-cc73f34e6ec5"
      },
      "source": [
        "# Selecting the best algorithm for statistical comparisons\n",
        "# We want to know if the best is statistically significantly better compared to the rest.\n",
        "best_algo = concated_df.mean().sort_values(ascending=True).index[0]\n",
        "best_algo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'GradientBoosting'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFR6AjtxMv-M",
        "outputId": "b731941f-5442-4b4e-dfa8-858545549a3c"
      },
      "source": [
        "print('AVG results across countries')\n",
        "concated_df.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AVG results across countries\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HT_Reg              5.868\n",
              "HAT_Reg             6.848\n",
              "ARF_Reg             8.004\n",
              "PA_Reg             44.463\n",
              "RandomForest        0.317\n",
              "GradientBoosting    0.250\n",
              "LinearSVR          16.813\n",
              "DecisionTree        0.259\n",
              "BayesianRidge       4.204\n",
              "LSTM                0.559\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLYuRNC8M0yF",
        "outputId": "6b2223a8-b62c-4272-ff3e-d40fd2ace328"
      },
      "source": [
        "print('STDEV across countries')\n",
        "concated_df.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STDEV across countries\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HT_Reg              5.002\n",
              "HAT_Reg             6.744\n",
              "ARF_Reg             9.213\n",
              "PA_Reg             43.835\n",
              "RandomForest        0.082\n",
              "GradientBoosting    0.064\n",
              "LinearSVR          14.063\n",
              "DecisionTree        0.053\n",
              "BayesianRidge       5.808\n",
              "LSTM                0.137\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpiiYDApAbeX",
        "outputId": "f70cd3b3-7a5f-4f19-8524-8ec3e38e17c8"
      },
      "source": [
        "# Iterate through all the other algorithms to see if the difference in results is significant\n",
        "competitors = list(concated_df.columns)\n",
        "competitors.remove(best_algo)\n",
        "\n",
        "for significance_thresh in [0.01,0.05]:\n",
        "  print(f'Running significane at: {significance_thresh}')\n",
        "  for competitor in competitors:\n",
        "    # print(competitor)\n",
        "    pval, significant = check_significance(concated_df[best_algo], concated_df[competitor], significance_at=significance_thresh)\n",
        "    print(f'Comparison of {best_algo} to {competitor} pvalue: {pval}   /  significant?: {significant}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running significane at: 0.01\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to HT_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to HAT_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to ARF_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to PA_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to RandomForest pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to LinearSVR pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to DecisionTree pvalue: 0.27332167829229814   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to BayesianRidge pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to LSTM pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Running significane at: 0.05\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to HT_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to HAT_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to ARF_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to PA_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to RandomForest pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to LinearSVR pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to DecisionTree pvalue: 0.27332167829229814   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to BayesianRidge pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of GradientBoosting to LSTM pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
            "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0kt9cV_XWS5",
        "outputId": "4b3263d6-d5ae-4710-8d52-bb3eb91be150"
      },
      "source": [
        "# Iterate through all the other algorithms to see if the difference in results is significant\n",
        "best_algo2 = concated_df.mean().sort_values(ascending=True).index[1]\n",
        "competitors = list(concated_df.columns)\n",
        "competitors.remove(best_algo2)\n",
        "for significance_thresh in [0.01,0.05]:\n",
        "  print(f'Running significane at: {significance_thresh}')\n",
        "  for competitor in competitors:\n",
        "    # print(competitor)\n",
        "    pval, significant = check_significance(concated_df[best_algo2], concated_df[competitor], significance_at=significance_thresh)\n",
        "    print(f'Comparison of {best_algo2} to {competitor} pvalue: {pval}   /  significant?: {significant}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running significane at: 0.01\n",
            "Population too small.\n",
            "Comparison of DecisionTree to HT_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to HAT_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to ARF_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to PA_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to RandomForest pvalue: 0.14412703481601533   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to GradientBoosting pvalue: 0.27332167829229814   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to LinearSVR pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to BayesianRidge pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to LSTM pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Running significane at: 0.05\n",
            "Population too small.\n",
            "Comparison of DecisionTree to HT_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to HAT_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to ARF_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to PA_Reg pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to RandomForest pvalue: 0.14412703481601533   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to GradientBoosting pvalue: 0.27332167829229814   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to LinearSVR pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to BayesianRidge pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of DecisionTree to LSTM pvalue: 0.06788915486182899   /  significant?: Not Significant (Wilcox Test)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
            "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3abKxWJwqNU6"
      },
      "source": [
        "# Experiment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM8GB6LTRPxY"
      },
      "source": [
        "### Creating Combined dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7vb_yHqvwPX"
      },
      "source": [
        "# Reading file \n",
        "#url = 'https://drive.google.com/file/d/1e7NsptfEFLG2gGLykYlrzjNbDJLiRbGm/view?usp=sharing'\n",
        "url = 'https://drive.google.com/file/d/1VH-nkePskK3gT6U5qkoOP-0hFT4beszC/view?usp=sharing'\n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "df = pd.read_csv(path)\n",
        "num_selected_countries = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxDuX7-ansDh"
      },
      "source": [
        "\n",
        "# Pre-processing dataset\n",
        "df = preprocess_dataset(df)\n",
        "\n",
        "# Grouping records by country\n",
        "df_grouped = df.groupby('country')\n",
        "\n",
        "# Taking only those countries which have sufficient data records\n",
        "#valid_countries = get_countries_with_valid_size(df_grouped)\n",
        "\n",
        "# Sorting countries by number of cases\n",
        "df_countries_sortedbycases = get_countries_sortedby_cases(valid_countries, df_grouped)\n",
        "\n",
        "# Taking only top selected countries\n",
        "top_selected_countries = df_countries_sortedbycases.iloc[0:num_selected_countries].index\n",
        "\n",
        "# Calculating targets and lags for the above countries\n",
        "result = get_dataset_with_target(top_selected_countries,df_grouped)\n",
        "\n",
        "# Calculating targets and lags for the above countries\n",
        "result = get_dataset_with_target(top_selected_countries,df_grouped)\n",
        "\n",
        "# Getting max of each subset in pretrain size\n",
        "max_of_pretrain_days = calc_max_of_pretrain_days(pretrain_days,result)\n",
        "\n",
        "# Mean of top selected countries\n",
        "max_selected_countries = result['cases'].max()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtxpRv3FvxdD"
      },
      "source": [
        "### Incremental Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcRN9CoVFigI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "78779a44-eab5-448c-cf75-cbb89ae94994"
      },
      "source": [
        "# Old Script\n",
        "\"\"\"\n",
        "def scikit_multiflow(df, pretrain_days):\n",
        "\n",
        "  model, model_names = instantiate_regressors()\n",
        "\n",
        "  len_countries = len(df['country'].unique())\n",
        "\n",
        "  frames , running_time_frames = [], []\n",
        "\n",
        "  # Setup the evaluator\n",
        "  for day in pretrain_days:\n",
        "\n",
        "      df_subset = create_subset(df,day)\n",
        "      \n",
        "      # Creating a stream from dataframe\n",
        "      stream = DataStream(np.array(df_subset.iloc[:,4:-1]), y=np.array(df_subset.iloc[:,-1])) #TODO: Drop columns with name \n",
        "\n",
        "      pretrain_size = day * len_countries\n",
        "      max_samples = (day + 30) * len_countries #Testing on set one month ahead only\n",
        "\n",
        "      evaluator = EvaluatePrequential(show_plot=False,\n",
        "                                    pretrain_size=pretrain_size,\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\n",
        "                                    max_samples=max_samples)\n",
        "      # Run evaluation\n",
        "      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
        "\n",
        "      # Dictionary to store each iteration error scores\n",
        "      mdl_evaluation_scores = {}\n",
        "\n",
        "      # Adding Evaluation Measurements and pretraining days\n",
        "      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\n",
        "      mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "\n",
        "      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\n",
        "\n",
        "      # Errors of each model on a specific pre-train days\n",
        "      frames.append(mdl_evaluation_df)\n",
        "\n",
        "      # Run time for each algorithm\n",
        "      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
        "\n",
        "   # Final Run Time DataFrame\n",
        "  running_time_df = pd.concat(running_time_frames,ignore_index=True)\n",
        "\n",
        "  # Final Evaluation Score Dataframe\n",
        "  evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
        "  return evaluation_scores_df, running_time_df\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef scikit_multiflow(df, pretrain_days):\\n\\n  model, model_names = instantiate_regressors()\\n\\n  len_countries = len(df['country'].unique())\\n\\n  frames , running_time_frames = [], []\\n\\n  # Setup the evaluator\\n  for day in pretrain_days:\\n\\n      df_subset = create_subset(df,day)\\n      \\n      # Creating a stream from dataframe\\n      stream = DataStream(np.array(df_subset.iloc[:,4:-1]), y=np.array(df_subset.iloc[:,-1])) #TODO: Drop columns with name \\n\\n      pretrain_size = day * len_countries\\n      max_samples = (day + 30) * len_countries #Testing on set one month ahead only\\n\\n      evaluator = EvaluatePrequential(show_plot=False,\\n                                    pretrain_size=pretrain_size,\\n                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\\n                                    max_samples=max_samples)\\n      # Run evaluation\\n      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\\n\\n      # Dictionary to store each iteration error scores\\n      mdl_evaluation_scores = {}\\n\\n      # Adding Evaluation Measurements and pretraining days\\n      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\\n      mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\\n\\n      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\\n\\n      # Errors of each model on a specific pre-train days\\n      frames.append(mdl_evaluation_df)\\n\\n      # Run time for each algorithm\\n      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\\n\\n   # Final Run Time DataFrame\\n  running_time_df = pd.concat(running_time_frames,ignore_index=True)\\n\\n  # Final Evaluation Score Dataframe\\n  evaluation_scores_df = pd.concat(frames, ignore_index=True)\\n  return evaluation_scores_df, running_time_df\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6RpyYrC-HSB"
      },
      "source": [
        "def scikit_multiflow(df, pretrain_days):\r\n",
        "    model, model_names = instantiate_regressors()\r\n",
        "\r\n",
        "    len_countries = len(df['country'].unique())\r\n",
        "\r\n",
        "    frames, running_time_frames = [], []\r\n",
        "\r\n",
        "    # Setup the evaluator\r\n",
        "    for day in pretrain_days:\r\n",
        "        df_subset = create_subset(df, day)\r\n",
        "\r\n",
        "        # Creating a stream from dataframe\r\n",
        "        stream = DataStream(np.array(df_subset.iloc[:, 4:-1]),\r\n",
        "                            y=np.array(df_subset.iloc[:, -1]))  # TODO: Drop columns with name\r\n",
        "\r\n",
        "        pretrain_size = day * len_countries\r\n",
        "        max_samples = pretrain_size + 1  # One Extra Sample\r\n",
        "        testing_samples_size = (day + 30) * len_countries\r\n",
        "\r\n",
        "        evaluator = EvaluatePrequential(show_plot=False,\r\n",
        "                                        pretrain_size=pretrain_size,\r\n",
        "                                        metrics=['mean_square_error', 'mean_absolute_error',\r\n",
        "                                                 'mean_absolute_percentage_error'],\r\n",
        "                                        max_samples=max_samples)\r\n",
        "        # Run evaluation\r\n",
        "        evaluator.evaluate(stream=stream, model=model, model_names=model_names)\r\n",
        "\r\n",
        "        # Added Now\r\n",
        "        X = stream.X[pretrain_size: pretrain_size + testing_samples_size]\r\n",
        "        y = stream.y[pretrain_size: pretrain_size + testing_samples_size]\r\n",
        "\r\n",
        "        prediction = evaluator.predict(X)\r\n",
        "\r\n",
        "        # Since we add one extra sample, reset the evaluator\r\n",
        "        evaluator = reset_evaluator(evaluator)\r\n",
        "\r\n",
        "        evaluator = update_incremental_metrics(evaluator, y, prediction)\r\n",
        "\r\n",
        "        # Dictionary to store each iteration error scores\r\n",
        "        mdl_evaluation_scores = {}\r\n",
        "\r\n",
        "        # Adding Evaluation Measurements and pretraining days\r\n",
        "        mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE', 'MAE', 'MAPE']  # ,'MSE']\r\n",
        "        mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\r\n",
        "\r\n",
        "        mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\r\n",
        "\r\n",
        "        # Errors of each model on a specific pre-train days\r\n",
        "        frames.append(mdl_evaluation_df)\r\n",
        "\r\n",
        "        # Run time for each algorithm\r\n",
        "        running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\r\n",
        "\r\n",
        "    # Final Run Time DataFrame\r\n",
        "    running_time_df = pd.concat(running_time_frames, ignore_index=True)\r\n",
        "\r\n",
        "    # Final Evaluation Score Dataframe\r\n",
        "    evaluation_scores_df = pd.concat(frames, ignore_index=True)\r\n",
        "    return evaluation_scores_df, running_time_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32jMOvemFnHG",
        "outputId": "6bce4863-3d3a-41c2-82b0-4897d7e0ea02"
      },
      "source": [
        "result_skmlflow, running_time_combined_incremental = scikit_multiflow(result, pretrain_days)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 750 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [12.56s]\n",
            "Processed samples: 751\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 659277103.5398\n",
            "HT_Reg - MAPE          : 0.8969\n",
            "HT_Reg - MAE          : 25676.391949\n",
            "HAT_Reg - MSE          : 665215060.7189\n",
            "HAT_Reg - MAPE          : 0.9009\n",
            "HAT_Reg - MAE          : 25791.763428\n",
            "ARF_Reg - MSE          : 617432218.5067\n",
            "ARF_Reg - MAPE          : 0.8679\n",
            "ARF_Reg - MAE          : 24848.183405\n",
            "PA_Reg - MSE          : 3944202760.3951\n",
            "PA_Reg - MAPE          : 2.1937\n",
            "PA_Reg - MAE          : 62802.888153\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 1500 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [32.25s]\n",
            "Processed samples: 1501\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 126089267.9679\n",
            "HT_Reg - MAPE          : 0.5267\n",
            "HT_Reg - MAE          : 11228.947768\n",
            "HAT_Reg - MSE          : 126089267.9679\n",
            "HAT_Reg - MAPE          : 0.5267\n",
            "HAT_Reg - MAE          : 11228.947768\n",
            "ARF_Reg - MSE          : 485466223.3304\n",
            "ARF_Reg - MAPE          : 1.0336\n",
            "ARF_Reg - MAE          : 22033.298058\n",
            "PA_Reg - MSE          : 187409907640.1493\n",
            "PA_Reg - MAPE          : 20.3076\n",
            "PA_Reg - MAE          : 432908.659696\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 2250 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [53.23s]\n",
            "Processed samples: 2251\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1329199402.8041\n",
            "HT_Reg - MAPE          : 1.1044\n",
            "HT_Reg - MAE          : 36458.187048\n",
            "HAT_Reg - MSE          : 1329199402.8041\n",
            "HAT_Reg - MAPE          : 1.1044\n",
            "HAT_Reg - MAE          : 36458.187048\n",
            "ARF_Reg - MSE          : 1293771026.6563\n",
            "ARF_Reg - MAPE          : 1.0896\n",
            "ARF_Reg - MAE          : 35969.028714\n",
            "PA_Reg - MSE          : 2691246400.5024\n",
            "PA_Reg - MAPE          : 1.5715\n",
            "PA_Reg - MAE          : 51877.224295\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 3000 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [90.43s]\n",
            "Processed samples: 3001\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2806816619.4232\n",
            "HT_Reg - MAPE          : 0.8054\n",
            "HT_Reg - MAE          : 52979.398066\n",
            "HAT_Reg - MSE          : 2806816619.4232\n",
            "HAT_Reg - MAPE          : 0.8054\n",
            "HAT_Reg - MAE          : 52979.398066\n",
            "ARF_Reg - MSE          : 3216334294.2183\n",
            "ARF_Reg - MAPE          : 0.8622\n",
            "ARF_Reg - MAE          : 56712.734850\n",
            "PA_Reg - MSE          : 26235553038.8082\n",
            "PA_Reg - MAPE          : 2.4624\n",
            "PA_Reg - MAE          : 161973.927034\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 3750 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [97.32s]\n",
            "Processed samples: 3751\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1529232457.1778\n",
            "HT_Reg - MAPE          : 0.9358\n",
            "HT_Reg - MAE          : 39105.401893\n",
            "HAT_Reg - MSE          : 1529232457.1778\n",
            "HAT_Reg - MAPE          : 0.9358\n",
            "HAT_Reg - MAE          : 39105.401893\n",
            "ARF_Reg - MSE          : 308649118.8651\n",
            "ARF_Reg - MAPE          : 0.4204\n",
            "ARF_Reg - MAE          : 17568.412531\n",
            "PA_Reg - MSE          : 2331702005.2665\n",
            "PA_Reg - MAPE          : 1.1556\n",
            "PA_Reg - MAE          : 48287.700352\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 4500 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [123.64s]\n",
            "Processed samples: 4501\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 45334377.7687\n",
            "HT_Reg - MAPE          : 0.1590\n",
            "HT_Reg - MAE          : 6733.080853\n",
            "HAT_Reg - MSE          : 914188297.1162\n",
            "HAT_Reg - MAPE          : 0.7138\n",
            "HAT_Reg - MAE          : 30235.546913\n",
            "ARF_Reg - MSE          : 1304723530.5725\n",
            "ARF_Reg - MAPE          : 0.8527\n",
            "ARF_Reg - MAE          : 36120.956944\n",
            "PA_Reg - MSE          : 211647096531.5542\n",
            "PA_Reg - MAPE          : 10.8606\n",
            "PA_Reg - MAE          : 460051.189034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xOniEJ11niO"
      },
      "source": [
        "df_skmlflow = calc_save_err_metric_combined(error_metrics, result_skmlflow, max_of_pretrain_days, max_selected_countries, path=exp2_path, static_learner=False, alternate_batch=False, transpose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "Cq1ffodM5C4f",
        "outputId": "7d1dfed3-28bd-4682-ece2-4580345dadce"
      },
      "source": [
        "save_runtime(running_time_combined_incremental, path=exp2_runtime_path, static_learner=False)\n",
        "running_time_combined_incremental"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PretrainDays</th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30</td>\n",
              "      <td>0.307</td>\n",
              "      <td>0.475</td>\n",
              "      <td>11.808</td>\n",
              "      <td>0.002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60</td>\n",
              "      <td>1.179</td>\n",
              "      <td>1.571</td>\n",
              "      <td>29.536</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>90</td>\n",
              "      <td>2.886</td>\n",
              "      <td>4.303</td>\n",
              "      <td>46.069</td>\n",
              "      <td>0.004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>120</td>\n",
              "      <td>3.940</td>\n",
              "      <td>16.657</td>\n",
              "      <td>69.849</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>150</td>\n",
              "      <td>5.302</td>\n",
              "      <td>13.564</td>\n",
              "      <td>78.481</td>\n",
              "      <td>0.004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>180</td>\n",
              "      <td>6.758</td>\n",
              "      <td>22.722</td>\n",
              "      <td>94.178</td>\n",
              "      <td>0.004</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
              "0            30   0.307    0.475   11.808   0.002\n",
              "1            60   1.179    1.571   29.536   0.003\n",
              "2            90   2.886    4.303   46.069   0.004\n",
              "3           120   3.940   16.657   69.849   0.003\n",
              "4           150   5.302   13.564   78.481   0.004\n",
              "5           180   6.758   22.722   94.178   0.004"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "yrGTNx3kTNDU",
        "outputId": "c8a83128-d7b4-4d40-bd86-b5d416fbbd2c"
      },
      "source": [
        "summary_table_incremental = get_summary_table(df_skmlflow, running_time_combined_incremental, error_metrics, static_learner=False)\n",
        "save_summary_table(summary_table_incremental, exp2_summary_path,static_learner=False,alternate_batch=False, transpose=True)\n",
        "summary_table_incremental"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Metric</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MAE</th>\n",
              "      <td>97008.805</td>\n",
              "      <td>97124.232</td>\n",
              "      <td>46052.247</td>\n",
              "      <td>139782.778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MAPE</th>\n",
              "      <td>256.524</td>\n",
              "      <td>256.098</td>\n",
              "      <td>113.012</td>\n",
              "      <td>273.883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RMSE</th>\n",
              "      <td>288704.649</td>\n",
              "      <td>288954.911</td>\n",
              "      <td>147885.278</td>\n",
              "      <td>365712.391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time(sec)</th>\n",
              "      <td>3.395</td>\n",
              "      <td>9.882</td>\n",
              "      <td>54.987</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              HT_Reg    HAT_Reg    ARF_Reg     PA_Reg\n",
              "Metric                                               \n",
              "MAE        97008.805  97124.232  46052.247 139782.778\n",
              "MAPE         256.524    256.098    113.012    273.883\n",
              "RMSE      288704.649 288954.911 147885.278 365712.391\n",
              "Time(sec)      3.395      9.882     54.987      0.003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m4-HvnEwA-l"
      },
      "source": [
        "### Static Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "uYtaCPtjjpv5",
        "outputId": "d6ddbf12-b988-4e56-b221-84addc63b0dd"
      },
      "source": [
        "\"\"\"\n",
        "def scikit_learn(df, training_days):\n",
        "  len_countries = len(df['country'].unique())\n",
        "  frames = []\n",
        "  model_predictions = {\n",
        "      'RandomForest':[],\n",
        "      'GradientBoosting':[],\n",
        "      'LinearSVR':[],\n",
        "      'DecisionTree':[],\n",
        "      'BayesianRidge':[],\n",
        "      'LSTM': []\n",
        "      #'MLPRegressor': [],\n",
        "      #'LinearRegression': []\n",
        "    }\n",
        "\n",
        "  # LSTM (TODO: feel free to put the whole LSTM definition in a funtion)\n",
        "  # params (others like epoch and batch size are also hardcoded in train_test_lstm())\n",
        "  layers = [20, 10, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\n",
        "  activations = ['tanh', 'tanh', 'relu']\n",
        "  patience = 20\n",
        "  total_execution_time = []\n",
        "\n",
        "  for day in training_days:\n",
        "\n",
        "    df_subset = create_subset(df,day)\n",
        "    \n",
        "    train_end_day = day * len_countries\n",
        "    test_end_day = (day+30) * len_countries\n",
        "\n",
        "    train = df_subset.iloc[:train_end_day, :] \n",
        "    test = df_subset.iloc[train_end_day:test_end_day, :]  # Testing on set one month ahead only, hence day+30.\n",
        "    test_df, val_df = get_validation_set(test, batch_size=10)  # Getting validation set from test set\n",
        "\n",
        "    # training and test sets for all models except LSTM\n",
        "    X_train, y_train = train.iloc[:,4:-1], train.iloc[:,-1]\n",
        "    X_test, y_test = test.iloc[:,4:-1], test.iloc[:,-1]\n",
        "\n",
        "    # Validation and test set for LSTM\n",
        "    X_test_lstm, y_test_lstm = test_df.iloc[:, 4:-1], test_df.iloc[:, -1]\n",
        "    X_val, y_val = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\n",
        "\n",
        "    X_train_lstm, X_val, X_test_lstm = reshape_dataframe(X_train, X_val, X_test_lstm)  # reshaping the vector for lstm\n",
        "    cur_exec_time = [day]\n",
        "\n",
        "    rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "    model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train,y_train,X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    gb_reg = GradientBoostingRegressor(random_state=0)\n",
        "    model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    lsv_reg = LinearSVR(random_state=0, tol=1e-5)\n",
        "    model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    dt_reg = DecisionTreeRegressor(random_state=0)\n",
        "    model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    br_reg = BayesianRidge()\n",
        "    model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    '''\n",
        "    mlp_reg = MLPRegressor(random_state=1, max_iter=500)\n",
        "    model_predictions['MLPRegressor'], exec_time = train_test_model(mlp_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    lin_reg = LinearRegression()\n",
        "    model_predictions['LinearRegression'], exec_time = train_test_model(lin_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "    '''\n",
        "\n",
        "    lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\n",
        "    model_predictions['LSTM'], exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train, X_val, y_val, X_test_lstm, patience)         \n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "    mdl_evaluation_df = get_scores(y_test,y_test_lstm, model_predictions, day)\n",
        "    total_execution_time.append(cur_exec_time)\n",
        "    frames.append(mdl_evaluation_df)\n",
        "\n",
        "  evaluation_score_df = pd.concat(frames, ignore_index=True)\n",
        "  running_time_df = get_running_time_per_model_static_learner(model_predictions,total_execution_time)\n",
        "  return evaluation_score_df, running_time_df\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef scikit_learn(df, training_days):\\n  len_countries = len(df['country'].unique())\\n  frames = []\\n  model_predictions = {\\n      'RandomForest':[],\\n      'GradientBoosting':[],\\n      'LinearSVR':[],\\n      'DecisionTree':[],\\n      'BayesianRidge':[],\\n      'LSTM': []\\n      #'MLPRegressor': [],\\n      #'LinearRegression': []\\n    }\\n\\n  # LSTM (TODO: feel free to put the whole LSTM definition in a funtion)\\n  # params (others like epoch and batch size are also hardcoded in train_test_lstm())\\n  layers = [20, 10, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\\n  activations = ['tanh', 'tanh', 'relu']\\n  patience = 20\\n  total_execution_time = []\\n\\n  for day in training_days:\\n\\n    df_subset = create_subset(df,day)\\n    \\n    train_end_day = day * len_countries\\n    test_end_day = (day+30) * len_countries\\n\\n    train = df_subset.iloc[:train_end_day, :] \\n    test = df_subset.iloc[train_end_day:test_end_day, :]  # Testing on set one month ahead only, hence day+30.\\n    test_df, val_df = get_validation_set(test, batch_size=10)  # Getting validation set from test set\\n\\n    # training and test sets for all models except LSTM\\n    X_train, y_train = train.iloc[:,4:-1], train.iloc[:,-1]\\n    X_test, y_test = test.iloc[:,4:-1], test.iloc[:,-1]\\n\\n    # Validation and test set for LSTM\\n    X_test_lstm, y_test_lstm = test_df.iloc[:, 4:-1], test_df.iloc[:, -1]\\n    X_val, y_val = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\\n\\n    X_train_lstm, X_val, X_test_lstm = reshape_dataframe(X_train, X_val, X_test_lstm)  # reshaping the vector for lstm\\n    cur_exec_time = [day]\\n\\n    rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\\n    model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train,y_train,X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    gb_reg = GradientBoostingRegressor(random_state=0)\\n    model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    lsv_reg = LinearSVR(random_state=0, tol=1e-5)\\n    model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    dt_reg = DecisionTreeRegressor(random_state=0)\\n    model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    br_reg = BayesianRidge()\\n    model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    '''\\n    mlp_reg = MLPRegressor(random_state=1, max_iter=500)\\n    model_predictions['MLPRegressor'], exec_time = train_test_model(mlp_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    lin_reg = LinearRegression()\\n    model_predictions['LinearRegression'], exec_time = train_test_model(lin_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n    '''\\n\\n    lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\\n    model_predictions['LSTM'], exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train, X_val, y_val, X_test_lstm, patience)         \\n    cur_exec_time.append(exec_time)\\n\\n    mdl_evaluation_df = get_scores(y_test,y_test_lstm, model_predictions, day)\\n    total_execution_time.append(cur_exec_time)\\n    frames.append(mdl_evaluation_df)\\n\\n  evaluation_score_df = pd.concat(frames, ignore_index=True)\\n  running_time_df = get_running_time_per_model_static_learner(model_predictions,total_execution_time)\\n  return evaluation_score_df, running_time_df\\n  \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2oijsU0UJkD"
      },
      "source": [
        "def scikit_learn(df, training_days):\n",
        "  len_countries = len(df['country'].unique())\n",
        "  frames = []\n",
        "  model_predictions = {\n",
        "      'RandomForest':[],\n",
        "      'GradientBoosting':[],\n",
        "      'LinearSVR':[],\n",
        "      'DecisionTree':[],\n",
        "      'BayesianRidge':[],\n",
        "      'LSTM': []\n",
        "      #'MLPRegressor': [],\n",
        "      #'LinearRegression': []\n",
        "    }\n",
        "  total_execution_time = []\n",
        "\n",
        "  # LSTM (TODO: feel free to put the whole LSTM definition in a funtion)\n",
        "  # params (others like epoch and batch size are also hardcoded in train_test_lstm())\n",
        "  layers = [50, 30, 20, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\n",
        "  activations = ['tanh', 'tanh', 'relu']\n",
        "  epochs = 500\n",
        "  patience = 20 * num_selected_countries\n",
        "  batch_size_lstm = 10 * num_selected_countries\n",
        "  \n",
        "\n",
        "  for day in training_days:\n",
        "\n",
        "    df_subset = create_subset(df,day)\n",
        "    \n",
        "    train_end_day = day * len_countries\n",
        "    test_end_day = (day+30) * len_countries\n",
        "\n",
        "    train = df_subset.iloc[:train_end_day, :] \n",
        "    test = df_subset.iloc[train_end_day:test_end_day, :]  # Testing on set one month ahead only, hence day+30.\n",
        "    cur_exec_time = [day]\n",
        "\n",
        "    # training and test sets for all models except LSTM\n",
        "    X_train, y_train = train.iloc[:,4:-1], train.iloc[:,-1]\n",
        "    X_test, y_test = test.iloc[:,4:-1], test.iloc[:,-1]\n",
        "\n",
        "    # Seperating validation set from train set\n",
        "    train_df, val_df = get_validation_set(train, batch_size=10)\n",
        "\n",
        "    # Splitting test and validation into dependent and independent sets\n",
        "    X_train_batch, y_train_batch = train_df.iloc[:, 4:-1], train_df.iloc[:, -1]  # Consist only odd batches\n",
        "    X_val_batch, y_val_batch = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\n",
        "\n",
        "    # Normalizing dataset\n",
        "    X_train_lstm_norm, X_test_lstm_norm, X_val_lstm_norm = normalize_dataset(X_train_batch, X_test, X_val_batch)\n",
        "\n",
        "    # Reshaping the dataframes\n",
        "    X_train_lstm, X_val_lstm, X_test_lstm = reshape_dataframe(X_train_lstm_norm, X_val_lstm_norm, X_test_lstm_norm)\n",
        "   \n",
        "\n",
        "    rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "    model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train,y_train,X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    gb_reg = GradientBoostingRegressor(random_state=0)\n",
        "    model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    lsv_reg = LinearSVR(random_state=0, tol=1e-5)\n",
        "    model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    dt_reg = DecisionTreeRegressor(random_state=0)\n",
        "    model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    br_reg = BayesianRidge()\n",
        "    model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\n",
        "    model_predictions['LSTM'],exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train_batch, X_val_lstm, y_val_batch, X_test_lstm, patience, epochs, batch_size_lstm)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    mdl_evaluation_df = get_scores(y_test, model_predictions, day)\n",
        "    total_execution_time.append(cur_exec_time)\n",
        "    frames.append(mdl_evaluation_df)\n",
        "\n",
        "  evaluation_score_df = pd.concat(frames, ignore_index=True)\n",
        "  running_time_df = get_running_time_per_model_static_learner(model_predictions,total_execution_time)\n",
        "  return evaluation_score_df, running_time_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inEIGOoYn0ng",
        "outputId": "6db48ce0-6b13-46a5-f780-f44bf1de6afe"
      },
      "source": [
        "result_sklearn, running_time_static = scikit_learn(result, pretrain_days)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 380 samples, validate on 370 samples\n",
            "Epoch 1/500\n",
            "380/380 [==============================] - 2s 5ms/step - loss: 2700.3827 - mse: 37416908.0000 - mae: 2700.3828 - val_loss: 2149.9685 - val_mse: 29119166.0000 - val_mae: 2149.9685\n",
            "Epoch 2/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2700.3526 - mse: 37416764.0000 - mae: 2700.3525 - val_loss: 2149.9390 - val_mse: 29119056.0000 - val_mae: 2149.9390\n",
            "Epoch 3/500\n",
            "380/380 [==============================] - 0s 70us/step - loss: 2700.3208 - mse: 37416608.0000 - mae: 2700.3208 - val_loss: 2149.8889 - val_mse: 29118920.0000 - val_mae: 2149.8889\n",
            "Epoch 4/500\n",
            "380/380 [==============================] - 0s 72us/step - loss: 2700.2522 - mse: 37416396.0000 - mae: 2700.2522 - val_loss: 2149.7351 - val_mse: 29118594.0000 - val_mae: 2149.7351\n",
            "Epoch 5/500\n",
            "380/380 [==============================] - 0s 74us/step - loss: 2700.0078 - mse: 37415712.0000 - mae: 2700.0078 - val_loss: 2149.1125 - val_mse: 29117388.0000 - val_mae: 2149.1125\n",
            "Epoch 6/500\n",
            "380/380 [==============================] - 0s 68us/step - loss: 2699.0423 - mse: 37413192.0000 - mae: 2699.0425 - val_loss: 2147.3426 - val_mse: 29113900.0000 - val_mae: 2147.3425\n",
            "Epoch 7/500\n",
            "380/380 [==============================] - 0s 73us/step - loss: 2696.7399 - mse: 37406756.0000 - mae: 2696.7400 - val_loss: 2144.9561 - val_mse: 29108900.0000 - val_mae: 2144.9561\n",
            "Epoch 8/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2694.0849 - mse: 37399692.0000 - mae: 2694.0850 - val_loss: 2142.3119 - val_mse: 29103200.0000 - val_mae: 2142.3120\n",
            "Epoch 9/500\n",
            "380/380 [==============================] - 0s 73us/step - loss: 2690.8413 - mse: 37387764.0000 - mae: 2690.8416 - val_loss: 2139.5010 - val_mse: 29097026.0000 - val_mae: 2139.5010\n",
            "Epoch 10/500\n",
            "380/380 [==============================] - 0s 73us/step - loss: 2687.5560 - mse: 37374932.0000 - mae: 2687.5559 - val_loss: 2136.5073 - val_mse: 29090290.0000 - val_mae: 2136.5073\n",
            "Epoch 11/500\n",
            "380/380 [==============================] - 0s 81us/step - loss: 2683.7457 - mse: 37363240.0000 - mae: 2683.7456 - val_loss: 2133.1999 - val_mse: 29082756.0000 - val_mae: 2133.2000\n",
            "Epoch 12/500\n",
            "380/380 [==============================] - 0s 75us/step - loss: 2679.2291 - mse: 37347576.0000 - mae: 2679.2290 - val_loss: 2129.8055 - val_mse: 29074586.0000 - val_mae: 2129.8054\n",
            "Epoch 13/500\n",
            "380/380 [==============================] - 0s 78us/step - loss: 2676.8231 - mse: 37333920.0000 - mae: 2676.8230 - val_loss: 2126.4049 - val_mse: 29066090.0000 - val_mae: 2126.4048\n",
            "Epoch 14/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2671.3079 - mse: 37310480.0000 - mae: 2671.3079 - val_loss: 2122.8025 - val_mse: 29056556.0000 - val_mae: 2122.8025\n",
            "Epoch 15/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2667.6693 - mse: 37299768.0000 - mae: 2667.6694 - val_loss: 2119.1935 - val_mse: 29046770.0000 - val_mae: 2119.1934\n",
            "Epoch 16/500\n",
            "380/380 [==============================] - 0s 87us/step - loss: 2662.0168 - mse: 37272160.0000 - mae: 2662.0168 - val_loss: 2115.4171 - val_mse: 29035586.0000 - val_mae: 2115.4172\n",
            "Epoch 17/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2656.1313 - mse: 37239200.0000 - mae: 2656.1313 - val_loss: 2111.3250 - val_mse: 29023410.0000 - val_mae: 2111.3250\n",
            "Epoch 18/500\n",
            "380/380 [==============================] - 0s 72us/step - loss: 2651.2179 - mse: 37221712.0000 - mae: 2651.2180 - val_loss: 2107.1288 - val_mse: 29010576.0000 - val_mae: 2107.1287\n",
            "Epoch 19/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2646.4045 - mse: 37197912.0000 - mae: 2646.4045 - val_loss: 2102.7471 - val_mse: 28996226.0000 - val_mae: 2102.7471\n",
            "Epoch 20/500\n",
            "380/380 [==============================] - 0s 101us/step - loss: 2641.3461 - mse: 37179684.0000 - mae: 2641.3459 - val_loss: 2098.2682 - val_mse: 28981852.0000 - val_mae: 2098.2683\n",
            "Epoch 21/500\n",
            "380/380 [==============================] - 0s 89us/step - loss: 2636.1727 - mse: 37156044.0000 - mae: 2636.1726 - val_loss: 2093.4214 - val_mse: 28965342.0000 - val_mae: 2093.4214\n",
            "Epoch 22/500\n",
            "380/380 [==============================] - 0s 87us/step - loss: 2632.1163 - mse: 37114992.0000 - mae: 2632.1162 - val_loss: 2088.4636 - val_mse: 28948432.0000 - val_mae: 2088.4636\n",
            "Epoch 23/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2622.0325 - mse: 37089028.0000 - mae: 2622.0325 - val_loss: 2083.1176 - val_mse: 28929668.0000 - val_mae: 2083.1177\n",
            "Epoch 24/500\n",
            "380/380 [==============================] - 0s 76us/step - loss: 2620.2341 - mse: 37079004.0000 - mae: 2620.2341 - val_loss: 2077.9924 - val_mse: 28911102.0000 - val_mae: 2077.9924\n",
            "Epoch 25/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2612.6688 - mse: 37028628.0000 - mae: 2612.6689 - val_loss: 2072.5468 - val_mse: 28890438.0000 - val_mae: 2072.5469\n",
            "Epoch 26/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2604.7264 - mse: 37025184.0000 - mae: 2604.7263 - val_loss: 2067.0973 - val_mse: 28867942.0000 - val_mae: 2067.0974\n",
            "Epoch 27/500\n",
            "380/380 [==============================] - 0s 92us/step - loss: 2598.0885 - mse: 36967208.0000 - mae: 2598.0884 - val_loss: 2061.4203 - val_mse: 28843606.0000 - val_mae: 2061.4202\n",
            "Epoch 28/500\n",
            "380/380 [==============================] - 0s 88us/step - loss: 2590.1259 - mse: 36942072.0000 - mae: 2590.1257 - val_loss: 2055.7795 - val_mse: 28817644.0000 - val_mae: 2055.7793\n",
            "Epoch 29/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2585.5745 - mse: 36914504.0000 - mae: 2585.5745 - val_loss: 2050.4853 - val_mse: 28790692.0000 - val_mae: 2050.4854\n",
            "Epoch 30/500\n",
            "380/380 [==============================] - 0s 78us/step - loss: 2578.5906 - mse: 36885224.0000 - mae: 2578.5906 - val_loss: 2044.8625 - val_mse: 28761116.0000 - val_mae: 2044.8625\n",
            "Epoch 31/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2568.5827 - mse: 36840220.0000 - mae: 2568.5830 - val_loss: 2038.6984 - val_mse: 28727370.0000 - val_mae: 2038.6984\n",
            "Epoch 32/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2564.7493 - mse: 36824276.0000 - mae: 2564.7493 - val_loss: 2033.1631 - val_mse: 28692920.0000 - val_mae: 2033.1632\n",
            "Epoch 33/500\n",
            "380/380 [==============================] - 0s 74us/step - loss: 2555.9404 - mse: 36783812.0000 - mae: 2555.9404 - val_loss: 2027.2862 - val_mse: 28655618.0000 - val_mae: 2027.2861\n",
            "Epoch 34/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2547.4486 - mse: 36747696.0000 - mae: 2547.4487 - val_loss: 2021.8221 - val_mse: 28616234.0000 - val_mae: 2021.8221\n",
            "Epoch 35/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2544.8839 - mse: 36720536.0000 - mae: 2544.8838 - val_loss: 2016.7489 - val_mse: 28579688.0000 - val_mae: 2016.7488\n",
            "Epoch 36/500\n",
            "380/380 [==============================] - 0s 91us/step - loss: 2537.4432 - mse: 36639740.0000 - mae: 2537.4431 - val_loss: 2011.9347 - val_mse: 28541308.0000 - val_mae: 2011.9346\n",
            "Epoch 37/500\n",
            "380/380 [==============================] - 0s 84us/step - loss: 2531.2814 - mse: 36637528.0000 - mae: 2531.2812 - val_loss: 2007.2235 - val_mse: 28501354.0000 - val_mae: 2007.2235\n",
            "Epoch 38/500\n",
            "380/380 [==============================] - 0s 97us/step - loss: 2528.2887 - mse: 36593552.0000 - mae: 2528.2888 - val_loss: 2002.6473 - val_mse: 28461820.0000 - val_mae: 2002.6473\n",
            "Epoch 39/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2520.9983 - mse: 36542964.0000 - mae: 2520.9983 - val_loss: 1997.9503 - val_mse: 28419664.0000 - val_mae: 1997.9503\n",
            "Epoch 40/500\n",
            "380/380 [==============================] - 0s 84us/step - loss: 2518.1105 - mse: 36563288.0000 - mae: 2518.1106 - val_loss: 1993.8424 - val_mse: 28381040.0000 - val_mae: 1993.8424\n",
            "Epoch 41/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2513.5075 - mse: 36492836.0000 - mae: 2513.5073 - val_loss: 1989.5453 - val_mse: 28339604.0000 - val_mae: 1989.5453\n",
            "Epoch 42/500\n",
            "380/380 [==============================] - 0s 77us/step - loss: 2506.4793 - mse: 36478104.0000 - mae: 2506.4792 - val_loss: 1985.8122 - val_mse: 28302090.0000 - val_mae: 1985.8124\n",
            "Epoch 43/500\n",
            "380/380 [==============================] - 0s 93us/step - loss: 2504.6046 - mse: 36392340.0000 - mae: 2504.6045 - val_loss: 1982.2250 - val_mse: 28263912.0000 - val_mae: 1982.2250\n",
            "Epoch 44/500\n",
            "380/380 [==============================] - 0s 84us/step - loss: 2511.7348 - mse: 36409380.0000 - mae: 2511.7349 - val_loss: 1979.0922 - val_mse: 28227540.0000 - val_mae: 1979.0923\n",
            "Epoch 45/500\n",
            "380/380 [==============================] - 0s 74us/step - loss: 2496.9597 - mse: 36345156.0000 - mae: 2496.9597 - val_loss: 1976.0440 - val_mse: 28189806.0000 - val_mae: 1976.0441\n",
            "Epoch 46/500\n",
            "380/380 [==============================] - 0s 73us/step - loss: 2494.1494 - mse: 36264520.0000 - mae: 2494.1494 - val_loss: 1973.1240 - val_mse: 28148248.0000 - val_mae: 1973.1240\n",
            "Epoch 47/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2488.3193 - mse: 36326224.0000 - mae: 2488.3193 - val_loss: 1970.7955 - val_mse: 28110798.0000 - val_mae: 1970.7957\n",
            "Epoch 48/500\n",
            "380/380 [==============================] - 0s 89us/step - loss: 2484.1041 - mse: 36220512.0000 - mae: 2484.1040 - val_loss: 1968.7977 - val_mse: 28076486.0000 - val_mae: 1968.7979\n",
            "Epoch 49/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2475.5404 - mse: 36213380.0000 - mae: 2475.5405 - val_loss: 1966.8572 - val_mse: 28042040.0000 - val_mae: 1966.8571\n",
            "Epoch 50/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2483.9441 - mse: 36208428.0000 - mae: 2483.9441 - val_loss: 1964.9536 - val_mse: 28007400.0000 - val_mae: 1964.9535\n",
            "Epoch 51/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2477.9881 - mse: 36083188.0000 - mae: 2477.9880 - val_loss: 1963.0570 - val_mse: 27971528.0000 - val_mae: 1963.0571\n",
            "Epoch 52/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2473.5876 - mse: 36115940.0000 - mae: 2473.5874 - val_loss: 1961.2004 - val_mse: 27933604.0000 - val_mae: 1961.2003\n",
            "Epoch 53/500\n",
            "380/380 [==============================] - 0s 76us/step - loss: 2465.4742 - mse: 36047020.0000 - mae: 2465.4744 - val_loss: 1959.2146 - val_mse: 27893618.0000 - val_mae: 1959.2145\n",
            "Epoch 54/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2471.8622 - mse: 36033136.0000 - mae: 2471.8621 - val_loss: 1957.5580 - val_mse: 27858704.0000 - val_mae: 1957.5580\n",
            "Epoch 55/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2461.8728 - mse: 35979940.0000 - mae: 2461.8728 - val_loss: 1956.1380 - val_mse: 27825332.0000 - val_mae: 1956.1381\n",
            "Epoch 56/500\n",
            "380/380 [==============================] - 0s 88us/step - loss: 2451.9147 - mse: 35965072.0000 - mae: 2451.9148 - val_loss: 1954.8564 - val_mse: 27785390.0000 - val_mae: 1954.8564\n",
            "Epoch 57/500\n",
            "380/380 [==============================] - 0s 80us/step - loss: 2464.0377 - mse: 35934892.0000 - mae: 2464.0378 - val_loss: 1953.8424 - val_mse: 27756334.0000 - val_mae: 1953.8424\n",
            "Epoch 58/500\n",
            "380/380 [==============================] - 0s 93us/step - loss: 2457.2007 - mse: 35907256.0000 - mae: 2457.2007 - val_loss: 1952.8413 - val_mse: 27723804.0000 - val_mae: 1952.8412\n",
            "Epoch 59/500\n",
            "380/380 [==============================] - 0s 78us/step - loss: 2460.5976 - mse: 35891388.0000 - mae: 2460.5977 - val_loss: 1951.8980 - val_mse: 27695906.0000 - val_mae: 1951.8979\n",
            "Epoch 60/500\n",
            "380/380 [==============================] - 0s 91us/step - loss: 2449.3119 - mse: 35850716.0000 - mae: 2449.3118 - val_loss: 1951.0770 - val_mse: 27662054.0000 - val_mae: 1951.0770\n",
            "Epoch 61/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2444.3074 - mse: 35779728.0000 - mae: 2444.3076 - val_loss: 1950.3692 - val_mse: 27627078.0000 - val_mae: 1950.3693\n",
            "Epoch 62/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2448.4422 - mse: 35844168.0000 - mae: 2448.4424 - val_loss: 1949.7960 - val_mse: 27601598.0000 - val_mae: 1949.7959\n",
            "Epoch 63/500\n",
            "380/380 [==============================] - 0s 78us/step - loss: 2450.4202 - mse: 35670640.0000 - mae: 2450.4202 - val_loss: 1949.2833 - val_mse: 27578020.0000 - val_mae: 1949.2833\n",
            "Epoch 64/500\n",
            "380/380 [==============================] - 0s 69us/step - loss: 2439.3276 - mse: 35695300.0000 - mae: 2439.3276 - val_loss: 1948.8800 - val_mse: 27548306.0000 - val_mae: 1948.8799\n",
            "Epoch 65/500\n",
            "380/380 [==============================] - 0s 92us/step - loss: 2434.4434 - mse: 35658788.0000 - mae: 2434.4434 - val_loss: 1948.4399 - val_mse: 27523774.0000 - val_mae: 1948.4398\n",
            "Epoch 66/500\n",
            "380/380 [==============================] - 0s 86us/step - loss: 2436.6167 - mse: 35647828.0000 - mae: 2436.6167 - val_loss: 1947.9489 - val_mse: 27497784.0000 - val_mae: 1947.9490\n",
            "Epoch 67/500\n",
            "380/380 [==============================] - 0s 77us/step - loss: 2432.1736 - mse: 35578956.0000 - mae: 2432.1736 - val_loss: 1947.5477 - val_mse: 27467270.0000 - val_mae: 1947.5476\n",
            "Epoch 68/500\n",
            "380/380 [==============================] - 0s 80us/step - loss: 2424.6749 - mse: 35525452.0000 - mae: 2424.6748 - val_loss: 1947.2634 - val_mse: 27431964.0000 - val_mae: 1947.2633\n",
            "Epoch 69/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2435.4722 - mse: 35535084.0000 - mae: 2435.4724 - val_loss: 1946.8725 - val_mse: 27401632.0000 - val_mae: 1946.8727\n",
            "Epoch 70/500\n",
            "380/380 [==============================] - 0s 77us/step - loss: 2433.4429 - mse: 35419632.0000 - mae: 2433.4429 - val_loss: 1946.4602 - val_mse: 27375600.0000 - val_mae: 1946.4601\n",
            "Epoch 71/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2424.6119 - mse: 35415072.0000 - mae: 2424.6118 - val_loss: 1946.1837 - val_mse: 27348128.0000 - val_mae: 1946.1836\n",
            "Epoch 72/500\n",
            "380/380 [==============================] - 0s 89us/step - loss: 2425.8453 - mse: 35436700.0000 - mae: 2425.8455 - val_loss: 1946.0072 - val_mse: 27318054.0000 - val_mae: 1946.0073\n",
            "Epoch 73/500\n",
            "380/380 [==============================] - 0s 88us/step - loss: 2430.1619 - mse: 35317796.0000 - mae: 2430.1621 - val_loss: 1945.7576 - val_mse: 27293378.0000 - val_mae: 1945.7576\n",
            "Epoch 74/500\n",
            "380/380 [==============================] - 0s 86us/step - loss: 2422.0916 - mse: 35305932.0000 - mae: 2422.0916 - val_loss: 1945.5781 - val_mse: 27270166.0000 - val_mae: 1945.5782\n",
            "Epoch 75/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2432.4771 - mse: 35304848.0000 - mae: 2432.4771 - val_loss: 1945.3862 - val_mse: 27247038.0000 - val_mae: 1945.3861\n",
            "Epoch 76/500\n",
            "380/380 [==============================] - 0s 81us/step - loss: 2424.6722 - mse: 35312004.0000 - mae: 2424.6721 - val_loss: 1945.1288 - val_mse: 27225472.0000 - val_mae: 1945.1287\n",
            "Epoch 77/500\n",
            "380/380 [==============================] - 0s 89us/step - loss: 2419.6653 - mse: 35198616.0000 - mae: 2419.6653 - val_loss: 1944.9248 - val_mse: 27199956.0000 - val_mae: 1944.9248\n",
            "Epoch 78/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2413.6404 - mse: 35312380.0000 - mae: 2413.6404 - val_loss: 1944.8298 - val_mse: 27168270.0000 - val_mae: 1944.8300\n",
            "Epoch 79/500\n",
            "380/380 [==============================] - 0s 75us/step - loss: 2405.2273 - mse: 35176156.0000 - mae: 2405.2273 - val_loss: 1944.7373 - val_mse: 27140780.0000 - val_mae: 1944.7373\n",
            "Epoch 80/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2421.2144 - mse: 35222264.0000 - mae: 2421.2144 - val_loss: 1944.6294 - val_mse: 27119700.0000 - val_mae: 1944.6294\n",
            "Epoch 81/500\n",
            "380/380 [==============================] - 0s 76us/step - loss: 2414.9612 - mse: 35178948.0000 - mae: 2414.9612 - val_loss: 1944.5878 - val_mse: 27096660.0000 - val_mae: 1944.5879\n",
            "Epoch 82/500\n",
            "380/380 [==============================] - 0s 94us/step - loss: 2401.3257 - mse: 34979264.0000 - mae: 2401.3257 - val_loss: 1944.5613 - val_mse: 27073098.0000 - val_mae: 1944.5613\n",
            "Epoch 83/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2410.1322 - mse: 35019872.0000 - mae: 2410.1323 - val_loss: 1944.4120 - val_mse: 27056504.0000 - val_mae: 1944.4120\n",
            "Epoch 84/500\n",
            "380/380 [==============================] - 0s 96us/step - loss: 2405.2278 - mse: 35117956.0000 - mae: 2405.2275 - val_loss: 1944.3563 - val_mse: 27034558.0000 - val_mae: 1944.3562\n",
            "Epoch 85/500\n",
            "380/380 [==============================] - 0s 93us/step - loss: 2408.6026 - mse: 34999672.0000 - mae: 2408.6025 - val_loss: 1944.2763 - val_mse: 27016518.0000 - val_mae: 1944.2764\n",
            "Epoch 86/500\n",
            "380/380 [==============================] - 0s 79us/step - loss: 2397.3235 - mse: 35036824.0000 - mae: 2397.3235 - val_loss: 1944.4010 - val_mse: 26990802.0000 - val_mae: 1944.4010\n",
            "Epoch 87/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2410.9439 - mse: 34839712.0000 - mae: 2410.9438 - val_loss: 1944.2590 - val_mse: 26976782.0000 - val_mae: 1944.2589\n",
            "Epoch 88/500\n",
            "380/380 [==============================] - 0s 101us/step - loss: 2401.9754 - mse: 34977344.0000 - mae: 2401.9753 - val_loss: 1944.1347 - val_mse: 26960932.0000 - val_mae: 1944.1348\n",
            "Epoch 89/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2405.3807 - mse: 34871312.0000 - mae: 2405.3809 - val_loss: 1944.0644 - val_mse: 26944092.0000 - val_mae: 1944.0646\n",
            "Epoch 90/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2401.8294 - mse: 34949540.0000 - mae: 2401.8293 - val_loss: 1943.9144 - val_mse: 26931664.0000 - val_mae: 1943.9146\n",
            "Epoch 91/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2396.3967 - mse: 34817972.0000 - mae: 2396.3967 - val_loss: 1943.8112 - val_mse: 26919466.0000 - val_mae: 1943.8112\n",
            "Epoch 92/500\n",
            "380/380 [==============================] - 0s 92us/step - loss: 2387.4074 - mse: 34954772.0000 - mae: 2387.4072 - val_loss: 1943.7726 - val_mse: 26902288.0000 - val_mae: 1943.7726\n",
            "Epoch 93/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2390.1651 - mse: 34771116.0000 - mae: 2390.1650 - val_loss: 1943.8036 - val_mse: 26883870.0000 - val_mae: 1943.8037\n",
            "Epoch 94/500\n",
            "380/380 [==============================] - 0s 75us/step - loss: 2393.2271 - mse: 34818136.0000 - mae: 2393.2271 - val_loss: 1943.6021 - val_mse: 26873164.0000 - val_mae: 1943.6021\n",
            "Epoch 95/500\n",
            "380/380 [==============================] - 0s 88us/step - loss: 2409.5717 - mse: 34788368.0000 - mae: 2409.5718 - val_loss: 1943.5142 - val_mse: 26858100.0000 - val_mae: 1943.5142\n",
            "Epoch 96/500\n",
            "380/380 [==============================] - 0s 87us/step - loss: 2407.6468 - mse: 34813972.0000 - mae: 2407.6467 - val_loss: 1943.2985 - val_mse: 26848344.0000 - val_mae: 1943.2985\n",
            "Epoch 97/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2389.9127 - mse: 34852940.0000 - mae: 2389.9128 - val_loss: 1943.3440 - val_mse: 26829132.0000 - val_mae: 1943.3439\n",
            "Epoch 98/500\n",
            "380/380 [==============================] - 0s 84us/step - loss: 2383.8723 - mse: 34722532.0000 - mae: 2383.8723 - val_loss: 1943.2488 - val_mse: 26813864.0000 - val_mae: 1943.2488\n",
            "Epoch 99/500\n",
            "380/380 [==============================] - 0s 94us/step - loss: 2395.8394 - mse: 34713836.0000 - mae: 2395.8394 - val_loss: 1943.1689 - val_mse: 26796618.0000 - val_mae: 1943.1689\n",
            "Epoch 100/500\n",
            "380/380 [==============================] - 0s 94us/step - loss: 2393.3387 - mse: 34674624.0000 - mae: 2393.3389 - val_loss: 1943.0314 - val_mse: 26784760.0000 - val_mae: 1943.0312\n",
            "Epoch 101/500\n",
            "380/380 [==============================] - 0s 93us/step - loss: 2383.4080 - mse: 34733232.0000 - mae: 2383.4080 - val_loss: 1943.0166 - val_mse: 26766878.0000 - val_mae: 1943.0167\n",
            "Epoch 102/500\n",
            "380/380 [==============================] - 0s 92us/step - loss: 2402.9171 - mse: 34761200.0000 - mae: 2402.9170 - val_loss: 1942.7241 - val_mse: 26758762.0000 - val_mae: 1942.7241\n",
            "Epoch 103/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2382.6819 - mse: 34608372.0000 - mae: 2382.6819 - val_loss: 1942.5332 - val_mse: 26747062.0000 - val_mae: 1942.5333\n",
            "Epoch 104/500\n",
            "380/380 [==============================] - 0s 92us/step - loss: 2389.8829 - mse: 34548888.0000 - mae: 2389.8828 - val_loss: 1942.3366 - val_mse: 26734770.0000 - val_mae: 1942.3367\n",
            "Epoch 105/500\n",
            "380/380 [==============================] - 0s 89us/step - loss: 2380.6001 - mse: 34584796.0000 - mae: 2380.6001 - val_loss: 1942.1542 - val_mse: 26719382.0000 - val_mae: 1942.1542\n",
            "Epoch 106/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2376.2717 - mse: 34371732.0000 - mae: 2376.2717 - val_loss: 1941.9176 - val_mse: 26706568.0000 - val_mae: 1941.9177\n",
            "Epoch 107/500\n",
            "380/380 [==============================] - 0s 83us/step - loss: 2397.3797 - mse: 34523788.0000 - mae: 2397.3799 - val_loss: 1941.7086 - val_mse: 26693900.0000 - val_mae: 1941.7086\n",
            "Epoch 108/500\n",
            "380/380 [==============================] - 0s 86us/step - loss: 2381.0802 - mse: 34396544.0000 - mae: 2381.0803 - val_loss: 1941.4681 - val_mse: 26685490.0000 - val_mae: 1941.4680\n",
            "Epoch 109/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2382.4461 - mse: 34435460.0000 - mae: 2382.4460 - val_loss: 1941.3141 - val_mse: 26668472.0000 - val_mae: 1941.3142\n",
            "Epoch 110/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2388.1343 - mse: 34499968.0000 - mae: 2388.1343 - val_loss: 1941.0050 - val_mse: 26659258.0000 - val_mae: 1941.0051\n",
            "Epoch 111/500\n",
            "380/380 [==============================] - 0s 87us/step - loss: 2376.7762 - mse: 34458148.0000 - mae: 2376.7764 - val_loss: 1940.7934 - val_mse: 26644430.0000 - val_mae: 1940.7935\n",
            "Epoch 112/500\n",
            "380/380 [==============================] - 0s 75us/step - loss: 2373.4775 - mse: 34361620.0000 - mae: 2373.4775 - val_loss: 1940.5810 - val_mse: 26627990.0000 - val_mae: 1940.5809\n",
            "Epoch 113/500\n",
            "380/380 [==============================] - 0s 91us/step - loss: 2387.4214 - mse: 34480348.0000 - mae: 2387.4214 - val_loss: 1940.3938 - val_mse: 26612626.0000 - val_mae: 1940.3939\n",
            "Epoch 114/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2392.4650 - mse: 34460024.0000 - mae: 2392.4651 - val_loss: 1939.8544 - val_mse: 26606728.0000 - val_mae: 1939.8544\n",
            "Epoch 115/500\n",
            "380/380 [==============================] - 0s 116us/step - loss: 2388.0777 - mse: 34520644.0000 - mae: 2388.0776 - val_loss: 1939.4642 - val_mse: 26600578.0000 - val_mae: 1939.4642\n",
            "Epoch 116/500\n",
            "380/380 [==============================] - 0s 101us/step - loss: 2373.6596 - mse: 34297100.0000 - mae: 2373.6594 - val_loss: 1938.9658 - val_mse: 26597556.0000 - val_mae: 1938.9658\n",
            "Epoch 117/500\n",
            "380/380 [==============================] - 0s 86us/step - loss: 2388.2498 - mse: 34426992.0000 - mae: 2388.2498 - val_loss: 1938.4962 - val_mse: 26598826.0000 - val_mae: 1938.4961\n",
            "Epoch 118/500\n",
            "380/380 [==============================] - 0s 78us/step - loss: 2374.2237 - mse: 34401188.0000 - mae: 2374.2236 - val_loss: 1938.1023 - val_mse: 26595900.0000 - val_mae: 1938.1022\n",
            "Epoch 119/500\n",
            "380/380 [==============================] - 0s 82us/step - loss: 2384.3206 - mse: 34303344.0000 - mae: 2384.3206 - val_loss: 1937.8055 - val_mse: 26586314.0000 - val_mae: 1937.8055\n",
            "Epoch 120/500\n",
            "380/380 [==============================] - 0s 89us/step - loss: 2376.2524 - mse: 34287636.0000 - mae: 2376.2524 - val_loss: 1937.5553 - val_mse: 26578346.0000 - val_mae: 1937.5552\n",
            "Epoch 121/500\n",
            "380/380 [==============================] - 0s 101us/step - loss: 2371.6392 - mse: 34301236.0000 - mae: 2371.6394 - val_loss: 1937.4785 - val_mse: 26560116.0000 - val_mae: 1937.4784\n",
            "Epoch 122/500\n",
            "380/380 [==============================] - 0s 101us/step - loss: 2371.6602 - mse: 34285768.0000 - mae: 2371.6602 - val_loss: 1937.3676 - val_mse: 26548288.0000 - val_mae: 1937.3676\n",
            "Epoch 123/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2383.7481 - mse: 34253152.0000 - mae: 2383.7480 - val_loss: 1936.9129 - val_mse: 26547770.0000 - val_mae: 1936.9130\n",
            "Epoch 124/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2372.9128 - mse: 34251212.0000 - mae: 2372.9128 - val_loss: 1936.6414 - val_mse: 26539320.0000 - val_mae: 1936.6416\n",
            "Epoch 125/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2367.3785 - mse: 34193780.0000 - mae: 2367.3787 - val_loss: 1936.6100 - val_mse: 26523772.0000 - val_mae: 1936.6100\n",
            "Epoch 126/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2369.0292 - mse: 34217864.0000 - mae: 2369.0293 - val_loss: 1936.5777 - val_mse: 26512868.0000 - val_mae: 1936.5778\n",
            "Epoch 127/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2379.6443 - mse: 34225432.0000 - mae: 2379.6443 - val_loss: 1936.5390 - val_mse: 26501584.0000 - val_mae: 1936.5391\n",
            "Epoch 128/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2368.6970 - mse: 34267508.0000 - mae: 2368.6970 - val_loss: 1936.5568 - val_mse: 26488740.0000 - val_mae: 1936.5569\n",
            "Epoch 129/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2379.5157 - mse: 34264480.0000 - mae: 2379.5159 - val_loss: 1936.3665 - val_mse: 26486184.0000 - val_mae: 1936.3666\n",
            "Epoch 130/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2362.0463 - mse: 34204060.0000 - mae: 2362.0464 - val_loss: 1936.3755 - val_mse: 26476372.0000 - val_mae: 1936.3755\n",
            "Epoch 131/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2367.4886 - mse: 34028044.0000 - mae: 2367.4888 - val_loss: 1936.4997 - val_mse: 26459714.0000 - val_mae: 1936.4996\n",
            "Epoch 132/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2355.9629 - mse: 34055400.0000 - mae: 2355.9629 - val_loss: 1936.4247 - val_mse: 26453740.0000 - val_mae: 1936.4247\n",
            "Epoch 133/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2364.5453 - mse: 34064304.0000 - mae: 2364.5454 - val_loss: 1936.0985 - val_mse: 26452950.0000 - val_mae: 1936.0985\n",
            "Epoch 134/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2376.1954 - mse: 34294448.0000 - mae: 2376.1953 - val_loss: 1936.0113 - val_mse: 26444376.0000 - val_mae: 1936.0114\n",
            "Epoch 135/500\n",
            "380/380 [==============================] - 0s 97us/step - loss: 2353.2959 - mse: 33867600.0000 - mae: 2353.2957 - val_loss: 1936.0752 - val_mse: 26429374.0000 - val_mae: 1936.0752\n",
            "Epoch 136/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2372.0758 - mse: 34190380.0000 - mae: 2372.0757 - val_loss: 1935.7224 - val_mse: 26427618.0000 - val_mae: 1935.7224\n",
            "Epoch 137/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2372.0155 - mse: 34143928.0000 - mae: 2372.0156 - val_loss: 1935.7604 - val_mse: 26415462.0000 - val_mae: 1935.7605\n",
            "Epoch 138/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2363.5845 - mse: 33979444.0000 - mae: 2363.5845 - val_loss: 1935.4186 - val_mse: 26415142.0000 - val_mae: 1935.4186\n",
            "Epoch 139/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2349.9684 - mse: 34047812.0000 - mae: 2349.9685 - val_loss: 1935.2953 - val_mse: 26408298.0000 - val_mae: 1935.2953\n",
            "Epoch 140/500\n",
            "380/380 [==============================] - 0s 122us/step - loss: 2356.9356 - mse: 33950236.0000 - mae: 2356.9355 - val_loss: 1935.2214 - val_mse: 26398120.0000 - val_mae: 1935.2214\n",
            "Epoch 141/500\n",
            "380/380 [==============================] - 0s 94us/step - loss: 2369.4837 - mse: 33930936.0000 - mae: 2369.4839 - val_loss: 1935.2173 - val_mse: 26386014.0000 - val_mae: 1935.2174\n",
            "Epoch 142/500\n",
            "380/380 [==============================] - 0s 94us/step - loss: 2370.5099 - mse: 33969612.0000 - mae: 2370.5098 - val_loss: 1935.0951 - val_mse: 26379720.0000 - val_mae: 1935.0951\n",
            "Epoch 143/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2362.1863 - mse: 34084412.0000 - mae: 2362.1863 - val_loss: 1934.7367 - val_mse: 26381124.0000 - val_mae: 1934.7367\n",
            "Epoch 144/500\n",
            "380/380 [==============================] - 0s 102us/step - loss: 2359.0746 - mse: 33995992.0000 - mae: 2359.0747 - val_loss: 1934.6866 - val_mse: 26372066.0000 - val_mae: 1934.6866\n",
            "Epoch 145/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2371.9700 - mse: 34131856.0000 - mae: 2371.9700 - val_loss: 1934.7985 - val_mse: 26359114.0000 - val_mae: 1934.7987\n",
            "Epoch 146/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2362.3873 - mse: 33947320.0000 - mae: 2362.3875 - val_loss: 1934.6433 - val_mse: 26358568.0000 - val_mae: 1934.6432\n",
            "Epoch 147/500\n",
            "380/380 [==============================] - 0s 78us/step - loss: 2348.8260 - mse: 33937896.0000 - mae: 2348.8259 - val_loss: 1934.4574 - val_mse: 26356976.0000 - val_mae: 1934.4574\n",
            "Epoch 148/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2354.7649 - mse: 33807488.0000 - mae: 2354.7649 - val_loss: 1934.4803 - val_mse: 26346568.0000 - val_mae: 1934.4803\n",
            "Epoch 149/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2351.5929 - mse: 33905172.0000 - mae: 2351.5928 - val_loss: 1934.6268 - val_mse: 26330456.0000 - val_mae: 1934.6268\n",
            "Epoch 150/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2356.8786 - mse: 33898728.0000 - mae: 2356.8787 - val_loss: 1934.5396 - val_mse: 26325106.0000 - val_mae: 1934.5396\n",
            "Epoch 151/500\n",
            "380/380 [==============================] - 0s 94us/step - loss: 2360.5274 - mse: 33895328.0000 - mae: 2360.5273 - val_loss: 1934.6419 - val_mse: 26312864.0000 - val_mae: 1934.6418\n",
            "Epoch 152/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2377.3874 - mse: 33927320.0000 - mae: 2377.3875 - val_loss: 1934.2355 - val_mse: 26320018.0000 - val_mae: 1934.2356\n",
            "Epoch 153/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2356.0893 - mse: 33775264.0000 - mae: 2356.0894 - val_loss: 1934.4093 - val_mse: 26303466.0000 - val_mae: 1934.4094\n",
            "Epoch 154/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2358.7962 - mse: 33843388.0000 - mae: 2358.7961 - val_loss: 1934.2026 - val_mse: 26303328.0000 - val_mae: 1934.2028\n",
            "Epoch 155/500\n",
            "380/380 [==============================] - 0s 80us/step - loss: 2370.1334 - mse: 34008708.0000 - mae: 2370.1335 - val_loss: 1933.9828 - val_mse: 26304788.0000 - val_mae: 1933.9828\n",
            "Epoch 156/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2356.7202 - mse: 33734884.0000 - mae: 2356.7205 - val_loss: 1933.8032 - val_mse: 26304388.0000 - val_mae: 1933.8032\n",
            "Epoch 157/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2367.1807 - mse: 33753984.0000 - mae: 2367.1807 - val_loss: 1933.7715 - val_mse: 26296614.0000 - val_mae: 1933.7715\n",
            "Epoch 158/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2365.2669 - mse: 33995804.0000 - mae: 2365.2668 - val_loss: 1933.6585 - val_mse: 26294618.0000 - val_mae: 1933.6584\n",
            "Epoch 159/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2359.8454 - mse: 34018492.0000 - mae: 2359.8455 - val_loss: 1933.6364 - val_mse: 26288078.0000 - val_mae: 1933.6365\n",
            "Epoch 160/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2357.1076 - mse: 33688036.0000 - mae: 2357.1077 - val_loss: 1933.5604 - val_mse: 26285078.0000 - val_mae: 1933.5604\n",
            "Epoch 161/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2357.6018 - mse: 33938468.0000 - mae: 2357.6021 - val_loss: 1933.3539 - val_mse: 26286976.0000 - val_mae: 1933.3539\n",
            "Epoch 162/500\n",
            "380/380 [==============================] - 0s 96us/step - loss: 2357.2047 - mse: 33915096.0000 - mae: 2357.2048 - val_loss: 1933.2187 - val_mse: 26286540.0000 - val_mae: 1933.2188\n",
            "Epoch 163/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2347.6041 - mse: 33634324.0000 - mae: 2347.6040 - val_loss: 1933.2807 - val_mse: 26277036.0000 - val_mae: 1933.2808\n",
            "Epoch 164/500\n",
            "380/380 [==============================] - 0s 136us/step - loss: 2351.7413 - mse: 33610052.0000 - mae: 2351.7412 - val_loss: 1933.2731 - val_mse: 26270460.0000 - val_mae: 1933.2729\n",
            "Epoch 165/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2366.0378 - mse: 33931740.0000 - mae: 2366.0378 - val_loss: 1933.2814 - val_mse: 26264908.0000 - val_mae: 1933.2814\n",
            "Epoch 166/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2355.7415 - mse: 33783704.0000 - mae: 2355.7415 - val_loss: 1933.1869 - val_mse: 26263696.0000 - val_mae: 1933.1868\n",
            "Epoch 167/500\n",
            "380/380 [==============================] - 0s 89us/step - loss: 2365.8052 - mse: 33794976.0000 - mae: 2365.8052 - val_loss: 1933.1712 - val_mse: 26259018.0000 - val_mae: 1933.1713\n",
            "Epoch 168/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2356.2910 - mse: 33636536.0000 - mae: 2356.2910 - val_loss: 1932.9975 - val_mse: 26259514.0000 - val_mae: 1932.9977\n",
            "Epoch 169/500\n",
            "380/380 [==============================] - 0s 93us/step - loss: 2378.9988 - mse: 33953144.0000 - mae: 2378.9988 - val_loss: 1932.7653 - val_mse: 26262782.0000 - val_mae: 1932.7654\n",
            "Epoch 170/500\n",
            "380/380 [==============================] - 0s 86us/step - loss: 2364.8375 - mse: 33858100.0000 - mae: 2364.8374 - val_loss: 1932.5794 - val_mse: 26263164.0000 - val_mae: 1932.5796\n",
            "Epoch 171/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2364.0317 - mse: 33663504.0000 - mae: 2364.0317 - val_loss: 1932.4596 - val_mse: 26261858.0000 - val_mae: 1932.4596\n",
            "Epoch 172/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2355.2068 - mse: 33617240.0000 - mae: 2355.2068 - val_loss: 1932.4042 - val_mse: 26257134.0000 - val_mae: 1932.4042\n",
            "Epoch 173/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2366.9319 - mse: 33704112.0000 - mae: 2366.9319 - val_loss: 1932.2643 - val_mse: 26258684.0000 - val_mae: 1932.2644\n",
            "Epoch 174/500\n",
            "380/380 [==============================] - 0s 78us/step - loss: 2336.2773 - mse: 33799948.0000 - mae: 2336.2773 - val_loss: 1932.2372 - val_mse: 26254364.0000 - val_mae: 1932.2372\n",
            "Epoch 175/500\n",
            "380/380 [==============================] - 0s 96us/step - loss: 2366.7501 - mse: 33810088.0000 - mae: 2366.7500 - val_loss: 1932.2806 - val_mse: 26249066.0000 - val_mae: 1932.2804\n",
            "Epoch 176/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2358.7822 - mse: 33699496.0000 - mae: 2358.7822 - val_loss: 1932.3222 - val_mse: 26243454.0000 - val_mae: 1932.3223\n",
            "Epoch 177/500\n",
            "380/380 [==============================] - 0s 84us/step - loss: 2356.0095 - mse: 33618100.0000 - mae: 2356.0095 - val_loss: 1932.3768 - val_mse: 26237132.0000 - val_mae: 1932.3768\n",
            "Epoch 178/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2369.8799 - mse: 33687160.0000 - mae: 2369.8799 - val_loss: 1932.2079 - val_mse: 26237514.0000 - val_mae: 1932.2079\n",
            "Epoch 179/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2363.1565 - mse: 33647892.0000 - mae: 2363.1565 - val_loss: 1932.0588 - val_mse: 26240164.0000 - val_mae: 1932.0588\n",
            "Epoch 180/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2359.8947 - mse: 33825928.0000 - mae: 2359.8948 - val_loss: 1931.7863 - val_mse: 26248728.0000 - val_mae: 1931.7864\n",
            "Epoch 181/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2368.6502 - mse: 33610564.0000 - mae: 2368.6504 - val_loss: 1931.6933 - val_mse: 26247790.0000 - val_mae: 1931.6934\n",
            "Epoch 182/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2358.4097 - mse: 33670548.0000 - mae: 2358.4097 - val_loss: 1931.6170 - val_mse: 26248824.0000 - val_mae: 1931.6171\n",
            "Epoch 183/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2365.6364 - mse: 33770952.0000 - mae: 2365.6365 - val_loss: 1931.5171 - val_mse: 26250080.0000 - val_mae: 1931.5171\n",
            "Epoch 184/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2356.3591 - mse: 33736676.0000 - mae: 2356.3591 - val_loss: 1931.4650 - val_mse: 26248086.0000 - val_mae: 1931.4651\n",
            "Epoch 185/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2349.0433 - mse: 33605100.0000 - mae: 2349.0435 - val_loss: 1931.5455 - val_mse: 26242774.0000 - val_mae: 1931.5454\n",
            "Epoch 186/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2365.9772 - mse: 33919936.0000 - mae: 2365.9773 - val_loss: 1931.4555 - val_mse: 26243352.0000 - val_mae: 1931.4554\n",
            "Epoch 187/500\n",
            "380/380 [==============================] - 0s 102us/step - loss: 2358.7053 - mse: 33775536.0000 - mae: 2358.7053 - val_loss: 1931.5104 - val_mse: 26237788.0000 - val_mae: 1931.5105\n",
            "Epoch 188/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2364.0782 - mse: 33859736.0000 - mae: 2364.0781 - val_loss: 1931.4193 - val_mse: 26239096.0000 - val_mae: 1931.4193\n",
            "Epoch 189/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2354.7974 - mse: 33447120.0000 - mae: 2354.7974 - val_loss: 1931.2314 - val_mse: 26240716.0000 - val_mae: 1931.2314\n",
            "Epoch 190/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2364.0286 - mse: 33684232.0000 - mae: 2364.0286 - val_loss: 1930.9056 - val_mse: 26248118.0000 - val_mae: 1930.9055\n",
            "Epoch 191/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2345.0732 - mse: 33826688.0000 - mae: 2345.0732 - val_loss: 1930.6600 - val_mse: 26251978.0000 - val_mae: 1930.6599\n",
            "Epoch 192/500\n",
            "380/380 [==============================] - 0s 91us/step - loss: 2361.1913 - mse: 33709708.0000 - mae: 2361.1912 - val_loss: 1930.4536 - val_mse: 26256134.0000 - val_mae: 1930.4537\n",
            "Epoch 193/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2358.1970 - mse: 33757512.0000 - mae: 2358.1968 - val_loss: 1930.2610 - val_mse: 26258102.0000 - val_mae: 1930.2610\n",
            "Epoch 194/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2346.7396 - mse: 33498586.0000 - mae: 2346.7397 - val_loss: 1930.3376 - val_mse: 26252280.0000 - val_mae: 1930.3376\n",
            "Epoch 195/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2351.1640 - mse: 33669876.0000 - mae: 2351.1641 - val_loss: 1930.1852 - val_mse: 26254078.0000 - val_mae: 1930.1852\n",
            "Epoch 196/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2364.7497 - mse: 33725560.0000 - mae: 2364.7498 - val_loss: 1930.0626 - val_mse: 26252540.0000 - val_mae: 1930.0626\n",
            "Epoch 197/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2360.1391 - mse: 33741580.0000 - mae: 2360.1392 - val_loss: 1929.8731 - val_mse: 26256046.0000 - val_mae: 1929.8729\n",
            "Epoch 198/500\n",
            "380/380 [==============================] - 0s 136us/step - loss: 2357.2972 - mse: 33608904.0000 - mae: 2357.2974 - val_loss: 1930.0886 - val_mse: 26243402.0000 - val_mae: 1930.0885\n",
            "Epoch 199/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2357.5285 - mse: 33847468.0000 - mae: 2357.5286 - val_loss: 1930.1768 - val_mse: 26235026.0000 - val_mae: 1930.1769\n",
            "Epoch 200/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2346.6025 - mse: 33651156.0000 - mae: 2346.6025 - val_loss: 1930.2258 - val_mse: 26228454.0000 - val_mae: 1930.2258\n",
            "Epoch 201/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2339.3584 - mse: 33415484.0000 - mae: 2339.3582 - val_loss: 1930.3441 - val_mse: 26218762.0000 - val_mae: 1930.3441\n",
            "Epoch 202/500\n",
            "380/380 [==============================] - 0s 102us/step - loss: 2340.4027 - mse: 33409846.0000 - mae: 2340.4026 - val_loss: 1930.2312 - val_mse: 26214654.0000 - val_mae: 1930.2312\n",
            "Epoch 203/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2347.6598 - mse: 33401164.0000 - mae: 2347.6599 - val_loss: 1930.3737 - val_mse: 26205566.0000 - val_mae: 1930.3737\n",
            "Epoch 204/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2336.3950 - mse: 33402482.0000 - mae: 2336.3950 - val_loss: 1930.2759 - val_mse: 26200574.0000 - val_mae: 1930.2760\n",
            "Epoch 205/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2354.6813 - mse: 33498856.0000 - mae: 2354.6814 - val_loss: 1929.9577 - val_mse: 26208140.0000 - val_mae: 1929.9578\n",
            "Epoch 206/500\n",
            "380/380 [==============================] - 0s 102us/step - loss: 2347.7488 - mse: 33426548.0000 - mae: 2347.7488 - val_loss: 1929.7329 - val_mse: 26208254.0000 - val_mae: 1929.7328\n",
            "Epoch 207/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2359.9872 - mse: 33523420.0000 - mae: 2359.9871 - val_loss: 1929.5134 - val_mse: 26212112.0000 - val_mae: 1929.5135\n",
            "Epoch 208/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2355.2833 - mse: 33778592.0000 - mae: 2355.2832 - val_loss: 1929.5641 - val_mse: 26205630.0000 - val_mae: 1929.5642\n",
            "Epoch 209/500\n",
            "380/380 [==============================] - 0s 102us/step - loss: 2363.7736 - mse: 33718264.0000 - mae: 2363.7737 - val_loss: 1929.2764 - val_mse: 26208818.0000 - val_mae: 1929.2765\n",
            "Epoch 210/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2358.8650 - mse: 33702620.0000 - mae: 2358.8652 - val_loss: 1929.0241 - val_mse: 26215750.0000 - val_mae: 1929.0242\n",
            "Epoch 211/500\n",
            "380/380 [==============================] - 0s 122us/step - loss: 2347.5157 - mse: 33660592.0000 - mae: 2347.5159 - val_loss: 1928.8205 - val_mse: 26216562.0000 - val_mae: 1928.8204\n",
            "Epoch 212/500\n",
            "380/380 [==============================] - 0s 135us/step - loss: 2347.2611 - mse: 33619220.0000 - mae: 2347.2612 - val_loss: 1928.7574 - val_mse: 26211074.0000 - val_mae: 1928.7574\n",
            "Epoch 213/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2368.2280 - mse: 33619160.0000 - mae: 2368.2280 - val_loss: 1928.6825 - val_mse: 26206744.0000 - val_mae: 1928.6824\n",
            "Epoch 214/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2352.8921 - mse: 33732328.0000 - mae: 2352.8921 - val_loss: 1928.6556 - val_mse: 26201858.0000 - val_mae: 1928.6555\n",
            "Epoch 215/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2354.2559 - mse: 33722760.0000 - mae: 2354.2559 - val_loss: 1928.5533 - val_mse: 26199706.0000 - val_mae: 1928.5533\n",
            "Epoch 216/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2353.4707 - mse: 33654312.0000 - mae: 2353.4707 - val_loss: 1928.3828 - val_mse: 26198116.0000 - val_mae: 1928.3828\n",
            "Epoch 217/500\n",
            "380/380 [==============================] - 0s 101us/step - loss: 2379.8494 - mse: 33762832.0000 - mae: 2379.8494 - val_loss: 1927.9936 - val_mse: 26210792.0000 - val_mae: 1927.9935\n",
            "Epoch 218/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2346.1167 - mse: 33582556.0000 - mae: 2346.1167 - val_loss: 1927.7652 - val_mse: 26208372.0000 - val_mae: 1927.7653\n",
            "Epoch 219/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2345.7971 - mse: 33765628.0000 - mae: 2345.7971 - val_loss: 1927.6167 - val_mse: 26205506.0000 - val_mae: 1927.6167\n",
            "Epoch 220/500\n",
            "380/380 [==============================] - 0s 85us/step - loss: 2342.6771 - mse: 33496284.0000 - mae: 2342.6770 - val_loss: 1927.5772 - val_mse: 26201392.0000 - val_mae: 1927.5774\n",
            "Epoch 221/500\n",
            "380/380 [==============================] - 0s 97us/step - loss: 2342.6810 - mse: 33457868.0000 - mae: 2342.6812 - val_loss: 1927.4731 - val_mse: 26205744.0000 - val_mae: 1927.4731\n",
            "Epoch 222/500\n",
            "380/380 [==============================] - 0s 93us/step - loss: 2335.5164 - mse: 33571640.0000 - mae: 2335.5164 - val_loss: 1927.3816 - val_mse: 26196998.0000 - val_mae: 1927.3816\n",
            "Epoch 223/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2360.0314 - mse: 33600092.0000 - mae: 2360.0315 - val_loss: 1927.3523 - val_mse: 26189226.0000 - val_mae: 1927.3524\n",
            "Epoch 224/500\n",
            "380/380 [==============================] - 0s 97us/step - loss: 2345.3190 - mse: 33519536.0000 - mae: 2345.3191 - val_loss: 1927.1792 - val_mse: 26186400.0000 - val_mae: 1927.1791\n",
            "Epoch 225/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2334.9175 - mse: 33508702.0000 - mae: 2334.9175 - val_loss: 1927.1286 - val_mse: 26183948.0000 - val_mae: 1927.1285\n",
            "Epoch 226/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2355.0978 - mse: 33679544.0000 - mae: 2355.0979 - val_loss: 1927.2318 - val_mse: 26185730.0000 - val_mae: 1927.2318\n",
            "Epoch 227/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2358.0152 - mse: 33726080.0000 - mae: 2358.0151 - val_loss: 1927.1775 - val_mse: 26188202.0000 - val_mae: 1927.1775\n",
            "Epoch 228/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2371.4935 - mse: 33641144.0000 - mae: 2371.4934 - val_loss: 1926.9024 - val_mse: 26187966.0000 - val_mae: 1926.9023\n",
            "Epoch 229/500\n",
            "380/380 [==============================] - 0s 120us/step - loss: 2348.3129 - mse: 33568012.0000 - mae: 2348.3127 - val_loss: 1926.8111 - val_mse: 26188910.0000 - val_mae: 1926.8112\n",
            "Epoch 230/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2347.2941 - mse: 33595356.0000 - mae: 2347.2942 - val_loss: 1926.7599 - val_mse: 26188546.0000 - val_mae: 1926.7598\n",
            "Epoch 231/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2343.6637 - mse: 33666192.0000 - mae: 2343.6636 - val_loss: 1926.5940 - val_mse: 26186170.0000 - val_mae: 1926.5939\n",
            "Epoch 232/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2350.9079 - mse: 33590716.0000 - mae: 2350.9077 - val_loss: 1926.4340 - val_mse: 26187350.0000 - val_mae: 1926.4341\n",
            "Epoch 233/500\n",
            "380/380 [==============================] - 0s 139us/step - loss: 2337.5863 - mse: 33483934.0000 - mae: 2337.5862 - val_loss: 1926.3584 - val_mse: 26185562.0000 - val_mae: 1926.3584\n",
            "Epoch 234/500\n",
            "380/380 [==============================] - 0s 141us/step - loss: 2348.9465 - mse: 33689548.0000 - mae: 2348.9465 - val_loss: 1926.3840 - val_mse: 26180614.0000 - val_mae: 1926.3839\n",
            "Epoch 235/500\n",
            "380/380 [==============================] - 0s 129us/step - loss: 2359.6711 - mse: 33594880.0000 - mae: 2359.6711 - val_loss: 1926.5321 - val_mse: 26177046.0000 - val_mae: 1926.5321\n",
            "Epoch 236/500\n",
            "380/380 [==============================] - 0s 149us/step - loss: 2354.4402 - mse: 33489378.0000 - mae: 2354.4402 - val_loss: 1926.5475 - val_mse: 26174960.0000 - val_mae: 1926.5475\n",
            "Epoch 237/500\n",
            "380/380 [==============================] - 0s 123us/step - loss: 2345.0970 - mse: 33717840.0000 - mae: 2345.0969 - val_loss: 1926.4937 - val_mse: 26176914.0000 - val_mae: 1926.4938\n",
            "Epoch 238/500\n",
            "380/380 [==============================] - 0s 120us/step - loss: 2353.8750 - mse: 33586732.0000 - mae: 2353.8750 - val_loss: 1926.2968 - val_mse: 26180666.0000 - val_mae: 1926.2970\n",
            "Epoch 239/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2362.7629 - mse: 33673976.0000 - mae: 2362.7629 - val_loss: 1926.1166 - val_mse: 26185114.0000 - val_mae: 1926.1166\n",
            "Epoch 240/500\n",
            "380/380 [==============================] - 0s 116us/step - loss: 2364.8484 - mse: 33814156.0000 - mae: 2364.8484 - val_loss: 1925.7912 - val_mse: 26191614.0000 - val_mae: 1925.7913\n",
            "Epoch 241/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2345.1580 - mse: 33576056.0000 - mae: 2345.1582 - val_loss: 1925.9212 - val_mse: 26181076.0000 - val_mae: 1925.9213\n",
            "Epoch 242/500\n",
            "380/380 [==============================] - 0s 91us/step - loss: 2345.9119 - mse: 33512976.0000 - mae: 2345.9119 - val_loss: 1925.8007 - val_mse: 26179360.0000 - val_mae: 1925.8007\n",
            "Epoch 243/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2358.3112 - mse: 33616148.0000 - mae: 2358.3113 - val_loss: 1925.6915 - val_mse: 26173534.0000 - val_mae: 1925.6915\n",
            "Epoch 244/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2351.1615 - mse: 33512422.0000 - mae: 2351.1616 - val_loss: 1925.7409 - val_mse: 26165450.0000 - val_mae: 1925.7408\n",
            "Epoch 245/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2358.9062 - mse: 33595024.0000 - mae: 2358.9062 - val_loss: 1925.3650 - val_mse: 26172176.0000 - val_mae: 1925.3650\n",
            "Epoch 246/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2361.0424 - mse: 33470190.0000 - mae: 2361.0425 - val_loss: 1925.2712 - val_mse: 26169264.0000 - val_mae: 1925.2712\n",
            "Epoch 247/500\n",
            "380/380 [==============================] - 0s 118us/step - loss: 2347.5929 - mse: 33357170.0000 - mae: 2347.5930 - val_loss: 1925.0007 - val_mse: 26177292.0000 - val_mae: 1925.0007\n",
            "Epoch 248/500\n",
            "380/380 [==============================] - 0s 102us/step - loss: 2339.5012 - mse: 33644712.0000 - mae: 2339.5012 - val_loss: 1925.0125 - val_mse: 26170592.0000 - val_mae: 1925.0123\n",
            "Epoch 249/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2342.4142 - mse: 33529128.0000 - mae: 2342.4141 - val_loss: 1924.6967 - val_mse: 26169238.0000 - val_mae: 1924.6967\n",
            "Epoch 250/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2360.8778 - mse: 33572308.0000 - mae: 2360.8779 - val_loss: 1924.3998 - val_mse: 26175148.0000 - val_mae: 1924.3998\n",
            "Epoch 251/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2353.4287 - mse: 33577592.0000 - mae: 2353.4287 - val_loss: 1924.0073 - val_mse: 26177664.0000 - val_mae: 1924.0074\n",
            "Epoch 252/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2344.9974 - mse: 33548310.0000 - mae: 2344.9973 - val_loss: 1923.9032 - val_mse: 26179988.0000 - val_mae: 1923.9032\n",
            "Epoch 253/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2349.3604 - mse: 33423246.0000 - mae: 2349.3604 - val_loss: 1923.5627 - val_mse: 26185432.0000 - val_mae: 1923.5626\n",
            "Epoch 254/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2340.8673 - mse: 33546714.0000 - mae: 2340.8672 - val_loss: 1923.4622 - val_mse: 26187270.0000 - val_mae: 1923.4622\n",
            "Epoch 255/500\n",
            "380/380 [==============================] - 0s 123us/step - loss: 2335.6425 - mse: 33409316.0000 - mae: 2335.6426 - val_loss: 1923.6000 - val_mse: 26177804.0000 - val_mae: 1923.6000\n",
            "Epoch 256/500\n",
            "380/380 [==============================] - 0s 149us/step - loss: 2348.4758 - mse: 33539206.0000 - mae: 2348.4758 - val_loss: 1923.6825 - val_mse: 26173900.0000 - val_mae: 1923.6824\n",
            "Epoch 257/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2353.3820 - mse: 33424728.0000 - mae: 2353.3818 - val_loss: 1923.4763 - val_mse: 26172280.0000 - val_mae: 1923.4763\n",
            "Epoch 258/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2338.3881 - mse: 33604316.0000 - mae: 2338.3882 - val_loss: 1923.6614 - val_mse: 26169504.0000 - val_mae: 1923.6614\n",
            "Epoch 259/500\n",
            "380/380 [==============================] - 0s 96us/step - loss: 2338.9962 - mse: 33786976.0000 - mae: 2338.9963 - val_loss: 1923.6921 - val_mse: 26170882.0000 - val_mae: 1923.6923\n",
            "Epoch 260/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2355.5471 - mse: 33520678.0000 - mae: 2355.5471 - val_loss: 1923.6902 - val_mse: 26166972.0000 - val_mae: 1923.6902\n",
            "Epoch 261/500\n",
            "380/380 [==============================] - 0s 96us/step - loss: 2357.5345 - mse: 33678520.0000 - mae: 2357.5344 - val_loss: 1923.7337 - val_mse: 26163296.0000 - val_mae: 1923.7336\n",
            "Epoch 262/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2339.0725 - mse: 33289572.0000 - mae: 2339.0723 - val_loss: 1923.5883 - val_mse: 26160870.0000 - val_mae: 1923.5884\n",
            "Epoch 263/500\n",
            "380/380 [==============================] - 0s 132us/step - loss: 2363.4367 - mse: 33552406.0000 - mae: 2363.4368 - val_loss: 1923.4941 - val_mse: 26161082.0000 - val_mae: 1923.4941\n",
            "Epoch 264/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2342.9493 - mse: 33537458.0000 - mae: 2342.9495 - val_loss: 1923.6923 - val_mse: 26154404.0000 - val_mae: 1923.6924\n",
            "Epoch 265/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2344.3473 - mse: 33407442.0000 - mae: 2344.3472 - val_loss: 1923.8470 - val_mse: 26148070.0000 - val_mae: 1923.8469\n",
            "Epoch 266/500\n",
            "380/380 [==============================] - 0s 97us/step - loss: 2344.0423 - mse: 33432126.0000 - mae: 2344.0425 - val_loss: 1923.7439 - val_mse: 26144754.0000 - val_mae: 1923.7439\n",
            "Epoch 267/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2364.6704 - mse: 33599524.0000 - mae: 2364.6704 - val_loss: 1923.7061 - val_mse: 26142044.0000 - val_mae: 1923.7061\n",
            "Epoch 268/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2347.5062 - mse: 33362282.0000 - mae: 2347.5063 - val_loss: 1923.6962 - val_mse: 26139360.0000 - val_mae: 1923.6963\n",
            "Epoch 269/500\n",
            "380/380 [==============================] - 0s 92us/step - loss: 2355.9124 - mse: 33607840.0000 - mae: 2355.9126 - val_loss: 1923.7773 - val_mse: 26134544.0000 - val_mae: 1923.7773\n",
            "Epoch 270/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2334.5058 - mse: 33385814.0000 - mae: 2334.5056 - val_loss: 1923.8261 - val_mse: 26134656.0000 - val_mae: 1923.8260\n",
            "Epoch 271/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2359.5651 - mse: 33606908.0000 - mae: 2359.5652 - val_loss: 1923.9264 - val_mse: 26135148.0000 - val_mae: 1923.9264\n",
            "Epoch 272/500\n",
            "380/380 [==============================] - 0s 99us/step - loss: 2340.7210 - mse: 33486434.0000 - mae: 2340.7209 - val_loss: 1923.8665 - val_mse: 26130016.0000 - val_mae: 1923.8666\n",
            "Epoch 273/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2361.1155 - mse: 33647104.0000 - mae: 2361.1155 - val_loss: 1923.5230 - val_mse: 26141956.0000 - val_mae: 1923.5232\n",
            "Epoch 274/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2345.4517 - mse: 33266108.0000 - mae: 2345.4517 - val_loss: 1923.8721 - val_mse: 26139186.0000 - val_mae: 1923.8721\n",
            "Epoch 275/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2335.2534 - mse: 33347364.0000 - mae: 2335.2534 - val_loss: 1924.0132 - val_mse: 26130208.0000 - val_mae: 1924.0132\n",
            "Epoch 276/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2350.0401 - mse: 33463142.0000 - mae: 2350.0400 - val_loss: 1923.9746 - val_mse: 26131910.0000 - val_mae: 1923.9746\n",
            "Epoch 277/500\n",
            "380/380 [==============================] - 0s 116us/step - loss: 2343.6939 - mse: 33429584.0000 - mae: 2343.6938 - val_loss: 1924.0331 - val_mse: 26128538.0000 - val_mae: 1924.0331\n",
            "Epoch 278/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2359.1917 - mse: 33666620.0000 - mae: 2359.1917 - val_loss: 1923.9691 - val_mse: 26128484.0000 - val_mae: 1923.9691\n",
            "Epoch 279/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2348.4028 - mse: 33347532.0000 - mae: 2348.4028 - val_loss: 1924.0138 - val_mse: 26128802.0000 - val_mae: 1924.0138\n",
            "Epoch 280/500\n",
            "380/380 [==============================] - 0s 137us/step - loss: 2343.4087 - mse: 33409654.0000 - mae: 2343.4087 - val_loss: 1924.1300 - val_mse: 26125160.0000 - val_mae: 1924.1301\n",
            "Epoch 281/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2357.5921 - mse: 33315198.0000 - mae: 2357.5920 - val_loss: 1924.0377 - val_mse: 26127490.0000 - val_mae: 1924.0377\n",
            "Epoch 282/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2352.2823 - mse: 33289356.0000 - mae: 2352.2822 - val_loss: 1924.0312 - val_mse: 26126170.0000 - val_mae: 1924.0311\n",
            "Epoch 283/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2345.3993 - mse: 33256680.0000 - mae: 2345.3994 - val_loss: 1923.8177 - val_mse: 26128210.0000 - val_mae: 1923.8177\n",
            "Epoch 284/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2335.2023 - mse: 33323106.0000 - mae: 2335.2024 - val_loss: 1923.6883 - val_mse: 26127538.0000 - val_mae: 1923.6884\n",
            "Epoch 285/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2345.0116 - mse: 33530374.0000 - mae: 2345.0115 - val_loss: 1923.5797 - val_mse: 26127628.0000 - val_mae: 1923.5797\n",
            "Epoch 286/500\n",
            "380/380 [==============================] - 0s 140us/step - loss: 2359.6420 - mse: 33601320.0000 - mae: 2359.6418 - val_loss: 1923.4301 - val_mse: 26138878.0000 - val_mae: 1923.4301\n",
            "Epoch 287/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2346.6527 - mse: 33421452.0000 - mae: 2346.6526 - val_loss: 1923.4308 - val_mse: 26137040.0000 - val_mae: 1923.4309\n",
            "Epoch 288/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2349.3144 - mse: 33455430.0000 - mae: 2349.3145 - val_loss: 1923.3098 - val_mse: 26143736.0000 - val_mae: 1923.3098\n",
            "Epoch 289/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2335.4962 - mse: 33380284.0000 - mae: 2335.4963 - val_loss: 1923.3040 - val_mse: 26149144.0000 - val_mae: 1923.3041\n",
            "Epoch 290/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2329.0608 - mse: 33454826.0000 - mae: 2329.0608 - val_loss: 1923.3948 - val_mse: 26144120.0000 - val_mae: 1923.3948\n",
            "Epoch 291/500\n",
            "380/380 [==============================] - 0s 125us/step - loss: 2346.4291 - mse: 33489236.0000 - mae: 2346.4292 - val_loss: 1923.4507 - val_mse: 26145230.0000 - val_mae: 1923.4507\n",
            "Epoch 292/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2362.0707 - mse: 33780136.0000 - mae: 2362.0708 - val_loss: 1923.3055 - val_mse: 26141848.0000 - val_mae: 1923.3054\n",
            "Epoch 293/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2340.4798 - mse: 33531464.0000 - mae: 2340.4800 - val_loss: 1923.6825 - val_mse: 26131014.0000 - val_mae: 1923.6824\n",
            "Epoch 294/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2334.1738 - mse: 33385896.0000 - mae: 2334.1736 - val_loss: 1923.8076 - val_mse: 26125412.0000 - val_mae: 1923.8076\n",
            "Epoch 295/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2336.8398 - mse: 33295214.0000 - mae: 2336.8398 - val_loss: 1923.8980 - val_mse: 26123810.0000 - val_mae: 1923.8979\n",
            "Epoch 296/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2352.3050 - mse: 33601696.0000 - mae: 2352.3049 - val_loss: 1923.8416 - val_mse: 26126400.0000 - val_mae: 1923.8417\n",
            "Epoch 297/500\n",
            "380/380 [==============================] - 0s 126us/step - loss: 2359.2641 - mse: 33557204.0000 - mae: 2359.2639 - val_loss: 1924.0918 - val_mse: 26112434.0000 - val_mae: 1924.0919\n",
            "Epoch 298/500\n",
            "380/380 [==============================] - 0s 123us/step - loss: 2344.3166 - mse: 33373998.0000 - mae: 2344.3169 - val_loss: 1924.4265 - val_mse: 26106714.0000 - val_mae: 1924.4264\n",
            "Epoch 299/500\n",
            "380/380 [==============================] - 0s 127us/step - loss: 2334.8751 - mse: 33373632.0000 - mae: 2334.8750 - val_loss: 1924.2515 - val_mse: 26110182.0000 - val_mae: 1924.2515\n",
            "Epoch 300/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2326.8773 - mse: 33492420.0000 - mae: 2326.8772 - val_loss: 1924.0597 - val_mse: 26116082.0000 - val_mae: 1924.0598\n",
            "Epoch 301/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2347.9607 - mse: 33454416.0000 - mae: 2347.9607 - val_loss: 1923.9482 - val_mse: 26113888.0000 - val_mae: 1923.9481\n",
            "Epoch 302/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2341.1248 - mse: 33356398.0000 - mae: 2341.1248 - val_loss: 1924.1091 - val_mse: 26108774.0000 - val_mae: 1924.1091\n",
            "Epoch 303/500\n",
            "380/380 [==============================] - 0s 125us/step - loss: 2345.2024 - mse: 33258696.0000 - mae: 2345.2024 - val_loss: 1923.8707 - val_mse: 26110708.0000 - val_mae: 1923.8706\n",
            "Epoch 304/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2340.6344 - mse: 33443986.0000 - mae: 2340.6345 - val_loss: 1923.8941 - val_mse: 26107984.0000 - val_mae: 1923.8940\n",
            "Epoch 305/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2341.9321 - mse: 33272912.0000 - mae: 2341.9321 - val_loss: 1923.9668 - val_mse: 26113494.0000 - val_mae: 1923.9669\n",
            "Epoch 306/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2332.8673 - mse: 33384210.0000 - mae: 2332.8674 - val_loss: 1924.0294 - val_mse: 26114238.0000 - val_mae: 1924.0295\n",
            "Epoch 307/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2355.6927 - mse: 33533946.0000 - mae: 2355.6929 - val_loss: 1923.7258 - val_mse: 26123168.0000 - val_mae: 1923.7258\n",
            "Epoch 308/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2364.1971 - mse: 33536716.0000 - mae: 2364.1970 - val_loss: 1923.4288 - val_mse: 26132206.0000 - val_mae: 1923.4288\n",
            "Epoch 309/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2340.9531 - mse: 33569268.0000 - mae: 2340.9529 - val_loss: 1923.6482 - val_mse: 26127186.0000 - val_mae: 1923.6482\n",
            "Epoch 310/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2350.5631 - mse: 33303948.0000 - mae: 2350.5632 - val_loss: 1923.4337 - val_mse: 26129914.0000 - val_mae: 1923.4338\n",
            "Epoch 311/500\n",
            "380/380 [==============================] - 0s 96us/step - loss: 2340.6538 - mse: 33392362.0000 - mae: 2340.6538 - val_loss: 1923.3221 - val_mse: 26131700.0000 - val_mae: 1923.3219\n",
            "Epoch 312/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2341.9685 - mse: 33408288.0000 - mae: 2341.9685 - val_loss: 1923.5614 - val_mse: 26127800.0000 - val_mae: 1923.5615\n",
            "Epoch 313/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2363.0716 - mse: 33726004.0000 - mae: 2363.0718 - val_loss: 1923.2646 - val_mse: 26133238.0000 - val_mae: 1923.2645\n",
            "Epoch 314/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2350.0519 - mse: 33560144.0000 - mae: 2350.0520 - val_loss: 1923.4738 - val_mse: 26129204.0000 - val_mae: 1923.4739\n",
            "Epoch 315/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2359.4749 - mse: 33602820.0000 - mae: 2359.4751 - val_loss: 1923.3460 - val_mse: 26131774.0000 - val_mae: 1923.3459\n",
            "Epoch 316/500\n",
            "380/380 [==============================] - 0s 97us/step - loss: 2345.2235 - mse: 33483030.0000 - mae: 2345.2236 - val_loss: 1923.5213 - val_mse: 26124750.0000 - val_mae: 1923.5212\n",
            "Epoch 317/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2352.9060 - mse: 33464508.0000 - mae: 2352.9060 - val_loss: 1923.7384 - val_mse: 26120198.0000 - val_mae: 1923.7385\n",
            "Epoch 318/500\n",
            "380/380 [==============================] - 0s 93us/step - loss: 2355.5970 - mse: 33540524.0000 - mae: 2355.5969 - val_loss: 1923.5533 - val_mse: 26123176.0000 - val_mae: 1923.5533\n",
            "Epoch 319/500\n",
            "380/380 [==============================] - 0s 97us/step - loss: 2325.2598 - mse: 33539320.0000 - mae: 2325.2598 - val_loss: 1923.6766 - val_mse: 26119910.0000 - val_mae: 1923.6766\n",
            "Epoch 320/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2355.0498 - mse: 33336730.0000 - mae: 2355.0496 - val_loss: 1923.6579 - val_mse: 26113036.0000 - val_mae: 1923.6580\n",
            "Epoch 321/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2328.8635 - mse: 33220070.0000 - mae: 2328.8635 - val_loss: 1923.8676 - val_mse: 26104134.0000 - val_mae: 1923.8677\n",
            "Epoch 322/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2329.4864 - mse: 33378040.0000 - mae: 2329.4863 - val_loss: 1923.8667 - val_mse: 26105698.0000 - val_mae: 1923.8667\n",
            "Epoch 323/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2345.6336 - mse: 33424174.0000 - mae: 2345.6335 - val_loss: 1923.8264 - val_mse: 26109540.0000 - val_mae: 1923.8263\n",
            "Epoch 324/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2344.3609 - mse: 33441142.0000 - mae: 2344.3608 - val_loss: 1923.8968 - val_mse: 26104856.0000 - val_mae: 1923.8969\n",
            "Epoch 325/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2368.0971 - mse: 33528452.0000 - mae: 2368.0969 - val_loss: 1923.8002 - val_mse: 26111194.0000 - val_mae: 1923.8002\n",
            "Epoch 326/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2345.4130 - mse: 33444082.0000 - mae: 2345.4131 - val_loss: 1923.7291 - val_mse: 26109838.0000 - val_mae: 1923.7292\n",
            "Epoch 327/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2356.1676 - mse: 33376316.0000 - mae: 2356.1675 - val_loss: 1923.6844 - val_mse: 26116768.0000 - val_mae: 1923.6844\n",
            "Epoch 328/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2367.0036 - mse: 33612004.0000 - mae: 2367.0037 - val_loss: 1923.4930 - val_mse: 26121404.0000 - val_mae: 1923.4930\n",
            "Epoch 329/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2346.5392 - mse: 33371940.0000 - mae: 2346.5391 - val_loss: 1923.3644 - val_mse: 26121586.0000 - val_mae: 1923.3644\n",
            "Epoch 330/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2328.8806 - mse: 33403928.0000 - mae: 2328.8806 - val_loss: 1923.5615 - val_mse: 26115296.0000 - val_mae: 1923.5615\n",
            "Epoch 331/500\n",
            "380/380 [==============================] - 0s 116us/step - loss: 2331.4187 - mse: 33370090.0000 - mae: 2331.4187 - val_loss: 1923.4901 - val_mse: 26115900.0000 - val_mae: 1923.4900\n",
            "Epoch 332/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2347.9322 - mse: 33387908.0000 - mae: 2347.9321 - val_loss: 1923.3047 - val_mse: 26121692.0000 - val_mae: 1923.3047\n",
            "Epoch 333/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2340.1485 - mse: 33405732.0000 - mae: 2340.1484 - val_loss: 1923.1569 - val_mse: 26130078.0000 - val_mae: 1923.1570\n",
            "Epoch 334/500\n",
            "380/380 [==============================] - 0s 123us/step - loss: 2338.3283 - mse: 33431274.0000 - mae: 2338.3284 - val_loss: 1923.3328 - val_mse: 26127972.0000 - val_mae: 1923.3328\n",
            "Epoch 335/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2351.1613 - mse: 33496108.0000 - mae: 2351.1611 - val_loss: 1923.3349 - val_mse: 26123884.0000 - val_mae: 1923.3348\n",
            "Epoch 336/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2335.7070 - mse: 33323196.0000 - mae: 2335.7068 - val_loss: 1923.3837 - val_mse: 26120344.0000 - val_mae: 1923.3838\n",
            "Epoch 337/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2347.7704 - mse: 33561540.0000 - mae: 2347.7705 - val_loss: 1923.3357 - val_mse: 26123634.0000 - val_mae: 1923.3357\n",
            "Epoch 338/500\n",
            "380/380 [==============================] - 0s 118us/step - loss: 2339.1221 - mse: 33501520.0000 - mae: 2339.1221 - val_loss: 1923.2424 - val_mse: 26126662.0000 - val_mae: 1923.2424\n",
            "Epoch 339/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2328.4896 - mse: 33476336.0000 - mae: 2328.4897 - val_loss: 1923.3196 - val_mse: 26123486.0000 - val_mae: 1923.3196\n",
            "Epoch 340/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2348.6058 - mse: 33468648.0000 - mae: 2348.6057 - val_loss: 1923.3729 - val_mse: 26124080.0000 - val_mae: 1923.3729\n",
            "Epoch 341/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2356.6276 - mse: 33560028.0000 - mae: 2356.6277 - val_loss: 1923.1812 - val_mse: 26127770.0000 - val_mae: 1923.1813\n",
            "Epoch 342/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2332.1741 - mse: 33439770.0000 - mae: 2332.1743 - val_loss: 1923.1586 - val_mse: 26124980.0000 - val_mae: 1923.1586\n",
            "Epoch 343/500\n",
            "380/380 [==============================] - 0s 141us/step - loss: 2341.7920 - mse: 33338316.0000 - mae: 2341.7920 - val_loss: 1923.1071 - val_mse: 26124982.0000 - val_mae: 1923.1071\n",
            "Epoch 344/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2345.3004 - mse: 33413190.0000 - mae: 2345.3003 - val_loss: 1922.8491 - val_mse: 26126308.0000 - val_mae: 1922.8490\n",
            "Epoch 345/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2338.6501 - mse: 33325592.0000 - mae: 2338.6501 - val_loss: 1922.8369 - val_mse: 26125636.0000 - val_mae: 1922.8368\n",
            "Epoch 346/500\n",
            "380/380 [==============================] - 0s 98us/step - loss: 2341.4491 - mse: 33473316.0000 - mae: 2341.4490 - val_loss: 1922.7203 - val_mse: 26126710.0000 - val_mae: 1922.7205\n",
            "Epoch 347/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2350.8228 - mse: 33488120.0000 - mae: 2350.8228 - val_loss: 1922.6719 - val_mse: 26126770.0000 - val_mae: 1922.6720\n",
            "Epoch 348/500\n",
            "380/380 [==============================] - 0s 127us/step - loss: 2350.3907 - mse: 33323456.0000 - mae: 2350.3909 - val_loss: 1922.7297 - val_mse: 26128988.0000 - val_mae: 1922.7296\n",
            "Epoch 349/500\n",
            "380/380 [==============================] - 0s 120us/step - loss: 2346.0915 - mse: 33662304.0000 - mae: 2346.0916 - val_loss: 1922.7072 - val_mse: 26131246.0000 - val_mae: 1922.7073\n",
            "Epoch 350/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2348.1477 - mse: 33509810.0000 - mae: 2348.1477 - val_loss: 1922.7065 - val_mse: 26131432.0000 - val_mae: 1922.7064\n",
            "Epoch 351/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2337.7938 - mse: 33457006.0000 - mae: 2337.7937 - val_loss: 1922.7060 - val_mse: 26134520.0000 - val_mae: 1922.7059\n",
            "Epoch 352/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2354.6751 - mse: 33628228.0000 - mae: 2354.6750 - val_loss: 1922.7034 - val_mse: 26133516.0000 - val_mae: 1922.7035\n",
            "Epoch 353/500\n",
            "380/380 [==============================] - 0s 129us/step - loss: 2341.3946 - mse: 33278206.0000 - mae: 2341.3945 - val_loss: 1922.7745 - val_mse: 26131990.0000 - val_mae: 1922.7745\n",
            "Epoch 354/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2359.6328 - mse: 33590552.0000 - mae: 2359.6328 - val_loss: 1922.8238 - val_mse: 26133352.0000 - val_mae: 1922.8239\n",
            "Epoch 355/500\n",
            "380/380 [==============================] - 0s 118us/step - loss: 2358.4727 - mse: 33557828.0000 - mae: 2358.4727 - val_loss: 1922.7164 - val_mse: 26131838.0000 - val_mae: 1922.7164\n",
            "Epoch 356/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2347.5721 - mse: 33692460.0000 - mae: 2347.5720 - val_loss: 1922.7246 - val_mse: 26130988.0000 - val_mae: 1922.7245\n",
            "Epoch 357/500\n",
            "380/380 [==============================] - 0s 127us/step - loss: 2341.4785 - mse: 33436186.0000 - mae: 2341.4785 - val_loss: 1922.7614 - val_mse: 26132510.0000 - val_mae: 1922.7614\n",
            "Epoch 358/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2337.9273 - mse: 33118914.0000 - mae: 2337.9272 - val_loss: 1922.8291 - val_mse: 26130658.0000 - val_mae: 1922.8292\n",
            "Epoch 359/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2339.3100 - mse: 33355464.0000 - mae: 2339.3101 - val_loss: 1922.9464 - val_mse: 26124202.0000 - val_mae: 1922.9464\n",
            "Epoch 360/500\n",
            "380/380 [==============================] - 0s 116us/step - loss: 2332.2969 - mse: 33509796.0000 - mae: 2332.2971 - val_loss: 1922.9175 - val_mse: 26121914.0000 - val_mae: 1922.9176\n",
            "Epoch 361/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2338.5550 - mse: 33485008.0000 - mae: 2338.5549 - val_loss: 1922.8782 - val_mse: 26118396.0000 - val_mae: 1922.8782\n",
            "Epoch 362/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2362.0173 - mse: 33338836.0000 - mae: 2362.0173 - val_loss: 1922.7854 - val_mse: 26123006.0000 - val_mae: 1922.7853\n",
            "Epoch 363/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2339.4724 - mse: 33355806.0000 - mae: 2339.4724 - val_loss: 1922.6717 - val_mse: 26120286.0000 - val_mae: 1922.6718\n",
            "Epoch 364/500\n",
            "380/380 [==============================] - 0s 141us/step - loss: 2330.9305 - mse: 33529516.0000 - mae: 2330.9307 - val_loss: 1922.8752 - val_mse: 26114400.0000 - val_mae: 1922.8754\n",
            "Epoch 365/500\n",
            "380/380 [==============================] - 0s 122us/step - loss: 2328.9857 - mse: 33221836.0000 - mae: 2328.9856 - val_loss: 1922.9704 - val_mse: 26109844.0000 - val_mae: 1922.9705\n",
            "Epoch 366/500\n",
            "380/380 [==============================] - 0s 95us/step - loss: 2341.8897 - mse: 33532076.0000 - mae: 2341.8896 - val_loss: 1922.6497 - val_mse: 26113298.0000 - val_mae: 1922.6497\n",
            "Epoch 367/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2346.6839 - mse: 33300518.0000 - mae: 2346.6838 - val_loss: 1922.8355 - val_mse: 26109940.0000 - val_mae: 1922.8354\n",
            "Epoch 368/500\n",
            "380/380 [==============================] - 0s 129us/step - loss: 2349.9409 - mse: 33419870.0000 - mae: 2349.9409 - val_loss: 1922.7656 - val_mse: 26109440.0000 - val_mae: 1922.7657\n",
            "Epoch 369/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2339.6506 - mse: 33393976.0000 - mae: 2339.6506 - val_loss: 1922.7990 - val_mse: 26106910.0000 - val_mae: 1922.7990\n",
            "Epoch 370/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2347.5343 - mse: 33320598.0000 - mae: 2347.5342 - val_loss: 1922.7249 - val_mse: 26106614.0000 - val_mae: 1922.7250\n",
            "Epoch 371/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2344.2751 - mse: 33586596.0000 - mae: 2344.2751 - val_loss: 1922.6718 - val_mse: 26111104.0000 - val_mae: 1922.6720\n",
            "Epoch 372/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2352.3747 - mse: 33468728.0000 - mae: 2352.3748 - val_loss: 1922.4873 - val_mse: 26110610.0000 - val_mae: 1922.4873\n",
            "Epoch 373/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2341.7541 - mse: 33217622.0000 - mae: 2341.7542 - val_loss: 1922.4510 - val_mse: 26108104.0000 - val_mae: 1922.4510\n",
            "Epoch 374/500\n",
            "380/380 [==============================] - 0s 125us/step - loss: 2351.8740 - mse: 33384152.0000 - mae: 2351.8740 - val_loss: 1922.5419 - val_mse: 26111284.0000 - val_mae: 1922.5419\n",
            "Epoch 375/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2341.1331 - mse: 33342704.0000 - mae: 2341.1331 - val_loss: 1922.4263 - val_mse: 26112880.0000 - val_mae: 1922.4264\n",
            "Epoch 376/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2333.0459 - mse: 33165410.0000 - mae: 2333.0461 - val_loss: 1922.5751 - val_mse: 26111356.0000 - val_mae: 1922.5750\n",
            "Epoch 377/500\n",
            "380/380 [==============================] - 0s 130us/step - loss: 2349.3012 - mse: 33405642.0000 - mae: 2349.3013 - val_loss: 1922.5509 - val_mse: 26108958.0000 - val_mae: 1922.5509\n",
            "Epoch 378/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2337.1818 - mse: 33306894.0000 - mae: 2337.1816 - val_loss: 1922.2393 - val_mse: 26113862.0000 - val_mae: 1922.2394\n",
            "Epoch 379/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2344.6325 - mse: 33321774.0000 - mae: 2344.6326 - val_loss: 1922.4504 - val_mse: 26109100.0000 - val_mae: 1922.4506\n",
            "Epoch 380/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2343.4679 - mse: 33279068.0000 - mae: 2343.4680 - val_loss: 1922.3359 - val_mse: 26113040.0000 - val_mae: 1922.3358\n",
            "Epoch 381/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2334.3192 - mse: 33368050.0000 - mae: 2334.3191 - val_loss: 1922.1703 - val_mse: 26111740.0000 - val_mae: 1922.1703\n",
            "Epoch 382/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2338.4976 - mse: 33498058.0000 - mae: 2338.4978 - val_loss: 1922.1962 - val_mse: 26108178.0000 - val_mae: 1922.1962\n",
            "Epoch 383/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2358.4140 - mse: 33516962.0000 - mae: 2358.4138 - val_loss: 1922.0014 - val_mse: 26114956.0000 - val_mae: 1922.0013\n",
            "Epoch 384/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2333.3769 - mse: 33402564.0000 - mae: 2333.3767 - val_loss: 1921.8697 - val_mse: 26113146.0000 - val_mae: 1921.8698\n",
            "Epoch 385/500\n",
            "380/380 [==============================] - 0s 144us/step - loss: 2334.1592 - mse: 33377142.0000 - mae: 2334.1589 - val_loss: 1922.0413 - val_mse: 26112124.0000 - val_mae: 1922.0413\n",
            "Epoch 386/500\n",
            "380/380 [==============================] - 0s 136us/step - loss: 2341.6081 - mse: 33430084.0000 - mae: 2341.6082 - val_loss: 1922.0752 - val_mse: 26110386.0000 - val_mae: 1922.0752\n",
            "Epoch 387/500\n",
            "380/380 [==============================] - 0s 117us/step - loss: 2349.6558 - mse: 33367008.0000 - mae: 2349.6558 - val_loss: 1921.9797 - val_mse: 26109406.0000 - val_mae: 1921.9797\n",
            "Epoch 388/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2331.4817 - mse: 33554088.0000 - mae: 2331.4817 - val_loss: 1921.6209 - val_mse: 26115986.0000 - val_mae: 1921.6210\n",
            "Epoch 389/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2321.5330 - mse: 33344044.0000 - mae: 2321.5330 - val_loss: 1921.5941 - val_mse: 26109290.0000 - val_mae: 1921.5941\n",
            "Epoch 390/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2324.7002 - mse: 33300364.0000 - mae: 2324.7002 - val_loss: 1921.5420 - val_mse: 26110296.0000 - val_mae: 1921.5421\n",
            "Epoch 391/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2343.4916 - mse: 33395690.0000 - mae: 2343.4915 - val_loss: 1921.4384 - val_mse: 26112712.0000 - val_mae: 1921.4385\n",
            "Epoch 392/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2345.8458 - mse: 33560232.0000 - mae: 2345.8459 - val_loss: 1921.2252 - val_mse: 26111264.0000 - val_mae: 1921.2252\n",
            "Epoch 393/500\n",
            "380/380 [==============================] - 0s 130us/step - loss: 2355.1615 - mse: 33494534.0000 - mae: 2355.1616 - val_loss: 1921.2771 - val_mse: 26106904.0000 - val_mae: 1921.2772\n",
            "Epoch 394/500\n",
            "380/380 [==============================] - 0s 123us/step - loss: 2344.8340 - mse: 33605064.0000 - mae: 2344.8340 - val_loss: 1921.4938 - val_mse: 26100376.0000 - val_mae: 1921.4938\n",
            "Epoch 395/500\n",
            "380/380 [==============================] - 0s 132us/step - loss: 2358.7549 - mse: 33612728.0000 - mae: 2358.7549 - val_loss: 1921.3073 - val_mse: 26103438.0000 - val_mae: 1921.3073\n",
            "Epoch 396/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2350.3967 - mse: 33436938.0000 - mae: 2350.3967 - val_loss: 1921.2409 - val_mse: 26103950.0000 - val_mae: 1921.2411\n",
            "Epoch 397/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2321.5233 - mse: 33429002.0000 - mae: 2321.5232 - val_loss: 1921.1486 - val_mse: 26104384.0000 - val_mae: 1921.1487\n",
            "Epoch 398/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2340.7833 - mse: 33352890.0000 - mae: 2340.7832 - val_loss: 1921.3064 - val_mse: 26095628.0000 - val_mae: 1921.3064\n",
            "Epoch 399/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2336.2686 - mse: 33615088.0000 - mae: 2336.2683 - val_loss: 1921.1996 - val_mse: 26096150.0000 - val_mae: 1921.1997\n",
            "Epoch 400/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2341.6631 - mse: 33264560.0000 - mae: 2341.6631 - val_loss: 1921.3351 - val_mse: 26093452.0000 - val_mae: 1921.3350\n",
            "Epoch 401/500\n",
            "380/380 [==============================] - 0s 130us/step - loss: 2336.8875 - mse: 33439084.0000 - mae: 2336.8875 - val_loss: 1921.1965 - val_mse: 26097742.0000 - val_mae: 1921.1964\n",
            "Epoch 402/500\n",
            "380/380 [==============================] - 0s 122us/step - loss: 2332.8161 - mse: 33435394.0000 - mae: 2332.8162 - val_loss: 1921.3196 - val_mse: 26092806.0000 - val_mae: 1921.3196\n",
            "Epoch 403/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2332.8275 - mse: 33418122.0000 - mae: 2332.8274 - val_loss: 1921.3637 - val_mse: 26094548.0000 - val_mae: 1921.3636\n",
            "Epoch 404/500\n",
            "380/380 [==============================] - 0s 127us/step - loss: 2340.2003 - mse: 33381408.0000 - mae: 2340.2004 - val_loss: 1921.3417 - val_mse: 26095882.0000 - val_mae: 1921.3416\n",
            "Epoch 405/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2363.3267 - mse: 33370780.0000 - mae: 2363.3267 - val_loss: 1921.2246 - val_mse: 26096166.0000 - val_mae: 1921.2246\n",
            "Epoch 406/500\n",
            "380/380 [==============================] - 0s 129us/step - loss: 2337.3812 - mse: 33271728.0000 - mae: 2337.3813 - val_loss: 1920.9704 - val_mse: 26090354.0000 - val_mae: 1920.9705\n",
            "Epoch 407/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2338.5306 - mse: 33433978.0000 - mae: 2338.5305 - val_loss: 1921.1630 - val_mse: 26087040.0000 - val_mae: 1921.1632\n",
            "Epoch 408/500\n",
            "380/380 [==============================] - 0s 116us/step - loss: 2344.0603 - mse: 33350096.0000 - mae: 2344.0603 - val_loss: 1921.6738 - val_mse: 26081324.0000 - val_mae: 1921.6738\n",
            "Epoch 409/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2347.2148 - mse: 33348808.0000 - mae: 2347.2148 - val_loss: 1921.7051 - val_mse: 26079434.0000 - val_mae: 1921.7051\n",
            "Epoch 410/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2354.2613 - mse: 33632620.0000 - mae: 2354.2612 - val_loss: 1921.2974 - val_mse: 26084994.0000 - val_mae: 1921.2975\n",
            "Epoch 411/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2331.8326 - mse: 33320292.0000 - mae: 2331.8325 - val_loss: 1920.8919 - val_mse: 26090332.0000 - val_mae: 1920.8918\n",
            "Epoch 412/500\n",
            "380/380 [==============================] - 0s 123us/step - loss: 2346.4194 - mse: 33361122.0000 - mae: 2346.4194 - val_loss: 1921.0267 - val_mse: 26091246.0000 - val_mae: 1921.0267\n",
            "Epoch 413/500\n",
            "380/380 [==============================] - 0s 143us/step - loss: 2329.5124 - mse: 33589080.0000 - mae: 2329.5125 - val_loss: 1921.0441 - val_mse: 26088254.0000 - val_mae: 1921.0441\n",
            "Epoch 414/500\n",
            "380/380 [==============================] - 0s 130us/step - loss: 2349.8870 - mse: 33479682.0000 - mae: 2349.8870 - val_loss: 1920.6902 - val_mse: 26091072.0000 - val_mae: 1920.6902\n",
            "Epoch 415/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2329.6316 - mse: 33302926.0000 - mae: 2329.6316 - val_loss: 1920.6311 - val_mse: 26093674.0000 - val_mae: 1920.6311\n",
            "Epoch 416/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2347.8289 - mse: 33470296.0000 - mae: 2347.8289 - val_loss: 1920.5362 - val_mse: 26095820.0000 - val_mae: 1920.5361\n",
            "Epoch 417/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2338.5893 - mse: 33295246.0000 - mae: 2338.5894 - val_loss: 1919.9758 - val_mse: 26100612.0000 - val_mae: 1919.9758\n",
            "Epoch 418/500\n",
            "380/380 [==============================] - 0s 136us/step - loss: 2354.4933 - mse: 33527172.0000 - mae: 2354.4934 - val_loss: 1919.6277 - val_mse: 26101312.0000 - val_mae: 1919.6277\n",
            "Epoch 419/500\n",
            "380/380 [==============================] - 0s 123us/step - loss: 2330.6747 - mse: 33193628.0000 - mae: 2330.6746 - val_loss: 1919.4070 - val_mse: 26101586.0000 - val_mae: 1919.4071\n",
            "Epoch 420/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2349.1440 - mse: 33455616.0000 - mae: 2349.1440 - val_loss: 1919.2372 - val_mse: 26095290.0000 - val_mae: 1919.2372\n",
            "Epoch 421/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2351.3594 - mse: 33285382.0000 - mae: 2351.3591 - val_loss: 1919.2725 - val_mse: 26095394.0000 - val_mae: 1919.2726\n",
            "Epoch 422/500\n",
            "380/380 [==============================] - 0s 126us/step - loss: 2341.3160 - mse: 33139482.0000 - mae: 2341.3162 - val_loss: 1919.1680 - val_mse: 26092090.0000 - val_mae: 1919.1681\n",
            "Epoch 423/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2332.7414 - mse: 33190300.0000 - mae: 2332.7415 - val_loss: 1919.2567 - val_mse: 26090128.0000 - val_mae: 1919.2567\n",
            "Epoch 424/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2352.5158 - mse: 33681632.0000 - mae: 2352.5159 - val_loss: 1918.8607 - val_mse: 26096200.0000 - val_mae: 1918.8606\n",
            "Epoch 425/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2346.3708 - mse: 33449416.0000 - mae: 2346.3708 - val_loss: 1918.0634 - val_mse: 26100174.0000 - val_mae: 1918.0634\n",
            "Epoch 426/500\n",
            "380/380 [==============================] - 0s 136us/step - loss: 2375.0037 - mse: 33725668.0000 - mae: 2375.0037 - val_loss: 1917.5291 - val_mse: 26107594.0000 - val_mae: 1917.5292\n",
            "Epoch 427/500\n",
            "380/380 [==============================] - 0s 122us/step - loss: 2335.5423 - mse: 33331598.0000 - mae: 2335.5420 - val_loss: 1917.5563 - val_mse: 26107010.0000 - val_mae: 1917.5563\n",
            "Epoch 428/500\n",
            "380/380 [==============================] - 0s 125us/step - loss: 2329.6132 - mse: 33211816.0000 - mae: 2329.6130 - val_loss: 1916.6771 - val_mse: 26109310.0000 - val_mae: 1916.6770\n",
            "Epoch 429/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2331.0748 - mse: 33429450.0000 - mae: 2331.0747 - val_loss: 1916.5816 - val_mse: 26104524.0000 - val_mae: 1916.5815\n",
            "Epoch 430/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2337.2942 - mse: 33550342.0000 - mae: 2337.2942 - val_loss: 1916.3838 - val_mse: 26105260.0000 - val_mae: 1916.3838\n",
            "Epoch 431/500\n",
            "380/380 [==============================] - 0s 132us/step - loss: 2338.1497 - mse: 33543556.0000 - mae: 2338.1497 - val_loss: 1915.7933 - val_mse: 26105040.0000 - val_mae: 1915.7935\n",
            "Epoch 432/500\n",
            "380/380 [==============================] - 0s 134us/step - loss: 2341.9772 - mse: 33368426.0000 - mae: 2341.9771 - val_loss: 1914.8028 - val_mse: 26103460.0000 - val_mae: 1914.8027\n",
            "Epoch 433/500\n",
            "380/380 [==============================] - 0s 125us/step - loss: 2327.4130 - mse: 33647104.0000 - mae: 2327.4131 - val_loss: 1914.0929 - val_mse: 26100100.0000 - val_mae: 1914.0929\n",
            "Epoch 434/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2347.0514 - mse: 33519934.0000 - mae: 2347.0515 - val_loss: 1913.0315 - val_mse: 26095088.0000 - val_mae: 1913.0316\n",
            "Epoch 435/500\n",
            "380/380 [==============================] - 0s 109us/step - loss: 2346.2147 - mse: 33468514.0000 - mae: 2346.2146 - val_loss: 1911.3946 - val_mse: 26094980.0000 - val_mae: 1911.3947\n",
            "Epoch 436/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2318.5838 - mse: 33208012.0000 - mae: 2318.5837 - val_loss: 1909.9088 - val_mse: 26091074.0000 - val_mae: 1909.9089\n",
            "Epoch 437/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2326.2559 - mse: 33319526.0000 - mae: 2326.2559 - val_loss: 1907.8694 - val_mse: 26092564.0000 - val_mae: 1907.8694\n",
            "Epoch 438/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2340.0328 - mse: 33489936.0000 - mae: 2340.0330 - val_loss: 1905.1737 - val_mse: 26089486.0000 - val_mae: 1905.1737\n",
            "Epoch 439/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2340.1289 - mse: 33647136.0000 - mae: 2340.1289 - val_loss: 1901.6097 - val_mse: 26089632.0000 - val_mae: 1901.6096\n",
            "Epoch 440/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2329.3289 - mse: 33462118.0000 - mae: 2329.3289 - val_loss: 1897.1574 - val_mse: 26090854.0000 - val_mae: 1897.1575\n",
            "Epoch 441/500\n",
            "380/380 [==============================] - 0s 111us/step - loss: 2334.6808 - mse: 33434934.0000 - mae: 2334.6807 - val_loss: 1890.1566 - val_mse: 26091760.0000 - val_mae: 1890.1567\n",
            "Epoch 442/500\n",
            "380/380 [==============================] - 0s 116us/step - loss: 2312.7239 - mse: 33260048.0000 - mae: 2312.7241 - val_loss: 1881.5041 - val_mse: 26092318.0000 - val_mae: 1881.5040\n",
            "Epoch 443/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2322.3476 - mse: 33554748.0000 - mae: 2322.3477 - val_loss: 1867.4073 - val_mse: 26107904.0000 - val_mae: 1867.4075\n",
            "Epoch 444/500\n",
            "380/380 [==============================] - 0s 115us/step - loss: 2295.3485 - mse: 33241662.0000 - mae: 2295.3484 - val_loss: 1852.0290 - val_mse: 26107466.0000 - val_mae: 1852.0291\n",
            "Epoch 445/500\n",
            "380/380 [==============================] - 0s 140us/step - loss: 2283.9194 - mse: 33333638.0000 - mae: 2283.9194 - val_loss: 1842.4218 - val_mse: 26098674.0000 - val_mae: 1842.4218\n",
            "Epoch 446/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2300.2739 - mse: 33258340.0000 - mae: 2300.2739 - val_loss: 1831.8715 - val_mse: 26095004.0000 - val_mae: 1831.8715\n",
            "Epoch 447/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2288.5717 - mse: 33337672.0000 - mae: 2288.5718 - val_loss: 1817.5273 - val_mse: 26108122.0000 - val_mae: 1817.5273\n",
            "Epoch 448/500\n",
            "380/380 [==============================] - 0s 134us/step - loss: 2275.7149 - mse: 33197676.0000 - mae: 2275.7148 - val_loss: 1811.1952 - val_mse: 26107962.0000 - val_mae: 1811.1953\n",
            "Epoch 449/500\n",
            "380/380 [==============================] - 0s 134us/step - loss: 2259.1679 - mse: 33397490.0000 - mae: 2259.1682 - val_loss: 1806.5767 - val_mse: 26107018.0000 - val_mae: 1806.5769\n",
            "Epoch 450/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2257.2560 - mse: 33291484.0000 - mae: 2257.2559 - val_loss: 1803.8581 - val_mse: 26108594.0000 - val_mae: 1803.8582\n",
            "Epoch 451/500\n",
            "380/380 [==============================] - 0s 130us/step - loss: 2259.4517 - mse: 33336062.0000 - mae: 2259.4517 - val_loss: 1802.1232 - val_mse: 26070608.0000 - val_mae: 1802.1232\n",
            "Epoch 452/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2255.9829 - mse: 33315484.0000 - mae: 2255.9829 - val_loss: 1801.8560 - val_mse: 26014972.0000 - val_mae: 1801.8561\n",
            "Epoch 453/500\n",
            "380/380 [==============================] - 0s 122us/step - loss: 2258.9522 - mse: 33135018.0000 - mae: 2258.9524 - val_loss: 1798.7935 - val_mse: 25997066.0000 - val_mae: 1798.7935\n",
            "Epoch 454/500\n",
            "380/380 [==============================] - 0s 133us/step - loss: 2252.2983 - mse: 33182940.0000 - mae: 2252.2983 - val_loss: 1797.1706 - val_mse: 25971040.0000 - val_mae: 1797.1707\n",
            "Epoch 455/500\n",
            "380/380 [==============================] - 0s 104us/step - loss: 2251.6971 - mse: 33257374.0000 - mae: 2251.6970 - val_loss: 1796.0242 - val_mse: 25967134.0000 - val_mae: 1796.0242\n",
            "Epoch 456/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2242.0656 - mse: 33216474.0000 - mae: 2242.0657 - val_loss: 1794.2652 - val_mse: 25920120.0000 - val_mae: 1794.2653\n",
            "Epoch 457/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2256.6674 - mse: 33172710.0000 - mae: 2256.6675 - val_loss: 1793.9780 - val_mse: 25918758.0000 - val_mae: 1793.9780\n",
            "Epoch 458/500\n",
            "380/380 [==============================] - 0s 120us/step - loss: 2229.6770 - mse: 32821140.0000 - mae: 2229.6770 - val_loss: 1791.0697 - val_mse: 25872008.0000 - val_mae: 1791.0698\n",
            "Epoch 459/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2247.0771 - mse: 33192166.0000 - mae: 2247.0771 - val_loss: 1789.6565 - val_mse: 25843804.0000 - val_mae: 1789.6564\n",
            "Epoch 460/500\n",
            "380/380 [==============================] - 0s 127us/step - loss: 2239.7608 - mse: 32842762.0000 - mae: 2239.7607 - val_loss: 1788.9216 - val_mse: 25798524.0000 - val_mae: 1788.9215\n",
            "Epoch 461/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2245.6432 - mse: 32983806.0000 - mae: 2245.6433 - val_loss: 1788.3032 - val_mse: 25758462.0000 - val_mae: 1788.3032\n",
            "Epoch 462/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2216.8180 - mse: 32737992.0000 - mae: 2216.8181 - val_loss: 1786.3589 - val_mse: 25769846.0000 - val_mae: 1786.3590\n",
            "Epoch 463/500\n",
            "380/380 [==============================] - 0s 153us/step - loss: 2223.8090 - mse: 32915446.0000 - mae: 2223.8088 - val_loss: 1786.3509 - val_mse: 25755386.0000 - val_mae: 1786.3511\n",
            "Epoch 464/500\n",
            "380/380 [==============================] - 0s 132us/step - loss: 2214.8312 - mse: 32770914.0000 - mae: 2214.8313 - val_loss: 1782.4268 - val_mse: 25705984.0000 - val_mae: 1782.4266\n",
            "Epoch 465/500\n",
            "380/380 [==============================] - 0s 131us/step - loss: 2213.6431 - mse: 32616256.0000 - mae: 2213.6431 - val_loss: 1780.8991 - val_mse: 25675392.0000 - val_mae: 1780.8989\n",
            "Epoch 466/500\n",
            "380/380 [==============================] - 0s 137us/step - loss: 2214.8101 - mse: 32900882.0000 - mae: 2214.8103 - val_loss: 1779.3776 - val_mse: 25646222.0000 - val_mae: 1779.3776\n",
            "Epoch 467/500\n",
            "380/380 [==============================] - 0s 114us/step - loss: 2230.8439 - mse: 33118834.0000 - mae: 2230.8440 - val_loss: 1781.0993 - val_mse: 25648748.0000 - val_mae: 1781.0994\n",
            "Epoch 468/500\n",
            "380/380 [==============================] - 0s 90us/step - loss: 2217.7422 - mse: 32784482.0000 - mae: 2217.7422 - val_loss: 1779.3694 - val_mse: 25622158.0000 - val_mae: 1779.3693\n",
            "Epoch 469/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2216.0359 - mse: 32684822.0000 - mae: 2216.0359 - val_loss: 1778.6585 - val_mse: 25603086.0000 - val_mae: 1778.6584\n",
            "Epoch 470/500\n",
            "380/380 [==============================] - 0s 137us/step - loss: 2221.0450 - mse: 32804298.0000 - mae: 2221.0452 - val_loss: 1782.6529 - val_mse: 25604570.0000 - val_mae: 1782.6528\n",
            "Epoch 471/500\n",
            "380/380 [==============================] - 0s 118us/step - loss: 2202.6740 - mse: 32645588.0000 - mae: 2202.6741 - val_loss: 1774.3276 - val_mse: 25541950.0000 - val_mae: 1774.3278\n",
            "Epoch 472/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2188.0030 - mse: 32134446.0000 - mae: 2188.0029 - val_loss: 1777.8469 - val_mse: 25541776.0000 - val_mae: 1777.8469\n",
            "Epoch 473/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2207.5245 - mse: 32534070.0000 - mae: 2207.5247 - val_loss: 1784.3038 - val_mse: 25545598.0000 - val_mae: 1784.3038\n",
            "Epoch 474/500\n",
            "380/380 [==============================] - 0s 129us/step - loss: 2196.6981 - mse: 32421402.0000 - mae: 2196.6982 - val_loss: 1776.7680 - val_mse: 25496362.0000 - val_mae: 1776.7679\n",
            "Epoch 475/500\n",
            "380/380 [==============================] - 0s 136us/step - loss: 2206.1596 - mse: 32541664.0000 - mae: 2206.1597 - val_loss: 1773.7692 - val_mse: 25465026.0000 - val_mae: 1773.7693\n",
            "Epoch 476/500\n",
            "380/380 [==============================] - 0s 112us/step - loss: 2184.9332 - mse: 32266882.0000 - mae: 2184.9331 - val_loss: 1768.1014 - val_mse: 25405800.0000 - val_mae: 1768.1013\n",
            "Epoch 477/500\n",
            "380/380 [==============================] - 0s 128us/step - loss: 2195.6467 - mse: 32316810.0000 - mae: 2195.6467 - val_loss: 1767.7556 - val_mse: 25386682.0000 - val_mae: 1767.7556\n",
            "Epoch 478/500\n",
            "380/380 [==============================] - 0s 133us/step - loss: 2199.8003 - mse: 32588676.0000 - mae: 2199.8003 - val_loss: 1769.3103 - val_mse: 25374670.0000 - val_mae: 1769.3103\n",
            "Epoch 479/500\n",
            "380/380 [==============================] - 0s 118us/step - loss: 2178.3730 - mse: 32048214.0000 - mae: 2178.3730 - val_loss: 1770.6429 - val_mse: 25363920.0000 - val_mae: 1770.6429\n",
            "Epoch 480/500\n",
            "380/380 [==============================] - 0s 135us/step - loss: 2180.1803 - mse: 32125802.0000 - mae: 2180.1802 - val_loss: 1765.3974 - val_mse: 25305962.0000 - val_mae: 1765.3973\n",
            "Epoch 481/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2187.5830 - mse: 32421220.0000 - mae: 2187.5830 - val_loss: 1768.2877 - val_mse: 25305854.0000 - val_mae: 1768.2877\n",
            "Epoch 482/500\n",
            "380/380 [==============================] - 0s 129us/step - loss: 2178.2017 - mse: 32061432.0000 - mae: 2178.2017 - val_loss: 1768.0874 - val_mse: 25285222.0000 - val_mae: 1768.0875\n",
            "Epoch 483/500\n",
            "380/380 [==============================] - 0s 142us/step - loss: 2192.2623 - mse: 31934846.0000 - mae: 2192.2625 - val_loss: 1772.4453 - val_mse: 25283460.0000 - val_mae: 1772.4453\n",
            "Epoch 484/500\n",
            "380/380 [==============================] - 0s 113us/step - loss: 2179.1956 - mse: 32004602.0000 - mae: 2179.1956 - val_loss: 1768.9540 - val_mse: 25242954.0000 - val_mae: 1768.9539\n",
            "Epoch 485/500\n",
            "380/380 [==============================] - 0s 124us/step - loss: 2152.1489 - mse: 31747012.0000 - mae: 2152.1489 - val_loss: 1767.0397 - val_mse: 25208974.0000 - val_mae: 1767.0397\n",
            "Epoch 486/500\n",
            "380/380 [==============================] - 0s 108us/step - loss: 2171.2688 - mse: 31927140.0000 - mae: 2171.2688 - val_loss: 1771.5316 - val_mse: 25210144.0000 - val_mae: 1771.5317\n",
            "Epoch 487/500\n",
            "380/380 [==============================] - 0s 146us/step - loss: 2163.7424 - mse: 32081316.0000 - mae: 2163.7424 - val_loss: 1767.7592 - val_mse: 25173780.0000 - val_mae: 1767.7592\n",
            "Epoch 488/500\n",
            "380/380 [==============================] - 0s 119us/step - loss: 2171.6214 - mse: 31955810.0000 - mae: 2171.6213 - val_loss: 1766.0467 - val_mse: 25144416.0000 - val_mae: 1766.0466\n",
            "Epoch 489/500\n",
            "380/380 [==============================] - 0s 100us/step - loss: 2176.8817 - mse: 32004126.0000 - mae: 2176.8818 - val_loss: 1766.0107 - val_mse: 25117514.0000 - val_mae: 1766.0106\n",
            "Epoch 490/500\n",
            "380/380 [==============================] - 0s 121us/step - loss: 2158.2004 - mse: 31696530.0000 - mae: 2158.2004 - val_loss: 1774.7146 - val_mse: 25133144.0000 - val_mae: 1774.7145\n",
            "Epoch 491/500\n",
            "380/380 [==============================] - 0s 110us/step - loss: 2173.4677 - mse: 31690366.0000 - mae: 2173.4678 - val_loss: 1766.9567 - val_mse: 25086716.0000 - val_mae: 1766.9568\n",
            "Epoch 492/500\n",
            "380/380 [==============================] - 0s 107us/step - loss: 2132.7767 - mse: 31645132.0000 - mae: 2132.7766 - val_loss: 1763.2207 - val_mse: 25041324.0000 - val_mae: 1763.2208\n",
            "Epoch 493/500\n",
            "380/380 [==============================] - 0s 102us/step - loss: 2162.2057 - mse: 31743124.0000 - mae: 2162.2056 - val_loss: 1766.1014 - val_mse: 25037300.0000 - val_mae: 1766.1016\n",
            "Epoch 494/500\n",
            "380/380 [==============================] - 0s 105us/step - loss: 2145.1160 - mse: 31632524.0000 - mae: 2145.1162 - val_loss: 1776.1406 - val_mse: 25058270.0000 - val_mae: 1776.1405\n",
            "Epoch 495/500\n",
            "380/380 [==============================] - 0s 103us/step - loss: 2168.3629 - mse: 31677066.0000 - mae: 2168.3628 - val_loss: 1767.4213 - val_mse: 24997902.0000 - val_mae: 1767.4213\n",
            "Epoch 496/500\n",
            "380/380 [==============================] - 0s 132us/step - loss: 2147.3823 - mse: 31468534.0000 - mae: 2147.3823 - val_loss: 1780.6792 - val_mse: 25022098.0000 - val_mae: 1780.6792\n",
            "Epoch 497/500\n",
            "380/380 [==============================] - 0s 120us/step - loss: 2153.1028 - mse: 31350554.0000 - mae: 2153.1025 - val_loss: 1774.1942 - val_mse: 24976126.0000 - val_mae: 1774.1942\n",
            "Epoch 498/500\n",
            "380/380 [==============================] - 0s 135us/step - loss: 2153.4251 - mse: 31478360.0000 - mae: 2153.4250 - val_loss: 1768.6183 - val_mse: 24931946.0000 - val_mae: 1768.6183\n",
            "Epoch 499/500\n",
            "380/380 [==============================] - 0s 106us/step - loss: 2142.0890 - mse: 31696440.0000 - mae: 2142.0889 - val_loss: 1769.7887 - val_mse: 24916810.0000 - val_mae: 1769.7887\n",
            "Epoch 500/500\n",
            "380/380 [==============================] - 0s 120us/step - loss: 2127.2591 - mse: 31268622.0000 - mae: 2127.2593 - val_loss: 1770.6304 - val_mse: 24898164.0000 - val_mae: 1770.6304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 750 samples, validate on 750 samples\n",
            "Epoch 1/500\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 2424.7415 - mse: 31068046.0000 - mae: 2424.7415 - val_loss: 2618.7375 - val_mse: 37074252.0000 - val_mae: 2618.7375\n",
            "Epoch 2/500\n",
            "750/750 [==============================] - 0s 67us/step - loss: 2424.7091 - mse: 31067884.0000 - mae: 2424.7092 - val_loss: 2618.7100 - val_mse: 37074104.0000 - val_mae: 2618.7100\n",
            "Epoch 3/500\n",
            "750/750 [==============================] - 0s 63us/step - loss: 2424.6800 - mse: 31067744.0000 - mae: 2424.6802 - val_loss: 2618.6679 - val_mse: 37073896.0000 - val_mae: 2618.6677\n",
            "Epoch 4/500\n",
            "750/750 [==============================] - 0s 59us/step - loss: 2424.6188 - mse: 31067456.0000 - mae: 2424.6189 - val_loss: 2618.5103 - val_mse: 37073184.0000 - val_mae: 2618.5103\n",
            "Epoch 5/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2424.3164 - mse: 31066014.0000 - mae: 2424.3164 - val_loss: 2617.5102 - val_mse: 37068848.0000 - val_mae: 2617.5103\n",
            "Epoch 6/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2422.7000 - mse: 31058946.0000 - mae: 2422.7000 - val_loss: 2613.8130 - val_mse: 37052188.0000 - val_mae: 2613.8130\n",
            "Epoch 7/500\n",
            "750/750 [==============================] - 0s 67us/step - loss: 2418.2027 - mse: 31038352.0000 - mae: 2418.2026 - val_loss: 2607.6422 - val_mse: 37023280.0000 - val_mae: 2607.6421\n",
            "Epoch 8/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 2412.5474 - mse: 31012746.0000 - mae: 2412.5474 - val_loss: 2601.1156 - val_mse: 36991472.0000 - val_mae: 2601.1157\n",
            "Epoch 9/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2405.8530 - mse: 30978796.0000 - mae: 2405.8530 - val_loss: 2594.1843 - val_mse: 36956556.0000 - val_mae: 2594.1843\n",
            "Epoch 10/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2397.6316 - mse: 30942418.0000 - mae: 2397.6316 - val_loss: 2586.6116 - val_mse: 36917872.0000 - val_mae: 2586.6116\n",
            "Epoch 11/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2390.8132 - mse: 30902854.0000 - mae: 2390.8130 - val_loss: 2578.8986 - val_mse: 36877384.0000 - val_mae: 2578.8984\n",
            "Epoch 12/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2383.2820 - mse: 30862032.0000 - mae: 2383.2820 - val_loss: 2570.7995 - val_mse: 36832764.0000 - val_mae: 2570.7996\n",
            "Epoch 13/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2376.0800 - mse: 30825906.0000 - mae: 2376.0801 - val_loss: 2562.7834 - val_mse: 36785160.0000 - val_mae: 2562.7834\n",
            "Epoch 14/500\n",
            "750/750 [==============================] - 0s 59us/step - loss: 2364.9820 - mse: 30766224.0000 - mae: 2364.9819 - val_loss: 2554.1198 - val_mse: 36732600.0000 - val_mae: 2554.1199\n",
            "Epoch 15/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2356.7030 - mse: 30717296.0000 - mae: 2356.7031 - val_loss: 2545.3688 - val_mse: 36677852.0000 - val_mae: 2545.3687\n",
            "Epoch 16/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2348.5086 - mse: 30668712.0000 - mae: 2348.5085 - val_loss: 2536.2834 - val_mse: 36619604.0000 - val_mae: 2536.2834\n",
            "Epoch 17/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2339.5995 - mse: 30617646.0000 - mae: 2339.5996 - val_loss: 2527.1936 - val_mse: 36560332.0000 - val_mae: 2527.1936\n",
            "Epoch 18/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 2331.7472 - mse: 30566640.0000 - mae: 2331.7471 - val_loss: 2517.8668 - val_mse: 36498608.0000 - val_mae: 2517.8669\n",
            "Epoch 19/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2321.3102 - mse: 30494932.0000 - mae: 2321.3101 - val_loss: 2508.2673 - val_mse: 36434560.0000 - val_mae: 2508.2673\n",
            "Epoch 20/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 2315.0736 - mse: 30445760.0000 - mae: 2315.0737 - val_loss: 2498.7987 - val_mse: 36369832.0000 - val_mae: 2498.7988\n",
            "Epoch 21/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2304.1532 - mse: 30373344.0000 - mae: 2304.1531 - val_loss: 2489.5923 - val_mse: 36304628.0000 - val_mae: 2489.5923\n",
            "Epoch 22/500\n",
            "750/750 [==============================] - 0s 58us/step - loss: 2290.4399 - mse: 30303880.0000 - mae: 2290.4399 - val_loss: 2480.2395 - val_mse: 36235972.0000 - val_mae: 2480.2395\n",
            "Epoch 23/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2286.9706 - mse: 30278150.0000 - mae: 2286.9707 - val_loss: 2471.7520 - val_mse: 36170612.0000 - val_mae: 2471.7520\n",
            "Epoch 24/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 2278.1212 - mse: 30200244.0000 - mae: 2278.1211 - val_loss: 2463.8184 - val_mse: 36104860.0000 - val_mae: 2463.8186\n",
            "Epoch 25/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2270.4736 - mse: 30112150.0000 - mae: 2270.4736 - val_loss: 2456.0261 - val_mse: 36036676.0000 - val_mae: 2456.0264\n",
            "Epoch 26/500\n",
            "750/750 [==============================] - 0s 61us/step - loss: 2264.9180 - mse: 30088888.0000 - mae: 2264.9180 - val_loss: 2448.8159 - val_mse: 35971592.0000 - val_mae: 2448.8159\n",
            "Epoch 27/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2258.1474 - mse: 30001474.0000 - mae: 2258.1472 - val_loss: 2441.6672 - val_mse: 35906952.0000 - val_mae: 2441.6672\n",
            "Epoch 28/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 2255.2153 - mse: 29987690.0000 - mae: 2255.2153 - val_loss: 2435.0192 - val_mse: 35846128.0000 - val_mae: 2435.0193\n",
            "Epoch 29/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2248.0793 - mse: 29935752.0000 - mae: 2248.0796 - val_loss: 2428.3293 - val_mse: 35783988.0000 - val_mae: 2428.3296\n",
            "Epoch 30/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2243.0234 - mse: 29839202.0000 - mae: 2243.0234 - val_loss: 2422.3315 - val_mse: 35724844.0000 - val_mae: 2422.3315\n",
            "Epoch 31/500\n",
            "750/750 [==============================] - 0s 63us/step - loss: 2243.0010 - mse: 29856486.0000 - mae: 2243.0010 - val_loss: 2417.2252 - val_mse: 35670028.0000 - val_mae: 2417.2251\n",
            "Epoch 32/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2238.6069 - mse: 29764308.0000 - mae: 2238.6069 - val_loss: 2412.8612 - val_mse: 35620968.0000 - val_mae: 2412.8611\n",
            "Epoch 33/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2234.5262 - mse: 29742798.0000 - mae: 2234.5261 - val_loss: 2408.8686 - val_mse: 35574376.0000 - val_mae: 2408.8687\n",
            "Epoch 34/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2223.8929 - mse: 29636772.0000 - mae: 2223.8928 - val_loss: 2404.2282 - val_mse: 35517540.0000 - val_mae: 2404.2283\n",
            "Epoch 35/500\n",
            "750/750 [==============================] - 0s 60us/step - loss: 2226.3045 - mse: 29640414.0000 - mae: 2226.3047 - val_loss: 2400.3998 - val_mse: 35468852.0000 - val_mae: 2400.3997\n",
            "Epoch 36/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2222.2995 - mse: 29647092.0000 - mae: 2222.2996 - val_loss: 2397.1194 - val_mse: 35425024.0000 - val_mae: 2397.1194\n",
            "Epoch 37/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2225.2987 - mse: 29561422.0000 - mae: 2225.2986 - val_loss: 2393.7857 - val_mse: 35378836.0000 - val_mae: 2393.7856\n",
            "Epoch 38/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2218.2253 - mse: 29488300.0000 - mae: 2218.2253 - val_loss: 2390.5295 - val_mse: 35332748.0000 - val_mae: 2390.5298\n",
            "Epoch 39/500\n",
            "750/750 [==============================] - 0s 67us/step - loss: 2215.6401 - mse: 29484710.0000 - mae: 2215.6399 - val_loss: 2387.4681 - val_mse: 35288220.0000 - val_mae: 2387.4683\n",
            "Epoch 40/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2211.9540 - mse: 29435336.0000 - mae: 2211.9541 - val_loss: 2384.6501 - val_mse: 35246480.0000 - val_mae: 2384.6501\n",
            "Epoch 41/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2216.6820 - mse: 29434100.0000 - mae: 2216.6819 - val_loss: 2381.8482 - val_mse: 35203932.0000 - val_mae: 2381.8481\n",
            "Epoch 42/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2212.9866 - mse: 29404272.0000 - mae: 2212.9866 - val_loss: 2379.3575 - val_mse: 35165348.0000 - val_mae: 2379.3574\n",
            "Epoch 43/500\n",
            "750/750 [==============================] - 0s 61us/step - loss: 2208.8182 - mse: 29289758.0000 - mae: 2208.8181 - val_loss: 2376.9130 - val_mse: 35126240.0000 - val_mae: 2376.9131\n",
            "Epoch 44/500\n",
            "750/750 [==============================] - 0s 59us/step - loss: 2202.4941 - mse: 29304330.0000 - mae: 2202.4939 - val_loss: 2374.4722 - val_mse: 35085708.0000 - val_mae: 2374.4722\n",
            "Epoch 45/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2203.2447 - mse: 29271676.0000 - mae: 2203.2446 - val_loss: 2372.0699 - val_mse: 35044684.0000 - val_mae: 2372.0698\n",
            "Epoch 46/500\n",
            "750/750 [==============================] - 0s 75us/step - loss: 2205.9620 - mse: 29244058.0000 - mae: 2205.9619 - val_loss: 2369.7129 - val_mse: 35001340.0000 - val_mae: 2369.7129\n",
            "Epoch 47/500\n",
            "750/750 [==============================] - 0s 67us/step - loss: 2202.2464 - mse: 29135832.0000 - mae: 2202.2463 - val_loss: 2367.3641 - val_mse: 34957480.0000 - val_mae: 2367.3640\n",
            "Epoch 48/500\n",
            "750/750 [==============================] - 0s 58us/step - loss: 2194.8490 - mse: 29110652.0000 - mae: 2194.8491 - val_loss: 2364.6948 - val_mse: 34907452.0000 - val_mae: 2364.6948\n",
            "Epoch 49/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2194.0022 - mse: 29078100.0000 - mae: 2194.0022 - val_loss: 2362.2479 - val_mse: 34860956.0000 - val_mae: 2362.2478\n",
            "Epoch 50/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2199.6083 - mse: 28981272.0000 - mae: 2199.6084 - val_loss: 2359.8922 - val_mse: 34816204.0000 - val_mae: 2359.8923\n",
            "Epoch 51/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 2184.5007 - mse: 28857262.0000 - mae: 2184.5007 - val_loss: 2357.3323 - val_mse: 34766092.0000 - val_mae: 2357.3325\n",
            "Epoch 52/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2189.4723 - mse: 28907510.0000 - mae: 2189.4724 - val_loss: 2355.0168 - val_mse: 34719680.0000 - val_mae: 2355.0166\n",
            "Epoch 53/500\n",
            "750/750 [==============================] - 0s 61us/step - loss: 2188.6219 - mse: 28920632.0000 - mae: 2188.6218 - val_loss: 2352.8668 - val_mse: 34675572.0000 - val_mae: 2352.8667\n",
            "Epoch 54/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2187.0348 - mse: 28941392.0000 - mae: 2187.0349 - val_loss: 2350.4905 - val_mse: 34625680.0000 - val_mae: 2350.4905\n",
            "Epoch 55/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 2185.3532 - mse: 28705396.0000 - mae: 2185.3533 - val_loss: 2348.4199 - val_mse: 34584904.0000 - val_mae: 2348.4199\n",
            "Epoch 56/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2180.2143 - mse: 28719246.0000 - mae: 2180.2144 - val_loss: 2345.7339 - val_mse: 34531192.0000 - val_mae: 2345.7339\n",
            "Epoch 57/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 2175.9530 - mse: 28649224.0000 - mae: 2175.9529 - val_loss: 2343.1363 - val_mse: 34480728.0000 - val_mae: 2343.1362\n",
            "Epoch 58/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2175.3322 - mse: 28666828.0000 - mae: 2175.3323 - val_loss: 2340.8788 - val_mse: 34439228.0000 - val_mae: 2340.8787\n",
            "Epoch 59/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 2170.0310 - mse: 28575066.0000 - mae: 2170.0310 - val_loss: 2338.5425 - val_mse: 34396896.0000 - val_mae: 2338.5427\n",
            "Epoch 60/500\n",
            "750/750 [==============================] - 0s 59us/step - loss: 2167.4863 - mse: 28433824.0000 - mae: 2167.4863 - val_loss: 2336.3500 - val_mse: 34358628.0000 - val_mae: 2336.3501\n",
            "Epoch 61/500\n",
            "750/750 [==============================] - 0s 63us/step - loss: 2177.6638 - mse: 28478204.0000 - mae: 2177.6641 - val_loss: 2334.4391 - val_mse: 34325092.0000 - val_mae: 2334.4390\n",
            "Epoch 62/500\n",
            "750/750 [==============================] - 0s 59us/step - loss: 2171.2740 - mse: 28467542.0000 - mae: 2171.2739 - val_loss: 2332.6473 - val_mse: 34291932.0000 - val_mae: 2332.6472\n",
            "Epoch 63/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2173.6043 - mse: 28607774.0000 - mae: 2173.6042 - val_loss: 2331.2010 - val_mse: 34268024.0000 - val_mae: 2331.2009\n",
            "Epoch 64/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2177.0427 - mse: 28450638.0000 - mae: 2177.0427 - val_loss: 2329.4026 - val_mse: 34232516.0000 - val_mae: 2329.4026\n",
            "Epoch 65/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2174.2793 - mse: 28419978.0000 - mae: 2174.2793 - val_loss: 2327.9843 - val_mse: 34207088.0000 - val_mae: 2327.9844\n",
            "Epoch 66/500\n",
            "750/750 [==============================] - 0s 60us/step - loss: 2165.8726 - mse: 28354380.0000 - mae: 2165.8726 - val_loss: 2326.6520 - val_mse: 34177528.0000 - val_mae: 2326.6521\n",
            "Epoch 67/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2163.0245 - mse: 28315436.0000 - mae: 2163.0244 - val_loss: 2325.0333 - val_mse: 34145464.0000 - val_mae: 2325.0332\n",
            "Epoch 68/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2170.9993 - mse: 28262368.0000 - mae: 2170.9993 - val_loss: 2323.8138 - val_mse: 34121084.0000 - val_mae: 2323.8137\n",
            "Epoch 69/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2166.1132 - mse: 28330634.0000 - mae: 2166.1133 - val_loss: 2322.5155 - val_mse: 34095948.0000 - val_mae: 2322.5154\n",
            "Epoch 70/500\n",
            "750/750 [==============================] - 0s 59us/step - loss: 2159.5086 - mse: 28355668.0000 - mae: 2159.5085 - val_loss: 2321.2881 - val_mse: 34072672.0000 - val_mae: 2321.2881\n",
            "Epoch 71/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2155.6557 - mse: 28234400.0000 - mae: 2155.6558 - val_loss: 2320.0290 - val_mse: 34038724.0000 - val_mae: 2320.0291\n",
            "Epoch 72/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 2164.3944 - mse: 28304294.0000 - mae: 2164.3943 - val_loss: 2319.0018 - val_mse: 34015332.0000 - val_mae: 2319.0020\n",
            "Epoch 73/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2154.8250 - mse: 28180808.0000 - mae: 2154.8252 - val_loss: 2317.9032 - val_mse: 33988552.0000 - val_mae: 2317.9031\n",
            "Epoch 74/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2168.1644 - mse: 28300264.0000 - mae: 2168.1643 - val_loss: 2316.9208 - val_mse: 33968724.0000 - val_mae: 2316.9209\n",
            "Epoch 75/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 2169.4382 - mse: 28216214.0000 - mae: 2169.4382 - val_loss: 2316.0820 - val_mse: 33952136.0000 - val_mae: 2316.0820\n",
            "Epoch 76/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 2155.3788 - mse: 28176786.0000 - mae: 2155.3787 - val_loss: 2314.9683 - val_mse: 33930840.0000 - val_mae: 2314.9683\n",
            "Epoch 77/500\n",
            "750/750 [==============================] - 0s 63us/step - loss: 2136.6230 - mse: 28180896.0000 - mae: 2136.6230 - val_loss: 2313.6995 - val_mse: 33901416.0000 - val_mae: 2313.6995\n",
            "Epoch 78/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2160.7316 - mse: 28254066.0000 - mae: 2160.7317 - val_loss: 2312.4991 - val_mse: 33895504.0000 - val_mae: 2312.4993\n",
            "Epoch 79/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2157.9004 - mse: 28151884.0000 - mae: 2157.9004 - val_loss: 2311.1820 - val_mse: 33881304.0000 - val_mae: 2311.1819\n",
            "Epoch 80/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2150.2583 - mse: 28059762.0000 - mae: 2150.2583 - val_loss: 2308.9612 - val_mse: 33881076.0000 - val_mae: 2308.9614\n",
            "Epoch 81/500\n",
            "750/750 [==============================] - 0s 67us/step - loss: 2150.2176 - mse: 28231478.0000 - mae: 2150.2178 - val_loss: 2306.8478 - val_mse: 33875572.0000 - val_mae: 2306.8477\n",
            "Epoch 82/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2140.0839 - mse: 28173358.0000 - mae: 2140.0840 - val_loss: 2305.0069 - val_mse: 33869616.0000 - val_mae: 2305.0068\n",
            "Epoch 83/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2150.4269 - mse: 28084134.0000 - mae: 2150.4270 - val_loss: 2303.1344 - val_mse: 33861180.0000 - val_mae: 2303.1343\n",
            "Epoch 84/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2139.7801 - mse: 28204576.0000 - mae: 2139.7803 - val_loss: 2300.9686 - val_mse: 33846816.0000 - val_mae: 2300.9688\n",
            "Epoch 85/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2149.2210 - mse: 28249968.0000 - mae: 2149.2209 - val_loss: 2298.3141 - val_mse: 33856032.0000 - val_mae: 2298.3140\n",
            "Epoch 86/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2137.3833 - mse: 28145528.0000 - mae: 2137.3833 - val_loss: 2294.9507 - val_mse: 33883552.0000 - val_mae: 2294.9509\n",
            "Epoch 87/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 2129.2915 - mse: 28246494.0000 - mae: 2129.2915 - val_loss: 2291.7541 - val_mse: 33878464.0000 - val_mae: 2291.7539\n",
            "Epoch 88/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2138.2375 - mse: 28262402.0000 - mae: 2138.2375 - val_loss: 2289.0369 - val_mse: 33915428.0000 - val_mae: 2289.0369\n",
            "Epoch 89/500\n",
            "750/750 [==============================] - 0s 64us/step - loss: 2128.4236 - mse: 28313554.0000 - mae: 2128.4236 - val_loss: 2286.9155 - val_mse: 33995784.0000 - val_mae: 2286.9158\n",
            "Epoch 90/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2113.4859 - mse: 28290512.0000 - mae: 2113.4861 - val_loss: 2283.7474 - val_mse: 33945756.0000 - val_mae: 2283.7476\n",
            "Epoch 91/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2122.7607 - mse: 28281564.0000 - mae: 2122.7607 - val_loss: 2283.1278 - val_mse: 34005064.0000 - val_mae: 2283.1279\n",
            "Epoch 92/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2111.1465 - mse: 28311566.0000 - mae: 2111.1465 - val_loss: 2277.2980 - val_mse: 33771096.0000 - val_mae: 2277.2981\n",
            "Epoch 93/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2098.5919 - mse: 28141186.0000 - mae: 2098.5920 - val_loss: 2274.2695 - val_mse: 33760896.0000 - val_mae: 2274.2693\n",
            "Epoch 94/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2113.7312 - mse: 28145838.0000 - mae: 2113.7314 - val_loss: 2272.5247 - val_mse: 33818352.0000 - val_mae: 2272.5247\n",
            "Epoch 95/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 2105.8810 - mse: 28092758.0000 - mae: 2105.8811 - val_loss: 2268.8182 - val_mse: 33701812.0000 - val_mae: 2268.8181\n",
            "Epoch 96/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2104.1017 - mse: 28197766.0000 - mae: 2104.1016 - val_loss: 2266.3412 - val_mse: 33529716.0000 - val_mae: 2266.3413\n",
            "Epoch 97/500\n",
            "750/750 [==============================] - 0s 59us/step - loss: 2100.7027 - mse: 27895090.0000 - mae: 2100.7026 - val_loss: 2263.0363 - val_mse: 33601120.0000 - val_mae: 2263.0364\n",
            "Epoch 98/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2093.6593 - mse: 28012396.0000 - mae: 2093.6594 - val_loss: 2261.2173 - val_mse: 33605884.0000 - val_mae: 2261.2173\n",
            "Epoch 99/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2085.5419 - mse: 28037332.0000 - mae: 2085.5420 - val_loss: 2258.5547 - val_mse: 33450420.0000 - val_mae: 2258.5549\n",
            "Epoch 100/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2098.1242 - mse: 27745850.0000 - mae: 2098.1243 - val_loss: 2257.0952 - val_mse: 33582260.0000 - val_mae: 2257.0952\n",
            "Epoch 101/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 2082.3062 - mse: 27931088.0000 - mae: 2082.3064 - val_loss: 2254.0058 - val_mse: 33451576.0000 - val_mae: 2254.0056\n",
            "Epoch 102/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2085.5372 - mse: 27896860.0000 - mae: 2085.5374 - val_loss: 2251.8579 - val_mse: 33476446.0000 - val_mae: 2251.8579\n",
            "Epoch 103/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2076.3055 - mse: 27916096.0000 - mae: 2076.3054 - val_loss: 2249.3255 - val_mse: 33390606.0000 - val_mae: 2249.3254\n",
            "Epoch 104/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2076.3456 - mse: 27744240.0000 - mae: 2076.3457 - val_loss: 2249.9373 - val_mse: 33510458.0000 - val_mae: 2249.9373\n",
            "Epoch 105/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2078.0780 - mse: 27772492.0000 - mae: 2078.0779 - val_loss: 2245.0329 - val_mse: 33280734.0000 - val_mae: 2245.0330\n",
            "Epoch 106/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 2072.7919 - mse: 27654614.0000 - mae: 2072.7917 - val_loss: 2243.0557 - val_mse: 33267068.0000 - val_mae: 2243.0557\n",
            "Epoch 107/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2063.5724 - mse: 27623536.0000 - mae: 2063.5723 - val_loss: 2241.5058 - val_mse: 33315394.0000 - val_mae: 2241.5059\n",
            "Epoch 108/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2079.6047 - mse: 27755586.0000 - mae: 2079.6047 - val_loss: 2241.1257 - val_mse: 33351924.0000 - val_mae: 2241.1257\n",
            "Epoch 109/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2058.9189 - mse: 27563344.0000 - mae: 2058.9189 - val_loss: 2237.2602 - val_mse: 33211518.0000 - val_mae: 2237.2603\n",
            "Epoch 110/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2072.5297 - mse: 27472338.0000 - mae: 2072.5298 - val_loss: 2242.0786 - val_mse: 33377580.0000 - val_mae: 2242.0786\n",
            "Epoch 111/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2071.2032 - mse: 27634352.0000 - mae: 2071.2031 - val_loss: 2237.1097 - val_mse: 33267134.0000 - val_mae: 2237.1096\n",
            "Epoch 112/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2067.8674 - mse: 27743394.0000 - mae: 2067.8674 - val_loss: 2232.8904 - val_mse: 33155974.0000 - val_mae: 2232.8904\n",
            "Epoch 113/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 2062.6108 - mse: 27514422.0000 - mae: 2062.6108 - val_loss: 2230.4177 - val_mse: 33043914.0000 - val_mae: 2230.4177\n",
            "Epoch 114/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 2036.1860 - mse: 27358536.0000 - mae: 2036.1860 - val_loss: 2228.6590 - val_mse: 32919116.0000 - val_mae: 2228.6589\n",
            "Epoch 115/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2051.1392 - mse: 27421238.0000 - mae: 2051.1392 - val_loss: 2228.0993 - val_mse: 32834074.0000 - val_mae: 2228.0994\n",
            "Epoch 116/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2052.3367 - mse: 27448310.0000 - mae: 2052.3367 - val_loss: 2226.0516 - val_mse: 32987928.0000 - val_mae: 2226.0515\n",
            "Epoch 117/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 2062.6064 - mse: 27565086.0000 - mae: 2062.6064 - val_loss: 2226.0504 - val_mse: 32726082.0000 - val_mae: 2226.0503\n",
            "Epoch 118/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 2051.9808 - mse: 27269536.0000 - mae: 2051.9810 - val_loss: 2222.9738 - val_mse: 32803578.0000 - val_mae: 2222.9739\n",
            "Epoch 119/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2051.0872 - mse: 27397496.0000 - mae: 2051.0874 - val_loss: 2223.1830 - val_mse: 32689072.0000 - val_mae: 2223.1831\n",
            "Epoch 120/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2045.8297 - mse: 27303728.0000 - mae: 2045.8297 - val_loss: 2222.7411 - val_mse: 32905120.0000 - val_mae: 2222.7410\n",
            "Epoch 121/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2036.6934 - mse: 27187670.0000 - mae: 2036.6934 - val_loss: 2219.4455 - val_mse: 32785724.0000 - val_mae: 2219.4456\n",
            "Epoch 122/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2039.5649 - mse: 27024532.0000 - mae: 2039.5648 - val_loss: 2219.6068 - val_mse: 32576066.0000 - val_mae: 2219.6069\n",
            "Epoch 123/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 2038.7233 - mse: 27324648.0000 - mae: 2038.7234 - val_loss: 2217.7504 - val_mse: 32765474.0000 - val_mae: 2217.7502\n",
            "Epoch 124/500\n",
            "750/750 [==============================] - 0s 62us/step - loss: 2033.6336 - mse: 26993826.0000 - mae: 2033.6337 - val_loss: 2217.0894 - val_mse: 32706498.0000 - val_mae: 2217.0896\n",
            "Epoch 125/500\n",
            "750/750 [==============================] - 0s 67us/step - loss: 2024.7017 - mse: 27105068.0000 - mae: 2024.7017 - val_loss: 2215.9691 - val_mse: 32625686.0000 - val_mae: 2215.9692\n",
            "Epoch 126/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 2042.5922 - mse: 27025656.0000 - mae: 2042.5922 - val_loss: 2215.7310 - val_mse: 32683332.0000 - val_mae: 2215.7310\n",
            "Epoch 127/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2021.3962 - mse: 27037308.0000 - mae: 2021.3961 - val_loss: 2215.4862 - val_mse: 32465014.0000 - val_mae: 2215.4861\n",
            "Epoch 128/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2022.6748 - mse: 26832004.0000 - mae: 2022.6748 - val_loss: 2216.1146 - val_mse: 32685626.0000 - val_mae: 2216.1145\n",
            "Epoch 129/500\n",
            "750/750 [==============================] - 0s 69us/step - loss: 2021.4074 - mse: 27185338.0000 - mae: 2021.4073 - val_loss: 2212.6098 - val_mse: 32492334.0000 - val_mae: 2212.6099\n",
            "Epoch 130/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 2035.7449 - mse: 27114920.0000 - mae: 2035.7450 - val_loss: 2212.3931 - val_mse: 32582180.0000 - val_mae: 2212.3931\n",
            "Epoch 131/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2017.7196 - mse: 27079234.0000 - mae: 2017.7197 - val_loss: 2214.9523 - val_mse: 32259296.0000 - val_mae: 2214.9524\n",
            "Epoch 132/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2028.3540 - mse: 26871066.0000 - mae: 2028.3540 - val_loss: 2211.4972 - val_mse: 32447758.0000 - val_mae: 2211.4971\n",
            "Epoch 133/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2023.8309 - mse: 26900788.0000 - mae: 2023.8311 - val_loss: 2211.7264 - val_mse: 32336940.0000 - val_mae: 2211.7263\n",
            "Epoch 134/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2030.0618 - mse: 27010466.0000 - mae: 2030.0619 - val_loss: 2211.0792 - val_mse: 32501102.0000 - val_mae: 2211.0791\n",
            "Epoch 135/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2032.6401 - mse: 26924718.0000 - mae: 2032.6400 - val_loss: 2210.7180 - val_mse: 32321932.0000 - val_mae: 2210.7180\n",
            "Epoch 136/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2031.2773 - mse: 26715138.0000 - mae: 2031.2773 - val_loss: 2210.1108 - val_mse: 32395516.0000 - val_mae: 2210.1108\n",
            "Epoch 137/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2024.8576 - mse: 26757628.0000 - mae: 2024.8577 - val_loss: 2220.1453 - val_mse: 32663890.0000 - val_mae: 2220.1453\n",
            "Epoch 138/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2031.0609 - mse: 26959556.0000 - mae: 2031.0610 - val_loss: 2208.8752 - val_mse: 32399518.0000 - val_mae: 2208.8752\n",
            "Epoch 139/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 2014.6034 - mse: 26567274.0000 - mae: 2014.6034 - val_loss: 2209.8644 - val_mse: 32461898.0000 - val_mae: 2209.8643\n",
            "Epoch 140/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2017.5780 - mse: 26834152.0000 - mae: 2017.5780 - val_loss: 2210.3403 - val_mse: 32461882.0000 - val_mae: 2210.3403\n",
            "Epoch 141/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2027.4804 - mse: 26963984.0000 - mae: 2027.4803 - val_loss: 2209.0086 - val_mse: 32422904.0000 - val_mae: 2209.0085\n",
            "Epoch 142/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 2013.2449 - mse: 26852014.0000 - mae: 2013.2450 - val_loss: 2219.4860 - val_mse: 32588746.0000 - val_mae: 2219.4861\n",
            "Epoch 143/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 2014.5920 - mse: 26789954.0000 - mae: 2014.5920 - val_loss: 2207.0740 - val_mse: 32301788.0000 - val_mae: 2207.0740\n",
            "Epoch 144/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2013.4893 - mse: 26661828.0000 - mae: 2013.4894 - val_loss: 2223.9497 - val_mse: 32610806.0000 - val_mae: 2223.9497\n",
            "Epoch 145/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 2019.0015 - mse: 26618826.0000 - mae: 2019.0017 - val_loss: 2226.4745 - val_mse: 32624070.0000 - val_mae: 2226.4744\n",
            "Epoch 146/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2022.7751 - mse: 26920304.0000 - mae: 2022.7750 - val_loss: 2216.9449 - val_mse: 32513892.0000 - val_mae: 2216.9448\n",
            "Epoch 147/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2011.6051 - mse: 26671256.0000 - mae: 2011.6050 - val_loss: 2206.7754 - val_mse: 32320804.0000 - val_mae: 2206.7754\n",
            "Epoch 148/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 2018.6658 - mse: 26636326.0000 - mae: 2018.6659 - val_loss: 2206.5971 - val_mse: 32274732.0000 - val_mae: 2206.5972\n",
            "Epoch 149/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 2020.2199 - mse: 26570062.0000 - mae: 2020.2200 - val_loss: 2208.3258 - val_mse: 32352816.0000 - val_mae: 2208.3259\n",
            "Epoch 150/500\n",
            "750/750 [==============================] - 0s 75us/step - loss: 2010.4115 - mse: 26734422.0000 - mae: 2010.4115 - val_loss: 2214.4669 - val_mse: 32453510.0000 - val_mae: 2214.4668\n",
            "Epoch 151/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 2005.2985 - mse: 26636622.0000 - mae: 2005.2987 - val_loss: 2206.0895 - val_mse: 32182412.0000 - val_mae: 2206.0896\n",
            "Epoch 152/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 2006.8222 - mse: 26441106.0000 - mae: 2006.8221 - val_loss: 2211.3662 - val_mse: 32366728.0000 - val_mae: 2211.3662\n",
            "Epoch 153/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 2017.6948 - mse: 26616568.0000 - mae: 2017.6948 - val_loss: 2209.2189 - val_mse: 32308582.0000 - val_mae: 2209.2188\n",
            "Epoch 154/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 2013.5194 - mse: 26694408.0000 - mae: 2013.5193 - val_loss: 2208.7890 - val_mse: 32314458.0000 - val_mae: 2208.7891\n",
            "Epoch 155/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 2017.1944 - mse: 26745562.0000 - mae: 2017.1945 - val_loss: 2206.0523 - val_mse: 32260826.0000 - val_mae: 2206.0522\n",
            "Epoch 156/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2011.2639 - mse: 26556006.0000 - mae: 2011.2638 - val_loss: 2205.8746 - val_mse: 32184352.0000 - val_mae: 2205.8745\n",
            "Epoch 157/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 2010.8442 - mse: 26672798.0000 - mae: 2010.8441 - val_loss: 2207.3926 - val_mse: 32247806.0000 - val_mae: 2207.3926\n",
            "Epoch 158/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 2010.2313 - mse: 26334468.0000 - mae: 2010.2313 - val_loss: 2209.7874 - val_mse: 32280188.0000 - val_mae: 2209.7874\n",
            "Epoch 159/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2011.7478 - mse: 26500020.0000 - mae: 2011.7478 - val_loss: 2206.3515 - val_mse: 32101862.0000 - val_mae: 2206.3516\n",
            "Epoch 160/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 2021.4459 - mse: 26537798.0000 - mae: 2021.4460 - val_loss: 2207.6755 - val_mse: 32184774.0000 - val_mae: 2207.6755\n",
            "Epoch 161/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2005.8773 - mse: 26405440.0000 - mae: 2005.8773 - val_loss: 2207.6804 - val_mse: 32198022.0000 - val_mae: 2207.6804\n",
            "Epoch 162/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 2005.3367 - mse: 26504902.0000 - mae: 2005.3367 - val_loss: 2210.3959 - val_mse: 32265722.0000 - val_mae: 2210.3960\n",
            "Epoch 163/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2015.9082 - mse: 26432810.0000 - mae: 2015.9083 - val_loss: 2214.3580 - val_mse: 32325634.0000 - val_mae: 2214.3579\n",
            "Epoch 164/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 2009.0258 - mse: 26350884.0000 - mae: 2009.0259 - val_loss: 2221.7317 - val_mse: 32395122.0000 - val_mae: 2221.7319\n",
            "Epoch 165/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 2012.0195 - mse: 26330200.0000 - mae: 2012.0195 - val_loss: 2229.7929 - val_mse: 32445634.0000 - val_mae: 2229.7927\n",
            "Epoch 166/500\n",
            "750/750 [==============================] - 0s 66us/step - loss: 2003.4469 - mse: 26330480.0000 - mae: 2003.4470 - val_loss: 2211.4707 - val_mse: 32211124.0000 - val_mae: 2211.4707\n",
            "Epoch 167/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2003.4384 - mse: 26272296.0000 - mae: 2003.4384 - val_loss: 2208.7228 - val_mse: 32134568.0000 - val_mae: 2208.7229\n",
            "Epoch 168/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 2002.1176 - mse: 26256212.0000 - mae: 2002.1177 - val_loss: 2213.0976 - val_mse: 32212022.0000 - val_mae: 2213.0977\n",
            "Epoch 169/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2001.9441 - mse: 26620740.0000 - mae: 2001.9440 - val_loss: 2211.5628 - val_mse: 32184222.0000 - val_mae: 2211.5627\n",
            "Epoch 170/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 2000.3335 - mse: 26473370.0000 - mae: 2000.3335 - val_loss: 2215.9494 - val_mse: 32236994.0000 - val_mae: 2215.9492\n",
            "Epoch 171/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 2004.8833 - mse: 26308916.0000 - mae: 2004.8833 - val_loss: 2210.8899 - val_mse: 32137926.0000 - val_mae: 2210.8899\n",
            "Epoch 172/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 2000.1853 - mse: 26297974.0000 - mae: 2000.1853 - val_loss: 2213.1704 - val_mse: 32160932.0000 - val_mae: 2213.1704\n",
            "Epoch 173/500\n",
            "750/750 [==============================] - 0s 75us/step - loss: 1998.5944 - mse: 26365820.0000 - mae: 1998.5944 - val_loss: 2211.8380 - val_mse: 32154802.0000 - val_mae: 2211.8381\n",
            "Epoch 174/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 2018.0839 - mse: 26510918.0000 - mae: 2018.0840 - val_loss: 2217.4451 - val_mse: 32198224.0000 - val_mae: 2217.4451\n",
            "Epoch 175/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 1997.7650 - mse: 26573046.0000 - mae: 1997.7650 - val_loss: 2208.7983 - val_mse: 32048784.0000 - val_mae: 2208.7983\n",
            "Epoch 176/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2000.7109 - mse: 26357910.0000 - mae: 2000.7108 - val_loss: 2214.0389 - val_mse: 32173010.0000 - val_mae: 2214.0388\n",
            "Epoch 177/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 2001.2823 - mse: 26145052.0000 - mae: 2001.2823 - val_loss: 2215.4069 - val_mse: 32163736.0000 - val_mae: 2215.4070\n",
            "Epoch 178/500\n",
            "750/750 [==============================] - 0s 67us/step - loss: 1990.1594 - mse: 26264638.0000 - mae: 1990.1593 - val_loss: 2215.8295 - val_mse: 32174740.0000 - val_mae: 2215.8296\n",
            "Epoch 179/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 1998.8508 - mse: 26420134.0000 - mae: 1998.8508 - val_loss: 2213.0396 - val_mse: 32143086.0000 - val_mae: 2213.0396\n",
            "Epoch 180/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 1996.4336 - mse: 26363128.0000 - mae: 1996.4337 - val_loss: 2215.5215 - val_mse: 32147222.0000 - val_mae: 2215.5212\n",
            "Epoch 181/500\n",
            "750/750 [==============================] - 0s 68us/step - loss: 2007.3561 - mse: 26163208.0000 - mae: 2007.3560 - val_loss: 2214.9377 - val_mse: 32121580.0000 - val_mae: 2214.9377\n",
            "Epoch 182/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1997.7810 - mse: 26233848.0000 - mae: 1997.7810 - val_loss: 2208.5582 - val_mse: 32004522.0000 - val_mae: 2208.5581\n",
            "Epoch 183/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1992.6981 - mse: 26080576.0000 - mae: 1992.6980 - val_loss: 2218.5907 - val_mse: 32146348.0000 - val_mae: 2218.5906\n",
            "Epoch 184/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 2000.3997 - mse: 26065458.0000 - mae: 2000.3997 - val_loss: 2214.4143 - val_mse: 32098040.0000 - val_mae: 2214.4143\n",
            "Epoch 185/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 1997.1808 - mse: 26412094.0000 - mae: 1997.1808 - val_loss: 2230.4087 - val_mse: 32279906.0000 - val_mae: 2230.4084\n",
            "Epoch 186/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1996.4925 - mse: 26071242.0000 - mae: 1996.4926 - val_loss: 2217.0223 - val_mse: 32148754.0000 - val_mae: 2217.0222\n",
            "Epoch 187/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1995.9793 - mse: 26118782.0000 - mae: 1995.9794 - val_loss: 2220.0884 - val_mse: 32154692.0000 - val_mae: 2220.0884\n",
            "Epoch 188/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1997.8176 - mse: 25995964.0000 - mae: 1997.8176 - val_loss: 2223.3190 - val_mse: 32159926.0000 - val_mae: 2223.3188\n",
            "Epoch 189/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 1986.0516 - mse: 26044804.0000 - mae: 1986.0515 - val_loss: 2223.6870 - val_mse: 32125950.0000 - val_mae: 2223.6870\n",
            "Epoch 190/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1996.0636 - mse: 26024318.0000 - mae: 1996.0637 - val_loss: 2208.9301 - val_mse: 31909460.0000 - val_mae: 2208.9302\n",
            "Epoch 191/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 1982.9341 - mse: 26179560.0000 - mae: 1982.9340 - val_loss: 2214.3460 - val_mse: 31972398.0000 - val_mae: 2214.3459\n",
            "Epoch 192/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1986.4256 - mse: 26180652.0000 - mae: 1986.4257 - val_loss: 2212.9617 - val_mse: 31943006.0000 - val_mae: 2212.9617\n",
            "Epoch 193/500\n",
            "750/750 [==============================] - 0s 99us/step - loss: 1988.5256 - mse: 26024564.0000 - mae: 1988.5256 - val_loss: 2212.0893 - val_mse: 31957134.0000 - val_mae: 2212.0894\n",
            "Epoch 194/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1989.6569 - mse: 26042018.0000 - mae: 1989.6570 - val_loss: 2218.0905 - val_mse: 32042980.0000 - val_mae: 2218.0906\n",
            "Epoch 195/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1980.7074 - mse: 25867084.0000 - mae: 1980.7073 - val_loss: 2240.1357 - val_mse: 32257800.0000 - val_mae: 2240.1357\n",
            "Epoch 196/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 1987.6257 - mse: 25966046.0000 - mae: 1987.6256 - val_loss: 2222.8317 - val_mse: 32056776.0000 - val_mae: 2222.8318\n",
            "Epoch 197/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1981.0366 - mse: 25773084.0000 - mae: 1981.0366 - val_loss: 2216.2095 - val_mse: 31965844.0000 - val_mae: 2216.2097\n",
            "Epoch 198/500\n",
            "750/750 [==============================] - 0s 111us/step - loss: 1981.5874 - mse: 25996826.0000 - mae: 1981.5873 - val_loss: 2216.4773 - val_mse: 31969226.0000 - val_mae: 2216.4771\n",
            "Epoch 199/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1977.9654 - mse: 25779606.0000 - mae: 1977.9653 - val_loss: 2224.6447 - val_mse: 32076112.0000 - val_mae: 2224.6448\n",
            "Epoch 200/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1978.5400 - mse: 26037108.0000 - mae: 1978.5400 - val_loss: 2222.0692 - val_mse: 32020346.0000 - val_mae: 2222.0693\n",
            "Epoch 201/500\n",
            "750/750 [==============================] - 0s 72us/step - loss: 1980.1426 - mse: 25986860.0000 - mae: 1980.1427 - val_loss: 2222.8695 - val_mse: 32031664.0000 - val_mae: 2222.8694\n",
            "Epoch 202/500\n",
            "750/750 [==============================] - 0s 73us/step - loss: 1991.7840 - mse: 26085778.0000 - mae: 1991.7841 - val_loss: 2233.0664 - val_mse: 32094610.0000 - val_mae: 2233.0664\n",
            "Epoch 203/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1984.1325 - mse: 25596882.0000 - mae: 1984.1327 - val_loss: 2227.0171 - val_mse: 32060484.0000 - val_mae: 2227.0171\n",
            "Epoch 204/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1985.9161 - mse: 26099550.0000 - mae: 1985.9160 - val_loss: 2237.2018 - val_mse: 32147396.0000 - val_mae: 2237.2017\n",
            "Epoch 205/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1996.6338 - mse: 26108658.0000 - mae: 1996.6338 - val_loss: 2240.4807 - val_mse: 32168668.0000 - val_mae: 2240.4807\n",
            "Epoch 206/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1995.0043 - mse: 26166128.0000 - mae: 1995.0043 - val_loss: 2225.9491 - val_mse: 32038050.0000 - val_mae: 2225.9490\n",
            "Epoch 207/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1993.4965 - mse: 25979388.0000 - mae: 1993.4963 - val_loss: 2224.8797 - val_mse: 32017012.0000 - val_mae: 2224.8799\n",
            "Epoch 208/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 2001.0179 - mse: 25843598.0000 - mae: 2001.0178 - val_loss: 2217.4842 - val_mse: 31925234.0000 - val_mae: 2217.4841\n",
            "Epoch 209/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1982.2142 - mse: 25818950.0000 - mae: 1982.2141 - val_loss: 2220.6664 - val_mse: 31947140.0000 - val_mae: 2220.6663\n",
            "Epoch 210/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1987.4453 - mse: 25851588.0000 - mae: 1987.4453 - val_loss: 2225.1312 - val_mse: 31999706.0000 - val_mae: 2225.1313\n",
            "Epoch 211/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1973.1889 - mse: 25786734.0000 - mae: 1973.1888 - val_loss: 2226.0073 - val_mse: 32034250.0000 - val_mae: 2226.0073\n",
            "Epoch 212/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1981.4565 - mse: 25983330.0000 - mae: 1981.4565 - val_loss: 2215.8866 - val_mse: 31891612.0000 - val_mae: 2215.8865\n",
            "Epoch 213/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1986.7547 - mse: 25705212.0000 - mae: 1986.7546 - val_loss: 2228.8099 - val_mse: 32028214.0000 - val_mae: 2228.8101\n",
            "Epoch 214/500\n",
            "750/750 [==============================] - 0s 92us/step - loss: 1973.1100 - mse: 26005832.0000 - mae: 1973.1100 - val_loss: 2225.8549 - val_mse: 32006346.0000 - val_mae: 2225.8547\n",
            "Epoch 215/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1984.9797 - mse: 25957046.0000 - mae: 1984.9799 - val_loss: 2225.3664 - val_mse: 31985162.0000 - val_mae: 2225.3665\n",
            "Epoch 216/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1987.9220 - mse: 26016298.0000 - mae: 1987.9220 - val_loss: 2255.2913 - val_mse: 32264926.0000 - val_mae: 2255.2913\n",
            "Epoch 217/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 1993.7325 - mse: 26201778.0000 - mae: 1993.7325 - val_loss: 2221.5534 - val_mse: 31915824.0000 - val_mae: 2221.5532\n",
            "Epoch 218/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1971.9306 - mse: 25658266.0000 - mae: 1971.9307 - val_loss: 2221.0700 - val_mse: 31921352.0000 - val_mae: 2221.0701\n",
            "Epoch 219/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1983.5223 - mse: 25607536.0000 - mae: 1983.5223 - val_loss: 2251.6007 - val_mse: 32161060.0000 - val_mae: 2251.6006\n",
            "Epoch 220/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1984.6355 - mse: 25966996.0000 - mae: 1984.6354 - val_loss: 2225.1522 - val_mse: 31917782.0000 - val_mae: 2225.1521\n",
            "Epoch 221/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 2000.0434 - mse: 25964796.0000 - mae: 2000.0433 - val_loss: 2215.2599 - val_mse: 31800694.0000 - val_mae: 2215.2598\n",
            "Epoch 222/500\n",
            "750/750 [==============================] - 0s 75us/step - loss: 1973.9426 - mse: 25642068.0000 - mae: 1973.9426 - val_loss: 2221.1658 - val_mse: 31871198.0000 - val_mae: 2221.1658\n",
            "Epoch 223/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1981.0113 - mse: 25805318.0000 - mae: 1981.0114 - val_loss: 2240.2635 - val_mse: 32043698.0000 - val_mae: 2240.2634\n",
            "Epoch 224/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1981.8087 - mse: 25807198.0000 - mae: 1981.8087 - val_loss: 2233.7785 - val_mse: 31982502.0000 - val_mae: 2233.7786\n",
            "Epoch 225/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1991.7833 - mse: 26055456.0000 - mae: 1991.7833 - val_loss: 2225.5348 - val_mse: 31913358.0000 - val_mae: 2225.5349\n",
            "Epoch 226/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 1989.6572 - mse: 25859638.0000 - mae: 1989.6573 - val_loss: 2244.9711 - val_mse: 32085748.0000 - val_mae: 2244.9709\n",
            "Epoch 227/500\n",
            "750/750 [==============================] - 0s 97us/step - loss: 1966.8735 - mse: 25861604.0000 - mae: 1966.8735 - val_loss: 2225.0000 - val_mse: 31893550.0000 - val_mae: 2225.0000\n",
            "Epoch 228/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1984.3671 - mse: 25909650.0000 - mae: 1984.3669 - val_loss: 2228.5448 - val_mse: 31967552.0000 - val_mae: 2228.5449\n",
            "Epoch 229/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1977.6503 - mse: 25999928.0000 - mae: 1977.6504 - val_loss: 2239.8058 - val_mse: 32019876.0000 - val_mae: 2239.8059\n",
            "Epoch 230/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1957.2962 - mse: 25810848.0000 - mae: 1957.2961 - val_loss: 2230.3639 - val_mse: 31936900.0000 - val_mae: 2230.3640\n",
            "Epoch 231/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1970.1265 - mse: 25550750.0000 - mae: 1970.1265 - val_loss: 2240.2589 - val_mse: 32023356.0000 - val_mae: 2240.2590\n",
            "Epoch 232/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1968.0766 - mse: 25713498.0000 - mae: 1968.0765 - val_loss: 2236.4096 - val_mse: 31996746.0000 - val_mae: 2236.4094\n",
            "Epoch 233/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1976.9194 - mse: 25913246.0000 - mae: 1976.9193 - val_loss: 2246.7692 - val_mse: 32099232.0000 - val_mae: 2246.7693\n",
            "Epoch 234/500\n",
            "750/750 [==============================] - 0s 75us/step - loss: 1978.8416 - mse: 25787002.0000 - mae: 1978.8416 - val_loss: 2236.8317 - val_mse: 31993356.0000 - val_mae: 2236.8318\n",
            "Epoch 235/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1970.2148 - mse: 25669268.0000 - mae: 1970.2147 - val_loss: 2251.9596 - val_mse: 32109510.0000 - val_mae: 2251.9597\n",
            "Epoch 236/500\n",
            "750/750 [==============================] - 0s 75us/step - loss: 1983.7795 - mse: 25893424.0000 - mae: 1983.7795 - val_loss: 2240.6092 - val_mse: 31984964.0000 - val_mae: 2240.6091\n",
            "Epoch 237/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1975.5674 - mse: 26041838.0000 - mae: 1975.5675 - val_loss: 2242.9169 - val_mse: 32006278.0000 - val_mae: 2242.9167\n",
            "Epoch 238/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1976.1778 - mse: 25616850.0000 - mae: 1976.1779 - val_loss: 2254.2724 - val_mse: 32092512.0000 - val_mae: 2254.2722\n",
            "Epoch 239/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1976.2598 - mse: 25871262.0000 - mae: 1976.2599 - val_loss: 2247.9393 - val_mse: 32052480.0000 - val_mae: 2247.9395\n",
            "Epoch 240/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1958.5549 - mse: 25792294.0000 - mae: 1958.5551 - val_loss: 2224.8750 - val_mse: 31860230.0000 - val_mae: 2224.8750\n",
            "Epoch 241/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 1967.4552 - mse: 25956958.0000 - mae: 1967.4553 - val_loss: 2254.3788 - val_mse: 32091276.0000 - val_mae: 2254.3789\n",
            "Epoch 242/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1973.4528 - mse: 25824390.0000 - mae: 1973.4529 - val_loss: 2245.5405 - val_mse: 32013566.0000 - val_mae: 2245.5405\n",
            "Epoch 243/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1972.3600 - mse: 25554234.0000 - mae: 1972.3600 - val_loss: 2252.9247 - val_mse: 32037978.0000 - val_mae: 2252.9246\n",
            "Epoch 244/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1970.1912 - mse: 25826646.0000 - mae: 1970.1912 - val_loss: 2240.1152 - val_mse: 31948382.0000 - val_mae: 2240.1152\n",
            "Epoch 245/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1974.3116 - mse: 26070440.0000 - mae: 1974.3116 - val_loss: 2240.1500 - val_mse: 31950192.0000 - val_mae: 2240.1499\n",
            "Epoch 246/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1964.8719 - mse: 25788700.0000 - mae: 1964.8719 - val_loss: 2235.1636 - val_mse: 31918910.0000 - val_mae: 2235.1636\n",
            "Epoch 247/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1958.0500 - mse: 25661670.0000 - mae: 1958.0500 - val_loss: 2239.6051 - val_mse: 31925272.0000 - val_mae: 2239.6050\n",
            "Epoch 248/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1950.9663 - mse: 25427028.0000 - mae: 1950.9663 - val_loss: 2236.5405 - val_mse: 31931888.0000 - val_mae: 2236.5405\n",
            "Epoch 249/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1969.7505 - mse: 25783754.0000 - mae: 1969.7505 - val_loss: 2257.3245 - val_mse: 32068262.0000 - val_mae: 2257.3245\n",
            "Epoch 250/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1971.7190 - mse: 25985366.0000 - mae: 1971.7190 - val_loss: 2239.8370 - val_mse: 31922930.0000 - val_mae: 2239.8372\n",
            "Epoch 251/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1979.8377 - mse: 25589202.0000 - mae: 1979.8376 - val_loss: 2257.1986 - val_mse: 32042304.0000 - val_mae: 2257.1985\n",
            "Epoch 252/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1948.0129 - mse: 25875672.0000 - mae: 1948.0128 - val_loss: 2232.6172 - val_mse: 31839112.0000 - val_mae: 2232.6172\n",
            "Epoch 253/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1971.4997 - mse: 25616034.0000 - mae: 1971.4996 - val_loss: 2242.8456 - val_mse: 31898162.0000 - val_mae: 2242.8457\n",
            "Epoch 254/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1983.1375 - mse: 25955994.0000 - mae: 1983.1375 - val_loss: 2226.6253 - val_mse: 31750638.0000 - val_mae: 2226.6252\n",
            "Epoch 255/500\n",
            "750/750 [==============================] - 0s 65us/step - loss: 1980.3352 - mse: 25586352.0000 - mae: 1980.3353 - val_loss: 2240.4148 - val_mse: 31877028.0000 - val_mae: 2240.4148\n",
            "Epoch 256/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1970.3396 - mse: 25358162.0000 - mae: 1970.3395 - val_loss: 2254.9585 - val_mse: 31947920.0000 - val_mae: 2254.9587\n",
            "Epoch 257/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 1962.5698 - mse: 25426684.0000 - mae: 1962.5698 - val_loss: 2252.5362 - val_mse: 31972770.0000 - val_mae: 2252.5364\n",
            "Epoch 258/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1980.2342 - mse: 26054478.0000 - mae: 1980.2341 - val_loss: 2266.5987 - val_mse: 32076620.0000 - val_mae: 2266.5986\n",
            "Epoch 259/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1959.5102 - mse: 25497136.0000 - mae: 1959.5101 - val_loss: 2244.0064 - val_mse: 31874708.0000 - val_mae: 2244.0066\n",
            "Epoch 260/500\n",
            "750/750 [==============================] - 0s 94us/step - loss: 1954.7781 - mse: 25378284.0000 - mae: 1954.7780 - val_loss: 2243.9831 - val_mse: 31891268.0000 - val_mae: 2243.9829\n",
            "Epoch 261/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1961.8933 - mse: 25676150.0000 - mae: 1961.8933 - val_loss: 2240.2031 - val_mse: 31875982.0000 - val_mae: 2240.2031\n",
            "Epoch 262/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1964.0700 - mse: 25561078.0000 - mae: 1964.0699 - val_loss: 2252.9571 - val_mse: 31953160.0000 - val_mae: 2252.9573\n",
            "Epoch 263/500\n",
            "750/750 [==============================] - 0s 96us/step - loss: 1981.7625 - mse: 25729836.0000 - mae: 1981.7625 - val_loss: 2258.9395 - val_mse: 32014988.0000 - val_mae: 2258.9395\n",
            "Epoch 264/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1972.3267 - mse: 25716646.0000 - mae: 1972.3267 - val_loss: 2242.3871 - val_mse: 31875708.0000 - val_mae: 2242.3872\n",
            "Epoch 265/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1965.0080 - mse: 25702126.0000 - mae: 1965.0081 - val_loss: 2243.1897 - val_mse: 31900554.0000 - val_mae: 2243.1897\n",
            "Epoch 266/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 1951.1691 - mse: 25644772.0000 - mae: 1951.1689 - val_loss: 2260.6008 - val_mse: 32041034.0000 - val_mae: 2260.6011\n",
            "Epoch 267/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1963.9510 - mse: 25743814.0000 - mae: 1963.9510 - val_loss: 2269.6811 - val_mse: 32082864.0000 - val_mae: 2269.6812\n",
            "Epoch 268/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1959.1704 - mse: 25415036.0000 - mae: 1959.1705 - val_loss: 2238.9051 - val_mse: 31812012.0000 - val_mae: 2238.9050\n",
            "Epoch 269/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1955.6158 - mse: 25529388.0000 - mae: 1955.6158 - val_loss: 2242.0948 - val_mse: 31833446.0000 - val_mae: 2242.0947\n",
            "Epoch 270/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1965.8505 - mse: 25591250.0000 - mae: 1965.8505 - val_loss: 2239.6317 - val_mse: 31801734.0000 - val_mae: 2239.6316\n",
            "Epoch 271/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1961.4334 - mse: 25664968.0000 - mae: 1961.4333 - val_loss: 2252.2985 - val_mse: 31925988.0000 - val_mae: 2252.2986\n",
            "Epoch 272/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1969.9738 - mse: 25760426.0000 - mae: 1969.9739 - val_loss: 2263.4991 - val_mse: 32001188.0000 - val_mae: 2263.4993\n",
            "Epoch 273/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1966.9755 - mse: 25668554.0000 - mae: 1966.9755 - val_loss: 2257.8298 - val_mse: 31971590.0000 - val_mae: 2257.8296\n",
            "Epoch 274/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1979.0249 - mse: 25772660.0000 - mae: 1979.0248 - val_loss: 2257.8867 - val_mse: 31958736.0000 - val_mae: 2257.8867\n",
            "Epoch 275/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1968.3112 - mse: 25623148.0000 - mae: 1968.3113 - val_loss: 2259.9117 - val_mse: 31957074.0000 - val_mae: 2259.9116\n",
            "Epoch 276/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1963.8234 - mse: 25359630.0000 - mae: 1963.8235 - val_loss: 2250.3395 - val_mse: 31885232.0000 - val_mae: 2250.3394\n",
            "Epoch 277/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 1956.2282 - mse: 25774624.0000 - mae: 1956.2281 - val_loss: 2253.1801 - val_mse: 31891328.0000 - val_mae: 2253.1799\n",
            "Epoch 278/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1955.2635 - mse: 25577376.0000 - mae: 1955.2637 - val_loss: 2248.1887 - val_mse: 31840684.0000 - val_mae: 2248.1887\n",
            "Epoch 279/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 1973.6123 - mse: 25388630.0000 - mae: 1973.6123 - val_loss: 2245.3193 - val_mse: 31833430.0000 - val_mae: 2245.3193\n",
            "Epoch 280/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1957.7952 - mse: 25597398.0000 - mae: 1957.7953 - val_loss: 2255.9066 - val_mse: 31909644.0000 - val_mae: 2255.9065\n",
            "Epoch 281/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1952.3648 - mse: 25457852.0000 - mae: 1952.3649 - val_loss: 2259.3067 - val_mse: 31908174.0000 - val_mae: 2259.3064\n",
            "Epoch 282/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1963.4179 - mse: 25274952.0000 - mae: 1963.4180 - val_loss: 2272.3657 - val_mse: 32014644.0000 - val_mae: 2272.3657\n",
            "Epoch 283/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1965.2574 - mse: 25517536.0000 - mae: 1965.2573 - val_loss: 2262.1652 - val_mse: 31939126.0000 - val_mae: 2262.1653\n",
            "Epoch 284/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1964.1781 - mse: 25294454.0000 - mae: 1964.1782 - val_loss: 2257.2180 - val_mse: 31930374.0000 - val_mae: 2257.2180\n",
            "Epoch 285/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1954.0053 - mse: 25407846.0000 - mae: 1954.0054 - val_loss: 2244.2597 - val_mse: 31823142.0000 - val_mae: 2244.2598\n",
            "Epoch 286/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1954.0715 - mse: 25407226.0000 - mae: 1954.0715 - val_loss: 2247.3666 - val_mse: 31831064.0000 - val_mae: 2247.3667\n",
            "Epoch 287/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1969.6177 - mse: 25664686.0000 - mae: 1969.6177 - val_loss: 2240.2552 - val_mse: 31804668.0000 - val_mae: 2240.2549\n",
            "Epoch 288/500\n",
            "750/750 [==============================] - 0s 92us/step - loss: 1955.1149 - mse: 25466858.0000 - mae: 1955.1150 - val_loss: 2261.8301 - val_mse: 31962346.0000 - val_mae: 2261.8301\n",
            "Epoch 289/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1954.0589 - mse: 25837814.0000 - mae: 1954.0590 - val_loss: 2258.1024 - val_mse: 31925360.0000 - val_mae: 2258.1023\n",
            "Epoch 290/500\n",
            "750/750 [==============================] - 0s 102us/step - loss: 1950.2459 - mse: 25398318.0000 - mae: 1950.2458 - val_loss: 2256.5374 - val_mse: 31891852.0000 - val_mae: 2256.5374\n",
            "Epoch 291/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1943.5480 - mse: 25409684.0000 - mae: 1943.5480 - val_loss: 2255.8418 - val_mse: 31884122.0000 - val_mae: 2255.8418\n",
            "Epoch 292/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1949.9147 - mse: 25447142.0000 - mae: 1949.9148 - val_loss: 2268.4636 - val_mse: 31999738.0000 - val_mae: 2268.4634\n",
            "Epoch 293/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1975.8193 - mse: 25700390.0000 - mae: 1975.8193 - val_loss: 2253.2564 - val_mse: 31894168.0000 - val_mae: 2253.2566\n",
            "Epoch 294/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1948.7020 - mse: 25133634.0000 - mae: 1948.7020 - val_loss: 2268.9900 - val_mse: 31993662.0000 - val_mae: 2268.9902\n",
            "Epoch 295/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1978.9786 - mse: 25637198.0000 - mae: 1978.9786 - val_loss: 2269.4695 - val_mse: 32000626.0000 - val_mae: 2269.4692\n",
            "Epoch 296/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1957.8261 - mse: 25336480.0000 - mae: 1957.8262 - val_loss: 2255.9049 - val_mse: 31900462.0000 - val_mae: 2255.9048\n",
            "Epoch 297/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1961.6237 - mse: 25728418.0000 - mae: 1961.6237 - val_loss: 2249.5116 - val_mse: 31828590.0000 - val_mae: 2249.5117\n",
            "Epoch 298/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1946.8801 - mse: 25289280.0000 - mae: 1946.8801 - val_loss: 2258.4389 - val_mse: 31901758.0000 - val_mae: 2258.4390\n",
            "Epoch 299/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1958.5459 - mse: 25419490.0000 - mae: 1958.5460 - val_loss: 2264.1249 - val_mse: 31951768.0000 - val_mae: 2264.1250\n",
            "Epoch 300/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1948.5160 - mse: 25399874.0000 - mae: 1948.5160 - val_loss: 2253.5579 - val_mse: 31835694.0000 - val_mae: 2253.5581\n",
            "Epoch 301/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1945.8535 - mse: 25174966.0000 - mae: 1945.8535 - val_loss: 2243.4777 - val_mse: 31754152.0000 - val_mae: 2243.4778\n",
            "Epoch 302/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 1946.8804 - mse: 25074778.0000 - mae: 1946.8804 - val_loss: 2244.0493 - val_mse: 31787126.0000 - val_mae: 2244.0493\n",
            "Epoch 303/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1945.9172 - mse: 24896992.0000 - mae: 1945.9174 - val_loss: 2262.6626 - val_mse: 31928886.0000 - val_mae: 2262.6628\n",
            "Epoch 304/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1959.4401 - mse: 25311478.0000 - mae: 1959.4399 - val_loss: 2261.8363 - val_mse: 31915694.0000 - val_mae: 2261.8362\n",
            "Epoch 305/500\n",
            "750/750 [==============================] - 0s 103us/step - loss: 1932.3741 - mse: 25211514.0000 - mae: 1932.3740 - val_loss: 2259.2248 - val_mse: 31893760.0000 - val_mae: 2259.2249\n",
            "Epoch 306/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1957.3321 - mse: 25417726.0000 - mae: 1957.3320 - val_loss: 2262.5271 - val_mse: 31897722.0000 - val_mae: 2262.5271\n",
            "Epoch 307/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1967.1285 - mse: 25470050.0000 - mae: 1967.1285 - val_loss: 2267.3303 - val_mse: 31922908.0000 - val_mae: 2267.3303\n",
            "Epoch 308/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1950.4369 - mse: 25465436.0000 - mae: 1950.4370 - val_loss: 2262.6749 - val_mse: 31897360.0000 - val_mae: 2262.6748\n",
            "Epoch 309/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 1950.8437 - mse: 24982376.0000 - mae: 1950.8436 - val_loss: 2254.4562 - val_mse: 31836578.0000 - val_mae: 2254.4561\n",
            "Epoch 310/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1946.5409 - mse: 25175192.0000 - mae: 1946.5409 - val_loss: 2253.5183 - val_mse: 31793180.0000 - val_mae: 2253.5181\n",
            "Epoch 311/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1953.9248 - mse: 25487572.0000 - mae: 1953.9248 - val_loss: 2270.3009 - val_mse: 31934590.0000 - val_mae: 2270.3010\n",
            "Epoch 312/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1950.3038 - mse: 25366946.0000 - mae: 1950.3038 - val_loss: 2273.4149 - val_mse: 31973682.0000 - val_mae: 2273.4150\n",
            "Epoch 313/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 1952.9133 - mse: 25138548.0000 - mae: 1952.9132 - val_loss: 2277.7227 - val_mse: 31999128.0000 - val_mae: 2277.7227\n",
            "Epoch 314/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1944.8768 - mse: 25393114.0000 - mae: 1944.8768 - val_loss: 2270.3305 - val_mse: 31944706.0000 - val_mae: 2270.3306\n",
            "Epoch 315/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1917.8676 - mse: 25003474.0000 - mae: 1917.8676 - val_loss: 2274.7555 - val_mse: 31951140.0000 - val_mae: 2274.7556\n",
            "Epoch 316/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1956.7621 - mse: 25589960.0000 - mae: 1956.7620 - val_loss: 2264.5483 - val_mse: 31868246.0000 - val_mae: 2264.5481\n",
            "Epoch 317/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1952.0203 - mse: 25471446.0000 - mae: 1952.0204 - val_loss: 2267.4831 - val_mse: 31885124.0000 - val_mae: 2267.4832\n",
            "Epoch 318/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1953.1124 - mse: 25458232.0000 - mae: 1953.1123 - val_loss: 2277.7404 - val_mse: 31996862.0000 - val_mae: 2277.7405\n",
            "Epoch 319/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1952.6868 - mse: 25440272.0000 - mae: 1952.6869 - val_loss: 2266.9496 - val_mse: 31920836.0000 - val_mae: 2266.9497\n",
            "Epoch 320/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1954.6681 - mse: 25300850.0000 - mae: 1954.6680 - val_loss: 2258.2186 - val_mse: 31852334.0000 - val_mae: 2258.2188\n",
            "Epoch 321/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1929.7019 - mse: 25025878.0000 - mae: 1929.7018 - val_loss: 2265.4111 - val_mse: 31886052.0000 - val_mae: 2265.4109\n",
            "Epoch 322/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1951.8577 - mse: 25497900.0000 - mae: 1951.8577 - val_loss: 2263.6934 - val_mse: 31854034.0000 - val_mae: 2263.6934\n",
            "Epoch 323/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1933.4984 - mse: 25160654.0000 - mae: 1933.4983 - val_loss: 2274.1900 - val_mse: 31921070.0000 - val_mae: 2274.1899\n",
            "Epoch 324/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1946.6860 - mse: 25400796.0000 - mae: 1946.6860 - val_loss: 2276.6713 - val_mse: 31925158.0000 - val_mae: 2276.6711\n",
            "Epoch 325/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 1939.8405 - mse: 25097620.0000 - mae: 1939.8405 - val_loss: 2272.1658 - val_mse: 31878906.0000 - val_mae: 2272.1658\n",
            "Epoch 326/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1941.9145 - mse: 25834314.0000 - mae: 1941.9146 - val_loss: 2272.3336 - val_mse: 31861394.0000 - val_mae: 2272.3335\n",
            "Epoch 327/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1945.1361 - mse: 25579174.0000 - mae: 1945.1360 - val_loss: 2277.0254 - val_mse: 31921002.0000 - val_mae: 2277.0254\n",
            "Epoch 328/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1929.6440 - mse: 25276276.0000 - mae: 1929.6440 - val_loss: 2276.1084 - val_mse: 31907408.0000 - val_mae: 2276.1084\n",
            "Epoch 329/500\n",
            "750/750 [==============================] - 0s 98us/step - loss: 1952.0574 - mse: 25505016.0000 - mae: 1952.0574 - val_loss: 2270.1163 - val_mse: 31860646.0000 - val_mae: 2270.1162\n",
            "Epoch 330/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1937.0088 - mse: 25185922.0000 - mae: 1937.0088 - val_loss: 2278.9244 - val_mse: 31958982.0000 - val_mae: 2278.9246\n",
            "Epoch 331/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1942.7504 - mse: 25436090.0000 - mae: 1942.7504 - val_loss: 2273.7073 - val_mse: 31922812.0000 - val_mae: 2273.7073\n",
            "Epoch 332/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1930.0531 - mse: 25429894.0000 - mae: 1930.0530 - val_loss: 2256.3890 - val_mse: 31763984.0000 - val_mae: 2256.3892\n",
            "Epoch 333/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 1945.2469 - mse: 25388030.0000 - mae: 1945.2469 - val_loss: 2270.0157 - val_mse: 31852412.0000 - val_mae: 2270.0156\n",
            "Epoch 334/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1928.7520 - mse: 25274800.0000 - mae: 1928.7520 - val_loss: 2259.7234 - val_mse: 31756054.0000 - val_mae: 2259.7234\n",
            "Epoch 335/500\n",
            "750/750 [==============================] - 0s 97us/step - loss: 1948.6101 - mse: 25071968.0000 - mae: 1948.6100 - val_loss: 2275.5318 - val_mse: 31881468.0000 - val_mae: 2275.5317\n",
            "Epoch 336/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1934.9514 - mse: 25174608.0000 - mae: 1934.9513 - val_loss: 2284.9842 - val_mse: 31922648.0000 - val_mae: 2284.9841\n",
            "Epoch 337/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1954.0027 - mse: 25361526.0000 - mae: 1954.0027 - val_loss: 2269.2811 - val_mse: 31790192.0000 - val_mae: 2269.2812\n",
            "Epoch 338/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1930.9621 - mse: 25100968.0000 - mae: 1930.9622 - val_loss: 2267.7692 - val_mse: 31782066.0000 - val_mae: 2267.7693\n",
            "Epoch 339/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1946.1875 - mse: 25315268.0000 - mae: 1946.1875 - val_loss: 2270.4358 - val_mse: 31810588.0000 - val_mae: 2270.4360\n",
            "Epoch 340/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1950.7847 - mse: 25249222.0000 - mae: 1950.7847 - val_loss: 2281.4595 - val_mse: 31909366.0000 - val_mae: 2281.4595\n",
            "Epoch 341/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1934.3815 - mse: 25090916.0000 - mae: 1934.3815 - val_loss: 2284.3628 - val_mse: 31907078.0000 - val_mae: 2284.3625\n",
            "Epoch 342/500\n",
            "750/750 [==============================] - 0s 74us/step - loss: 1928.5130 - mse: 25154192.0000 - mae: 1928.5129 - val_loss: 2272.6641 - val_mse: 31814664.0000 - val_mae: 2272.6641\n",
            "Epoch 343/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1938.3691 - mse: 25386844.0000 - mae: 1938.3690 - val_loss: 2263.2772 - val_mse: 31753652.0000 - val_mae: 2263.2773\n",
            "Epoch 344/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1937.5085 - mse: 25306912.0000 - mae: 1937.5083 - val_loss: 2279.0090 - val_mse: 31903930.0000 - val_mae: 2279.0090\n",
            "Epoch 345/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1944.7520 - mse: 25452504.0000 - mae: 1944.7520 - val_loss: 2275.5695 - val_mse: 31873434.0000 - val_mae: 2275.5696\n",
            "Epoch 346/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1936.1217 - mse: 25531312.0000 - mae: 1936.1217 - val_loss: 2280.7223 - val_mse: 31869280.0000 - val_mae: 2280.7224\n",
            "Epoch 347/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1939.4211 - mse: 25212792.0000 - mae: 1939.4210 - val_loss: 2274.5257 - val_mse: 31787880.0000 - val_mae: 2274.5256\n",
            "Epoch 348/500\n",
            "750/750 [==============================] - 0s 100us/step - loss: 1920.5286 - mse: 25033846.0000 - mae: 1920.5287 - val_loss: 2268.0226 - val_mse: 31732626.0000 - val_mae: 2268.0227\n",
            "Epoch 349/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1926.9556 - mse: 25295356.0000 - mae: 1926.9554 - val_loss: 2272.6682 - val_mse: 31743688.0000 - val_mae: 2272.6682\n",
            "Epoch 350/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1939.4320 - mse: 25475908.0000 - mae: 1939.4320 - val_loss: 2281.3411 - val_mse: 31801732.0000 - val_mae: 2281.3411\n",
            "Epoch 351/500\n",
            "750/750 [==============================] - 0s 113us/step - loss: 1930.1402 - mse: 25273036.0000 - mae: 1930.1401 - val_loss: 2271.1997 - val_mse: 31735846.0000 - val_mae: 2271.1997\n",
            "Epoch 352/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1933.3706 - mse: 25168934.0000 - mae: 1933.3707 - val_loss: 2279.7712 - val_mse: 31800544.0000 - val_mae: 2279.7712\n",
            "Epoch 353/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1953.2950 - mse: 25480598.0000 - mae: 1953.2950 - val_loss: 2289.8250 - val_mse: 31844068.0000 - val_mae: 2289.8250\n",
            "Epoch 354/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1923.0634 - mse: 25088466.0000 - mae: 1923.0634 - val_loss: 2278.5990 - val_mse: 31765038.0000 - val_mae: 2278.5991\n",
            "Epoch 355/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1935.8704 - mse: 25547008.0000 - mae: 1935.8704 - val_loss: 2286.1529 - val_mse: 31826048.0000 - val_mae: 2286.1531\n",
            "Epoch 356/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1938.5704 - mse: 25565544.0000 - mae: 1938.5703 - val_loss: 2274.8008 - val_mse: 31753924.0000 - val_mae: 2274.8008\n",
            "Epoch 357/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1939.6908 - mse: 25242714.0000 - mae: 1939.6907 - val_loss: 2271.7673 - val_mse: 31744984.0000 - val_mae: 2271.7673\n",
            "Epoch 358/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1931.8075 - mse: 24905790.0000 - mae: 1931.8075 - val_loss: 2277.8235 - val_mse: 31767004.0000 - val_mae: 2277.8235\n",
            "Epoch 359/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1925.6955 - mse: 25136934.0000 - mae: 1925.6956 - val_loss: 2284.1133 - val_mse: 31800442.0000 - val_mae: 2284.1133\n",
            "Epoch 360/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1944.5231 - mse: 25086354.0000 - mae: 1944.5232 - val_loss: 2292.3458 - val_mse: 31878228.0000 - val_mae: 2292.3457\n",
            "Epoch 361/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 1932.3256 - mse: 25368600.0000 - mae: 1932.3257 - val_loss: 2276.2920 - val_mse: 31738820.0000 - val_mae: 2276.2920\n",
            "Epoch 362/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1928.3008 - mse: 24992352.0000 - mae: 1928.3008 - val_loss: 2273.1997 - val_mse: 31690138.0000 - val_mae: 2273.2000\n",
            "Epoch 363/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1931.3788 - mse: 25254858.0000 - mae: 1931.3788 - val_loss: 2287.4701 - val_mse: 31813930.0000 - val_mae: 2287.4702\n",
            "Epoch 364/500\n",
            "750/750 [==============================] - 0s 96us/step - loss: 1917.7983 - mse: 25159330.0000 - mae: 1917.7983 - val_loss: 2295.8222 - val_mse: 31868390.0000 - val_mae: 2295.8223\n",
            "Epoch 365/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1938.5331 - mse: 25130430.0000 - mae: 1938.5330 - val_loss: 2271.0133 - val_mse: 31681762.0000 - val_mae: 2271.0134\n",
            "Epoch 366/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1929.3119 - mse: 24973170.0000 - mae: 1929.3119 - val_loss: 2284.5083 - val_mse: 31768758.0000 - val_mae: 2284.5083\n",
            "Epoch 367/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1928.1406 - mse: 24942068.0000 - mae: 1928.1406 - val_loss: 2282.2009 - val_mse: 31763664.0000 - val_mae: 2282.2009\n",
            "Epoch 368/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1929.9488 - mse: 25178286.0000 - mae: 1929.9489 - val_loss: 2290.8112 - val_mse: 31828860.0000 - val_mae: 2290.8113\n",
            "Epoch 369/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1932.9365 - mse: 25189082.0000 - mae: 1932.9365 - val_loss: 2290.0112 - val_mse: 31804424.0000 - val_mae: 2290.0110\n",
            "Epoch 370/500\n",
            "750/750 [==============================] - 0s 92us/step - loss: 1934.0730 - mse: 24801278.0000 - mae: 1934.0730 - val_loss: 2283.3313 - val_mse: 31731644.0000 - val_mae: 2283.3313\n",
            "Epoch 371/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1919.2657 - mse: 24844094.0000 - mae: 1919.2656 - val_loss: 2291.5574 - val_mse: 31816572.0000 - val_mae: 2291.5574\n",
            "Epoch 372/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1932.1166 - mse: 25151390.0000 - mae: 1932.1167 - val_loss: 2283.8848 - val_mse: 31757546.0000 - val_mae: 2283.8848\n",
            "Epoch 373/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1937.1799 - mse: 25084548.0000 - mae: 1937.1798 - val_loss: 2282.6462 - val_mse: 31734590.0000 - val_mae: 2282.6462\n",
            "Epoch 374/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1940.8364 - mse: 24982796.0000 - mae: 1940.8365 - val_loss: 2291.3084 - val_mse: 31789618.0000 - val_mae: 2291.3083\n",
            "Epoch 375/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1925.2199 - mse: 25083356.0000 - mae: 1925.2198 - val_loss: 2291.9710 - val_mse: 31777122.0000 - val_mae: 2291.9709\n",
            "Epoch 376/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1909.8170 - mse: 24870518.0000 - mae: 1909.8170 - val_loss: 2278.3144 - val_mse: 31618154.0000 - val_mae: 2278.3145\n",
            "Epoch 377/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1923.0919 - mse: 24720884.0000 - mae: 1923.0920 - val_loss: 2293.4472 - val_mse: 31772084.0000 - val_mae: 2293.4473\n",
            "Epoch 378/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 1924.3720 - mse: 24877630.0000 - mae: 1924.3719 - val_loss: 2296.6709 - val_mse: 31805876.0000 - val_mae: 2296.6709\n",
            "Epoch 379/500\n",
            "750/750 [==============================] - 0s 94us/step - loss: 1945.5501 - mse: 25374862.0000 - mae: 1945.5502 - val_loss: 2289.9588 - val_mse: 31763912.0000 - val_mae: 2289.9587\n",
            "Epoch 380/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1917.7350 - mse: 25111142.0000 - mae: 1917.7350 - val_loss: 2291.4423 - val_mse: 31771126.0000 - val_mae: 2291.4424\n",
            "Epoch 381/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1922.8499 - mse: 25039268.0000 - mae: 1922.8500 - val_loss: 2285.1742 - val_mse: 31726456.0000 - val_mae: 2285.1741\n",
            "Epoch 382/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1926.8929 - mse: 25310724.0000 - mae: 1926.8929 - val_loss: 2279.1069 - val_mse: 31650120.0000 - val_mae: 2279.1069\n",
            "Epoch 383/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1910.6421 - mse: 24959068.0000 - mae: 1910.6422 - val_loss: 2286.6676 - val_mse: 31694842.0000 - val_mae: 2286.6675\n",
            "Epoch 384/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1932.6065 - mse: 25152220.0000 - mae: 1932.6064 - val_loss: 2281.7955 - val_mse: 31661114.0000 - val_mae: 2281.7954\n",
            "Epoch 385/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 1911.0350 - mse: 24788194.0000 - mae: 1911.0350 - val_loss: 2285.3173 - val_mse: 31693886.0000 - val_mae: 2285.3174\n",
            "Epoch 386/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1931.0118 - mse: 25189510.0000 - mae: 1931.0118 - val_loss: 2290.2226 - val_mse: 31743828.0000 - val_mae: 2290.2224\n",
            "Epoch 387/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1936.8189 - mse: 25403788.0000 - mae: 1936.8190 - val_loss: 2287.6987 - val_mse: 31735078.0000 - val_mae: 2287.6987\n",
            "Epoch 388/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1919.3826 - mse: 24744798.0000 - mae: 1919.3824 - val_loss: 2295.7094 - val_mse: 31770736.0000 - val_mae: 2295.7092\n",
            "Epoch 389/500\n",
            "750/750 [==============================] - 0s 94us/step - loss: 1919.8662 - mse: 25101910.0000 - mae: 1919.8660 - val_loss: 2296.4660 - val_mse: 31798658.0000 - val_mae: 2296.4661\n",
            "Epoch 390/500\n",
            "750/750 [==============================] - 0s 100us/step - loss: 1917.3102 - mse: 25141112.0000 - mae: 1917.3102 - val_loss: 2294.4012 - val_mse: 31789660.0000 - val_mae: 2294.4014\n",
            "Epoch 391/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1914.6709 - mse: 24800860.0000 - mae: 1914.6708 - val_loss: 2293.7849 - val_mse: 31746226.0000 - val_mae: 2293.7849\n",
            "Epoch 392/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1930.1319 - mse: 25042752.0000 - mae: 1930.1320 - val_loss: 2293.3967 - val_mse: 31716006.0000 - val_mae: 2293.3967\n",
            "Epoch 393/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1929.3750 - mse: 25054610.0000 - mae: 1929.3750 - val_loss: 2298.1805 - val_mse: 31769676.0000 - val_mae: 2298.1804\n",
            "Epoch 394/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1934.6926 - mse: 25188882.0000 - mae: 1934.6926 - val_loss: 2286.6483 - val_mse: 31688150.0000 - val_mae: 2286.6482\n",
            "Epoch 395/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1924.8121 - mse: 25086302.0000 - mae: 1924.8120 - val_loss: 2293.6536 - val_mse: 31729478.0000 - val_mae: 2293.6536\n",
            "Epoch 396/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1928.8279 - mse: 25325502.0000 - mae: 1928.8280 - val_loss: 2288.6000 - val_mse: 31711136.0000 - val_mae: 2288.6001\n",
            "Epoch 397/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1922.2529 - mse: 25114406.0000 - mae: 1922.2531 - val_loss: 2282.4854 - val_mse: 31623594.0000 - val_mae: 2282.4854\n",
            "Epoch 398/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1928.1875 - mse: 24811212.0000 - mae: 1928.1875 - val_loss: 2296.4699 - val_mse: 31700826.0000 - val_mae: 2296.4697\n",
            "Epoch 399/500\n",
            "750/750 [==============================] - 0s 70us/step - loss: 1925.4347 - mse: 24916858.0000 - mae: 1925.4347 - val_loss: 2284.5469 - val_mse: 31624638.0000 - val_mae: 2284.5471\n",
            "Epoch 400/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1925.4782 - mse: 25082252.0000 - mae: 1925.4784 - val_loss: 2285.9216 - val_mse: 31639388.0000 - val_mae: 2285.9216\n",
            "Epoch 401/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1926.4146 - mse: 24949870.0000 - mae: 1926.4147 - val_loss: 2289.0713 - val_mse: 31662182.0000 - val_mae: 2289.0713\n",
            "Epoch 402/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1937.1106 - mse: 25373332.0000 - mae: 1937.1107 - val_loss: 2284.2689 - val_mse: 31619438.0000 - val_mae: 2284.2688\n",
            "Epoch 403/500\n",
            "750/750 [==============================] - 0s 96us/step - loss: 1942.2298 - mse: 25337222.0000 - mae: 1942.2299 - val_loss: 2292.0990 - val_mse: 31666564.0000 - val_mae: 2292.0991\n",
            "Epoch 404/500\n",
            "750/750 [==============================] - 0s 98us/step - loss: 1913.6874 - mse: 25004584.0000 - mae: 1913.6874 - val_loss: 2289.8846 - val_mse: 31666244.0000 - val_mae: 2289.8848\n",
            "Epoch 405/500\n",
            "750/750 [==============================] - 0s 71us/step - loss: 1933.7950 - mse: 24861230.0000 - mae: 1933.7950 - val_loss: 2294.8676 - val_mse: 31695992.0000 - val_mae: 2294.8677\n",
            "Epoch 406/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1923.3938 - mse: 24712356.0000 - mae: 1923.3938 - val_loss: 2301.6880 - val_mse: 31737978.0000 - val_mae: 2301.6882\n",
            "Epoch 407/500\n",
            "750/750 [==============================] - 0s 94us/step - loss: 1901.1416 - mse: 24904568.0000 - mae: 1901.1417 - val_loss: 2290.9227 - val_mse: 31638602.0000 - val_mae: 2290.9229\n",
            "Epoch 408/500\n",
            "750/750 [==============================] - 0s 114us/step - loss: 1911.9958 - mse: 24619130.0000 - mae: 1911.9958 - val_loss: 2297.5431 - val_mse: 31661910.0000 - val_mae: 2297.5432\n",
            "Epoch 409/500\n",
            "750/750 [==============================] - 0s 96us/step - loss: 1904.6994 - mse: 24772598.0000 - mae: 1904.6993 - val_loss: 2294.3489 - val_mse: 31625896.0000 - val_mae: 2294.3489\n",
            "Epoch 410/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 1923.1025 - mse: 25024928.0000 - mae: 1923.1023 - val_loss: 2291.6161 - val_mse: 31622890.0000 - val_mae: 2291.6162\n",
            "Epoch 411/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1939.9010 - mse: 25231158.0000 - mae: 1939.9010 - val_loss: 2292.3317 - val_mse: 31605700.0000 - val_mae: 2292.3318\n",
            "Epoch 412/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1907.4004 - mse: 24834838.0000 - mae: 1907.4004 - val_loss: 2285.7847 - val_mse: 31545418.0000 - val_mae: 2285.7847\n",
            "Epoch 413/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1930.5124 - mse: 25044484.0000 - mae: 1930.5123 - val_loss: 2291.0461 - val_mse: 31570024.0000 - val_mae: 2291.0461\n",
            "Epoch 414/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1915.6642 - mse: 24884016.0000 - mae: 1915.6643 - val_loss: 2290.5759 - val_mse: 31567740.0000 - val_mae: 2290.5759\n",
            "Epoch 415/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1927.1355 - mse: 25015398.0000 - mae: 1927.1355 - val_loss: 2296.4898 - val_mse: 31605710.0000 - val_mae: 2296.4897\n",
            "Epoch 416/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1917.2777 - mse: 24919954.0000 - mae: 1917.2777 - val_loss: 2312.3771 - val_mse: 31752340.0000 - val_mae: 2312.3772\n",
            "Epoch 417/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1930.6568 - mse: 25344256.0000 - mae: 1930.6569 - val_loss: 2297.5982 - val_mse: 31624082.0000 - val_mae: 2297.5981\n",
            "Epoch 418/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1902.8902 - mse: 24666300.0000 - mae: 1902.8901 - val_loss: 2288.8045 - val_mse: 31556042.0000 - val_mae: 2288.8047\n",
            "Epoch 419/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1914.9957 - mse: 24546702.0000 - mae: 1914.9958 - val_loss: 2299.3590 - val_mse: 31643440.0000 - val_mae: 2299.3589\n",
            "Epoch 420/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1922.9028 - mse: 25033600.0000 - mae: 1922.9028 - val_loss: 2288.8393 - val_mse: 31612270.0000 - val_mae: 2288.8396\n",
            "Epoch 421/500\n",
            "750/750 [==============================] - 0s 97us/step - loss: 1915.9122 - mse: 24720382.0000 - mae: 1915.9121 - val_loss: 2284.8611 - val_mse: 31590048.0000 - val_mae: 2284.8611\n",
            "Epoch 422/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1899.6913 - mse: 24586464.0000 - mae: 1899.6913 - val_loss: 2285.8222 - val_mse: 31570426.0000 - val_mae: 2285.8223\n",
            "Epoch 423/500\n",
            "750/750 [==============================] - 0s 97us/step - loss: 1927.8277 - mse: 24565908.0000 - mae: 1927.8276 - val_loss: 2298.2856 - val_mse: 31683044.0000 - val_mae: 2298.2856\n",
            "Epoch 424/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1916.3730 - mse: 24932740.0000 - mae: 1916.3730 - val_loss: 2294.2662 - val_mse: 31645914.0000 - val_mae: 2294.2661\n",
            "Epoch 425/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1915.1320 - mse: 24839280.0000 - mae: 1915.1320 - val_loss: 2293.1365 - val_mse: 31626546.0000 - val_mae: 2293.1365\n",
            "Epoch 426/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1902.7511 - mse: 24714910.0000 - mae: 1902.7512 - val_loss: 2296.3183 - val_mse: 31643022.0000 - val_mae: 2296.3184\n",
            "Epoch 427/500\n",
            "750/750 [==============================] - 0s 104us/step - loss: 1896.0438 - mse: 24634590.0000 - mae: 1896.0438 - val_loss: 2282.3929 - val_mse: 31544112.0000 - val_mae: 2282.3928\n",
            "Epoch 428/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1914.2350 - mse: 24806488.0000 - mae: 1914.2350 - val_loss: 2287.8136 - val_mse: 31567422.0000 - val_mae: 2287.8137\n",
            "Epoch 429/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1912.2392 - mse: 24995938.0000 - mae: 1912.2391 - val_loss: 2287.8590 - val_mse: 31596146.0000 - val_mae: 2287.8589\n",
            "Epoch 430/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1905.1390 - mse: 24528430.0000 - mae: 1905.1390 - val_loss: 2294.6440 - val_mse: 31637782.0000 - val_mae: 2294.6440\n",
            "Epoch 431/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1924.2970 - mse: 25080884.0000 - mae: 1924.2970 - val_loss: 2294.2044 - val_mse: 31633100.0000 - val_mae: 2294.2043\n",
            "Epoch 432/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1914.8794 - mse: 24761922.0000 - mae: 1914.8793 - val_loss: 2298.1547 - val_mse: 31601320.0000 - val_mae: 2298.1548\n",
            "Epoch 433/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1910.2045 - mse: 25049696.0000 - mae: 1910.2045 - val_loss: 2292.5339 - val_mse: 31586476.0000 - val_mae: 2292.5339\n",
            "Epoch 434/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1911.5900 - mse: 24723128.0000 - mae: 1911.5900 - val_loss: 2289.5777 - val_mse: 31577694.0000 - val_mae: 2289.5776\n",
            "Epoch 435/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 1916.4956 - mse: 24918374.0000 - mae: 1916.4957 - val_loss: 2298.0297 - val_mse: 31640870.0000 - val_mae: 2298.0298\n",
            "Epoch 436/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1898.2955 - mse: 24707724.0000 - mae: 1898.2953 - val_loss: 2300.7394 - val_mse: 31670212.0000 - val_mae: 2300.7395\n",
            "Epoch 437/500\n",
            "750/750 [==============================] - 0s 108us/step - loss: 1940.7740 - mse: 25408674.0000 - mae: 1940.7740 - val_loss: 2296.8170 - val_mse: 31637954.0000 - val_mae: 2296.8171\n",
            "Epoch 438/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1903.5837 - mse: 24908720.0000 - mae: 1903.5836 - val_loss: 2297.6396 - val_mse: 31623798.0000 - val_mae: 2297.6396\n",
            "Epoch 439/500\n",
            "750/750 [==============================] - 0s 98us/step - loss: 1922.6776 - mse: 24885226.0000 - mae: 1922.6776 - val_loss: 2300.0068 - val_mse: 31634280.0000 - val_mae: 2300.0068\n",
            "Epoch 440/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1910.7947 - mse: 24897628.0000 - mae: 1910.7947 - val_loss: 2289.2326 - val_mse: 31535480.0000 - val_mae: 2289.2327\n",
            "Epoch 441/500\n",
            "750/750 [==============================] - 0s 79us/step - loss: 1926.5766 - mse: 24930910.0000 - mae: 1926.5765 - val_loss: 2293.0768 - val_mse: 31559802.0000 - val_mae: 2293.0769\n",
            "Epoch 442/500\n",
            "750/750 [==============================] - 0s 76us/step - loss: 1911.6959 - mse: 25129844.0000 - mae: 1911.6960 - val_loss: 2292.9551 - val_mse: 31577788.0000 - val_mae: 2292.9551\n",
            "Epoch 443/500\n",
            "750/750 [==============================] - 0s 95us/step - loss: 1917.5794 - mse: 24912284.0000 - mae: 1917.5793 - val_loss: 2302.3068 - val_mse: 31640100.0000 - val_mae: 2302.3069\n",
            "Epoch 444/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1922.5940 - mse: 24855798.0000 - mae: 1922.5940 - val_loss: 2290.3993 - val_mse: 31585886.0000 - val_mae: 2290.3994\n",
            "Epoch 445/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1910.2167 - mse: 24822508.0000 - mae: 1910.2167 - val_loss: 2295.3881 - val_mse: 31596930.0000 - val_mae: 2295.3882\n",
            "Epoch 446/500\n",
            "750/750 [==============================] - 0s 100us/step - loss: 1914.2565 - mse: 24625236.0000 - mae: 1914.2567 - val_loss: 2294.0455 - val_mse: 31588778.0000 - val_mae: 2294.0454\n",
            "Epoch 447/500\n",
            "750/750 [==============================] - 0s 92us/step - loss: 1921.3052 - mse: 25083140.0000 - mae: 1921.3052 - val_loss: 2294.8211 - val_mse: 31571302.0000 - val_mae: 2294.8210\n",
            "Epoch 448/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1912.3317 - mse: 25137242.0000 - mae: 1912.3317 - val_loss: 2291.4371 - val_mse: 31546680.0000 - val_mae: 2291.4373\n",
            "Epoch 449/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1914.1453 - mse: 24986480.0000 - mae: 1914.1454 - val_loss: 2300.5489 - val_mse: 31631908.0000 - val_mae: 2300.5491\n",
            "Epoch 450/500\n",
            "750/750 [==============================] - 0s 95us/step - loss: 1909.8600 - mse: 24778602.0000 - mae: 1909.8600 - val_loss: 2296.1969 - val_mse: 31587442.0000 - val_mae: 2296.1970\n",
            "Epoch 451/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1936.3190 - mse: 25136534.0000 - mae: 1936.3190 - val_loss: 2300.0898 - val_mse: 31597286.0000 - val_mae: 2300.0898\n",
            "Epoch 452/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1904.9620 - mse: 24909272.0000 - mae: 1904.9620 - val_loss: 2306.4356 - val_mse: 31624648.0000 - val_mae: 2306.4355\n",
            "Epoch 453/500\n",
            "750/750 [==============================] - 0s 97us/step - loss: 1921.3444 - mse: 25232412.0000 - mae: 1921.3445 - val_loss: 2304.8721 - val_mse: 31643200.0000 - val_mae: 2304.8721\n",
            "Epoch 454/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 1913.1326 - mse: 24762800.0000 - mae: 1913.1327 - val_loss: 2302.5306 - val_mse: 31620804.0000 - val_mae: 2302.5308\n",
            "Epoch 455/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1926.4561 - mse: 24884574.0000 - mae: 1926.4561 - val_loss: 2305.7803 - val_mse: 31620820.0000 - val_mae: 2305.7803\n",
            "Epoch 456/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1915.2838 - mse: 25236146.0000 - mae: 1915.2837 - val_loss: 2296.2577 - val_mse: 31573612.0000 - val_mae: 2296.2578\n",
            "Epoch 457/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1900.9775 - mse: 24476768.0000 - mae: 1900.9775 - val_loss: 2294.5613 - val_mse: 31554312.0000 - val_mae: 2294.5613\n",
            "Epoch 458/500\n",
            "750/750 [==============================] - 0s 90us/step - loss: 1922.6990 - mse: 25061750.0000 - mae: 1922.6990 - val_loss: 2299.4829 - val_mse: 31592838.0000 - val_mae: 2299.4829\n",
            "Epoch 459/500\n",
            "750/750 [==============================] - 0s 97us/step - loss: 1910.6244 - mse: 24854994.0000 - mae: 1910.6244 - val_loss: 2304.6500 - val_mse: 31636344.0000 - val_mae: 2304.6499\n",
            "Epoch 460/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1902.5445 - mse: 25026304.0000 - mae: 1902.5443 - val_loss: 2300.4556 - val_mse: 31605826.0000 - val_mae: 2300.4556\n",
            "Epoch 461/500\n",
            "750/750 [==============================] - 0s 95us/step - loss: 1916.6330 - mse: 24877260.0000 - mae: 1916.6331 - val_loss: 2294.4254 - val_mse: 31538256.0000 - val_mae: 2294.4253\n",
            "Epoch 462/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1919.6847 - mse: 24798908.0000 - mae: 1919.6847 - val_loss: 2304.8093 - val_mse: 31585782.0000 - val_mae: 2304.8093\n",
            "Epoch 463/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1901.8016 - mse: 24694034.0000 - mae: 1901.8016 - val_loss: 2295.9619 - val_mse: 31524594.0000 - val_mae: 2295.9619\n",
            "Epoch 464/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1907.4535 - mse: 24578554.0000 - mae: 1907.4535 - val_loss: 2302.0320 - val_mse: 31548072.0000 - val_mae: 2302.0322\n",
            "Epoch 465/500\n",
            "750/750 [==============================] - 0s 99us/step - loss: 1895.7017 - mse: 24519536.0000 - mae: 1895.7018 - val_loss: 2299.5739 - val_mse: 31495794.0000 - val_mae: 2299.5737\n",
            "Epoch 466/500\n",
            "750/750 [==============================] - 0s 97us/step - loss: 1911.1325 - mse: 24618640.0000 - mae: 1911.1324 - val_loss: 2303.4985 - val_mse: 31541324.0000 - val_mae: 2303.4985\n",
            "Epoch 467/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1899.6913 - mse: 24656670.0000 - mae: 1899.6913 - val_loss: 2300.7533 - val_mse: 31520580.0000 - val_mae: 2300.7534\n",
            "Epoch 468/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1914.4944 - mse: 24516718.0000 - mae: 1914.4944 - val_loss: 2302.3594 - val_mse: 31536380.0000 - val_mae: 2302.3594\n",
            "Epoch 469/500\n",
            "750/750 [==============================] - 0s 87us/step - loss: 1896.9718 - mse: 24428008.0000 - mae: 1896.9718 - val_loss: 2295.5227 - val_mse: 31450544.0000 - val_mae: 2295.5229\n",
            "Epoch 470/500\n",
            "750/750 [==============================] - 0s 88us/step - loss: 1899.5593 - mse: 24582624.0000 - mae: 1899.5593 - val_loss: 2300.7422 - val_mse: 31487972.0000 - val_mae: 2300.7422\n",
            "Epoch 471/500\n",
            "750/750 [==============================] - 0s 78us/step - loss: 1907.8474 - mse: 24850666.0000 - mae: 1907.8473 - val_loss: 2300.4895 - val_mse: 31487016.0000 - val_mae: 2300.4893\n",
            "Epoch 472/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1907.7489 - mse: 24727218.0000 - mae: 1907.7490 - val_loss: 2299.0771 - val_mse: 31467652.0000 - val_mae: 2299.0771\n",
            "Epoch 473/500\n",
            "750/750 [==============================] - 0s 81us/step - loss: 1917.5810 - mse: 24976588.0000 - mae: 1917.5811 - val_loss: 2298.4596 - val_mse: 31444378.0000 - val_mae: 2298.4597\n",
            "Epoch 474/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1913.8070 - mse: 24768848.0000 - mae: 1913.8070 - val_loss: 2305.1710 - val_mse: 31523020.0000 - val_mae: 2305.1709\n",
            "Epoch 475/500\n",
            "750/750 [==============================] - 0s 96us/step - loss: 1884.1726 - mse: 24866462.0000 - mae: 1884.1726 - val_loss: 2301.6481 - val_mse: 31481578.0000 - val_mae: 2301.6482\n",
            "Epoch 476/500\n",
            "750/750 [==============================] - 0s 77us/step - loss: 1905.1740 - mse: 24813738.0000 - mae: 1905.1740 - val_loss: 2306.4026 - val_mse: 31509818.0000 - val_mae: 2306.4026\n",
            "Epoch 477/500\n",
            "750/750 [==============================] - 0s 89us/step - loss: 1907.9950 - mse: 24857636.0000 - mae: 1907.9950 - val_loss: 2312.7462 - val_mse: 31577718.0000 - val_mae: 2312.7461\n",
            "Epoch 478/500\n",
            "750/750 [==============================] - 0s 83us/step - loss: 1898.0142 - mse: 24950544.0000 - mae: 1898.0143 - val_loss: 2298.8478 - val_mse: 31425216.0000 - val_mae: 2298.8479\n",
            "Epoch 479/500\n",
            "750/750 [==============================] - 0s 92us/step - loss: 1894.2192 - mse: 24639130.0000 - mae: 1894.2191 - val_loss: 2298.6502 - val_mse: 31418408.0000 - val_mae: 2298.6501\n",
            "Epoch 480/500\n",
            "750/750 [==============================] - 0s 95us/step - loss: 1904.0959 - mse: 24500462.0000 - mae: 1904.0958 - val_loss: 2309.5443 - val_mse: 31506344.0000 - val_mae: 2309.5442\n",
            "Epoch 481/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1879.4742 - mse: 24490550.0000 - mae: 1879.4744 - val_loss: 2307.9071 - val_mse: 31510446.0000 - val_mae: 2307.9072\n",
            "Epoch 482/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1908.1206 - mse: 24730290.0000 - mae: 1908.1207 - val_loss: 2307.2484 - val_mse: 31464454.0000 - val_mae: 2307.2483\n",
            "Epoch 483/500\n",
            "750/750 [==============================] - 0s 92us/step - loss: 1905.6899 - mse: 24807142.0000 - mae: 1905.6898 - val_loss: 2304.7352 - val_mse: 31430792.0000 - val_mae: 2304.7351\n",
            "Epoch 484/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1898.6721 - mse: 24760604.0000 - mae: 1898.6720 - val_loss: 2301.9955 - val_mse: 31418862.0000 - val_mae: 2301.9956\n",
            "Epoch 485/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1887.7439 - mse: 24358678.0000 - mae: 1887.7440 - val_loss: 2297.1142 - val_mse: 31377418.0000 - val_mae: 2297.1143\n",
            "Epoch 486/500\n",
            "750/750 [==============================] - 0s 95us/step - loss: 1899.1149 - mse: 24239358.0000 - mae: 1899.1150 - val_loss: 2310.6146 - val_mse: 31453822.0000 - val_mae: 2310.6145\n",
            "Epoch 487/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1900.3820 - mse: 24922570.0000 - mae: 1900.3820 - val_loss: 2294.0991 - val_mse: 31313362.0000 - val_mae: 2294.0991\n",
            "Epoch 488/500\n",
            "750/750 [==============================] - 0s 82us/step - loss: 1896.3091 - mse: 24598102.0000 - mae: 1896.3092 - val_loss: 2300.4139 - val_mse: 31375158.0000 - val_mae: 2300.4141\n",
            "Epoch 489/500\n",
            "750/750 [==============================] - 0s 100us/step - loss: 1907.3782 - mse: 24373748.0000 - mae: 1907.3783 - val_loss: 2297.2665 - val_mse: 31343476.0000 - val_mae: 2297.2664\n",
            "Epoch 490/500\n",
            "750/750 [==============================] - 0s 94us/step - loss: 1920.1912 - mse: 24682460.0000 - mae: 1920.1912 - val_loss: 2300.8556 - val_mse: 31409996.0000 - val_mae: 2300.8555\n",
            "Epoch 491/500\n",
            "750/750 [==============================] - 0s 84us/step - loss: 1907.7276 - mse: 24649324.0000 - mae: 1907.7277 - val_loss: 2301.6801 - val_mse: 31410780.0000 - val_mae: 2301.6799\n",
            "Epoch 492/500\n",
            "750/750 [==============================] - 0s 86us/step - loss: 1886.2599 - mse: 24278986.0000 - mae: 1886.2599 - val_loss: 2305.0095 - val_mse: 31440282.0000 - val_mae: 2305.0093\n",
            "Epoch 493/500\n",
            "750/750 [==============================] - 0s 108us/step - loss: 1906.6218 - mse: 24693566.0000 - mae: 1906.6218 - val_loss: 2313.0752 - val_mse: 31492924.0000 - val_mae: 2313.0750\n",
            "Epoch 494/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1901.4957 - mse: 24666020.0000 - mae: 1901.4957 - val_loss: 2308.4597 - val_mse: 31414202.0000 - val_mae: 2308.4597\n",
            "Epoch 495/500\n",
            "750/750 [==============================] - 0s 105us/step - loss: 1889.2739 - mse: 24171908.0000 - mae: 1889.2740 - val_loss: 2313.9529 - val_mse: 31444298.0000 - val_mae: 2313.9529\n",
            "Epoch 496/500\n",
            "750/750 [==============================] - 0s 80us/step - loss: 1902.6336 - mse: 24809474.0000 - mae: 1902.6337 - val_loss: 2299.1341 - val_mse: 31355598.0000 - val_mae: 2299.1340\n",
            "Epoch 497/500\n",
            "750/750 [==============================] - 0s 91us/step - loss: 1884.6264 - mse: 24512444.0000 - mae: 1884.6263 - val_loss: 2295.5488 - val_mse: 31350412.0000 - val_mae: 2295.5486\n",
            "Epoch 498/500\n",
            "750/750 [==============================] - 0s 111us/step - loss: 1893.3578 - mse: 24497322.0000 - mae: 1893.3578 - val_loss: 2306.0730 - val_mse: 31463030.0000 - val_mae: 2306.0730\n",
            "Epoch 499/500\n",
            "750/750 [==============================] - 0s 93us/step - loss: 1897.4460 - mse: 24705158.0000 - mae: 1897.4460 - val_loss: 2305.1502 - val_mse: 31442516.0000 - val_mae: 2305.1501\n",
            "Epoch 500/500\n",
            "750/750 [==============================] - 0s 85us/step - loss: 1910.3576 - mse: 24846620.0000 - mae: 1910.3577 - val_loss: 2295.7720 - val_mse: 31343812.0000 - val_mae: 2295.7717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1130 samples, validate on 1120 samples\n",
            "Epoch 1/500\n",
            "1130/1130 [==============================] - 2s 2ms/step - loss: 2909.3071 - mse: 44133796.0000 - mae: 2909.3071 - val_loss: 2775.3115 - val_mse: 39073272.0000 - val_mae: 2775.3115\n",
            "Epoch 2/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2909.2685 - mse: 44133560.0000 - mae: 2909.2686 - val_loss: 2775.2707 - val_mse: 39073052.0000 - val_mae: 2775.2710\n",
            "Epoch 3/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2909.1677 - mse: 44133072.0000 - mae: 2909.1675 - val_loss: 2774.8077 - val_mse: 39071140.0000 - val_mae: 2774.8081\n",
            "Epoch 4/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2907.9254 - mse: 44127908.0000 - mae: 2907.9253 - val_loss: 2772.0912 - val_mse: 39059424.0000 - val_mae: 2772.0911\n",
            "Epoch 5/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2904.4046 - mse: 44110644.0000 - mae: 2904.4045 - val_loss: 2767.0448 - val_mse: 39035596.0000 - val_mae: 2767.0449\n",
            "Epoch 6/500\n",
            "1130/1130 [==============================] - 0s 59us/step - loss: 2898.6392 - mse: 44081848.0000 - mae: 2898.6392 - val_loss: 2760.0133 - val_mse: 38999160.0000 - val_mae: 2760.0134\n",
            "Epoch 7/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2890.2096 - mse: 44034768.0000 - mae: 2890.2097 - val_loss: 2750.8371 - val_mse: 38947280.0000 - val_mae: 2750.8372\n",
            "Epoch 8/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2880.4779 - mse: 43980372.0000 - mae: 2880.4778 - val_loss: 2739.8253 - val_mse: 38882100.0000 - val_mae: 2739.8254\n",
            "Epoch 9/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2869.3734 - mse: 43912684.0000 - mae: 2869.3738 - val_loss: 2727.9321 - val_mse: 38807956.0000 - val_mae: 2727.9319\n",
            "Epoch 10/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2855.7034 - mse: 43819760.0000 - mae: 2855.7036 - val_loss: 2714.3942 - val_mse: 38717300.0000 - val_mae: 2714.3943\n",
            "Epoch 11/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2843.2973 - mse: 43726840.0000 - mae: 2843.2974 - val_loss: 2699.9548 - val_mse: 38617392.0000 - val_mae: 2699.9548\n",
            "Epoch 12/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2827.0089 - mse: 43619072.0000 - mae: 2827.0088 - val_loss: 2683.9291 - val_mse: 38504096.0000 - val_mae: 2683.9290\n",
            "Epoch 13/500\n",
            "1130/1130 [==============================] - 0s 59us/step - loss: 2810.4657 - mse: 43492708.0000 - mae: 2810.4658 - val_loss: 2666.8645 - val_mse: 38382844.0000 - val_mae: 2666.8645\n",
            "Epoch 14/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2790.6889 - mse: 43373380.0000 - mae: 2790.6890 - val_loss: 2648.4042 - val_mse: 38248956.0000 - val_mae: 2648.4043\n",
            "Epoch 15/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2772.9473 - mse: 43206636.0000 - mae: 2772.9473 - val_loss: 2630.4233 - val_mse: 38111408.0000 - val_mae: 2630.4231\n",
            "Epoch 16/500\n",
            "1130/1130 [==============================] - 0s 57us/step - loss: 2756.6283 - mse: 43072944.0000 - mae: 2756.6282 - val_loss: 2613.0660 - val_mse: 37972500.0000 - val_mae: 2613.0662\n",
            "Epoch 17/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2741.0756 - mse: 42924128.0000 - mae: 2741.0757 - val_loss: 2596.5667 - val_mse: 37830316.0000 - val_mae: 2596.5664\n",
            "Epoch 18/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2724.8831 - mse: 42766608.0000 - mae: 2724.8833 - val_loss: 2581.8603 - val_mse: 37692948.0000 - val_mae: 2581.8604\n",
            "Epoch 19/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2713.1029 - mse: 42632152.0000 - mae: 2713.1028 - val_loss: 2568.5296 - val_mse: 37558416.0000 - val_mae: 2568.5298\n",
            "Epoch 20/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2697.8725 - mse: 42461320.0000 - mae: 2697.8726 - val_loss: 2556.8003 - val_mse: 37426412.0000 - val_mae: 2556.8003\n",
            "Epoch 21/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2686.3256 - mse: 42334344.0000 - mae: 2686.3257 - val_loss: 2546.3512 - val_mse: 37294792.0000 - val_mae: 2546.3511\n",
            "Epoch 22/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2673.9456 - mse: 42251624.0000 - mae: 2673.9453 - val_loss: 2537.2803 - val_mse: 37168104.0000 - val_mae: 2537.2800\n",
            "Epoch 23/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2669.1698 - mse: 42137612.0000 - mae: 2669.1699 - val_loss: 2529.6000 - val_mse: 37051284.0000 - val_mae: 2529.6001\n",
            "Epoch 24/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2652.8783 - mse: 41971880.0000 - mae: 2652.8782 - val_loss: 2522.9467 - val_mse: 36932320.0000 - val_mae: 2522.9468\n",
            "Epoch 25/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2647.7160 - mse: 41839984.0000 - mae: 2647.7158 - val_loss: 2517.3672 - val_mse: 36824556.0000 - val_mae: 2517.3674\n",
            "Epoch 26/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2646.6589 - mse: 41785596.0000 - mae: 2646.6589 - val_loss: 2512.4380 - val_mse: 36716436.0000 - val_mae: 2512.4382\n",
            "Epoch 27/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2639.5938 - mse: 41610260.0000 - mae: 2639.5938 - val_loss: 2508.4590 - val_mse: 36620468.0000 - val_mae: 2508.4590\n",
            "Epoch 28/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2637.0962 - mse: 41480084.0000 - mae: 2637.0959 - val_loss: 2505.2756 - val_mse: 36538320.0000 - val_mae: 2505.2756\n",
            "Epoch 29/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2626.3127 - mse: 41448008.0000 - mae: 2626.3127 - val_loss: 2501.0737 - val_mse: 36428008.0000 - val_mae: 2501.0737\n",
            "Epoch 30/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2627.2906 - mse: 41348420.0000 - mae: 2627.2908 - val_loss: 2498.4798 - val_mse: 36352596.0000 - val_mae: 2498.4800\n",
            "Epoch 31/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2623.3563 - mse: 41267840.0000 - mae: 2623.3564 - val_loss: 2496.9521 - val_mse: 36291168.0000 - val_mae: 2496.9521\n",
            "Epoch 32/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2623.1412 - mse: 41225876.0000 - mae: 2623.1411 - val_loss: 2494.9590 - val_mse: 36223544.0000 - val_mae: 2494.9587\n",
            "Epoch 33/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2612.2333 - mse: 41061416.0000 - mae: 2612.2334 - val_loss: 2493.9471 - val_mse: 36144232.0000 - val_mae: 2493.9473\n",
            "Epoch 34/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2618.0881 - mse: 41052240.0000 - mae: 2618.0881 - val_loss: 2492.1213 - val_mse: 36093044.0000 - val_mae: 2492.1211\n",
            "Epoch 35/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2602.0082 - mse: 40838428.0000 - mae: 2602.0081 - val_loss: 2490.8006 - val_mse: 36031292.0000 - val_mae: 2490.8008\n",
            "Epoch 36/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2616.5900 - mse: 40862792.0000 - mae: 2616.5898 - val_loss: 2489.6198 - val_mse: 35990748.0000 - val_mae: 2489.6194\n",
            "Epoch 37/500\n",
            "1130/1130 [==============================] - 0s 59us/step - loss: 2615.1111 - mse: 40903660.0000 - mae: 2615.1113 - val_loss: 2488.4067 - val_mse: 35940740.0000 - val_mae: 2488.4067\n",
            "Epoch 38/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2614.2737 - mse: 40880764.0000 - mae: 2614.2734 - val_loss: 2487.2050 - val_mse: 35888132.0000 - val_mae: 2487.2048\n",
            "Epoch 39/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2615.4128 - mse: 40894628.0000 - mae: 2615.4131 - val_loss: 2484.8538 - val_mse: 35855632.0000 - val_mae: 2484.8538\n",
            "Epoch 40/500\n",
            "1130/1130 [==============================] - 0s 59us/step - loss: 2613.6430 - mse: 40886756.0000 - mae: 2613.6428 - val_loss: 2482.1640 - val_mse: 35837604.0000 - val_mae: 2482.1641\n",
            "Epoch 41/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2606.5314 - mse: 40769760.0000 - mae: 2606.5315 - val_loss: 2478.8706 - val_mse: 35839580.0000 - val_mae: 2478.8706\n",
            "Epoch 42/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2610.1990 - mse: 40919600.0000 - mae: 2610.1990 - val_loss: 2477.4559 - val_mse: 35791996.0000 - val_mae: 2477.4561\n",
            "Epoch 43/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2601.3708 - mse: 40755720.0000 - mae: 2601.3708 - val_loss: 2474.7327 - val_mse: 35781448.0000 - val_mae: 2474.7329\n",
            "Epoch 44/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2592.9972 - mse: 40779212.0000 - mae: 2592.9973 - val_loss: 2474.3495 - val_mse: 35692580.0000 - val_mae: 2474.3496\n",
            "Epoch 45/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2599.5896 - mse: 40671152.0000 - mae: 2599.5896 - val_loss: 2474.0158 - val_mse: 35619480.0000 - val_mae: 2474.0159\n",
            "Epoch 46/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2598.3004 - mse: 40535284.0000 - mae: 2598.3005 - val_loss: 2471.7432 - val_mse: 35586284.0000 - val_mae: 2471.7434\n",
            "Epoch 47/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2592.0008 - mse: 40521700.0000 - mae: 2592.0010 - val_loss: 2470.2910 - val_mse: 35530640.0000 - val_mae: 2470.2910\n",
            "Epoch 48/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2593.3024 - mse: 40569800.0000 - mae: 2593.3025 - val_loss: 2467.1182 - val_mse: 35521192.0000 - val_mae: 2467.1184\n",
            "Epoch 49/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2585.2829 - mse: 40501360.0000 - mae: 2585.2830 - val_loss: 2464.5845 - val_mse: 35494472.0000 - val_mae: 2464.5847\n",
            "Epoch 50/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2599.9821 - mse: 40638672.0000 - mae: 2599.9822 - val_loss: 2462.6384 - val_mse: 35470080.0000 - val_mae: 2462.6384\n",
            "Epoch 51/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2586.1114 - mse: 40369168.0000 - mae: 2586.1116 - val_loss: 2462.1896 - val_mse: 35407864.0000 - val_mae: 2462.1895\n",
            "Epoch 52/500\n",
            "1130/1130 [==============================] - 0s 58us/step - loss: 2589.6300 - mse: 40466660.0000 - mae: 2589.6296 - val_loss: 2461.1242 - val_mse: 35351376.0000 - val_mae: 2461.1243\n",
            "Epoch 53/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2583.2592 - mse: 40232336.0000 - mae: 2583.2593 - val_loss: 2458.8902 - val_mse: 35334580.0000 - val_mae: 2458.8901\n",
            "Epoch 54/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2586.3035 - mse: 40335428.0000 - mae: 2586.3032 - val_loss: 2452.6483 - val_mse: 35404560.0000 - val_mae: 2452.6482\n",
            "Epoch 55/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2579.2507 - mse: 40235288.0000 - mae: 2579.2507 - val_loss: 2454.0351 - val_mse: 35297952.0000 - val_mae: 2454.0349\n",
            "Epoch 56/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2579.7076 - mse: 40232440.0000 - mae: 2579.7078 - val_loss: 2453.5849 - val_mse: 35247480.0000 - val_mae: 2453.5850\n",
            "Epoch 57/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2580.2119 - mse: 40244816.0000 - mae: 2580.2117 - val_loss: 2452.1331 - val_mse: 35202804.0000 - val_mae: 2452.1335\n",
            "Epoch 58/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2580.9274 - mse: 40162788.0000 - mae: 2580.9275 - val_loss: 2448.9603 - val_mse: 35211692.0000 - val_mae: 2448.9600\n",
            "Epoch 59/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2573.8294 - mse: 40156392.0000 - mae: 2573.8293 - val_loss: 2451.3087 - val_mse: 35099104.0000 - val_mae: 2451.3086\n",
            "Epoch 60/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2569.3347 - mse: 40035288.0000 - mae: 2569.3347 - val_loss: 2452.8407 - val_mse: 35009776.0000 - val_mae: 2452.8408\n",
            "Epoch 61/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2573.6142 - mse: 40025640.0000 - mae: 2573.6145 - val_loss: 2446.5786 - val_mse: 35051740.0000 - val_mae: 2446.5784\n",
            "Epoch 62/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2572.8946 - mse: 39964456.0000 - mae: 2572.8945 - val_loss: 2448.0067 - val_mse: 34963988.0000 - val_mae: 2448.0066\n",
            "Epoch 63/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2566.2136 - mse: 39996440.0000 - mae: 2566.2134 - val_loss: 2446.5426 - val_mse: 34934740.0000 - val_mae: 2446.5425\n",
            "Epoch 64/500\n",
            "1130/1130 [==============================] - 0s 56us/step - loss: 2565.7963 - mse: 39901124.0000 - mae: 2565.7959 - val_loss: 2442.8830 - val_mse: 34944212.0000 - val_mae: 2442.8831\n",
            "Epoch 65/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2559.4858 - mse: 39757424.0000 - mae: 2559.4858 - val_loss: 2445.6674 - val_mse: 34838724.0000 - val_mae: 2445.6675\n",
            "Epoch 66/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2574.6596 - mse: 39798008.0000 - mae: 2574.6594 - val_loss: 2434.6851 - val_mse: 34976980.0000 - val_mae: 2434.6853\n",
            "Epoch 67/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2570.7981 - mse: 39909176.0000 - mae: 2570.7981 - val_loss: 2441.0786 - val_mse: 34816616.0000 - val_mae: 2441.0789\n",
            "Epoch 68/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2563.8382 - mse: 39785860.0000 - mae: 2563.8384 - val_loss: 2436.1352 - val_mse: 34854848.0000 - val_mae: 2436.1353\n",
            "Epoch 69/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2562.9430 - mse: 39984964.0000 - mae: 2562.9429 - val_loss: 2443.7787 - val_mse: 34686800.0000 - val_mae: 2443.7786\n",
            "Epoch 70/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2567.4688 - mse: 39606532.0000 - mae: 2567.4688 - val_loss: 2441.3968 - val_mse: 34676800.0000 - val_mae: 2441.3970\n",
            "Epoch 71/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2570.7008 - mse: 39840088.0000 - mae: 2570.7009 - val_loss: 2434.1928 - val_mse: 34746788.0000 - val_mae: 2434.1929\n",
            "Epoch 72/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2563.2046 - mse: 39736516.0000 - mae: 2563.2043 - val_loss: 2431.7545 - val_mse: 34753552.0000 - val_mae: 2431.7544\n",
            "Epoch 73/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2561.5808 - mse: 39781456.0000 - mae: 2561.5811 - val_loss: 2435.4748 - val_mse: 34654864.0000 - val_mae: 2435.4746\n",
            "Epoch 74/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2561.9812 - mse: 39624328.0000 - mae: 2561.9812 - val_loss: 2431.2550 - val_mse: 34665516.0000 - val_mae: 2431.2549\n",
            "Epoch 75/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2558.1055 - mse: 39724632.0000 - mae: 2558.1055 - val_loss: 2437.7344 - val_mse: 34545052.0000 - val_mae: 2437.7344\n",
            "Epoch 76/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2560.9190 - mse: 39517508.0000 - mae: 2560.9189 - val_loss: 2432.5638 - val_mse: 34564648.0000 - val_mae: 2432.5637\n",
            "Epoch 77/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2554.1802 - mse: 39573944.0000 - mae: 2554.1802 - val_loss: 2428.5970 - val_mse: 34578952.0000 - val_mae: 2428.5969\n",
            "Epoch 78/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2542.3551 - mse: 39428792.0000 - mae: 2542.3550 - val_loss: 2426.9586 - val_mse: 34563656.0000 - val_mae: 2426.9583\n",
            "Epoch 79/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2550.7935 - mse: 39476304.0000 - mae: 2550.7937 - val_loss: 2429.3530 - val_mse: 34475760.0000 - val_mae: 2429.3530\n",
            "Epoch 80/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2544.0156 - mse: 39416444.0000 - mae: 2544.0154 - val_loss: 2429.1620 - val_mse: 34426164.0000 - val_mae: 2429.1621\n",
            "Epoch 81/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2540.6097 - mse: 39290860.0000 - mae: 2540.6096 - val_loss: 2420.2595 - val_mse: 34514656.0000 - val_mae: 2420.2593\n",
            "Epoch 82/500\n",
            "1130/1130 [==============================] - 0s 59us/step - loss: 2539.8262 - mse: 39279008.0000 - mae: 2539.8264 - val_loss: 2429.2216 - val_mse: 34361192.0000 - val_mae: 2429.2214\n",
            "Epoch 83/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2549.7297 - mse: 39474716.0000 - mae: 2549.7297 - val_loss: 2411.0801 - val_mse: 34626760.0000 - val_mae: 2411.0798\n",
            "Epoch 84/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2541.9681 - mse: 39450572.0000 - mae: 2541.9683 - val_loss: 2422.8271 - val_mse: 34338284.0000 - val_mae: 2422.8269\n",
            "Epoch 85/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2545.6786 - mse: 39241840.0000 - mae: 2545.6787 - val_loss: 2410.8420 - val_mse: 34476564.0000 - val_mae: 2410.8420\n",
            "Epoch 86/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2545.8002 - mse: 39252280.0000 - mae: 2545.8005 - val_loss: 2412.2164 - val_mse: 34406788.0000 - val_mae: 2412.2166\n",
            "Epoch 87/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2544.0445 - mse: 39444300.0000 - mae: 2544.0444 - val_loss: 2408.4280 - val_mse: 34417368.0000 - val_mae: 2408.4282\n",
            "Epoch 88/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2540.6447 - mse: 39404760.0000 - mae: 2540.6448 - val_loss: 2406.1864 - val_mse: 34415704.0000 - val_mae: 2406.1863\n",
            "Epoch 89/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2543.7006 - mse: 39323616.0000 - mae: 2543.7004 - val_loss: 2419.9801 - val_mse: 34174784.0000 - val_mae: 2419.9800\n",
            "Epoch 90/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2541.7486 - mse: 39323928.0000 - mae: 2541.7485 - val_loss: 2411.8869 - val_mse: 34240140.0000 - val_mae: 2411.8867\n",
            "Epoch 91/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2536.5505 - mse: 39280040.0000 - mae: 2536.5508 - val_loss: 2413.3117 - val_mse: 34179152.0000 - val_mae: 2413.3118\n",
            "Epoch 92/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2535.2653 - mse: 39152564.0000 - mae: 2535.2654 - val_loss: 2412.2833 - val_mse: 34157164.0000 - val_mae: 2412.2834\n",
            "Epoch 93/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2536.6126 - mse: 39070836.0000 - mae: 2536.6123 - val_loss: 2410.3144 - val_mse: 34124280.0000 - val_mae: 2410.3142\n",
            "Epoch 94/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2536.3467 - mse: 39062072.0000 - mae: 2536.3469 - val_loss: 2406.9677 - val_mse: 34130880.0000 - val_mae: 2406.9675\n",
            "Epoch 95/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2526.7250 - mse: 39019316.0000 - mae: 2526.7251 - val_loss: 2418.7649 - val_mse: 33955120.0000 - val_mae: 2418.7646\n",
            "Epoch 96/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2522.8709 - mse: 38933140.0000 - mae: 2522.8711 - val_loss: 2408.5417 - val_mse: 34023444.0000 - val_mae: 2408.5415\n",
            "Epoch 97/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2526.7792 - mse: 38858876.0000 - mae: 2526.7793 - val_loss: 2407.3204 - val_mse: 33994856.0000 - val_mae: 2407.3206\n",
            "Epoch 98/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2523.8505 - mse: 38923436.0000 - mae: 2523.8503 - val_loss: 2403.8831 - val_mse: 33995356.0000 - val_mae: 2403.8828\n",
            "Epoch 99/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2531.6071 - mse: 38985552.0000 - mae: 2531.6072 - val_loss: 2404.9245 - val_mse: 33936300.0000 - val_mae: 2404.9246\n",
            "Epoch 100/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2531.1335 - mse: 38724576.0000 - mae: 2531.1335 - val_loss: 2408.1635 - val_mse: 33871936.0000 - val_mae: 2408.1633\n",
            "Epoch 101/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2532.0774 - mse: 38882564.0000 - mae: 2532.0774 - val_loss: 2393.1481 - val_mse: 34041644.0000 - val_mae: 2393.1477\n",
            "Epoch 102/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2530.7728 - mse: 38890192.0000 - mae: 2530.7725 - val_loss: 2398.7304 - val_mse: 33914640.0000 - val_mae: 2398.7305\n",
            "Epoch 103/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2522.5971 - mse: 38782388.0000 - mae: 2522.5972 - val_loss: 2405.0073 - val_mse: 33799864.0000 - val_mae: 2405.0073\n",
            "Epoch 104/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2528.6669 - mse: 38862704.0000 - mae: 2528.6667 - val_loss: 2391.4487 - val_mse: 33941484.0000 - val_mae: 2391.4490\n",
            "Epoch 105/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2523.3319 - mse: 38758540.0000 - mae: 2523.3318 - val_loss: 2387.7005 - val_mse: 33978876.0000 - val_mae: 2387.7007\n",
            "Epoch 106/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2524.4379 - mse: 38818088.0000 - mae: 2524.4377 - val_loss: 2407.2282 - val_mse: 33671888.0000 - val_mae: 2407.2283\n",
            "Epoch 107/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2529.2902 - mse: 38903824.0000 - mae: 2529.2903 - val_loss: 2392.2497 - val_mse: 33813648.0000 - val_mae: 2392.2498\n",
            "Epoch 108/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2516.0083 - mse: 38724632.0000 - mae: 2516.0083 - val_loss: 2394.9430 - val_mse: 33737724.0000 - val_mae: 2394.9431\n",
            "Epoch 109/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2529.7222 - mse: 38664208.0000 - mae: 2529.7222 - val_loss: 2392.2863 - val_mse: 33747692.0000 - val_mae: 2392.2864\n",
            "Epoch 110/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2516.7449 - mse: 38616812.0000 - mae: 2516.7446 - val_loss: 2385.9045 - val_mse: 33815744.0000 - val_mae: 2385.9048\n",
            "Epoch 111/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2513.7630 - mse: 38482520.0000 - mae: 2513.7629 - val_loss: 2388.7762 - val_mse: 33713900.0000 - val_mae: 2388.7764\n",
            "Epoch 112/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2516.4848 - mse: 38546516.0000 - mae: 2516.4849 - val_loss: 2391.7736 - val_mse: 33627144.0000 - val_mae: 2391.7734\n",
            "Epoch 113/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2510.5925 - mse: 38615524.0000 - mae: 2510.5925 - val_loss: 2381.3056 - val_mse: 33732992.0000 - val_mae: 2381.3059\n",
            "Epoch 114/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2517.9881 - mse: 38454952.0000 - mae: 2517.9880 - val_loss: 2384.6509 - val_mse: 33630916.0000 - val_mae: 2384.6509\n",
            "Epoch 115/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2518.0902 - mse: 38634016.0000 - mae: 2518.0903 - val_loss: 2392.1036 - val_mse: 33493076.0000 - val_mae: 2392.1035\n",
            "Epoch 116/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2516.6110 - mse: 38506904.0000 - mae: 2516.6108 - val_loss: 2389.6056 - val_mse: 33480562.0000 - val_mae: 2389.6057\n",
            "Epoch 117/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2513.7839 - mse: 38415152.0000 - mae: 2513.7839 - val_loss: 2392.3372 - val_mse: 33425716.0000 - val_mae: 2392.3372\n",
            "Epoch 118/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2521.8154 - mse: 38533320.0000 - mae: 2521.8154 - val_loss: 2391.5375 - val_mse: 33406804.0000 - val_mae: 2391.5376\n",
            "Epoch 119/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2513.9033 - mse: 38333184.0000 - mae: 2513.9033 - val_loss: 2387.0724 - val_mse: 33428802.0000 - val_mae: 2387.0723\n",
            "Epoch 120/500\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 2504.6914 - mse: 38343428.0000 - mae: 2504.6914 - val_loss: 2386.9241 - val_mse: 33409880.0000 - val_mae: 2386.9241\n",
            "Epoch 121/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2509.4682 - mse: 38456132.0000 - mae: 2509.4683 - val_loss: 2382.7425 - val_mse: 33424776.0000 - val_mae: 2382.7424\n",
            "Epoch 122/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2509.3147 - mse: 38395924.0000 - mae: 2509.3149 - val_loss: 2383.0931 - val_mse: 33390600.0000 - val_mae: 2383.0933\n",
            "Epoch 123/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2514.9005 - mse: 38357824.0000 - mae: 2514.9004 - val_loss: 2376.7320 - val_mse: 33462334.0000 - val_mae: 2376.7319\n",
            "Epoch 124/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2510.9982 - mse: 38557184.0000 - mae: 2510.9983 - val_loss: 2396.0134 - val_mse: 33193128.0000 - val_mae: 2396.0134\n",
            "Epoch 125/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2510.3486 - mse: 38104084.0000 - mae: 2510.3486 - val_loss: 2371.4971 - val_mse: 33479478.0000 - val_mae: 2371.4971\n",
            "Epoch 126/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2500.7696 - mse: 38215788.0000 - mae: 2500.7695 - val_loss: 2372.3370 - val_mse: 33393906.0000 - val_mae: 2372.3372\n",
            "Epoch 127/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2505.0398 - mse: 38155824.0000 - mae: 2505.0398 - val_loss: 2368.5699 - val_mse: 33456186.0000 - val_mae: 2368.5698\n",
            "Epoch 128/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2505.0866 - mse: 38270928.0000 - mae: 2505.0867 - val_loss: 2379.1330 - val_mse: 33210514.0000 - val_mae: 2379.1328\n",
            "Epoch 129/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2506.3267 - mse: 37981880.0000 - mae: 2506.3267 - val_loss: 2377.6852 - val_mse: 33191300.0000 - val_mae: 2377.6851\n",
            "Epoch 130/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2514.0818 - mse: 38188496.0000 - mae: 2514.0818 - val_loss: 2373.5256 - val_mse: 33230534.0000 - val_mae: 2373.5254\n",
            "Epoch 131/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2501.1318 - mse: 38068364.0000 - mae: 2501.1316 - val_loss: 2372.3352 - val_mse: 33213638.0000 - val_mae: 2372.3352\n",
            "Epoch 132/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2500.4849 - mse: 38059680.0000 - mae: 2500.4849 - val_loss: 2365.5748 - val_mse: 33315192.0000 - val_mae: 2365.5747\n",
            "Epoch 133/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2508.5434 - mse: 38062368.0000 - mae: 2508.5437 - val_loss: 2371.4316 - val_mse: 33170546.0000 - val_mae: 2371.4316\n",
            "Epoch 134/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2501.2806 - mse: 38110836.0000 - mae: 2501.2808 - val_loss: 2370.8225 - val_mse: 33144938.0000 - val_mae: 2370.8223\n",
            "Epoch 135/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2492.8882 - mse: 37816052.0000 - mae: 2492.8879 - val_loss: 2377.2988 - val_mse: 33010512.0000 - val_mae: 2377.2983\n",
            "Epoch 136/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2510.6855 - mse: 38069260.0000 - mae: 2510.6853 - val_loss: 2366.8718 - val_mse: 33153718.0000 - val_mae: 2366.8718\n",
            "Epoch 137/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2503.1488 - mse: 37890820.0000 - mae: 2503.1487 - val_loss: 2370.4488 - val_mse: 33070790.0000 - val_mae: 2370.4490\n",
            "Epoch 138/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2493.5596 - mse: 37794500.0000 - mae: 2493.5596 - val_loss: 2364.8049 - val_mse: 33138506.0000 - val_mae: 2364.8047\n",
            "Epoch 139/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2509.5540 - mse: 38092084.0000 - mae: 2509.5537 - val_loss: 2371.7572 - val_mse: 33003740.0000 - val_mae: 2371.7571\n",
            "Epoch 140/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2494.1178 - mse: 37606440.0000 - mae: 2494.1177 - val_loss: 2363.8711 - val_mse: 33085638.0000 - val_mae: 2363.8711\n",
            "Epoch 141/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2493.2453 - mse: 37778172.0000 - mae: 2493.2451 - val_loss: 2363.9989 - val_mse: 33067070.0000 - val_mae: 2363.9990\n",
            "Epoch 142/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2489.0609 - mse: 37737804.0000 - mae: 2489.0608 - val_loss: 2361.6519 - val_mse: 33073616.0000 - val_mae: 2361.6521\n",
            "Epoch 143/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2503.7421 - mse: 37973612.0000 - mae: 2503.7419 - val_loss: 2363.5605 - val_mse: 32998426.0000 - val_mae: 2363.5608\n",
            "Epoch 144/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2493.9813 - mse: 37871648.0000 - mae: 2493.9812 - val_loss: 2368.5910 - val_mse: 32900074.0000 - val_mae: 2368.5911\n",
            "Epoch 145/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2498.4501 - mse: 37822716.0000 - mae: 2498.4502 - val_loss: 2356.7215 - val_mse: 33078436.0000 - val_mae: 2356.7214\n",
            "Epoch 146/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2492.8630 - mse: 37802776.0000 - mae: 2492.8630 - val_loss: 2359.8028 - val_mse: 32960530.0000 - val_mae: 2359.8027\n",
            "Epoch 147/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2482.6434 - mse: 37447416.0000 - mae: 2482.6433 - val_loss: 2359.9812 - val_mse: 32929686.0000 - val_mae: 2359.9810\n",
            "Epoch 148/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2495.7300 - mse: 37611280.0000 - mae: 2495.7300 - val_loss: 2369.0205 - val_mse: 32776886.0000 - val_mae: 2369.0205\n",
            "Epoch 149/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2492.1114 - mse: 37414424.0000 - mae: 2492.1116 - val_loss: 2358.0065 - val_mse: 32889932.0000 - val_mae: 2358.0066\n",
            "Epoch 150/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2495.7668 - mse: 37534380.0000 - mae: 2495.7668 - val_loss: 2353.5612 - val_mse: 32967050.0000 - val_mae: 2353.5613\n",
            "Epoch 151/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2474.6948 - mse: 37454296.0000 - mae: 2474.6948 - val_loss: 2360.1511 - val_mse: 32782172.0000 - val_mae: 2360.1514\n",
            "Epoch 152/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2490.9793 - mse: 37601392.0000 - mae: 2490.9792 - val_loss: 2351.2707 - val_mse: 32951022.0000 - val_mae: 2351.2705\n",
            "Epoch 153/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2492.6504 - mse: 37544328.0000 - mae: 2492.6501 - val_loss: 2350.9029 - val_mse: 32934568.0000 - val_mae: 2350.9028\n",
            "Epoch 154/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2485.2916 - mse: 37392392.0000 - mae: 2485.2915 - val_loss: 2349.6360 - val_mse: 32983648.0000 - val_mae: 2349.6362\n",
            "Epoch 155/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2494.4606 - mse: 37574716.0000 - mae: 2494.4607 - val_loss: 2357.7071 - val_mse: 32720092.0000 - val_mae: 2357.7073\n",
            "Epoch 156/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2479.2207 - mse: 37278176.0000 - mae: 2479.2207 - val_loss: 2351.6950 - val_mse: 32810152.0000 - val_mae: 2351.6951\n",
            "Epoch 157/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2484.3604 - mse: 37507572.0000 - mae: 2484.3604 - val_loss: 2353.8197 - val_mse: 32704604.0000 - val_mae: 2353.8196\n",
            "Epoch 158/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2488.2266 - mse: 37360172.0000 - mae: 2488.2266 - val_loss: 2350.8333 - val_mse: 32735002.0000 - val_mae: 2350.8333\n",
            "Epoch 159/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2487.1997 - mse: 37505828.0000 - mae: 2487.1997 - val_loss: 2349.0256 - val_mse: 32737254.0000 - val_mae: 2349.0256\n",
            "Epoch 160/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2481.0502 - mse: 37421736.0000 - mae: 2481.0500 - val_loss: 2346.4101 - val_mse: 32747172.0000 - val_mae: 2346.4099\n",
            "Epoch 161/500\n",
            "1130/1130 [==============================] - 0s 59us/step - loss: 2488.9289 - mse: 37486776.0000 - mae: 2488.9287 - val_loss: 2349.4892 - val_mse: 32641356.0000 - val_mae: 2349.4893\n",
            "Epoch 162/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2484.2350 - mse: 37459212.0000 - mae: 2484.2349 - val_loss: 2343.7654 - val_mse: 32748862.0000 - val_mae: 2343.7656\n",
            "Epoch 163/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2479.8559 - mse: 37505176.0000 - mae: 2479.8560 - val_loss: 2354.3259 - val_mse: 32509532.0000 - val_mae: 2354.3259\n",
            "Epoch 164/500\n",
            "1130/1130 [==============================] - 0s 61us/step - loss: 2476.7700 - mse: 37111808.0000 - mae: 2476.7700 - val_loss: 2341.0454 - val_mse: 32724228.0000 - val_mae: 2341.0457\n",
            "Epoch 165/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2471.8601 - mse: 37133432.0000 - mae: 2471.8601 - val_loss: 2339.6870 - val_mse: 32752216.0000 - val_mae: 2339.6870\n",
            "Epoch 166/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2468.9853 - mse: 37240092.0000 - mae: 2468.9854 - val_loss: 2339.3489 - val_mse: 32623242.0000 - val_mae: 2339.3491\n",
            "Epoch 167/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2482.2255 - mse: 37362244.0000 - mae: 2482.2256 - val_loss: 2343.9952 - val_mse: 32449996.0000 - val_mae: 2343.9954\n",
            "Epoch 168/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2480.3301 - mse: 37087688.0000 - mae: 2480.3301 - val_loss: 2338.1518 - val_mse: 32530868.0000 - val_mae: 2338.1519\n",
            "Epoch 169/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2491.4511 - mse: 37392616.0000 - mae: 2491.4514 - val_loss: 2339.8545 - val_mse: 32509846.0000 - val_mae: 2339.8545\n",
            "Epoch 170/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2477.2950 - mse: 37276220.0000 - mae: 2477.2952 - val_loss: 2335.7431 - val_mse: 32584064.0000 - val_mae: 2335.7432\n",
            "Epoch 171/500\n",
            "1130/1130 [==============================] - 0s 60us/step - loss: 2481.3912 - mse: 37193992.0000 - mae: 2481.3911 - val_loss: 2335.2803 - val_mse: 32528274.0000 - val_mae: 2335.2803\n",
            "Epoch 172/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2476.1717 - mse: 36921556.0000 - mae: 2476.1716 - val_loss: 2334.1171 - val_mse: 32677530.0000 - val_mae: 2334.1172\n",
            "Epoch 173/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2466.8355 - mse: 37133180.0000 - mae: 2466.8357 - val_loss: 2333.4454 - val_mse: 32484088.0000 - val_mae: 2333.4456\n",
            "Epoch 174/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2476.3977 - mse: 37154472.0000 - mae: 2476.3975 - val_loss: 2331.0807 - val_mse: 32511796.0000 - val_mae: 2331.0806\n",
            "Epoch 175/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2474.1147 - mse: 37082012.0000 - mae: 2474.1145 - val_loss: 2335.2518 - val_mse: 32346928.0000 - val_mae: 2335.2520\n",
            "Epoch 176/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2458.2217 - mse: 36724480.0000 - mae: 2458.2217 - val_loss: 2329.8314 - val_mse: 32530640.0000 - val_mae: 2329.8315\n",
            "Epoch 177/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2469.1707 - mse: 37063472.0000 - mae: 2469.1709 - val_loss: 2328.1833 - val_mse: 32454554.0000 - val_mae: 2328.1833\n",
            "Epoch 178/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2466.8295 - mse: 36744812.0000 - mae: 2466.8293 - val_loss: 2326.9324 - val_mse: 32418892.0000 - val_mae: 2326.9324\n",
            "Epoch 179/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2458.5184 - mse: 36750136.0000 - mae: 2458.5186 - val_loss: 2329.6694 - val_mse: 32254094.0000 - val_mae: 2329.6692\n",
            "Epoch 180/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2467.0909 - mse: 37070296.0000 - mae: 2467.0908 - val_loss: 2328.8003 - val_mse: 32231066.0000 - val_mae: 2328.8003\n",
            "Epoch 181/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2463.9821 - mse: 36747532.0000 - mae: 2463.9822 - val_loss: 2326.0432 - val_mse: 32455778.0000 - val_mae: 2326.0435\n",
            "Epoch 182/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2472.9039 - mse: 36793332.0000 - mae: 2472.9041 - val_loss: 2326.1002 - val_mse: 32216078.0000 - val_mae: 2326.1003\n",
            "Epoch 183/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2468.3009 - mse: 36648708.0000 - mae: 2468.3008 - val_loss: 2322.6570 - val_mse: 32268318.0000 - val_mae: 2322.6572\n",
            "Epoch 184/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2456.3916 - mse: 36801816.0000 - mae: 2456.3916 - val_loss: 2321.4809 - val_mse: 32269038.0000 - val_mae: 2321.4807\n",
            "Epoch 185/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2461.8116 - mse: 36593780.0000 - mae: 2461.8115 - val_loss: 2320.1647 - val_mse: 32265136.0000 - val_mae: 2320.1646\n",
            "Epoch 186/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2463.6698 - mse: 36958136.0000 - mae: 2463.6697 - val_loss: 2322.2154 - val_mse: 32179804.0000 - val_mae: 2322.2153\n",
            "Epoch 187/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2458.5426 - mse: 36191744.0000 - mae: 2458.5425 - val_loss: 2319.5816 - val_mse: 32269834.0000 - val_mae: 2319.5818\n",
            "Epoch 188/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2461.4115 - mse: 36678688.0000 - mae: 2461.4116 - val_loss: 2318.8332 - val_mse: 32260600.0000 - val_mae: 2318.8333\n",
            "Epoch 189/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2476.2817 - mse: 36835444.0000 - mae: 2476.2817 - val_loss: 2319.5709 - val_mse: 32281994.0000 - val_mae: 2319.5710\n",
            "Epoch 190/500\n",
            "1130/1130 [==============================] - 0s 63us/step - loss: 2459.0146 - mse: 36615728.0000 - mae: 2459.0146 - val_loss: 2322.8260 - val_mse: 32339190.0000 - val_mae: 2322.8259\n",
            "Epoch 191/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2469.4003 - mse: 36500384.0000 - mae: 2469.4001 - val_loss: 2316.4635 - val_mse: 32185376.0000 - val_mae: 2316.4634\n",
            "Epoch 192/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2456.2758 - mse: 36520512.0000 - mae: 2456.2756 - val_loss: 2321.5530 - val_mse: 32321576.0000 - val_mae: 2321.5530\n",
            "Epoch 193/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2461.3468 - mse: 36671036.0000 - mae: 2461.3467 - val_loss: 2313.8504 - val_mse: 32100326.0000 - val_mae: 2313.8503\n",
            "Epoch 194/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2460.7625 - mse: 36594832.0000 - mae: 2460.7625 - val_loss: 2313.9302 - val_mse: 32126544.0000 - val_mae: 2313.9304\n",
            "Epoch 195/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2455.0538 - mse: 36395680.0000 - mae: 2455.0537 - val_loss: 2313.2641 - val_mse: 32120316.0000 - val_mae: 2313.2642\n",
            "Epoch 196/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2452.9469 - mse: 36462684.0000 - mae: 2452.9470 - val_loss: 2327.4301 - val_mse: 32342674.0000 - val_mae: 2327.4302\n",
            "Epoch 197/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2465.9821 - mse: 36664224.0000 - mae: 2465.9819 - val_loss: 2310.3932 - val_mse: 31989600.0000 - val_mae: 2310.3933\n",
            "Epoch 198/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2452.8985 - mse: 36491812.0000 - mae: 2452.8984 - val_loss: 2314.8704 - val_mse: 31781116.0000 - val_mae: 2314.8704\n",
            "Epoch 199/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2456.5413 - mse: 36183360.0000 - mae: 2456.5413 - val_loss: 2309.7014 - val_mse: 32009300.0000 - val_mae: 2309.7017\n",
            "Epoch 200/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2446.8257 - mse: 36290336.0000 - mae: 2446.8257 - val_loss: 2312.7401 - val_mse: 32082026.0000 - val_mae: 2312.7400\n",
            "Epoch 201/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2444.9956 - mse: 36347884.0000 - mae: 2444.9956 - val_loss: 2307.4030 - val_mse: 31937700.0000 - val_mae: 2307.4028\n",
            "Epoch 202/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2443.6437 - mse: 36234936.0000 - mae: 2443.6438 - val_loss: 2307.3724 - val_mse: 31919956.0000 - val_mae: 2307.3726\n",
            "Epoch 203/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2452.9888 - mse: 36285268.0000 - mae: 2452.9890 - val_loss: 2317.4862 - val_mse: 32089856.0000 - val_mae: 2317.4861\n",
            "Epoch 204/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2444.8221 - mse: 36342940.0000 - mae: 2444.8220 - val_loss: 2311.0598 - val_mse: 31993352.0000 - val_mae: 2311.0598\n",
            "Epoch 205/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2462.9040 - mse: 36469096.0000 - mae: 2462.9041 - val_loss: 2309.2882 - val_mse: 31962386.0000 - val_mae: 2309.2881\n",
            "Epoch 206/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2437.0551 - mse: 36290472.0000 - mae: 2437.0554 - val_loss: 2308.6514 - val_mse: 31940082.0000 - val_mae: 2308.6516\n",
            "Epoch 207/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2447.2165 - mse: 36093464.0000 - mae: 2447.2166 - val_loss: 2305.6784 - val_mse: 31862582.0000 - val_mae: 2305.6785\n",
            "Epoch 208/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2446.4649 - mse: 36177132.0000 - mae: 2446.4651 - val_loss: 2302.1521 - val_mse: 31753366.0000 - val_mae: 2302.1523\n",
            "Epoch 209/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2455.5742 - mse: 36301240.0000 - mae: 2455.5742 - val_loss: 2300.7116 - val_mse: 31609614.0000 - val_mae: 2300.7117\n",
            "Epoch 210/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2442.6409 - mse: 36006492.0000 - mae: 2442.6409 - val_loss: 2301.7925 - val_mse: 31728318.0000 - val_mae: 2301.7927\n",
            "Epoch 211/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2444.0829 - mse: 36045244.0000 - mae: 2444.0830 - val_loss: 2300.4687 - val_mse: 31667006.0000 - val_mae: 2300.4685\n",
            "Epoch 212/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2448.5828 - mse: 36277152.0000 - mae: 2448.5828 - val_loss: 2298.6944 - val_mse: 31569214.0000 - val_mae: 2298.6943\n",
            "Epoch 213/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2449.1825 - mse: 35929636.0000 - mae: 2449.1826 - val_loss: 2297.9200 - val_mse: 31531356.0000 - val_mae: 2297.9202\n",
            "Epoch 214/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2442.5818 - mse: 35770432.0000 - mae: 2442.5818 - val_loss: 2303.8372 - val_mse: 31714592.0000 - val_mae: 2303.8372\n",
            "Epoch 215/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2440.8889 - mse: 35951040.0000 - mae: 2440.8889 - val_loss: 2297.6917 - val_mse: 31556728.0000 - val_mae: 2297.6917\n",
            "Epoch 216/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2441.7078 - mse: 35958008.0000 - mae: 2441.7078 - val_loss: 2309.7224 - val_mse: 31765438.0000 - val_mae: 2309.7227\n",
            "Epoch 217/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2434.8840 - mse: 35969512.0000 - mae: 2434.8840 - val_loss: 2309.3008 - val_mse: 31735544.0000 - val_mae: 2309.3008\n",
            "Epoch 218/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2430.3157 - mse: 35842304.0000 - mae: 2430.3157 - val_loss: 2295.6818 - val_mse: 31463778.0000 - val_mae: 2295.6819\n",
            "Epoch 219/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2425.9194 - mse: 35400748.0000 - mae: 2425.9194 - val_loss: 2297.4113 - val_mse: 31510646.0000 - val_mae: 2297.4111\n",
            "Epoch 220/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2447.7358 - mse: 36186420.0000 - mae: 2447.7358 - val_loss: 2298.6462 - val_mse: 31523580.0000 - val_mae: 2298.6462\n",
            "Epoch 221/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2448.7762 - mse: 35941632.0000 - mae: 2448.7761 - val_loss: 2312.9613 - val_mse: 31696160.0000 - val_mae: 2312.9612\n",
            "Epoch 222/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2436.8220 - mse: 35921336.0000 - mae: 2436.8220 - val_loss: 2304.9861 - val_mse: 31596380.0000 - val_mae: 2304.9863\n",
            "Epoch 223/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2443.4275 - mse: 36112272.0000 - mae: 2443.4275 - val_loss: 2294.5353 - val_mse: 31389308.0000 - val_mae: 2294.5352\n",
            "Epoch 224/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2442.5015 - mse: 35906840.0000 - mae: 2442.5015 - val_loss: 2295.7237 - val_mse: 31424538.0000 - val_mae: 2295.7239\n",
            "Epoch 225/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2433.7244 - mse: 35795056.0000 - mae: 2433.7244 - val_loss: 2299.5141 - val_mse: 31479460.0000 - val_mae: 2299.5139\n",
            "Epoch 226/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2424.7182 - mse: 35775064.0000 - mae: 2424.7183 - val_loss: 2308.9605 - val_mse: 31571698.0000 - val_mae: 2308.9604\n",
            "Epoch 227/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2432.7576 - mse: 35743924.0000 - mae: 2432.7576 - val_loss: 2294.4533 - val_mse: 31351296.0000 - val_mae: 2294.4534\n",
            "Epoch 228/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2440.9093 - mse: 35709192.0000 - mae: 2440.9092 - val_loss: 2300.5388 - val_mse: 31452336.0000 - val_mae: 2300.5388\n",
            "Epoch 229/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2416.9424 - mse: 35332128.0000 - mae: 2416.9424 - val_loss: 2314.8786 - val_mse: 31572298.0000 - val_mae: 2314.8789\n",
            "Epoch 230/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2428.2363 - mse: 35690868.0000 - mae: 2428.2363 - val_loss: 2298.4393 - val_mse: 31382766.0000 - val_mae: 2298.4392\n",
            "Epoch 231/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2438.6264 - mse: 35763480.0000 - mae: 2438.6265 - val_loss: 2291.7339 - val_mse: 31239106.0000 - val_mae: 2291.7339\n",
            "Epoch 232/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2443.1451 - mse: 35485484.0000 - mae: 2443.1450 - val_loss: 2292.1544 - val_mse: 31240934.0000 - val_mae: 2292.1545\n",
            "Epoch 233/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2433.7699 - mse: 35568064.0000 - mae: 2433.7700 - val_loss: 2313.0508 - val_mse: 31481458.0000 - val_mae: 2313.0508\n",
            "Epoch 234/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2428.7681 - mse: 35499184.0000 - mae: 2428.7681 - val_loss: 2314.1124 - val_mse: 31473824.0000 - val_mae: 2314.1123\n",
            "Epoch 235/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2424.4860 - mse: 35322664.0000 - mae: 2424.4861 - val_loss: 2291.2527 - val_mse: 31179366.0000 - val_mae: 2291.2529\n",
            "Epoch 236/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2413.3028 - mse: 35055288.0000 - mae: 2413.3030 - val_loss: 2288.8566 - val_mse: 30956720.0000 - val_mae: 2288.8567\n",
            "Epoch 237/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2416.2958 - mse: 35112828.0000 - mae: 2416.2959 - val_loss: 2292.7098 - val_mse: 31179740.0000 - val_mae: 2292.7097\n",
            "Epoch 238/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2409.4711 - mse: 35102168.0000 - mae: 2409.4709 - val_loss: 2301.0136 - val_mse: 31279374.0000 - val_mae: 2301.0139\n",
            "Epoch 239/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2430.2844 - mse: 35608792.0000 - mae: 2430.2844 - val_loss: 2303.4211 - val_mse: 31301928.0000 - val_mae: 2303.4209\n",
            "Epoch 240/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2413.7445 - mse: 35209332.0000 - mae: 2413.7444 - val_loss: 2309.4499 - val_mse: 31348092.0000 - val_mae: 2309.4497\n",
            "Epoch 241/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2423.9138 - mse: 35370920.0000 - mae: 2423.9138 - val_loss: 2311.5414 - val_mse: 31351010.0000 - val_mae: 2311.5413\n",
            "Epoch 242/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2422.5786 - mse: 35222328.0000 - mae: 2422.5786 - val_loss: 2312.6081 - val_mse: 31353798.0000 - val_mae: 2312.6082\n",
            "Epoch 243/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2423.4100 - mse: 35381556.0000 - mae: 2423.4099 - val_loss: 2294.1938 - val_mse: 31152124.0000 - val_mae: 2294.1938\n",
            "Epoch 244/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2411.6465 - mse: 35145244.0000 - mae: 2411.6467 - val_loss: 2293.8179 - val_mse: 31140356.0000 - val_mae: 2293.8179\n",
            "Epoch 245/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2428.3817 - mse: 35195124.0000 - mae: 2428.3816 - val_loss: 2294.3907 - val_mse: 31128774.0000 - val_mae: 2294.3909\n",
            "Epoch 246/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2413.7003 - mse: 35078024.0000 - mae: 2413.7002 - val_loss: 2286.8849 - val_mse: 30979872.0000 - val_mae: 2286.8850\n",
            "Epoch 247/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2420.2783 - mse: 35285880.0000 - mae: 2420.2783 - val_loss: 2301.8915 - val_mse: 31186506.0000 - val_mae: 2301.8914\n",
            "Epoch 248/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2424.9239 - mse: 35128544.0000 - mae: 2424.9238 - val_loss: 2292.6444 - val_mse: 31047186.0000 - val_mae: 2292.6445\n",
            "Epoch 249/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2412.5578 - mse: 34884860.0000 - mae: 2412.5579 - val_loss: 2282.9083 - val_mse: 30777354.0000 - val_mae: 2282.9082\n",
            "Epoch 250/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2423.9344 - mse: 34689968.0000 - mae: 2423.9346 - val_loss: 2304.5875 - val_mse: 31139734.0000 - val_mae: 2304.5874\n",
            "Epoch 251/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2422.2890 - mse: 35120364.0000 - mae: 2422.2888 - val_loss: 2309.7787 - val_mse: 31170882.0000 - val_mae: 2309.7788\n",
            "Epoch 252/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2410.8173 - mse: 35160564.0000 - mae: 2410.8171 - val_loss: 2307.1964 - val_mse: 31130924.0000 - val_mae: 2307.1965\n",
            "Epoch 253/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2415.0452 - mse: 34892356.0000 - mae: 2415.0452 - val_loss: 2288.5619 - val_mse: 30911540.0000 - val_mae: 2288.5618\n",
            "Epoch 254/500\n",
            "1130/1130 [==============================] - 0s 64us/step - loss: 2409.1282 - mse: 34679552.0000 - mae: 2409.1282 - val_loss: 2295.7847 - val_mse: 30979658.0000 - val_mae: 2295.7847\n",
            "Epoch 255/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2411.0397 - mse: 35007148.0000 - mae: 2411.0398 - val_loss: 2313.5575 - val_mse: 31131018.0000 - val_mae: 2313.5579\n",
            "Epoch 256/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2405.9327 - mse: 34868888.0000 - mae: 2405.9326 - val_loss: 2285.0633 - val_mse: 30830296.0000 - val_mae: 2285.0635\n",
            "Epoch 257/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2419.1029 - mse: 34969820.0000 - mae: 2419.1028 - val_loss: 2305.3641 - val_mse: 31062026.0000 - val_mae: 2305.3640\n",
            "Epoch 258/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2412.9970 - mse: 34555492.0000 - mae: 2412.9968 - val_loss: 2297.4118 - val_mse: 30961376.0000 - val_mae: 2297.4119\n",
            "Epoch 259/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2407.9071 - mse: 34706012.0000 - mae: 2407.9070 - val_loss: 2322.9249 - val_mse: 31169990.0000 - val_mae: 2322.9248\n",
            "Epoch 260/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2421.3795 - mse: 35309688.0000 - mae: 2421.3794 - val_loss: 2304.1739 - val_mse: 31014224.0000 - val_mae: 2304.1738\n",
            "Epoch 261/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2427.4174 - mse: 35093272.0000 - mae: 2427.4175 - val_loss: 2295.5127 - val_mse: 30927624.0000 - val_mae: 2295.5127\n",
            "Epoch 262/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2409.2247 - mse: 34691692.0000 - mae: 2409.2249 - val_loss: 2315.2967 - val_mse: 31042334.0000 - val_mae: 2315.2966\n",
            "Epoch 263/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2408.3315 - mse: 34920968.0000 - mae: 2408.3315 - val_loss: 2294.8078 - val_mse: 30859158.0000 - val_mae: 2294.8081\n",
            "Epoch 264/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2410.6955 - mse: 34947736.0000 - mae: 2410.6953 - val_loss: 2285.8341 - val_mse: 30759962.0000 - val_mae: 2285.8340\n",
            "Epoch 265/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2406.7746 - mse: 34576536.0000 - mae: 2406.7747 - val_loss: 2306.8732 - val_mse: 30966666.0000 - val_mae: 2306.8733\n",
            "Epoch 266/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2404.5963 - mse: 34880944.0000 - mae: 2404.5962 - val_loss: 2285.5209 - val_mse: 30732430.0000 - val_mae: 2285.5208\n",
            "Epoch 267/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2399.0702 - mse: 34661260.0000 - mae: 2399.0701 - val_loss: 2279.6829 - val_mse: 30614544.0000 - val_mae: 2279.6829\n",
            "Epoch 268/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2424.5856 - mse: 34900636.0000 - mae: 2424.5859 - val_loss: 2287.7347 - val_mse: 30754922.0000 - val_mae: 2287.7346\n",
            "Epoch 269/500\n",
            "1130/1130 [==============================] - 0s 85us/step - loss: 2413.6148 - mse: 34974680.0000 - mae: 2413.6145 - val_loss: 2297.4075 - val_mse: 30850538.0000 - val_mae: 2297.4075\n",
            "Epoch 270/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2409.9635 - mse: 34877044.0000 - mae: 2409.9634 - val_loss: 2287.0529 - val_mse: 30715616.0000 - val_mae: 2287.0527\n",
            "Epoch 271/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2389.9497 - mse: 34004192.0000 - mae: 2389.9497 - val_loss: 2303.0408 - val_mse: 30848186.0000 - val_mae: 2303.0408\n",
            "Epoch 272/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2405.8815 - mse: 34775036.0000 - mae: 2405.8816 - val_loss: 2290.5449 - val_mse: 30718880.0000 - val_mae: 2290.5449\n",
            "Epoch 273/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2409.8733 - mse: 34793332.0000 - mae: 2409.8733 - val_loss: 2278.7278 - val_mse: 30541426.0000 - val_mae: 2278.7280\n",
            "Epoch 274/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2402.5978 - mse: 34525868.0000 - mae: 2402.5979 - val_loss: 2297.0788 - val_mse: 30740590.0000 - val_mae: 2297.0786\n",
            "Epoch 275/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2409.2578 - mse: 34606412.0000 - mae: 2409.2578 - val_loss: 2288.9945 - val_mse: 30654346.0000 - val_mae: 2288.9946\n",
            "Epoch 276/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2414.1566 - mse: 34618444.0000 - mae: 2414.1567 - val_loss: 2295.0238 - val_mse: 30697278.0000 - val_mae: 2295.0237\n",
            "Epoch 277/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2400.7061 - mse: 34416112.0000 - mae: 2400.7063 - val_loss: 2293.1549 - val_mse: 30662318.0000 - val_mae: 2293.1548\n",
            "Epoch 278/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2393.3967 - mse: 34425896.0000 - mae: 2393.3967 - val_loss: 2296.7261 - val_mse: 30689568.0000 - val_mae: 2296.7263\n",
            "Epoch 279/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2408.9665 - mse: 34380276.0000 - mae: 2408.9663 - val_loss: 2287.5062 - val_mse: 30588450.0000 - val_mae: 2287.5061\n",
            "Epoch 280/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2412.2957 - mse: 34405972.0000 - mae: 2412.2957 - val_loss: 2276.5188 - val_mse: 30425128.0000 - val_mae: 2276.5186\n",
            "Epoch 281/500\n",
            "1130/1130 [==============================] - 0s 83us/step - loss: 2392.1092 - mse: 34557924.0000 - mae: 2392.1091 - val_loss: 2314.5528 - val_mse: 30765410.0000 - val_mae: 2314.5527\n",
            "Epoch 282/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2405.9336 - mse: 34525664.0000 - mae: 2405.9336 - val_loss: 2306.2498 - val_mse: 30699560.0000 - val_mae: 2306.2498\n",
            "Epoch 283/500\n",
            "1130/1130 [==============================] - 0s 83us/step - loss: 2395.1041 - mse: 34426352.0000 - mae: 2395.1042 - val_loss: 2298.0107 - val_mse: 30622866.0000 - val_mae: 2298.0105\n",
            "Epoch 284/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2400.2323 - mse: 34317156.0000 - mae: 2400.2324 - val_loss: 2298.8825 - val_mse: 30608858.0000 - val_mae: 2298.8826\n",
            "Epoch 285/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2400.7394 - mse: 34405440.0000 - mae: 2400.7395 - val_loss: 2304.4533 - val_mse: 30657428.0000 - val_mae: 2304.4534\n",
            "Epoch 286/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2401.3083 - mse: 34363180.0000 - mae: 2401.3081 - val_loss: 2290.6853 - val_mse: 30514614.0000 - val_mae: 2290.6855\n",
            "Epoch 287/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2395.7265 - mse: 34231268.0000 - mae: 2395.7266 - val_loss: 2321.5534 - val_mse: 30746854.0000 - val_mae: 2321.5535\n",
            "Epoch 288/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2396.8269 - mse: 34270664.0000 - mae: 2396.8267 - val_loss: 2320.0960 - val_mse: 30739404.0000 - val_mae: 2320.0959\n",
            "Epoch 289/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2401.2276 - mse: 34203016.0000 - mae: 2401.2275 - val_loss: 2305.5958 - val_mse: 30639744.0000 - val_mae: 2305.5955\n",
            "Epoch 290/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2410.8264 - mse: 34557596.0000 - mae: 2410.8264 - val_loss: 2298.5271 - val_mse: 30569008.0000 - val_mae: 2298.5271\n",
            "Epoch 291/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2399.6372 - mse: 34209784.0000 - mae: 2399.6372 - val_loss: 2283.2382 - val_mse: 30428042.0000 - val_mae: 2283.2383\n",
            "Epoch 292/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2382.9148 - mse: 33807084.0000 - mae: 2382.9148 - val_loss: 2299.3956 - val_mse: 30568242.0000 - val_mae: 2299.3955\n",
            "Epoch 293/500\n",
            "1130/1130 [==============================] - 0s 90us/step - loss: 2412.3877 - mse: 34355444.0000 - mae: 2412.3877 - val_loss: 2330.8132 - val_mse: 30789592.0000 - val_mae: 2330.8135\n",
            "Epoch 294/500\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 2405.5981 - mse: 34663980.0000 - mae: 2405.5981 - val_loss: 2280.3623 - val_mse: 30367006.0000 - val_mae: 2280.3625\n",
            "Epoch 295/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2408.2040 - mse: 34283708.0000 - mae: 2408.2041 - val_loss: 2302.1114 - val_mse: 30560126.0000 - val_mae: 2302.1113\n",
            "Epoch 296/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2391.2377 - mse: 34306244.0000 - mae: 2391.2375 - val_loss: 2286.5296 - val_mse: 30428534.0000 - val_mae: 2286.5298\n",
            "Epoch 297/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2396.8077 - mse: 33963832.0000 - mae: 2396.8076 - val_loss: 2275.8297 - val_mse: 30296980.0000 - val_mae: 2275.8296\n",
            "Epoch 298/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2399.2234 - mse: 34216696.0000 - mae: 2399.2234 - val_loss: 2270.7195 - val_mse: 30205366.0000 - val_mae: 2270.7197\n",
            "Epoch 299/500\n",
            "1130/1130 [==============================] - 0s 62us/step - loss: 2399.8405 - mse: 33784732.0000 - mae: 2399.8406 - val_loss: 2297.6192 - val_mse: 30493660.0000 - val_mae: 2297.6191\n",
            "Epoch 300/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2380.9977 - mse: 34107400.0000 - mae: 2380.9976 - val_loss: 2296.1435 - val_mse: 30483000.0000 - val_mae: 2296.1436\n",
            "Epoch 301/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2378.1703 - mse: 33951420.0000 - mae: 2378.1704 - val_loss: 2279.2229 - val_mse: 30306632.0000 - val_mae: 2279.2227\n",
            "Epoch 302/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2382.7660 - mse: 33907044.0000 - mae: 2382.7661 - val_loss: 2276.6720 - val_mse: 30247570.0000 - val_mae: 2276.6719\n",
            "Epoch 303/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2406.2425 - mse: 34392856.0000 - mae: 2406.2427 - val_loss: 2285.8804 - val_mse: 30319520.0000 - val_mae: 2285.8804\n",
            "Epoch 304/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2401.8591 - mse: 34240328.0000 - mae: 2401.8589 - val_loss: 2273.1249 - val_mse: 30182412.0000 - val_mae: 2273.1250\n",
            "Epoch 305/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2387.3096 - mse: 33998820.0000 - mae: 2387.3096 - val_loss: 2290.3372 - val_mse: 30348112.0000 - val_mae: 2290.3372\n",
            "Epoch 306/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2394.2772 - mse: 34117340.0000 - mae: 2394.2771 - val_loss: 2286.7313 - val_mse: 30288212.0000 - val_mae: 2286.7312\n",
            "Epoch 307/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2382.9622 - mse: 34007940.0000 - mae: 2382.9622 - val_loss: 2283.9005 - val_mse: 30271694.0000 - val_mae: 2283.9001\n",
            "Epoch 308/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2392.2716 - mse: 33976224.0000 - mae: 2392.2715 - val_loss: 2296.8244 - val_mse: 30359328.0000 - val_mae: 2296.8242\n",
            "Epoch 309/500\n",
            "1130/1130 [==============================] - 0s 66us/step - loss: 2376.3310 - mse: 33410082.0000 - mae: 2376.3311 - val_loss: 2299.6118 - val_mse: 30357806.0000 - val_mae: 2299.6116\n",
            "Epoch 310/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2373.0413 - mse: 33450962.0000 - mae: 2373.0413 - val_loss: 2296.7405 - val_mse: 30315402.0000 - val_mae: 2296.7407\n",
            "Epoch 311/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2387.4329 - mse: 33909568.0000 - mae: 2387.4329 - val_loss: 2290.6760 - val_mse: 30247770.0000 - val_mae: 2290.6758\n",
            "Epoch 312/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2376.1163 - mse: 33797936.0000 - mae: 2376.1165 - val_loss: 2305.5182 - val_mse: 30327950.0000 - val_mae: 2305.5183\n",
            "Epoch 313/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2394.2410 - mse: 34253416.0000 - mae: 2394.2412 - val_loss: 2293.2693 - val_mse: 30232288.0000 - val_mae: 2293.2693\n",
            "Epoch 314/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2379.3149 - mse: 33730680.0000 - mae: 2379.3149 - val_loss: 2279.8990 - val_mse: 30100292.0000 - val_mae: 2279.8987\n",
            "Epoch 315/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2409.5271 - mse: 34213256.0000 - mae: 2409.5271 - val_loss: 2291.3998 - val_mse: 30227502.0000 - val_mae: 2291.3997\n",
            "Epoch 316/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2379.5590 - mse: 33627976.0000 - mae: 2379.5591 - val_loss: 2273.7029 - val_mse: 30062646.0000 - val_mae: 2273.7026\n",
            "Epoch 317/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2398.9066 - mse: 34018268.0000 - mae: 2398.9065 - val_loss: 2286.1320 - val_mse: 30175276.0000 - val_mae: 2286.1318\n",
            "Epoch 318/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2373.0001 - mse: 33719684.0000 - mae: 2373.0000 - val_loss: 2286.1636 - val_mse: 30162070.0000 - val_mae: 2286.1633\n",
            "Epoch 319/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2378.8421 - mse: 33713240.0000 - mae: 2378.8420 - val_loss: 2278.7898 - val_mse: 30092432.0000 - val_mae: 2278.7900\n",
            "Epoch 320/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2376.5667 - mse: 33653452.0000 - mae: 2376.5667 - val_loss: 2273.6340 - val_mse: 30005946.0000 - val_mae: 2273.6340\n",
            "Epoch 321/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2372.9876 - mse: 33703684.0000 - mae: 2372.9878 - val_loss: 2269.4555 - val_mse: 29954706.0000 - val_mae: 2269.4553\n",
            "Epoch 322/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2382.5851 - mse: 33666760.0000 - mae: 2382.5852 - val_loss: 2269.8733 - val_mse: 29947656.0000 - val_mae: 2269.8735\n",
            "Epoch 323/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2371.2067 - mse: 33658664.0000 - mae: 2371.2065 - val_loss: 2293.6876 - val_mse: 30141666.0000 - val_mae: 2293.6877\n",
            "Epoch 324/500\n",
            "1130/1130 [==============================] - 0s 68us/step - loss: 2364.0168 - mse: 33545888.0000 - mae: 2364.0166 - val_loss: 2299.2001 - val_mse: 30188390.0000 - val_mae: 2299.1997\n",
            "Epoch 325/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2384.5225 - mse: 34037476.0000 - mae: 2384.5225 - val_loss: 2292.6959 - val_mse: 30137954.0000 - val_mae: 2292.6960\n",
            "Epoch 326/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2381.8076 - mse: 33746476.0000 - mae: 2381.8076 - val_loss: 2267.4014 - val_mse: 29895872.0000 - val_mae: 2267.4014\n",
            "Epoch 327/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2379.9215 - mse: 33591928.0000 - mae: 2379.9216 - val_loss: 2274.7242 - val_mse: 29954724.0000 - val_mae: 2274.7241\n",
            "Epoch 328/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2391.0177 - mse: 33648176.0000 - mae: 2391.0176 - val_loss: 2311.1263 - val_mse: 30265800.0000 - val_mae: 2311.1262\n",
            "Epoch 329/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2381.0819 - mse: 33932420.0000 - mae: 2381.0818 - val_loss: 2264.2500 - val_mse: 29858038.0000 - val_mae: 2264.2500\n",
            "Epoch 330/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2398.6239 - mse: 33990520.0000 - mae: 2398.6238 - val_loss: 2275.4420 - val_mse: 29987556.0000 - val_mae: 2275.4419\n",
            "Epoch 331/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2380.9949 - mse: 33823560.0000 - mae: 2380.9949 - val_loss: 2285.0398 - val_mse: 30069534.0000 - val_mae: 2285.0398\n",
            "Epoch 332/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2370.2741 - mse: 33648204.0000 - mae: 2370.2744 - val_loss: 2293.2939 - val_mse: 30092926.0000 - val_mae: 2293.2937\n",
            "Epoch 333/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2383.1617 - mse: 33959044.0000 - mae: 2383.1616 - val_loss: 2292.6101 - val_mse: 30077308.0000 - val_mae: 2292.6101\n",
            "Epoch 334/500\n",
            "1130/1130 [==============================] - 0s 67us/step - loss: 2391.1395 - mse: 33944572.0000 - mae: 2391.1394 - val_loss: 2284.9079 - val_mse: 30006148.0000 - val_mae: 2284.9077\n",
            "Epoch 335/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2396.7343 - mse: 34129672.0000 - mae: 2396.7344 - val_loss: 2263.4928 - val_mse: 29807796.0000 - val_mae: 2263.4932\n",
            "Epoch 336/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2378.8927 - mse: 33411616.0000 - mae: 2378.8926 - val_loss: 2284.3580 - val_mse: 29994930.0000 - val_mae: 2284.3582\n",
            "Epoch 337/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2401.9185 - mse: 33944864.0000 - mae: 2401.9187 - val_loss: 2289.2807 - val_mse: 30029682.0000 - val_mae: 2289.2805\n",
            "Epoch 338/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2384.0253 - mse: 33401778.0000 - mae: 2384.0251 - val_loss: 2286.2314 - val_mse: 30013004.0000 - val_mae: 2286.2314\n",
            "Epoch 339/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2378.1044 - mse: 33484714.0000 - mae: 2378.1045 - val_loss: 2315.7946 - val_mse: 30224030.0000 - val_mae: 2315.7944\n",
            "Epoch 340/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2370.7715 - mse: 33436786.0000 - mae: 2370.7715 - val_loss: 2281.7850 - val_mse: 29960866.0000 - val_mae: 2281.7852\n",
            "Epoch 341/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2383.2506 - mse: 33437884.0000 - mae: 2383.2505 - val_loss: 2288.6438 - val_mse: 30020668.0000 - val_mae: 2288.6438\n",
            "Epoch 342/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2362.8490 - mse: 33460612.0000 - mae: 2362.8489 - val_loss: 2283.7642 - val_mse: 29962990.0000 - val_mae: 2283.7644\n",
            "Epoch 343/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2374.6488 - mse: 33251666.0000 - mae: 2374.6487 - val_loss: 2277.2575 - val_mse: 29884572.0000 - val_mae: 2277.2573\n",
            "Epoch 344/500\n",
            "1130/1130 [==============================] - 0s 85us/step - loss: 2374.9307 - mse: 33519728.0000 - mae: 2374.9304 - val_loss: 2265.3563 - val_mse: 29751988.0000 - val_mae: 2265.3560\n",
            "Epoch 345/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2391.6657 - mse: 33765300.0000 - mae: 2391.6658 - val_loss: 2272.3470 - val_mse: 29818732.0000 - val_mae: 2272.3469\n",
            "Epoch 346/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2369.9091 - mse: 33401300.0000 - mae: 2369.9092 - val_loss: 2281.3929 - val_mse: 29915586.0000 - val_mae: 2281.3926\n",
            "Epoch 347/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2374.5351 - mse: 33428142.0000 - mae: 2374.5349 - val_loss: 2267.1763 - val_mse: 29796362.0000 - val_mae: 2267.1763\n",
            "Epoch 348/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2407.3137 - mse: 34132448.0000 - mae: 2407.3135 - val_loss: 2298.5787 - val_mse: 30049168.0000 - val_mae: 2298.5789\n",
            "Epoch 349/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2387.0199 - mse: 33926020.0000 - mae: 2387.0200 - val_loss: 2260.1419 - val_mse: 29729686.0000 - val_mae: 2260.1418\n",
            "Epoch 350/500\n",
            "1130/1130 [==============================] - 0s 86us/step - loss: 2381.6088 - mse: 33273142.0000 - mae: 2381.6086 - val_loss: 2303.7311 - val_mse: 30072218.0000 - val_mae: 2303.7310\n",
            "Epoch 351/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2396.3674 - mse: 34084732.0000 - mae: 2396.3674 - val_loss: 2283.1746 - val_mse: 29906172.0000 - val_mae: 2283.1743\n",
            "Epoch 352/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2377.4302 - mse: 33556236.0000 - mae: 2377.4302 - val_loss: 2278.6141 - val_mse: 29870272.0000 - val_mae: 2278.6138\n",
            "Epoch 353/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2367.4231 - mse: 33121778.0000 - mae: 2367.4231 - val_loss: 2306.1863 - val_mse: 30077456.0000 - val_mae: 2306.1863\n",
            "Epoch 354/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2369.5715 - mse: 33170670.0000 - mae: 2369.5715 - val_loss: 2286.5001 - val_mse: 29915222.0000 - val_mae: 2286.5000\n",
            "Epoch 355/500\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 2358.9352 - mse: 33330030.0000 - mae: 2358.9351 - val_loss: 2293.8781 - val_mse: 29964702.0000 - val_mae: 2293.8782\n",
            "Epoch 356/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2371.8893 - mse: 33567660.0000 - mae: 2371.8894 - val_loss: 2285.9623 - val_mse: 29888052.0000 - val_mae: 2285.9622\n",
            "Epoch 357/500\n",
            "1130/1130 [==============================] - 0s 85us/step - loss: 2373.4478 - mse: 33570208.0000 - mae: 2373.4478 - val_loss: 2292.5616 - val_mse: 29922022.0000 - val_mae: 2292.5615\n",
            "Epoch 358/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2378.9756 - mse: 33280334.0000 - mae: 2378.9756 - val_loss: 2294.4587 - val_mse: 29945024.0000 - val_mae: 2294.4590\n",
            "Epoch 359/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2371.1290 - mse: 33135568.0000 - mae: 2371.1289 - val_loss: 2291.3098 - val_mse: 29919850.0000 - val_mae: 2291.3098\n",
            "Epoch 360/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2362.7512 - mse: 33286832.0000 - mae: 2362.7512 - val_loss: 2264.0234 - val_mse: 29679268.0000 - val_mae: 2264.0232\n",
            "Epoch 361/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2381.7676 - mse: 33579800.0000 - mae: 2381.7676 - val_loss: 2280.0839 - val_mse: 29796220.0000 - val_mae: 2280.0840\n",
            "Epoch 362/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2365.0063 - mse: 33323178.0000 - mae: 2365.0063 - val_loss: 2266.4300 - val_mse: 29691506.0000 - val_mae: 2266.4302\n",
            "Epoch 363/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2378.6697 - mse: 33077360.0000 - mae: 2378.6697 - val_loss: 2255.6640 - val_mse: 29593036.0000 - val_mae: 2255.6641\n",
            "Epoch 364/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2363.5124 - mse: 32802460.0000 - mae: 2363.5127 - val_loss: 2305.8115 - val_mse: 29977506.0000 - val_mae: 2305.8115\n",
            "Epoch 365/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2360.8001 - mse: 33312874.0000 - mae: 2360.8003 - val_loss: 2267.0944 - val_mse: 29645856.0000 - val_mae: 2267.0947\n",
            "Epoch 366/500\n",
            "1130/1130 [==============================] - 0s 65us/step - loss: 2381.9761 - mse: 33499422.0000 - mae: 2381.9761 - val_loss: 2285.8992 - val_mse: 29794086.0000 - val_mae: 2285.8992\n",
            "Epoch 367/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2370.8787 - mse: 33248820.0000 - mae: 2370.8784 - val_loss: 2261.9753 - val_mse: 29601502.0000 - val_mae: 2261.9753\n",
            "Epoch 368/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2381.4874 - mse: 33204648.0000 - mae: 2381.4875 - val_loss: 2279.9737 - val_mse: 29734128.0000 - val_mae: 2279.9736\n",
            "Epoch 369/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2368.6124 - mse: 32778146.0000 - mae: 2368.6123 - val_loss: 2287.6502 - val_mse: 29790612.0000 - val_mae: 2287.6501\n",
            "Epoch 370/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2349.0039 - mse: 33001522.0000 - mae: 2349.0039 - val_loss: 2272.1378 - val_mse: 29678934.0000 - val_mae: 2272.1377\n",
            "Epoch 371/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2370.2709 - mse: 32995694.0000 - mae: 2370.2710 - val_loss: 2253.8473 - val_mse: 29526158.0000 - val_mae: 2253.8474\n",
            "Epoch 372/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2366.5694 - mse: 33304514.0000 - mae: 2366.5693 - val_loss: 2264.0412 - val_mse: 29611050.0000 - val_mae: 2264.0410\n",
            "Epoch 373/500\n",
            "1130/1130 [==============================] - 0s 112us/step - loss: 2379.3533 - mse: 33397442.0000 - mae: 2379.3533 - val_loss: 2282.2180 - val_mse: 29755602.0000 - val_mae: 2282.2178\n",
            "Epoch 374/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2363.0620 - mse: 33147370.0000 - mae: 2363.0620 - val_loss: 2275.5800 - val_mse: 29704052.0000 - val_mae: 2275.5798\n",
            "Epoch 375/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2351.0092 - mse: 33057628.0000 - mae: 2351.0090 - val_loss: 2271.9893 - val_mse: 29660492.0000 - val_mae: 2271.9890\n",
            "Epoch 376/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2364.9046 - mse: 32978738.0000 - mae: 2364.9045 - val_loss: 2261.7337 - val_mse: 29560696.0000 - val_mae: 2261.7336\n",
            "Epoch 377/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2369.0627 - mse: 33177238.0000 - mae: 2369.0625 - val_loss: 2276.9025 - val_mse: 29682368.0000 - val_mae: 2276.9023\n",
            "Epoch 378/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2364.9971 - mse: 33111720.0000 - mae: 2364.9971 - val_loss: 2291.7484 - val_mse: 29795002.0000 - val_mae: 2291.7488\n",
            "Epoch 379/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2359.9637 - mse: 33058634.0000 - mae: 2359.9636 - val_loss: 2260.9738 - val_mse: 29549942.0000 - val_mae: 2260.9736\n",
            "Epoch 380/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2364.1757 - mse: 33314102.0000 - mae: 2364.1758 - val_loss: 2253.4177 - val_mse: 29443812.0000 - val_mae: 2253.4180\n",
            "Epoch 381/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2368.0216 - mse: 32674270.0000 - mae: 2368.0217 - val_loss: 2271.1442 - val_mse: 29590326.0000 - val_mae: 2271.1443\n",
            "Epoch 382/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2361.1814 - mse: 33100516.0000 - mae: 2361.1814 - val_loss: 2262.9202 - val_mse: 29509944.0000 - val_mae: 2262.9204\n",
            "Epoch 383/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2369.9256 - mse: 32743268.0000 - mae: 2369.9258 - val_loss: 2277.3982 - val_mse: 29619104.0000 - val_mae: 2277.3979\n",
            "Epoch 384/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2357.9510 - mse: 32845270.0000 - mae: 2357.9512 - val_loss: 2262.2472 - val_mse: 29477692.0000 - val_mae: 2262.2473\n",
            "Epoch 385/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2370.4922 - mse: 33214032.0000 - mae: 2370.4922 - val_loss: 2263.1055 - val_mse: 29474838.0000 - val_mae: 2263.1055\n",
            "Epoch 386/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2364.8554 - mse: 32995314.0000 - mae: 2364.8552 - val_loss: 2270.4453 - val_mse: 29522890.0000 - val_mae: 2270.4453\n",
            "Epoch 387/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2365.7371 - mse: 33248516.0000 - mae: 2365.7371 - val_loss: 2258.0991 - val_mse: 29400474.0000 - val_mae: 2258.0989\n",
            "Epoch 388/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2376.4676 - mse: 32937992.0000 - mae: 2376.4675 - val_loss: 2295.9293 - val_mse: 29697866.0000 - val_mae: 2295.9292\n",
            "Epoch 389/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2353.0750 - mse: 32939492.0000 - mae: 2353.0750 - val_loss: 2254.2336 - val_mse: 29370640.0000 - val_mae: 2254.2334\n",
            "Epoch 390/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2369.3252 - mse: 32965206.0000 - mae: 2369.3252 - val_loss: 2273.2226 - val_mse: 29549016.0000 - val_mae: 2273.2224\n",
            "Epoch 391/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2372.8352 - mse: 33283160.0000 - mae: 2372.8352 - val_loss: 2266.0107 - val_mse: 29480712.0000 - val_mae: 2266.0107\n",
            "Epoch 392/500\n",
            "1130/1130 [==============================] - 0s 71us/step - loss: 2350.1897 - mse: 32772472.0000 - mae: 2350.1899 - val_loss: 2257.2468 - val_mse: 29398908.0000 - val_mae: 2257.2471\n",
            "Epoch 393/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2359.5807 - mse: 32808206.0000 - mae: 2359.5808 - val_loss: 2269.7063 - val_mse: 29471790.0000 - val_mae: 2269.7063\n",
            "Epoch 394/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2365.0841 - mse: 32871568.0000 - mae: 2365.0840 - val_loss: 2280.0314 - val_mse: 29542174.0000 - val_mae: 2280.0315\n",
            "Epoch 395/500\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 2354.3779 - mse: 33046572.0000 - mae: 2354.3782 - val_loss: 2249.7511 - val_mse: 29294936.0000 - val_mae: 2249.7512\n",
            "Epoch 396/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2356.8049 - mse: 33054576.0000 - mae: 2356.8049 - val_loss: 2259.6420 - val_mse: 29348974.0000 - val_mae: 2259.6421\n",
            "Epoch 397/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2355.9276 - mse: 32736232.0000 - mae: 2355.9275 - val_loss: 2265.9322 - val_mse: 29420238.0000 - val_mae: 2265.9321\n",
            "Epoch 398/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2354.7691 - mse: 32820596.0000 - mae: 2354.7690 - val_loss: 2270.8310 - val_mse: 29464700.0000 - val_mae: 2270.8311\n",
            "Epoch 399/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2365.9213 - mse: 32969820.0000 - mae: 2365.9211 - val_loss: 2279.0804 - val_mse: 29513286.0000 - val_mae: 2279.0806\n",
            "Epoch 400/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2365.7389 - mse: 33167526.0000 - mae: 2365.7390 - val_loss: 2269.7176 - val_mse: 29457530.0000 - val_mae: 2269.7175\n",
            "Epoch 401/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2346.9894 - mse: 32585870.0000 - mae: 2346.9895 - val_loss: 2256.6223 - val_mse: 29330040.0000 - val_mae: 2256.6221\n",
            "Epoch 402/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2363.6973 - mse: 32559068.0000 - mae: 2363.6973 - val_loss: 2283.5983 - val_mse: 29524232.0000 - val_mae: 2283.5984\n",
            "Epoch 403/500\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 2376.2553 - mse: 33000142.0000 - mae: 2376.2554 - val_loss: 2249.3860 - val_mse: 29240460.0000 - val_mae: 2249.3860\n",
            "Epoch 404/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2362.6018 - mse: 32881118.0000 - mae: 2362.6021 - val_loss: 2281.9950 - val_mse: 29480484.0000 - val_mae: 2281.9951\n",
            "Epoch 405/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2346.3123 - mse: 32718518.0000 - mae: 2346.3125 - val_loss: 2265.6631 - val_mse: 29346414.0000 - val_mae: 2265.6631\n",
            "Epoch 406/500\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 2348.6048 - mse: 32316298.0000 - mae: 2348.6047 - val_loss: 2256.1339 - val_mse: 29254348.0000 - val_mae: 2256.1338\n",
            "Epoch 407/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2335.3878 - mse: 32119256.0000 - mae: 2335.3879 - val_loss: 2257.4204 - val_mse: 29254070.0000 - val_mae: 2257.4207\n",
            "Epoch 408/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2352.3541 - mse: 32590632.0000 - mae: 2352.3542 - val_loss: 2255.8734 - val_mse: 29205982.0000 - val_mae: 2255.8735\n",
            "Epoch 409/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2357.9499 - mse: 32320270.0000 - mae: 2357.9500 - val_loss: 2263.3193 - val_mse: 29245032.0000 - val_mae: 2263.3193\n",
            "Epoch 410/500\n",
            "1130/1130 [==============================] - 0s 83us/step - loss: 2342.6745 - mse: 32194042.0000 - mae: 2342.6746 - val_loss: 2262.7261 - val_mse: 29229948.0000 - val_mae: 2262.7261\n",
            "Epoch 411/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2355.1415 - mse: 32302198.0000 - mae: 2355.1414 - val_loss: 2262.2410 - val_mse: 29191458.0000 - val_mae: 2262.2412\n",
            "Epoch 412/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2358.4058 - mse: 32426006.0000 - mae: 2358.4058 - val_loss: 2270.4372 - val_mse: 29268006.0000 - val_mae: 2270.4373\n",
            "Epoch 413/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2352.7322 - mse: 32290120.0000 - mae: 2352.7322 - val_loss: 2279.9356 - val_mse: 29341750.0000 - val_mae: 2279.9358\n",
            "Epoch 414/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2344.3469 - mse: 32644764.0000 - mae: 2344.3469 - val_loss: 2246.0350 - val_mse: 29083618.0000 - val_mae: 2246.0352\n",
            "Epoch 415/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2351.4264 - mse: 32378224.0000 - mae: 2351.4260 - val_loss: 2267.9218 - val_mse: 29271450.0000 - val_mae: 2267.9216\n",
            "Epoch 416/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2363.7556 - mse: 32956586.0000 - mae: 2363.7559 - val_loss: 2247.5187 - val_mse: 29112638.0000 - val_mae: 2247.5188\n",
            "Epoch 417/500\n",
            "1130/1130 [==============================] - 0s 91us/step - loss: 2343.2892 - mse: 32219882.0000 - mae: 2343.2891 - val_loss: 2273.2174 - val_mse: 29332864.0000 - val_mae: 2273.2175\n",
            "Epoch 418/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2343.0572 - mse: 32551608.0000 - mae: 2343.0574 - val_loss: 2273.6196 - val_mse: 29324070.0000 - val_mae: 2273.6199\n",
            "Epoch 419/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2351.1023 - mse: 32797400.0000 - mae: 2351.1023 - val_loss: 2261.1795 - val_mse: 29213936.0000 - val_mae: 2261.1794\n",
            "Epoch 420/500\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 2372.8654 - mse: 32837410.0000 - mae: 2372.8652 - val_loss: 2261.2554 - val_mse: 29197070.0000 - val_mae: 2261.2551\n",
            "Epoch 421/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2335.9082 - mse: 32331156.0000 - mae: 2335.9082 - val_loss: 2261.9052 - val_mse: 29182304.0000 - val_mae: 2261.9053\n",
            "Epoch 422/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2357.7162 - mse: 32781890.0000 - mae: 2357.7161 - val_loss: 2235.8743 - val_mse: 28973102.0000 - val_mae: 2235.8745\n",
            "Epoch 423/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2358.5668 - mse: 32642332.0000 - mae: 2358.5669 - val_loss: 2261.4224 - val_mse: 29189430.0000 - val_mae: 2261.4224\n",
            "Epoch 424/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2333.7346 - mse: 32194690.0000 - mae: 2333.7346 - val_loss: 2261.7984 - val_mse: 29173372.0000 - val_mae: 2261.7986\n",
            "Epoch 425/500\n",
            "1130/1130 [==============================] - 0s 73us/step - loss: 2363.3980 - mse: 33226640.0000 - mae: 2363.3979 - val_loss: 2257.2631 - val_mse: 29135144.0000 - val_mae: 2257.2632\n",
            "Epoch 426/500\n",
            "1130/1130 [==============================] - 0s 69us/step - loss: 2345.5124 - mse: 32328188.0000 - mae: 2345.5125 - val_loss: 2244.6220 - val_mse: 29023900.0000 - val_mae: 2244.6218\n",
            "Epoch 427/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2352.1044 - mse: 32759120.0000 - mae: 2352.1042 - val_loss: 2229.6896 - val_mse: 28879926.0000 - val_mae: 2229.6897\n",
            "Epoch 428/500\n",
            "1130/1130 [==============================] - 0s 88us/step - loss: 2340.2162 - mse: 32059508.0000 - mae: 2340.2161 - val_loss: 2256.0861 - val_mse: 29082634.0000 - val_mae: 2256.0862\n",
            "Epoch 429/500\n",
            "1130/1130 [==============================] - 0s 90us/step - loss: 2354.0654 - mse: 32430540.0000 - mae: 2354.0652 - val_loss: 2273.1122 - val_mse: 29213594.0000 - val_mae: 2273.1123\n",
            "Epoch 430/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2341.5458 - mse: 32404428.0000 - mae: 2341.5459 - val_loss: 2244.4346 - val_mse: 28978092.0000 - val_mae: 2244.4346\n",
            "Epoch 431/500\n",
            "1130/1130 [==============================] - 0s 88us/step - loss: 2343.0964 - mse: 32289598.0000 - mae: 2343.0964 - val_loss: 2262.7985 - val_mse: 29089436.0000 - val_mae: 2262.7983\n",
            "Epoch 432/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2350.6344 - mse: 32115130.0000 - mae: 2350.6345 - val_loss: 2267.6573 - val_mse: 29100934.0000 - val_mae: 2267.6575\n",
            "Epoch 433/500\n",
            "1130/1130 [==============================] - 0s 88us/step - loss: 2349.6690 - mse: 32267044.0000 - mae: 2349.6689 - val_loss: 2267.7204 - val_mse: 29099368.0000 - val_mae: 2267.7205\n",
            "Epoch 434/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2358.6310 - mse: 32397668.0000 - mae: 2358.6309 - val_loss: 2234.6039 - val_mse: 28827116.0000 - val_mae: 2234.6040\n",
            "Epoch 435/500\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 2355.1248 - mse: 32120614.0000 - mae: 2355.1248 - val_loss: 2248.6869 - val_mse: 28937390.0000 - val_mae: 2248.6870\n",
            "Epoch 436/500\n",
            "1130/1130 [==============================] - 0s 88us/step - loss: 2372.5167 - mse: 32617134.0000 - mae: 2372.5166 - val_loss: 2254.7557 - val_mse: 29006460.0000 - val_mae: 2254.7556\n",
            "Epoch 437/500\n",
            "1130/1130 [==============================] - 0s 98us/step - loss: 2331.7840 - mse: 32067338.0000 - mae: 2331.7842 - val_loss: 2257.0259 - val_mse: 29006248.0000 - val_mae: 2257.0259\n",
            "Epoch 438/500\n",
            "1130/1130 [==============================] - 0s 88us/step - loss: 2364.8496 - mse: 32560590.0000 - mae: 2364.8496 - val_loss: 2275.6021 - val_mse: 29124448.0000 - val_mae: 2275.6023\n",
            "Epoch 439/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2326.8380 - mse: 32159850.0000 - mae: 2326.8379 - val_loss: 2236.1393 - val_mse: 28827832.0000 - val_mae: 2236.1394\n",
            "Epoch 440/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2333.0354 - mse: 31941706.0000 - mae: 2333.0354 - val_loss: 2260.3582 - val_mse: 28995738.0000 - val_mae: 2260.3582\n",
            "Epoch 441/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2354.8717 - mse: 32558408.0000 - mae: 2354.8718 - val_loss: 2268.1110 - val_mse: 29045330.0000 - val_mae: 2268.1108\n",
            "Epoch 442/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2345.6619 - mse: 32332504.0000 - mae: 2345.6619 - val_loss: 2238.0760 - val_mse: 28810620.0000 - val_mae: 2238.0759\n",
            "Epoch 443/500\n",
            "1130/1130 [==============================] - 0s 70us/step - loss: 2346.8691 - mse: 31916522.0000 - mae: 2346.8691 - val_loss: 2254.7068 - val_mse: 28932924.0000 - val_mae: 2254.7070\n",
            "Epoch 444/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2350.6249 - mse: 32201254.0000 - mae: 2350.6248 - val_loss: 2258.6157 - val_mse: 28949922.0000 - val_mae: 2258.6160\n",
            "Epoch 445/500\n",
            "1130/1130 [==============================] - 0s 72us/step - loss: 2349.2003 - mse: 32308872.0000 - mae: 2349.2004 - val_loss: 2243.6631 - val_mse: 28826034.0000 - val_mae: 2243.6631\n",
            "Epoch 446/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2325.1756 - mse: 31594340.0000 - mae: 2325.1758 - val_loss: 2269.3348 - val_mse: 29005918.0000 - val_mae: 2269.3347\n",
            "Epoch 447/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2372.0775 - mse: 32561086.0000 - mae: 2372.0776 - val_loss: 2233.3117 - val_mse: 28740396.0000 - val_mae: 2233.3115\n",
            "Epoch 448/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2337.3519 - mse: 32093708.0000 - mae: 2337.3518 - val_loss: 2256.2581 - val_mse: 28927342.0000 - val_mae: 2256.2581\n",
            "Epoch 449/500\n",
            "1130/1130 [==============================] - 0s 102us/step - loss: 2340.6528 - mse: 32030016.0000 - mae: 2340.6526 - val_loss: 2236.6181 - val_mse: 28760626.0000 - val_mae: 2236.6184\n",
            "Epoch 450/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2320.6934 - mse: 31812860.0000 - mae: 2320.6936 - val_loss: 2234.8420 - val_mse: 28725942.0000 - val_mae: 2234.8418\n",
            "Epoch 451/500\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 2371.5424 - mse: 32125116.0000 - mae: 2371.5422 - val_loss: 2260.7987 - val_mse: 28931694.0000 - val_mae: 2260.7986\n",
            "Epoch 452/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2337.1456 - mse: 32063984.0000 - mae: 2337.1455 - val_loss: 2254.8652 - val_mse: 28876540.0000 - val_mae: 2254.8650\n",
            "Epoch 453/500\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 2340.7793 - mse: 31912958.0000 - mae: 2340.7795 - val_loss: 2246.8071 - val_mse: 28797644.0000 - val_mae: 2246.8071\n",
            "Epoch 454/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2331.3103 - mse: 32351890.0000 - mae: 2331.3101 - val_loss: 2231.3858 - val_mse: 28655858.0000 - val_mae: 2231.3857\n",
            "Epoch 455/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2340.8730 - mse: 31761094.0000 - mae: 2340.8730 - val_loss: 2239.1496 - val_mse: 28729188.0000 - val_mae: 2239.1497\n",
            "Epoch 456/500\n",
            "1130/1130 [==============================] - 0s 85us/step - loss: 2332.9034 - mse: 31758278.0000 - mae: 2332.9036 - val_loss: 2258.1421 - val_mse: 28850328.0000 - val_mae: 2258.1418\n",
            "Epoch 457/500\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 2346.1868 - mse: 31992794.0000 - mae: 2346.1870 - val_loss: 2256.2826 - val_mse: 28843846.0000 - val_mae: 2256.2825\n",
            "Epoch 458/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2355.5669 - mse: 31872038.0000 - mae: 2355.5671 - val_loss: 2253.9532 - val_mse: 28826702.0000 - val_mae: 2253.9534\n",
            "Epoch 459/500\n",
            "1130/1130 [==============================] - 0s 101us/step - loss: 2328.2806 - mse: 31933090.0000 - mae: 2328.2805 - val_loss: 2218.3187 - val_mse: 28524468.0000 - val_mae: 2218.3186\n",
            "Epoch 460/500\n",
            "1130/1130 [==============================] - 0s 83us/step - loss: 2330.7003 - mse: 31820606.0000 - mae: 2330.7002 - val_loss: 2232.6019 - val_mse: 28598390.0000 - val_mae: 2232.6018\n",
            "Epoch 461/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2320.5914 - mse: 31641030.0000 - mae: 2320.5916 - val_loss: 2222.6495 - val_mse: 28518318.0000 - val_mae: 2222.6494\n",
            "Epoch 462/500\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 2341.3434 - mse: 31692886.0000 - mae: 2341.3433 - val_loss: 2274.1945 - val_mse: 28903258.0000 - val_mae: 2274.1943\n",
            "Epoch 463/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2337.9027 - mse: 31866542.0000 - mae: 2337.9026 - val_loss: 2243.9817 - val_mse: 28679358.0000 - val_mae: 2243.9814\n",
            "Epoch 464/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2342.5586 - mse: 32057236.0000 - mae: 2342.5583 - val_loss: 2239.8736 - val_mse: 28640364.0000 - val_mae: 2239.8738\n",
            "Epoch 465/500\n",
            "1130/1130 [==============================] - 0s 86us/step - loss: 2356.9728 - mse: 31902998.0000 - mae: 2356.9729 - val_loss: 2231.7209 - val_mse: 28571236.0000 - val_mae: 2231.7209\n",
            "Epoch 466/500\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 2350.2371 - mse: 32230652.0000 - mae: 2350.2371 - val_loss: 2233.2448 - val_mse: 28593360.0000 - val_mae: 2233.2446\n",
            "Epoch 467/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2348.4115 - mse: 31800042.0000 - mae: 2348.4116 - val_loss: 2232.8256 - val_mse: 28583464.0000 - val_mae: 2232.8254\n",
            "Epoch 468/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2344.1169 - mse: 31590410.0000 - mae: 2344.1169 - val_loss: 2228.9319 - val_mse: 28537444.0000 - val_mae: 2228.9319\n",
            "Epoch 469/500\n",
            "1130/1130 [==============================] - 0s 100us/step - loss: 2340.6397 - mse: 32011096.0000 - mae: 2340.6399 - val_loss: 2220.6440 - val_mse: 28466482.0000 - val_mae: 2220.6440\n",
            "Epoch 470/500\n",
            "1130/1130 [==============================] - 0s 81us/step - loss: 2330.8095 - mse: 31547566.0000 - mae: 2330.8096 - val_loss: 2237.5585 - val_mse: 28599140.0000 - val_mae: 2237.5583\n",
            "Epoch 471/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2320.8469 - mse: 31811856.0000 - mae: 2320.8469 - val_loss: 2238.0417 - val_mse: 28605786.0000 - val_mae: 2238.0417\n",
            "Epoch 472/500\n",
            "1130/1130 [==============================] - 0s 84us/step - loss: 2350.2317 - mse: 31805488.0000 - mae: 2350.2319 - val_loss: 2224.9283 - val_mse: 28475550.0000 - val_mae: 2224.9282\n",
            "Epoch 473/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2350.2232 - mse: 31607596.0000 - mae: 2350.2231 - val_loss: 2240.1741 - val_mse: 28609874.0000 - val_mae: 2240.1741\n",
            "Epoch 474/500\n",
            "1130/1130 [==============================] - 0s 91us/step - loss: 2328.9491 - mse: 31744214.0000 - mae: 2328.9492 - val_loss: 2255.7784 - val_mse: 28684082.0000 - val_mae: 2255.7783\n",
            "Epoch 475/500\n",
            "1130/1130 [==============================] - 0s 74us/step - loss: 2344.6767 - mse: 31878186.0000 - mae: 2344.6768 - val_loss: 2232.9486 - val_mse: 28516130.0000 - val_mae: 2232.9487\n",
            "Epoch 476/500\n",
            "1130/1130 [==============================] - 0s 78us/step - loss: 2345.3966 - mse: 31553816.0000 - mae: 2345.3965 - val_loss: 2225.3571 - val_mse: 28441912.0000 - val_mae: 2225.3569\n",
            "Epoch 477/500\n",
            "1130/1130 [==============================] - 0s 82us/step - loss: 2348.5616 - mse: 31840828.0000 - mae: 2348.5615 - val_loss: 2233.1838 - val_mse: 28523236.0000 - val_mae: 2233.1836\n",
            "Epoch 478/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2335.7807 - mse: 31436408.0000 - mae: 2335.7808 - val_loss: 2222.9032 - val_mse: 28420090.0000 - val_mae: 2222.9033\n",
            "Epoch 479/500\n",
            "1130/1130 [==============================] - 0s 79us/step - loss: 2333.9922 - mse: 31581896.0000 - mae: 2333.9922 - val_loss: 2254.8477 - val_mse: 28662758.0000 - val_mae: 2254.8477\n",
            "Epoch 480/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2357.2208 - mse: 31887044.0000 - mae: 2357.2207 - val_loss: 2233.2930 - val_mse: 28460694.0000 - val_mae: 2233.2930\n",
            "Epoch 481/500\n",
            "1130/1130 [==============================] - 0s 77us/step - loss: 2323.0996 - mse: 31566336.0000 - mae: 2323.0996 - val_loss: 2231.4226 - val_mse: 28428326.0000 - val_mae: 2231.4226\n",
            "Epoch 482/500\n",
            "1130/1130 [==============================] - 0s 96us/step - loss: 2335.7097 - mse: 31616280.0000 - mae: 2335.7097 - val_loss: 2219.2371 - val_mse: 28326626.0000 - val_mae: 2219.2368\n",
            "Epoch 483/500\n",
            "1130/1130 [==============================] - 0s 92us/step - loss: 2343.7663 - mse: 31440422.0000 - mae: 2343.7664 - val_loss: 2247.0993 - val_mse: 28550392.0000 - val_mae: 2247.0994\n",
            "Epoch 484/500\n",
            "1130/1130 [==============================] - 0s 85us/step - loss: 2308.4154 - mse: 30973750.0000 - mae: 2308.4155 - val_loss: 2236.4608 - val_mse: 28474250.0000 - val_mae: 2236.4607\n",
            "Epoch 485/500\n",
            "1130/1130 [==============================] - 0s 99us/step - loss: 2326.0990 - mse: 31493136.0000 - mae: 2326.0991 - val_loss: 2222.0392 - val_mse: 28345014.0000 - val_mae: 2222.0393\n",
            "Epoch 486/500\n",
            "1130/1130 [==============================] - 0s 87us/step - loss: 2340.9194 - mse: 31485322.0000 - mae: 2340.9194 - val_loss: 2254.2848 - val_mse: 28586208.0000 - val_mae: 2254.2852\n",
            "Epoch 487/500\n",
            "1130/1130 [==============================] - 0s 99us/step - loss: 2328.1169 - mse: 31618894.0000 - mae: 2328.1169 - val_loss: 2237.9548 - val_mse: 28462148.0000 - val_mae: 2237.9548\n",
            "Epoch 488/500\n",
            "1130/1130 [==============================] - 0s 93us/step - loss: 2341.7793 - mse: 31812892.0000 - mae: 2341.7793 - val_loss: 2249.2922 - val_mse: 28538698.0000 - val_mae: 2249.2922\n",
            "Epoch 489/500\n",
            "1130/1130 [==============================] - 0s 76us/step - loss: 2340.4129 - mse: 31761812.0000 - mae: 2340.4131 - val_loss: 2232.6978 - val_mse: 28420060.0000 - val_mae: 2232.6975\n",
            "Epoch 490/500\n",
            "1130/1130 [==============================] - 0s 75us/step - loss: 2314.7893 - mse: 31355572.0000 - mae: 2314.7891 - val_loss: 2231.8893 - val_mse: 28391334.0000 - val_mae: 2231.8894\n",
            "Epoch 491/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2322.4543 - mse: 31056750.0000 - mae: 2322.4541 - val_loss: 2237.1143 - val_mse: 28436822.0000 - val_mae: 2237.1145\n",
            "Epoch 492/500\n",
            "1130/1130 [==============================] - 0s 88us/step - loss: 2326.8052 - mse: 31501810.0000 - mae: 2326.8054 - val_loss: 2229.8785 - val_mse: 28380464.0000 - val_mae: 2229.8789\n",
            "Epoch 493/500\n",
            "1130/1130 [==============================] - 0s 95us/step - loss: 2313.4077 - mse: 30632124.0000 - mae: 2313.4077 - val_loss: 2229.9295 - val_mse: 28355686.0000 - val_mae: 2229.9297\n",
            "Epoch 494/500\n",
            "1130/1130 [==============================] - 0s 88us/step - loss: 2331.7737 - mse: 31572910.0000 - mae: 2331.7737 - val_loss: 2241.3151 - val_mse: 28422304.0000 - val_mae: 2241.3149\n",
            "Epoch 495/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2323.9548 - mse: 31064246.0000 - mae: 2323.9548 - val_loss: 2237.1561 - val_mse: 28400432.0000 - val_mae: 2237.1560\n",
            "Epoch 496/500\n",
            "1130/1130 [==============================] - 0s 103us/step - loss: 2328.6004 - mse: 31677296.0000 - mae: 2328.6003 - val_loss: 2243.2558 - val_mse: 28437328.0000 - val_mae: 2243.2559\n",
            "Epoch 497/500\n",
            "1130/1130 [==============================] - 0s 98us/step - loss: 2341.8943 - mse: 31519170.0000 - mae: 2341.8943 - val_loss: 2222.8455 - val_mse: 28276140.0000 - val_mae: 2222.8452\n",
            "Epoch 498/500\n",
            "1130/1130 [==============================] - 0s 80us/step - loss: 2341.8442 - mse: 31617796.0000 - mae: 2341.8442 - val_loss: 2234.6243 - val_mse: 28352542.0000 - val_mae: 2234.6243\n",
            "Epoch 499/500\n",
            "1130/1130 [==============================] - 0s 89us/step - loss: 2342.7684 - mse: 31327782.0000 - mae: 2342.7683 - val_loss: 2249.7845 - val_mse: 28464298.0000 - val_mae: 2249.7847\n",
            "Epoch 500/500\n",
            "1130/1130 [==============================] - 0s 99us/step - loss: 2348.0812 - mse: 31429970.0000 - mae: 2348.0813 - val_loss: 2238.1209 - val_mse: 28376800.0000 - val_mae: 2238.1211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 1500 samples\n",
            "Epoch 1/500\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 3347.3600 - mse: 68493192.0000 - mae: 3347.3601 - val_loss: 3852.4648 - val_mse: 98320360.0000 - val_mae: 3852.4646\n",
            "Epoch 2/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 3347.2397 - mse: 68492496.0000 - mae: 3347.2397 - val_loss: 3852.0328 - val_mse: 98318328.0000 - val_mae: 3852.0330\n",
            "Epoch 3/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 3345.6834 - mse: 68486384.0000 - mae: 3345.6833 - val_loss: 3848.2326 - val_mse: 98301520.0000 - val_mae: 3848.2327\n",
            "Epoch 4/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 3340.2757 - mse: 68458040.0000 - mae: 3340.2756 - val_loss: 3840.0774 - val_mse: 98256736.0000 - val_mae: 3840.0769\n",
            "Epoch 5/500\n",
            "1500/1500 [==============================] - 0s 57us/step - loss: 3331.5935 - mse: 68409264.0000 - mae: 3331.5933 - val_loss: 3829.3968 - val_mse: 98184608.0000 - val_mae: 3829.3967\n",
            "Epoch 6/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 3320.0264 - mse: 68330232.0000 - mae: 3320.0264 - val_loss: 3816.5970 - val_mse: 98089712.0000 - val_mae: 3816.5969\n",
            "Epoch 7/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 3306.7723 - mse: 68242296.0000 - mae: 3306.7722 - val_loss: 3802.4966 - val_mse: 97980136.0000 - val_mae: 3802.4966\n",
            "Epoch 8/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 3292.5942 - mse: 68142752.0000 - mae: 3292.5942 - val_loss: 3787.1248 - val_mse: 97852776.0000 - val_mae: 3787.1243\n",
            "Epoch 9/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 3278.0695 - mse: 68011920.0000 - mae: 3278.0696 - val_loss: 3770.7106 - val_mse: 97708784.0000 - val_mae: 3770.7107\n",
            "Epoch 10/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 3261.1166 - mse: 67878568.0000 - mae: 3261.1167 - val_loss: 3753.4996 - val_mse: 97548296.0000 - val_mae: 3753.4998\n",
            "Epoch 11/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 3246.1568 - mse: 67764736.0000 - mae: 3246.1567 - val_loss: 3735.7881 - val_mse: 97376544.0000 - val_mae: 3735.7881\n",
            "Epoch 12/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 3229.9021 - mse: 67603480.0000 - mae: 3229.9021 - val_loss: 3718.1698 - val_mse: 97196784.0000 - val_mae: 3718.1697\n",
            "Epoch 13/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 3213.6088 - mse: 67416056.0000 - mae: 3213.6086 - val_loss: 3701.0411 - val_mse: 97008328.0000 - val_mae: 3701.0410\n",
            "Epoch 14/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 3197.4552 - mse: 67262456.0000 - mae: 3197.4553 - val_loss: 3685.1861 - val_mse: 96818792.0000 - val_mae: 3685.1860\n",
            "Epoch 15/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 3181.2190 - mse: 67089276.0000 - mae: 3181.2190 - val_loss: 3669.9218 - val_mse: 96623376.0000 - val_mae: 3669.9221\n",
            "Epoch 16/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 3165.9387 - mse: 66893372.0000 - mae: 3165.9387 - val_loss: 3655.6766 - val_mse: 96432464.0000 - val_mae: 3655.6768\n",
            "Epoch 17/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 3157.1457 - mse: 66788952.0000 - mae: 3157.1458 - val_loss: 3642.5157 - val_mse: 96248008.0000 - val_mae: 3642.5156\n",
            "Epoch 18/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 3144.4670 - mse: 66593160.0000 - mae: 3144.4670 - val_loss: 3630.4012 - val_mse: 96066000.0000 - val_mae: 3630.4014\n",
            "Epoch 19/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 3134.8822 - mse: 66408580.0000 - mae: 3134.8821 - val_loss: 3619.7660 - val_mse: 95889312.0000 - val_mae: 3619.7664\n",
            "Epoch 20/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 3125.2454 - mse: 66265160.0000 - mae: 3125.2454 - val_loss: 3611.1086 - val_mse: 95729656.0000 - val_mae: 3611.1084\n",
            "Epoch 21/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 3122.1273 - mse: 66245884.0000 - mae: 3122.1274 - val_loss: 3603.8579 - val_mse: 95581536.0000 - val_mae: 3603.8584\n",
            "Epoch 22/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 3113.2231 - mse: 66042936.0000 - mae: 3113.2229 - val_loss: 3597.2445 - val_mse: 95435504.0000 - val_mae: 3597.2444\n",
            "Epoch 23/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 3107.9714 - mse: 65929856.0000 - mae: 3107.9714 - val_loss: 3591.7903 - val_mse: 95302656.0000 - val_mae: 3591.7903\n",
            "Epoch 24/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 3105.0423 - mse: 65885360.0000 - mae: 3105.0422 - val_loss: 3586.9069 - val_mse: 95173968.0000 - val_mae: 3586.9067\n",
            "Epoch 25/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 3099.5603 - mse: 65809276.0000 - mae: 3099.5603 - val_loss: 3582.4502 - val_mse: 95058112.0000 - val_mae: 3582.4500\n",
            "Epoch 26/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 3094.4335 - mse: 65667436.0000 - mae: 3094.4333 - val_loss: 3577.7971 - val_mse: 94933192.0000 - val_mae: 3577.7966\n",
            "Epoch 27/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 3093.7232 - mse: 65559144.0000 - mae: 3093.7234 - val_loss: 3573.5018 - val_mse: 94814416.0000 - val_mae: 3573.5017\n",
            "Epoch 28/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 3083.4337 - mse: 65416960.0000 - mae: 3083.4336 - val_loss: 3568.6822 - val_mse: 94701608.0000 - val_mae: 3568.6819\n",
            "Epoch 29/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 3082.2765 - mse: 65281184.0000 - mae: 3082.2766 - val_loss: 3561.0593 - val_mse: 94619552.0000 - val_mae: 3561.0593\n",
            "Epoch 30/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 3071.6813 - mse: 65299656.0000 - mae: 3071.6814 - val_loss: 3555.0558 - val_mse: 94491376.0000 - val_mae: 3555.0557\n",
            "Epoch 31/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 3063.1842 - mse: 65205096.0000 - mae: 3063.1841 - val_loss: 3549.7484 - val_mse: 94308576.0000 - val_mae: 3549.7480\n",
            "Epoch 32/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 3057.7243 - mse: 64984792.0000 - mae: 3057.7244 - val_loss: 3547.0256 - val_mse: 94254800.0000 - val_mae: 3547.0256\n",
            "Epoch 33/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 3056.7172 - mse: 64974536.0000 - mae: 3056.7170 - val_loss: 3539.9026 - val_mse: 94022136.0000 - val_mae: 3539.9023\n",
            "Epoch 34/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 3049.4682 - mse: 64947820.0000 - mae: 3049.4680 - val_loss: 3535.2928 - val_mse: 93912640.0000 - val_mae: 3535.2927\n",
            "Epoch 35/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 3042.2262 - mse: 64748596.0000 - mae: 3042.2261 - val_loss: 3530.5717 - val_mse: 93778344.0000 - val_mae: 3530.5718\n",
            "Epoch 36/500\n",
            "1500/1500 [==============================] - 0s 55us/step - loss: 3044.0300 - mse: 64706684.0000 - mae: 3044.0300 - val_loss: 3525.7372 - val_mse: 93605792.0000 - val_mae: 3525.7366\n",
            "Epoch 37/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 3042.2597 - mse: 64587972.0000 - mae: 3042.2600 - val_loss: 3523.4604 - val_mse: 93526680.0000 - val_mae: 3523.4604\n",
            "Epoch 38/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 3029.9955 - mse: 64412100.0000 - mae: 3029.9956 - val_loss: 3517.2019 - val_mse: 93273992.0000 - val_mae: 3517.2019\n",
            "Epoch 39/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 3030.7203 - mse: 64326376.0000 - mae: 3030.7202 - val_loss: 3513.7988 - val_mse: 93166744.0000 - val_mae: 3513.7986\n",
            "Epoch 40/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 3022.6179 - mse: 64227580.0000 - mae: 3022.6179 - val_loss: 3510.1915 - val_mse: 93050960.0000 - val_mae: 3510.1917\n",
            "Epoch 41/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 3014.9992 - mse: 63971668.0000 - mae: 3014.9993 - val_loss: 3506.1373 - val_mse: 92876104.0000 - val_mae: 3506.1375\n",
            "Epoch 42/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 3013.6007 - mse: 63956920.0000 - mae: 3013.6006 - val_loss: 3503.8731 - val_mse: 92813928.0000 - val_mae: 3503.8730\n",
            "Epoch 43/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 3008.8749 - mse: 63673156.0000 - mae: 3008.8748 - val_loss: 3500.2535 - val_mse: 92585008.0000 - val_mae: 3500.2529\n",
            "Epoch 44/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 3014.3574 - mse: 63667700.0000 - mae: 3014.3574 - val_loss: 3496.9386 - val_mse: 92451112.0000 - val_mae: 3496.9387\n",
            "Epoch 45/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 3006.6278 - mse: 63587456.0000 - mae: 3006.6277 - val_loss: 3494.7924 - val_mse: 92444544.0000 - val_mae: 3494.7920\n",
            "Epoch 46/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 3004.6181 - mse: 63620776.0000 - mae: 3004.6179 - val_loss: 3490.7758 - val_mse: 92212792.0000 - val_mae: 3490.7756\n",
            "Epoch 47/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 3005.7644 - mse: 63353116.0000 - mae: 3005.7644 - val_loss: 3487.1506 - val_mse: 92131000.0000 - val_mae: 3487.1504\n",
            "Epoch 48/500\n",
            "1500/1500 [==============================] - 0s 53us/step - loss: 2994.8483 - mse: 63275312.0000 - mae: 2994.8486 - val_loss: 3486.5096 - val_mse: 92086608.0000 - val_mae: 3486.5098\n",
            "Epoch 49/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2992.5766 - mse: 63133272.0000 - mae: 2992.5767 - val_loss: 3481.3754 - val_mse: 91887008.0000 - val_mae: 3481.3757\n",
            "Epoch 50/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2988.3011 - mse: 63095616.0000 - mae: 2988.3013 - val_loss: 3478.8631 - val_mse: 91752968.0000 - val_mae: 3478.8630\n",
            "Epoch 51/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2984.3688 - mse: 62984740.0000 - mae: 2984.3687 - val_loss: 3478.1417 - val_mse: 91556440.0000 - val_mae: 3478.1421\n",
            "Epoch 52/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2994.0871 - mse: 63021688.0000 - mae: 2994.0869 - val_loss: 3474.0292 - val_mse: 91520912.0000 - val_mae: 3474.0293\n",
            "Epoch 53/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2985.3769 - mse: 62795076.0000 - mae: 2985.3767 - val_loss: 3474.4547 - val_mse: 91560736.0000 - val_mae: 3474.4551\n",
            "Epoch 54/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2985.1222 - mse: 62735084.0000 - mae: 2985.1223 - val_loss: 3469.5998 - val_mse: 91317624.0000 - val_mae: 3469.5996\n",
            "Epoch 55/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 2985.8824 - mse: 62944792.0000 - mae: 2985.8826 - val_loss: 3467.6540 - val_mse: 91236224.0000 - val_mae: 3467.6541\n",
            "Epoch 56/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2982.4492 - mse: 62726292.0000 - mae: 2982.4492 - val_loss: 3466.9936 - val_mse: 91059352.0000 - val_mae: 3466.9937\n",
            "Epoch 57/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2972.8082 - mse: 62339128.0000 - mae: 2972.8083 - val_loss: 3465.1787 - val_mse: 90979088.0000 - val_mae: 3465.1787\n",
            "Epoch 58/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2969.0817 - mse: 62375820.0000 - mae: 2969.0818 - val_loss: 3463.1410 - val_mse: 90886424.0000 - val_mae: 3463.1406\n",
            "Epoch 59/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2972.4453 - mse: 62439828.0000 - mae: 2972.4453 - val_loss: 3462.5408 - val_mse: 90751224.0000 - val_mae: 3462.5408\n",
            "Epoch 60/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2970.0583 - mse: 62262508.0000 - mae: 2970.0583 - val_loss: 3458.7448 - val_mse: 90780896.0000 - val_mae: 3458.7451\n",
            "Epoch 61/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2967.6178 - mse: 62156920.0000 - mae: 2967.6177 - val_loss: 3458.6704 - val_mse: 90535512.0000 - val_mae: 3458.6699\n",
            "Epoch 62/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2973.4642 - mse: 62054460.0000 - mae: 2973.4641 - val_loss: 3454.4799 - val_mse: 90516752.0000 - val_mae: 3454.4797\n",
            "Epoch 63/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2966.2495 - mse: 62149860.0000 - mae: 2966.2493 - val_loss: 3452.3580 - val_mse: 90509504.0000 - val_mae: 3452.3577\n",
            "Epoch 64/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2956.3202 - mse: 61925108.0000 - mae: 2956.3203 - val_loss: 3453.6479 - val_mse: 90242712.0000 - val_mae: 3453.6479\n",
            "Epoch 65/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 2958.8825 - mse: 61735452.0000 - mae: 2958.8826 - val_loss: 3450.5369 - val_mse: 90165000.0000 - val_mae: 3450.5364\n",
            "Epoch 66/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2964.0944 - mse: 61963984.0000 - mae: 2964.0947 - val_loss: 3447.9117 - val_mse: 90083104.0000 - val_mae: 3447.9114\n",
            "Epoch 67/500\n",
            "1500/1500 [==============================] - 0s 56us/step - loss: 2955.5320 - mse: 61614676.0000 - mae: 2955.5320 - val_loss: 3444.0081 - val_mse: 90153920.0000 - val_mae: 3444.0081\n",
            "Epoch 68/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2964.0902 - mse: 61825188.0000 - mae: 2964.0901 - val_loss: 3442.3833 - val_mse: 89926696.0000 - val_mae: 3442.3833\n",
            "Epoch 69/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2952.2225 - mse: 61437336.0000 - mae: 2952.2227 - val_loss: 3437.7941 - val_mse: 89971456.0000 - val_mae: 3437.7939\n",
            "Epoch 70/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2949.1934 - mse: 61611540.0000 - mae: 2949.1934 - val_loss: 3434.9680 - val_mse: 89745000.0000 - val_mae: 3434.9680\n",
            "Epoch 71/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2947.3007 - mse: 61331188.0000 - mae: 2947.3008 - val_loss: 3431.4654 - val_mse: 89653600.0000 - val_mae: 3431.4656\n",
            "Epoch 72/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2940.0741 - mse: 61364168.0000 - mae: 2940.0740 - val_loss: 3425.4592 - val_mse: 89602472.0000 - val_mae: 3425.4590\n",
            "Epoch 73/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2949.1973 - mse: 61398776.0000 - mae: 2949.1973 - val_loss: 3421.0357 - val_mse: 89541272.0000 - val_mae: 3421.0354\n",
            "Epoch 74/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2943.6576 - mse: 61337736.0000 - mae: 2943.6577 - val_loss: 3418.9347 - val_mse: 89417168.0000 - val_mae: 3418.9343\n",
            "Epoch 75/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2937.9017 - mse: 61283436.0000 - mae: 2937.9016 - val_loss: 3415.0735 - val_mse: 89328816.0000 - val_mae: 3415.0740\n",
            "Epoch 76/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2936.3003 - mse: 61176588.0000 - mae: 2936.3003 - val_loss: 3412.2221 - val_mse: 89295568.0000 - val_mae: 3412.2224\n",
            "Epoch 77/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2934.0496 - mse: 61322084.0000 - mae: 2934.0496 - val_loss: 3409.6646 - val_mse: 89165120.0000 - val_mae: 3409.6643\n",
            "Epoch 78/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 2921.4698 - mse: 60951188.0000 - mae: 2921.4697 - val_loss: 3406.5633 - val_mse: 89031272.0000 - val_mae: 3406.5632\n",
            "Epoch 79/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2935.0185 - mse: 60912000.0000 - mae: 2935.0183 - val_loss: 3404.5575 - val_mse: 88901800.0000 - val_mae: 3404.5574\n",
            "Epoch 80/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2906.8217 - mse: 60616424.0000 - mae: 2906.8218 - val_loss: 3400.2836 - val_mse: 88873888.0000 - val_mae: 3400.2839\n",
            "Epoch 81/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2921.8425 - mse: 60835560.0000 - mae: 2921.8428 - val_loss: 3397.4440 - val_mse: 88737896.0000 - val_mae: 3397.4441\n",
            "Epoch 82/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2924.0795 - mse: 60700632.0000 - mae: 2924.0793 - val_loss: 3396.0127 - val_mse: 88598784.0000 - val_mae: 3396.0129\n",
            "Epoch 83/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2910.2524 - mse: 60400932.0000 - mae: 2910.2524 - val_loss: 3395.0533 - val_mse: 88437304.0000 - val_mae: 3395.0532\n",
            "Epoch 84/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2904.3237 - mse: 60243888.0000 - mae: 2904.3237 - val_loss: 3390.3739 - val_mse: 88483968.0000 - val_mae: 3390.3743\n",
            "Epoch 85/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2918.9625 - mse: 60468604.0000 - mae: 2918.9626 - val_loss: 3387.2729 - val_mse: 88284064.0000 - val_mae: 3387.2729\n",
            "Epoch 86/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 2904.7920 - mse: 60452044.0000 - mae: 2904.7920 - val_loss: 3388.5102 - val_mse: 88099320.0000 - val_mae: 3388.5103\n",
            "Epoch 87/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2917.2943 - mse: 60443524.0000 - mae: 2917.2944 - val_loss: 3384.3330 - val_mse: 88010688.0000 - val_mae: 3384.3330\n",
            "Epoch 88/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2906.6282 - mse: 60247260.0000 - mae: 2906.6284 - val_loss: 3378.8823 - val_mse: 87973208.0000 - val_mae: 3378.8823\n",
            "Epoch 89/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2900.2507 - mse: 60239080.0000 - mae: 2900.2507 - val_loss: 3376.3923 - val_mse: 87857432.0000 - val_mae: 3376.3923\n",
            "Epoch 90/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2897.8721 - mse: 60127712.0000 - mae: 2897.8723 - val_loss: 3376.2126 - val_mse: 87714408.0000 - val_mae: 3376.2124\n",
            "Epoch 91/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2893.7572 - mse: 59615992.0000 - mae: 2893.7573 - val_loss: 3371.5179 - val_mse: 87722800.0000 - val_mae: 3371.5181\n",
            "Epoch 92/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2897.7479 - mse: 59896872.0000 - mae: 2897.7480 - val_loss: 3370.3078 - val_mse: 87521056.0000 - val_mae: 3370.3076\n",
            "Epoch 93/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2883.5139 - mse: 59748304.0000 - mae: 2883.5137 - val_loss: 3369.7501 - val_mse: 87377640.0000 - val_mae: 3369.7502\n",
            "Epoch 94/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 2886.4262 - mse: 59709588.0000 - mae: 2886.4263 - val_loss: 3364.1972 - val_mse: 87323520.0000 - val_mae: 3364.1970\n",
            "Epoch 95/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2888.7992 - mse: 59695136.0000 - mae: 2888.7993 - val_loss: 3363.7517 - val_mse: 87154704.0000 - val_mae: 3363.7517\n",
            "Epoch 96/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2878.4347 - mse: 59288272.0000 - mae: 2878.4346 - val_loss: 3358.4955 - val_mse: 87097608.0000 - val_mae: 3358.4956\n",
            "Epoch 97/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2875.7997 - mse: 59237052.0000 - mae: 2875.7996 - val_loss: 3356.9899 - val_mse: 87006888.0000 - val_mae: 3356.9900\n",
            "Epoch 98/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2874.8834 - mse: 59390576.0000 - mae: 2874.8833 - val_loss: 3354.9788 - val_mse: 86795592.0000 - val_mae: 3354.9788\n",
            "Epoch 99/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2874.2791 - mse: 59122488.0000 - mae: 2874.2793 - val_loss: 3351.5692 - val_mse: 86751056.0000 - val_mae: 3351.5693\n",
            "Epoch 100/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2870.1278 - mse: 59039192.0000 - mae: 2870.1279 - val_loss: 3349.7868 - val_mse: 86547920.0000 - val_mae: 3349.7866\n",
            "Epoch 101/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2881.9541 - mse: 59325624.0000 - mae: 2881.9541 - val_loss: 3347.6790 - val_mse: 86558504.0000 - val_mae: 3347.6790\n",
            "Epoch 102/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2865.9261 - mse: 58912220.0000 - mae: 2865.9260 - val_loss: 3343.7691 - val_mse: 86340584.0000 - val_mae: 3343.7690\n",
            "Epoch 103/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2864.5995 - mse: 58929092.0000 - mae: 2864.5996 - val_loss: 3344.7579 - val_mse: 86104520.0000 - val_mae: 3344.7581\n",
            "Epoch 104/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2857.3652 - mse: 58699808.0000 - mae: 2857.3652 - val_loss: 3341.6128 - val_mse: 86014048.0000 - val_mae: 3341.6130\n",
            "Epoch 105/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2856.4512 - mse: 58829252.0000 - mae: 2856.4514 - val_loss: 3336.2799 - val_mse: 85982216.0000 - val_mae: 3336.2800\n",
            "Epoch 106/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 2842.0466 - mse: 58649964.0000 - mae: 2842.0466 - val_loss: 3336.1153 - val_mse: 85747400.0000 - val_mae: 3336.1152\n",
            "Epoch 107/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2846.1878 - mse: 58246344.0000 - mae: 2846.1877 - val_loss: 3335.2860 - val_mse: 85574120.0000 - val_mae: 3335.2859\n",
            "Epoch 108/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2838.2894 - mse: 58009580.0000 - mae: 2838.2893 - val_loss: 3333.6895 - val_mse: 85407576.0000 - val_mae: 3333.6895\n",
            "Epoch 109/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2831.3691 - mse: 58070332.0000 - mae: 2831.3689 - val_loss: 3326.1025 - val_mse: 85389288.0000 - val_mae: 3326.1021\n",
            "Epoch 110/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 2843.7745 - mse: 58062308.0000 - mae: 2843.7744 - val_loss: 3323.4397 - val_mse: 85340064.0000 - val_mae: 3323.4397\n",
            "Epoch 111/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2835.2241 - mse: 57922548.0000 - mae: 2835.2241 - val_loss: 3320.4204 - val_mse: 85166128.0000 - val_mae: 3320.4199\n",
            "Epoch 112/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2830.5979 - mse: 58107496.0000 - mae: 2830.5979 - val_loss: 3319.4172 - val_mse: 84996504.0000 - val_mae: 3319.4170\n",
            "Epoch 113/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2836.9832 - mse: 57948544.0000 - mae: 2836.9834 - val_loss: 3318.3184 - val_mse: 84830752.0000 - val_mae: 3318.3186\n",
            "Epoch 114/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2834.6771 - mse: 57964484.0000 - mae: 2834.6770 - val_loss: 3313.3999 - val_mse: 84750792.0000 - val_mae: 3313.3999\n",
            "Epoch 115/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2836.5284 - mse: 57493548.0000 - mae: 2836.5283 - val_loss: 3312.1878 - val_mse: 84603064.0000 - val_mae: 3312.1877\n",
            "Epoch 116/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2819.4778 - mse: 57437292.0000 - mae: 2819.4780 - val_loss: 3307.5351 - val_mse: 84552048.0000 - val_mae: 3307.5349\n",
            "Epoch 117/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2830.4485 - mse: 57259200.0000 - mae: 2830.4487 - val_loss: 3305.6142 - val_mse: 84483984.0000 - val_mae: 3305.6143\n",
            "Epoch 118/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2832.2023 - mse: 57663296.0000 - mae: 2832.2024 - val_loss: 3302.5959 - val_mse: 84337600.0000 - val_mae: 3302.5964\n",
            "Epoch 119/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 2823.6084 - mse: 57375048.0000 - mae: 2823.6084 - val_loss: 3304.0095 - val_mse: 84062320.0000 - val_mae: 3304.0093\n",
            "Epoch 120/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2815.2393 - mse: 57174512.0000 - mae: 2815.2393 - val_loss: 3297.7807 - val_mse: 84008584.0000 - val_mae: 3297.7808\n",
            "Epoch 121/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2810.3460 - mse: 56875052.0000 - mae: 2810.3459 - val_loss: 3301.6324 - val_mse: 84087056.0000 - val_mae: 3301.6323\n",
            "Epoch 122/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2809.3595 - mse: 57205120.0000 - mae: 2809.3594 - val_loss: 3294.7399 - val_mse: 83787000.0000 - val_mae: 3294.7397\n",
            "Epoch 123/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2813.8814 - mse: 56772596.0000 - mae: 2813.8816 - val_loss: 3292.2177 - val_mse: 83677936.0000 - val_mae: 3292.2180\n",
            "Epoch 124/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 2812.7228 - mse: 56770332.0000 - mae: 2812.7227 - val_loss: 3289.4859 - val_mse: 83570744.0000 - val_mae: 3289.4856\n",
            "Epoch 125/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 2805.8555 - mse: 56363132.0000 - mae: 2805.8552 - val_loss: 3287.2588 - val_mse: 83455720.0000 - val_mae: 3287.2585\n",
            "Epoch 126/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2817.1737 - mse: 56816620.0000 - mae: 2817.1736 - val_loss: 3285.2478 - val_mse: 83319232.0000 - val_mae: 3285.2483\n",
            "Epoch 127/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2799.3744 - mse: 56806308.0000 - mae: 2799.3743 - val_loss: 3283.9581 - val_mse: 83261384.0000 - val_mae: 3283.9583\n",
            "Epoch 128/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2809.3053 - mse: 56203608.0000 - mae: 2809.3054 - val_loss: 3281.9718 - val_mse: 83037960.0000 - val_mae: 3281.9719\n",
            "Epoch 129/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2812.7806 - mse: 56410396.0000 - mae: 2812.7808 - val_loss: 3278.7119 - val_mse: 82960136.0000 - val_mae: 3278.7119\n",
            "Epoch 130/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2799.3815 - mse: 56369172.0000 - mae: 2799.3813 - val_loss: 3276.2051 - val_mse: 82879736.0000 - val_mae: 3276.2051\n",
            "Epoch 131/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2792.7848 - mse: 56373520.0000 - mae: 2792.7849 - val_loss: 3274.1363 - val_mse: 82727440.0000 - val_mae: 3274.1362\n",
            "Epoch 132/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2815.1223 - mse: 56376920.0000 - mae: 2815.1223 - val_loss: 3273.0270 - val_mse: 82622312.0000 - val_mae: 3273.0271\n",
            "Epoch 133/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2795.4988 - mse: 56386160.0000 - mae: 2795.4988 - val_loss: 3269.9330 - val_mse: 82553224.0000 - val_mae: 3269.9331\n",
            "Epoch 134/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2783.7177 - mse: 55888928.0000 - mae: 2783.7178 - val_loss: 3270.2344 - val_mse: 82546888.0000 - val_mae: 3270.2346\n",
            "Epoch 135/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2780.3174 - mse: 55888512.0000 - mae: 2780.3174 - val_loss: 3265.7246 - val_mse: 82315136.0000 - val_mae: 3265.7246\n",
            "Epoch 136/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2812.2129 - mse: 56524504.0000 - mae: 2812.2129 - val_loss: 3274.6325 - val_mse: 82431472.0000 - val_mae: 3274.6326\n",
            "Epoch 137/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2784.9001 - mse: 55901844.0000 - mae: 2784.8999 - val_loss: 3261.1771 - val_mse: 82050416.0000 - val_mae: 3261.1770\n",
            "Epoch 138/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2793.7441 - mse: 55928484.0000 - mae: 2793.7441 - val_loss: 3259.8409 - val_mse: 81894432.0000 - val_mae: 3259.8411\n",
            "Epoch 139/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2780.4077 - mse: 55267792.0000 - mae: 2780.4077 - val_loss: 3256.9509 - val_mse: 81817024.0000 - val_mae: 3256.9509\n",
            "Epoch 140/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2788.6829 - mse: 55536244.0000 - mae: 2788.6831 - val_loss: 3255.3408 - val_mse: 81687552.0000 - val_mae: 3255.3406\n",
            "Epoch 141/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2769.1092 - mse: 55387772.0000 - mae: 2769.1089 - val_loss: 3254.5242 - val_mse: 81467592.0000 - val_mae: 3254.5244\n",
            "Epoch 142/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2778.2937 - mse: 55320272.0000 - mae: 2778.2939 - val_loss: 3253.0624 - val_mse: 81307288.0000 - val_mae: 3253.0620\n",
            "Epoch 143/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2767.4729 - mse: 55204292.0000 - mae: 2767.4729 - val_loss: 3248.6288 - val_mse: 81314008.0000 - val_mae: 3248.6294\n",
            "Epoch 144/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2766.8577 - mse: 55415428.0000 - mae: 2766.8577 - val_loss: 3249.2131 - val_mse: 81061624.0000 - val_mae: 3249.2134\n",
            "Epoch 145/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2766.9524 - mse: 55260080.0000 - mae: 2766.9524 - val_loss: 3248.0629 - val_mse: 80938912.0000 - val_mae: 3248.0630\n",
            "Epoch 146/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2771.1670 - mse: 55304712.0000 - mae: 2771.1670 - val_loss: 3242.7975 - val_mse: 80916712.0000 - val_mae: 3242.7976\n",
            "Epoch 147/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2755.6849 - mse: 54652484.0000 - mae: 2755.6851 - val_loss: 3246.4671 - val_mse: 80928968.0000 - val_mae: 3246.4670\n",
            "Epoch 148/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2764.4494 - mse: 54975300.0000 - mae: 2764.4495 - val_loss: 3239.9344 - val_mse: 80707088.0000 - val_mae: 3239.9343\n",
            "Epoch 149/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2764.7811 - mse: 54732844.0000 - mae: 2764.7810 - val_loss: 3237.2118 - val_mse: 80559808.0000 - val_mae: 3237.2119\n",
            "Epoch 150/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2749.0003 - mse: 54295548.0000 - mae: 2749.0002 - val_loss: 3237.1530 - val_mse: 80516312.0000 - val_mae: 3237.1531\n",
            "Epoch 151/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2760.5482 - mse: 54277488.0000 - mae: 2760.5483 - val_loss: 3235.5912 - val_mse: 80366936.0000 - val_mae: 3235.5913\n",
            "Epoch 152/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2759.4462 - mse: 54533020.0000 - mae: 2759.4460 - val_loss: 3235.4022 - val_mse: 80250872.0000 - val_mae: 3235.4021\n",
            "Epoch 153/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2737.5119 - mse: 54108648.0000 - mae: 2737.5117 - val_loss: 3229.6494 - val_mse: 80018904.0000 - val_mae: 3229.6497\n",
            "Epoch 154/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2740.7412 - mse: 54384516.0000 - mae: 2740.7412 - val_loss: 3229.5325 - val_mse: 79889480.0000 - val_mae: 3229.5327\n",
            "Epoch 155/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2734.5078 - mse: 53898292.0000 - mae: 2734.5076 - val_loss: 3227.4586 - val_mse: 79557920.0000 - val_mae: 3227.4587\n",
            "Epoch 156/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2756.0711 - mse: 53764280.0000 - mae: 2756.0710 - val_loss: 3225.1542 - val_mse: 79609176.0000 - val_mae: 3225.1541\n",
            "Epoch 157/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2740.6214 - mse: 54026484.0000 - mae: 2740.6213 - val_loss: 3226.2457 - val_mse: 79521816.0000 - val_mae: 3226.2456\n",
            "Epoch 158/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2745.4408 - mse: 53909248.0000 - mae: 2745.4407 - val_loss: 3221.2094 - val_mse: 79306000.0000 - val_mae: 3221.2092\n",
            "Epoch 159/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2735.1110 - mse: 53329532.0000 - mae: 2735.1111 - val_loss: 3219.2095 - val_mse: 79199488.0000 - val_mae: 3219.2092\n",
            "Epoch 160/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2742.1581 - mse: 54227044.0000 - mae: 2742.1580 - val_loss: 3221.9521 - val_mse: 79180888.0000 - val_mae: 3221.9519\n",
            "Epoch 161/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2748.0689 - mse: 53742752.0000 - mae: 2748.0691 - val_loss: 3217.3465 - val_mse: 78982344.0000 - val_mae: 3217.3464\n",
            "Epoch 162/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2722.8869 - mse: 53187444.0000 - mae: 2722.8867 - val_loss: 3216.8562 - val_mse: 78860200.0000 - val_mae: 3216.8560\n",
            "Epoch 163/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2740.8565 - mse: 53895572.0000 - mae: 2740.8564 - val_loss: 3229.6290 - val_mse: 78905280.0000 - val_mae: 3229.6287\n",
            "Epoch 164/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2734.4186 - mse: 53334304.0000 - mae: 2734.4187 - val_loss: 3233.0882 - val_mse: 78823296.0000 - val_mae: 3233.0884\n",
            "Epoch 165/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2723.5257 - mse: 52684424.0000 - mae: 2723.5256 - val_loss: 3216.9137 - val_mse: 78532376.0000 - val_mae: 3216.9141\n",
            "Epoch 166/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2727.6326 - mse: 52715420.0000 - mae: 2727.6328 - val_loss: 3209.2245 - val_mse: 78337336.0000 - val_mae: 3209.2244\n",
            "Epoch 167/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2723.0675 - mse: 52933264.0000 - mae: 2723.0674 - val_loss: 3207.9320 - val_mse: 78195728.0000 - val_mae: 3207.9319\n",
            "Epoch 168/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2714.5987 - mse: 53009136.0000 - mae: 2714.5986 - val_loss: 3214.6313 - val_mse: 78164232.0000 - val_mae: 3214.6313\n",
            "Epoch 169/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2723.0483 - mse: 52778116.0000 - mae: 2723.0483 - val_loss: 3206.0874 - val_mse: 77945944.0000 - val_mae: 3206.0874\n",
            "Epoch 170/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 2705.4339 - mse: 52905700.0000 - mae: 2705.4338 - val_loss: 3198.6033 - val_mse: 77571328.0000 - val_mae: 3198.6038\n",
            "Epoch 171/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2717.8673 - mse: 52780008.0000 - mae: 2717.8674 - val_loss: 3202.0524 - val_mse: 77647840.0000 - val_mae: 3202.0520\n",
            "Epoch 172/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2716.8497 - mse: 52597076.0000 - mae: 2716.8496 - val_loss: 3199.0063 - val_mse: 77499104.0000 - val_mae: 3199.0066\n",
            "Epoch 173/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 2721.9792 - mse: 52510720.0000 - mae: 2721.9792 - val_loss: 3201.6880 - val_mse: 77026608.0000 - val_mae: 3201.6880\n",
            "Epoch 174/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2722.7671 - mse: 52835784.0000 - mae: 2722.7671 - val_loss: 3204.9103 - val_mse: 77370336.0000 - val_mae: 3204.9099\n",
            "Epoch 175/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2709.4473 - mse: 51723828.0000 - mae: 2709.4473 - val_loss: 3199.4597 - val_mse: 77207424.0000 - val_mae: 3199.4600\n",
            "Epoch 176/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 2704.1276 - mse: 52300256.0000 - mae: 2704.1277 - val_loss: 3186.3205 - val_mse: 76882960.0000 - val_mae: 3186.3203\n",
            "Epoch 177/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2705.9583 - mse: 51789000.0000 - mae: 2705.9583 - val_loss: 3189.2790 - val_mse: 76837272.0000 - val_mae: 3189.2791\n",
            "Epoch 178/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2718.3161 - mse: 51797632.0000 - mae: 2718.3162 - val_loss: 3196.1720 - val_mse: 76822328.0000 - val_mae: 3196.1724\n",
            "Epoch 179/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 2694.3991 - mse: 51726532.0000 - mae: 2694.3992 - val_loss: 3191.2941 - val_mse: 76641848.0000 - val_mae: 3191.2939\n",
            "Epoch 180/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2711.2474 - mse: 51819840.0000 - mae: 2711.2476 - val_loss: 3188.0770 - val_mse: 76507368.0000 - val_mae: 3188.0769\n",
            "Epoch 181/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2688.6750 - mse: 51082140.0000 - mae: 2688.6750 - val_loss: 3196.5293 - val_mse: 76455448.0000 - val_mae: 3196.5293\n",
            "Epoch 182/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2695.1934 - mse: 51249956.0000 - mae: 2695.1934 - val_loss: 3187.9794 - val_mse: 76277968.0000 - val_mae: 3187.9792\n",
            "Epoch 183/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2683.0293 - mse: 50768676.0000 - mae: 2683.0293 - val_loss: 3175.1639 - val_mse: 75999448.0000 - val_mae: 3175.1641\n",
            "Epoch 184/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2685.4172 - mse: 51379188.0000 - mae: 2685.4172 - val_loss: 3188.5357 - val_mse: 76067464.0000 - val_mae: 3188.5356\n",
            "Epoch 185/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2677.1143 - mse: 50745264.0000 - mae: 2677.1143 - val_loss: 3175.1375 - val_mse: 75790728.0000 - val_mae: 3175.1375\n",
            "Epoch 186/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2694.2276 - mse: 51076372.0000 - mae: 2694.2278 - val_loss: 3171.2476 - val_mse: 75628696.0000 - val_mae: 3171.2473\n",
            "Epoch 187/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2675.0994 - mse: 50928628.0000 - mae: 2675.0994 - val_loss: 3175.7589 - val_mse: 75553152.0000 - val_mae: 3175.7590\n",
            "Epoch 188/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2684.3878 - mse: 51462424.0000 - mae: 2684.3879 - val_loss: 3176.7660 - val_mse: 75481976.0000 - val_mae: 3176.7664\n",
            "Epoch 189/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2675.2595 - mse: 50944152.0000 - mae: 2675.2593 - val_loss: 3166.9665 - val_mse: 75255112.0000 - val_mae: 3166.9663\n",
            "Epoch 190/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2656.3173 - mse: 50100676.0000 - mae: 2656.3169 - val_loss: 3166.2563 - val_mse: 75136488.0000 - val_mae: 3166.2563\n",
            "Epoch 191/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2686.7564 - mse: 51011028.0000 - mae: 2686.7563 - val_loss: 3167.9192 - val_mse: 75029488.0000 - val_mae: 3167.9194\n",
            "Epoch 192/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2669.6133 - mse: 49912480.0000 - mae: 2669.6135 - val_loss: 3161.8321 - val_mse: 74818904.0000 - val_mae: 3161.8323\n",
            "Epoch 193/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2661.0790 - mse: 50149864.0000 - mae: 2661.0791 - val_loss: 3156.8860 - val_mse: 74686664.0000 - val_mae: 3156.8860\n",
            "Epoch 194/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2661.9708 - mse: 49826520.0000 - mae: 2661.9709 - val_loss: 3150.7500 - val_mse: 74463368.0000 - val_mae: 3150.7502\n",
            "Epoch 195/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2676.0601 - mse: 50689208.0000 - mae: 2676.0601 - val_loss: 3155.0135 - val_mse: 74447304.0000 - val_mae: 3155.0134\n",
            "Epoch 196/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2684.3405 - mse: 50404808.0000 - mae: 2684.3406 - val_loss: 3148.9611 - val_mse: 74280776.0000 - val_mae: 3148.9614\n",
            "Epoch 197/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2656.2992 - mse: 49872760.0000 - mae: 2656.2991 - val_loss: 3156.8899 - val_mse: 74285080.0000 - val_mae: 3156.8899\n",
            "Epoch 198/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2665.2770 - mse: 50142092.0000 - mae: 2665.2771 - val_loss: 3150.0146 - val_mse: 74093672.0000 - val_mae: 3150.0144\n",
            "Epoch 199/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2641.5116 - mse: 48951804.0000 - mae: 2641.5117 - val_loss: 3143.9440 - val_mse: 73961416.0000 - val_mae: 3143.9441\n",
            "Epoch 200/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2658.4377 - mse: 49577628.0000 - mae: 2658.4377 - val_loss: 3140.8141 - val_mse: 73817176.0000 - val_mae: 3140.8140\n",
            "Epoch 201/500\n",
            "1500/1500 [==============================] - 0s 72us/step - loss: 2675.2986 - mse: 50615920.0000 - mae: 2675.2986 - val_loss: 3133.1430 - val_mse: 73578920.0000 - val_mae: 3133.1431\n",
            "Epoch 202/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2649.2128 - mse: 49403096.0000 - mae: 2649.2126 - val_loss: 3131.7473 - val_mse: 73474328.0000 - val_mae: 3131.7473\n",
            "Epoch 203/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2644.3203 - mse: 49529068.0000 - mae: 2644.3203 - val_loss: 3132.6581 - val_mse: 73374224.0000 - val_mae: 3132.6580\n",
            "Epoch 204/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2662.0699 - mse: 49854584.0000 - mae: 2662.0701 - val_loss: 3124.9393 - val_mse: 73149152.0000 - val_mae: 3124.9395\n",
            "Epoch 205/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2635.8116 - mse: 48596320.0000 - mae: 2635.8113 - val_loss: 3149.0161 - val_mse: 73330448.0000 - val_mae: 3149.0164\n",
            "Epoch 206/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2675.6632 - mse: 50007692.0000 - mae: 2675.6631 - val_loss: 3121.3900 - val_mse: 72906448.0000 - val_mae: 3121.3899\n",
            "Epoch 207/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2673.8618 - mse: 50055660.0000 - mae: 2673.8618 - val_loss: 3117.5338 - val_mse: 72826480.0000 - val_mae: 3117.5337\n",
            "Epoch 208/500\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 2670.6145 - mse: 48593100.0000 - mae: 2670.6145 - val_loss: 3115.9517 - val_mse: 72775752.0000 - val_mae: 3115.9517\n",
            "Epoch 209/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2655.6801 - mse: 49737236.0000 - mae: 2655.6799 - val_loss: 3116.6300 - val_mse: 72700904.0000 - val_mae: 3116.6299\n",
            "Epoch 210/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2645.0767 - mse: 48843040.0000 - mae: 2645.0767 - val_loss: 3114.5741 - val_mse: 72626752.0000 - val_mae: 3114.5742\n",
            "Epoch 211/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2621.1732 - mse: 48757376.0000 - mae: 2621.1733 - val_loss: 3117.1208 - val_mse: 72546992.0000 - val_mae: 3117.1211\n",
            "Epoch 212/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2675.1259 - mse: 49380064.0000 - mae: 2675.1260 - val_loss: 3114.9864 - val_mse: 72490224.0000 - val_mae: 3114.9866\n",
            "Epoch 213/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2630.9475 - mse: 48378152.0000 - mae: 2630.9473 - val_loss: 3116.1546 - val_mse: 72436384.0000 - val_mae: 3116.1550\n",
            "Epoch 214/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2636.9374 - mse: 48346184.0000 - mae: 2636.9373 - val_loss: 3103.5086 - val_mse: 72173136.0000 - val_mae: 3103.5083\n",
            "Epoch 215/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 2638.1676 - mse: 48635244.0000 - mae: 2638.1672 - val_loss: 3122.5496 - val_mse: 72343112.0000 - val_mae: 3122.5496\n",
            "Epoch 216/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2635.6277 - mse: 48305652.0000 - mae: 2635.6277 - val_loss: 3111.2690 - val_mse: 72081064.0000 - val_mae: 3111.2690\n",
            "Epoch 217/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2633.3747 - mse: 48134348.0000 - mae: 2633.3748 - val_loss: 3096.3882 - val_mse: 71822472.0000 - val_mae: 3096.3884\n",
            "Epoch 218/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2642.7062 - mse: 48506224.0000 - mae: 2642.7061 - val_loss: 3099.4660 - val_mse: 71796328.0000 - val_mae: 3099.4656\n",
            "Epoch 219/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2633.5751 - mse: 48587380.0000 - mae: 2633.5752 - val_loss: 3107.5663 - val_mse: 71789616.0000 - val_mae: 3107.5664\n",
            "Epoch 220/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2612.0331 - mse: 48103680.0000 - mae: 2612.0332 - val_loss: 3095.3866 - val_mse: 71556352.0000 - val_mae: 3095.3862\n",
            "Epoch 221/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2661.9248 - mse: 48609848.0000 - mae: 2661.9248 - val_loss: 3108.9438 - val_mse: 71633096.0000 - val_mae: 3108.9436\n",
            "Epoch 222/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2637.8858 - mse: 48136536.0000 - mae: 2637.8857 - val_loss: 3085.8415 - val_mse: 71246128.0000 - val_mae: 3085.8416\n",
            "Epoch 223/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2623.4528 - mse: 47789200.0000 - mae: 2623.4529 - val_loss: 3088.6723 - val_mse: 71299336.0000 - val_mae: 3088.6724\n",
            "Epoch 224/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2622.1169 - mse: 48049956.0000 - mae: 2622.1169 - val_loss: 3090.1509 - val_mse: 71264856.0000 - val_mae: 3090.1506\n",
            "Epoch 225/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2625.8641 - mse: 48195900.0000 - mae: 2625.8640 - val_loss: 3082.5430 - val_mse: 71099152.0000 - val_mae: 3082.5430\n",
            "Epoch 226/500\n",
            "1500/1500 [==============================] - 0s 60us/step - loss: 2616.4515 - mse: 47911844.0000 - mae: 2616.4514 - val_loss: 3088.9255 - val_mse: 71076464.0000 - val_mae: 3088.9253\n",
            "Epoch 227/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2621.7297 - mse: 47871760.0000 - mae: 2621.7297 - val_loss: 3089.7078 - val_mse: 71018736.0000 - val_mae: 3089.7080\n",
            "Epoch 228/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2636.4817 - mse: 48543360.0000 - mae: 2636.4819 - val_loss: 3082.9605 - val_mse: 70944800.0000 - val_mae: 3082.9600\n",
            "Epoch 229/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2605.6939 - mse: 47234192.0000 - mae: 2605.6938 - val_loss: 3075.5396 - val_mse: 70750640.0000 - val_mae: 3075.5400\n",
            "Epoch 230/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2639.9566 - mse: 48223244.0000 - mae: 2639.9568 - val_loss: 3072.8635 - val_mse: 70668808.0000 - val_mae: 3072.8633\n",
            "Epoch 231/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2621.2485 - mse: 47459080.0000 - mae: 2621.2488 - val_loss: 3072.4441 - val_mse: 70625480.0000 - val_mae: 3072.4441\n",
            "Epoch 232/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2622.9847 - mse: 47693200.0000 - mae: 2622.9846 - val_loss: 3076.2695 - val_mse: 70583264.0000 - val_mae: 3076.2690\n",
            "Epoch 233/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2626.1334 - mse: 47523944.0000 - mae: 2626.1335 - val_loss: 3062.7178 - val_mse: 70337536.0000 - val_mae: 3062.7180\n",
            "Epoch 234/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2605.7095 - mse: 47619956.0000 - mae: 2605.7097 - val_loss: 3067.1542 - val_mse: 70394008.0000 - val_mae: 3067.1543\n",
            "Epoch 235/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2624.1080 - mse: 47475372.0000 - mae: 2624.1079 - val_loss: 3067.5522 - val_mse: 70402240.0000 - val_mae: 3067.5520\n",
            "Epoch 236/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 2615.4294 - mse: 47027800.0000 - mae: 2615.4294 - val_loss: 3063.0297 - val_mse: 70294544.0000 - val_mae: 3063.0298\n",
            "Epoch 237/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2616.5162 - mse: 47300880.0000 - mae: 2616.5161 - val_loss: 3067.9437 - val_mse: 70240248.0000 - val_mae: 3067.9436\n",
            "Epoch 238/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2609.9843 - mse: 46651048.0000 - mae: 2609.9844 - val_loss: 3060.4992 - val_mse: 70028080.0000 - val_mae: 3060.4990\n",
            "Epoch 239/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2611.0144 - mse: 47346532.0000 - mae: 2611.0146 - val_loss: 3056.7464 - val_mse: 69983752.0000 - val_mae: 3056.7463\n",
            "Epoch 240/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2600.8852 - mse: 47318544.0000 - mae: 2600.8853 - val_loss: 3052.8298 - val_mse: 69839768.0000 - val_mae: 3052.8301\n",
            "Epoch 241/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2617.1147 - mse: 47376832.0000 - mae: 2617.1145 - val_loss: 3043.7538 - val_mse: 69657360.0000 - val_mae: 3043.7539\n",
            "Epoch 242/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2592.6128 - mse: 47103508.0000 - mae: 2592.6130 - val_loss: 3039.1714 - val_mse: 69471048.0000 - val_mae: 3039.1714\n",
            "Epoch 243/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2635.7919 - mse: 47803336.0000 - mae: 2635.7920 - val_loss: 3046.2823 - val_mse: 69567504.0000 - val_mae: 3046.2822\n",
            "Epoch 244/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2599.0527 - mse: 46801132.0000 - mae: 2599.0527 - val_loss: 3048.6357 - val_mse: 69519384.0000 - val_mae: 3048.6357\n",
            "Epoch 245/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2602.3459 - mse: 46645288.0000 - mae: 2602.3459 - val_loss: 3057.7736 - val_mse: 69606008.0000 - val_mae: 3057.7739\n",
            "Epoch 246/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2601.9274 - mse: 47381364.0000 - mae: 2601.9275 - val_loss: 3051.8550 - val_mse: 69450080.0000 - val_mae: 3051.8547\n",
            "Epoch 247/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2586.5081 - mse: 46515108.0000 - mae: 2586.5081 - val_loss: 3046.3331 - val_mse: 69336584.0000 - val_mae: 3046.3330\n",
            "Epoch 248/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2596.7703 - mse: 47575164.0000 - mae: 2596.7703 - val_loss: 3043.4710 - val_mse: 69242624.0000 - val_mae: 3043.4709\n",
            "Epoch 249/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2555.3922 - mse: 46559280.0000 - mae: 2555.3921 - val_loss: 3024.2376 - val_mse: 68912440.0000 - val_mae: 3024.2375\n",
            "Epoch 250/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2595.5085 - mse: 47027152.0000 - mae: 2595.5085 - val_loss: 3024.4508 - val_mse: 68832928.0000 - val_mae: 3024.4507\n",
            "Epoch 251/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2598.5302 - mse: 46884224.0000 - mae: 2598.5303 - val_loss: 3036.5818 - val_mse: 68927104.0000 - val_mae: 3036.5818\n",
            "Epoch 252/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2562.8636 - mse: 46075368.0000 - mae: 2562.8638 - val_loss: 3028.9434 - val_mse: 68791664.0000 - val_mae: 3028.9436\n",
            "Epoch 253/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2584.0422 - mse: 46945260.0000 - mae: 2584.0422 - val_loss: 3022.2053 - val_mse: 68723000.0000 - val_mae: 3022.2053\n",
            "Epoch 254/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2579.5050 - mse: 46236136.0000 - mae: 2579.5049 - val_loss: 3014.0727 - val_mse: 68540432.0000 - val_mae: 3014.0728\n",
            "Epoch 255/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2585.1858 - mse: 45749032.0000 - mae: 2585.1858 - val_loss: 3028.7636 - val_mse: 68703736.0000 - val_mae: 3028.7637\n",
            "Epoch 256/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2561.5971 - mse: 46267472.0000 - mae: 2561.5972 - val_loss: 3009.2996 - val_mse: 68374472.0000 - val_mae: 3009.2996\n",
            "Epoch 257/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2579.7992 - mse: 45710520.0000 - mae: 2579.7993 - val_loss: 3024.0305 - val_mse: 68452928.0000 - val_mae: 3024.0308\n",
            "Epoch 258/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2576.8258 - mse: 46441364.0000 - mae: 2576.8259 - val_loss: 3011.7038 - val_mse: 68265208.0000 - val_mae: 3011.7036\n",
            "Epoch 259/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2577.6827 - mse: 46006568.0000 - mae: 2577.6826 - val_loss: 3015.8721 - val_mse: 68334336.0000 - val_mae: 3015.8716\n",
            "Epoch 260/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2570.9254 - mse: 46243260.0000 - mae: 2570.9253 - val_loss: 3011.2577 - val_mse: 68242088.0000 - val_mae: 3011.2581\n",
            "Epoch 261/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2574.1896 - mse: 45831912.0000 - mae: 2574.1895 - val_loss: 3002.7858 - val_mse: 68111104.0000 - val_mae: 3002.7859\n",
            "Epoch 262/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2574.2127 - mse: 46480408.0000 - mae: 2574.2129 - val_loss: 2996.4180 - val_mse: 67966368.0000 - val_mae: 2996.4180\n",
            "Epoch 263/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2558.5581 - mse: 46382396.0000 - mae: 2558.5581 - val_loss: 2989.5889 - val_mse: 67724472.0000 - val_mae: 2989.5891\n",
            "Epoch 264/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2529.3671 - mse: 45183180.0000 - mae: 2529.3674 - val_loss: 2998.5551 - val_mse: 67884224.0000 - val_mae: 2998.5549\n",
            "Epoch 265/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2549.5709 - mse: 45541800.0000 - mae: 2549.5708 - val_loss: 2990.3495 - val_mse: 67634752.0000 - val_mae: 2990.3496\n",
            "Epoch 266/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2523.9101 - mse: 45009932.0000 - mae: 2523.9099 - val_loss: 2994.1542 - val_mse: 67580984.0000 - val_mae: 2994.1543\n",
            "Epoch 267/500\n",
            "1500/1500 [==============================] - 0s 70us/step - loss: 2519.6610 - mse: 45252748.0000 - mae: 2519.6609 - val_loss: 2983.9843 - val_mse: 67337784.0000 - val_mae: 2983.9844\n",
            "Epoch 268/500\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 2524.5204 - mse: 44562624.0000 - mae: 2524.5203 - val_loss: 2992.9685 - val_mse: 67392720.0000 - val_mae: 2992.9688\n",
            "Epoch 269/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2545.5341 - mse: 45382584.0000 - mae: 2545.5342 - val_loss: 2984.7498 - val_mse: 67257728.0000 - val_mae: 2984.7500\n",
            "Epoch 270/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2546.4945 - mse: 45878904.0000 - mae: 2546.4946 - val_loss: 3002.4217 - val_mse: 67427576.0000 - val_mae: 3002.4216\n",
            "Epoch 271/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2535.0458 - mse: 46230496.0000 - mae: 2535.0457 - val_loss: 2985.5361 - val_mse: 67158760.0000 - val_mae: 2985.5359\n",
            "Epoch 272/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2548.0063 - mse: 45099840.0000 - mae: 2548.0063 - val_loss: 2970.2559 - val_mse: 66839824.0000 - val_mae: 2970.2561\n",
            "Epoch 273/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2520.3124 - mse: 44908048.0000 - mae: 2520.3123 - val_loss: 2975.2097 - val_mse: 66778536.0000 - val_mae: 2975.2097\n",
            "Epoch 274/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2524.5999 - mse: 45245736.0000 - mae: 2524.6001 - val_loss: 2982.2216 - val_mse: 66895456.0000 - val_mae: 2982.2217\n",
            "Epoch 275/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2526.2615 - mse: 44447308.0000 - mae: 2526.2612 - val_loss: 2980.0889 - val_mse: 66697324.0000 - val_mae: 2980.0891\n",
            "Epoch 276/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2492.8632 - mse: 44454640.0000 - mae: 2492.8633 - val_loss: 2981.1220 - val_mse: 66608464.0000 - val_mae: 2981.1221\n",
            "Epoch 277/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2535.2725 - mse: 44833996.0000 - mae: 2535.2725 - val_loss: 2971.2614 - val_mse: 66441996.0000 - val_mae: 2971.2612\n",
            "Epoch 278/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2514.5757 - mse: 44576884.0000 - mae: 2514.5757 - val_loss: 2964.6882 - val_mse: 66222032.0000 - val_mae: 2964.6880\n",
            "Epoch 279/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2526.8049 - mse: 44347196.0000 - mae: 2526.8049 - val_loss: 2956.0905 - val_mse: 66143916.0000 - val_mae: 2956.0906\n",
            "Epoch 280/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2471.3431 - mse: 43461964.0000 - mae: 2471.3430 - val_loss: 2957.2703 - val_mse: 66044740.0000 - val_mae: 2957.2703\n",
            "Epoch 281/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2529.1195 - mse: 44771548.0000 - mae: 2529.1194 - val_loss: 2967.5204 - val_mse: 66086392.0000 - val_mae: 2967.5203\n",
            "Epoch 282/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2500.6033 - mse: 44316144.0000 - mae: 2500.6033 - val_loss: 2960.2429 - val_mse: 65875820.0000 - val_mae: 2960.2427\n",
            "Epoch 283/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2502.8904 - mse: 44178608.0000 - mae: 2502.8904 - val_loss: 2958.6839 - val_mse: 65726708.0000 - val_mae: 2958.6841\n",
            "Epoch 284/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2487.4978 - mse: 44014140.0000 - mae: 2487.4978 - val_loss: 2969.0782 - val_mse: 65762620.0000 - val_mae: 2969.0784\n",
            "Epoch 285/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2516.2334 - mse: 44268628.0000 - mae: 2516.2336 - val_loss: 2947.4856 - val_mse: 65485560.0000 - val_mae: 2947.4854\n",
            "Epoch 286/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2501.1450 - mse: 44108200.0000 - mae: 2501.1450 - val_loss: 2949.2060 - val_mse: 65358168.0000 - val_mae: 2949.2061\n",
            "Epoch 287/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2494.2955 - mse: 43777684.0000 - mae: 2494.2957 - val_loss: 2969.9834 - val_mse: 65595080.0000 - val_mae: 2969.9834\n",
            "Epoch 288/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2475.7222 - mse: 43221988.0000 - mae: 2475.7224 - val_loss: 2948.4623 - val_mse: 65255112.0000 - val_mae: 2948.4624\n",
            "Epoch 289/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2466.2048 - mse: 43102580.0000 - mae: 2466.2048 - val_loss: 2951.8360 - val_mse: 65069456.0000 - val_mae: 2951.8359\n",
            "Epoch 290/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2515.8918 - mse: 44255680.0000 - mae: 2515.8916 - val_loss: 2942.3406 - val_mse: 64809288.0000 - val_mae: 2942.3406\n",
            "Epoch 291/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2513.9628 - mse: 44209008.0000 - mae: 2513.9629 - val_loss: 2957.5992 - val_mse: 64947704.0000 - val_mae: 2957.5991\n",
            "Epoch 292/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2486.7379 - mse: 43403028.0000 - mae: 2486.7378 - val_loss: 2958.9870 - val_mse: 64870308.0000 - val_mae: 2958.9871\n",
            "Epoch 293/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2451.8694 - mse: 42574952.0000 - mae: 2451.8694 - val_loss: 2963.8154 - val_mse: 64786552.0000 - val_mae: 2963.8154\n",
            "Epoch 294/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2504.8961 - mse: 43661900.0000 - mae: 2504.8962 - val_loss: 2947.1516 - val_mse: 64661252.0000 - val_mae: 2947.1516\n",
            "Epoch 295/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2453.7406 - mse: 41990140.0000 - mae: 2453.7407 - val_loss: 2941.0183 - val_mse: 64401704.0000 - val_mae: 2941.0181\n",
            "Epoch 296/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2471.1954 - mse: 43267576.0000 - mae: 2471.1953 - val_loss: 2950.6134 - val_mse: 64402540.0000 - val_mae: 2950.6133\n",
            "Epoch 297/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2463.9495 - mse: 42085820.0000 - mae: 2463.9492 - val_loss: 2952.2671 - val_mse: 64397700.0000 - val_mae: 2952.2666\n",
            "Epoch 298/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2446.2628 - mse: 42404936.0000 - mae: 2446.2629 - val_loss: 2942.2217 - val_mse: 64116092.0000 - val_mae: 2942.2217\n",
            "Epoch 299/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2481.6155 - mse: 43084792.0000 - mae: 2481.6155 - val_loss: 2947.0876 - val_mse: 64067664.0000 - val_mae: 2947.0876\n",
            "Epoch 300/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2483.7734 - mse: 43273792.0000 - mae: 2483.7734 - val_loss: 2951.9870 - val_mse: 64129672.0000 - val_mae: 2951.9873\n",
            "Epoch 301/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2483.0259 - mse: 43267772.0000 - mae: 2483.0259 - val_loss: 2938.3096 - val_mse: 63834544.0000 - val_mae: 2938.3096\n",
            "Epoch 302/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2455.9644 - mse: 42773568.0000 - mae: 2455.9644 - val_loss: 2940.7308 - val_mse: 63776972.0000 - val_mae: 2940.7307\n",
            "Epoch 303/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2473.5726 - mse: 42638372.0000 - mae: 2473.5728 - val_loss: 2926.1633 - val_mse: 63437580.0000 - val_mae: 2926.1631\n",
            "Epoch 304/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2458.3163 - mse: 41429984.0000 - mae: 2458.3164 - val_loss: 2927.2659 - val_mse: 63441820.0000 - val_mae: 2927.2661\n",
            "Epoch 305/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2448.6621 - mse: 42575808.0000 - mae: 2448.6621 - val_loss: 2932.2609 - val_mse: 63407048.0000 - val_mae: 2932.2610\n",
            "Epoch 306/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2445.0120 - mse: 42102392.0000 - mae: 2445.0120 - val_loss: 2935.2377 - val_mse: 63419104.0000 - val_mae: 2935.2375\n",
            "Epoch 307/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2497.1830 - mse: 42821156.0000 - mae: 2497.1831 - val_loss: 2935.4597 - val_mse: 63358224.0000 - val_mae: 2935.4597\n",
            "Epoch 308/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2447.5736 - mse: 41415332.0000 - mae: 2447.5737 - val_loss: 2927.9069 - val_mse: 63086232.0000 - val_mae: 2927.9070\n",
            "Epoch 309/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2470.9295 - mse: 42043532.0000 - mae: 2470.9294 - val_loss: 2921.9711 - val_mse: 62792428.0000 - val_mae: 2921.9707\n",
            "Epoch 310/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2460.1835 - mse: 42245236.0000 - mae: 2460.1836 - val_loss: 2926.6497 - val_mse: 62922188.0000 - val_mae: 2926.6497\n",
            "Epoch 311/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2452.4764 - mse: 41761660.0000 - mae: 2452.4766 - val_loss: 2928.8675 - val_mse: 62856000.0000 - val_mae: 2928.8674\n",
            "Epoch 312/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2457.5679 - mse: 41439020.0000 - mae: 2457.5681 - val_loss: 2933.6776 - val_mse: 62918436.0000 - val_mae: 2933.6777\n",
            "Epoch 313/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2444.0397 - mse: 41989320.0000 - mae: 2444.0396 - val_loss: 2928.4416 - val_mse: 62721744.0000 - val_mae: 2928.4417\n",
            "Epoch 314/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2441.2874 - mse: 41269648.0000 - mae: 2441.2874 - val_loss: 2923.8713 - val_mse: 62577348.0000 - val_mae: 2923.8713\n",
            "Epoch 315/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2471.1294 - mse: 42803276.0000 - mae: 2471.1294 - val_loss: 2921.0986 - val_mse: 62471464.0000 - val_mae: 2921.0986\n",
            "Epoch 316/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2456.2407 - mse: 40654360.0000 - mae: 2456.2407 - val_loss: 2928.5902 - val_mse: 62561140.0000 - val_mae: 2928.5903\n",
            "Epoch 317/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2423.5143 - mse: 41743620.0000 - mae: 2423.5144 - val_loss: 2932.2968 - val_mse: 62441308.0000 - val_mae: 2932.2966\n",
            "Epoch 318/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2443.0439 - mse: 42016392.0000 - mae: 2443.0439 - val_loss: 2935.6685 - val_mse: 62416064.0000 - val_mae: 2935.6687\n",
            "Epoch 319/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2428.8974 - mse: 41770280.0000 - mae: 2428.8972 - val_loss: 2919.9754 - val_mse: 62036580.0000 - val_mae: 2919.9753\n",
            "Epoch 320/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2448.1128 - mse: 42021276.0000 - mae: 2448.1130 - val_loss: 2920.4951 - val_mse: 62101520.0000 - val_mae: 2920.4951\n",
            "Epoch 321/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2430.8008 - mse: 41109436.0000 - mae: 2430.8008 - val_loss: 2918.9343 - val_mse: 61960060.0000 - val_mae: 2918.9343\n",
            "Epoch 322/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2416.8436 - mse: 40413984.0000 - mae: 2416.8438 - val_loss: 2918.3858 - val_mse: 61731024.0000 - val_mae: 2918.3857\n",
            "Epoch 323/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2441.4113 - mse: 41411968.0000 - mae: 2441.4111 - val_loss: 2917.5054 - val_mse: 61649436.0000 - val_mae: 2917.5054\n",
            "Epoch 324/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2485.2047 - mse: 42126936.0000 - mae: 2485.2046 - val_loss: 2919.0766 - val_mse: 61765676.0000 - val_mae: 2919.0767\n",
            "Epoch 325/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2437.4163 - mse: 41148308.0000 - mae: 2437.4163 - val_loss: 2937.1681 - val_mse: 61776784.0000 - val_mae: 2937.1682\n",
            "Epoch 326/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2421.1060 - mse: 40201328.0000 - mae: 2421.1060 - val_loss: 2935.7099 - val_mse: 61617640.0000 - val_mae: 2935.7100\n",
            "Epoch 327/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2422.0410 - mse: 40671648.0000 - mae: 2422.0410 - val_loss: 2911.7516 - val_mse: 61352668.0000 - val_mae: 2911.7517\n",
            "Epoch 328/500\n",
            "1500/1500 [==============================] - 0s 91us/step - loss: 2415.4009 - mse: 40343520.0000 - mae: 2415.4009 - val_loss: 2916.1828 - val_mse: 61299260.0000 - val_mae: 2916.1826\n",
            "Epoch 329/500\n",
            "1500/1500 [==============================] - 0s 87us/step - loss: 2433.9752 - mse: 41472424.0000 - mae: 2433.9751 - val_loss: 2910.0016 - val_mse: 61293476.0000 - val_mae: 2910.0017\n",
            "Epoch 330/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2429.6961 - mse: 40477468.0000 - mae: 2429.6960 - val_loss: 2910.5324 - val_mse: 61066468.0000 - val_mae: 2910.5322\n",
            "Epoch 331/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2449.9410 - mse: 41146192.0000 - mae: 2449.9409 - val_loss: 2902.2467 - val_mse: 60857008.0000 - val_mae: 2902.2471\n",
            "Epoch 332/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2398.3712 - mse: 39679636.0000 - mae: 2398.3711 - val_loss: 2898.6788 - val_mse: 60750980.0000 - val_mae: 2898.6787\n",
            "Epoch 333/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2402.1311 - mse: 39714140.0000 - mae: 2402.1311 - val_loss: 2901.4024 - val_mse: 60648828.0000 - val_mae: 2901.4026\n",
            "Epoch 334/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2412.5647 - mse: 40120524.0000 - mae: 2412.5647 - val_loss: 2900.5759 - val_mse: 60657452.0000 - val_mae: 2900.5759\n",
            "Epoch 335/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2391.4976 - mse: 39546720.0000 - mae: 2391.4976 - val_loss: 2904.8003 - val_mse: 60650948.0000 - val_mae: 2904.8000\n",
            "Epoch 336/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2407.5699 - mse: 40098388.0000 - mae: 2407.5698 - val_loss: 2903.9053 - val_mse: 60503868.0000 - val_mae: 2903.9053\n",
            "Epoch 337/500\n",
            "1500/1500 [==============================] - 0s 71us/step - loss: 2412.3230 - mse: 40847356.0000 - mae: 2412.3228 - val_loss: 2898.5251 - val_mse: 60294484.0000 - val_mae: 2898.5249\n",
            "Epoch 338/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2407.3514 - mse: 39780020.0000 - mae: 2407.3511 - val_loss: 2898.8164 - val_mse: 60192848.0000 - val_mae: 2898.8164\n",
            "Epoch 339/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2431.3144 - mse: 40502324.0000 - mae: 2431.3145 - val_loss: 2896.3604 - val_mse: 60187912.0000 - val_mae: 2896.3604\n",
            "Epoch 340/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2397.5564 - mse: 39903696.0000 - mae: 2397.5564 - val_loss: 2900.0283 - val_mse: 60024508.0000 - val_mae: 2900.0283\n",
            "Epoch 341/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2394.2536 - mse: 38512724.0000 - mae: 2394.2537 - val_loss: 2896.4917 - val_mse: 59852192.0000 - val_mae: 2896.4917\n",
            "Epoch 342/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2394.2165 - mse: 39924052.0000 - mae: 2394.2166 - val_loss: 2890.4144 - val_mse: 59625560.0000 - val_mae: 2890.4146\n",
            "Epoch 343/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2397.5011 - mse: 39362540.0000 - mae: 2397.5010 - val_loss: 2893.9413 - val_mse: 59595800.0000 - val_mae: 2893.9414\n",
            "Epoch 344/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2406.6508 - mse: 39533400.0000 - mae: 2406.6506 - val_loss: 2899.3377 - val_mse: 59631008.0000 - val_mae: 2899.3376\n",
            "Epoch 345/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2378.0210 - mse: 39035672.0000 - mae: 2378.0208 - val_loss: 2883.3896 - val_mse: 59169668.0000 - val_mae: 2883.3896\n",
            "Epoch 346/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2375.4042 - mse: 37901564.0000 - mae: 2375.4041 - val_loss: 2892.5212 - val_mse: 59219236.0000 - val_mae: 2892.5210\n",
            "Epoch 347/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2368.0967 - mse: 38201392.0000 - mae: 2368.0969 - val_loss: 2892.3241 - val_mse: 59224060.0000 - val_mae: 2892.3240\n",
            "Epoch 348/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2413.7733 - mse: 40396120.0000 - mae: 2413.7734 - val_loss: 2893.7079 - val_mse: 59231176.0000 - val_mae: 2893.7080\n",
            "Epoch 349/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2409.3169 - mse: 39732184.0000 - mae: 2409.3169 - val_loss: 2904.3323 - val_mse: 59316468.0000 - val_mae: 2904.3323\n",
            "Epoch 350/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2395.2027 - mse: 39538684.0000 - mae: 2395.2026 - val_loss: 2881.7054 - val_mse: 58716740.0000 - val_mae: 2881.7053\n",
            "Epoch 351/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2361.2359 - mse: 38038208.0000 - mae: 2361.2358 - val_loss: 2880.4921 - val_mse: 58688460.0000 - val_mae: 2880.4924\n",
            "Epoch 352/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2395.8598 - mse: 38258804.0000 - mae: 2395.8596 - val_loss: 2878.8913 - val_mse: 58739812.0000 - val_mae: 2878.8911\n",
            "Epoch 353/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2378.6842 - mse: 38789960.0000 - mae: 2378.6841 - val_loss: 2885.8423 - val_mse: 58605020.0000 - val_mae: 2885.8423\n",
            "Epoch 354/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2362.9427 - mse: 38542304.0000 - mae: 2362.9426 - val_loss: 2885.4821 - val_mse: 58558340.0000 - val_mae: 2885.4819\n",
            "Epoch 355/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2407.3634 - mse: 39414436.0000 - mae: 2407.3635 - val_loss: 2874.5403 - val_mse: 58245848.0000 - val_mae: 2874.5408\n",
            "Epoch 356/500\n",
            "1500/1500 [==============================] - 0s 62us/step - loss: 2363.0409 - mse: 37419892.0000 - mae: 2363.0408 - val_loss: 2877.0711 - val_mse: 58045024.0000 - val_mae: 2877.0713\n",
            "Epoch 357/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2374.4717 - mse: 38088168.0000 - mae: 2374.4717 - val_loss: 2874.9120 - val_mse: 58098400.0000 - val_mae: 2874.9121\n",
            "Epoch 358/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2329.0131 - mse: 36790444.0000 - mae: 2329.0132 - val_loss: 2868.8463 - val_mse: 57646012.0000 - val_mae: 2868.8464\n",
            "Epoch 359/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2342.6283 - mse: 38016616.0000 - mae: 2342.6284 - val_loss: 2879.9584 - val_mse: 58044416.0000 - val_mae: 2879.9583\n",
            "Epoch 360/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2356.5871 - mse: 36942004.0000 - mae: 2356.5869 - val_loss: 2876.5836 - val_mse: 57702620.0000 - val_mae: 2876.5837\n",
            "Epoch 361/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2363.8659 - mse: 37323308.0000 - mae: 2363.8657 - val_loss: 2874.9165 - val_mse: 57621164.0000 - val_mae: 2874.9163\n",
            "Epoch 362/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2345.5280 - mse: 37729116.0000 - mae: 2345.5281 - val_loss: 2861.4625 - val_mse: 57213768.0000 - val_mae: 2861.4624\n",
            "Epoch 363/500\n",
            "1500/1500 [==============================] - 0s 93us/step - loss: 2353.8732 - mse: 36980956.0000 - mae: 2353.8733 - val_loss: 2856.7560 - val_mse: 57074964.0000 - val_mae: 2856.7561\n",
            "Epoch 364/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2342.8783 - mse: 37590672.0000 - mae: 2342.8784 - val_loss: 2865.0190 - val_mse: 57162524.0000 - val_mae: 2865.0193\n",
            "Epoch 365/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2327.9456 - mse: 36940536.0000 - mae: 2327.9456 - val_loss: 2853.7084 - val_mse: 56622568.0000 - val_mae: 2853.7087\n",
            "Epoch 366/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2388.5210 - mse: 38505612.0000 - mae: 2388.5210 - val_loss: 2849.0643 - val_mse: 56444372.0000 - val_mae: 2849.0645\n",
            "Epoch 367/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2277.3373 - mse: 35228780.0000 - mae: 2277.3372 - val_loss: 2859.3669 - val_mse: 56593596.0000 - val_mae: 2859.3669\n",
            "Epoch 368/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2315.9779 - mse: 36325776.0000 - mae: 2315.9780 - val_loss: 2845.4656 - val_mse: 56133108.0000 - val_mae: 2845.4656\n",
            "Epoch 369/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2334.3422 - mse: 36705464.0000 - mae: 2334.3423 - val_loss: 2858.3924 - val_mse: 56387048.0000 - val_mae: 2858.3923\n",
            "Epoch 370/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2336.9526 - mse: 37623408.0000 - mae: 2336.9526 - val_loss: 2848.2245 - val_mse: 56164024.0000 - val_mae: 2848.2246\n",
            "Epoch 371/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2344.6131 - mse: 36243712.0000 - mae: 2344.6130 - val_loss: 2845.9649 - val_mse: 55855772.0000 - val_mae: 2845.9651\n",
            "Epoch 372/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2299.3583 - mse: 35972256.0000 - mae: 2299.3584 - val_loss: 2850.6284 - val_mse: 55944600.0000 - val_mae: 2850.6287\n",
            "Epoch 373/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2309.9897 - mse: 35938344.0000 - mae: 2309.9897 - val_loss: 2846.9303 - val_mse: 55686096.0000 - val_mae: 2846.9304\n",
            "Epoch 374/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2317.8210 - mse: 35961656.0000 - mae: 2317.8210 - val_loss: 2843.5404 - val_mse: 55503000.0000 - val_mae: 2843.5403\n",
            "Epoch 375/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2314.1896 - mse: 35485572.0000 - mae: 2314.1897 - val_loss: 2845.1039 - val_mse: 55608508.0000 - val_mae: 2845.1040\n",
            "Epoch 376/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2313.9482 - mse: 36203092.0000 - mae: 2313.9482 - val_loss: 2837.0443 - val_mse: 55218296.0000 - val_mae: 2837.0444\n",
            "Epoch 377/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2330.8706 - mse: 36116128.0000 - mae: 2330.8706 - val_loss: 2834.4350 - val_mse: 55097596.0000 - val_mae: 2834.4351\n",
            "Epoch 378/500\n",
            "1500/1500 [==============================] - 0s 63us/step - loss: 2290.6073 - mse: 35717068.0000 - mae: 2290.6072 - val_loss: 2827.6720 - val_mse: 54925776.0000 - val_mae: 2827.6721\n",
            "Epoch 379/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2310.2413 - mse: 35838508.0000 - mae: 2310.2415 - val_loss: 2826.1437 - val_mse: 54636808.0000 - val_mae: 2826.1440\n",
            "Epoch 380/500\n",
            "1500/1500 [==============================] - 0s 69us/step - loss: 2296.3523 - mse: 35070112.0000 - mae: 2296.3525 - val_loss: 2828.3531 - val_mse: 54774340.0000 - val_mae: 2828.3533\n",
            "Epoch 381/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2295.6595 - mse: 35829488.0000 - mae: 2295.6594 - val_loss: 2822.9437 - val_mse: 54534468.0000 - val_mae: 2822.9436\n",
            "Epoch 382/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2301.4680 - mse: 34993940.0000 - mae: 2301.4680 - val_loss: 2824.2960 - val_mse: 54295708.0000 - val_mae: 2824.2957\n",
            "Epoch 383/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2281.4033 - mse: 35074696.0000 - mae: 2281.4033 - val_loss: 2822.8236 - val_mse: 54085284.0000 - val_mae: 2822.8237\n",
            "Epoch 384/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2276.5088 - mse: 35139632.0000 - mae: 2276.5088 - val_loss: 2828.4141 - val_mse: 53998604.0000 - val_mae: 2828.4141\n",
            "Epoch 385/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2268.1923 - mse: 34515724.0000 - mae: 2268.1924 - val_loss: 2816.1360 - val_mse: 53812564.0000 - val_mae: 2816.1360\n",
            "Epoch 386/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2311.6265 - mse: 36015852.0000 - mae: 2311.6267 - val_loss: 2815.1405 - val_mse: 53717004.0000 - val_mae: 2815.1404\n",
            "Epoch 387/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2281.2030 - mse: 34905920.0000 - mae: 2281.2029 - val_loss: 2827.3412 - val_mse: 53775816.0000 - val_mae: 2827.3413\n",
            "Epoch 388/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2310.3431 - mse: 35828408.0000 - mae: 2310.3430 - val_loss: 2809.2620 - val_mse: 53616944.0000 - val_mae: 2809.2620\n",
            "Epoch 389/500\n",
            "1500/1500 [==============================] - 0s 68us/step - loss: 2227.1766 - mse: 32871894.0000 - mae: 2227.1765 - val_loss: 2808.4603 - val_mse: 53232264.0000 - val_mae: 2808.4604\n",
            "Epoch 390/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2283.9897 - mse: 34111776.0000 - mae: 2283.9897 - val_loss: 2812.2860 - val_mse: 53272652.0000 - val_mae: 2812.2859\n",
            "Epoch 391/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2307.5170 - mse: 36669968.0000 - mae: 2307.5171 - val_loss: 2801.2989 - val_mse: 53051544.0000 - val_mae: 2801.2991\n",
            "Epoch 392/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2268.9864 - mse: 33958552.0000 - mae: 2268.9863 - val_loss: 2796.9667 - val_mse: 52778004.0000 - val_mae: 2796.9670\n",
            "Epoch 393/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2272.9671 - mse: 34543096.0000 - mae: 2272.9673 - val_loss: 2800.5157 - val_mse: 52589512.0000 - val_mae: 2800.5161\n",
            "Epoch 394/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2296.6900 - mse: 34697092.0000 - mae: 2296.6899 - val_loss: 2805.5527 - val_mse: 52645736.0000 - val_mae: 2805.5527\n",
            "Epoch 395/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2245.2687 - mse: 33060258.0000 - mae: 2245.2686 - val_loss: 2805.8261 - val_mse: 52607440.0000 - val_mae: 2805.8259\n",
            "Epoch 396/500\n",
            "1500/1500 [==============================] - 0s 64us/step - loss: 2303.3998 - mse: 34774796.0000 - mae: 2303.3997 - val_loss: 2804.6535 - val_mse: 52525136.0000 - val_mae: 2804.6533\n",
            "Epoch 397/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2237.0809 - mse: 33059068.0000 - mae: 2237.0808 - val_loss: 2799.5187 - val_mse: 52517476.0000 - val_mae: 2799.5186\n",
            "Epoch 398/500\n",
            "1500/1500 [==============================] - 0s 75us/step - loss: 2230.6199 - mse: 33778072.0000 - mae: 2230.6201 - val_loss: 2790.1463 - val_mse: 51821084.0000 - val_mae: 2790.1465\n",
            "Epoch 399/500\n",
            "1500/1500 [==============================] - 0s 66us/step - loss: 2234.6814 - mse: 33837980.0000 - mae: 2234.6814 - val_loss: 2774.9175 - val_mse: 51522256.0000 - val_mae: 2774.9175\n",
            "Epoch 400/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2253.1331 - mse: 33669196.0000 - mae: 2253.1331 - val_loss: 2786.6093 - val_mse: 51727592.0000 - val_mae: 2786.6094\n",
            "Epoch 401/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2268.5860 - mse: 34605708.0000 - mae: 2268.5859 - val_loss: 2790.9591 - val_mse: 51896904.0000 - val_mae: 2790.9592\n",
            "Epoch 402/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2229.7973 - mse: 33681888.0000 - mae: 2229.7971 - val_loss: 2771.1364 - val_mse: 51068420.0000 - val_mae: 2771.1365\n",
            "Epoch 403/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2234.1958 - mse: 33008446.0000 - mae: 2234.1956 - val_loss: 2771.1504 - val_mse: 50991460.0000 - val_mae: 2771.1504\n",
            "Epoch 404/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2223.5514 - mse: 33834524.0000 - mae: 2223.5515 - val_loss: 2774.3082 - val_mse: 51189868.0000 - val_mae: 2774.3081\n",
            "Epoch 405/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2256.1483 - mse: 34853144.0000 - mae: 2256.1482 - val_loss: 2773.6433 - val_mse: 50864512.0000 - val_mae: 2773.6433\n",
            "Epoch 406/500\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 2197.5301 - mse: 32401580.0000 - mae: 2197.5300 - val_loss: 2791.0808 - val_mse: 51030244.0000 - val_mae: 2791.0806\n",
            "Epoch 407/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2214.7023 - mse: 32520448.0000 - mae: 2214.7024 - val_loss: 2776.9989 - val_mse: 50687100.0000 - val_mae: 2776.9993\n",
            "Epoch 408/500\n",
            "1500/1500 [==============================] - 0s 67us/step - loss: 2242.4790 - mse: 33543032.0000 - mae: 2242.4790 - val_loss: 2772.5561 - val_mse: 50593892.0000 - val_mae: 2772.5564\n",
            "Epoch 409/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2218.1005 - mse: 32097286.0000 - mae: 2218.1006 - val_loss: 2767.9783 - val_mse: 50456296.0000 - val_mae: 2767.9783\n",
            "Epoch 410/500\n",
            "1500/1500 [==============================] - 0s 73us/step - loss: 2254.7093 - mse: 33788692.0000 - mae: 2254.7092 - val_loss: 2771.5042 - val_mse: 50405344.0000 - val_mae: 2771.5044\n",
            "Epoch 411/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2183.1802 - mse: 32158988.0000 - mae: 2183.1802 - val_loss: 2758.2402 - val_mse: 50084724.0000 - val_mae: 2758.2400\n",
            "Epoch 412/500\n",
            "1500/1500 [==============================] - 0s 87us/step - loss: 2200.3581 - mse: 32373684.0000 - mae: 2200.3582 - val_loss: 2760.6554 - val_mse: 49970840.0000 - val_mae: 2760.6553\n",
            "Epoch 413/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2165.8484 - mse: 31859810.0000 - mae: 2165.8484 - val_loss: 2753.9091 - val_mse: 49717784.0000 - val_mae: 2753.9089\n",
            "Epoch 414/500\n",
            "1500/1500 [==============================] - 0s 74us/step - loss: 2212.8210 - mse: 32070456.0000 - mae: 2212.8210 - val_loss: 2751.2792 - val_mse: 49321356.0000 - val_mae: 2751.2793\n",
            "Epoch 415/500\n",
            "1500/1500 [==============================] - 0s 78us/step - loss: 2171.3533 - mse: 31419614.0000 - mae: 2171.3533 - val_loss: 2747.0877 - val_mse: 49259144.0000 - val_mae: 2747.0879\n",
            "Epoch 416/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2236.4548 - mse: 32568844.0000 - mae: 2236.4548 - val_loss: 2745.0002 - val_mse: 49222720.0000 - val_mae: 2745.0002\n",
            "Epoch 417/500\n",
            "1500/1500 [==============================] - 0s 91us/step - loss: 2171.3055 - mse: 32334072.0000 - mae: 2171.3054 - val_loss: 2744.1867 - val_mse: 49073532.0000 - val_mae: 2744.1868\n",
            "Epoch 418/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2178.3449 - mse: 32081784.0000 - mae: 2178.3450 - val_loss: 2750.1383 - val_mse: 49129572.0000 - val_mae: 2750.1384\n",
            "Epoch 419/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2205.3284 - mse: 32730742.0000 - mae: 2205.3284 - val_loss: 2757.6717 - val_mse: 49042172.0000 - val_mae: 2757.6716\n",
            "Epoch 420/500\n",
            "1500/1500 [==============================] - 0s 65us/step - loss: 2120.9581 - mse: 30386522.0000 - mae: 2120.9583 - val_loss: 2734.4195 - val_mse: 48598172.0000 - val_mae: 2734.4194\n",
            "Epoch 421/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2216.6456 - mse: 31884358.0000 - mae: 2216.6458 - val_loss: 2750.0980 - val_mse: 48824364.0000 - val_mae: 2750.0979\n",
            "Epoch 422/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2174.1769 - mse: 30998620.0000 - mae: 2174.1770 - val_loss: 2735.0696 - val_mse: 48649016.0000 - val_mae: 2735.0696\n",
            "Epoch 423/500\n",
            "1500/1500 [==============================] - 0s 92us/step - loss: 2178.3748 - mse: 31054280.0000 - mae: 2178.3748 - val_loss: 2723.0786 - val_mse: 48117892.0000 - val_mae: 2723.0786\n",
            "Epoch 424/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2198.8041 - mse: 31517622.0000 - mae: 2198.8044 - val_loss: 2726.4683 - val_mse: 48348512.0000 - val_mae: 2726.4685\n",
            "Epoch 425/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2174.7790 - mse: 31426806.0000 - mae: 2174.7791 - val_loss: 2733.9689 - val_mse: 48398608.0000 - val_mae: 2733.9688\n",
            "Epoch 426/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2189.1946 - mse: 32273206.0000 - mae: 2189.1946 - val_loss: 2721.4152 - val_mse: 47849712.0000 - val_mae: 2721.4153\n",
            "Epoch 427/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2168.1429 - mse: 31542008.0000 - mae: 2168.1431 - val_loss: 2715.2466 - val_mse: 47545800.0000 - val_mae: 2715.2466\n",
            "Epoch 428/500\n",
            "1500/1500 [==============================] - 0s 90us/step - loss: 2167.9308 - mse: 30724664.0000 - mae: 2167.9307 - val_loss: 2721.4506 - val_mse: 47701572.0000 - val_mae: 2721.4504\n",
            "Epoch 429/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2184.3380 - mse: 31487486.0000 - mae: 2184.3379 - val_loss: 2716.6883 - val_mse: 47561840.0000 - val_mae: 2716.6882\n",
            "Epoch 430/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2182.2667 - mse: 31889682.0000 - mae: 2182.2666 - val_loss: 2711.6992 - val_mse: 47463344.0000 - val_mae: 2711.6990\n",
            "Epoch 431/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2151.5065 - mse: 31011098.0000 - mae: 2151.5066 - val_loss: 2727.0716 - val_mse: 47840544.0000 - val_mae: 2727.0718\n",
            "Epoch 432/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2131.6213 - mse: 31129794.0000 - mae: 2131.6213 - val_loss: 2701.5326 - val_mse: 47219948.0000 - val_mae: 2701.5327\n",
            "Epoch 433/500\n",
            "1500/1500 [==============================] - 0s 90us/step - loss: 2200.5527 - mse: 31676506.0000 - mae: 2200.5525 - val_loss: 2705.8786 - val_mse: 47133072.0000 - val_mae: 2705.8787\n",
            "Epoch 434/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2196.5747 - mse: 32002028.0000 - mae: 2196.5747 - val_loss: 2710.5651 - val_mse: 47076480.0000 - val_mae: 2710.5652\n",
            "Epoch 435/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2165.0474 - mse: 31538428.0000 - mae: 2165.0476 - val_loss: 2694.5595 - val_mse: 46998608.0000 - val_mae: 2694.5596\n",
            "Epoch 436/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2162.6925 - mse: 30953036.0000 - mae: 2162.6924 - val_loss: 2696.0936 - val_mse: 46816624.0000 - val_mae: 2696.0938\n",
            "Epoch 437/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2104.4559 - mse: 29700882.0000 - mae: 2104.4558 - val_loss: 2691.1227 - val_mse: 46657732.0000 - val_mae: 2691.1226\n",
            "Epoch 438/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2119.7603 - mse: 29254560.0000 - mae: 2119.7603 - val_loss: 2686.5646 - val_mse: 46477264.0000 - val_mae: 2686.5647\n",
            "Epoch 439/500\n",
            "1500/1500 [==============================] - 0s 77us/step - loss: 2115.9890 - mse: 29166886.0000 - mae: 2115.9890 - val_loss: 2683.9777 - val_mse: 46474380.0000 - val_mae: 2683.9778\n",
            "Epoch 440/500\n",
            "1500/1500 [==============================] - 0s 76us/step - loss: 2144.1110 - mse: 31962820.0000 - mae: 2144.1111 - val_loss: 2669.8361 - val_mse: 45954696.0000 - val_mae: 2669.8362\n",
            "Epoch 441/500\n",
            "1500/1500 [==============================] - 0s 87us/step - loss: 2120.8717 - mse: 29770804.0000 - mae: 2120.8716 - val_loss: 2674.9786 - val_mse: 46000260.0000 - val_mae: 2674.9785\n",
            "Epoch 442/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2131.8857 - mse: 29490266.0000 - mae: 2131.8857 - val_loss: 2669.9411 - val_mse: 46015916.0000 - val_mae: 2669.9414\n",
            "Epoch 443/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2144.9933 - mse: 30728318.0000 - mae: 2144.9932 - val_loss: 2671.1444 - val_mse: 45965944.0000 - val_mae: 2671.1445\n",
            "Epoch 444/500\n",
            "1500/1500 [==============================] - 0s 87us/step - loss: 2110.7435 - mse: 28508760.0000 - mae: 2110.7434 - val_loss: 2682.6318 - val_mse: 46052584.0000 - val_mae: 2682.6318\n",
            "Epoch 445/500\n",
            "1500/1500 [==============================] - 0s 92us/step - loss: 2114.2536 - mse: 30194942.0000 - mae: 2114.2537 - val_loss: 2667.4625 - val_mse: 45778700.0000 - val_mae: 2667.4624\n",
            "Epoch 446/500\n",
            "1500/1500 [==============================] - 0s 94us/step - loss: 2112.7143 - mse: 29982160.0000 - mae: 2112.7144 - val_loss: 2658.2478 - val_mse: 45375136.0000 - val_mae: 2658.2478\n",
            "Epoch 447/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2121.2774 - mse: 29170358.0000 - mae: 2121.2773 - val_loss: 2668.1616 - val_mse: 45653016.0000 - val_mae: 2668.1616\n",
            "Epoch 448/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2115.8951 - mse: 29073752.0000 - mae: 2115.8953 - val_loss: 2667.0371 - val_mse: 45456624.0000 - val_mae: 2667.0371\n",
            "Epoch 449/500\n",
            "1500/1500 [==============================] - 0s 92us/step - loss: 2133.8901 - mse: 29898354.0000 - mae: 2133.8901 - val_loss: 2649.3627 - val_mse: 45263680.0000 - val_mae: 2649.3625\n",
            "Epoch 450/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2120.4393 - mse: 30011736.0000 - mae: 2120.4395 - val_loss: 2664.7915 - val_mse: 45280336.0000 - val_mae: 2664.7915\n",
            "Epoch 451/500\n",
            "1500/1500 [==============================] - 0s 89us/step - loss: 2132.1819 - mse: 29937112.0000 - mae: 2132.1819 - val_loss: 2656.3879 - val_mse: 45385472.0000 - val_mae: 2656.3879\n",
            "Epoch 452/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2098.9393 - mse: 28602778.0000 - mae: 2098.9395 - val_loss: 2641.9051 - val_mse: 44856848.0000 - val_mae: 2641.9048\n",
            "Epoch 453/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2120.2745 - mse: 30607534.0000 - mae: 2120.2744 - val_loss: 2644.6233 - val_mse: 44843384.0000 - val_mae: 2644.6233\n",
            "Epoch 454/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2079.4140 - mse: 29132636.0000 - mae: 2079.4141 - val_loss: 2645.0924 - val_mse: 44715148.0000 - val_mae: 2645.0923\n",
            "Epoch 455/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2097.9686 - mse: 29291586.0000 - mae: 2097.9688 - val_loss: 2640.1366 - val_mse: 44504940.0000 - val_mae: 2640.1367\n",
            "Epoch 456/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2036.0188 - mse: 27156158.0000 - mae: 2036.0188 - val_loss: 2640.0857 - val_mse: 44649692.0000 - val_mae: 2640.0859\n",
            "Epoch 457/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2100.2168 - mse: 29518288.0000 - mae: 2100.2166 - val_loss: 2642.3685 - val_mse: 44527800.0000 - val_mae: 2642.3687\n",
            "Epoch 458/500\n",
            "1500/1500 [==============================] - 0s 89us/step - loss: 2038.5396 - mse: 27769998.0000 - mae: 2038.5396 - val_loss: 2627.8495 - val_mse: 44110960.0000 - val_mae: 2627.8496\n",
            "Epoch 459/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2062.3169 - mse: 28488740.0000 - mae: 2062.3169 - val_loss: 2619.9386 - val_mse: 43953272.0000 - val_mae: 2619.9387\n",
            "Epoch 460/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2143.5954 - mse: 29712050.0000 - mae: 2143.5955 - val_loss: 2629.4999 - val_mse: 44229584.0000 - val_mae: 2629.5000\n",
            "Epoch 461/500\n",
            "1500/1500 [==============================] - 0s 92us/step - loss: 2108.9718 - mse: 29072250.0000 - mae: 2108.9719 - val_loss: 2630.3051 - val_mse: 44028228.0000 - val_mae: 2630.3049\n",
            "Epoch 462/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2093.9415 - mse: 28148422.0000 - mae: 2093.9414 - val_loss: 2615.5173 - val_mse: 43669612.0000 - val_mae: 2615.5173\n",
            "Epoch 463/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2104.0529 - mse: 29633040.0000 - mae: 2104.0527 - val_loss: 2635.9034 - val_mse: 44049128.0000 - val_mae: 2635.9036\n",
            "Epoch 464/500\n",
            "1500/1500 [==============================] - 0s 93us/step - loss: 2035.6507 - mse: 28695764.0000 - mae: 2035.6509 - val_loss: 2622.2385 - val_mse: 43604548.0000 - val_mae: 2622.2385\n",
            "Epoch 465/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 1998.3763 - mse: 26682172.0000 - mae: 1998.3763 - val_loss: 2620.9708 - val_mse: 43681448.0000 - val_mae: 2620.9709\n",
            "Epoch 466/500\n",
            "1500/1500 [==============================] - 0s 92us/step - loss: 2072.7627 - mse: 28482970.0000 - mae: 2072.7627 - val_loss: 2616.8516 - val_mse: 43671704.0000 - val_mae: 2616.8516\n",
            "Epoch 467/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2050.9329 - mse: 28687900.0000 - mae: 2050.9331 - val_loss: 2602.7612 - val_mse: 43476872.0000 - val_mae: 2602.7615\n",
            "Epoch 468/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2065.0765 - mse: 28000668.0000 - mae: 2065.0764 - val_loss: 2603.8939 - val_mse: 43276920.0000 - val_mae: 2603.8936\n",
            "Epoch 469/500\n",
            "1500/1500 [==============================] - 0s 89us/step - loss: 2086.4839 - mse: 29093986.0000 - mae: 2086.4839 - val_loss: 2589.3814 - val_mse: 43072684.0000 - val_mae: 2589.3816\n",
            "Epoch 470/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2032.8994 - mse: 27109670.0000 - mae: 2032.8993 - val_loss: 2588.0186 - val_mse: 42934420.0000 - val_mae: 2588.0186\n",
            "Epoch 471/500\n",
            "1500/1500 [==============================] - 0s 97us/step - loss: 2055.4613 - mse: 27702218.0000 - mae: 2055.4614 - val_loss: 2588.8492 - val_mse: 42868140.0000 - val_mae: 2588.8494\n",
            "Epoch 472/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2056.7865 - mse: 27157200.0000 - mae: 2056.7866 - val_loss: 2595.9679 - val_mse: 43137248.0000 - val_mae: 2595.9678\n",
            "Epoch 473/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 2068.5706 - mse: 28163938.0000 - mae: 2068.5706 - val_loss: 2591.3255 - val_mse: 42884136.0000 - val_mae: 2591.3257\n",
            "Epoch 474/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2003.7563 - mse: 26028742.0000 - mae: 2003.7563 - val_loss: 2597.3813 - val_mse: 42830240.0000 - val_mae: 2597.3813\n",
            "Epoch 475/500\n",
            "1500/1500 [==============================] - 0s 89us/step - loss: 2020.9626 - mse: 28112954.0000 - mae: 2020.9626 - val_loss: 2575.7317 - val_mse: 42489740.0000 - val_mae: 2575.7317\n",
            "Epoch 476/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 2045.8533 - mse: 27701868.0000 - mae: 2045.8534 - val_loss: 2572.6606 - val_mse: 42358608.0000 - val_mae: 2572.6606\n",
            "Epoch 477/500\n",
            "1500/1500 [==============================] - 0s 79us/step - loss: 2070.6503 - mse: 28272482.0000 - mae: 2070.6504 - val_loss: 2559.0055 - val_mse: 42084408.0000 - val_mae: 2559.0056\n",
            "Epoch 478/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 1971.1738 - mse: 26845496.0000 - mae: 1971.1737 - val_loss: 2560.0970 - val_mse: 42077588.0000 - val_mae: 2560.0969\n",
            "Epoch 479/500\n",
            "1500/1500 [==============================] - 0s 90us/step - loss: 2027.9677 - mse: 27559092.0000 - mae: 2027.9677 - val_loss: 2564.7448 - val_mse: 42131476.0000 - val_mae: 2564.7449\n",
            "Epoch 480/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2027.9689 - mse: 28351818.0000 - mae: 2027.9689 - val_loss: 2548.6968 - val_mse: 41795960.0000 - val_mae: 2548.6968\n",
            "Epoch 481/500\n",
            "1500/1500 [==============================] - 0s 84us/step - loss: 2034.2190 - mse: 28772788.0000 - mae: 2034.2189 - val_loss: 2549.6434 - val_mse: 41680448.0000 - val_mae: 2549.6431\n",
            "Epoch 482/500\n",
            "1500/1500 [==============================] - 0s 93us/step - loss: 2023.8267 - mse: 27671408.0000 - mae: 2023.8267 - val_loss: 2540.3923 - val_mse: 41666272.0000 - val_mae: 2540.3926\n",
            "Epoch 483/500\n",
            "1500/1500 [==============================] - 0s 83us/step - loss: 1951.8979 - mse: 25867316.0000 - mae: 1951.8978 - val_loss: 2537.1420 - val_mse: 41502656.0000 - val_mae: 2537.1421\n",
            "Epoch 484/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2050.9528 - mse: 29308144.0000 - mae: 2050.9526 - val_loss: 2557.8857 - val_mse: 41793212.0000 - val_mae: 2557.8857\n",
            "Epoch 485/500\n",
            "1500/1500 [==============================] - 0s 81us/step - loss: 2015.2309 - mse: 27343410.0000 - mae: 2015.2310 - val_loss: 2534.9536 - val_mse: 41428084.0000 - val_mae: 2534.9536\n",
            "Epoch 486/500\n",
            "1500/1500 [==============================] - 0s 90us/step - loss: 2041.5184 - mse: 28110796.0000 - mae: 2041.5183 - val_loss: 2532.5718 - val_mse: 41275028.0000 - val_mae: 2532.5718\n",
            "Epoch 487/500\n",
            "1500/1500 [==============================] - 0s 90us/step - loss: 2043.7938 - mse: 28465452.0000 - mae: 2043.7937 - val_loss: 2516.2450 - val_mse: 40895568.0000 - val_mae: 2516.2451\n",
            "Epoch 488/500\n",
            "1500/1500 [==============================] - 0s 85us/step - loss: 2005.3057 - mse: 25899890.0000 - mae: 2005.3057 - val_loss: 2531.1046 - val_mse: 41271312.0000 - val_mae: 2531.1045\n",
            "Epoch 489/500\n",
            "1500/1500 [==============================] - 0s 90us/step - loss: 2015.7817 - mse: 26877706.0000 - mae: 2015.7816 - val_loss: 2513.1707 - val_mse: 40817116.0000 - val_mae: 2513.1709\n",
            "Epoch 490/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 2028.4624 - mse: 26982264.0000 - mae: 2028.4625 - val_loss: 2517.3497 - val_mse: 40822376.0000 - val_mae: 2517.3496\n",
            "Epoch 491/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 1986.0050 - mse: 26122058.0000 - mae: 1986.0050 - val_loss: 2510.8239 - val_mse: 40709632.0000 - val_mae: 2510.8237\n",
            "Epoch 492/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 1994.5328 - mse: 26357220.0000 - mae: 1994.5327 - val_loss: 2492.3038 - val_mse: 40303856.0000 - val_mae: 2492.3037\n",
            "Epoch 493/500\n",
            "1500/1500 [==============================] - 0s 82us/step - loss: 2010.2039 - mse: 27590238.0000 - mae: 2010.2039 - val_loss: 2498.0805 - val_mse: 40340016.0000 - val_mae: 2498.0806\n",
            "Epoch 494/500\n",
            "1500/1500 [==============================] - 0s 92us/step - loss: 2054.0516 - mse: 28302426.0000 - mae: 2054.0518 - val_loss: 2487.8872 - val_mse: 40206924.0000 - val_mae: 2487.8872\n",
            "Epoch 495/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 1968.1455 - mse: 25254734.0000 - mae: 1968.1454 - val_loss: 2489.4562 - val_mse: 40192192.0000 - val_mae: 2489.4561\n",
            "Epoch 496/500\n",
            "1500/1500 [==============================] - 0s 86us/step - loss: 1942.0428 - mse: 25336142.0000 - mae: 1942.0428 - val_loss: 2483.6729 - val_mse: 39986420.0000 - val_mae: 2483.6731\n",
            "Epoch 497/500\n",
            "1500/1500 [==============================] - 0s 90us/step - loss: 1992.8701 - mse: 25556006.0000 - mae: 1992.8701 - val_loss: 2487.3255 - val_mse: 40195616.0000 - val_mae: 2487.3254\n",
            "Epoch 498/500\n",
            "1500/1500 [==============================] - 0s 88us/step - loss: 2032.8518 - mse: 27461818.0000 - mae: 2032.8518 - val_loss: 2485.6149 - val_mse: 40010524.0000 - val_mae: 2485.6150\n",
            "Epoch 499/500\n",
            "1500/1500 [==============================] - 0s 80us/step - loss: 1984.0784 - mse: 27347876.0000 - mae: 1984.0785 - val_loss: 2475.9459 - val_mse: 39728236.0000 - val_mae: 2475.9460\n",
            "Epoch 500/500\n",
            "1500/1500 [==============================] - 0s 89us/step - loss: 1967.1793 - mse: 26313064.0000 - mae: 1967.1793 - val_loss: 2472.6536 - val_mse: 39689512.0000 - val_mae: 2472.6536\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1880 samples, validate on 1870 samples\n",
            "Epoch 1/500\n",
            "1880/1880 [==============================] - 2s 1ms/step - loss: 4426.2392 - mse: 132218808.0000 - mae: 4426.2393 - val_loss: 4425.5934 - val_mse: 136897792.0000 - val_mae: 4425.5933\n",
            "Epoch 2/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 4422.6077 - mse: 132204440.0000 - mae: 4422.6079 - val_loss: 4415.9759 - val_mse: 136860720.0000 - val_mae: 4415.9756\n",
            "Epoch 3/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 4408.4292 - mse: 132151960.0000 - mae: 4408.4292 - val_loss: 4396.6287 - val_mse: 136781584.0000 - val_mae: 4396.6284\n",
            "Epoch 4/500\n",
            "1880/1880 [==============================] - 0s 85us/step - loss: 4390.0719 - mse: 132075872.0000 - mae: 4390.0718 - val_loss: 4376.8444 - val_mse: 136693728.0000 - val_mae: 4376.8442\n",
            "Epoch 5/500\n",
            "1880/1880 [==============================] - 0s 63us/step - loss: 4369.2856 - mse: 131992464.0000 - mae: 4369.2856 - val_loss: 4356.5874 - val_mse: 136594496.0000 - val_mae: 4356.5874\n",
            "Epoch 6/500\n",
            "1880/1880 [==============================] - 0s 63us/step - loss: 4346.4382 - mse: 131880792.0000 - mae: 4346.4385 - val_loss: 4332.6294 - val_mse: 136468880.0000 - val_mae: 4332.6294\n",
            "Epoch 7/500\n",
            "1880/1880 [==============================] - 0s 62us/step - loss: 4324.3413 - mse: 131766656.0000 - mae: 4324.3413 - val_loss: 4309.0036 - val_mse: 136330016.0000 - val_mae: 4309.0034\n",
            "Epoch 8/500\n",
            "1880/1880 [==============================] - 0s 64us/step - loss: 4299.2670 - mse: 131623792.0000 - mae: 4299.2671 - val_loss: 4285.5147 - val_mse: 136171840.0000 - val_mae: 4285.5146\n",
            "Epoch 9/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 4276.8453 - mse: 131451600.0000 - mae: 4276.8452 - val_loss: 4265.3420 - val_mse: 136009200.0000 - val_mae: 4265.3423\n",
            "Epoch 10/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4257.0360 - mse: 131299464.0000 - mae: 4257.0361 - val_loss: 4248.0402 - val_mse: 135841696.0000 - val_mae: 4248.0400\n",
            "Epoch 11/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 4240.2388 - mse: 131143968.0000 - mae: 4240.2388 - val_loss: 4233.4877 - val_mse: 135676432.0000 - val_mae: 4233.4873\n",
            "Epoch 12/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 4228.7544 - mse: 130990384.0000 - mae: 4228.7544 - val_loss: 4220.3978 - val_mse: 135508448.0000 - val_mae: 4220.3979\n",
            "Epoch 13/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 4216.0931 - mse: 130800472.0000 - mae: 4216.0933 - val_loss: 4210.0024 - val_mse: 135354176.0000 - val_mae: 4210.0024\n",
            "Epoch 14/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4203.2006 - mse: 130642584.0000 - mae: 4203.2007 - val_loss: 4200.3807 - val_mse: 135183344.0000 - val_mae: 4200.3809\n",
            "Epoch 15/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 4194.5725 - mse: 130432528.0000 - mae: 4194.5723 - val_loss: 4193.8663 - val_mse: 135039280.0000 - val_mae: 4193.8662\n",
            "Epoch 16/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 4184.1740 - mse: 130269896.0000 - mae: 4184.1738 - val_loss: 4188.1167 - val_mse: 134888448.0000 - val_mae: 4188.1167\n",
            "Epoch 17/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 4177.7779 - mse: 130163248.0000 - mae: 4177.7778 - val_loss: 4183.3236 - val_mse: 134738368.0000 - val_mae: 4183.3237\n",
            "Epoch 18/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4171.9230 - mse: 130006520.0000 - mae: 4171.9229 - val_loss: 4178.9911 - val_mse: 134585984.0000 - val_mae: 4178.9912\n",
            "Epoch 19/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 4171.5443 - mse: 129814440.0000 - mae: 4171.5439 - val_loss: 4175.5011 - val_mse: 134454672.0000 - val_mae: 4175.5010\n",
            "Epoch 20/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4167.9653 - mse: 129735296.0000 - mae: 4167.9653 - val_loss: 4172.3153 - val_mse: 134315472.0000 - val_mae: 4172.3154\n",
            "Epoch 21/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 4166.9620 - mse: 129563224.0000 - mae: 4166.9619 - val_loss: 4169.1117 - val_mse: 134186552.0000 - val_mae: 4169.1113\n",
            "Epoch 22/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 4162.1147 - mse: 129511752.0000 - mae: 4162.1147 - val_loss: 4166.0174 - val_mse: 134061320.0000 - val_mae: 4166.0176\n",
            "Epoch 23/500\n",
            "1880/1880 [==============================] - 0s 64us/step - loss: 4148.4572 - mse: 129318600.0000 - mae: 4148.4570 - val_loss: 4162.7365 - val_mse: 133911136.0000 - val_mae: 4162.7368\n",
            "Epoch 24/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 4149.4524 - mse: 129154384.0000 - mae: 4149.4526 - val_loss: 4158.6624 - val_mse: 133736776.0000 - val_mae: 4158.6626\n",
            "Epoch 25/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 4150.5214 - mse: 129091696.0000 - mae: 4150.5215 - val_loss: 4153.2372 - val_mse: 133444704.0000 - val_mae: 4153.2373\n",
            "Epoch 26/500\n",
            "1880/1880 [==============================] - 0s 88us/step - loss: 4143.7411 - mse: 128645856.0000 - mae: 4143.7407 - val_loss: 4148.5248 - val_mse: 133200904.0000 - val_mae: 4148.5249\n",
            "Epoch 27/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 4138.5681 - mse: 128448352.0000 - mae: 4138.5679 - val_loss: 4143.2547 - val_mse: 132934080.0000 - val_mae: 4143.2549\n",
            "Epoch 28/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4133.7497 - mse: 128134976.0000 - mae: 4133.7495 - val_loss: 4137.2972 - val_mse: 132613968.0000 - val_mae: 4137.2969\n",
            "Epoch 29/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 4122.4704 - mse: 127869704.0000 - mae: 4122.4702 - val_loss: 4132.9904 - val_mse: 132387064.0000 - val_mae: 4132.9902\n",
            "Epoch 30/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 4131.2852 - mse: 127590312.0000 - mae: 4131.2852 - val_loss: 4129.2307 - val_mse: 132177904.0000 - val_mae: 4129.2310\n",
            "Epoch 31/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 4115.8839 - mse: 127497528.0000 - mae: 4115.8843 - val_loss: 4125.9962 - val_mse: 131980576.0000 - val_mae: 4125.9961\n",
            "Epoch 32/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 4119.3174 - mse: 127283560.0000 - mae: 4119.3174 - val_loss: 4123.6080 - val_mse: 131828456.0000 - val_mae: 4123.6079\n",
            "Epoch 33/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 4118.8545 - mse: 127121936.0000 - mae: 4118.8545 - val_loss: 4121.9616 - val_mse: 131705856.0000 - val_mae: 4121.9619\n",
            "Epoch 34/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 4120.8435 - mse: 127058296.0000 - mae: 4120.8433 - val_loss: 4120.3569 - val_mse: 131587168.0000 - val_mae: 4120.3564\n",
            "Epoch 35/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4121.6995 - mse: 126994544.0000 - mae: 4121.6997 - val_loss: 4118.9190 - val_mse: 131470856.0000 - val_mae: 4118.9189\n",
            "Epoch 36/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 4113.0512 - mse: 126742984.0000 - mae: 4113.0513 - val_loss: 4117.1889 - val_mse: 131375048.0000 - val_mae: 4117.1890\n",
            "Epoch 37/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4108.4935 - mse: 126531168.0000 - mae: 4108.4937 - val_loss: 4115.5006 - val_mse: 131269320.0000 - val_mae: 4115.5010\n",
            "Epoch 38/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 4108.3935 - mse: 126636608.0000 - mae: 4108.3936 - val_loss: 4113.6721 - val_mse: 131153960.0000 - val_mae: 4113.6719\n",
            "Epoch 39/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 4109.4363 - mse: 126301208.0000 - mae: 4109.4360 - val_loss: 4111.7323 - val_mse: 131010768.0000 - val_mae: 4111.7324\n",
            "Epoch 40/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 4115.3950 - mse: 126298672.0000 - mae: 4115.3950 - val_loss: 4110.6320 - val_mse: 130931016.0000 - val_mae: 4110.6318\n",
            "Epoch 41/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 4102.6184 - mse: 126164912.0000 - mae: 4102.6182 - val_loss: 4108.9150 - val_mse: 130779024.0000 - val_mae: 4108.9150\n",
            "Epoch 42/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 4108.1436 - mse: 126156528.0000 - mae: 4108.1436 - val_loss: 4107.8490 - val_mse: 130752640.0000 - val_mae: 4107.8491\n",
            "Epoch 43/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 4103.6193 - mse: 126324656.0000 - mae: 4103.6196 - val_loss: 4106.8301 - val_mse: 130673744.0000 - val_mae: 4106.8301\n",
            "Epoch 44/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 4099.6402 - mse: 125995080.0000 - mae: 4099.6401 - val_loss: 4106.0340 - val_mse: 130613360.0000 - val_mae: 4106.0342\n",
            "Epoch 45/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 4108.1733 - mse: 126021024.0000 - mae: 4108.1729 - val_loss: 4104.7387 - val_mse: 130570520.0000 - val_mae: 4104.7393\n",
            "Epoch 46/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 4099.9278 - mse: 125893768.0000 - mae: 4099.9277 - val_loss: 4102.8995 - val_mse: 130511520.0000 - val_mae: 4102.8999\n",
            "Epoch 47/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 4095.9531 - mse: 125778976.0000 - mae: 4095.9529 - val_loss: 4100.8067 - val_mse: 130425320.0000 - val_mae: 4100.8066\n",
            "Epoch 48/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 4089.4468 - mse: 125728224.0000 - mae: 4089.4468 - val_loss: 4098.3348 - val_mse: 130338528.0000 - val_mae: 4098.3350\n",
            "Epoch 49/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 4095.4044 - mse: 125664928.0000 - mae: 4095.4048 - val_loss: 4094.8082 - val_mse: 130293904.0000 - val_mae: 4094.8083\n",
            "Epoch 50/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 4096.3075 - mse: 125695264.0000 - mae: 4096.3076 - val_loss: 4084.1173 - val_mse: 130281000.0000 - val_mae: 4084.1172\n",
            "Epoch 51/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 4075.6644 - mse: 125654160.0000 - mae: 4075.6643 - val_loss: 4057.3907 - val_mse: 130297376.0000 - val_mae: 4057.3904\n",
            "Epoch 52/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 4052.2800 - mse: 125585560.0000 - mae: 4052.2800 - val_loss: 4044.9939 - val_mse: 129993080.0000 - val_mae: 4044.9939\n",
            "Epoch 53/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 4042.3868 - mse: 125046880.0000 - mae: 4042.3867 - val_loss: 4035.1746 - val_mse: 129479592.0000 - val_mae: 4035.1746\n",
            "Epoch 54/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 4036.8713 - mse: 124668872.0000 - mae: 4036.8713 - val_loss: 4023.1786 - val_mse: 129161120.0000 - val_mae: 4023.1787\n",
            "Epoch 55/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 4025.8471 - mse: 124372128.0000 - mae: 4025.8474 - val_loss: 4016.1238 - val_mse: 128679896.0000 - val_mae: 4016.1235\n",
            "Epoch 56/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 4015.7965 - mse: 123964336.0000 - mae: 4015.7966 - val_loss: 4005.0720 - val_mse: 128546896.0000 - val_mae: 4005.0723\n",
            "Epoch 57/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 4002.9143 - mse: 123476632.0000 - mae: 4002.9141 - val_loss: 4005.1768 - val_mse: 128332408.0000 - val_mae: 4005.1770\n",
            "Epoch 58/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 4003.3410 - mse: 123670296.0000 - mae: 4003.3411 - val_loss: 3994.7468 - val_mse: 127957808.0000 - val_mae: 3994.7471\n",
            "Epoch 59/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3990.3862 - mse: 123121744.0000 - mae: 3990.3862 - val_loss: 3979.5960 - val_mse: 127394240.0000 - val_mae: 3979.5962\n",
            "Epoch 60/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3986.3610 - mse: 122688248.0000 - mae: 3986.3608 - val_loss: 3974.1633 - val_mse: 127155608.0000 - val_mae: 3974.1633\n",
            "Epoch 61/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3984.8787 - mse: 122375808.0000 - mae: 3984.8784 - val_loss: 3967.9326 - val_mse: 126821808.0000 - val_mae: 3967.9324\n",
            "Epoch 62/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3970.0125 - mse: 121851472.0000 - mae: 3970.0122 - val_loss: 3956.5107 - val_mse: 126309696.0000 - val_mae: 3956.5110\n",
            "Epoch 63/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3957.0604 - mse: 121634392.0000 - mae: 3957.0601 - val_loss: 3954.8751 - val_mse: 126116224.0000 - val_mae: 3954.8752\n",
            "Epoch 64/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3953.6903 - mse: 121323832.0000 - mae: 3953.6904 - val_loss: 3943.4394 - val_mse: 125647952.0000 - val_mae: 3943.4390\n",
            "Epoch 65/500\n",
            "1880/1880 [==============================] - 0s 63us/step - loss: 3950.8650 - mse: 120584312.0000 - mae: 3950.8650 - val_loss: 3942.2371 - val_mse: 125425192.0000 - val_mae: 3942.2371\n",
            "Epoch 66/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3953.2751 - mse: 120611688.0000 - mae: 3953.2749 - val_loss: 3937.3524 - val_mse: 125105456.0000 - val_mae: 3937.3521\n",
            "Epoch 67/500\n",
            "1880/1880 [==============================] - 0s 63us/step - loss: 3933.8563 - mse: 120305040.0000 - mae: 3933.8564 - val_loss: 3933.7394 - val_mse: 124835344.0000 - val_mae: 3933.7393\n",
            "Epoch 68/500\n",
            "1880/1880 [==============================] - 0s 64us/step - loss: 3934.2848 - mse: 120257624.0000 - mae: 3934.2849 - val_loss: 3915.4696 - val_mse: 124312592.0000 - val_mae: 3915.4692\n",
            "Epoch 69/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3935.1794 - mse: 119746696.0000 - mae: 3935.1794 - val_loss: 3913.3757 - val_mse: 124061784.0000 - val_mae: 3913.3755\n",
            "Epoch 70/500\n",
            "1880/1880 [==============================] - 0s 62us/step - loss: 3924.9783 - mse: 119307176.0000 - mae: 3924.9785 - val_loss: 3902.2561 - val_mse: 123651232.0000 - val_mae: 3902.2559\n",
            "Epoch 71/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3931.5016 - mse: 119018992.0000 - mae: 3931.5020 - val_loss: 3893.6675 - val_mse: 123259504.0000 - val_mae: 3893.6677\n",
            "Epoch 72/500\n",
            "1880/1880 [==============================] - 0s 64us/step - loss: 3907.8648 - mse: 118516496.0000 - mae: 3907.8650 - val_loss: 3886.2236 - val_mse: 122894096.0000 - val_mae: 3886.2236\n",
            "Epoch 73/500\n",
            "1880/1880 [==============================] - 0s 61us/step - loss: 3916.5617 - mse: 118166048.0000 - mae: 3916.5615 - val_loss: 3883.6558 - val_mse: 122733808.0000 - val_mae: 3883.6562\n",
            "Epoch 74/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3888.0876 - mse: 117757552.0000 - mae: 3888.0876 - val_loss: 3873.0029 - val_mse: 122217944.0000 - val_mae: 3873.0027\n",
            "Epoch 75/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3890.8276 - mse: 117554952.0000 - mae: 3890.8274 - val_loss: 3865.5068 - val_mse: 121824240.0000 - val_mae: 3865.5063\n",
            "Epoch 76/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3899.3770 - mse: 117731464.0000 - mae: 3899.3770 - val_loss: 3863.3973 - val_mse: 121717248.0000 - val_mae: 3863.3972\n",
            "Epoch 77/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3891.9230 - mse: 116987184.0000 - mae: 3891.9231 - val_loss: 3851.6772 - val_mse: 121251048.0000 - val_mae: 3851.6772\n",
            "Epoch 78/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3883.6966 - mse: 116835360.0000 - mae: 3883.6963 - val_loss: 3847.2761 - val_mse: 121038648.0000 - val_mae: 3847.2761\n",
            "Epoch 79/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3878.5004 - mse: 116617872.0000 - mae: 3878.5002 - val_loss: 3840.0352 - val_mse: 120696424.0000 - val_mae: 3840.0349\n",
            "Epoch 80/500\n",
            "1880/1880 [==============================] - 0s 62us/step - loss: 3856.5744 - mse: 116041064.0000 - mae: 3856.5747 - val_loss: 3833.7705 - val_mse: 120349896.0000 - val_mae: 3833.7700\n",
            "Epoch 81/500\n",
            "1880/1880 [==============================] - 0s 158us/step - loss: 3872.7341 - mse: 115838224.0000 - mae: 3872.7341 - val_loss: 3827.9001 - val_mse: 119867760.0000 - val_mae: 3827.8999\n",
            "Epoch 82/500\n",
            "1880/1880 [==============================] - 0s 135us/step - loss: 3879.4849 - mse: 116235400.0000 - mae: 3879.4851 - val_loss: 3822.3880 - val_mse: 119713200.0000 - val_mae: 3822.3877\n",
            "Epoch 83/500\n",
            "1880/1880 [==============================] - 0s 150us/step - loss: 3830.9123 - mse: 114706680.0000 - mae: 3830.9124 - val_loss: 3818.5521 - val_mse: 119480128.0000 - val_mae: 3818.5518\n",
            "Epoch 84/500\n",
            "1880/1880 [==============================] - 0s 143us/step - loss: 3852.2089 - mse: 115073328.0000 - mae: 3852.2087 - val_loss: 3810.6312 - val_mse: 119014496.0000 - val_mae: 3810.6311\n",
            "Epoch 85/500\n",
            "1880/1880 [==============================] - 0s 180us/step - loss: 3838.6232 - mse: 113693048.0000 - mae: 3838.6230 - val_loss: 3810.0833 - val_mse: 118917272.0000 - val_mae: 3810.0833\n",
            "Epoch 86/500\n",
            "1880/1880 [==============================] - 0s 123us/step - loss: 3844.4365 - mse: 114672240.0000 - mae: 3844.4368 - val_loss: 3799.9321 - val_mse: 118386440.0000 - val_mae: 3799.9321\n",
            "Epoch 87/500\n",
            "1880/1880 [==============================] - 0s 106us/step - loss: 3825.5135 - mse: 113833960.0000 - mae: 3825.5137 - val_loss: 3796.2256 - val_mse: 118001184.0000 - val_mae: 3796.2256\n",
            "Epoch 88/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3831.0253 - mse: 113794008.0000 - mae: 3831.0251 - val_loss: 3790.2177 - val_mse: 117801520.0000 - val_mae: 3790.2180\n",
            "Epoch 89/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3806.3746 - mse: 113032088.0000 - mae: 3806.3745 - val_loss: 3784.4823 - val_mse: 117534792.0000 - val_mae: 3784.4824\n",
            "Epoch 90/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3834.0682 - mse: 113646176.0000 - mae: 3834.0684 - val_loss: 3783.0637 - val_mse: 117247360.0000 - val_mae: 3783.0637\n",
            "Epoch 91/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3803.3475 - mse: 112136232.0000 - mae: 3803.3472 - val_loss: 3775.5501 - val_mse: 116874928.0000 - val_mae: 3775.5503\n",
            "Epoch 92/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3785.0104 - mse: 112218328.0000 - mae: 3785.0100 - val_loss: 3770.9343 - val_mse: 116492784.0000 - val_mae: 3770.9343\n",
            "Epoch 93/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3793.0396 - mse: 112285392.0000 - mae: 3793.0393 - val_loss: 3765.1609 - val_mse: 116190632.0000 - val_mae: 3765.1609\n",
            "Epoch 94/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3789.4656 - mse: 111868192.0000 - mae: 3789.4653 - val_loss: 3758.7418 - val_mse: 115696632.0000 - val_mae: 3758.7422\n",
            "Epoch 95/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3762.8121 - mse: 110994024.0000 - mae: 3762.8123 - val_loss: 3760.2720 - val_mse: 115207848.0000 - val_mae: 3760.2720\n",
            "Epoch 96/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3778.3242 - mse: 111612176.0000 - mae: 3778.3242 - val_loss: 3747.4433 - val_mse: 115158232.0000 - val_mae: 3747.4436\n",
            "Epoch 97/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3767.2310 - mse: 110927928.0000 - mae: 3767.2312 - val_loss: 3739.2837 - val_mse: 114718416.0000 - val_mae: 3739.2837\n",
            "Epoch 98/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3735.4852 - mse: 109702784.0000 - mae: 3735.4854 - val_loss: 3743.7608 - val_mse: 113989184.0000 - val_mae: 3743.7607\n",
            "Epoch 99/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3729.5737 - mse: 109909160.0000 - mae: 3729.5740 - val_loss: 3726.6931 - val_mse: 113735920.0000 - val_mae: 3726.6934\n",
            "Epoch 100/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3739.3322 - mse: 109628928.0000 - mae: 3739.3315 - val_loss: 3717.2047 - val_mse: 113348768.0000 - val_mae: 3717.2046\n",
            "Epoch 101/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3714.8196 - mse: 109116048.0000 - mae: 3714.8196 - val_loss: 3707.0572 - val_mse: 113067200.0000 - val_mae: 3707.0571\n",
            "Epoch 102/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 3720.5421 - mse: 109089904.0000 - mae: 3720.5420 - val_loss: 3709.2255 - val_mse: 112371120.0000 - val_mae: 3709.2251\n",
            "Epoch 103/500\n",
            "1880/1880 [==============================] - 0s 64us/step - loss: 3707.3326 - mse: 107859368.0000 - mae: 3707.3328 - val_loss: 3699.0509 - val_mse: 112403056.0000 - val_mae: 3699.0505\n",
            "Epoch 104/500\n",
            "1880/1880 [==============================] - 0s 63us/step - loss: 3704.3308 - mse: 108095976.0000 - mae: 3704.3311 - val_loss: 3686.9531 - val_mse: 111658728.0000 - val_mae: 3686.9529\n",
            "Epoch 105/500\n",
            "1880/1880 [==============================] - 0s 64us/step - loss: 3681.9091 - mse: 107021968.0000 - mae: 3681.9094 - val_loss: 3677.6387 - val_mse: 111369416.0000 - val_mae: 3677.6384\n",
            "Epoch 106/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3692.8966 - mse: 107618184.0000 - mae: 3692.8967 - val_loss: 3675.2946 - val_mse: 110721376.0000 - val_mae: 3675.2944\n",
            "Epoch 107/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3657.9016 - mse: 106012312.0000 - mae: 3657.9016 - val_loss: 3667.2783 - val_mse: 110332456.0000 - val_mae: 3667.2786\n",
            "Epoch 108/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3665.9763 - mse: 105715200.0000 - mae: 3665.9766 - val_loss: 3656.4760 - val_mse: 110074008.0000 - val_mae: 3656.4763\n",
            "Epoch 109/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3648.9066 - mse: 105761592.0000 - mae: 3648.9065 - val_loss: 3650.0245 - val_mse: 109694504.0000 - val_mae: 3650.0247\n",
            "Epoch 110/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3634.9406 - mse: 105640384.0000 - mae: 3634.9407 - val_loss: 3647.8964 - val_mse: 109452592.0000 - val_mae: 3647.8962\n",
            "Epoch 111/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3635.1130 - mse: 105501168.0000 - mae: 3635.1128 - val_loss: 3637.1472 - val_mse: 108860992.0000 - val_mae: 3637.1470\n",
            "Epoch 112/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3638.3257 - mse: 105378080.0000 - mae: 3638.3257 - val_loss: 3655.5546 - val_mse: 108252184.0000 - val_mae: 3655.5549\n",
            "Epoch 113/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3616.6670 - mse: 103382272.0000 - mae: 3616.6672 - val_loss: 3625.7520 - val_mse: 107959320.0000 - val_mae: 3625.7520\n",
            "Epoch 114/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3632.3155 - mse: 104042128.0000 - mae: 3632.3152 - val_loss: 3631.2820 - val_mse: 107414496.0000 - val_mae: 3631.2820\n",
            "Epoch 115/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3589.3640 - mse: 103076424.0000 - mae: 3589.3638 - val_loss: 3614.2649 - val_mse: 107097592.0000 - val_mae: 3614.2651\n",
            "Epoch 116/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3602.0269 - mse: 102852600.0000 - mae: 3602.0269 - val_loss: 3609.7347 - val_mse: 106613968.0000 - val_mae: 3609.7349\n",
            "Epoch 117/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3577.1706 - mse: 101507328.0000 - mae: 3577.1707 - val_loss: 3617.2878 - val_mse: 105962488.0000 - val_mae: 3617.2881\n",
            "Epoch 118/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3600.7099 - mse: 102878576.0000 - mae: 3600.7104 - val_loss: 3634.9284 - val_mse: 105480064.0000 - val_mae: 3634.9282\n",
            "Epoch 119/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3615.3924 - mse: 102292192.0000 - mae: 3615.3923 - val_loss: 3599.7220 - val_mse: 105366568.0000 - val_mae: 3599.7219\n",
            "Epoch 120/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 3580.1410 - mse: 100923032.0000 - mae: 3580.1409 - val_loss: 3590.6577 - val_mse: 105067496.0000 - val_mae: 3590.6580\n",
            "Epoch 121/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3576.9776 - mse: 100472152.0000 - mae: 3576.9775 - val_loss: 3599.9300 - val_mse: 104627232.0000 - val_mae: 3599.9299\n",
            "Epoch 122/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3583.8831 - mse: 101104408.0000 - mae: 3583.8831 - val_loss: 3593.7038 - val_mse: 104379176.0000 - val_mae: 3593.7039\n",
            "Epoch 123/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3583.4242 - mse: 99985296.0000 - mae: 3583.4243 - val_loss: 3578.7804 - val_mse: 104512632.0000 - val_mae: 3578.7805\n",
            "Epoch 124/500\n",
            "1880/1880 [==============================] - 0s 63us/step - loss: 3564.3038 - mse: 98960608.0000 - mae: 3564.3037 - val_loss: 3586.8873 - val_mse: 103872392.0000 - val_mae: 3586.8872\n",
            "Epoch 125/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3578.0571 - mse: 99128376.0000 - mae: 3578.0569 - val_loss: 3586.3352 - val_mse: 103516576.0000 - val_mae: 3586.3350\n",
            "Epoch 126/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3562.8121 - mse: 99174880.0000 - mae: 3562.8120 - val_loss: 3568.1259 - val_mse: 103597424.0000 - val_mae: 3568.1260\n",
            "Epoch 127/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3577.3779 - mse: 99613256.0000 - mae: 3577.3779 - val_loss: 3569.9580 - val_mse: 103174544.0000 - val_mae: 3569.9580\n",
            "Epoch 128/500\n",
            "1880/1880 [==============================] - 0s 65us/step - loss: 3564.7084 - mse: 99395424.0000 - mae: 3564.7085 - val_loss: 3570.0910 - val_mse: 102905128.0000 - val_mae: 3570.0908\n",
            "Epoch 129/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3581.2420 - mse: 99440040.0000 - mae: 3581.2419 - val_loss: 3561.6049 - val_mse: 102745680.0000 - val_mae: 3561.6050\n",
            "Epoch 130/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3563.6931 - mse: 99150224.0000 - mae: 3563.6931 - val_loss: 3558.4692 - val_mse: 102683424.0000 - val_mae: 3558.4692\n",
            "Epoch 131/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3547.1122 - mse: 98515200.0000 - mae: 3547.1121 - val_loss: 3556.0543 - val_mse: 102361768.0000 - val_mae: 3556.0542\n",
            "Epoch 132/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3574.8518 - mse: 98687104.0000 - mae: 3574.8518 - val_loss: 3554.2366 - val_mse: 102076072.0000 - val_mae: 3554.2366\n",
            "Epoch 133/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3558.5313 - mse: 98394120.0000 - mae: 3558.5315 - val_loss: 3554.5671 - val_mse: 101604520.0000 - val_mae: 3554.5669\n",
            "Epoch 134/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3549.9388 - mse: 97669792.0000 - mae: 3549.9385 - val_loss: 3549.7132 - val_mse: 101619656.0000 - val_mae: 3549.7131\n",
            "Epoch 135/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3544.0183 - mse: 96956680.0000 - mae: 3544.0181 - val_loss: 3547.7804 - val_mse: 101386624.0000 - val_mae: 3547.7800\n",
            "Epoch 136/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3552.2480 - mse: 97578528.0000 - mae: 3552.2478 - val_loss: 3545.7847 - val_mse: 101146112.0000 - val_mae: 3545.7844\n",
            "Epoch 137/500\n",
            "1880/1880 [==============================] - 0s 62us/step - loss: 3553.6068 - mse: 97975120.0000 - mae: 3553.6067 - val_loss: 3546.9011 - val_mse: 100888168.0000 - val_mae: 3546.9011\n",
            "Epoch 138/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3530.5713 - mse: 97332216.0000 - mae: 3530.5713 - val_loss: 3548.5053 - val_mse: 100655552.0000 - val_mae: 3548.5051\n",
            "Epoch 139/500\n",
            "1880/1880 [==============================] - 0s 64us/step - loss: 3536.7885 - mse: 96731904.0000 - mae: 3536.7883 - val_loss: 3551.8565 - val_mse: 100376112.0000 - val_mae: 3551.8562\n",
            "Epoch 140/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3553.2236 - mse: 97253048.0000 - mae: 3553.2236 - val_loss: 3547.5328 - val_mse: 100292824.0000 - val_mae: 3547.5327\n",
            "Epoch 141/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3550.6347 - mse: 97262576.0000 - mae: 3550.6345 - val_loss: 3563.9350 - val_mse: 99980592.0000 - val_mae: 3563.9353\n",
            "Epoch 142/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3541.1982 - mse: 96190136.0000 - mae: 3541.1985 - val_loss: 3543.9059 - val_mse: 100252664.0000 - val_mae: 3543.9058\n",
            "Epoch 143/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3542.8651 - mse: 95877728.0000 - mae: 3542.8650 - val_loss: 3533.8725 - val_mse: 99889904.0000 - val_mae: 3533.8723\n",
            "Epoch 144/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3553.6487 - mse: 97260592.0000 - mae: 3553.6484 - val_loss: 3533.4522 - val_mse: 99958568.0000 - val_mae: 3533.4524\n",
            "Epoch 145/500\n",
            "1880/1880 [==============================] - 0s 65us/step - loss: 3510.1533 - mse: 95596168.0000 - mae: 3510.1531 - val_loss: 3531.3780 - val_mse: 99688224.0000 - val_mae: 3531.3784\n",
            "Epoch 146/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 3509.8943 - mse: 95025432.0000 - mae: 3509.8940 - val_loss: 3531.4978 - val_mse: 99471064.0000 - val_mae: 3531.4980\n",
            "Epoch 147/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3553.7630 - mse: 96651176.0000 - mae: 3553.7629 - val_loss: 3534.0059 - val_mse: 99290144.0000 - val_mae: 3534.0059\n",
            "Epoch 148/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 3514.1925 - mse: 95775984.0000 - mae: 3514.1926 - val_loss: 3532.0584 - val_mse: 99098192.0000 - val_mae: 3532.0588\n",
            "Epoch 149/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3514.8724 - mse: 95065272.0000 - mae: 3514.8723 - val_loss: 3526.1970 - val_mse: 99031824.0000 - val_mae: 3526.1968\n",
            "Epoch 150/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3513.4657 - mse: 95280800.0000 - mae: 3513.4661 - val_loss: 3542.0955 - val_mse: 98617336.0000 - val_mae: 3542.0952\n",
            "Epoch 151/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3515.5128 - mse: 93901976.0000 - mae: 3515.5129 - val_loss: 3524.0412 - val_mse: 98690504.0000 - val_mae: 3524.0417\n",
            "Epoch 152/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3514.4872 - mse: 95134296.0000 - mae: 3514.4873 - val_loss: 3522.6519 - val_mse: 98547752.0000 - val_mae: 3522.6519\n",
            "Epoch 153/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3522.9915 - mse: 93730656.0000 - mae: 3522.9912 - val_loss: 3532.1533 - val_mse: 98373400.0000 - val_mae: 3532.1533\n",
            "Epoch 154/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3524.7328 - mse: 94141496.0000 - mae: 3524.7324 - val_loss: 3531.5944 - val_mse: 98431816.0000 - val_mae: 3531.5947\n",
            "Epoch 155/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 3502.8880 - mse: 93889672.0000 - mae: 3502.8879 - val_loss: 3539.8856 - val_mse: 97852872.0000 - val_mae: 3539.8853\n",
            "Epoch 156/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3519.7047 - mse: 94962264.0000 - mae: 3519.7046 - val_loss: 3521.9283 - val_mse: 98121224.0000 - val_mae: 3521.9280\n",
            "Epoch 157/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3487.4649 - mse: 92401568.0000 - mae: 3487.4648 - val_loss: 3518.4733 - val_mse: 97973232.0000 - val_mae: 3518.4729\n",
            "Epoch 158/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3538.6451 - mse: 95176272.0000 - mae: 3538.6453 - val_loss: 3520.1264 - val_mse: 97848680.0000 - val_mae: 3520.1267\n",
            "Epoch 159/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3510.4053 - mse: 93714976.0000 - mae: 3510.4053 - val_loss: 3525.9075 - val_mse: 97745536.0000 - val_mae: 3525.9072\n",
            "Epoch 160/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3499.4299 - mse: 92704536.0000 - mae: 3499.4297 - val_loss: 3533.0903 - val_mse: 97698472.0000 - val_mae: 3533.0903\n",
            "Epoch 161/500\n",
            "1880/1880 [==============================] - 0s 63us/step - loss: 3501.3525 - mse: 93909120.0000 - mae: 3501.3525 - val_loss: 3512.9137 - val_mse: 97527680.0000 - val_mae: 3512.9136\n",
            "Epoch 162/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3477.6506 - mse: 92350280.0000 - mae: 3477.6506 - val_loss: 3522.7388 - val_mse: 97071888.0000 - val_mae: 3522.7388\n",
            "Epoch 163/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3504.2086 - mse: 94042672.0000 - mae: 3504.2085 - val_loss: 3521.9553 - val_mse: 97634368.0000 - val_mae: 3521.9553\n",
            "Epoch 164/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3533.7202 - mse: 94262928.0000 - mae: 3533.7202 - val_loss: 3577.1808 - val_mse: 97057176.0000 - val_mae: 3577.1804\n",
            "Epoch 165/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3523.9357 - mse: 93827992.0000 - mae: 3523.9355 - val_loss: 3511.8311 - val_mse: 97332616.0000 - val_mae: 3511.8313\n",
            "Epoch 166/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3514.2351 - mse: 93116536.0000 - mae: 3514.2351 - val_loss: 3513.0957 - val_mse: 97334752.0000 - val_mae: 3513.0957\n",
            "Epoch 167/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3478.4422 - mse: 92777752.0000 - mae: 3478.4424 - val_loss: 3512.8281 - val_mse: 97035912.0000 - val_mae: 3512.8281\n",
            "Epoch 168/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3495.0286 - mse: 94038384.0000 - mae: 3495.0288 - val_loss: 3513.0864 - val_mse: 96712024.0000 - val_mae: 3513.0864\n",
            "Epoch 169/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3505.0226 - mse: 92464304.0000 - mae: 3505.0229 - val_loss: 3511.3589 - val_mse: 96816568.0000 - val_mae: 3511.3589\n",
            "Epoch 170/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3481.6232 - mse: 91772432.0000 - mae: 3481.6233 - val_loss: 3507.6707 - val_mse: 96851224.0000 - val_mae: 3507.6709\n",
            "Epoch 171/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3517.2930 - mse: 93875864.0000 - mae: 3517.2930 - val_loss: 3501.5436 - val_mse: 96636232.0000 - val_mae: 3501.5432\n",
            "Epoch 172/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3503.3612 - mse: 91802496.0000 - mae: 3503.3611 - val_loss: 3501.7659 - val_mse: 96534928.0000 - val_mae: 3501.7656\n",
            "Epoch 173/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3473.9024 - mse: 92164096.0000 - mae: 3473.9023 - val_loss: 3511.0228 - val_mse: 96092280.0000 - val_mae: 3511.0227\n",
            "Epoch 174/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3502.5849 - mse: 93650840.0000 - mae: 3502.5850 - val_loss: 3502.8343 - val_mse: 96391056.0000 - val_mae: 3502.8342\n",
            "Epoch 175/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3478.6770 - mse: 92914256.0000 - mae: 3478.6772 - val_loss: 3503.8537 - val_mse: 96351824.0000 - val_mae: 3503.8540\n",
            "Epoch 176/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3503.3623 - mse: 91968712.0000 - mae: 3503.3623 - val_loss: 3499.5429 - val_mse: 96118032.0000 - val_mae: 3499.5430\n",
            "Epoch 177/500\n",
            "1880/1880 [==============================] - 0s 82us/step - loss: 3461.5508 - mse: 90454424.0000 - mae: 3461.5505 - val_loss: 3505.9172 - val_mse: 95770064.0000 - val_mae: 3505.9170\n",
            "Epoch 178/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3474.9868 - mse: 90765792.0000 - mae: 3474.9871 - val_loss: 3497.5677 - val_mse: 95857248.0000 - val_mae: 3497.5681\n",
            "Epoch 179/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3468.2925 - mse: 92172616.0000 - mae: 3468.2922 - val_loss: 3502.1267 - val_mse: 95854832.0000 - val_mae: 3502.1265\n",
            "Epoch 180/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3480.9276 - mse: 90978640.0000 - mae: 3480.9277 - val_loss: 3504.8742 - val_mse: 95959984.0000 - val_mae: 3504.8743\n",
            "Epoch 181/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3472.3955 - mse: 90887680.0000 - mae: 3472.3953 - val_loss: 3500.6011 - val_mse: 95288648.0000 - val_mae: 3500.6008\n",
            "Epoch 182/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3444.4079 - mse: 90460104.0000 - mae: 3444.4080 - val_loss: 3493.3377 - val_mse: 95086640.0000 - val_mae: 3493.3376\n",
            "Epoch 183/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3473.5196 - mse: 91076144.0000 - mae: 3473.5198 - val_loss: 3487.3654 - val_mse: 95072632.0000 - val_mae: 3487.3652\n",
            "Epoch 184/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3486.6887 - mse: 92323384.0000 - mae: 3486.6885 - val_loss: 3497.0115 - val_mse: 94666928.0000 - val_mae: 3497.0112\n",
            "Epoch 185/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3486.7724 - mse: 91376752.0000 - mae: 3486.7725 - val_loss: 3486.4859 - val_mse: 95084328.0000 - val_mae: 3486.4858\n",
            "Epoch 186/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 3485.8555 - mse: 91963120.0000 - mae: 3485.8555 - val_loss: 3483.3193 - val_mse: 94772288.0000 - val_mae: 3483.3193\n",
            "Epoch 187/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3475.0279 - mse: 91362288.0000 - mae: 3475.0276 - val_loss: 3485.0774 - val_mse: 94704440.0000 - val_mae: 3485.0776\n",
            "Epoch 188/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3486.8405 - mse: 91596080.0000 - mae: 3486.8403 - val_loss: 3478.8803 - val_mse: 94639040.0000 - val_mae: 3478.8799\n",
            "Epoch 189/500\n",
            "1880/1880 [==============================] - 0s 65us/step - loss: 3491.1610 - mse: 91113448.0000 - mae: 3491.1611 - val_loss: 3475.4369 - val_mse: 94434872.0000 - val_mae: 3475.4370\n",
            "Epoch 190/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3439.1455 - mse: 89652736.0000 - mae: 3439.1455 - val_loss: 3471.4648 - val_mse: 94134368.0000 - val_mae: 3471.4646\n",
            "Epoch 191/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3475.2738 - mse: 90315080.0000 - mae: 3475.2737 - val_loss: 3479.9547 - val_mse: 93892120.0000 - val_mae: 3479.9548\n",
            "Epoch 192/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3440.1333 - mse: 90027944.0000 - mae: 3440.1335 - val_loss: 3471.8971 - val_mse: 93706352.0000 - val_mae: 3471.8972\n",
            "Epoch 193/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3434.0591 - mse: 89403824.0000 - mae: 3434.0591 - val_loss: 3464.7570 - val_mse: 93516384.0000 - val_mae: 3464.7568\n",
            "Epoch 194/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3448.4255 - mse: 88405232.0000 - mae: 3448.4255 - val_loss: 3465.8966 - val_mse: 93650832.0000 - val_mae: 3465.8967\n",
            "Epoch 195/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3447.7676 - mse: 89299120.0000 - mae: 3447.7676 - val_loss: 3489.9525 - val_mse: 93264416.0000 - val_mae: 3489.9521\n",
            "Epoch 196/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 3454.9151 - mse: 90377640.0000 - mae: 3454.9150 - val_loss: 3489.5821 - val_mse: 93067296.0000 - val_mae: 3489.5825\n",
            "Epoch 197/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3447.4762 - mse: 89144072.0000 - mae: 3447.4758 - val_loss: 3461.2324 - val_mse: 93045528.0000 - val_mae: 3461.2327\n",
            "Epoch 198/500\n",
            "1880/1880 [==============================] - 0s 65us/step - loss: 3445.8862 - mse: 89331448.0000 - mae: 3445.8862 - val_loss: 3470.5215 - val_mse: 92798200.0000 - val_mae: 3470.5215\n",
            "Epoch 199/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3452.0562 - mse: 89168376.0000 - mae: 3452.0562 - val_loss: 3468.2088 - val_mse: 92585080.0000 - val_mae: 3468.2087\n",
            "Epoch 200/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3442.0036 - mse: 89704128.0000 - mae: 3442.0034 - val_loss: 3456.4257 - val_mse: 92640712.0000 - val_mae: 3456.4253\n",
            "Epoch 201/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3444.2669 - mse: 88956632.0000 - mae: 3444.2666 - val_loss: 3460.0611 - val_mse: 92509344.0000 - val_mae: 3460.0608\n",
            "Epoch 202/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3428.1635 - mse: 88701144.0000 - mae: 3428.1633 - val_loss: 3448.7077 - val_mse: 92454288.0000 - val_mae: 3448.7078\n",
            "Epoch 203/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3431.8841 - mse: 89147192.0000 - mae: 3431.8840 - val_loss: 3469.4886 - val_mse: 92150784.0000 - val_mae: 3469.4885\n",
            "Epoch 204/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3423.1640 - mse: 89636496.0000 - mae: 3423.1643 - val_loss: 3461.5835 - val_mse: 91971112.0000 - val_mae: 3461.5835\n",
            "Epoch 205/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 3437.5519 - mse: 89106224.0000 - mae: 3437.5522 - val_loss: 3441.9678 - val_mse: 92055616.0000 - val_mae: 3441.9680\n",
            "Epoch 206/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3423.2211 - mse: 88632760.0000 - mae: 3423.2209 - val_loss: 3446.9436 - val_mse: 91679800.0000 - val_mae: 3446.9436\n",
            "Epoch 207/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3425.5756 - mse: 87941432.0000 - mae: 3425.5754 - val_loss: 3438.4441 - val_mse: 91742360.0000 - val_mae: 3438.4441\n",
            "Epoch 208/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3413.8737 - mse: 87530520.0000 - mae: 3413.8733 - val_loss: 3435.8016 - val_mse: 91624432.0000 - val_mae: 3435.8015\n",
            "Epoch 209/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3379.1804 - mse: 86550624.0000 - mae: 3379.1807 - val_loss: 3434.7039 - val_mse: 91386728.0000 - val_mae: 3434.7039\n",
            "Epoch 210/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3404.6126 - mse: 87281296.0000 - mae: 3404.6125 - val_loss: 3431.9590 - val_mse: 91042568.0000 - val_mae: 3431.9590\n",
            "Epoch 211/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 3412.9642 - mse: 88398240.0000 - mae: 3412.9644 - val_loss: 3429.9437 - val_mse: 90882232.0000 - val_mae: 3429.9438\n",
            "Epoch 212/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3408.8252 - mse: 87262472.0000 - mae: 3408.8252 - val_loss: 3476.5875 - val_mse: 90359840.0000 - val_mae: 3476.5872\n",
            "Epoch 213/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3407.4352 - mse: 87468032.0000 - mae: 3407.4351 - val_loss: 3426.0801 - val_mse: 90543272.0000 - val_mae: 3426.0798\n",
            "Epoch 214/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 3429.1682 - mse: 88699912.0000 - mae: 3429.1680 - val_loss: 3427.3182 - val_mse: 90444480.0000 - val_mae: 3427.3181\n",
            "Epoch 215/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3375.2326 - mse: 85413568.0000 - mae: 3375.2324 - val_loss: 3431.2752 - val_mse: 90060416.0000 - val_mae: 3431.2754\n",
            "Epoch 216/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3404.8693 - mse: 86879192.0000 - mae: 3404.8691 - val_loss: 3431.2753 - val_mse: 89926336.0000 - val_mae: 3431.2751\n",
            "Epoch 217/500\n",
            "1880/1880 [==============================] - 0s 65us/step - loss: 3371.3985 - mse: 84208392.0000 - mae: 3371.3987 - val_loss: 3422.8944 - val_mse: 89691920.0000 - val_mae: 3422.8943\n",
            "Epoch 218/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3394.3013 - mse: 86239384.0000 - mae: 3394.3013 - val_loss: 3437.1189 - val_mse: 89201144.0000 - val_mae: 3437.1187\n",
            "Epoch 219/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3378.6488 - mse: 84750944.0000 - mae: 3378.6487 - val_loss: 3413.1166 - val_mse: 89065008.0000 - val_mae: 3413.1169\n",
            "Epoch 220/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3370.2179 - mse: 85904568.0000 - mae: 3370.2178 - val_loss: 3410.2130 - val_mse: 89025896.0000 - val_mae: 3410.2131\n",
            "Epoch 221/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 3404.7662 - mse: 86333384.0000 - mae: 3404.7661 - val_loss: 3415.9040 - val_mse: 88604336.0000 - val_mae: 3415.9038\n",
            "Epoch 222/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3384.0002 - mse: 85043080.0000 - mae: 3384.0000 - val_loss: 3412.3053 - val_mse: 88661160.0000 - val_mae: 3412.3054\n",
            "Epoch 223/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 3391.2519 - mse: 86065136.0000 - mae: 3391.2520 - val_loss: 3408.0174 - val_mse: 88510240.0000 - val_mae: 3408.0173\n",
            "Epoch 224/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3363.4427 - mse: 84687656.0000 - mae: 3363.4426 - val_loss: 3403.4683 - val_mse: 88345752.0000 - val_mae: 3403.4685\n",
            "Epoch 225/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 3364.4232 - mse: 84162424.0000 - mae: 3364.4233 - val_loss: 3403.3354 - val_mse: 88105424.0000 - val_mae: 3403.3354\n",
            "Epoch 226/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3359.3021 - mse: 84770432.0000 - mae: 3359.3018 - val_loss: 3397.2083 - val_mse: 88011656.0000 - val_mae: 3397.2083\n",
            "Epoch 227/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3379.8198 - mse: 83356688.0000 - mae: 3379.8196 - val_loss: 3400.7249 - val_mse: 87825776.0000 - val_mae: 3400.7249\n",
            "Epoch 228/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3376.8381 - mse: 85209088.0000 - mae: 3376.8381 - val_loss: 3393.2494 - val_mse: 87807920.0000 - val_mae: 3393.2498\n",
            "Epoch 229/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3316.7685 - mse: 82367616.0000 - mae: 3316.7686 - val_loss: 3414.4274 - val_mse: 87217768.0000 - val_mae: 3414.4272\n",
            "Epoch 230/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 3350.2901 - mse: 84073728.0000 - mae: 3350.2905 - val_loss: 3387.4743 - val_mse: 87256792.0000 - val_mae: 3387.4741\n",
            "Epoch 231/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3339.1200 - mse: 83553584.0000 - mae: 3339.1201 - val_loss: 3388.0471 - val_mse: 87001592.0000 - val_mae: 3388.0471\n",
            "Epoch 232/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3370.8728 - mse: 83989328.0000 - mae: 3370.8728 - val_loss: 3387.0423 - val_mse: 86812392.0000 - val_mae: 3387.0422\n",
            "Epoch 233/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3350.8914 - mse: 84121624.0000 - mae: 3350.8916 - val_loss: 3380.4894 - val_mse: 86703168.0000 - val_mae: 3380.4893\n",
            "Epoch 234/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3354.5221 - mse: 83508392.0000 - mae: 3354.5220 - val_loss: 3385.3191 - val_mse: 86209424.0000 - val_mae: 3385.3191\n",
            "Epoch 235/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 3334.5552 - mse: 82129472.0000 - mae: 3334.5554 - val_loss: 3386.7095 - val_mse: 85929824.0000 - val_mae: 3386.7090\n",
            "Epoch 236/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3366.9096 - mse: 84172464.0000 - mae: 3366.9094 - val_loss: 3374.3204 - val_mse: 86064376.0000 - val_mae: 3374.3206\n",
            "Epoch 237/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3353.9763 - mse: 83602704.0000 - mae: 3353.9763 - val_loss: 3385.5627 - val_mse: 85465544.0000 - val_mae: 3385.5627\n",
            "Epoch 238/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 3334.3920 - mse: 81197376.0000 - mae: 3334.3921 - val_loss: 3371.5571 - val_mse: 85508320.0000 - val_mae: 3371.5569\n",
            "Epoch 239/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3310.4809 - mse: 82078288.0000 - mae: 3310.4810 - val_loss: 3376.7883 - val_mse: 85155280.0000 - val_mae: 3376.7881\n",
            "Epoch 240/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 3333.1538 - mse: 81984040.0000 - mae: 3333.1538 - val_loss: 3363.6961 - val_mse: 85163152.0000 - val_mae: 3363.6963\n",
            "Epoch 241/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3328.8051 - mse: 82039648.0000 - mae: 3328.8054 - val_loss: 3362.3237 - val_mse: 84985912.0000 - val_mae: 3362.3237\n",
            "Epoch 242/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3339.3903 - mse: 81232840.0000 - mae: 3339.3901 - val_loss: 3356.5895 - val_mse: 84716792.0000 - val_mae: 3356.5896\n",
            "Epoch 243/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3308.2331 - mse: 82049744.0000 - mae: 3308.2329 - val_loss: 3355.9314 - val_mse: 84514312.0000 - val_mae: 3355.9316\n",
            "Epoch 244/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3290.5956 - mse: 80464048.0000 - mae: 3290.5955 - val_loss: 3358.1752 - val_mse: 83987936.0000 - val_mae: 3358.1750\n",
            "Epoch 245/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3316.0505 - mse: 79922400.0000 - mae: 3316.0503 - val_loss: 3346.0280 - val_mse: 83966568.0000 - val_mae: 3346.0281\n",
            "Epoch 246/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3281.8472 - mse: 79650720.0000 - mae: 3281.8474 - val_loss: 3360.6912 - val_mse: 83597512.0000 - val_mae: 3360.6912\n",
            "Epoch 247/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3326.4185 - mse: 81322792.0000 - mae: 3326.4185 - val_loss: 3344.3355 - val_mse: 83441792.0000 - val_mae: 3344.3352\n",
            "Epoch 248/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3285.1635 - mse: 79625032.0000 - mae: 3285.1633 - val_loss: 3341.1485 - val_mse: 83275064.0000 - val_mae: 3341.1484\n",
            "Epoch 249/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3322.5553 - mse: 80806552.0000 - mae: 3322.5554 - val_loss: 3333.1445 - val_mse: 83104584.0000 - val_mae: 3333.1445\n",
            "Epoch 250/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3301.0862 - mse: 80774664.0000 - mae: 3301.0862 - val_loss: 3330.1170 - val_mse: 82956992.0000 - val_mae: 3330.1174\n",
            "Epoch 251/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3296.9352 - mse: 79634272.0000 - mae: 3296.9351 - val_loss: 3334.9011 - val_mse: 82510600.0000 - val_mae: 3334.9011\n",
            "Epoch 252/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3269.3794 - mse: 79224656.0000 - mae: 3269.3796 - val_loss: 3324.0850 - val_mse: 82242248.0000 - val_mae: 3324.0850\n",
            "Epoch 253/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3289.9473 - mse: 80293680.0000 - mae: 3289.9473 - val_loss: 3320.3373 - val_mse: 81971872.0000 - val_mae: 3320.3376\n",
            "Epoch 254/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 3255.4342 - mse: 78399472.0000 - mae: 3255.4343 - val_loss: 3313.4401 - val_mse: 81753856.0000 - val_mae: 3313.4402\n",
            "Epoch 255/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3251.5762 - mse: 78489096.0000 - mae: 3251.5762 - val_loss: 3309.4975 - val_mse: 81541400.0000 - val_mae: 3309.4973\n",
            "Epoch 256/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3269.9771 - mse: 78845296.0000 - mae: 3269.9771 - val_loss: 3314.1321 - val_mse: 81194504.0000 - val_mae: 3314.1321\n",
            "Epoch 257/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 3247.6495 - mse: 77301984.0000 - mae: 3247.6494 - val_loss: 3305.9277 - val_mse: 80871648.0000 - val_mae: 3305.9272\n",
            "Epoch 258/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3265.2223 - mse: 77999968.0000 - mae: 3265.2224 - val_loss: 3316.4992 - val_mse: 80537336.0000 - val_mae: 3316.4990\n",
            "Epoch 259/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3266.8113 - mse: 77044304.0000 - mae: 3266.8113 - val_loss: 3298.2480 - val_mse: 80568512.0000 - val_mae: 3298.2480\n",
            "Epoch 260/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3236.9514 - mse: 78100896.0000 - mae: 3236.9517 - val_loss: 3297.3472 - val_mse: 80281656.0000 - val_mae: 3297.3477\n",
            "Epoch 261/500\n",
            "1880/1880 [==============================] - 0s 66us/step - loss: 3246.2943 - mse: 76575248.0000 - mae: 3246.2942 - val_loss: 3288.2040 - val_mse: 80043840.0000 - val_mae: 3288.2039\n",
            "Epoch 262/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3218.8657 - mse: 76462280.0000 - mae: 3218.8660 - val_loss: 3296.4310 - val_mse: 79559088.0000 - val_mae: 3296.4309\n",
            "Epoch 263/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3250.0318 - mse: 77655168.0000 - mae: 3250.0320 - val_loss: 3282.3702 - val_mse: 79481536.0000 - val_mae: 3282.3701\n",
            "Epoch 264/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3233.4938 - mse: 77246712.0000 - mae: 3233.4937 - val_loss: 3283.7579 - val_mse: 79188056.0000 - val_mae: 3283.7581\n",
            "Epoch 265/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3265.1016 - mse: 77127928.0000 - mae: 3265.1018 - val_loss: 3286.3673 - val_mse: 78873504.0000 - val_mae: 3286.3674\n",
            "Epoch 266/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3222.3980 - mse: 74840064.0000 - mae: 3222.3979 - val_loss: 3275.2259 - val_mse: 78602088.0000 - val_mae: 3275.2263\n",
            "Epoch 267/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 3180.0024 - mse: 74302880.0000 - mae: 3180.0024 - val_loss: 3263.8041 - val_mse: 78381736.0000 - val_mae: 3263.8042\n",
            "Epoch 268/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 3196.9759 - mse: 75539304.0000 - mae: 3196.9761 - val_loss: 3257.7565 - val_mse: 78089296.0000 - val_mae: 3257.7563\n",
            "Epoch 269/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3191.8445 - mse: 74299944.0000 - mae: 3191.8445 - val_loss: 3263.6129 - val_mse: 77694432.0000 - val_mae: 3263.6130\n",
            "Epoch 270/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 3156.6831 - mse: 72246768.0000 - mae: 3156.6829 - val_loss: 3247.7671 - val_mse: 77582016.0000 - val_mae: 3247.7673\n",
            "Epoch 271/500\n",
            "1880/1880 [==============================] - 0s 85us/step - loss: 3181.7699 - mse: 73699160.0000 - mae: 3181.7700 - val_loss: 3241.5786 - val_mse: 77086000.0000 - val_mae: 3241.5789\n",
            "Epoch 272/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3155.3464 - mse: 74212576.0000 - mae: 3155.3462 - val_loss: 3238.4413 - val_mse: 76724448.0000 - val_mae: 3238.4412\n",
            "Epoch 273/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3150.0427 - mse: 71787904.0000 - mae: 3150.0425 - val_loss: 3226.8995 - val_mse: 76433128.0000 - val_mae: 3226.8997\n",
            "Epoch 274/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 3143.3185 - mse: 71987776.0000 - mae: 3143.3184 - val_loss: 3223.4620 - val_mse: 75963808.0000 - val_mae: 3223.4619\n",
            "Epoch 275/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3139.1939 - mse: 71903496.0000 - mae: 3139.1936 - val_loss: 3227.7184 - val_mse: 75621560.0000 - val_mae: 3227.7185\n",
            "Epoch 276/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 3192.1642 - mse: 73882296.0000 - mae: 3192.1643 - val_loss: 3213.2049 - val_mse: 75217064.0000 - val_mae: 3213.2048\n",
            "Epoch 277/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3148.2423 - mse: 72386192.0000 - mae: 3148.2422 - val_loss: 3224.7139 - val_mse: 74555360.0000 - val_mae: 3224.7141\n",
            "Epoch 278/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 3113.7614 - mse: 69549376.0000 - mae: 3113.7615 - val_loss: 3209.6100 - val_mse: 74399416.0000 - val_mae: 3209.6101\n",
            "Epoch 279/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3134.5027 - mse: 71078848.0000 - mae: 3134.5027 - val_loss: 3208.8233 - val_mse: 73908520.0000 - val_mae: 3208.8232\n",
            "Epoch 280/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3108.6162 - mse: 70369048.0000 - mae: 3108.6162 - val_loss: 3197.0214 - val_mse: 73615968.0000 - val_mae: 3197.0215\n",
            "Epoch 281/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 3133.5640 - mse: 69951160.0000 - mae: 3133.5637 - val_loss: 3196.0169 - val_mse: 73118944.0000 - val_mae: 3196.0171\n",
            "Epoch 282/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3099.8904 - mse: 69797824.0000 - mae: 3099.8901 - val_loss: 3187.9206 - val_mse: 72732440.0000 - val_mae: 3187.9209\n",
            "Epoch 283/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 3134.9030 - mse: 72075968.0000 - mae: 3134.9028 - val_loss: 3176.9025 - val_mse: 72548880.0000 - val_mae: 3176.9026\n",
            "Epoch 284/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3067.1951 - mse: 67754832.0000 - mae: 3067.1953 - val_loss: 3179.3616 - val_mse: 72194912.0000 - val_mae: 3179.3621\n",
            "Epoch 285/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 3062.3173 - mse: 67757032.0000 - mae: 3062.3174 - val_loss: 3173.4405 - val_mse: 71491144.0000 - val_mae: 3173.4407\n",
            "Epoch 286/500\n",
            "1880/1880 [==============================] - 0s 89us/step - loss: 3101.8988 - mse: 67062984.0000 - mae: 3101.8987 - val_loss: 3164.3380 - val_mse: 71316216.0000 - val_mae: 3164.3376\n",
            "Epoch 287/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 3038.6050 - mse: 67907504.0000 - mae: 3038.6050 - val_loss: 3158.0616 - val_mse: 70973672.0000 - val_mae: 3158.0615\n",
            "Epoch 288/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 3037.6383 - mse: 65684280.0000 - mae: 3037.6382 - val_loss: 3152.7358 - val_mse: 70407592.0000 - val_mae: 3152.7358\n",
            "Epoch 289/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 3045.0811 - mse: 65612660.0000 - mae: 3045.0808 - val_loss: 3150.6815 - val_mse: 69887400.0000 - val_mae: 3150.6814\n",
            "Epoch 290/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2977.7640 - mse: 63235240.0000 - mae: 2977.7639 - val_loss: 3139.7353 - val_mse: 69384064.0000 - val_mae: 3139.7354\n",
            "Epoch 291/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3063.1581 - mse: 66078440.0000 - mae: 3063.1580 - val_loss: 3135.2320 - val_mse: 68914496.0000 - val_mae: 3135.2322\n",
            "Epoch 292/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 3023.6204 - mse: 66279396.0000 - mae: 3023.6204 - val_loss: 3126.8495 - val_mse: 68563856.0000 - val_mae: 3126.8499\n",
            "Epoch 293/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2958.7073 - mse: 63621120.0000 - mae: 2958.7075 - val_loss: 3121.8977 - val_mse: 67987672.0000 - val_mae: 3121.8977\n",
            "Epoch 294/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2994.2168 - mse: 63834836.0000 - mae: 2994.2168 - val_loss: 3126.7584 - val_mse: 67345208.0000 - val_mae: 3126.7585\n",
            "Epoch 295/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2934.3000 - mse: 62127444.0000 - mae: 2934.3000 - val_loss: 3117.6095 - val_mse: 66899868.0000 - val_mae: 3117.6096\n",
            "Epoch 296/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 2938.3941 - mse: 61108860.0000 - mae: 2938.3940 - val_loss: 3120.7693 - val_mse: 66292004.0000 - val_mae: 3120.7695\n",
            "Epoch 297/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2978.6257 - mse: 63766768.0000 - mae: 2978.6257 - val_loss: 3114.6177 - val_mse: 65828996.0000 - val_mae: 3114.6174\n",
            "Epoch 298/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2980.5761 - mse: 63800976.0000 - mae: 2980.5764 - val_loss: 3091.8138 - val_mse: 65822488.0000 - val_mae: 3091.8137\n",
            "Epoch 299/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2916.3830 - mse: 60725544.0000 - mae: 2916.3831 - val_loss: 3083.2270 - val_mse: 65298164.0000 - val_mae: 3083.2273\n",
            "Epoch 300/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2918.0754 - mse: 60173552.0000 - mae: 2918.0752 - val_loss: 3078.8211 - val_mse: 64673896.0000 - val_mae: 3078.8213\n",
            "Epoch 301/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 2896.6223 - mse: 59670460.0000 - mae: 2896.6223 - val_loss: 3076.8084 - val_mse: 64004132.0000 - val_mae: 3076.8083\n",
            "Epoch 302/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2914.4275 - mse: 58463732.0000 - mae: 2914.4277 - val_loss: 3061.6620 - val_mse: 63657756.0000 - val_mae: 3061.6619\n",
            "Epoch 303/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2931.0248 - mse: 61834432.0000 - mae: 2931.0247 - val_loss: 3057.3367 - val_mse: 63153380.0000 - val_mae: 3057.3367\n",
            "Epoch 304/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2929.2177 - mse: 61099516.0000 - mae: 2929.2178 - val_loss: 3070.1962 - val_mse: 62701248.0000 - val_mae: 3070.1958\n",
            "Epoch 305/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2915.8858 - mse: 59256984.0000 - mae: 2915.8857 - val_loss: 3069.6270 - val_mse: 62209956.0000 - val_mae: 3069.6272\n",
            "Epoch 306/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2920.0774 - mse: 60556008.0000 - mae: 2920.0776 - val_loss: 3078.0621 - val_mse: 61755352.0000 - val_mae: 3078.0620\n",
            "Epoch 307/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 2866.6252 - mse: 58698936.0000 - mae: 2866.6252 - val_loss: 3050.4221 - val_mse: 61325300.0000 - val_mae: 3050.4219\n",
            "Epoch 308/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2813.3528 - mse: 56196388.0000 - mae: 2813.3525 - val_loss: 3029.8925 - val_mse: 61313824.0000 - val_mae: 3029.8926\n",
            "Epoch 309/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2869.9504 - mse: 58108364.0000 - mae: 2869.9502 - val_loss: 3021.1622 - val_mse: 60730580.0000 - val_mae: 3021.1624\n",
            "Epoch 310/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2824.9527 - mse: 56613888.0000 - mae: 2824.9526 - val_loss: 3003.9732 - val_mse: 60596676.0000 - val_mae: 3003.9736\n",
            "Epoch 311/500\n",
            "1880/1880 [==============================] - 0s 83us/step - loss: 2863.6505 - mse: 56552192.0000 - mae: 2863.6506 - val_loss: 3000.9884 - val_mse: 59859184.0000 - val_mae: 3000.9883\n",
            "Epoch 312/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2796.8151 - mse: 54647280.0000 - mae: 2796.8152 - val_loss: 2999.7070 - val_mse: 59340144.0000 - val_mae: 2999.7070\n",
            "Epoch 313/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 2657.9558 - mse: 50073652.0000 - mae: 2657.9556 - val_loss: 3010.2822 - val_mse: 58573000.0000 - val_mae: 3010.2825\n",
            "Epoch 314/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 2792.9855 - mse: 54919232.0000 - mae: 2792.9854 - val_loss: 2973.8929 - val_mse: 58829944.0000 - val_mae: 2973.8928\n",
            "Epoch 315/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2828.5983 - mse: 55609804.0000 - mae: 2828.5984 - val_loss: 2968.1053 - val_mse: 58004344.0000 - val_mae: 2968.1055\n",
            "Epoch 316/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 2796.1838 - mse: 54452224.0000 - mae: 2796.1841 - val_loss: 2995.5602 - val_mse: 57353448.0000 - val_mae: 2995.5598\n",
            "Epoch 317/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2802.8263 - mse: 53985216.0000 - mae: 2802.8264 - val_loss: 2959.3754 - val_mse: 57095508.0000 - val_mae: 2959.3755\n",
            "Epoch 318/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2744.9388 - mse: 52769980.0000 - mae: 2744.9385 - val_loss: 2953.7019 - val_mse: 56623500.0000 - val_mae: 2953.7019\n",
            "Epoch 319/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2831.8002 - mse: 54921836.0000 - mae: 2831.8003 - val_loss: 2936.3487 - val_mse: 56196064.0000 - val_mae: 2936.3486\n",
            "Epoch 320/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2776.4401 - mse: 53429584.0000 - mae: 2776.4399 - val_loss: 2931.0889 - val_mse: 55672356.0000 - val_mae: 2931.0891\n",
            "Epoch 321/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2756.0453 - mse: 52620812.0000 - mae: 2756.0454 - val_loss: 2920.9460 - val_mse: 55273988.0000 - val_mae: 2920.9458\n",
            "Epoch 322/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2670.4487 - mse: 48127944.0000 - mae: 2670.4490 - val_loss: 2913.4493 - val_mse: 54860480.0000 - val_mae: 2913.4495\n",
            "Epoch 323/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2728.1455 - mse: 50688508.0000 - mae: 2728.1458 - val_loss: 2911.4793 - val_mse: 54261908.0000 - val_mae: 2911.4795\n",
            "Epoch 324/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2678.9864 - mse: 49252640.0000 - mae: 2678.9863 - val_loss: 2900.1797 - val_mse: 53873548.0000 - val_mae: 2900.1797\n",
            "Epoch 325/500\n",
            "1880/1880 [==============================] - 0s 67us/step - loss: 2729.1457 - mse: 51701104.0000 - mae: 2729.1458 - val_loss: 2874.6310 - val_mse: 53561192.0000 - val_mae: 2874.6311\n",
            "Epoch 326/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2726.6363 - mse: 51267700.0000 - mae: 2726.6362 - val_loss: 2868.5510 - val_mse: 53260064.0000 - val_mae: 2868.5508\n",
            "Epoch 327/500\n",
            "1880/1880 [==============================] - 0s 82us/step - loss: 2699.8066 - mse: 50166544.0000 - mae: 2699.8066 - val_loss: 2860.9181 - val_mse: 52761716.0000 - val_mae: 2860.9180\n",
            "Epoch 328/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2694.5129 - mse: 51210284.0000 - mae: 2694.5129 - val_loss: 2855.5942 - val_mse: 52313460.0000 - val_mae: 2855.5942\n",
            "Epoch 329/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 2590.8499 - mse: 45530824.0000 - mae: 2590.8501 - val_loss: 2854.3320 - val_mse: 51893540.0000 - val_mae: 2854.3320\n",
            "Epoch 330/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 2583.7210 - mse: 47175680.0000 - mae: 2583.7209 - val_loss: 2830.5640 - val_mse: 51550108.0000 - val_mae: 2830.5640\n",
            "Epoch 331/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2666.8996 - mse: 49891720.0000 - mae: 2666.8997 - val_loss: 2818.0791 - val_mse: 51135864.0000 - val_mae: 2818.0791\n",
            "Epoch 332/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2664.7042 - mse: 49712048.0000 - mae: 2664.7043 - val_loss: 2813.5396 - val_mse: 50948796.0000 - val_mae: 2813.5398\n",
            "Epoch 333/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2646.6290 - mse: 47096008.0000 - mae: 2646.6289 - val_loss: 2815.9989 - val_mse: 50424784.0000 - val_mae: 2815.9988\n",
            "Epoch 334/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 2560.4703 - mse: 44542832.0000 - mae: 2560.4702 - val_loss: 2794.5544 - val_mse: 50162272.0000 - val_mae: 2794.5542\n",
            "Epoch 335/500\n",
            "1880/1880 [==============================] - 0s 83us/step - loss: 2608.0116 - mse: 46376652.0000 - mae: 2608.0115 - val_loss: 2789.4423 - val_mse: 49479836.0000 - val_mae: 2789.4421\n",
            "Epoch 336/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2590.5124 - mse: 45967728.0000 - mae: 2590.5122 - val_loss: 2785.5309 - val_mse: 49221804.0000 - val_mae: 2785.5308\n",
            "Epoch 337/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2605.3129 - mse: 46545696.0000 - mae: 2605.3130 - val_loss: 2771.5953 - val_mse: 49026008.0000 - val_mae: 2771.5955\n",
            "Epoch 338/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2572.3708 - mse: 45856396.0000 - mae: 2572.3708 - val_loss: 2778.6500 - val_mse: 48262216.0000 - val_mae: 2778.6501\n",
            "Epoch 339/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2530.5360 - mse: 43202752.0000 - mae: 2530.5359 - val_loss: 2759.6815 - val_mse: 48013096.0000 - val_mae: 2759.6816\n",
            "Epoch 340/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 2599.9137 - mse: 45846180.0000 - mae: 2599.9138 - val_loss: 2763.4358 - val_mse: 47480552.0000 - val_mae: 2763.4358\n",
            "Epoch 341/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 2499.1254 - mse: 42335604.0000 - mae: 2499.1252 - val_loss: 2752.8479 - val_mse: 47031564.0000 - val_mae: 2752.8479\n",
            "Epoch 342/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2479.2559 - mse: 40662444.0000 - mae: 2479.2559 - val_loss: 2732.6015 - val_mse: 46957868.0000 - val_mae: 2732.6013\n",
            "Epoch 343/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2527.9257 - mse: 41922056.0000 - mae: 2527.9255 - val_loss: 2725.8378 - val_mse: 46744388.0000 - val_mae: 2725.8381\n",
            "Epoch 344/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2508.5429 - mse: 42383740.0000 - mae: 2508.5430 - val_loss: 2724.4890 - val_mse: 46062852.0000 - val_mae: 2724.4890\n",
            "Epoch 345/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2454.5686 - mse: 39861484.0000 - mae: 2454.5688 - val_loss: 2710.9560 - val_mse: 45860996.0000 - val_mae: 2710.9561\n",
            "Epoch 346/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2485.4392 - mse: 40206048.0000 - mae: 2485.4392 - val_loss: 2712.9880 - val_mse: 45166832.0000 - val_mae: 2712.9880\n",
            "Epoch 347/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2591.5522 - mse: 43582500.0000 - mae: 2591.5525 - val_loss: 2688.4767 - val_mse: 44928084.0000 - val_mae: 2688.4768\n",
            "Epoch 348/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2518.1024 - mse: 41887536.0000 - mae: 2518.1023 - val_loss: 2682.6327 - val_mse: 44590944.0000 - val_mae: 2682.6323\n",
            "Epoch 349/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 2418.5891 - mse: 39039664.0000 - mae: 2418.5891 - val_loss: 2674.2029 - val_mse: 44043044.0000 - val_mae: 2674.2026\n",
            "Epoch 350/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2470.6988 - mse: 39886264.0000 - mae: 2470.6987 - val_loss: 2671.3655 - val_mse: 43777772.0000 - val_mae: 2671.3655\n",
            "Epoch 351/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2440.7303 - mse: 41883196.0000 - mae: 2440.7302 - val_loss: 2672.3370 - val_mse: 43636336.0000 - val_mae: 2672.3369\n",
            "Epoch 352/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2399.0749 - mse: 36185084.0000 - mae: 2399.0750 - val_loss: 2663.7679 - val_mse: 42932512.0000 - val_mae: 2663.7678\n",
            "Epoch 353/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2353.9564 - mse: 35916220.0000 - mae: 2353.9563 - val_loss: 2687.6299 - val_mse: 42516124.0000 - val_mae: 2687.6296\n",
            "Epoch 354/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2368.3268 - mse: 37056808.0000 - mae: 2368.3269 - val_loss: 2634.9501 - val_mse: 42347904.0000 - val_mae: 2634.9500\n",
            "Epoch 355/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2414.9227 - mse: 39818972.0000 - mae: 2414.9226 - val_loss: 2636.5997 - val_mse: 41741612.0000 - val_mae: 2636.5999\n",
            "Epoch 356/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 2362.5666 - mse: 36789484.0000 - mae: 2362.5667 - val_loss: 2616.9198 - val_mse: 41584052.0000 - val_mae: 2616.9197\n",
            "Epoch 357/500\n",
            "1880/1880 [==============================] - 0s 84us/step - loss: 2414.8156 - mse: 37173752.0000 - mae: 2414.8157 - val_loss: 2616.7183 - val_mse: 41313852.0000 - val_mae: 2616.7185\n",
            "Epoch 358/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2359.3542 - mse: 36163884.0000 - mae: 2359.3542 - val_loss: 2620.1279 - val_mse: 40962916.0000 - val_mae: 2620.1282\n",
            "Epoch 359/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2310.9506 - mse: 34517484.0000 - mae: 2310.9504 - val_loss: 2599.2611 - val_mse: 40522888.0000 - val_mae: 2599.2610\n",
            "Epoch 360/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 2313.3023 - mse: 36599500.0000 - mae: 2313.3025 - val_loss: 2587.7283 - val_mse: 39935204.0000 - val_mae: 2587.7285\n",
            "Epoch 361/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 2242.3061 - mse: 33609912.0000 - mae: 2242.3059 - val_loss: 2567.1962 - val_mse: 39654564.0000 - val_mae: 2567.1965\n",
            "Epoch 362/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 2322.8205 - mse: 35882648.0000 - mae: 2322.8203 - val_loss: 2563.6152 - val_mse: 39603816.0000 - val_mae: 2563.6152\n",
            "Epoch 363/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2253.9466 - mse: 31587456.0000 - mae: 2253.9465 - val_loss: 2559.7368 - val_mse: 39006360.0000 - val_mae: 2559.7366\n",
            "Epoch 364/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 2277.7798 - mse: 34329228.0000 - mae: 2277.7798 - val_loss: 2582.9123 - val_mse: 38701264.0000 - val_mae: 2582.9124\n",
            "Epoch 365/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2312.5194 - mse: 34149236.0000 - mae: 2312.5195 - val_loss: 2557.0365 - val_mse: 38426748.0000 - val_mae: 2557.0364\n",
            "Epoch 366/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2269.6307 - mse: 34221296.0000 - mae: 2269.6309 - val_loss: 2578.1237 - val_mse: 38045700.0000 - val_mae: 2578.1240\n",
            "Epoch 367/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 2224.7806 - mse: 32169754.0000 - mae: 2224.7808 - val_loss: 2550.5966 - val_mse: 38013880.0000 - val_mae: 2550.5967\n",
            "Epoch 368/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2208.4687 - mse: 31698958.0000 - mae: 2208.4685 - val_loss: 2538.0802 - val_mse: 37487256.0000 - val_mae: 2538.0803\n",
            "Epoch 369/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2127.4395 - mse: 28369572.0000 - mae: 2127.4395 - val_loss: 2525.3436 - val_mse: 37611344.0000 - val_mae: 2525.3435\n",
            "Epoch 370/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2248.2242 - mse: 32649428.0000 - mae: 2248.2241 - val_loss: 2562.8952 - val_mse: 37172148.0000 - val_mae: 2562.8953\n",
            "Epoch 371/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2220.4870 - mse: 31325176.0000 - mae: 2220.4871 - val_loss: 2535.8041 - val_mse: 37011704.0000 - val_mae: 2535.8042\n",
            "Epoch 372/500\n",
            "1880/1880 [==============================] - 0s 70us/step - loss: 2226.1588 - mse: 33438886.0000 - mae: 2226.1587 - val_loss: 2516.5700 - val_mse: 37071548.0000 - val_mae: 2516.5701\n",
            "Epoch 373/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2150.5216 - mse: 29672532.0000 - mae: 2150.5217 - val_loss: 2514.1633 - val_mse: 36830460.0000 - val_mae: 2514.1631\n",
            "Epoch 374/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 2121.6547 - mse: 28447158.0000 - mae: 2121.6548 - val_loss: 2521.2549 - val_mse: 36426316.0000 - val_mae: 2521.2549\n",
            "Epoch 375/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 2174.6990 - mse: 29624874.0000 - mae: 2174.6987 - val_loss: 2505.5769 - val_mse: 36547100.0000 - val_mae: 2505.5769\n",
            "Epoch 376/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2162.3825 - mse: 29486666.0000 - mae: 2162.3826 - val_loss: 2521.0141 - val_mse: 36035896.0000 - val_mae: 2521.0139\n",
            "Epoch 377/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 2138.4722 - mse: 29160498.0000 - mae: 2138.4722 - val_loss: 2506.6252 - val_mse: 35934156.0000 - val_mae: 2506.6252\n",
            "Epoch 378/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 2163.2604 - mse: 29534818.0000 - mae: 2163.2603 - val_loss: 2482.9182 - val_mse: 35779532.0000 - val_mae: 2482.9182\n",
            "Epoch 379/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2138.1203 - mse: 29133006.0000 - mae: 2138.1204 - val_loss: 2492.1960 - val_mse: 35211428.0000 - val_mae: 2492.1960\n",
            "Epoch 380/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2181.3559 - mse: 31343260.0000 - mae: 2181.3560 - val_loss: 2496.8631 - val_mse: 35090912.0000 - val_mae: 2496.8630\n",
            "Epoch 381/500\n",
            "1880/1880 [==============================] - 0s 83us/step - loss: 2164.7498 - mse: 30396016.0000 - mae: 2164.7498 - val_loss: 2491.9850 - val_mse: 34861512.0000 - val_mae: 2491.9851\n",
            "Epoch 382/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2090.3200 - mse: 26840512.0000 - mae: 2090.3201 - val_loss: 2478.2495 - val_mse: 34863628.0000 - val_mae: 2478.2493\n",
            "Epoch 383/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2150.1916 - mse: 29104616.0000 - mae: 2150.1914 - val_loss: 2469.4873 - val_mse: 34983200.0000 - val_mae: 2469.4875\n",
            "Epoch 384/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 2132.7329 - mse: 29397046.0000 - mae: 2132.7329 - val_loss: 2444.2581 - val_mse: 34423088.0000 - val_mae: 2444.2578\n",
            "Epoch 385/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 2125.3854 - mse: 27576764.0000 - mae: 2125.3853 - val_loss: 2434.9163 - val_mse: 34013960.0000 - val_mae: 2434.9163\n",
            "Epoch 386/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2088.7614 - mse: 27515958.0000 - mae: 2088.7615 - val_loss: 2431.8931 - val_mse: 33646708.0000 - val_mae: 2431.8931\n",
            "Epoch 387/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2063.4179 - mse: 27486950.0000 - mae: 2063.4180 - val_loss: 2469.3373 - val_mse: 33323890.0000 - val_mae: 2469.3372\n",
            "Epoch 388/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2055.1079 - mse: 27378574.0000 - mae: 2055.1079 - val_loss: 2431.2951 - val_mse: 33459952.0000 - val_mae: 2431.2949\n",
            "Epoch 389/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2048.5575 - mse: 25687278.0000 - mae: 2048.5574 - val_loss: 2402.8846 - val_mse: 33371396.0000 - val_mae: 2402.8845\n",
            "Epoch 390/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 2114.0184 - mse: 28203072.0000 - mae: 2114.0183 - val_loss: 2421.4817 - val_mse: 32663992.0000 - val_mae: 2421.4822\n",
            "Epoch 391/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 2104.7302 - mse: 28155378.0000 - mae: 2104.7300 - val_loss: 2429.2036 - val_mse: 32231512.0000 - val_mae: 2429.2034\n",
            "Epoch 392/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2046.9976 - mse: 26342858.0000 - mae: 2046.9976 - val_loss: 2391.5193 - val_mse: 32370436.0000 - val_mae: 2391.5193\n",
            "Epoch 393/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2067.4503 - mse: 26924012.0000 - mae: 2067.4504 - val_loss: 2394.1107 - val_mse: 32454154.0000 - val_mae: 2394.1111\n",
            "Epoch 394/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2088.8936 - mse: 27256734.0000 - mae: 2088.8936 - val_loss: 2388.9008 - val_mse: 32473052.0000 - val_mae: 2388.9011\n",
            "Epoch 395/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 2012.8141 - mse: 24460762.0000 - mae: 2012.8141 - val_loss: 2424.7221 - val_mse: 31921880.0000 - val_mae: 2424.7222\n",
            "Epoch 396/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1984.1394 - mse: 23071910.0000 - mae: 1984.1394 - val_loss: 2390.4956 - val_mse: 32120954.0000 - val_mae: 2390.4961\n",
            "Epoch 397/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 2016.6899 - mse: 24600040.0000 - mae: 2016.6898 - val_loss: 2383.3801 - val_mse: 31918220.0000 - val_mae: 2383.3799\n",
            "Epoch 398/500\n",
            "1880/1880 [==============================] - 0s 101us/step - loss: 2081.9309 - mse: 27473690.0000 - mae: 2081.9309 - val_loss: 2368.7577 - val_mse: 31950922.0000 - val_mae: 2368.7578\n",
            "Epoch 399/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2036.7821 - mse: 26418980.0000 - mae: 2036.7821 - val_loss: 2378.0066 - val_mse: 31438774.0000 - val_mae: 2378.0066\n",
            "Epoch 400/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 1986.4237 - mse: 25432982.0000 - mae: 1986.4238 - val_loss: 2362.7714 - val_mse: 31655410.0000 - val_mae: 2362.7715\n",
            "Epoch 401/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2013.9581 - mse: 24673466.0000 - mae: 2013.9581 - val_loss: 2359.3708 - val_mse: 31672186.0000 - val_mae: 2359.3706\n",
            "Epoch 402/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1996.5138 - mse: 24669624.0000 - mae: 1996.5138 - val_loss: 2351.6421 - val_mse: 31749728.0000 - val_mae: 2351.6423\n",
            "Epoch 403/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1986.1688 - mse: 24065480.0000 - mae: 1986.1687 - val_loss: 2360.1586 - val_mse: 31500320.0000 - val_mae: 2360.1584\n",
            "Epoch 404/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 2096.2163 - mse: 27830708.0000 - mae: 2096.2163 - val_loss: 2350.4685 - val_mse: 30976270.0000 - val_mae: 2350.4685\n",
            "Epoch 405/500\n",
            "1880/1880 [==============================] - 0s 93us/step - loss: 1983.9007 - mse: 24026696.0000 - mae: 1983.9005 - val_loss: 2361.4510 - val_mse: 30713426.0000 - val_mae: 2361.4512\n",
            "Epoch 406/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1967.3950 - mse: 24119520.0000 - mae: 1967.3949 - val_loss: 2344.1385 - val_mse: 31393342.0000 - val_mae: 2344.1384\n",
            "Epoch 407/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 2056.8227 - mse: 25629460.0000 - mae: 2056.8228 - val_loss: 2336.0982 - val_mse: 30829664.0000 - val_mae: 2336.0981\n",
            "Epoch 408/500\n",
            "1880/1880 [==============================] - 0s 73us/step - loss: 2024.7560 - mse: 25580184.0000 - mae: 2024.7560 - val_loss: 2328.1240 - val_mse: 30743814.0000 - val_mae: 2328.1240\n",
            "Epoch 409/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2006.0970 - mse: 24994554.0000 - mae: 2006.0969 - val_loss: 2331.2688 - val_mse: 30449950.0000 - val_mae: 2331.2688\n",
            "Epoch 410/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2061.4755 - mse: 26355620.0000 - mae: 2061.4756 - val_loss: 2312.3398 - val_mse: 29471964.0000 - val_mae: 2312.3401\n",
            "Epoch 411/500\n",
            "1880/1880 [==============================] - 0s 68us/step - loss: 1962.4834 - mse: 23711022.0000 - mae: 1962.4833 - val_loss: 2305.8595 - val_mse: 29917462.0000 - val_mae: 2305.8596\n",
            "Epoch 412/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 2016.1711 - mse: 23955088.0000 - mae: 2016.1711 - val_loss: 2298.5179 - val_mse: 29776546.0000 - val_mae: 2298.5178\n",
            "Epoch 413/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1926.6191 - mse: 22846546.0000 - mae: 1926.6191 - val_loss: 2292.1424 - val_mse: 29362806.0000 - val_mae: 2292.1426\n",
            "Epoch 414/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 2043.4024 - mse: 25586576.0000 - mae: 2043.4022 - val_loss: 2295.9773 - val_mse: 29802026.0000 - val_mae: 2295.9773\n",
            "Epoch 415/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 1961.6336 - mse: 22604540.0000 - mae: 1961.6335 - val_loss: 2298.1724 - val_mse: 29335874.0000 - val_mae: 2298.1724\n",
            "Epoch 416/500\n",
            "1880/1880 [==============================] - 0s 83us/step - loss: 1906.2181 - mse: 21821800.0000 - mae: 1906.2183 - val_loss: 2320.7060 - val_mse: 29358472.0000 - val_mae: 2320.7058\n",
            "Epoch 417/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 1994.6510 - mse: 24169518.0000 - mae: 1994.6509 - val_loss: 2321.6153 - val_mse: 29025660.0000 - val_mae: 2321.6152\n",
            "Epoch 418/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 2006.2280 - mse: 25580854.0000 - mae: 2006.2279 - val_loss: 2292.2212 - val_mse: 29296210.0000 - val_mae: 2292.2214\n",
            "Epoch 419/500\n",
            "1880/1880 [==============================] - 0s 82us/step - loss: 1917.4230 - mse: 22031416.0000 - mae: 1917.4230 - val_loss: 2308.3994 - val_mse: 29193596.0000 - val_mae: 2308.3997\n",
            "Epoch 420/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1970.9522 - mse: 24129984.0000 - mae: 1970.9523 - val_loss: 2285.3019 - val_mse: 29245318.0000 - val_mae: 2285.3022\n",
            "Epoch 421/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1973.6101 - mse: 23933914.0000 - mae: 1973.6100 - val_loss: 2305.4530 - val_mse: 29204056.0000 - val_mae: 2305.4529\n",
            "Epoch 422/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 1924.2137 - mse: 22437804.0000 - mae: 1924.2137 - val_loss: 2307.0898 - val_mse: 28812764.0000 - val_mae: 2307.0898\n",
            "Epoch 423/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 1947.6228 - mse: 21773036.0000 - mae: 1947.6227 - val_loss: 2255.8264 - val_mse: 28403708.0000 - val_mae: 2255.8264\n",
            "Epoch 424/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1995.6466 - mse: 24458198.0000 - mae: 1995.6465 - val_loss: 2260.2060 - val_mse: 28561142.0000 - val_mae: 2260.2061\n",
            "Epoch 425/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1981.4864 - mse: 24232170.0000 - mae: 1981.4866 - val_loss: 2270.2345 - val_mse: 27981288.0000 - val_mae: 2270.2344\n",
            "Epoch 426/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1936.6481 - mse: 22128814.0000 - mae: 1936.6479 - val_loss: 2265.0117 - val_mse: 27946734.0000 - val_mae: 2265.0117\n",
            "Epoch 427/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1938.1757 - mse: 22476570.0000 - mae: 1938.1757 - val_loss: 2256.6201 - val_mse: 28140432.0000 - val_mae: 2256.6201\n",
            "Epoch 428/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 1937.1147 - mse: 22929660.0000 - mae: 1937.1147 - val_loss: 2272.6644 - val_mse: 28192912.0000 - val_mae: 2272.6646\n",
            "Epoch 429/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 1968.8945 - mse: 22916684.0000 - mae: 1968.8945 - val_loss: 2243.2416 - val_mse: 27993244.0000 - val_mae: 2243.2417\n",
            "Epoch 430/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1982.3845 - mse: 23409868.0000 - mae: 1982.3843 - val_loss: 2219.8950 - val_mse: 27545360.0000 - val_mae: 2219.8950\n",
            "Epoch 431/500\n",
            "1880/1880 [==============================] - 0s 84us/step - loss: 1972.5174 - mse: 23755824.0000 - mae: 1972.5175 - val_loss: 2271.9488 - val_mse: 27639760.0000 - val_mae: 2271.9490\n",
            "Epoch 432/500\n",
            "1880/1880 [==============================] - 0s 82us/step - loss: 1945.8116 - mse: 22202276.0000 - mae: 1945.8116 - val_loss: 2257.6031 - val_mse: 27248228.0000 - val_mae: 2257.6033\n",
            "Epoch 433/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 1966.9823 - mse: 24646404.0000 - mae: 1966.9824 - val_loss: 2244.6873 - val_mse: 27344590.0000 - val_mae: 2244.6875\n",
            "Epoch 434/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1857.2998 - mse: 20355502.0000 - mae: 1857.2997 - val_loss: 2226.0261 - val_mse: 27409720.0000 - val_mae: 2226.0259\n",
            "Epoch 435/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1958.4321 - mse: 23590106.0000 - mae: 1958.4321 - val_loss: 2265.7829 - val_mse: 27474502.0000 - val_mae: 2265.7830\n",
            "Epoch 436/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 1911.2264 - mse: 22320638.0000 - mae: 1911.2263 - val_loss: 2227.0632 - val_mse: 27412716.0000 - val_mae: 2227.0630\n",
            "Epoch 437/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1871.3645 - mse: 21670258.0000 - mae: 1871.3645 - val_loss: 2230.5449 - val_mse: 27641580.0000 - val_mae: 2230.5449\n",
            "Epoch 438/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 2020.9663 - mse: 25004500.0000 - mae: 2020.9662 - val_loss: 2233.1819 - val_mse: 27015620.0000 - val_mae: 2233.1819\n",
            "Epoch 439/500\n",
            "1880/1880 [==============================] - 0s 92us/step - loss: 1820.8008 - mse: 19860774.0000 - mae: 1820.8009 - val_loss: 2214.5500 - val_mse: 27072564.0000 - val_mae: 2214.5500\n",
            "Epoch 440/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 1946.6422 - mse: 22623188.0000 - mae: 1946.6423 - val_loss: 2225.0837 - val_mse: 26859660.0000 - val_mae: 2225.0837\n",
            "Epoch 441/500\n",
            "1880/1880 [==============================] - 0s 87us/step - loss: 1925.8654 - mse: 22443634.0000 - mae: 1925.8655 - val_loss: 2207.7628 - val_mse: 27049316.0000 - val_mae: 2207.7629\n",
            "Epoch 442/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 1886.7218 - mse: 22374846.0000 - mae: 1886.7218 - val_loss: 2266.5088 - val_mse: 26856408.0000 - val_mae: 2266.5088\n",
            "Epoch 443/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1948.0061 - mse: 23064136.0000 - mae: 1948.0061 - val_loss: 2204.9057 - val_mse: 27037932.0000 - val_mae: 2204.9055\n",
            "Epoch 444/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 1924.3430 - mse: 22318328.0000 - mae: 1924.3429 - val_loss: 2198.1262 - val_mse: 26725158.0000 - val_mae: 2198.1262\n",
            "Epoch 445/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1882.3305 - mse: 21176700.0000 - mae: 1882.3306 - val_loss: 2220.8691 - val_mse: 27024672.0000 - val_mae: 2220.8691\n",
            "Epoch 446/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 1928.4025 - mse: 22287438.0000 - mae: 1928.4027 - val_loss: 2208.2999 - val_mse: 26532844.0000 - val_mae: 2208.2998\n",
            "Epoch 447/500\n",
            "1880/1880 [==============================] - 0s 69us/step - loss: 1901.6397 - mse: 20856948.0000 - mae: 1901.6398 - val_loss: 2213.6884 - val_mse: 26575670.0000 - val_mae: 2213.6885\n",
            "Epoch 448/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1824.4978 - mse: 19825472.0000 - mae: 1824.4979 - val_loss: 2199.4560 - val_mse: 26297642.0000 - val_mae: 2199.4561\n",
            "Epoch 449/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 1879.7519 - mse: 21151740.0000 - mae: 1879.7518 - val_loss: 2179.9698 - val_mse: 26460876.0000 - val_mae: 2179.9697\n",
            "Epoch 450/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1865.8251 - mse: 21577088.0000 - mae: 1865.8253 - val_loss: 2188.4941 - val_mse: 26534240.0000 - val_mae: 2188.4941\n",
            "Epoch 451/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1941.5572 - mse: 22496616.0000 - mae: 1941.5574 - val_loss: 2192.1971 - val_mse: 25717632.0000 - val_mae: 2192.1970\n",
            "Epoch 452/500\n",
            "1880/1880 [==============================] - 0s 82us/step - loss: 1931.2306 - mse: 22997656.0000 - mae: 1931.2307 - val_loss: 2195.5605 - val_mse: 26217822.0000 - val_mae: 2195.5608\n",
            "Epoch 453/500\n",
            "1880/1880 [==============================] - 0s 89us/step - loss: 1902.5553 - mse: 22263640.0000 - mae: 1902.5553 - val_loss: 2177.3553 - val_mse: 25768820.0000 - val_mae: 2177.3552\n",
            "Epoch 454/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1843.9457 - mse: 19603746.0000 - mae: 1843.9456 - val_loss: 2175.7071 - val_mse: 25888004.0000 - val_mae: 2175.7070\n",
            "Epoch 455/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1898.7587 - mse: 22248366.0000 - mae: 1898.7588 - val_loss: 2168.2771 - val_mse: 25836052.0000 - val_mae: 2168.2771\n",
            "Epoch 456/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1913.9404 - mse: 22946212.0000 - mae: 1913.9404 - val_loss: 2182.0047 - val_mse: 25532642.0000 - val_mae: 2182.0046\n",
            "Epoch 457/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1849.1994 - mse: 19865724.0000 - mae: 1849.1996 - val_loss: 2174.4276 - val_mse: 26207526.0000 - val_mae: 2174.4277\n",
            "Epoch 458/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 1855.8929 - mse: 20769484.0000 - mae: 1855.8928 - val_loss: 2177.9936 - val_mse: 25938780.0000 - val_mae: 2177.9937\n",
            "Epoch 459/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1868.0805 - mse: 21231576.0000 - mae: 1868.0806 - val_loss: 2164.3104 - val_mse: 24983286.0000 - val_mae: 2164.3105\n",
            "Epoch 460/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1852.4970 - mse: 20145984.0000 - mae: 1852.4969 - val_loss: 2136.5413 - val_mse: 24857660.0000 - val_mae: 2136.5415\n",
            "Epoch 461/500\n",
            "1880/1880 [==============================] - 0s 83us/step - loss: 1836.7153 - mse: 20417674.0000 - mae: 1836.7155 - val_loss: 2178.1622 - val_mse: 25620706.0000 - val_mae: 2178.1621\n",
            "Epoch 462/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 1843.6448 - mse: 19984534.0000 - mae: 1843.6449 - val_loss: 2175.4553 - val_mse: 26196614.0000 - val_mae: 2175.4553\n",
            "Epoch 463/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 1828.3478 - mse: 19282470.0000 - mae: 1828.3479 - val_loss: 2153.4991 - val_mse: 25310794.0000 - val_mae: 2153.4990\n",
            "Epoch 464/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 1873.2646 - mse: 21333466.0000 - mae: 1873.2646 - val_loss: 2187.3928 - val_mse: 25224090.0000 - val_mae: 2187.3931\n",
            "Epoch 465/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1943.7649 - mse: 22821122.0000 - mae: 1943.7648 - val_loss: 2173.6896 - val_mse: 25563356.0000 - val_mae: 2173.6895\n",
            "Epoch 466/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1877.9323 - mse: 20374240.0000 - mae: 1877.9324 - val_loss: 2176.9418 - val_mse: 25104432.0000 - val_mae: 2176.9419\n",
            "Epoch 467/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1866.6839 - mse: 20694854.0000 - mae: 1866.6840 - val_loss: 2138.7670 - val_mse: 25009334.0000 - val_mae: 2138.7671\n",
            "Epoch 468/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 1954.8800 - mse: 22564890.0000 - mae: 1954.8800 - val_loss: 2170.2834 - val_mse: 25032200.0000 - val_mae: 2170.2832\n",
            "Epoch 469/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1896.7172 - mse: 21552142.0000 - mae: 1896.7172 - val_loss: 2177.3922 - val_mse: 24973800.0000 - val_mae: 2177.3923\n",
            "Epoch 470/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1848.1437 - mse: 19259218.0000 - mae: 1848.1437 - val_loss: 2153.2132 - val_mse: 25226640.0000 - val_mae: 2153.2131\n",
            "Epoch 471/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1871.2699 - mse: 21354204.0000 - mae: 1871.2699 - val_loss: 2120.6448 - val_mse: 24847458.0000 - val_mae: 2120.6448\n",
            "Epoch 472/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 1856.3001 - mse: 21076656.0000 - mae: 1856.3002 - val_loss: 2152.7495 - val_mse: 25255426.0000 - val_mae: 2152.7495\n",
            "Epoch 473/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1941.5730 - mse: 21725144.0000 - mae: 1941.5730 - val_loss: 2166.8747 - val_mse: 24997556.0000 - val_mae: 2166.8745\n",
            "Epoch 474/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1827.3568 - mse: 19552948.0000 - mae: 1827.3568 - val_loss: 2133.3004 - val_mse: 24971486.0000 - val_mae: 2133.3000\n",
            "Epoch 475/500\n",
            "1880/1880 [==============================] - 0s 87us/step - loss: 1881.2350 - mse: 20666142.0000 - mae: 1881.2349 - val_loss: 2150.8149 - val_mse: 24675416.0000 - val_mae: 2150.8147\n",
            "Epoch 476/500\n",
            "1880/1880 [==============================] - 0s 71us/step - loss: 1915.8807 - mse: 21827216.0000 - mae: 1915.8806 - val_loss: 2116.8778 - val_mse: 24399892.0000 - val_mae: 2116.8777\n",
            "Epoch 477/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 1828.8271 - mse: 19985160.0000 - mae: 1828.8271 - val_loss: 2133.9220 - val_mse: 24969418.0000 - val_mae: 2133.9221\n",
            "Epoch 478/500\n",
            "1880/1880 [==============================] - 0s 83us/step - loss: 1870.8643 - mse: 20768100.0000 - mae: 1870.8645 - val_loss: 2119.7406 - val_mse: 24693180.0000 - val_mae: 2119.7407\n",
            "Epoch 479/500\n",
            "1880/1880 [==============================] - 0s 85us/step - loss: 1832.1700 - mse: 20588406.0000 - mae: 1832.1699 - val_loss: 2155.3896 - val_mse: 24401438.0000 - val_mae: 2155.3896\n",
            "Epoch 480/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1812.6663 - mse: 19456522.0000 - mae: 1812.6664 - val_loss: 2148.4870 - val_mse: 24609116.0000 - val_mae: 2148.4871\n",
            "Epoch 481/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1943.4544 - mse: 24029716.0000 - mae: 1943.4543 - val_loss: 2130.1046 - val_mse: 24590148.0000 - val_mae: 2130.1047\n",
            "Epoch 482/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 1887.7774 - mse: 21401178.0000 - mae: 1887.7773 - val_loss: 2117.5047 - val_mse: 24718794.0000 - val_mae: 2117.5049\n",
            "Epoch 483/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1862.7789 - mse: 21103608.0000 - mae: 1862.7788 - val_loss: 2119.2490 - val_mse: 24631668.0000 - val_mae: 2119.2493\n",
            "Epoch 484/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 1803.4899 - mse: 18743672.0000 - mae: 1803.4899 - val_loss: 2130.9585 - val_mse: 25018526.0000 - val_mae: 2130.9585\n",
            "Epoch 485/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1927.4582 - mse: 22391826.0000 - mae: 1927.4581 - val_loss: 2120.5760 - val_mse: 24228032.0000 - val_mae: 2120.5762\n",
            "Epoch 486/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1925.4522 - mse: 23963976.0000 - mae: 1925.4521 - val_loss: 2110.3094 - val_mse: 24515294.0000 - val_mae: 2110.3096\n",
            "Epoch 487/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1890.8767 - mse: 21306142.0000 - mae: 1890.8767 - val_loss: 2136.2232 - val_mse: 24693646.0000 - val_mae: 2136.2231\n",
            "Epoch 488/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 1790.8046 - mse: 18088174.0000 - mae: 1790.8047 - val_loss: 2158.5309 - val_mse: 24175664.0000 - val_mae: 2158.5308\n",
            "Epoch 489/500\n",
            "1880/1880 [==============================] - 0s 79us/step - loss: 1866.9492 - mse: 20776926.0000 - mae: 1866.9492 - val_loss: 2114.9038 - val_mse: 24338300.0000 - val_mae: 2114.9038\n",
            "Epoch 490/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1875.1756 - mse: 21135702.0000 - mae: 1875.1758 - val_loss: 2111.3309 - val_mse: 24691534.0000 - val_mae: 2111.3308\n",
            "Epoch 491/500\n",
            "1880/1880 [==============================] - 0s 77us/step - loss: 1810.9882 - mse: 19082002.0000 - mae: 1810.9883 - val_loss: 2110.4669 - val_mse: 24617650.0000 - val_mae: 2110.4668\n",
            "Epoch 492/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1935.1754 - mse: 23120380.0000 - mae: 1935.1755 - val_loss: 2117.8572 - val_mse: 24210242.0000 - val_mae: 2117.8574\n",
            "Epoch 493/500\n",
            "1880/1880 [==============================] - 0s 74us/step - loss: 1770.1808 - mse: 18293056.0000 - mae: 1770.1807 - val_loss: 2124.0532 - val_mse: 24433218.0000 - val_mae: 2124.0532\n",
            "Epoch 494/500\n",
            "1880/1880 [==============================] - 0s 80us/step - loss: 1838.8313 - mse: 20073644.0000 - mae: 1838.8313 - val_loss: 2119.2619 - val_mse: 24815874.0000 - val_mae: 2119.2620\n",
            "Epoch 495/500\n",
            "1880/1880 [==============================] - 0s 75us/step - loss: 1936.4967 - mse: 23220230.0000 - mae: 1936.4968 - val_loss: 2107.3995 - val_mse: 24082314.0000 - val_mae: 2107.3997\n",
            "Epoch 496/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 1881.4267 - mse: 20759176.0000 - mae: 1881.4268 - val_loss: 2102.5272 - val_mse: 24258726.0000 - val_mae: 2102.5273\n",
            "Epoch 497/500\n",
            "1880/1880 [==============================] - 0s 78us/step - loss: 1877.8921 - mse: 21389434.0000 - mae: 1877.8923 - val_loss: 2124.6236 - val_mse: 24168918.0000 - val_mae: 2124.6235\n",
            "Epoch 498/500\n",
            "1880/1880 [==============================] - 0s 81us/step - loss: 1846.2029 - mse: 20313840.0000 - mae: 1846.2029 - val_loss: 2111.9935 - val_mse: 24202206.0000 - val_mae: 2111.9937\n",
            "Epoch 499/500\n",
            "1880/1880 [==============================] - 0s 76us/step - loss: 1868.8292 - mse: 21266938.0000 - mae: 1868.8290 - val_loss: 2098.1166 - val_mse: 23963224.0000 - val_mae: 2098.1167\n",
            "Epoch 500/500\n",
            "1880/1880 [==============================] - 0s 72us/step - loss: 1884.0337 - mse: 21675112.0000 - mae: 1884.0337 - val_loss: 2121.3892 - val_mse: 24023444.0000 - val_mae: 2121.3892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 2250 samples, validate on 2250 samples\n",
            "Epoch 1/500\n",
            "2250/2250 [==============================] - 2s 965us/step - loss: 4883.8504 - mse: 166353072.0000 - mae: 4883.8506 - val_loss: 5319.2644 - val_mse: 191311728.0000 - val_mae: 5319.2646\n",
            "Epoch 2/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4880.1178 - mse: 166338208.0000 - mae: 4880.1177 - val_loss: 5307.6227 - val_mse: 191261536.0000 - val_mae: 5307.6226\n",
            "Epoch 3/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 4859.1408 - mse: 166253744.0000 - mae: 4859.1411 - val_loss: 5273.4137 - val_mse: 191102320.0000 - val_mae: 5273.4136\n",
            "Epoch 4/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 4825.5197 - mse: 166098480.0000 - mae: 4825.5200 - val_loss: 5234.6889 - val_mse: 190890304.0000 - val_mae: 5234.6890\n",
            "Epoch 5/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4786.9626 - mse: 165883424.0000 - mae: 4786.9624 - val_loss: 5190.3081 - val_mse: 190605760.0000 - val_mae: 5190.3086\n",
            "Epoch 6/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4745.7734 - mse: 165600736.0000 - mae: 4745.7734 - val_loss: 5149.1397 - val_mse: 190282816.0000 - val_mae: 5149.1401\n",
            "Epoch 7/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4710.5349 - mse: 165323824.0000 - mae: 4710.5352 - val_loss: 5112.5593 - val_mse: 189943872.0000 - val_mae: 5112.5596\n",
            "Epoch 8/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4678.8869 - mse: 165010976.0000 - mae: 4678.8872 - val_loss: 5082.2222 - val_mse: 189596960.0000 - val_mae: 5082.2227\n",
            "Epoch 9/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4661.3894 - mse: 164741904.0000 - mae: 4661.3892 - val_loss: 5059.6337 - val_mse: 189276528.0000 - val_mae: 5059.6328\n",
            "Epoch 10/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4649.1574 - mse: 164459008.0000 - mae: 4649.1577 - val_loss: 5041.2570 - val_mse: 188966064.0000 - val_mae: 5041.2573\n",
            "Epoch 11/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4635.0069 - mse: 164158384.0000 - mae: 4635.0073 - val_loss: 5025.8634 - val_mse: 188656624.0000 - val_mae: 5025.8633\n",
            "Epoch 12/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4622.3211 - mse: 163871216.0000 - mae: 4622.3208 - val_loss: 5013.0833 - val_mse: 188350800.0000 - val_mae: 5013.0830\n",
            "Epoch 13/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4615.2822 - mse: 163547456.0000 - mae: 4615.2827 - val_loss: 5002.0004 - val_mse: 188065664.0000 - val_mae: 5002.0005\n",
            "Epoch 14/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4603.4742 - mse: 163296016.0000 - mae: 4603.4741 - val_loss: 4991.8260 - val_mse: 187777392.0000 - val_mae: 4991.8257\n",
            "Epoch 15/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4596.1300 - mse: 163001440.0000 - mae: 4596.1304 - val_loss: 4983.8046 - val_mse: 187516800.0000 - val_mae: 4983.8052\n",
            "Epoch 16/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 4586.2362 - mse: 162784432.0000 - mae: 4586.2363 - val_loss: 4977.9703 - val_mse: 187302624.0000 - val_mae: 4977.9697\n",
            "Epoch 17/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 4593.6402 - mse: 162693296.0000 - mae: 4593.6406 - val_loss: 4973.5148 - val_mse: 187117856.0000 - val_mae: 4973.5146\n",
            "Epoch 18/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4591.5138 - mse: 162510576.0000 - mae: 4591.5137 - val_loss: 4970.1864 - val_mse: 186963408.0000 - val_mae: 4970.1855\n",
            "Epoch 19/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4582.9630 - mse: 162313616.0000 - mae: 4582.9629 - val_loss: 4967.7672 - val_mse: 186847024.0000 - val_mae: 4967.7676\n",
            "Epoch 20/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4582.5913 - mse: 162152688.0000 - mae: 4582.5918 - val_loss: 4964.5649 - val_mse: 186690688.0000 - val_mae: 4964.5654\n",
            "Epoch 21/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4573.7004 - mse: 162036704.0000 - mae: 4573.7007 - val_loss: 4962.2660 - val_mse: 186577568.0000 - val_mae: 4962.2656\n",
            "Epoch 22/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4573.4169 - mse: 161974624.0000 - mae: 4573.4170 - val_loss: 4959.4836 - val_mse: 186435072.0000 - val_mae: 4959.4839\n",
            "Epoch 23/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4576.2749 - mse: 161872784.0000 - mae: 4576.2749 - val_loss: 4957.2423 - val_mse: 186316704.0000 - val_mae: 4957.2422\n",
            "Epoch 24/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4570.6306 - mse: 161734032.0000 - mae: 4570.6309 - val_loss: 4953.7739 - val_mse: 186088832.0000 - val_mae: 4953.7744\n",
            "Epoch 25/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 4574.2954 - mse: 161491808.0000 - mae: 4574.2949 - val_loss: 4950.0762 - val_mse: 185791328.0000 - val_mae: 4950.0767\n",
            "Epoch 26/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4565.9868 - mse: 161315152.0000 - mae: 4565.9868 - val_loss: 4946.7970 - val_mse: 185593120.0000 - val_mae: 4946.7974\n",
            "Epoch 27/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4562.8359 - mse: 161206624.0000 - mae: 4562.8359 - val_loss: 4942.6055 - val_mse: 185249072.0000 - val_mae: 4942.6060\n",
            "Epoch 28/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4560.8897 - mse: 160744160.0000 - mae: 4560.8896 - val_loss: 4939.3072 - val_mse: 185048320.0000 - val_mae: 4939.3066\n",
            "Epoch 29/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4551.1994 - mse: 160512512.0000 - mae: 4551.1992 - val_loss: 4935.6840 - val_mse: 184843808.0000 - val_mae: 4935.6841\n",
            "Epoch 30/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4558.0940 - mse: 160394656.0000 - mae: 4558.0938 - val_loss: 4931.4069 - val_mse: 184525136.0000 - val_mae: 4931.4067\n",
            "Epoch 31/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4555.8772 - mse: 160188464.0000 - mae: 4555.8774 - val_loss: 4927.8090 - val_mse: 184299856.0000 - val_mae: 4927.8096\n",
            "Epoch 32/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4545.7233 - mse: 159992384.0000 - mae: 4545.7231 - val_loss: 4923.8887 - val_mse: 184045936.0000 - val_mae: 4923.8887\n",
            "Epoch 33/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4536.8178 - mse: 159640048.0000 - mae: 4536.8184 - val_loss: 4919.6636 - val_mse: 183745184.0000 - val_mae: 4919.6641\n",
            "Epoch 34/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4538.0840 - mse: 159315392.0000 - mae: 4538.0835 - val_loss: 4916.4733 - val_mse: 183545824.0000 - val_mae: 4916.4731\n",
            "Epoch 35/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4530.7519 - mse: 158999520.0000 - mae: 4530.7520 - val_loss: 4914.3214 - val_mse: 183394496.0000 - val_mae: 4914.3213\n",
            "Epoch 36/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4543.9666 - mse: 159212320.0000 - mae: 4543.9668 - val_loss: 4911.3601 - val_mse: 183238240.0000 - val_mae: 4911.3604\n",
            "Epoch 37/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4527.7515 - mse: 159050464.0000 - mae: 4527.7510 - val_loss: 4900.8219 - val_mse: 182976000.0000 - val_mae: 4900.8218\n",
            "Epoch 38/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4498.3601 - mse: 158756768.0000 - mae: 4498.3599 - val_loss: 4875.2430 - val_mse: 182722192.0000 - val_mae: 4875.2432\n",
            "Epoch 39/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4472.6171 - mse: 158309424.0000 - mae: 4472.6167 - val_loss: 4873.0747 - val_mse: 182208768.0000 - val_mae: 4873.0747\n",
            "Epoch 40/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4458.3078 - mse: 157709600.0000 - mae: 4458.3076 - val_loss: 4830.3900 - val_mse: 181117296.0000 - val_mae: 4830.3901\n",
            "Epoch 41/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4449.9414 - mse: 157053024.0000 - mae: 4449.9414 - val_loss: 4830.5144 - val_mse: 180729408.0000 - val_mae: 4830.5137\n",
            "Epoch 42/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4443.6269 - mse: 156640816.0000 - mae: 4443.6265 - val_loss: 4819.3513 - val_mse: 180090512.0000 - val_mae: 4819.3516\n",
            "Epoch 43/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4419.1905 - mse: 155761776.0000 - mae: 4419.1904 - val_loss: 4775.7583 - val_mse: 178960128.0000 - val_mae: 4775.7573\n",
            "Epoch 44/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4402.9413 - mse: 155151872.0000 - mae: 4402.9409 - val_loss: 4760.0243 - val_mse: 178210096.0000 - val_mae: 4760.0244\n",
            "Epoch 45/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 4380.5629 - mse: 154061344.0000 - mae: 4380.5625 - val_loss: 4735.9059 - val_mse: 177221408.0000 - val_mae: 4735.9058\n",
            "Epoch 46/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 4374.2915 - mse: 153519856.0000 - mae: 4374.2920 - val_loss: 4721.6130 - val_mse: 176546208.0000 - val_mae: 4721.6128\n",
            "Epoch 47/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4362.6594 - mse: 152957856.0000 - mae: 4362.6592 - val_loss: 4704.7423 - val_mse: 175711504.0000 - val_mae: 4704.7427\n",
            "Epoch 48/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4343.7299 - mse: 151783376.0000 - mae: 4343.7300 - val_loss: 4687.1945 - val_mse: 174875104.0000 - val_mae: 4687.1953\n",
            "Epoch 49/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4331.4880 - mse: 151716928.0000 - mae: 4331.4878 - val_loss: 4680.7439 - val_mse: 174147488.0000 - val_mae: 4680.7437\n",
            "Epoch 50/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4317.1765 - mse: 150647760.0000 - mae: 4317.1763 - val_loss: 4650.5461 - val_mse: 173062352.0000 - val_mae: 4650.5464\n",
            "Epoch 51/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4303.7523 - mse: 149488960.0000 - mae: 4303.7524 - val_loss: 4647.1739 - val_mse: 172432384.0000 - val_mae: 4647.1743\n",
            "Epoch 52/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 4290.8866 - mse: 149177264.0000 - mae: 4290.8867 - val_loss: 4621.4260 - val_mse: 171436992.0000 - val_mae: 4621.4263\n",
            "Epoch 53/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4280.2277 - mse: 148286288.0000 - mae: 4280.2275 - val_loss: 4615.2152 - val_mse: 170762896.0000 - val_mae: 4615.2153\n",
            "Epoch 54/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4283.2910 - mse: 148033088.0000 - mae: 4283.2910 - val_loss: 4601.4727 - val_mse: 170063248.0000 - val_mae: 4601.4731\n",
            "Epoch 55/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 4270.0456 - mse: 147512944.0000 - mae: 4270.0459 - val_loss: 4587.9030 - val_mse: 169311360.0000 - val_mae: 4587.9033\n",
            "Epoch 56/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4245.4218 - mse: 146203680.0000 - mae: 4245.4219 - val_loss: 4576.3540 - val_mse: 168629072.0000 - val_mae: 4576.3540\n",
            "Epoch 57/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 4248.2166 - mse: 145804576.0000 - mae: 4248.2163 - val_loss: 4562.4620 - val_mse: 167837104.0000 - val_mae: 4562.4619\n",
            "Epoch 58/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4238.1489 - mse: 145085376.0000 - mae: 4238.1489 - val_loss: 4543.4876 - val_mse: 166972512.0000 - val_mae: 4543.4878\n",
            "Epoch 59/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 4223.9382 - mse: 144584320.0000 - mae: 4223.9380 - val_loss: 4538.6327 - val_mse: 166377648.0000 - val_mae: 4538.6323\n",
            "Epoch 60/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4211.3869 - mse: 143944976.0000 - mae: 4211.3872 - val_loss: 4535.1430 - val_mse: 165731440.0000 - val_mae: 4535.1426\n",
            "Epoch 61/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4199.4924 - mse: 142795360.0000 - mae: 4199.4927 - val_loss: 4513.8577 - val_mse: 164883808.0000 - val_mae: 4513.8579\n",
            "Epoch 62/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 4195.5599 - mse: 142996064.0000 - mae: 4195.5601 - val_loss: 4500.6046 - val_mse: 164108224.0000 - val_mae: 4500.6055\n",
            "Epoch 63/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 4178.7192 - mse: 142419552.0000 - mae: 4178.7197 - val_loss: 4488.3095 - val_mse: 163322448.0000 - val_mae: 4488.3096\n",
            "Epoch 64/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4188.8239 - mse: 142258464.0000 - mae: 4188.8242 - val_loss: 4486.6768 - val_mse: 162784832.0000 - val_mae: 4486.6768\n",
            "Epoch 65/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4169.9990 - mse: 140303184.0000 - mae: 4169.9985 - val_loss: 4470.4149 - val_mse: 161944368.0000 - val_mae: 4470.4150\n",
            "Epoch 66/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 4162.0320 - mse: 140519056.0000 - mae: 4162.0322 - val_loss: 4460.9871 - val_mse: 161189440.0000 - val_mae: 4460.9873\n",
            "Epoch 67/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 4147.4449 - mse: 139306880.0000 - mae: 4147.4448 - val_loss: 4458.5066 - val_mse: 160553568.0000 - val_mae: 4458.5073\n",
            "Epoch 68/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 4140.6414 - mse: 139474224.0000 - mae: 4140.6411 - val_loss: 4439.6685 - val_mse: 159759632.0000 - val_mae: 4439.6680\n",
            "Epoch 69/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4127.9454 - mse: 138318528.0000 - mae: 4127.9453 - val_loss: 4429.3746 - val_mse: 159131216.0000 - val_mae: 4429.3750\n",
            "Epoch 70/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 4114.9530 - mse: 138169008.0000 - mae: 4114.9531 - val_loss: 4415.0353 - val_mse: 158387920.0000 - val_mae: 4415.0352\n",
            "Epoch 71/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 4112.2183 - mse: 137712800.0000 - mae: 4112.2183 - val_loss: 4409.6575 - val_mse: 157822944.0000 - val_mae: 4409.6577\n",
            "Epoch 72/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4083.0698 - mse: 135741952.0000 - mae: 4083.0698 - val_loss: 4394.8834 - val_mse: 157009088.0000 - val_mae: 4394.8838\n",
            "Epoch 73/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4086.4071 - mse: 135932640.0000 - mae: 4086.4067 - val_loss: 4384.2573 - val_mse: 156300768.0000 - val_mae: 4384.2578\n",
            "Epoch 74/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4095.1858 - mse: 135619712.0000 - mae: 4095.1858 - val_loss: 4377.0456 - val_mse: 155625888.0000 - val_mae: 4377.0459\n",
            "Epoch 75/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4046.8295 - mse: 133714240.0000 - mae: 4046.8293 - val_loss: 4361.4545 - val_mse: 154762304.0000 - val_mae: 4361.4541\n",
            "Epoch 76/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4057.7376 - mse: 133656000.0000 - mae: 4057.7373 - val_loss: 4357.7265 - val_mse: 154145616.0000 - val_mae: 4357.7261\n",
            "Epoch 77/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4040.4719 - mse: 133292576.0000 - mae: 4040.4719 - val_loss: 4341.8973 - val_mse: 153352624.0000 - val_mae: 4341.8979\n",
            "Epoch 78/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 4034.2041 - mse: 132365072.0000 - mae: 4034.2043 - val_loss: 4324.1834 - val_mse: 152475792.0000 - val_mae: 4324.1841\n",
            "Epoch 79/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 4007.2634 - mse: 131146096.0000 - mae: 4007.2632 - val_loss: 4320.9317 - val_mse: 151825552.0000 - val_mae: 4320.9321\n",
            "Epoch 80/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 4001.6653 - mse: 131037968.0000 - mae: 4001.6653 - val_loss: 4309.0043 - val_mse: 151189856.0000 - val_mae: 4309.0049\n",
            "Epoch 81/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 4003.6385 - mse: 131132384.0000 - mae: 4003.6382 - val_loss: 4311.8016 - val_mse: 150703936.0000 - val_mae: 4311.8013\n",
            "Epoch 82/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 4001.5378 - mse: 130668440.0000 - mae: 4001.5378 - val_loss: 4290.1755 - val_mse: 149882672.0000 - val_mae: 4290.1758\n",
            "Epoch 83/500\n",
            "2250/2250 [==============================] - 0s 59us/step - loss: 3980.5092 - mse: 130058944.0000 - mae: 3980.5093 - val_loss: 4278.0903 - val_mse: 149072528.0000 - val_mae: 4278.0908\n",
            "Epoch 84/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 3958.7013 - mse: 128475408.0000 - mae: 3958.7014 - val_loss: 4273.5219 - val_mse: 148577664.0000 - val_mae: 4273.5215\n",
            "Epoch 85/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 3957.1961 - mse: 128203152.0000 - mae: 3957.1960 - val_loss: 4269.7286 - val_mse: 147914528.0000 - val_mae: 4269.7285\n",
            "Epoch 86/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3956.4309 - mse: 127453000.0000 - mae: 3956.4307 - val_loss: 4262.2213 - val_mse: 147277696.0000 - val_mae: 4262.2217\n",
            "Epoch 87/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 3961.3971 - mse: 127664568.0000 - mae: 3961.3972 - val_loss: 4258.3569 - val_mse: 146798912.0000 - val_mae: 4258.3569\n",
            "Epoch 88/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3937.2668 - mse: 126456008.0000 - mae: 3937.2666 - val_loss: 4248.3116 - val_mse: 146173264.0000 - val_mae: 4248.3120\n",
            "Epoch 89/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 3935.5989 - mse: 126045944.0000 - mae: 3935.5991 - val_loss: 4233.5955 - val_mse: 145472400.0000 - val_mae: 4233.5957\n",
            "Epoch 90/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 3948.5386 - mse: 125951552.0000 - mae: 3948.5386 - val_loss: 4224.9846 - val_mse: 144838592.0000 - val_mae: 4224.9854\n",
            "Epoch 91/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 3900.9678 - mse: 125225472.0000 - mae: 3900.9675 - val_loss: 4228.2401 - val_mse: 144350800.0000 - val_mae: 4228.2402\n",
            "Epoch 92/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 3906.8409 - mse: 124498304.0000 - mae: 3906.8408 - val_loss: 4205.8121 - val_mse: 143615072.0000 - val_mae: 4205.8120\n",
            "Epoch 93/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3910.8047 - mse: 124892712.0000 - mae: 3910.8049 - val_loss: 4210.7828 - val_mse: 143492192.0000 - val_mae: 4210.7827\n",
            "Epoch 94/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 3893.0538 - mse: 123939848.0000 - mae: 3893.0537 - val_loss: 4190.2693 - val_mse: 142619984.0000 - val_mae: 4190.2700\n",
            "Epoch 95/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 3898.7620 - mse: 123387352.0000 - mae: 3898.7617 - val_loss: 4193.1596 - val_mse: 142233824.0000 - val_mae: 4193.1597\n",
            "Epoch 96/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 3879.0879 - mse: 121599608.0000 - mae: 3879.0879 - val_loss: 4176.2955 - val_mse: 141498016.0000 - val_mae: 4176.2954\n",
            "Epoch 97/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 3873.1655 - mse: 121768760.0000 - mae: 3873.1658 - val_loss: 4161.1630 - val_mse: 140791040.0000 - val_mae: 4161.1631\n",
            "Epoch 98/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3858.4880 - mse: 121360912.0000 - mae: 3858.4875 - val_loss: 4153.0927 - val_mse: 140187856.0000 - val_mae: 4153.0928\n",
            "Epoch 99/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3831.7480 - mse: 120198584.0000 - mae: 3831.7480 - val_loss: 4138.1947 - val_mse: 139469680.0000 - val_mae: 4138.1948\n",
            "Epoch 100/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 3835.9297 - mse: 120346472.0000 - mae: 3835.9297 - val_loss: 4135.2499 - val_mse: 138893824.0000 - val_mae: 4135.2500\n",
            "Epoch 101/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3864.2916 - mse: 122051632.0000 - mae: 3864.2915 - val_loss: 4110.6201 - val_mse: 138107120.0000 - val_mae: 4110.6201\n",
            "Epoch 102/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3854.9375 - mse: 119629440.0000 - mae: 3854.9373 - val_loss: 4103.7969 - val_mse: 137662400.0000 - val_mae: 4103.7969\n",
            "Epoch 103/500\n",
            "2250/2250 [==============================] - 0s 60us/step - loss: 3837.0633 - mse: 119481952.0000 - mae: 3837.0632 - val_loss: 4091.1400 - val_mse: 137087328.0000 - val_mae: 4091.1399\n",
            "Epoch 104/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 3824.1013 - mse: 119229736.0000 - mae: 3824.1013 - val_loss: 4078.4168 - val_mse: 136543424.0000 - val_mae: 4078.4170\n",
            "Epoch 105/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 3811.4397 - mse: 118838144.0000 - mae: 3811.4395 - val_loss: 4072.7005 - val_mse: 136154880.0000 - val_mae: 4072.7004\n",
            "Epoch 106/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3773.2590 - mse: 118396400.0000 - mae: 3773.2590 - val_loss: 4054.3598 - val_mse: 135267184.0000 - val_mae: 4054.3601\n",
            "Epoch 107/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 3759.2539 - mse: 116972184.0000 - mae: 3759.2537 - val_loss: 4036.3255 - val_mse: 134578160.0000 - val_mae: 4036.3250\n",
            "Epoch 108/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 3742.1587 - mse: 116142872.0000 - mae: 3742.1587 - val_loss: 4011.9403 - val_mse: 133629344.0000 - val_mae: 4011.9399\n",
            "Epoch 109/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 3728.8094 - mse: 116039216.0000 - mae: 3728.8088 - val_loss: 3994.6034 - val_mse: 132702288.0000 - val_mae: 3994.6035\n",
            "Epoch 110/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 3710.7007 - mse: 114283992.0000 - mae: 3710.7009 - val_loss: 3983.9753 - val_mse: 131692568.0000 - val_mae: 3983.9751\n",
            "Epoch 111/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3716.3850 - mse: 113730152.0000 - mae: 3716.3850 - val_loss: 3984.7536 - val_mse: 130908032.0000 - val_mae: 3984.7537\n",
            "Epoch 112/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3693.7704 - mse: 113198824.0000 - mae: 3693.7703 - val_loss: 4015.2688 - val_mse: 130400080.0000 - val_mae: 4015.2688\n",
            "Epoch 113/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 3649.6962 - mse: 109954112.0000 - mae: 3649.6960 - val_loss: 3952.6917 - val_mse: 128709776.0000 - val_mae: 3952.6917\n",
            "Epoch 114/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 3657.8651 - mse: 112055928.0000 - mae: 3657.8650 - val_loss: 3980.3837 - val_mse: 128138304.0000 - val_mae: 3980.3831\n",
            "Epoch 115/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 3673.1070 - mse: 111040672.0000 - mae: 3673.1069 - val_loss: 3953.6314 - val_mse: 127015232.0000 - val_mae: 3953.6311\n",
            "Epoch 116/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 3609.3691 - mse: 108984832.0000 - mae: 3609.3691 - val_loss: 3950.4864 - val_mse: 126329376.0000 - val_mae: 3950.4866\n",
            "Epoch 117/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3636.2797 - mse: 108502680.0000 - mae: 3636.2795 - val_loss: 3920.7245 - val_mse: 124984200.0000 - val_mae: 3920.7244\n",
            "Epoch 118/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 3622.3105 - mse: 108304944.0000 - mae: 3622.3105 - val_loss: 3913.5036 - val_mse: 124156424.0000 - val_mae: 3913.5037\n",
            "Epoch 119/500\n",
            "2250/2250 [==============================] - 0s 77us/step - loss: 3637.9478 - mse: 107109952.0000 - mae: 3637.9478 - val_loss: 3887.9935 - val_mse: 122876872.0000 - val_mae: 3887.9934\n",
            "Epoch 120/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3545.0053 - mse: 103412328.0000 - mae: 3545.0054 - val_loss: 3868.8240 - val_mse: 121453552.0000 - val_mae: 3868.8240\n",
            "Epoch 121/500\n",
            "2250/2250 [==============================] - 0s 60us/step - loss: 3588.9602 - mse: 104847976.0000 - mae: 3588.9602 - val_loss: 3865.6367 - val_mse: 120731488.0000 - val_mae: 3865.6370\n",
            "Epoch 122/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 3563.6689 - mse: 103281024.0000 - mae: 3563.6689 - val_loss: 3855.4339 - val_mse: 119775192.0000 - val_mae: 3855.4343\n",
            "Epoch 123/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 3541.3438 - mse: 102705784.0000 - mae: 3541.3438 - val_loss: 3867.4553 - val_mse: 118831744.0000 - val_mae: 3867.4551\n",
            "Epoch 124/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3508.0177 - mse: 99620912.0000 - mae: 3508.0176 - val_loss: 3812.4175 - val_mse: 116903272.0000 - val_mae: 3812.4177\n",
            "Epoch 125/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 3538.0320 - mse: 100231416.0000 - mae: 3538.0322 - val_loss: 3821.8040 - val_mse: 116129984.0000 - val_mae: 3821.8040\n",
            "Epoch 126/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 3504.4792 - mse: 99015752.0000 - mae: 3504.4792 - val_loss: 3791.2045 - val_mse: 114813600.0000 - val_mae: 3791.2043\n",
            "Epoch 127/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3458.5442 - mse: 94155216.0000 - mae: 3458.5444 - val_loss: 3783.0769 - val_mse: 113658936.0000 - val_mae: 3783.0769\n",
            "Epoch 128/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3494.2080 - mse: 98118448.0000 - mae: 3494.2080 - val_loss: 3769.1471 - val_mse: 112503496.0000 - val_mae: 3769.1472\n",
            "Epoch 129/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3425.0120 - mse: 94169032.0000 - mae: 3425.0117 - val_loss: 3774.8239 - val_mse: 111388088.0000 - val_mae: 3774.8235\n",
            "Epoch 130/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 3459.9683 - mse: 95204984.0000 - mae: 3459.9683 - val_loss: 3738.0313 - val_mse: 110190312.0000 - val_mae: 3738.0315\n",
            "Epoch 131/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 3416.2478 - mse: 93985264.0000 - mae: 3416.2476 - val_loss: 3732.3283 - val_mse: 108895720.0000 - val_mae: 3732.3284\n",
            "Epoch 132/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3377.3077 - mse: 90895416.0000 - mae: 3377.3081 - val_loss: 3749.6520 - val_mse: 107789376.0000 - val_mae: 3749.6521\n",
            "Epoch 133/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3390.9492 - mse: 90863800.0000 - mae: 3390.9492 - val_loss: 3694.1183 - val_mse: 106088312.0000 - val_mae: 3694.1187\n",
            "Epoch 134/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 3382.0563 - mse: 90567440.0000 - mae: 3382.0562 - val_loss: 3652.9152 - val_mse: 104508720.0000 - val_mae: 3652.9150\n",
            "Epoch 135/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3333.0370 - mse: 86797000.0000 - mae: 3333.0369 - val_loss: 3662.2014 - val_mse: 103203392.0000 - val_mae: 3662.2017\n",
            "Epoch 136/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 3311.3912 - mse: 87294504.0000 - mae: 3311.3914 - val_loss: 3622.3421 - val_mse: 101545000.0000 - val_mae: 3622.3425\n",
            "Epoch 137/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3328.9196 - mse: 86966512.0000 - mae: 3328.9194 - val_loss: 3618.2829 - val_mse: 100403568.0000 - val_mae: 3618.2830\n",
            "Epoch 138/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3270.4508 - mse: 83728760.0000 - mae: 3270.4507 - val_loss: 3576.6741 - val_mse: 98695688.0000 - val_mae: 3576.6741\n",
            "Epoch 139/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3257.9001 - mse: 83279856.0000 - mae: 3257.9001 - val_loss: 3543.9673 - val_mse: 97219832.0000 - val_mae: 3543.9673\n",
            "Epoch 140/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 3212.5757 - mse: 80666192.0000 - mae: 3212.5757 - val_loss: 3537.6815 - val_mse: 95817552.0000 - val_mae: 3537.6816\n",
            "Epoch 141/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3131.0700 - mse: 75338704.0000 - mae: 3131.0701 - val_loss: 3508.6694 - val_mse: 94128776.0000 - val_mae: 3508.6694\n",
            "Epoch 142/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3206.4538 - mse: 80203936.0000 - mae: 3206.4539 - val_loss: 3529.0928 - val_mse: 93111104.0000 - val_mae: 3529.0930\n",
            "Epoch 143/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3208.2447 - mse: 80942168.0000 - mae: 3208.2449 - val_loss: 3444.6505 - val_mse: 91239472.0000 - val_mae: 3444.6501\n",
            "Epoch 144/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3130.8104 - mse: 75485424.0000 - mae: 3130.8105 - val_loss: 3476.3510 - val_mse: 90030416.0000 - val_mae: 3476.3506\n",
            "Epoch 145/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 3156.6396 - mse: 77615016.0000 - mae: 3156.6396 - val_loss: 3405.7891 - val_mse: 88214056.0000 - val_mae: 3405.7893\n",
            "Epoch 146/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3086.5508 - mse: 75558384.0000 - mae: 3086.5508 - val_loss: 3378.6642 - val_mse: 86614552.0000 - val_mae: 3378.6646\n",
            "Epoch 147/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3099.5237 - mse: 73740656.0000 - mae: 3099.5239 - val_loss: 3370.7396 - val_mse: 85624280.0000 - val_mae: 3370.7395\n",
            "Epoch 148/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 3061.5742 - mse: 71491976.0000 - mae: 3061.5742 - val_loss: 3386.7985 - val_mse: 84472896.0000 - val_mae: 3386.7983\n",
            "Epoch 149/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 3072.2789 - mse: 72678360.0000 - mae: 3072.2788 - val_loss: 3334.8094 - val_mse: 82928992.0000 - val_mae: 3334.8096\n",
            "Epoch 150/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 3029.7949 - mse: 71314816.0000 - mae: 3029.7947 - val_loss: 3287.3708 - val_mse: 81156400.0000 - val_mae: 3287.3706\n",
            "Epoch 151/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 2944.3387 - mse: 65833364.0000 - mae: 2944.3386 - val_loss: 3271.6603 - val_mse: 79709096.0000 - val_mae: 3271.6599\n",
            "Epoch 152/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 2955.2734 - mse: 68098920.0000 - mae: 2955.2734 - val_loss: 3224.5422 - val_mse: 78111424.0000 - val_mae: 3224.5425\n",
            "Epoch 153/500\n",
            "2250/2250 [==============================] - 0s 77us/step - loss: 2979.4754 - mse: 66421936.0000 - mae: 2979.4756 - val_loss: 3216.0557 - val_mse: 76895136.0000 - val_mae: 3216.0557\n",
            "Epoch 154/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2938.1821 - mse: 65171364.0000 - mae: 2938.1819 - val_loss: 3186.6146 - val_mse: 75519312.0000 - val_mae: 3186.6147\n",
            "Epoch 155/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2879.6286 - mse: 63578856.0000 - mae: 2879.6284 - val_loss: 3191.5278 - val_mse: 74655904.0000 - val_mae: 3191.5278\n",
            "Epoch 156/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2883.9187 - mse: 63763004.0000 - mae: 2883.9189 - val_loss: 3201.0213 - val_mse: 73436600.0000 - val_mae: 3201.0215\n",
            "Epoch 157/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 2871.6359 - mse: 61933056.0000 - mae: 2871.6360 - val_loss: 3144.9251 - val_mse: 71887272.0000 - val_mae: 3144.9253\n",
            "Epoch 158/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 2813.6041 - mse: 60968316.0000 - mae: 2813.6040 - val_loss: 3092.3432 - val_mse: 70329728.0000 - val_mae: 3092.3428\n",
            "Epoch 159/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2802.2177 - mse: 59636296.0000 - mae: 2802.2175 - val_loss: 3098.7153 - val_mse: 69224456.0000 - val_mae: 3098.7151\n",
            "Epoch 160/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 2785.1570 - mse: 59394132.0000 - mae: 2785.1570 - val_loss: 3042.1154 - val_mse: 67650896.0000 - val_mae: 3042.1152\n",
            "Epoch 161/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2742.4008 - mse: 56767792.0000 - mae: 2742.4006 - val_loss: 2997.2658 - val_mse: 66094620.0000 - val_mae: 2997.2656\n",
            "Epoch 162/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 2752.2550 - mse: 57132036.0000 - mae: 2752.2549 - val_loss: 3020.3557 - val_mse: 65442604.0000 - val_mae: 3020.3555\n",
            "Epoch 163/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2672.7618 - mse: 52937484.0000 - mae: 2672.7617 - val_loss: 3001.0132 - val_mse: 64314036.0000 - val_mae: 3001.0134\n",
            "Epoch 164/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2706.1783 - mse: 54728060.0000 - mae: 2706.1785 - val_loss: 2948.0798 - val_mse: 62879456.0000 - val_mae: 2948.0796\n",
            "Epoch 165/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2704.3256 - mse: 53823772.0000 - mae: 2704.3254 - val_loss: 2936.1744 - val_mse: 62037884.0000 - val_mae: 2936.1743\n",
            "Epoch 166/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2678.7656 - mse: 53935864.0000 - mae: 2678.7656 - val_loss: 2900.3946 - val_mse: 60837348.0000 - val_mae: 2900.3948\n",
            "Epoch 167/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2633.1653 - mse: 52392996.0000 - mae: 2633.1653 - val_loss: 2897.7381 - val_mse: 60033120.0000 - val_mae: 2897.7380\n",
            "Epoch 168/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2600.3300 - mse: 50809848.0000 - mae: 2600.3301 - val_loss: 2881.1365 - val_mse: 59265448.0000 - val_mae: 2881.1365\n",
            "Epoch 169/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 2612.5312 - mse: 50155200.0000 - mae: 2612.5312 - val_loss: 2863.3985 - val_mse: 58136148.0000 - val_mae: 2863.3984\n",
            "Epoch 170/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2555.6196 - mse: 48013940.0000 - mae: 2555.6196 - val_loss: 2805.1928 - val_mse: 56803484.0000 - val_mae: 2805.1929\n",
            "Epoch 171/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2545.8803 - mse: 48237176.0000 - mae: 2545.8801 - val_loss: 2818.8921 - val_mse: 56274508.0000 - val_mae: 2818.8921\n",
            "Epoch 172/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2585.4091 - mse: 47990868.0000 - mae: 2585.4094 - val_loss: 2789.9105 - val_mse: 55185392.0000 - val_mae: 2789.9106\n",
            "Epoch 173/500\n",
            "2250/2250 [==============================] - 0s 77us/step - loss: 2521.9129 - mse: 45810304.0000 - mae: 2521.9128 - val_loss: 2809.3125 - val_mse: 54736356.0000 - val_mae: 2809.3125\n",
            "Epoch 174/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2583.7153 - mse: 49684788.0000 - mae: 2583.7153 - val_loss: 2760.6894 - val_mse: 53755372.0000 - val_mae: 2760.6897\n",
            "Epoch 175/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2536.6858 - mse: 45704068.0000 - mae: 2536.6858 - val_loss: 2730.9659 - val_mse: 52939812.0000 - val_mae: 2730.9656\n",
            "Epoch 176/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 2482.6005 - mse: 44767692.0000 - mae: 2482.6006 - val_loss: 2706.4068 - val_mse: 52122152.0000 - val_mae: 2706.4067\n",
            "Epoch 177/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2418.8184 - mse: 40972008.0000 - mae: 2418.8184 - val_loss: 2707.1678 - val_mse: 51772364.0000 - val_mae: 2707.1677\n",
            "Epoch 178/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2471.4090 - mse: 44775240.0000 - mae: 2471.4089 - val_loss: 2701.5997 - val_mse: 51302836.0000 - val_mae: 2701.6001\n",
            "Epoch 179/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2520.1693 - mse: 45463892.0000 - mae: 2520.1694 - val_loss: 2693.2099 - val_mse: 50722272.0000 - val_mae: 2693.2097\n",
            "Epoch 180/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 2468.8184 - mse: 44836448.0000 - mae: 2468.8184 - val_loss: 2641.5534 - val_mse: 49904020.0000 - val_mae: 2641.5532\n",
            "Epoch 181/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2517.5664 - mse: 44799496.0000 - mae: 2517.5664 - val_loss: 2653.7429 - val_mse: 49550332.0000 - val_mae: 2653.7429\n",
            "Epoch 182/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2440.2862 - mse: 42424792.0000 - mae: 2440.2861 - val_loss: 2599.0963 - val_mse: 48549804.0000 - val_mae: 2599.0964\n",
            "Epoch 183/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 2424.0380 - mse: 41695564.0000 - mae: 2424.0378 - val_loss: 2606.8420 - val_mse: 48256460.0000 - val_mae: 2606.8416\n",
            "Epoch 184/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2465.1545 - mse: 43201912.0000 - mae: 2465.1545 - val_loss: 2564.5439 - val_mse: 47327852.0000 - val_mae: 2564.5439\n",
            "Epoch 185/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2362.1794 - mse: 40670640.0000 - mae: 2362.1794 - val_loss: 2614.6199 - val_mse: 47504664.0000 - val_mae: 2614.6199\n",
            "Epoch 186/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2367.2896 - mse: 37504660.0000 - mae: 2367.2893 - val_loss: 2572.8729 - val_mse: 46848076.0000 - val_mae: 2572.8730\n",
            "Epoch 187/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 2365.3282 - mse: 40211084.0000 - mae: 2365.3281 - val_loss: 2532.6289 - val_mse: 46130324.0000 - val_mae: 2532.6289\n",
            "Epoch 188/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2427.4553 - mse: 41079692.0000 - mae: 2427.4556 - val_loss: 2536.0296 - val_mse: 45891976.0000 - val_mae: 2536.0298\n",
            "Epoch 189/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2345.1644 - mse: 40306556.0000 - mae: 2345.1646 - val_loss: 2510.6221 - val_mse: 45212544.0000 - val_mae: 2510.6223\n",
            "Epoch 190/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2375.1833 - mse: 39112504.0000 - mae: 2375.1833 - val_loss: 2513.8703 - val_mse: 45040752.0000 - val_mae: 2513.8706\n",
            "Epoch 191/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 2368.6730 - mse: 39608948.0000 - mae: 2368.6729 - val_loss: 2498.2246 - val_mse: 44620312.0000 - val_mae: 2498.2246\n",
            "Epoch 192/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2348.5900 - mse: 37154268.0000 - mae: 2348.5898 - val_loss: 2514.4337 - val_mse: 44450776.0000 - val_mae: 2514.4341\n",
            "Epoch 193/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2359.2707 - mse: 37762160.0000 - mae: 2359.2710 - val_loss: 2491.8843 - val_mse: 43976340.0000 - val_mae: 2491.8840\n",
            "Epoch 194/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2385.1055 - mse: 39554804.0000 - mae: 2385.1052 - val_loss: 2463.5511 - val_mse: 43250100.0000 - val_mae: 2463.5510\n",
            "Epoch 195/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2324.2298 - mse: 38873576.0000 - mae: 2324.2297 - val_loss: 2464.2828 - val_mse: 43112972.0000 - val_mae: 2464.2830\n",
            "Epoch 196/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2348.4584 - mse: 37559660.0000 - mae: 2348.4583 - val_loss: 2460.9668 - val_mse: 42788540.0000 - val_mae: 2460.9668\n",
            "Epoch 197/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2339.6299 - mse: 39544200.0000 - mae: 2339.6299 - val_loss: 2444.1204 - val_mse: 42599816.0000 - val_mae: 2444.1204\n",
            "Epoch 198/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2358.4780 - mse: 38339412.0000 - mae: 2358.4780 - val_loss: 2438.0741 - val_mse: 42308284.0000 - val_mae: 2438.0740\n",
            "Epoch 199/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 2375.8424 - mse: 37578276.0000 - mae: 2375.8425 - val_loss: 2436.5130 - val_mse: 41977672.0000 - val_mae: 2436.5129\n",
            "Epoch 200/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2330.9478 - mse: 37876080.0000 - mae: 2330.9475 - val_loss: 2418.0330 - val_mse: 41496364.0000 - val_mae: 2418.0334\n",
            "Epoch 201/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 2320.7441 - mse: 38361432.0000 - mae: 2320.7439 - val_loss: 2426.0403 - val_mse: 41654448.0000 - val_mae: 2426.0405\n",
            "Epoch 202/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2331.7839 - mse: 40771716.0000 - mae: 2331.7839 - val_loss: 2417.6907 - val_mse: 41344680.0000 - val_mae: 2417.6904\n",
            "Epoch 203/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2297.0777 - mse: 35387388.0000 - mae: 2297.0776 - val_loss: 2403.9945 - val_mse: 41055824.0000 - val_mae: 2403.9946\n",
            "Epoch 204/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 2255.7049 - mse: 34599172.0000 - mae: 2255.7048 - val_loss: 2377.9026 - val_mse: 40432212.0000 - val_mae: 2377.9026\n",
            "Epoch 205/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2332.0668 - mse: 37720936.0000 - mae: 2332.0671 - val_loss: 2389.9409 - val_mse: 40484280.0000 - val_mae: 2389.9409\n",
            "Epoch 206/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2270.9116 - mse: 34989108.0000 - mae: 2270.9116 - val_loss: 2376.6326 - val_mse: 40284652.0000 - val_mae: 2376.6326\n",
            "Epoch 207/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 2278.5237 - mse: 34188796.0000 - mae: 2278.5237 - val_loss: 2375.4198 - val_mse: 40012028.0000 - val_mae: 2375.4199\n",
            "Epoch 208/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2173.2126 - mse: 31887510.0000 - mae: 2173.2126 - val_loss: 2366.7981 - val_mse: 39675812.0000 - val_mae: 2366.7983\n",
            "Epoch 209/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 2237.5714 - mse: 33236808.0000 - mae: 2237.5713 - val_loss: 2347.7754 - val_mse: 39354204.0000 - val_mae: 2347.7754\n",
            "Epoch 210/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2342.6940 - mse: 40247572.0000 - mae: 2342.6941 - val_loss: 2337.0043 - val_mse: 39034924.0000 - val_mae: 2337.0044\n",
            "Epoch 211/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2309.7305 - mse: 35347816.0000 - mae: 2309.7307 - val_loss: 2346.5541 - val_mse: 39147168.0000 - val_mae: 2346.5542\n",
            "Epoch 212/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2251.4394 - mse: 34216124.0000 - mae: 2251.4395 - val_loss: 2374.7473 - val_mse: 39390456.0000 - val_mae: 2374.7473\n",
            "Epoch 213/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2277.8297 - mse: 34840144.0000 - mae: 2277.8296 - val_loss: 2343.3135 - val_mse: 38905936.0000 - val_mae: 2343.3135\n",
            "Epoch 214/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2191.9412 - mse: 33029204.0000 - mae: 2191.9412 - val_loss: 2342.9188 - val_mse: 38728760.0000 - val_mae: 2342.9187\n",
            "Epoch 215/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2192.3535 - mse: 31726182.0000 - mae: 2192.3535 - val_loss: 2334.5668 - val_mse: 38463568.0000 - val_mae: 2334.5669\n",
            "Epoch 216/500\n",
            "2250/2250 [==============================] - 0s 62us/step - loss: 2275.2499 - mse: 35593780.0000 - mae: 2275.2498 - val_loss: 2330.3236 - val_mse: 38362748.0000 - val_mae: 2330.3235\n",
            "Epoch 217/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2214.8662 - mse: 33470874.0000 - mae: 2214.8662 - val_loss: 2325.0569 - val_mse: 38228280.0000 - val_mae: 2325.0566\n",
            "Epoch 218/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2304.0313 - mse: 37564192.0000 - mae: 2304.0312 - val_loss: 2328.0559 - val_mse: 38155352.0000 - val_mae: 2328.0557\n",
            "Epoch 219/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2230.7366 - mse: 34487568.0000 - mae: 2230.7366 - val_loss: 2331.1430 - val_mse: 37972892.0000 - val_mae: 2331.1428\n",
            "Epoch 220/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2180.5615 - mse: 34433396.0000 - mae: 2180.5615 - val_loss: 2305.9183 - val_mse: 37564952.0000 - val_mae: 2305.9182\n",
            "Epoch 221/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2219.2667 - mse: 32565668.0000 - mae: 2219.2666 - val_loss: 2304.2820 - val_mse: 37558852.0000 - val_mae: 2304.2817\n",
            "Epoch 222/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2212.8723 - mse: 32175788.0000 - mae: 2212.8726 - val_loss: 2332.6727 - val_mse: 37618640.0000 - val_mae: 2332.6726\n",
            "Epoch 223/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2259.4932 - mse: 34231152.0000 - mae: 2259.4934 - val_loss: 2346.4502 - val_mse: 37652312.0000 - val_mae: 2346.4504\n",
            "Epoch 224/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 2134.1112 - mse: 29998846.0000 - mae: 2134.1111 - val_loss: 2292.2768 - val_mse: 36965856.0000 - val_mae: 2292.2766\n",
            "Epoch 225/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2277.8719 - mse: 34710780.0000 - mae: 2277.8721 - val_loss: 2319.6491 - val_mse: 37113476.0000 - val_mae: 2319.6492\n",
            "Epoch 226/500\n",
            "2250/2250 [==============================] - 0s 63us/step - loss: 2214.9476 - mse: 31498088.0000 - mae: 2214.9475 - val_loss: 2293.5065 - val_mse: 36737612.0000 - val_mae: 2293.5063\n",
            "Epoch 227/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2136.0329 - mse: 30809290.0000 - mae: 2136.0330 - val_loss: 2295.2181 - val_mse: 36659944.0000 - val_mae: 2295.2178\n",
            "Epoch 228/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2184.2183 - mse: 32653708.0000 - mae: 2184.2183 - val_loss: 2328.8469 - val_mse: 36843020.0000 - val_mae: 2328.8469\n",
            "Epoch 229/500\n",
            "2250/2250 [==============================] - 0s 61us/step - loss: 2170.1110 - mse: 31624062.0000 - mae: 2170.1111 - val_loss: 2282.1539 - val_mse: 36389148.0000 - val_mae: 2282.1538\n",
            "Epoch 230/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2171.2286 - mse: 32375742.0000 - mae: 2171.2285 - val_loss: 2301.5610 - val_mse: 36413420.0000 - val_mae: 2301.5610\n",
            "Epoch 231/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2141.7611 - mse: 30279850.0000 - mae: 2141.7612 - val_loss: 2274.5713 - val_mse: 36025372.0000 - val_mae: 2274.5708\n",
            "Epoch 232/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 2197.9292 - mse: 33556916.0000 - mae: 2197.9292 - val_loss: 2283.9654 - val_mse: 36030288.0000 - val_mae: 2283.9653\n",
            "Epoch 233/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2122.2611 - mse: 30932552.0000 - mae: 2122.2612 - val_loss: 2275.8208 - val_mse: 35853120.0000 - val_mae: 2275.8210\n",
            "Epoch 234/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2159.9068 - mse: 30870100.0000 - mae: 2159.9067 - val_loss: 2257.0729 - val_mse: 35457672.0000 - val_mae: 2257.0730\n",
            "Epoch 235/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2084.2859 - mse: 27351700.0000 - mae: 2084.2859 - val_loss: 2272.3996 - val_mse: 35572712.0000 - val_mae: 2272.3997\n",
            "Epoch 236/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2226.7973 - mse: 33563768.0000 - mae: 2226.7974 - val_loss: 2261.6250 - val_mse: 35288592.0000 - val_mae: 2261.6250\n",
            "Epoch 237/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2173.0512 - mse: 32324172.0000 - mae: 2173.0510 - val_loss: 2298.8805 - val_mse: 35439300.0000 - val_mae: 2298.8806\n",
            "Epoch 238/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2143.5554 - mse: 31128060.0000 - mae: 2143.5554 - val_loss: 2286.3513 - val_mse: 35328796.0000 - val_mae: 2286.3516\n",
            "Epoch 239/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2136.9002 - mse: 30759132.0000 - mae: 2136.9004 - val_loss: 2270.8905 - val_mse: 35168284.0000 - val_mae: 2270.8904\n",
            "Epoch 240/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2176.8507 - mse: 31986046.0000 - mae: 2176.8508 - val_loss: 2271.6057 - val_mse: 35209356.0000 - val_mae: 2271.6057\n",
            "Epoch 241/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2136.4655 - mse: 30076544.0000 - mae: 2136.4656 - val_loss: 2252.8052 - val_mse: 34825468.0000 - val_mae: 2252.8054\n",
            "Epoch 242/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2093.3703 - mse: 30002500.0000 - mae: 2093.3701 - val_loss: 2257.9124 - val_mse: 34744204.0000 - val_mae: 2257.9124\n",
            "Epoch 243/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2192.7150 - mse: 31406476.0000 - mae: 2192.7151 - val_loss: 2252.5237 - val_mse: 34672064.0000 - val_mae: 2252.5234\n",
            "Epoch 244/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2156.9644 - mse: 31199608.0000 - mae: 2156.9644 - val_loss: 2281.2602 - val_mse: 35014796.0000 - val_mae: 2281.2603\n",
            "Epoch 245/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2106.7262 - mse: 29054784.0000 - mae: 2106.7263 - val_loss: 2257.1156 - val_mse: 34693468.0000 - val_mae: 2257.1155\n",
            "Epoch 246/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2140.2657 - mse: 30694422.0000 - mae: 2140.2659 - val_loss: 2257.6078 - val_mse: 34922128.0000 - val_mae: 2257.6079\n",
            "Epoch 247/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 2151.5179 - mse: 29656700.0000 - mae: 2151.5181 - val_loss: 2262.5463 - val_mse: 34870756.0000 - val_mae: 2262.5464\n",
            "Epoch 248/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2087.8379 - mse: 31024612.0000 - mae: 2087.8379 - val_loss: 2252.1622 - val_mse: 34763688.0000 - val_mae: 2252.1621\n",
            "Epoch 249/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2163.1847 - mse: 30886814.0000 - mae: 2163.1846 - val_loss: 2251.4057 - val_mse: 34834776.0000 - val_mae: 2251.4055\n",
            "Epoch 250/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2121.6996 - mse: 30458292.0000 - mae: 2121.6997 - val_loss: 2262.4684 - val_mse: 34794916.0000 - val_mae: 2262.4685\n",
            "Epoch 251/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2064.4597 - mse: 29248716.0000 - mae: 2064.4595 - val_loss: 2243.7517 - val_mse: 34456640.0000 - val_mae: 2243.7520\n",
            "Epoch 252/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2076.0889 - mse: 27324854.0000 - mae: 2076.0891 - val_loss: 2237.0102 - val_mse: 34389300.0000 - val_mae: 2237.0100\n",
            "Epoch 253/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 2107.4604 - mse: 30535414.0000 - mae: 2107.4604 - val_loss: 2249.8693 - val_mse: 34389384.0000 - val_mae: 2249.8691\n",
            "Epoch 254/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2102.1734 - mse: 29692606.0000 - mae: 2102.1733 - val_loss: 2264.6822 - val_mse: 34633208.0000 - val_mae: 2264.6821\n",
            "Epoch 255/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2130.6074 - mse: 30222152.0000 - mae: 2130.6074 - val_loss: 2250.4401 - val_mse: 34333844.0000 - val_mae: 2250.4399\n",
            "Epoch 256/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2048.5978 - mse: 26732032.0000 - mae: 2048.5979 - val_loss: 2228.2414 - val_mse: 34043668.0000 - val_mae: 2228.2415\n",
            "Epoch 257/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2153.0345 - mse: 29762118.0000 - mae: 2153.0344 - val_loss: 2239.7750 - val_mse: 34143764.0000 - val_mae: 2239.7749\n",
            "Epoch 258/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2189.0570 - mse: 32489708.0000 - mae: 2189.0569 - val_loss: 2252.5511 - val_mse: 34133444.0000 - val_mae: 2252.5510\n",
            "Epoch 259/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2071.8094 - mse: 28238796.0000 - mae: 2071.8093 - val_loss: 2230.3946 - val_mse: 33835016.0000 - val_mae: 2230.3945\n",
            "Epoch 260/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2106.7527 - mse: 29352922.0000 - mae: 2106.7529 - val_loss: 2260.9889 - val_mse: 34212140.0000 - val_mae: 2260.9888\n",
            "Epoch 261/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2153.1983 - mse: 30980706.0000 - mae: 2153.1982 - val_loss: 2262.1316 - val_mse: 34135888.0000 - val_mae: 2262.1318\n",
            "Epoch 262/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2066.8821 - mse: 29496252.0000 - mae: 2066.8823 - val_loss: 2248.5615 - val_mse: 33762860.0000 - val_mae: 2248.5615\n",
            "Epoch 263/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2149.9716 - mse: 30964108.0000 - mae: 2149.9714 - val_loss: 2211.4459 - val_mse: 33418990.0000 - val_mae: 2211.4460\n",
            "Epoch 264/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2051.2551 - mse: 28010978.0000 - mae: 2051.2551 - val_loss: 2264.3795 - val_mse: 33912460.0000 - val_mae: 2264.3796\n",
            "Epoch 265/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2094.7688 - mse: 29897720.0000 - mae: 2094.7686 - val_loss: 2221.5750 - val_mse: 33381118.0000 - val_mae: 2221.5750\n",
            "Epoch 266/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2103.3594 - mse: 29238566.0000 - mae: 2103.3594 - val_loss: 2226.4045 - val_mse: 33519162.0000 - val_mae: 2226.4045\n",
            "Epoch 267/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2101.8235 - mse: 29830860.0000 - mae: 2101.8235 - val_loss: 2252.1605 - val_mse: 33751188.0000 - val_mae: 2252.1604\n",
            "Epoch 268/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2046.4281 - mse: 28865632.0000 - mae: 2046.4280 - val_loss: 2256.7811 - val_mse: 33865436.0000 - val_mae: 2256.7810\n",
            "Epoch 269/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2075.2984 - mse: 27581908.0000 - mae: 2075.2983 - val_loss: 2242.2074 - val_mse: 33585696.0000 - val_mae: 2242.2073\n",
            "Epoch 270/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2178.9430 - mse: 33688408.0000 - mae: 2178.9431 - val_loss: 2248.0988 - val_mse: 33465562.0000 - val_mae: 2248.0986\n",
            "Epoch 271/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2109.5261 - mse: 31572598.0000 - mae: 2109.5259 - val_loss: 2228.7205 - val_mse: 33232664.0000 - val_mae: 2228.7207\n",
            "Epoch 272/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2104.9545 - mse: 28807608.0000 - mae: 2104.9546 - val_loss: 2231.9404 - val_mse: 33160648.0000 - val_mae: 2231.9404\n",
            "Epoch 273/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 2026.9044 - mse: 25641956.0000 - mae: 2026.9042 - val_loss: 2217.2800 - val_mse: 32756454.0000 - val_mae: 2217.2800\n",
            "Epoch 274/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2074.0335 - mse: 28848288.0000 - mae: 2074.0334 - val_loss: 2232.7306 - val_mse: 33036334.0000 - val_mae: 2232.7307\n",
            "Epoch 275/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2107.2845 - mse: 30679888.0000 - mae: 2107.2847 - val_loss: 2215.5290 - val_mse: 32770858.0000 - val_mae: 2215.5293\n",
            "Epoch 276/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2121.4626 - mse: 30673896.0000 - mae: 2121.4626 - val_loss: 2244.9525 - val_mse: 33194134.0000 - val_mae: 2244.9521\n",
            "Epoch 277/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2052.6966 - mse: 27684404.0000 - mae: 2052.6965 - val_loss: 2249.5529 - val_mse: 33386150.0000 - val_mae: 2249.5530\n",
            "Epoch 278/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2116.0349 - mse: 29680250.0000 - mae: 2116.0349 - val_loss: 2239.5571 - val_mse: 33153046.0000 - val_mae: 2239.5569\n",
            "Epoch 279/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2020.5125 - mse: 26299340.0000 - mae: 2020.5125 - val_loss: 2273.1651 - val_mse: 33465482.0000 - val_mae: 2273.1650\n",
            "Epoch 280/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2067.0492 - mse: 29333592.0000 - mae: 2067.0493 - val_loss: 2246.4189 - val_mse: 33231962.0000 - val_mae: 2246.4187\n",
            "Epoch 281/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2098.8274 - mse: 30501072.0000 - mae: 2098.8274 - val_loss: 2224.9236 - val_mse: 32814404.0000 - val_mae: 2224.9236\n",
            "Epoch 282/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2035.8432 - mse: 26885828.0000 - mae: 2035.8431 - val_loss: 2238.1940 - val_mse: 33008546.0000 - val_mae: 2238.1941\n",
            "Epoch 283/500\n",
            "2250/2250 [==============================] - 0s 94us/step - loss: 2115.2004 - mse: 30479692.0000 - mae: 2115.2007 - val_loss: 2270.6688 - val_mse: 33141960.0000 - val_mae: 2270.6685\n",
            "Epoch 284/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2126.0185 - mse: 31543628.0000 - mae: 2126.0186 - val_loss: 2266.1498 - val_mse: 33240336.0000 - val_mae: 2266.1497\n",
            "Epoch 285/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2044.2133 - mse: 27764514.0000 - mae: 2044.2134 - val_loss: 2217.2752 - val_mse: 32833314.0000 - val_mae: 2217.2754\n",
            "Epoch 286/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 2118.6915 - mse: 30903810.0000 - mae: 2118.6914 - val_loss: 2231.2544 - val_mse: 32842690.0000 - val_mae: 2231.2542\n",
            "Epoch 287/500\n",
            "2250/2250 [==============================] - 0s 84us/step - loss: 2019.4381 - mse: 26835736.0000 - mae: 2019.4380 - val_loss: 2249.8920 - val_mse: 32942500.0000 - val_mae: 2249.8921\n",
            "Epoch 288/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 2066.4539 - mse: 30424302.0000 - mae: 2066.4541 - val_loss: 2227.7290 - val_mse: 32793556.0000 - val_mae: 2227.7290\n",
            "Epoch 289/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 2073.9053 - mse: 28683014.0000 - mae: 2073.9053 - val_loss: 2225.6432 - val_mse: 32605146.0000 - val_mae: 2225.6433\n",
            "Epoch 290/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 2086.1926 - mse: 30841004.0000 - mae: 2086.1924 - val_loss: 2227.2774 - val_mse: 32658300.0000 - val_mae: 2227.2773\n",
            "Epoch 291/500\n",
            "2250/2250 [==============================] - 0s 84us/step - loss: 2023.6679 - mse: 28696670.0000 - mae: 2023.6680 - val_loss: 2206.3909 - val_mse: 32408280.0000 - val_mae: 2206.3911\n",
            "Epoch 292/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 2011.2994 - mse: 26821748.0000 - mae: 2011.2993 - val_loss: 2212.3629 - val_mse: 32377206.0000 - val_mae: 2212.3625\n",
            "Epoch 293/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2101.1505 - mse: 29263740.0000 - mae: 2101.1506 - val_loss: 2239.9487 - val_mse: 32611674.0000 - val_mae: 2239.9490\n",
            "Epoch 294/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 2032.5167 - mse: 27496786.0000 - mae: 2032.5167 - val_loss: 2233.7018 - val_mse: 32602514.0000 - val_mae: 2233.7019\n",
            "Epoch 295/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1996.5301 - mse: 26888568.0000 - mae: 1996.5300 - val_loss: 2203.0828 - val_mse: 32378214.0000 - val_mae: 2203.0830\n",
            "Epoch 296/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 2060.0169 - mse: 30725810.0000 - mae: 2060.0168 - val_loss: 2229.3613 - val_mse: 32645746.0000 - val_mae: 2229.3616\n",
            "Epoch 297/500\n",
            "2250/2250 [==============================] - 0s 77us/step - loss: 1985.5855 - mse: 26430466.0000 - mae: 1985.5853 - val_loss: 2246.6109 - val_mse: 32572750.0000 - val_mae: 2246.6108\n",
            "Epoch 298/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 2053.3465 - mse: 27497836.0000 - mae: 2053.3467 - val_loss: 2211.8013 - val_mse: 32279830.0000 - val_mae: 2211.8010\n",
            "Epoch 299/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2014.1507 - mse: 25897532.0000 - mae: 2014.1506 - val_loss: 2213.4150 - val_mse: 32228428.0000 - val_mae: 2213.4150\n",
            "Epoch 300/500\n",
            "2250/2250 [==============================] - 0s 77us/step - loss: 2067.7514 - mse: 29065914.0000 - mae: 2067.7515 - val_loss: 2239.2163 - val_mse: 32436336.0000 - val_mae: 2239.2161\n",
            "Epoch 301/500\n",
            "2250/2250 [==============================] - 0s 91us/step - loss: 2062.4542 - mse: 29483758.0000 - mae: 2062.4541 - val_loss: 2220.5643 - val_mse: 32297520.0000 - val_mae: 2220.5645\n",
            "Epoch 302/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2079.1355 - mse: 29275622.0000 - mae: 2079.1355 - val_loss: 2211.9291 - val_mse: 32177542.0000 - val_mae: 2211.9292\n",
            "Epoch 303/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2017.8472 - mse: 27324610.0000 - mae: 2017.8472 - val_loss: 2211.2433 - val_mse: 32097734.0000 - val_mae: 2211.2434\n",
            "Epoch 304/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 1989.4410 - mse: 25953102.0000 - mae: 1989.4412 - val_loss: 2217.8279 - val_mse: 32191346.0000 - val_mae: 2217.8279\n",
            "Epoch 305/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2069.7736 - mse: 27971562.0000 - mae: 2069.7734 - val_loss: 2213.8281 - val_mse: 32177964.0000 - val_mae: 2213.8281\n",
            "Epoch 306/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 2012.0313 - mse: 27056292.0000 - mae: 2012.0314 - val_loss: 2216.4585 - val_mse: 32264232.0000 - val_mae: 2216.4585\n",
            "Epoch 307/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2006.9926 - mse: 26728216.0000 - mae: 2006.9924 - val_loss: 2233.5137 - val_mse: 32372440.0000 - val_mae: 2233.5137\n",
            "Epoch 308/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 1946.4559 - mse: 24670546.0000 - mae: 1946.4561 - val_loss: 2227.5754 - val_mse: 32303860.0000 - val_mae: 2227.5754\n",
            "Epoch 309/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 2035.3115 - mse: 27910382.0000 - mae: 2035.3115 - val_loss: 2238.5779 - val_mse: 32471658.0000 - val_mae: 2238.5779\n",
            "Epoch 310/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2007.0407 - mse: 26668802.0000 - mae: 2007.0409 - val_loss: 2246.2723 - val_mse: 32612086.0000 - val_mae: 2246.2720\n",
            "Epoch 311/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 2009.4357 - mse: 27817136.0000 - mae: 2009.4355 - val_loss: 2217.5399 - val_mse: 32087504.0000 - val_mae: 2217.5398\n",
            "Epoch 312/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 2078.0346 - mse: 28515542.0000 - mae: 2078.0347 - val_loss: 2216.4671 - val_mse: 32229528.0000 - val_mae: 2216.4668\n",
            "Epoch 313/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1959.8468 - mse: 24991442.0000 - mae: 1959.8469 - val_loss: 2225.7318 - val_mse: 32322330.0000 - val_mae: 2225.7314\n",
            "Epoch 314/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2087.8232 - mse: 30887554.0000 - mae: 2087.8230 - val_loss: 2205.0127 - val_mse: 31848614.0000 - val_mae: 2205.0125\n",
            "Epoch 315/500\n",
            "2250/2250 [==============================] - 0s 81us/step - loss: 2001.1326 - mse: 27469364.0000 - mae: 2001.1324 - val_loss: 2224.9739 - val_mse: 32050516.0000 - val_mae: 2224.9741\n",
            "Epoch 316/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1979.1432 - mse: 27385418.0000 - mae: 1979.1431 - val_loss: 2204.8164 - val_mse: 31656178.0000 - val_mae: 2204.8164\n",
            "Epoch 317/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 1976.1222 - mse: 26402142.0000 - mae: 1976.1219 - val_loss: 2231.1026 - val_mse: 32025630.0000 - val_mae: 2231.1028\n",
            "Epoch 318/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1951.8502 - mse: 24737546.0000 - mae: 1951.8502 - val_loss: 2258.9237 - val_mse: 32328308.0000 - val_mae: 2258.9236\n",
            "Epoch 319/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 2015.3190 - mse: 26862620.0000 - mae: 2015.3188 - val_loss: 2225.6972 - val_mse: 31920706.0000 - val_mae: 2225.6970\n",
            "Epoch 320/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2031.8936 - mse: 29124798.0000 - mae: 2031.8938 - val_loss: 2240.7334 - val_mse: 32082154.0000 - val_mae: 2240.7332\n",
            "Epoch 321/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1993.1455 - mse: 26360852.0000 - mae: 1993.1454 - val_loss: 2234.5339 - val_mse: 32254878.0000 - val_mae: 2234.5344\n",
            "Epoch 322/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2104.0286 - mse: 30260764.0000 - mae: 2104.0286 - val_loss: 2232.9122 - val_mse: 32254248.0000 - val_mae: 2232.9121\n",
            "Epoch 323/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2003.2973 - mse: 26813394.0000 - mae: 2003.2971 - val_loss: 2233.6233 - val_mse: 32043732.0000 - val_mae: 2233.6233\n",
            "Epoch 324/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 2003.2425 - mse: 27925296.0000 - mae: 2003.2424 - val_loss: 2227.7849 - val_mse: 32014248.0000 - val_mae: 2227.7847\n",
            "Epoch 325/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1989.9006 - mse: 26418336.0000 - mae: 1989.9006 - val_loss: 2208.0510 - val_mse: 31919870.0000 - val_mae: 2208.0508\n",
            "Epoch 326/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 2007.9283 - mse: 26126548.0000 - mae: 2007.9282 - val_loss: 2235.4904 - val_mse: 32258538.0000 - val_mae: 2235.4905\n",
            "Epoch 327/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 1998.3120 - mse: 26107848.0000 - mae: 1998.3120 - val_loss: 2205.9982 - val_mse: 31898848.0000 - val_mae: 2205.9985\n",
            "Epoch 328/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1956.3713 - mse: 25915654.0000 - mae: 1956.3713 - val_loss: 2226.7212 - val_mse: 31921832.0000 - val_mae: 2226.7214\n",
            "Epoch 329/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1954.2205 - mse: 25821498.0000 - mae: 1954.2205 - val_loss: 2220.1875 - val_mse: 31982904.0000 - val_mae: 2220.1875\n",
            "Epoch 330/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1993.6735 - mse: 27244300.0000 - mae: 1993.6736 - val_loss: 2241.2910 - val_mse: 32174050.0000 - val_mae: 2241.2908\n",
            "Epoch 331/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 1994.1935 - mse: 27991368.0000 - mae: 1994.1936 - val_loss: 2241.8641 - val_mse: 32180112.0000 - val_mae: 2241.8643\n",
            "Epoch 332/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2009.5293 - mse: 27942300.0000 - mae: 2009.5293 - val_loss: 2226.6754 - val_mse: 32055062.0000 - val_mae: 2226.6755\n",
            "Epoch 333/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 2045.9796 - mse: 29911086.0000 - mae: 2045.9796 - val_loss: 2211.1653 - val_mse: 31966324.0000 - val_mae: 2211.1655\n",
            "Epoch 334/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 2033.3949 - mse: 27975670.0000 - mae: 2033.3949 - val_loss: 2200.2993 - val_mse: 31734636.0000 - val_mae: 2200.2988\n",
            "Epoch 335/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 2029.7198 - mse: 28080270.0000 - mae: 2029.7197 - val_loss: 2225.3163 - val_mse: 32005358.0000 - val_mae: 2225.3162\n",
            "Epoch 336/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 2004.2201 - mse: 27293180.0000 - mae: 2004.2202 - val_loss: 2218.3079 - val_mse: 32116074.0000 - val_mae: 2218.3079\n",
            "Epoch 337/500\n",
            "2250/2250 [==============================] - 0s 81us/step - loss: 2040.3054 - mse: 28472040.0000 - mae: 2040.3055 - val_loss: 2226.5953 - val_mse: 32076596.0000 - val_mae: 2226.5952\n",
            "Epoch 338/500\n",
            "2250/2250 [==============================] - 0s 83us/step - loss: 2004.9944 - mse: 27264552.0000 - mae: 2004.9943 - val_loss: 2198.0388 - val_mse: 31787486.0000 - val_mae: 2198.0388\n",
            "Epoch 339/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 1901.3567 - mse: 24278354.0000 - mae: 1901.3567 - val_loss: 2223.9040 - val_mse: 31987768.0000 - val_mae: 2223.9041\n",
            "Epoch 340/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2017.5721 - mse: 27981720.0000 - mae: 2017.5720 - val_loss: 2211.4805 - val_mse: 31617646.0000 - val_mae: 2211.4805\n",
            "Epoch 341/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1988.3855 - mse: 26196298.0000 - mae: 1988.3855 - val_loss: 2196.7945 - val_mse: 31558748.0000 - val_mae: 2196.7947\n",
            "Epoch 342/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1963.5833 - mse: 25439598.0000 - mae: 1963.5831 - val_loss: 2239.1286 - val_mse: 32034802.0000 - val_mae: 2239.1284\n",
            "Epoch 343/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2005.7041 - mse: 27395588.0000 - mae: 2005.7040 - val_loss: 2211.1398 - val_mse: 31627994.0000 - val_mae: 2211.1399\n",
            "Epoch 344/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 1973.8695 - mse: 27245848.0000 - mae: 1973.8695 - val_loss: 2198.6823 - val_mse: 31551980.0000 - val_mae: 2198.6819\n",
            "Epoch 345/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1918.5145 - mse: 23310794.0000 - mae: 1918.5144 - val_loss: 2224.2015 - val_mse: 31770630.0000 - val_mae: 2224.2017\n",
            "Epoch 346/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1984.3059 - mse: 24897936.0000 - mae: 1984.3063 - val_loss: 2216.7255 - val_mse: 31710632.0000 - val_mae: 2216.7253\n",
            "Epoch 347/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2054.5621 - mse: 29236488.0000 - mae: 2054.5623 - val_loss: 2230.0997 - val_mse: 31769602.0000 - val_mae: 2230.0999\n",
            "Epoch 348/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 2059.0153 - mse: 30377690.0000 - mae: 2059.0154 - val_loss: 2225.3297 - val_mse: 31852258.0000 - val_mae: 2225.3298\n",
            "Epoch 349/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2004.1361 - mse: 28247668.0000 - mae: 2004.1362 - val_loss: 2205.3343 - val_mse: 31540092.0000 - val_mae: 2205.3345\n",
            "Epoch 350/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2013.9275 - mse: 26136112.0000 - mae: 2013.9276 - val_loss: 2214.0237 - val_mse: 31635188.0000 - val_mae: 2214.0234\n",
            "Epoch 351/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 2005.2678 - mse: 27379068.0000 - mae: 2005.2678 - val_loss: 2227.4819 - val_mse: 31593020.0000 - val_mae: 2227.4817\n",
            "Epoch 352/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1981.9704 - mse: 24880766.0000 - mae: 1981.9705 - val_loss: 2187.9785 - val_mse: 31212616.0000 - val_mae: 2187.9788\n",
            "Epoch 353/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2013.3181 - mse: 26805398.0000 - mae: 2013.3180 - val_loss: 2230.8299 - val_mse: 31637246.0000 - val_mae: 2230.8298\n",
            "Epoch 354/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1911.8575 - mse: 24325990.0000 - mae: 1911.8575 - val_loss: 2223.0570 - val_mse: 31532316.0000 - val_mae: 2223.0571\n",
            "Epoch 355/500\n",
            "2250/2250 [==============================] - 0s 66us/step - loss: 1923.3515 - mse: 24394838.0000 - mae: 1923.3516 - val_loss: 2234.1342 - val_mse: 31731188.0000 - val_mae: 2234.1340\n",
            "Epoch 356/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1994.6489 - mse: 25837676.0000 - mae: 1994.6489 - val_loss: 2206.2761 - val_mse: 31288288.0000 - val_mae: 2206.2759\n",
            "Epoch 357/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1999.0709 - mse: 27715064.0000 - mae: 1999.0709 - val_loss: 2193.0835 - val_mse: 31002234.0000 - val_mae: 2193.0835\n",
            "Epoch 358/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1949.6640 - mse: 24982366.0000 - mae: 1949.6639 - val_loss: 2200.2807 - val_mse: 31103386.0000 - val_mae: 2200.2810\n",
            "Epoch 359/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 1889.3057 - mse: 22065458.0000 - mae: 1889.3055 - val_loss: 2207.5133 - val_mse: 31142132.0000 - val_mae: 2207.5134\n",
            "Epoch 360/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 1979.1765 - mse: 26353694.0000 - mae: 1979.1764 - val_loss: 2204.2758 - val_mse: 31252064.0000 - val_mae: 2204.2756\n",
            "Epoch 361/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 2034.9556 - mse: 28828846.0000 - mae: 2034.9556 - val_loss: 2197.3291 - val_mse: 31308808.0000 - val_mae: 2197.3289\n",
            "Epoch 362/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1951.8462 - mse: 24263192.0000 - mae: 1951.8462 - val_loss: 2218.2763 - val_mse: 31532438.0000 - val_mae: 2218.2764\n",
            "Epoch 363/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 2027.9804 - mse: 29542180.0000 - mae: 2027.9805 - val_loss: 2244.1900 - val_mse: 31795978.0000 - val_mae: 2244.1899\n",
            "Epoch 364/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2044.5309 - mse: 28473314.0000 - mae: 2044.5311 - val_loss: 2202.8749 - val_mse: 31324556.0000 - val_mae: 2202.8750\n",
            "Epoch 365/500\n",
            "2250/2250 [==============================] - 0s 64us/step - loss: 1937.9657 - mse: 26761026.0000 - mae: 1937.9658 - val_loss: 2218.9396 - val_mse: 31456154.0000 - val_mae: 2218.9395\n",
            "Epoch 366/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 1982.7811 - mse: 25767270.0000 - mae: 1982.7811 - val_loss: 2193.9761 - val_mse: 31111218.0000 - val_mae: 2193.9763\n",
            "Epoch 367/500\n",
            "2250/2250 [==============================] - 0s 83us/step - loss: 1974.5776 - mse: 26102928.0000 - mae: 1974.5775 - val_loss: 2205.0422 - val_mse: 31436108.0000 - val_mae: 2205.0417\n",
            "Epoch 368/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1975.4081 - mse: 25653558.0000 - mae: 1975.4082 - val_loss: 2190.4085 - val_mse: 31204752.0000 - val_mae: 2190.4084\n",
            "Epoch 369/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 1978.9958 - mse: 26370416.0000 - mae: 1978.9957 - val_loss: 2193.2192 - val_mse: 31189544.0000 - val_mae: 2193.2190\n",
            "Epoch 370/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 1945.4837 - mse: 25673090.0000 - mae: 1945.4835 - val_loss: 2198.7771 - val_mse: 31235634.0000 - val_mae: 2198.7771\n",
            "Epoch 371/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1927.2757 - mse: 25921592.0000 - mae: 1927.2755 - val_loss: 2192.4106 - val_mse: 31077962.0000 - val_mae: 2192.4106\n",
            "Epoch 372/500\n",
            "2250/2250 [==============================] - 0s 81us/step - loss: 1973.1009 - mse: 26389880.0000 - mae: 1973.1008 - val_loss: 2209.9565 - val_mse: 31281000.0000 - val_mae: 2209.9565\n",
            "Epoch 373/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1902.2431 - mse: 24940274.0000 - mae: 1902.2429 - val_loss: 2207.0303 - val_mse: 31286916.0000 - val_mae: 2207.0303\n",
            "Epoch 374/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 2025.1488 - mse: 27589272.0000 - mae: 2025.1489 - val_loss: 2210.2548 - val_mse: 31207220.0000 - val_mae: 2210.2551\n",
            "Epoch 375/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 2108.9668 - mse: 31675806.0000 - mae: 2108.9666 - val_loss: 2203.3211 - val_mse: 31062888.0000 - val_mae: 2203.3210\n",
            "Epoch 376/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1970.1868 - mse: 26996082.0000 - mae: 1970.1869 - val_loss: 2188.0595 - val_mse: 30922172.0000 - val_mae: 2188.0596\n",
            "Epoch 377/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 1952.4397 - mse: 25720882.0000 - mae: 1952.4398 - val_loss: 2184.4440 - val_mse: 30811408.0000 - val_mae: 2184.4436\n",
            "Epoch 378/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1963.1443 - mse: 26422166.0000 - mae: 1963.1444 - val_loss: 2197.7473 - val_mse: 30893040.0000 - val_mae: 2197.7471\n",
            "Epoch 379/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1909.5431 - mse: 23524760.0000 - mae: 1909.5431 - val_loss: 2197.1989 - val_mse: 30886286.0000 - val_mae: 2197.1990\n",
            "Epoch 380/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2041.1106 - mse: 29242384.0000 - mae: 2041.1107 - val_loss: 2192.3306 - val_mse: 30889076.0000 - val_mae: 2192.3306\n",
            "Epoch 381/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 1950.8115 - mse: 27281106.0000 - mae: 1950.8113 - val_loss: 2194.0289 - val_mse: 30976370.0000 - val_mae: 2194.0288\n",
            "Epoch 382/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 1955.2344 - mse: 25420754.0000 - mae: 1955.2345 - val_loss: 2183.3695 - val_mse: 30791140.0000 - val_mae: 2183.3694\n",
            "Epoch 383/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1981.8247 - mse: 27184078.0000 - mae: 1981.8247 - val_loss: 2187.8200 - val_mse: 30798152.0000 - val_mae: 2187.8201\n",
            "Epoch 384/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1896.8130 - mse: 24093486.0000 - mae: 1896.8131 - val_loss: 2181.2558 - val_mse: 30741754.0000 - val_mae: 2181.2559\n",
            "Epoch 385/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1981.9264 - mse: 26604802.0000 - mae: 1981.9264 - val_loss: 2189.9328 - val_mse: 30816134.0000 - val_mae: 2189.9329\n",
            "Epoch 386/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1961.5283 - mse: 26893812.0000 - mae: 1961.5284 - val_loss: 2197.3676 - val_mse: 30990354.0000 - val_mae: 2197.3677\n",
            "Epoch 387/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1936.0869 - mse: 25525498.0000 - mae: 1936.0869 - val_loss: 2221.1744 - val_mse: 31145062.0000 - val_mae: 2221.1743\n",
            "Epoch 388/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 2006.6725 - mse: 27600092.0000 - mae: 2006.6726 - val_loss: 2207.5998 - val_mse: 31106414.0000 - val_mae: 2207.5999\n",
            "Epoch 389/500\n",
            "2250/2250 [==============================] - 0s 93us/step - loss: 1976.5648 - mse: 25398768.0000 - mae: 1976.5649 - val_loss: 2212.2153 - val_mse: 31192430.0000 - val_mae: 2212.2151\n",
            "Epoch 390/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 2027.1156 - mse: 27698886.0000 - mae: 2027.1156 - val_loss: 2194.8919 - val_mse: 30942808.0000 - val_mae: 2194.8921\n",
            "Epoch 391/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1922.3204 - mse: 23820598.0000 - mae: 1922.3204 - val_loss: 2202.0115 - val_mse: 30878760.0000 - val_mae: 2202.0112\n",
            "Epoch 392/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1926.7319 - mse: 24925278.0000 - mae: 1926.7318 - val_loss: 2193.6668 - val_mse: 30751664.0000 - val_mae: 2193.6670\n",
            "Epoch 393/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 1952.5507 - mse: 27293938.0000 - mae: 1952.5507 - val_loss: 2206.2971 - val_mse: 30784742.0000 - val_mae: 2206.2971\n",
            "Epoch 394/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1974.2406 - mse: 25445396.0000 - mae: 1974.2407 - val_loss: 2172.4798 - val_mse: 30418732.0000 - val_mae: 2172.4797\n",
            "Epoch 395/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1931.2608 - mse: 25881838.0000 - mae: 1931.2609 - val_loss: 2200.3660 - val_mse: 30764780.0000 - val_mae: 2200.3660\n",
            "Epoch 396/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 2043.3632 - mse: 30177696.0000 - mae: 2043.3633 - val_loss: 2191.1296 - val_mse: 30638728.0000 - val_mae: 2191.1296\n",
            "Epoch 397/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1997.2066 - mse: 28013580.0000 - mae: 1997.2067 - val_loss: 2182.9292 - val_mse: 30715882.0000 - val_mae: 2182.9292\n",
            "Epoch 398/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1951.0643 - mse: 27561820.0000 - mae: 1951.0645 - val_loss: 2194.3229 - val_mse: 30883702.0000 - val_mae: 2194.3230\n",
            "Epoch 399/500\n",
            "2250/2250 [==============================] - 0s 65us/step - loss: 1921.2075 - mse: 24978128.0000 - mae: 1921.2073 - val_loss: 2197.7986 - val_mse: 31061130.0000 - val_mae: 2197.7986\n",
            "Epoch 400/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 1934.3828 - mse: 24590972.0000 - mae: 1934.3827 - val_loss: 2184.5238 - val_mse: 30853336.0000 - val_mae: 2184.5234\n",
            "Epoch 401/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 1977.8154 - mse: 27308632.0000 - mae: 1977.8153 - val_loss: 2193.6268 - val_mse: 30933378.0000 - val_mae: 2193.6270\n",
            "Epoch 402/500\n",
            "2250/2250 [==============================] - 0s 67us/step - loss: 1930.2035 - mse: 26005954.0000 - mae: 1930.2036 - val_loss: 2205.8847 - val_mse: 31152544.0000 - val_mae: 2205.8850\n",
            "Epoch 403/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1926.0644 - mse: 25820934.0000 - mae: 1926.0645 - val_loss: 2187.5876 - val_mse: 30825346.0000 - val_mae: 2187.5876\n",
            "Epoch 404/500\n",
            "2250/2250 [==============================] - 0s 84us/step - loss: 1942.5042 - mse: 26423186.0000 - mae: 1942.5043 - val_loss: 2222.6908 - val_mse: 30923248.0000 - val_mae: 2222.6909\n",
            "Epoch 405/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1964.2971 - mse: 26672380.0000 - mae: 1964.2971 - val_loss: 2173.4221 - val_mse: 30557420.0000 - val_mae: 2173.4219\n",
            "Epoch 406/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1954.5211 - mse: 25908448.0000 - mae: 1954.5211 - val_loss: 2189.0128 - val_mse: 30784328.0000 - val_mae: 2189.0127\n",
            "Epoch 407/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1935.0012 - mse: 25161380.0000 - mae: 1935.0011 - val_loss: 2176.4873 - val_mse: 30506816.0000 - val_mae: 2176.4873\n",
            "Epoch 408/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1995.8101 - mse: 27054000.0000 - mae: 1995.8102 - val_loss: 2196.1664 - val_mse: 30675548.0000 - val_mae: 2196.1663\n",
            "Epoch 409/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1995.7820 - mse: 28122478.0000 - mae: 1995.7822 - val_loss: 2177.8294 - val_mse: 30464872.0000 - val_mae: 2177.8291\n",
            "Epoch 410/500\n",
            "2250/2250 [==============================] - 0s 81us/step - loss: 1908.6888 - mse: 24387024.0000 - mae: 1908.6888 - val_loss: 2174.4583 - val_mse: 30453360.0000 - val_mae: 2174.4587\n",
            "Epoch 411/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1945.7137 - mse: 26422736.0000 - mae: 1945.7135 - val_loss: 2179.6228 - val_mse: 30681690.0000 - val_mae: 2179.6226\n",
            "Epoch 412/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1888.3041 - mse: 22838458.0000 - mae: 1888.3042 - val_loss: 2188.2788 - val_mse: 30659900.0000 - val_mae: 2188.2788\n",
            "Epoch 413/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1978.0191 - mse: 28070060.0000 - mae: 1978.0192 - val_loss: 2174.4956 - val_mse: 30522430.0000 - val_mae: 2174.4956\n",
            "Epoch 414/500\n",
            "2250/2250 [==============================] - 0s 68us/step - loss: 1965.1483 - mse: 27457324.0000 - mae: 1965.1482 - val_loss: 2208.5243 - val_mse: 30871924.0000 - val_mae: 2208.5242\n",
            "Epoch 415/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1961.3374 - mse: 26410266.0000 - mae: 1961.3373 - val_loss: 2194.2765 - val_mse: 30762446.0000 - val_mae: 2194.2764\n",
            "Epoch 416/500\n",
            "2250/2250 [==============================] - 0s 77us/step - loss: 1935.8917 - mse: 26008858.0000 - mae: 1935.8917 - val_loss: 2183.8989 - val_mse: 30592776.0000 - val_mae: 2183.8987\n",
            "Epoch 417/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1957.9062 - mse: 27130232.0000 - mae: 1957.9062 - val_loss: 2168.1893 - val_mse: 30504632.0000 - val_mae: 2168.1895\n",
            "Epoch 418/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1962.6489 - mse: 25127878.0000 - mae: 1962.6489 - val_loss: 2175.5330 - val_mse: 30525642.0000 - val_mae: 2175.5332\n",
            "Epoch 419/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 1960.3480 - mse: 27465356.0000 - mae: 1960.3480 - val_loss: 2197.0939 - val_mse: 30682492.0000 - val_mae: 2197.0938\n",
            "Epoch 420/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1955.0331 - mse: 26652728.0000 - mae: 1955.0333 - val_loss: 2199.3024 - val_mse: 30651046.0000 - val_mae: 2199.3025\n",
            "Epoch 421/500\n",
            "2250/2250 [==============================] - 0s 86us/step - loss: 1894.1605 - mse: 23681206.0000 - mae: 1894.1603 - val_loss: 2170.8955 - val_mse: 30294968.0000 - val_mae: 2170.8955\n",
            "Epoch 422/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 1926.2375 - mse: 24661418.0000 - mae: 1926.2375 - val_loss: 2171.0988 - val_mse: 30237320.0000 - val_mae: 2171.0986\n",
            "Epoch 423/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1919.1387 - mse: 25216770.0000 - mae: 1919.1387 - val_loss: 2165.3169 - val_mse: 30218322.0000 - val_mae: 2165.3169\n",
            "Epoch 424/500\n",
            "2250/2250 [==============================] - 0s 73us/step - loss: 1966.8689 - mse: 25660388.0000 - mae: 1966.8689 - val_loss: 2165.4314 - val_mse: 30252726.0000 - val_mae: 2165.4309\n",
            "Epoch 425/500\n",
            "2250/2250 [==============================] - 0s 84us/step - loss: 1914.7205 - mse: 25564044.0000 - mae: 1914.7205 - val_loss: 2171.5651 - val_mse: 30209526.0000 - val_mae: 2171.5652\n",
            "Epoch 426/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 1950.3015 - mse: 26319462.0000 - mae: 1950.3015 - val_loss: 2177.2967 - val_mse: 30273414.0000 - val_mae: 2177.2969\n",
            "Epoch 427/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1925.3246 - mse: 24717040.0000 - mae: 1925.3247 - val_loss: 2189.0824 - val_mse: 30407084.0000 - val_mae: 2189.0825\n",
            "Epoch 428/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 1863.1615 - mse: 23023872.0000 - mae: 1863.1615 - val_loss: 2165.3175 - val_mse: 30136576.0000 - val_mae: 2165.3174\n",
            "Epoch 429/500\n",
            "2250/2250 [==============================] - 0s 83us/step - loss: 1944.1193 - mse: 26714544.0000 - mae: 1944.1194 - val_loss: 2154.5008 - val_mse: 29956680.0000 - val_mae: 2154.5010\n",
            "Epoch 430/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1957.5223 - mse: 25855532.0000 - mae: 1957.5222 - val_loss: 2168.9514 - val_mse: 30245354.0000 - val_mae: 2168.9514\n",
            "Epoch 431/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1898.1519 - mse: 25110938.0000 - mae: 1898.1517 - val_loss: 2162.7447 - val_mse: 30251464.0000 - val_mae: 2162.7446\n",
            "Epoch 432/500\n",
            "2250/2250 [==============================] - 0s 83us/step - loss: 1948.4899 - mse: 26372908.0000 - mae: 1948.4897 - val_loss: 2175.3269 - val_mse: 30460234.0000 - val_mae: 2175.3271\n",
            "Epoch 433/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 1914.6702 - mse: 25464286.0000 - mae: 1914.6702 - val_loss: 2182.9863 - val_mse: 30506742.0000 - val_mae: 2182.9863\n",
            "Epoch 434/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 1927.0100 - mse: 24563598.0000 - mae: 1927.0100 - val_loss: 2189.6882 - val_mse: 30636358.0000 - val_mae: 2189.6880\n",
            "Epoch 435/500\n",
            "2250/2250 [==============================] - 0s 94us/step - loss: 1952.1389 - mse: 26387290.0000 - mae: 1952.1389 - val_loss: 2193.6475 - val_mse: 30587726.0000 - val_mae: 2193.6475\n",
            "Epoch 436/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1911.2304 - mse: 26213146.0000 - mae: 1911.2305 - val_loss: 2201.1169 - val_mse: 30518442.0000 - val_mae: 2201.1172\n",
            "Epoch 437/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 1980.2079 - mse: 27101358.0000 - mae: 1980.2078 - val_loss: 2165.1829 - val_mse: 30160788.0000 - val_mae: 2165.1831\n",
            "Epoch 438/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 1939.6023 - mse: 26549840.0000 - mae: 1939.6024 - val_loss: 2174.3627 - val_mse: 29988706.0000 - val_mae: 2174.3625\n",
            "Epoch 439/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 1894.4837 - mse: 24967410.0000 - mae: 1894.4835 - val_loss: 2162.9184 - val_mse: 30003036.0000 - val_mae: 2162.9185\n",
            "Epoch 440/500\n",
            "2250/2250 [==============================] - 0s 86us/step - loss: 1928.5916 - mse: 25507962.0000 - mae: 1928.5916 - val_loss: 2165.2586 - val_mse: 30074694.0000 - val_mae: 2165.2585\n",
            "Epoch 441/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 1958.0813 - mse: 26915626.0000 - mae: 1958.0813 - val_loss: 2194.4050 - val_mse: 30215696.0000 - val_mae: 2194.4050\n",
            "Epoch 442/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1902.0701 - mse: 24008328.0000 - mae: 1902.0699 - val_loss: 2175.5680 - val_mse: 30118054.0000 - val_mae: 2175.5681\n",
            "Epoch 443/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1910.9354 - mse: 25769702.0000 - mae: 1910.9353 - val_loss: 2195.0370 - val_mse: 30337494.0000 - val_mae: 2195.0374\n",
            "Epoch 444/500\n",
            "2250/2250 [==============================] - 0s 81us/step - loss: 1903.0790 - mse: 24037920.0000 - mae: 1903.0791 - val_loss: 2215.5790 - val_mse: 30521914.0000 - val_mae: 2215.5789\n",
            "Epoch 445/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1956.2601 - mse: 25661958.0000 - mae: 1956.2603 - val_loss: 2174.8408 - val_mse: 30284032.0000 - val_mae: 2174.8411\n",
            "Epoch 446/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1933.0716 - mse: 24990850.0000 - mae: 1933.0715 - val_loss: 2178.4036 - val_mse: 30372186.0000 - val_mae: 2178.4036\n",
            "Epoch 447/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 2025.9578 - mse: 29775084.0000 - mae: 2025.9578 - val_loss: 2179.6073 - val_mse: 30364978.0000 - val_mae: 2179.6074\n",
            "Epoch 448/500\n",
            "2250/2250 [==============================] - 0s 94us/step - loss: 1897.3616 - mse: 24906582.0000 - mae: 1897.3616 - val_loss: 2163.0827 - val_mse: 30136044.0000 - val_mae: 2163.0828\n",
            "Epoch 449/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 1842.3167 - mse: 23227062.0000 - mae: 1842.3168 - val_loss: 2157.9379 - val_mse: 30153398.0000 - val_mae: 2157.9380\n",
            "Epoch 450/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 1964.5963 - mse: 27004550.0000 - mae: 1964.5964 - val_loss: 2198.8753 - val_mse: 30446870.0000 - val_mae: 2198.8755\n",
            "Epoch 451/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1962.6670 - mse: 28236172.0000 - mae: 1962.6669 - val_loss: 2210.6595 - val_mse: 30498604.0000 - val_mae: 2210.6597\n",
            "Epoch 452/500\n",
            "2250/2250 [==============================] - 0s 94us/step - loss: 1880.4082 - mse: 22859214.0000 - mae: 1880.4080 - val_loss: 2189.5537 - val_mse: 30339796.0000 - val_mae: 2189.5535\n",
            "Epoch 453/500\n",
            "2250/2250 [==============================] - 0s 81us/step - loss: 1911.0503 - mse: 24226310.0000 - mae: 1911.0502 - val_loss: 2166.1712 - val_mse: 29964882.0000 - val_mae: 2166.1714\n",
            "Epoch 454/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1901.8138 - mse: 24755580.0000 - mae: 1901.8137 - val_loss: 2180.0909 - val_mse: 29988844.0000 - val_mae: 2180.0906\n",
            "Epoch 455/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 1930.0622 - mse: 24671058.0000 - mae: 1930.0623 - val_loss: 2156.5089 - val_mse: 29709252.0000 - val_mae: 2156.5090\n",
            "Epoch 456/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 1909.6832 - mse: 24199776.0000 - mae: 1909.6831 - val_loss: 2194.4132 - val_mse: 30018016.0000 - val_mae: 2194.4128\n",
            "Epoch 457/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1867.9810 - mse: 24990478.0000 - mae: 1867.9808 - val_loss: 2171.8384 - val_mse: 29856796.0000 - val_mae: 2171.8384\n",
            "Epoch 458/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 1910.3634 - mse: 24127214.0000 - mae: 1910.3633 - val_loss: 2158.8570 - val_mse: 29612100.0000 - val_mae: 2158.8572\n",
            "Epoch 459/500\n",
            "2250/2250 [==============================] - 0s 77us/step - loss: 1930.5051 - mse: 25192538.0000 - mae: 1930.5054 - val_loss: 2158.3337 - val_mse: 29529980.0000 - val_mae: 2158.3337\n",
            "Epoch 460/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1920.9088 - mse: 25090428.0000 - mae: 1920.9087 - val_loss: 2157.9993 - val_mse: 29532304.0000 - val_mae: 2157.9993\n",
            "Epoch 461/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1886.4433 - mse: 24989644.0000 - mae: 1886.4434 - val_loss: 2180.2867 - val_mse: 29709110.0000 - val_mae: 2180.2869\n",
            "Epoch 462/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1998.3507 - mse: 28106924.0000 - mae: 1998.3507 - val_loss: 2162.6719 - val_mse: 29535712.0000 - val_mae: 2162.6721\n",
            "Epoch 463/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1901.0423 - mse: 24235150.0000 - mae: 1901.0422 - val_loss: 2169.5656 - val_mse: 29569788.0000 - val_mae: 2169.5657\n",
            "Epoch 464/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1922.9619 - mse: 24024158.0000 - mae: 1922.9618 - val_loss: 2187.4689 - val_mse: 29683246.0000 - val_mae: 2187.4690\n",
            "Epoch 465/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 1940.7919 - mse: 25830296.0000 - mae: 1940.7920 - val_loss: 2171.3779 - val_mse: 29618040.0000 - val_mae: 2171.3777\n",
            "Epoch 466/500\n",
            "2250/2250 [==============================] - 0s 84us/step - loss: 1960.0071 - mse: 27217818.0000 - mae: 1960.0071 - val_loss: 2171.8734 - val_mse: 29712430.0000 - val_mae: 2171.8733\n",
            "Epoch 467/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1906.1185 - mse: 26115428.0000 - mae: 1906.1184 - val_loss: 2160.1220 - val_mse: 29553330.0000 - val_mae: 2160.1218\n",
            "Epoch 468/500\n",
            "2250/2250 [==============================] - 0s 86us/step - loss: 1892.3402 - mse: 23503982.0000 - mae: 1892.3402 - val_loss: 2158.2556 - val_mse: 29566762.0000 - val_mae: 2158.2556\n",
            "Epoch 469/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1922.9302 - mse: 25293480.0000 - mae: 1922.9302 - val_loss: 2174.1694 - val_mse: 29769814.0000 - val_mae: 2174.1694\n",
            "Epoch 470/500\n",
            "2250/2250 [==============================] - 0s 83us/step - loss: 1953.8246 - mse: 27124640.0000 - mae: 1953.8247 - val_loss: 2161.3516 - val_mse: 29634992.0000 - val_mae: 2161.3513\n",
            "Epoch 471/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 1990.6553 - mse: 28145252.0000 - mae: 1990.6553 - val_loss: 2167.5027 - val_mse: 29706646.0000 - val_mae: 2167.5024\n",
            "Epoch 472/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1878.6498 - mse: 23421130.0000 - mae: 1878.6500 - val_loss: 2149.8089 - val_mse: 29520464.0000 - val_mae: 2149.8086\n",
            "Epoch 473/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1899.1113 - mse: 25619144.0000 - mae: 1899.1111 - val_loss: 2180.6240 - val_mse: 29850898.0000 - val_mae: 2180.6240\n",
            "Epoch 474/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 1890.1844 - mse: 23826730.0000 - mae: 1890.1844 - val_loss: 2170.7530 - val_mse: 29798088.0000 - val_mae: 2170.7534\n",
            "Epoch 475/500\n",
            "2250/2250 [==============================] - 0s 78us/step - loss: 1933.3749 - mse: 25994958.0000 - mae: 1933.3751 - val_loss: 2170.8114 - val_mse: 29647096.0000 - val_mae: 2170.8115\n",
            "Epoch 476/500\n",
            "2250/2250 [==============================] - 0s 90us/step - loss: 1864.7989 - mse: 24050836.0000 - mae: 1864.7988 - val_loss: 2172.9143 - val_mse: 29582018.0000 - val_mae: 2172.9146\n",
            "Epoch 477/500\n",
            "2250/2250 [==============================] - 0s 72us/step - loss: 1901.3442 - mse: 24747972.0000 - mae: 1901.3440 - val_loss: 2166.9171 - val_mse: 29575436.0000 - val_mae: 2166.9172\n",
            "Epoch 478/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1888.9055 - mse: 25317440.0000 - mae: 1888.9055 - val_loss: 2188.3607 - val_mse: 29701620.0000 - val_mae: 2188.3608\n",
            "Epoch 479/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1934.3365 - mse: 25319332.0000 - mae: 1934.3364 - val_loss: 2180.2628 - val_mse: 29660484.0000 - val_mae: 2180.2629\n",
            "Epoch 480/500\n",
            "2250/2250 [==============================] - 0s 87us/step - loss: 1851.6225 - mse: 23668606.0000 - mae: 1851.6224 - val_loss: 2166.0827 - val_mse: 29594558.0000 - val_mae: 2166.0830\n",
            "Epoch 481/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1911.6592 - mse: 25256116.0000 - mae: 1911.6591 - val_loss: 2172.0245 - val_mse: 29541232.0000 - val_mae: 2172.0244\n",
            "Epoch 482/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1959.2816 - mse: 25966270.0000 - mae: 1959.2817 - val_loss: 2173.5989 - val_mse: 29732204.0000 - val_mae: 2173.5991\n",
            "Epoch 483/500\n",
            "2250/2250 [==============================] - 0s 83us/step - loss: 1909.8481 - mse: 23475240.0000 - mae: 1909.8483 - val_loss: 2165.6948 - val_mse: 29530536.0000 - val_mae: 2165.6951\n",
            "Epoch 484/500\n",
            "2250/2250 [==============================] - 0s 76us/step - loss: 1903.4863 - mse: 25552722.0000 - mae: 1903.4862 - val_loss: 2178.0513 - val_mse: 29655056.0000 - val_mae: 2178.0513\n",
            "Epoch 485/500\n",
            "2250/2250 [==============================] - 0s 81us/step - loss: 1900.9847 - mse: 25904224.0000 - mae: 1900.9846 - val_loss: 2163.9012 - val_mse: 29563282.0000 - val_mae: 2163.9011\n",
            "Epoch 486/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 1958.9441 - mse: 26676850.0000 - mae: 1958.9440 - val_loss: 2151.5337 - val_mse: 29565508.0000 - val_mae: 2151.5334\n",
            "Epoch 487/500\n",
            "2250/2250 [==============================] - 0s 71us/step - loss: 1866.9436 - mse: 22796982.0000 - mae: 1866.9436 - val_loss: 2176.3080 - val_mse: 30078742.0000 - val_mae: 2176.3081\n",
            "Epoch 488/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1875.1491 - mse: 23731690.0000 - mae: 1875.1492 - val_loss: 2154.1489 - val_mse: 29684498.0000 - val_mae: 2154.1487\n",
            "Epoch 489/500\n",
            "2250/2250 [==============================] - 0s 80us/step - loss: 1910.6133 - mse: 25390546.0000 - mae: 1910.6133 - val_loss: 2165.3821 - val_mse: 29705364.0000 - val_mae: 2165.3821\n",
            "Epoch 490/500\n",
            "2250/2250 [==============================] - 0s 85us/step - loss: 1896.7850 - mse: 25919146.0000 - mae: 1896.7849 - val_loss: 2166.0400 - val_mse: 29739318.0000 - val_mae: 2166.0398\n",
            "Epoch 491/500\n",
            "2250/2250 [==============================] - 0s 89us/step - loss: 1902.2803 - mse: 23534312.0000 - mae: 1902.2803 - val_loss: 2169.8551 - val_mse: 29564958.0000 - val_mae: 2169.8550\n",
            "Epoch 492/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1898.9983 - mse: 23496768.0000 - mae: 1898.9982 - val_loss: 2159.2294 - val_mse: 29438676.0000 - val_mae: 2159.2295\n",
            "Epoch 493/500\n",
            "2250/2250 [==============================] - 0s 79us/step - loss: 1850.1024 - mse: 24056408.0000 - mae: 1850.1023 - val_loss: 2170.8259 - val_mse: 29552350.0000 - val_mae: 2170.8257\n",
            "Epoch 494/500\n",
            "2250/2250 [==============================] - 0s 88us/step - loss: 1915.7436 - mse: 25137914.0000 - mae: 1915.7435 - val_loss: 2186.7719 - val_mse: 29652336.0000 - val_mae: 2186.7717\n",
            "Epoch 495/500\n",
            "2250/2250 [==============================] - 0s 75us/step - loss: 1934.6237 - mse: 24639164.0000 - mae: 1934.6235 - val_loss: 2163.2913 - val_mse: 29543768.0000 - val_mae: 2163.2913\n",
            "Epoch 496/500\n",
            "2250/2250 [==============================] - 0s 89us/step - loss: 1944.2041 - mse: 26075758.0000 - mae: 1944.2042 - val_loss: 2173.6279 - val_mse: 29622430.0000 - val_mae: 2173.6277\n",
            "Epoch 497/500\n",
            "2250/2250 [==============================] - 0s 74us/step - loss: 1920.3618 - mse: 25035022.0000 - mae: 1920.3618 - val_loss: 2173.5647 - val_mse: 29611544.0000 - val_mae: 2173.5647\n",
            "Epoch 498/500\n",
            "2250/2250 [==============================] - 0s 70us/step - loss: 1970.5764 - mse: 27172844.0000 - mae: 1970.5764 - val_loss: 2150.8030 - val_mse: 29182598.0000 - val_mae: 2150.8032\n",
            "Epoch 499/500\n",
            "2250/2250 [==============================] - 0s 69us/step - loss: 1954.8154 - mse: 26302856.0000 - mae: 1954.8153 - val_loss: 2156.3583 - val_mse: 29281642.0000 - val_mae: 2156.3584\n",
            "Epoch 500/500\n",
            "2250/2250 [==============================] - 0s 82us/step - loss: 1880.6592 - mse: 24475998.0000 - mae: 1880.6593 - val_loss: 2148.3421 - val_mse: 29143314.0000 - val_mae: 2148.3420\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZSPkuft7YMp",
        "outputId": "71d02271-3b7a-4226-ab68-a72dc3ac6ca2"
      },
      "source": [
        "df_sklearn = calc_save_err_metric_combined(error_metrics, result_sklearn, max_of_pretrain_days, max_selected_countries, path=exp2_path, static_learner=True, alternate_batch=False, transpose=True)\n",
        "display_scores(df_sklearn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE Score\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge     LSTM\n",
            "0                      MAE        30.000     10719.064         26199.626 215350.292      7937.441       5687.090 2295.154\n",
            "1                      MAE        60.000      2998.439          3095.116   7455.608      2937.458       3861.348 3212.296\n",
            "2                      MAE        90.000      3517.174          3313.749   4953.637      3386.874       3816.471 5097.798\n",
            "3                      MAE       120.000      3309.609          2535.159   6236.067      2640.058       3885.423 4969.858\n",
            "4                      MAE       150.000      3726.906          2896.881  16026.703      3606.688       4680.568 4052.543\n",
            "5                      MAE       180.000      4657.525          4725.560  16702.749      4907.716       4403.925 5008.744\n",
            "mean                   NaN       105.000      4821.453          7127.682  44454.176      4236.039       4389.138 4106.065\n",
            "-----------------------------------------------------------------------------------\n",
            "RMSE Score\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge      LSTM\n",
            "0                     RMSE        30.000     13332.744         32452.411 650415.497     12162.997      14498.576  5524.865\n",
            "1                     RMSE        60.000      4797.720          5626.990  13858.035      6806.359       8495.825  6844.168\n",
            "2                     RMSE        90.000      7727.203          7498.593   9690.161      7958.597       9283.466 13213.493\n",
            "3                     RMSE       120.000      7129.238          5563.058  13341.399      5979.999       8465.979 11616.704\n",
            "4                     RMSE       150.000      8238.341          8712.535  41220.706      8924.896       9971.748  8608.255\n",
            "5                     RMSE       180.000      8289.249          9240.603  37313.388      9455.006       8893.869  7990.939\n",
            "mean                   NaN       105.000      8252.416         11515.698 127639.864      8547.976       9934.910  8966.404\n",
            "-----------------------------------------------------------------------------------\n",
            "MAPE Score\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge  LSTM\n",
            "0                     MAPE        30.000        65.045           151.906    588.491        38.443         12.928 1.501\n",
            "1                     MAPE        60.000        10.649             6.256     17.627         3.444          5.582 1.434\n",
            "2                     MAPE        90.000         4.535             1.610      5.625         1.624          3.862 1.325\n",
            "3                     MAPE       120.000         1.903             0.791      2.266         0.830          1.766 0.750\n",
            "4                     MAPE       150.000         1.123             0.578      1.760         0.787          1.355 0.783\n",
            "5                     MAPE       180.000         0.636             0.519      1.517         0.655          0.693 0.636\n",
            "mean                   NaN       105.000        13.982            26.943    102.881         7.631          4.365 1.071\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "U44wYr8WAqq8",
        "outputId": "681279c6-b485-4217-855e-b3bbfb16f951"
      },
      "source": [
        "save_runtime(running_time_static, path=exp2_runtime_path, static_learner=True)\n",
        "running_time_static"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PretrainDays</th>\n",
              "      <th>RandomForest</th>\n",
              "      <th>GradientBoosting</th>\n",
              "      <th>LinearSVR</th>\n",
              "      <th>DecisionTree</th>\n",
              "      <th>BayesianRidge</th>\n",
              "      <th>LSTM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30</td>\n",
              "      <td>0.212</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.017</td>\n",
              "      <td>26.291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60</td>\n",
              "      <td>0.514</td>\n",
              "      <td>0.924</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.020</td>\n",
              "      <td>35.858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>90</td>\n",
              "      <td>1.011</td>\n",
              "      <td>2.043</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.012</td>\n",
              "      <td>46.551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>120</td>\n",
              "      <td>1.512</td>\n",
              "      <td>3.219</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.014</td>\n",
              "      <td>61.248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>150</td>\n",
              "      <td>1.995</td>\n",
              "      <td>4.351</td>\n",
              "      <td>0.680</td>\n",
              "      <td>0.246</td>\n",
              "      <td>0.019</td>\n",
              "      <td>75.595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>180</td>\n",
              "      <td>2.499</td>\n",
              "      <td>5.507</td>\n",
              "      <td>0.865</td>\n",
              "      <td>0.309</td>\n",
              "      <td>0.022</td>\n",
              "      <td>85.747</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PretrainDays  RandomForest  ...  BayesianRidge   LSTM\n",
              "0            30         0.212  ...          0.017 26.291\n",
              "1            60         0.514  ...          0.020 35.858\n",
              "2            90         1.011  ...          0.012 46.551\n",
              "3           120         1.512  ...          0.014 61.248\n",
              "4           150         1.995  ...          0.019 75.595\n",
              "5           180         2.499  ...          0.022 85.747\n",
              "\n",
              "[6 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "-7NiYw19VchA",
        "outputId": "9c87251c-5023-43b7-d28e-0c88963cd02c"
      },
      "source": [
        "summary_table_static = get_summary_table(df_sklearn, running_time_static, error_metrics, static_learner=True)\n",
        "save_summary_table(summary_table_static, exp2_summary_path,static_learner=True,alternate_batch=False, transpose=True)\n",
        "summary_table_static"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RandomForest</th>\n",
              "      <th>GradientBoosting</th>\n",
              "      <th>LinearSVR</th>\n",
              "      <th>DecisionTree</th>\n",
              "      <th>BayesianRidge</th>\n",
              "      <th>LSTM</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Metric</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MAE</th>\n",
              "      <td>4821.453</td>\n",
              "      <td>7127.682</td>\n",
              "      <td>44454.176</td>\n",
              "      <td>4236.039</td>\n",
              "      <td>4389.138</td>\n",
              "      <td>4106.065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MAPE</th>\n",
              "      <td>13.982</td>\n",
              "      <td>26.943</td>\n",
              "      <td>102.881</td>\n",
              "      <td>7.631</td>\n",
              "      <td>4.365</td>\n",
              "      <td>1.071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RMSE</th>\n",
              "      <td>8252.416</td>\n",
              "      <td>11515.698</td>\n",
              "      <td>127639.864</td>\n",
              "      <td>8547.976</td>\n",
              "      <td>9934.910</td>\n",
              "      <td>8966.404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time(sec)</th>\n",
              "      <td>1.290</td>\n",
              "      <td>2.713</td>\n",
              "      <td>0.433</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.017</td>\n",
              "      <td>55.215</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           RandomForest  GradientBoosting  ...  BayesianRidge     LSTM\n",
              "Metric                                     ...                        \n",
              "MAE            4821.453          7127.682  ...       4389.138 4106.065\n",
              "MAPE             13.982            26.943  ...          4.365    1.071\n",
              "RMSE           8252.416         11515.698  ...       9934.910 8966.404\n",
              "Time(sec)         1.290             2.713  ...          0.017   55.215\n",
              "\n",
              "[4 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAdncNlrKXD8"
      },
      "source": [
        "## Incremental Learner: Alternate Batches\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGGSn84iKraj"
      },
      "source": [
        "def scikit_multiflow_alternate_batch(df, pretrain_days):\r\n",
        "\r\n",
        "    model, model_names = instantiate_regressors()\r\n",
        "\r\n",
        "    len_countries = len(df['country'].unique())\r\n",
        "\r\n",
        "    frames, running_time_frames = [], []\r\n",
        "\r\n",
        "    # Setup the evaluator\r\n",
        "    for day in pretrain_days:\r\n",
        "\r\n",
        "        df_subset = create_alternate_batch_subset(df, day, batch_size=10)\r\n",
        "\r\n",
        "        # Creating a stream from dataframe\r\n",
        "        stream = DataStream(np.array(df_subset.iloc[:, 4:-1]), y=np.array(df_subset.iloc[:, -1])) #TODO: Drop columns with name\r\n",
        "\r\n",
        "        pretrain_size = (day//2) * len_countries\r\n",
        "        max_samples = pretrain_size + 1\r\n",
        "        testing_samples_size = (day//2 + 20) * len_countries  # Testing on set one month ahead only\r\n",
        "\r\n",
        "        evaluator = EvaluatePrequential(show_plot=False,\r\n",
        "                                    pretrain_size=pretrain_size,\r\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\r\n",
        "                                    max_samples=max_samples)\r\n",
        "        # Run evaluation\r\n",
        "        evaluator.evaluate(stream=stream, model=model, model_names=model_names)\r\n",
        "\r\n",
        "        # Added Now\r\n",
        "        X = stream.X[pretrain_size: pretrain_size + testing_samples_size]\r\n",
        "        y = stream.y[pretrain_size: pretrain_size + testing_samples_size]\r\n",
        "\r\n",
        "        prediction = evaluator.predict(X)\r\n",
        "\r\n",
        "        # Since we add one extra sample, reset the evaluator\r\n",
        "        evaluator = reset_evaluator(evaluator)\r\n",
        "\r\n",
        "        evaluator = update_incremental_metrics(evaluator, y, prediction)\r\n",
        "\r\n",
        "        # Dictionary to store each iteration error scores\r\n",
        "        mdl_evaluation_scores = {}\r\n",
        "\r\n",
        "        # Adding Evaluation Measurements and pretraining days\r\n",
        "        mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\r\n",
        "        mdl_evaluation_scores['PretrainDays'] = [day//2] * len(mdl_evaluation_scores['EvaluationMeasurement'])\r\n",
        "\r\n",
        "        mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\r\n",
        "\r\n",
        "        # Errors of each model on a specific pre-train days\r\n",
        "        frames.append(mdl_evaluation_df)\r\n",
        "\r\n",
        "        # Run time for each algorithm\r\n",
        "        running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\r\n",
        "\r\n",
        "    # Final Run Time DataFrame\r\n",
        "    running_time_df = pd.concat(running_time_frames,ignore_index=True)\r\n",
        "\r\n",
        "    # Final Evaluation Score Dataframe\r\n",
        "    evaluation_scores_df = pd.concat(frames, ignore_index=True)\r\n",
        "    return evaluation_scores_df, running_time_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DpEHhhFiDfw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "918b9140-f0ea-4590-e072-d91284f6a867"
      },
      "source": [
        "# Old Script\n",
        "\"\"\"\n",
        "def scikit_multiflow_alternate_batch(df, pretrain_days):\n",
        "\n",
        "  model, model_names = instantiate_regressors()\n",
        "\n",
        "  len_countries = len(df['country'].unique())\n",
        "\n",
        "  frames , running_time_frames = [], []\n",
        "\n",
        "  # Setup the evaluator\n",
        "  for day in pretrain_days:\n",
        "\n",
        "      df_subset = create_alternate_batch_subset(df,day,batch_size=10)\n",
        "\n",
        "      # Creating a stream from dataframe\n",
        "      stream = DataStream(np.array(df_subset.iloc[:,4:-1]), y=np.array(df_subset.iloc[:,-1])) #TODO: Drop columns with name \n",
        "\n",
        "      pretrain_size = (day//2) * len_countries\n",
        "      max_samples = (day//2 + 20) * len_countries #Testing on set one month ahead only\n",
        "\n",
        "      evaluator = EvaluatePrequential(show_plot=False,\n",
        "                                    pretrain_size=pretrain_size,\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\n",
        "                                    max_samples=max_samples)\n",
        "      # Run evaluation\n",
        "      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
        "\n",
        "      # Dictionary to store each iteration error scores\n",
        "      mdl_evaluation_scores = {}\n",
        "\n",
        "      # Adding Evaluation Measurements and pretraining days\n",
        "      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\n",
        "      mdl_evaluation_scores['PretrainDays'] = [day//2] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "\n",
        "      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\n",
        "\n",
        "      # Errors of each model on a specific pre-train days\n",
        "      frames.append(mdl_evaluation_df)\n",
        "\n",
        "      # Run time for each algorithm\n",
        "      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
        "\n",
        "  # Final Run Time DataFrame\n",
        "  running_time_df = pd.concat(running_time_frames,ignore_index=True)\n",
        "\n",
        "  # Final Evaluation Score Dataframe\n",
        "  evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
        "  return evaluation_scores_df, running_time_df\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef scikit_multiflow_alternate_batch(df, pretrain_days):\\n\\n  model, model_names = instantiate_regressors()\\n\\n  len_countries = len(df['country'].unique())\\n\\n  frames , running_time_frames = [], []\\n\\n  # Setup the evaluator\\n  for day in pretrain_days:\\n\\n      df_subset = create_alternate_batch_subset(df,day,batch_size=10)\\n\\n      # Creating a stream from dataframe\\n      stream = DataStream(np.array(df_subset.iloc[:,4:-1]), y=np.array(df_subset.iloc[:,-1])) #TODO: Drop columns with name \\n\\n      pretrain_size = (day//2) * len_countries\\n      max_samples = (day//2 + 20) * len_countries #Testing on set one month ahead only\\n\\n      evaluator = EvaluatePrequential(show_plot=False,\\n                                    pretrain_size=pretrain_size,\\n                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\\n                                    max_samples=max_samples)\\n      # Run evaluation\\n      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\\n\\n      # Dictionary to store each iteration error scores\\n      mdl_evaluation_scores = {}\\n\\n      # Adding Evaluation Measurements and pretraining days\\n      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\\n      mdl_evaluation_scores['PretrainDays'] = [day//2] * len(mdl_evaluation_scores['EvaluationMeasurement'])\\n\\n      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\\n\\n      # Errors of each model on a specific pre-train days\\n      frames.append(mdl_evaluation_df)\\n\\n      # Run time for each algorithm\\n      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\\n\\n  # Final Run Time DataFrame\\n  running_time_df = pd.concat(running_time_frames,ignore_index=True)\\n\\n  # Final Evaluation Score Dataframe\\n  evaluation_scores_df = pd.concat(frames, ignore_index=True)\\n  return evaluation_scores_df, running_time_df\\n  \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv6KT2eyheiH",
        "outputId": "8e35a1d2-ab99-4b7f-9c4e-047eaad84366"
      },
      "source": [
        "result_skmlflow_alternate_batch, running_time_incremental_alternate_batch = scikit_multiflow_alternate_batch(result, pretrain_days)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 375 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.17s]\n",
            "Processed samples: 376\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 910424067.9708\n",
            "HT_Reg - MAPE          : 1.0540\n",
            "HT_Reg - MAE          : 30173.234297\n",
            "HAT_Reg - MSE          : 1275829323.4549\n",
            "HAT_Reg - MAPE          : 1.2477\n",
            "HAT_Reg - MAE          : 35718.753106\n",
            "ARF_Reg - MSE          : 2378353464.1700\n",
            "ARF_Reg - MAPE          : 1.7035\n",
            "ARF_Reg - MAE          : 48768.365404\n",
            "PA_Reg - MSE          : 307079836609.3058\n",
            "PA_Reg - MAPE          : 19.3564\n",
            "PA_Reg - MAE          : 554147.847248\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 750 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [13.10s]\n",
            "Processed samples: 751\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 1072431014199.4456\n",
            "HT_Reg - MAPE          : 48.5788\n",
            "HT_Reg - MAE          : 1035582.451666\n",
            "HAT_Reg - MSE          : 1072430891961.0686\n",
            "HAT_Reg - MAPE          : 48.5788\n",
            "HAT_Reg - MAE          : 1035582.392647\n",
            "ARF_Reg - MSE          : 84049121968.8927\n",
            "ARF_Reg - MAPE          : 13.5997\n",
            "ARF_Reg - MAE          : 289912.265986\n",
            "PA_Reg - MSE          : 232267102451343.0938\n",
            "PA_Reg - MAPE          : 714.9169\n",
            "PA_Reg - MAE          : 15240311.757026\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 1125 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [20.59s]\n",
            "Processed samples: 1126\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 42322325469251.2891\n",
            "HT_Reg - MAPE          : 197.0749\n",
            "HT_Reg - MAE          : 6505561.118708\n",
            "HAT_Reg - MSE          : 42322325393609.4453\n",
            "HAT_Reg - MAPE          : 197.0749\n",
            "HAT_Reg - MAE          : 6505561.112895\n",
            "ARF_Reg - MSE          : 22829047621088.2617\n",
            "ARF_Reg - MAPE          : 144.7406\n",
            "ARF_Reg - MAE          : 4777975.263759\n",
            "PA_Reg - MSE          : 848581836522646.2500\n",
            "PA_Reg - MAPE          : 882.4568\n",
            "PA_Reg - MAE          : 29130428.018185\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 1500 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [29.65s]\n",
            "Processed samples: 1501\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 14675022635.5407\n",
            "HT_Reg - MAPE          : 1.8416\n",
            "HT_Reg - MAE          : 121140.507823\n",
            "HAT_Reg - MSE          : 14675022635.5407\n",
            "HAT_Reg - MAPE          : 1.8416\n",
            "HAT_Reg - MAE          : 121140.507823\n",
            "ARF_Reg - MSE          : 18501818049.0357\n",
            "ARF_Reg - MAPE          : 2.0678\n",
            "ARF_Reg - MAE          : 136021.388204\n",
            "PA_Reg - MSE          : 361585348499.0104\n",
            "PA_Reg - MAPE          : 9.1414\n",
            "PA_Reg - MAE          : 601319.672470\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 1875 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [38.68s]\n",
            "Processed samples: 1876\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3201498650.6280\n",
            "HT_Reg - MAPE          : 1.3541\n",
            "HT_Reg - MAE          : 56581.787270\n",
            "HAT_Reg - MSE          : 3201498650.6280\n",
            "HAT_Reg - MAPE          : 1.3541\n",
            "HAT_Reg - MAE          : 56581.787270\n",
            "ARF_Reg - MSE          : 4814708885.6661\n",
            "ARF_Reg - MAPE          : 1.6605\n",
            "ARF_Reg - MAE          : 69388.103344\n",
            "PA_Reg - MSE          : 139561355856.8973\n",
            "PA_Reg - MAPE          : 8.9402\n",
            "PA_Reg - MAE          : 373579.115927\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 2250 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [46.89s]\n",
            "Processed samples: 2251\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3276201565.8252\n",
            "HT_Reg - MAPE          : 1.3512\n",
            "HT_Reg - MAE          : 57238.112878\n",
            "HAT_Reg - MSE          : 3276201565.8252\n",
            "HAT_Reg - MAPE          : 1.3512\n",
            "HAT_Reg - MAE          : 57238.112878\n",
            "ARF_Reg - MSE          : 10571217760.8724\n",
            "ARF_Reg - MAPE          : 2.4272\n",
            "ARF_Reg - MAE          : 102816.427485\n",
            "PA_Reg - MSE          : 19303308.2842\n",
            "PA_Reg - MAPE          : 0.1037\n",
            "PA_Reg - MAE          : 4393.553036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfluozdk7zod"
      },
      "source": [
        "df_alternate_batch = calc_save_err_metric_combined(error_metrics, result_skmlflow_alternate_batch, max_of_pretrain_days, max_selected_countries, path=exp2_path, static_learner=False, alternate_batch=True, transpose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL_NOS6va8bo",
        "outputId": "7c253b39-c61c-4088-941a-6f4763cd2b06"
      },
      "source": [
        "display_scores(df_alternate_batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE Score\n",
            "     EvaluationMeasurement  PretrainDays      HT_Reg     HAT_Reg     ARF_Reg      PA_Reg\n",
            "0                      MAE        15.000 1762087.482 1672923.625  128648.402  611473.408\n",
            "1                      MAE        30.000 1164499.952 1164499.952 1083883.275 1656716.234\n",
            "2                      MAE        45.000   16673.452   16673.452   13683.411  467890.062\n",
            "3                      MAE        60.000   17526.079   17526.079    9861.444  174974.998\n",
            "4                      MAE        75.000   17731.968   17731.968   14270.845  111131.619\n",
            "5                      MAE        90.000   16295.151   16295.151   12610.185   96585.100\n",
            "mean                   NaN        52.500  499135.681  484275.038  210492.927  519795.237\n",
            "-----------------------------------------------------------------------------------\n",
            "RMSE Score\n",
            "     EvaluationMeasurement  PretrainDays      HT_Reg     HAT_Reg     ARF_Reg      PA_Reg\n",
            "0                     RMSE        15.000 4635819.019 4422191.293  354625.472 1673381.609\n",
            "1                     RMSE        30.000 3225576.592 3225576.592 2979108.614 3987600.105\n",
            "2                     RMSE        45.000   35005.519   35005.519   30999.965 1056687.792\n",
            "3                     RMSE        60.000   35725.755   35725.755   26777.169  436286.706\n",
            "4                     RMSE        75.000   43014.504   43014.504   40325.897  270509.876\n",
            "5                     RMSE        90.000   37216.907   37216.907   29183.663  231309.228\n",
            "mean                   NaN        52.500 1335393.049 1299788.428  576836.797 1275962.553\n",
            "-----------------------------------------------------------------------------------\n",
            "MAPE Score\n",
            "     EvaluationMeasurement  PretrainDays   HT_Reg  HAT_Reg  ARF_Reg   PA_Reg\n",
            "0                     MAPE        15.000 4238.249 4047.296  400.269 1504.420\n",
            "1                     MAPE        30.000 3105.156 3105.156 2925.655 3962.616\n",
            "2                     MAPE        45.000   21.919   21.919   14.978  260.919\n",
            "3                     MAPE        60.000    8.760    8.760    1.680   51.456\n",
            "4                     MAPE        75.000    4.398    4.398    1.685   31.511\n",
            "5                     MAPE        90.000    1.709    1.709    0.973   10.935\n",
            "mean                   NaN        52.500 1230.032 1198.206  557.540  970.310\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "zNBEQbIeA-7H",
        "outputId": "b328bc8f-f09b-4079-cb81-5d52dc961432"
      },
      "source": [
        "save_runtime(running_time_incremental_alternate_batch, path=exp2_runtime_path, static_learner=False, alternate_batch=True)\n",
        "running_time_incremental_alternate_batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PretrainDays</th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.198</td>\n",
              "      <td>5.873</td>\n",
              "      <td>0.002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.546</td>\n",
              "      <td>12.257</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>90</td>\n",
              "      <td>0.686</td>\n",
              "      <td>2.160</td>\n",
              "      <td>17.760</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>120</td>\n",
              "      <td>1.042</td>\n",
              "      <td>3.553</td>\n",
              "      <td>25.074</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>150</td>\n",
              "      <td>1.677</td>\n",
              "      <td>3.929</td>\n",
              "      <td>33.092</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>180</td>\n",
              "      <td>2.183</td>\n",
              "      <td>5.229</td>\n",
              "      <td>39.505</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
              "0            30   0.119    0.198    5.873   0.002\n",
              "1            60   0.318    0.546   12.257   0.003\n",
              "2            90   0.686    2.160   17.760   0.003\n",
              "3           120   1.042    3.553   25.074   0.003\n",
              "4           150   1.677    3.929   33.092   0.003\n",
              "5           180   2.183    5.229   39.505   0.003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "1RuBt5n7Vqzf",
        "outputId": "9c405439-0248-4d1b-820e-fb93b7864395"
      },
      "source": [
        "summary_table_incremental_alternate = get_summary_table(df_alternate_batch, running_time_incremental_alternate_batch, error_metrics, static_learner=False)\n",
        "save_summary_table(summary_table_incremental_alternate, exp2_summary_path,static_learner=False,alternate_batch=True, transpose=True)\n",
        "summary_table_incremental_alternate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Metric</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MAE</th>\n",
              "      <td>499135.681</td>\n",
              "      <td>484275.038</td>\n",
              "      <td>210492.927</td>\n",
              "      <td>519795.237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MAPE</th>\n",
              "      <td>1230.032</td>\n",
              "      <td>1198.206</td>\n",
              "      <td>557.540</td>\n",
              "      <td>970.310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RMSE</th>\n",
              "      <td>1335393.049</td>\n",
              "      <td>1299788.428</td>\n",
              "      <td>576836.797</td>\n",
              "      <td>1275962.553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time(sec)</th>\n",
              "      <td>1.004</td>\n",
              "      <td>2.602</td>\n",
              "      <td>22.260</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               HT_Reg     HAT_Reg    ARF_Reg      PA_Reg\n",
              "Metric                                                  \n",
              "MAE        499135.681  484275.038 210492.927  519795.237\n",
              "MAPE         1230.032    1198.206    557.540     970.310\n",
              "RMSE      1335393.049 1299788.428 576836.797 1275962.553\n",
              "Time(sec)       1.004       2.602     22.260       0.003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQYy8fGQ_EJ5"
      },
      "source": [
        "## Significance tests for Experiment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "3yH-IbaPBk2D",
        "outputId": "27ecc410-88d2-42d9-85a9-9a0e715707c6"
      },
      "source": [
        "## EXP2\n",
        "# Significance results for Experiment 2\n",
        "err_metric_for_significance = 'MAPE'\n",
        "significance_thresh = 0.05\n",
        "plot_pop = False\n",
        "\n",
        "# Concatenating a population of all results (as in boxplot) for experiment 2\n",
        "# This is done for runs per batch for experiment 2. But for experiment 1 it's done for runs per country (their final averages, like the result sent to the boxplots).\n",
        "static = df_sklearn[df_sklearn['EvaluationMeasurement'] == err_metric_for_significance].drop(columns=['EvaluationMeasurement'], axis=1).transpose()\n",
        "incremental = df_alternate_batch[df_alternate_batch['EvaluationMeasurement'] == err_metric_for_significance].drop(columns=['EvaluationMeasurement', 'PretrainDays'], axis=1).transpose()\n",
        "concated_df = pd.concat([static, incremental]).transpose()\n",
        "concated_df.set_index('PretrainDays', inplace=True, drop=True)\n",
        "concated_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RandomForest</th>\n",
              "      <th>GradientBoosting</th>\n",
              "      <th>LinearSVR</th>\n",
              "      <th>DecisionTree</th>\n",
              "      <th>BayesianRidge</th>\n",
              "      <th>LSTM</th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PretrainDays</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30.000</th>\n",
              "      <td>65.045</td>\n",
              "      <td>151.906</td>\n",
              "      <td>588.491</td>\n",
              "      <td>38.443</td>\n",
              "      <td>12.928</td>\n",
              "      <td>1.501</td>\n",
              "      <td>4238.249</td>\n",
              "      <td>4047.296</td>\n",
              "      <td>400.269</td>\n",
              "      <td>1504.420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60.000</th>\n",
              "      <td>10.649</td>\n",
              "      <td>6.256</td>\n",
              "      <td>17.627</td>\n",
              "      <td>3.444</td>\n",
              "      <td>5.582</td>\n",
              "      <td>1.434</td>\n",
              "      <td>3105.156</td>\n",
              "      <td>3105.156</td>\n",
              "      <td>2925.655</td>\n",
              "      <td>3962.616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90.000</th>\n",
              "      <td>4.535</td>\n",
              "      <td>1.610</td>\n",
              "      <td>5.625</td>\n",
              "      <td>1.624</td>\n",
              "      <td>3.862</td>\n",
              "      <td>1.325</td>\n",
              "      <td>21.919</td>\n",
              "      <td>21.919</td>\n",
              "      <td>14.978</td>\n",
              "      <td>260.919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120.000</th>\n",
              "      <td>1.903</td>\n",
              "      <td>0.791</td>\n",
              "      <td>2.266</td>\n",
              "      <td>0.830</td>\n",
              "      <td>1.766</td>\n",
              "      <td>0.750</td>\n",
              "      <td>8.760</td>\n",
              "      <td>8.760</td>\n",
              "      <td>1.680</td>\n",
              "      <td>51.456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150.000</th>\n",
              "      <td>1.123</td>\n",
              "      <td>0.578</td>\n",
              "      <td>1.760</td>\n",
              "      <td>0.787</td>\n",
              "      <td>1.355</td>\n",
              "      <td>0.783</td>\n",
              "      <td>4.398</td>\n",
              "      <td>4.398</td>\n",
              "      <td>1.685</td>\n",
              "      <td>31.511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180.000</th>\n",
              "      <td>0.636</td>\n",
              "      <td>0.519</td>\n",
              "      <td>1.517</td>\n",
              "      <td>0.655</td>\n",
              "      <td>0.693</td>\n",
              "      <td>0.636</td>\n",
              "      <td>1.709</td>\n",
              "      <td>1.709</td>\n",
              "      <td>0.973</td>\n",
              "      <td>10.935</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              RandomForest  GradientBoosting  ...  ARF_Reg   PA_Reg\n",
              "PretrainDays                                  ...                  \n",
              "30.000              65.045           151.906  ...  400.269 1504.420\n",
              "60.000              10.649             6.256  ... 2925.655 3962.616\n",
              "90.000               4.535             1.610  ...   14.978  260.919\n",
              "120.000              1.903             0.791  ...    1.680   51.456\n",
              "150.000              1.123             0.578  ...    1.685   31.511\n",
              "180.000              0.636             0.519  ...    0.973   10.935\n",
              "\n",
              "[6 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kbQZ10ONBwJZ",
        "outputId": "c8d1dbe9-1aac-4f23-c6fe-180e708989ac"
      },
      "source": [
        "# Selecting the best algorithm for statistical comparisons\n",
        "# We want to know if the best is statistically significantly better compared to the rest.\n",
        "best_algo = concated_df.mean().sort_values(ascending=True).index[0]\n",
        "best_algo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'LSTM'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i0TvT9BNHwS",
        "outputId": "dafb6efa-d061-4312-f216-d599e382896b"
      },
      "source": [
        "print('AVG results across countries')\n",
        "concated_df.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AVG results across countries\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForest         13.982\n",
              "GradientBoosting     26.943\n",
              "LinearSVR           102.881\n",
              "DecisionTree          7.631\n",
              "BayesianRidge         4.365\n",
              "LSTM                  1.071\n",
              "HT_Reg             1230.032\n",
              "HAT_Reg            1198.206\n",
              "ARF_Reg             557.540\n",
              "PA_Reg              970.310\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v6za-agNIrF",
        "outputId": "c7374304-0eed-42f7-8b34-674891d9c98d"
      },
      "source": [
        "print('STEDEV across countries')\n",
        "concated_df.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STEDEV across countries\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForest         25.287\n",
              "GradientBoosting     61.258\n",
              "LinearSVR           237.978\n",
              "DecisionTree         15.131\n",
              "BayesianRidge         4.569\n",
              "LSTM                  0.389\n",
              "HT_Reg             1924.966\n",
              "HAT_Reg            1865.958\n",
              "ARF_Reg            1170.880\n",
              "PA_Reg             1574.068\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG_TSWRXBwtM",
        "outputId": "4b6c3888-aa99-408f-ee8e-626052aef3be"
      },
      "source": [
        "# Iterate through all the other algorithms to see if the difference in results is significant\n",
        "competitors = list(concated_df.columns)\n",
        "competitors.remove(best_algo)\n",
        "for significance_thresh in [0.01,0.05]:\n",
        "  print(f'Running significane at: {significance_thresh}')\n",
        "  for competitor in competitors:\n",
        "    # print(competitor)\n",
        "    pval, significant = check_significance(concated_df[best_algo], concated_df[competitor], significance_at=significance_thresh)\n",
        "    print(f'Comparison of {best_algo} to {competitor} pvalue: {pval}   /  significant?: {significant}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running significane at: 0.01\n",
            "Population too small.\n",
            "Comparison of LSTM to RandomForest pvalue: 0.046399461870904594   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to GradientBoosting pvalue: 0.24886387493792206   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to LinearSVR pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to DecisionTree pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to BayesianRidge pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to HT_Reg pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to HAT_Reg pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to ARF_Reg pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to PA_Reg pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Running significane at: 0.05\n",
            "Population too small.\n",
            "Comparison of LSTM to RandomForest pvalue: 0.046399461870904594   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to GradientBoosting pvalue: 0.24886387493792206   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to LinearSVR pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to DecisionTree pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to BayesianRidge pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to HT_Reg pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to HAT_Reg pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to ARF_Reg pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of LSTM to PA_Reg pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
            "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsx2eNV0WqxC",
        "outputId": "eb732304-25c9-46da-ddde-17792c6bd051"
      },
      "source": [
        "# Iterate through all the other algorithms to see if the difference in results is significant\n",
        "best_algo2 = concated_df.mean().sort_values(ascending=True).index[1]\n",
        "competitors = list(concated_df.columns)\n",
        "competitors.remove(best_algo2)\n",
        "for significance_thresh in [0.01,0.05]:\n",
        "  print(f'Running significance at: {significance_thresh}')\n",
        "  for competitor in competitors:\n",
        "    # print(competitor)\n",
        "    pval, significant = check_significance(concated_df[best_algo2], concated_df[competitor], significance_at=significance_thresh)\n",
        "    print(f'Comparison of {best_algo2} to {competitor} pvalue: {pval}   /  significant?: {significant}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running significance at: 0.01\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to RandomForest pvalue: 0.17295491798842066   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to GradientBoosting pvalue: 0.6001794871405538   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to LinearSVR pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to DecisionTree pvalue: 0.3454475304692257   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to LSTM pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to HT_Reg pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to HAT_Reg pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to ARF_Reg pvalue: 0.046399461870904594   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to PA_Reg pvalue: 0.027707849358079864   /  significant?: Not Significant (Wilcox Test)\n",
            "Running significance at: 0.05\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to RandomForest pvalue: 0.17295491798842066   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to GradientBoosting pvalue: 0.6001794871405538   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to LinearSVR pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to DecisionTree pvalue: 0.3454475304692257   /  significant?: Not Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to LSTM pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to HT_Reg pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to HAT_Reg pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to ARF_Reg pvalue: 0.046399461870904594   /  significant?: Significant (Wilcox Test)\n",
            "Population too small.\n",
            "Comparison of BayesianRidge to PA_Reg pvalue: 0.027707849358079864   /  significant?: Significant (Wilcox Test)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:2879: UserWarning: Sample size too small for normal approximation.\n",
            "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPwMeUDwn5Tk"
      },
      "source": [
        "# Download Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-fVUOfpj78ES",
        "outputId": "e04ef632-e706-45d8-cb96-8fe98878ae95"
      },
      "source": [
        "\n",
        "!zip -r /content/Result.zip /content/Result\n",
        "from google.colab import files\n",
        "files.download(\"/content/Result.zip\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/Result/ (stored 0%)\n",
            "  adding: content/Result/exp2/ (stored 0%)\n",
            "  adding: content/Result/exp2/combined25country_MAPE_incremental_alternate_batch.csv (deflated 42%)\n",
            "  adding: content/Result/exp2/combined25country_MAPE_static.tex (deflated 55%)\n",
            "  adding: content/Result/exp2/summary/ (stored 0%)\n",
            "  adding: content/Result/exp2/summary/combined25country_summary_table_incremental_alternate_batch.tex (deflated 40%)\n",
            "  adding: content/Result/exp2/summary/combined25country_summary_table_incremental_alternate_batch.csv (deflated 28%)\n",
            "  adding: content/Result/exp2/summary/combined25country_summary_table_incremental.csv (deflated 26%)\n",
            "  adding: content/Result/exp2/summary/combined25country_summary_table_static.csv (deflated 25%)\n",
            "  adding: content/Result/exp2/summary/combined25country_summary_table_static.tex (deflated 41%)\n",
            "  adding: content/Result/exp2/summary/combined25country_summary_table_incremental.tex (deflated 39%)\n",
            "  adding: content/Result/exp2/combined25country_MAE_incremental.csv (deflated 41%)\n",
            "  adding: content/Result/exp2/combined25country_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp2/combined25country_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp2/combined25country_MAPE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp2/combined25country_MAPE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp2/combined25country_RMSE_incremental_alternate_batch.tex (deflated 59%)\n",
            "  adding: content/Result/exp2/combined25country_RMSE_incremental.csv (deflated 43%)\n",
            "  adding: content/Result/exp2/combined25country_MAPE_incremental_alternate_batch.tex (deflated 58%)\n",
            "  adding: content/Result/exp2/combined25country_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp2/combined25country_MAE_incremental_alternate_batch.tex (deflated 58%)\n",
            "  adding: content/Result/exp2/combined25country_MAE_incremental_alternate_batch.csv (deflated 42%)\n",
            "  adding: content/Result/exp2/runtime/ (stored 0%)\n",
            "  adding: content/Result/exp2/runtime/combined25country_runtime_incremental_alternate_batch.tex (deflated 54%)\n",
            "  adding: content/Result/exp2/runtime/combined25country_runtime_incremental_alternate_batch.csv (deflated 33%)\n",
            "  adding: content/Result/exp2/runtime/combined25country_runtime_incremental.tex (deflated 52%)\n",
            "  adding: content/Result/exp2/runtime/combined25country_runtime_static.csv (deflated 34%)\n",
            "  adding: content/Result/exp2/runtime/combined25country_runtime_incremental.csv (deflated 33%)\n",
            "  adding: content/Result/exp2/runtime/combined25country_runtime_static.tex (deflated 62%)\n",
            "  adding: content/Result/exp2/combined25country_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp2/combined25country_RMSE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp2/combined25country_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp2/combined25country_MAE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp2/combined25country_RMSE_incremental_alternate_batch.csv (deflated 44%)\n",
            "  adding: content/Result/exp1/ (stored 0%)\n",
            "  adding: content/Result/exp1/France_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Switzerland_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Israel_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Indonesia_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_RMSE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Israel_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Italy_MAPE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Czechia_MAPE_incremental.csv (deflated 46%)\n",
            "  adding: content/Result/exp1/Pakistan_RMSE_incremental.tex (deflated 55%)\n",
            "  adding: content/Result/exp1/Mexico_MAE_static.tex (deflated 54%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Israel_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Italy_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Spain_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Brazil_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Israel_MAE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Canada_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Belgium_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Spain_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Ecuador_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Russia_MAE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Israel_RMSE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Germany_MAPE_incremental.csv (deflated 43%)\n",
            "  adding: content/Result/exp1/Iran_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Germany_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Spain_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Belgium_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Ecuador_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Brazil_MAPE_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/Philippines_RMSE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Spain_MAE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Mexico_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/India_MAPE_incremental.csv (deflated 43%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/United_Kingdom_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/Nepal_RMSE_incremental.csv (deflated 43%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAPE_incremental.tex (deflated 60%)\n",
            "  adding: content/Result/exp1/Germany_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Nepal_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Iraq_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/Indonesia_MAE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Switzerland_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Italy_MAE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/France_MAPE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/Germany_MAPE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Belgium_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Germany_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Switzerland_MAPE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Pakistan_MAE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Netherlands_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Ecuador_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAPE_incremental.tex (deflated 60%)\n",
            "  adding: content/Result/exp1/Canada_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Canada_MAPE_static.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Belgium_MAPE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Iraq_RMSE_incremental.csv (deflated 43%)\n",
            "  adding: content/Result/exp1/United_States_of_America_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Italy_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Nepal_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Brazil_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Spain_MAPE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Indonesia_MAPE_static.csv (deflated 43%)\n",
            "  adding: content/Result/exp1/Canada_MAE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/France_MAPE_static.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Russia_MAPE_incremental.csv (deflated 45%)\n",
            "  adding: content/Result/exp1/Israel_MAPE_incremental.tex (deflated 60%)\n",
            "  adding: content/Result/exp1/Nepal_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Romania_MAPE_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Nepal_MAPE_incremental.csv (deflated 45%)\n",
            "  adding: content/Result/exp1/Germany_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Belgium_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Pakistan_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Belgium_MAE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Mexico_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Indonesia_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Romania_MAPE_incremental.tex (deflated 61%)\n",
            "  adding: content/Result/exp1/Indonesia_RMSE_static.tex (deflated 53%)\n",
            "  adding: content/Result/exp1/Czechia_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Russia_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Switzerland_MAE_incremental.tex (deflated 55%)\n",
            "  adding: content/Result/exp1/Switzerland_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Mexico_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Czechia_MAE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Romania_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Russia_MAPE_static.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Germany_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/India_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Iran_MAPE_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAE_static.tex (deflated 53%)\n",
            "  adding: content/Result/exp1/Mexico_MAPE_incremental.tex (deflated 61%)\n",
            "  adding: content/Result/exp1/Czechia_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Pakistan_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Canada_MAPE_incremental.tex (deflated 60%)\n",
            "  adding: content/Result/exp1/Mexico_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Romania_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Indonesia_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Iran_MAPE_incremental.tex (deflated 61%)\n",
            "  adding: content/Result/exp1/Russia_MAE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Iraq_MAPE_incremental.csv (deflated 45%)\n",
            "  adding: content/Result/exp1/Ecuador_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Romania_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/Ecuador_MAPE_static.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/summary/ (stored 0%)\n",
            "  adding: content/Result/exp1/summary/top_countries_MAPE_summary_table_incremental.csv (deflated 49%)\n",
            "  adding: content/Result/exp1/summary/summary_table_combined_mean_incremental.tex (deflated 39%)\n",
            "  adding: content/Result/exp1/summary/summary_table_combined_mean_static.tex (deflated 42%)\n",
            "  adding: content/Result/exp1/summary/top_countries_MAPE_summary_table_static.csv (deflated 49%)\n",
            "  adding: content/Result/exp1/summary/top_countries_MAPE_summary_table_static.tex (deflated 62%)\n",
            "  adding: content/Result/exp1/summary/summary_table_combined_mean_static.csv (deflated 25%)\n",
            "  adding: content/Result/exp1/summary/top_countries_MAPE_summary_table_incremental.tex (deflated 62%)\n",
            "  adding: content/Result/exp1/summary/summary_table_combined_mean_incremental.csv (deflated 25%)\n",
            "  adding: content/Result/exp1/Nepal_MAE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/United_Kingdom_RMSE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Iraq_MAPE_static.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Iran_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Netherlands_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/France_RMSE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Brazil_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Ecuador_MAPE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Canada_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Pakistan_MAPE_incremental.tex (deflated 60%)\n",
            "  adding: content/Result/exp1/Pakistan_MAPE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Indonesia_MAE_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Israel_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Mexico_MAPE_incremental.csv (deflated 44%)\n",
            "  adding: content/Result/exp1/Spain_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Romania_MAE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Mexico_RMSE_static.tex (deflated 53%)\n",
            "  adding: content/Result/exp1/Germany_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Nepal_MAPE_static.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Russia_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Russia_MAE_static.tex (deflated 53%)\n",
            "  adding: content/Result/exp1/India_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Philippines_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Philippines_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Germany_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Belgium_MAPE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/Pakistan_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Netherlands_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Netherlands_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Brazil_MAPE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/Brazil_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Ecuador_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/Mexico_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Philippines_RMSE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Italy_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Brazil_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Nepal_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/Netherlands_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Russia_RMSE_incremental.csv (deflated 44%)\n",
            "  adding: content/Result/exp1/Belgium_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Indonesia_MAPE_incremental.tex (deflated 62%)\n",
            "  adding: content/Result/exp1/Italy_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Iraq_MAPE_incremental.tex (deflated 62%)\n",
            "  adding: content/Result/exp1/India_RMSE_static.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Israel_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/Philippines_MAPE_static.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Russia_MAPE_incremental.tex (deflated 61%)\n",
            "  adding: content/Result/exp1/Romania_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Pakistan_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Canada_MAPE_incremental.csv (deflated 44%)\n",
            "  adding: content/Result/exp1/Netherlands_MAE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Romania_MAPE_incremental.csv (deflated 44%)\n",
            "  adding: content/Result/exp1/Spain_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Russia_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Netherlands_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Ecuador_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Brazil_MAE_incremental.tex (deflated 55%)\n",
            "  adding: content/Result/exp1/Spain_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Germany_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Romania_RMSE_incremental.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Pakistan_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/Israel_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Switzerland_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/France_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Pakistan_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/France_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Switzerland_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/France_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Netherlands_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Canada_MAE_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/France_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Czechia_MAPE_static.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Czechia_MAE_incremental.tex (deflated 55%)\n",
            "  adding: content/Result/exp1/Czechia_RMSE_incremental.tex (deflated 55%)\n",
            "  adding: content/Result/exp1/United_States_of_America_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Ecuador_MAE_incremental.tex (deflated 55%)\n",
            "  adding: content/Result/exp1/Italy_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Russia_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Philippines_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/India_MAPE_static.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Indonesia_MAPE_incremental.csv (deflated 46%)\n",
            "  adding: content/Result/exp1/Canada_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAPE_incremental.csv (deflated 44%)\n",
            "  adding: content/Result/exp1/Spain_RMSE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Brazil_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Ecuador_MAPE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Spain_MAPE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/France_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Nepal_MAPE_incremental.tex (deflated 61%)\n",
            "  adding: content/Result/exp1/Philippines_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Mexico_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Iran_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Mexico_MAPE_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/Iraq_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Brazil_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Czechia_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Philippines_MAE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Italy_MAPE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/India_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Iran_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Germany_MAPE_static.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Israel_MAPE_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/Czechia_MAPE_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/Czechia_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Romania_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Philippines_MAPE_incremental.csv (deflated 44%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAPE_static.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Romania_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Philippines_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Belgium_MAPE_static.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Czechia_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/India_MAPE_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/United_States_of_America_RMSE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/France_MAPE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Iran_MAE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAPE_static.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAPE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/United_Kingdom_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/runtime/ (stored 0%)\n",
            "  adding: content/Result/exp1/runtime/Netherlands_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Iraq_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Switzerland_runtime_static.tex (deflated 66%)\n",
            "  adding: content/Result/exp1/runtime/Romania_runtime_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/runtime/Nepal_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Ecuador_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/India_runtime_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/runtime/India_runtime_static.csv (deflated 43%)\n",
            "  adding: content/Result/exp1/runtime/Mexico_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/United_States_of_America_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Canada_runtime_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/runtime/Romania_runtime_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/runtime/Belgium_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Indonesia_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Iraq_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Pakistan_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Philippines_runtime_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/runtime/Indonesia_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/United_States_of_America_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Nepal_runtime_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/runtime/Belgium_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Spain_runtime_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/runtime/Netherlands_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/India_runtime_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/runtime/Italy_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Germany_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/India_runtime_static.tex (deflated 66%)\n",
            "  adding: content/Result/exp1/runtime/Romania_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Israel_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/United_Kingdom_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Israel_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Nepal_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/United_Kingdom_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Germany_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/United_Arab_Emirates_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Iran_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/France_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Canada_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Nepal_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Italy_runtime_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/runtime/Netherlands_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Czechia_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Pakistan_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Iran_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Iraq_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Iran_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Spain_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Ecuador_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Ecuador_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Germany_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/United_Kingdom_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Pakistan_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Romania_runtime_static.tex (deflated 66%)\n",
            "  adding: content/Result/exp1/runtime/United_States_of_America_runtime_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/runtime/Italy_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Czechia_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/France_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Switzerland_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Brazil_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Brazil_runtime_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/runtime/Iran_runtime_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/runtime/Spain_runtime_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/runtime/Switzerland_runtime_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/runtime/Russia_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/United_States_of_America_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Belgium_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Ecuador_runtime_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/runtime/United_Arab_Emirates_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Israel_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Germany_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/United_Arab_Emirates_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Mexico_runtime_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/runtime/France_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Russia_runtime_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/runtime/Mexico_runtime_static.tex (deflated 66%)\n",
            "  adding: content/Result/exp1/runtime/Russia_runtime_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/runtime/Canada_runtime_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/runtime/Israel_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Czechia_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Iraq_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Belgium_runtime_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/runtime/France_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Pakistan_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Philippines_runtime_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/runtime/Czechia_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/runtime/Switzerland_runtime_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/runtime/United_Kingdom_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Indonesia_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Netherlands_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Brazil_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/United_Arab_Emirates_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Russia_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Italy_runtime_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/runtime/Brazil_runtime_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/runtime/Spain_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Mexico_runtime_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/runtime/Philippines_runtime_static.tex (deflated 65%)\n",
            "  adding: content/Result/exp1/runtime/Philippines_runtime_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/runtime/Canada_runtime_static.tex (deflated 64%)\n",
            "  adding: content/Result/exp1/runtime/Indonesia_runtime_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Switzerland_MAPE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Romania_MAPE_static.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Ecuador_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Indonesia_MAPE_static.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/Switzerland_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Canada_MAPE_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/India_MAPE_incremental.tex (deflated 60%)\n",
            "  adding: content/Result/exp1/Iraq_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Iran_RMSE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Brazil_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Indonesia_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Iran_MAPE_incremental.csv (deflated 45%)\n",
            "  adding: content/Result/exp1/Iraq_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_MAPE_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/Belgium_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Belgium_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Spain_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Netherlands_MAPE_static.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Israel_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/United_States_of_America_MAPE_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/Belgium_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Italy_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Philippines_MAPE_incremental.tex (deflated 61%)\n",
            "  adding: content/Result/exp1/Canada_MAE_static.csv (deflated 36%)\n",
            "  adding: content/Result/exp1/United_Kingdom_RMSE_static.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/India_RMSE_incremental.tex (deflated 55%)\n",
            "  adding: content/Result/exp1/Ecuador_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Iran_MAPE_static.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Switzerland_RMSE_incremental.tex (deflated 56%)\n",
            "  adding: content/Result/exp1/Iraq_MAE_incremental.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/India_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/United_Kingdom_MAPE_incremental.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/Iraq_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/France_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Iran_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Nepal_RMSE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Italy_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Netherlands_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Mexico_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Philippines_MAPE_static.csv (deflated 42%)\n",
            "  adding: content/Result/exp1/United_States_of_America_RMSE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/Pakistan_RMSE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/Indonesia_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Germany_MAE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Iraq_MAPE_static.csv (deflated 41%)\n",
            "  adding: content/Result/exp1/Switzerland_MAPE_incremental.csv (deflated 40%)\n",
            "  adding: content/Result/exp1/Switzerland_MAE_incremental.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/United_Arab_Emirates_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Czechia_MAPE_incremental.tex (deflated 61%)\n",
            "  adding: content/Result/exp1/Russia_RMSE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Netherlands_MAPE_incremental.tex (deflated 59%)\n",
            "  adding: content/Result/exp1/Nepal_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/France_RMSE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/Pakistan_RMSE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Iraq_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Italy_RMSE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Netherlands_MAE_incremental.csv (deflated 38%)\n",
            "  adding: content/Result/exp1/Brazil_RMSE_static.tex (deflated 53%)\n",
            "  adding: content/Result/exp1/Iran_RMSE_incremental.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Canada_RMSE_static.tex (deflated 52%)\n",
            "  adding: content/Result/exp1/Spain_MAPE_static.csv (deflated 39%)\n",
            "  adding: content/Result/exp1/Israel_MAE_incremental.tex (deflated 57%)\n",
            "  adding: content/Result/exp1/Italy_MAPE_incremental.tex (deflated 58%)\n",
            "  adding: content/Result/exp1/India_MAE_static.csv (deflated 37%)\n",
            "  adding: content/Result/exp1/Nepal_MAE_static.tex (deflated 51%)\n",
            "  adding: content/Result/exp1/India_MAE_incremental.tex (deflated 55%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_08c75791-6148-4580-bade-5ddd1b9dfdfa\", \"Result.zip\", 233109)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X-m0P0s0hKNA",
        "outputId": "c4d6d7c8-7e99-4f73-ea3e-137ae2bcd0bf"
      },
      "source": [
        "\"\"\"\n",
        "!zip -r /content/csv_files.zip /content/csv_files\n",
        "from google.colab import files\n",
        "files.download(\"/content/csv_files.zip\")\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/csv_files/ (stored 0%)\n",
            "  adding: content/csv_files/processed/ (stored 0%)\n",
            "  adding: content/csv_files/processed/Monaco.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/Denmark.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Indonesia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Malaysia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Japan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/United_States_of_America.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/Finland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Australia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Taiwan.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/Vietnam.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/Ireland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Iraq.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Lithuania.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Sri_Lanka.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Italy.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Afghanistan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Mexico.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Pakistan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Oman.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Azerbaijan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Germany.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Philippines.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Thailand.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/Iran.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Israel.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Brazil.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Russia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Luxembourg.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Canada.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Czechia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Singapore.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Qatar.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Belarus.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/San_Marino.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/South_Korea.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/United_Arab_Emirates.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Spain.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Lebanon.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Switzerland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Belgium.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Bahrain.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Georgia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/United_Kingdom.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Ecuador.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Norway.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Dominican_Republic.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Netherlands.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Estonia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Algeria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/China.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Sweden.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/New_Zealand.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/Egypt.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Cambodia.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed/Greece.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/India.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Romania.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Croatia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Armenia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/North_Macedonia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Iceland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Nepal.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Austria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/France.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Nigeria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed/Kuwait.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/ (stored 0%)\n",
            "  adding: content/csv_files/processed_null/Monaco.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Denmark.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Indonesia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Mali.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Malaysia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Japan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Uzbekistan.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/United_States_of_America.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Uganda.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Finland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Namibia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Australia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Equatorial_Guinea.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Aruba.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Kazakhstan.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Chile.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Taiwan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Tajikistan.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Jordan.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Vietnam.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Ukraine.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Ireland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Antigua_and_Barbuda.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Iraq.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Tunisia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Northern_Mariana_Islands.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Jamaica.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Malta.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Sudan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Lithuania.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Sri_Lanka.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Italy.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Benin.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Cyprus.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Turkey.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Trinidad_and_Tobago.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Afghanistan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Mexico.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Montserrat.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Falkland_Islands_(Malvinas).csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Pakistan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Syria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Zimbabwe.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Oman.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Jersey.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Bhutan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Azerbaijan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/French_Polynesia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/South_Africa.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Liechtenstein.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Somalia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Andorra.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Ethiopia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Germany.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/South_Sudan.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Philippines.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Bahamas.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Rwanda.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Thailand.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Djibouti.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Laos.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Bolivia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Cote_dIvoire.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Burkina_Faso.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Iran.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Greenland.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Eritrea.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Israel.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Brazil.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Russia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Faroe_Islands.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Luxembourg.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Senegal.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Saint_Lucia.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Chad.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Holy_See.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Bermuda.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Serbia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Poland.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Angola.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Slovenia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Canada.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Slovakia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Czechia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Singapore.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Eswatini.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Marshall_Islands.csv (deflated 80%)\n",
            "  adding: content/csv_files/processed_null/Honduras.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Haiti.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Qatar.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Myanmar.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Barbados.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Latvia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Guinea.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Saudi_Arabia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Timor_Leste.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Montenegro.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Belarus.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Isle_of_Man.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Mauritania.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Fiji.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/San_Marino.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/South_Korea.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Guernsey.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/United_States_Virgin_Islands.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Saint_Kitts_and_Nevis.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/United_Arab_Emirates.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Spain.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Lebanon.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Bonaire, Saint Eustatius and Saba.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Seychelles.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Wallis_and_Futuna.csv (deflated 86%)\n",
            "  adding: content/csv_files/processed_null/Cuba.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Saint_Vincent_and_the_Grenadines.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Guyana.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Switzerland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Paraguay.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Belgium.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Mongolia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Bahrain.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Guinea_Bissau.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Dominica.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Georgia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Morocco.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Kosovo.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Moldova.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Western_Sahara.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Mauritius.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Niger.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Turks_and_Caicos_islands.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Kyrgyzstan.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Bangladesh.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/United_Kingdom.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Gabon.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Ecuador.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Sierra_Leone.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Norway.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Costa_Rica.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Dominican_Republic.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Netherlands.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Botswana.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/New_Caledonia.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Estonia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Portugal.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Algeria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/China.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Colombia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Peru.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Grenada.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Argentina.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Anguilla.csv (deflated 96%)\n",
            "  adding: content/csv_files/processed_null/Sweden.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Congo.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Bosnia_and_Herzegovina.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Cases_on_an_international_conveyance_Japan.csv (deflated 92%)\n",
            "  adding: content/csv_files/processed_null/Zambia.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Cayman_Islands.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/New_Zealand.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Egypt.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Guatemala.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Nicaragua.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Ghana.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Mozambique.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Kenya.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Hungary.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/United_Republic_of_Tanzania.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Cambodia.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Greece.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Comoros.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/India.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Gibraltar.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Romania.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Togo.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Croatia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Democratic_Republic_of_the_Congo.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Maldives.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Armenia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Gambia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Papua_New_Guinea.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Guam.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Belize.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Uruguay.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/North_Macedonia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Curaçao.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Cameroon.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Cape_Verde.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Sint_Maarten.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Panama.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Iceland.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Nepal.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Venezuela.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Austria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Bulgaria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/France.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Burundi.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Suriname.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/El_Salvador.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Madagascar.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/British_Virgin_Islands.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Solomon_Islands.csv (deflated 86%)\n",
            "  adding: content/csv_files/processed_null/Liberia.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Nigeria.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Sao_Tome_and_Principe.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Brunei_Darussalam.csv (deflated 95%)\n",
            "  adding: content/csv_files/processed_null/Lesotho.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Albania.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Palestine.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Malawi.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Kuwait.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Libya.csv (deflated 93%)\n",
            "  adding: content/csv_files/processed_null/Central_African_Republic.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Yemen.csv (deflated 94%)\n",
            "  adding: content/csv_files/processed_null/Puerto_Rico.csv (deflated 93%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c68c5803-b1e9-4972-a4bd-e535276b249f\", \"csv_files.zip\", 1053581)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDICI28viZYg"
      },
      "source": [
        "# Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DR7Z02IGVCk"
      },
      "source": [
        "## Bar Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkebu9hm0GMy"
      },
      "source": [
        "\"\"\"\n",
        "def get_filenames(pattern):\n",
        "  filenames=[]\n",
        "  for name in glob.glob(f\"{exp1_path}/*{pattern}*.csv\"):\n",
        "    filenames.append(name)\n",
        "  return filenames\n",
        "\n",
        "def get_countryname(filename):\n",
        "  country_name = filename.split('/')[-1].split('.')[0].split('_')[:-2]\n",
        "  return '_'.join(country_name)\n",
        "\n",
        "def get_error_stat_name(pattern):\n",
        "  return pattern.split('_')\n",
        "\n",
        "def plot_graph(df, statistics, metric):\n",
        "  sns.set_theme(style=\"darkgrid\")\n",
        "  df.plot(kind='bar',figsize=(12,10))\n",
        "  plt.title(f'{statistics}')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yscale('log')\n",
        "  plt.ylabel(metric)\n",
        "  ax = plt.gca()\n",
        "  ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))  # Changing scientific notation to plain text\n",
        "  plt.show()\n",
        "\n",
        "def  get_list_of_error_metric_per_country(filenames):\n",
        "  metric_dataframes_list = []\n",
        "  for filename in filenames:\n",
        "    country_name = get_countryname(filename)\n",
        "    df = pd.read_csv(filename)\n",
        "    df = df.set_index('Unnamed: 0')\n",
        "    df['country'] = country_name\n",
        "    metric_dataframes_list.append(df)\n",
        "  return metric_dataframes_list\n",
        "\n",
        "\n",
        "def get_mean_error_dataframes(metric_dataframes_list, learner_type):\n",
        "  mean_error_dataframes=[]\n",
        "  start_row = 'mean'\n",
        "  if learner_type == 'static':\n",
        "    start_col = 'RandomForest'\n",
        "  elif learner_type=='incremental':\n",
        "    start_col='HT_Reg'\n",
        "\n",
        "  for i in range(len(metric_dataframes_list)):\n",
        "    row = pd.DataFrame([metric_dataframes_list[i].loc[start_row][start_col:]])\n",
        "    mean_error_dataframes.append(row) # Passing values as a list for grouped bar chart\n",
        "    \n",
        "  result_df = pd.concat(mean_error_dataframes, ignore_index=True)\n",
        "  result_df.set_index('country', inplace=True)\n",
        "\n",
        "  return result_df\n",
        "\n",
        "filename_patterns = ['MAE_static','MAPE_static','MAE_incremental','MAPE_incremental']\n",
        "error_metric_mapper = {\n",
        "    'MAE': 'Mean Absolute Error(MAE)', \n",
        "    'MAPE':'Mean Absolute Percentage Error(MAPE)',\n",
        "    'RMSE':'Root Mean Square Error(RMSE)'}\n",
        "\n",
        "for pattern in filename_patterns:\n",
        "  filenames = get_filenames(pattern)\n",
        "  stat,learner_type = get_error_stat_name(pattern)\n",
        "  metric_dataframes_list = get_list_of_error_metric_per_country(filenames)\n",
        "  mean_error_dataframes = get_mean_error_dataframes(metric_dataframes_list,learner_type)\n",
        "  plot_graph(mean_error_dataframes,pattern,error_metric_mapper[stat])\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otlR4XEz2Kmf"
      },
      "source": [
        "\"\"\"!zip -r /content/summary_1.zip /content/Result/exp1/summary\n",
        "!zip -r /content/summary_2.zip /content/Result/exp2/summary\n",
        "from google.colab import files\n",
        "files.download(\"/content/summary_1.zip\")\n",
        "files.download(\"/content/summary_2.zip\")\"\"\"\n",
        "\n",
        "\"\"\"!zip -r /content/Result.zip /content/Result\n",
        "from google.colab import files\n",
        "files.download(\"/content/Result.zip\")\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmQXDjmrXAiS"
      },
      "source": [
        "!zip -r /content/csv_files.zip /content/csv_files\n",
        "from google.colab import files\n",
        "files.download(\"/content/csv_files.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQRkdX0oGaec"
      },
      "source": [
        "## Box Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oL2TxDdHRwM"
      },
      "source": [
        "def preprocess_data(df, metric_type, learner_type, col_mapper):\n",
        "  # Only pretrain days records are required not the mean row\n",
        "  df.drop(['mean'], axis=1, inplace=True)\n",
        "\n",
        "  # Renaming the Algorithm Columns\n",
        "  df.rename(columns={'Unnamed: 0': 'Algorithms'}, inplace=True)  \n",
        "\n",
        "  # Dropping first two rows: \"EvaluationMeasurement\" & \"PretrainDays\"\n",
        "  df.drop([0,1], axis=0, inplace=True)  \n",
        "\n",
        "  # Renaming columns based on mapper\n",
        "  df['Algorithms'].replace(col_mapper, inplace=True)  \n",
        "\n",
        "  # Melting the dataframe based on 'Algorithms'\n",
        "  df_melt = df.melt(id_vars=['Algorithms'])  \n",
        "\n",
        "  # Dropping unwanted varibale column(created bcoz of index)\n",
        "  df_melt.drop('variable', axis=1, inplace=True)  \n",
        "\n",
        "  # Renaming the value column by metric type\n",
        "  df_melt.rename(columns={'value':metric_type}, inplace=True)  \n",
        "\n",
        "  # Converting to float value bcoz by default the values are of type object\n",
        "  df_melt[metric_type] = df_melt[metric_type].astype('float64')  \n",
        "  \n",
        "  df_melt['Learner Type']= learner_type  # Adding the learner type\n",
        "\n",
        "  return df_melt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKugGWSVBPH6"
      },
      "source": [
        "def order_by_median(df,reverse = False):\n",
        "    grouped_df = df.groupby('Algorithms')\n",
        "    algo_medians = {}\n",
        "    for cur_group in grouped_df.groups.keys():\n",
        "        df_cur_grp = grouped_df.get_group(cur_group)\n",
        "        algo_medians[cur_group] = df_cur_grp['MAPE'].median()\n",
        "    sorted_algo_medians = dict(sorted(algo_medians.items(), key=lambda kv: kv[1], reverse=reverse))\n",
        "    return list(sorted_algo_medians.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-GmwJSgmvw6"
      },
      "source": [
        "# If value is less than zero return float value otherwise an integer value\n",
        "def format_values(y_val,pos):\n",
        "    if y_val < 1:\n",
        "        return format(float(y_val))\n",
        "    else:\n",
        "        return format(int(y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1676SLhHSe_"
      },
      "source": [
        "def draw_boxplot(df, save_filename):\n",
        "  colors = ['#1f77b4', '#ff7f0e','#2ca02c']\n",
        "  # Setting custom color palette\n",
        "  sns.set_palette(sns.color_palette(colors))\n",
        "\n",
        "  plt.figure(figsize=(10,6),dpi=90)\n",
        "  ordered_algo_list = order_by_median(df, reverse= False)\n",
        "\n",
        "  ax = sns.boxplot(x=\"Algorithms\", y=metric_type, hue='Learner Type', data=df, order=ordered_algo_list, dodge =False, width=0.5) #, hue_order=['Incremental','Static'])\n",
        "  ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
        "  ax.set(yscale = 'log')\n",
        "  ax.set_ylim(top =100)\n",
        "  ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(format_values))  # lambda x, p: format(int(x), ',')\n",
        "  ax.tick_params(axis='both', which='major', labelsize=16)\n",
        "  ax.set_ylabel(metric_type,fontsize=18)\n",
        "  ax.legend(loc='upper left')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(f'{box_plot_path}/{save_filename}.pdf')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKWFUip8HZd3"
      },
      "source": [
        "def read_preprocess_plot_graph(filenames, col_mapper, save_filename, metric_type='MAPE'):\n",
        "  metric_type = metric_type\n",
        "  frames = []\n",
        "  for filename in filenames:\n",
        "      if 'static' in filename:\n",
        "          learner_type = 'Static'\n",
        "      else:\n",
        "          learner_type = 'Incremental'\n",
        "\n",
        "      df = pd.read_csv(filename)\n",
        "      df_melt = preprocess_data(df, metric_type, learner_type, col_mapper)\n",
        "      frames.append(df_melt)\n",
        "\n",
        "  final_df = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "  # Updating LSTM learner type as Sequential\n",
        "  final_df.loc[final_df['Algorithms'] == 'LSTM', 'Learner Type'] = 'Sequential'\n",
        "\n",
        "  # Sorting final dataframe\n",
        "  final_df = final_df.sort_values(by=['MAPE'])\n",
        "  \n",
        "  draw_boxplot(final_df, save_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeltxQL_JnUH"
      },
      "source": [
        "\n",
        "col_mapper = {'HT_Reg':'Hoeffding Trees',\n",
        "              'HAT_Reg':'Hoeffding Adapt Tr',\n",
        "              'ARF_Reg':'Adaptive RF',\n",
        "              'PA_Reg':'Pass Agg Regr',\n",
        "              'RandomForest':'Random Forest',\n",
        "              'GradientBoosting': 'Gradient Boosting',\n",
        "              'DecisionTree': 'Decision Trees',\n",
        "              'LinearSVR': 'Linear SVR',\n",
        "              'BayesianRidge':'Bayesian Ridge'\n",
        "              }\n",
        "\n",
        "metric_type = 'MAPE'\n",
        "exp1_filenames = glob.glob(f'{exp1_path}/*{metric_type}*.csv')\n",
        "exp2_filenames = []\n",
        "\n",
        "for filename in glob.glob(f'{exp2_path}/*{metric_type}*.csv'):\n",
        "  if 'alternate' in filename or 'static' in filename:\n",
        "    exp2_filenames.append(filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uXMLcXK_ZkQ"
      },
      "source": [
        "'''\n",
        "col_mapper = {'HT_Reg':'HT',\n",
        "              'HAT_Reg':'HAT',\n",
        "              'ARF_Reg':'ARF',\n",
        "              'PA_Reg':'PA',\n",
        "              'RandomForest':'Random Forest',\n",
        "              'GradientBoosting': 'Gradient Boosting',\n",
        "              'DecisionTree': 'Decision Tree',\n",
        "              'LinearSVR': 'Linear SVR',\n",
        "              'BayesianRidge':'Bayesian Ridge',\n",
        "              'LSTM': 'LSTM'\n",
        "              }\n",
        "\n",
        "metric_type = 'MAPE'\n",
        "exp1_filenames = glob.glob(f'{exp1_path}/*{metric_type}*.csv')\n",
        "exp2_filenames = []\n",
        "\n",
        "for filename in glob.glob(f'{exp2_path}/*{metric_type}*.csv'):\n",
        "  if 'alternate' in filename or 'static' in filename:\n",
        "    exp2_filenames.append(filename)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "yFw4A4RoXGhs",
        "outputId": "8157bf23-36f6-4316-db2b-2f48bd783a51"
      },
      "source": [
        "save_filename = 'fig1'\n",
        "read_preprocess_plot_graph(exp1_filenames, col_mapper, save_filename, metric_type)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 900x540 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "r9MYJNUYjvfZ",
        "outputId": "4a26e0a8-dd3c-4797-c240-09da4241c332"
      },
      "source": [
        "save_filename = 'fig2'\n",
        "read_preprocess_plot_graph(exp2_filenames, col_mapper, save_filename,  metric_type)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 900x540 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "eA9pc2xtzDUL",
        "outputId": "300ec9f4-e058-494d-c0d5-df1412dd6d7c"
      },
      "source": [
        "!zip -r /content/Plots.zip /content/Plots\n",
        "from google.colab import files\n",
        "files.download(\"/content/Plots.zip\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: content/Plots/ (stored 0%)\n",
            "updating: content/Plots/boxplots/ (stored 0%)\n",
            "updating: content/Plots/boxplots/fig2.pdf (deflated 30%)\n",
            "updating: content/Plots/boxplots/fig1.pdf (deflated 36%)\n",
            "updating: content/Plots/barplot/ (stored 0%)\n",
            "updating: content/Plots/barplot/top_selected_country_cases.pdf (deflated 32%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5bf61ebf-84a2-462a-8dab-7697d67fdf5d\", \"Plots.zip\", 35625)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPCN6qhVzOLb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
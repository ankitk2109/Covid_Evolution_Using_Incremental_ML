{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Incremental_learning_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riu9BVr_-nl2"
      },
      "source": [
        "# Covid Dataset\n",
        "\n",
        "**Required Dataset features and target**\n",
        "\n",
        "The dataset has 53 columns; 1 to represent the country, 1 to represent the day (it will be an integer), 50 floats to represent the positive cases of the 50 previous days, and 1 column to represent the output that is the average of a full week of cases.\n",
        "\n",
        "![required_features.jpg](https://drive.google.com/uc?id=1smUwSHRwMT8h-M8kjG3ymmxdhQbe1HvY)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhjnZH15MMQA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "2c06f560-31f9-4676-e479-453e184a49bc"
      },
      "source": [
        "# CHANGED BY ASC.\n",
        "\n",
        "# Installing Incremental learner: Scikit-Multiflow\n",
        "!pip install scikit-multiflow\n",
        "!gdown https://drive.google.com/uc?id=1f5GgBqjAsTUFnubHODmfqn6qiNyDBqIw\n",
        "!unzip /content/src.zip -d /content/src\n",
        "!cp -r /content/src/src /content/\n",
        "!rm -r /content/src/src\n",
        "\n",
        "# Creating a seperate directory to store all csv's\n",
        "! mkdir -p /content/csv_files\n",
        "! mkdir -p /content/csv_files/processed_null\n",
        "! mkdir -p /content/csv_files/processed\n",
        "! mkdir -p /content/Result/exp1\n",
        "! mkdir -p /content/Result/exp2\n",
        "! mkdir -p /content/Result/exp1/runtime\n",
        "! mkdir -p /content/Result/exp2/runtime\n",
        "! mkdir -p /content/Result/exp1/summary\n",
        "! mkdir -p /content/Result/exp2/summary\n",
        "# Download the zip file\n",
        "\"\"\"\n",
        "!zip -r /content/file.zip /content/csv_files\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-multiflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/ac/5f4675aa1e9f4c2a1d139241b50288a17e4048f5bad3484b18efc6acc4b8/scikit_multiflow-0.5.3-cp36-cp36m-manylinux2010_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (1.1.4)\n",
            "Requirement already satisfied: sortedcontainers>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (2.2.2)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (1.18.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.3->scikit-multiflow) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.3->scikit-multiflow) (2018.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (2.4.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->scikit-multiflow) (0.17.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.3->scikit-multiflow) (1.15.0)\n",
            "Installing collected packages: scikit-multiflow\n",
            "Successfully installed scikit-multiflow-0.5.3\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1f5GgBqjAsTUFnubHODmfqn6qiNyDBqIw\n",
            "To: /content/src.zip\n",
            "100% 26.1k/26.1k [00:00<00:00, 46.2MB/s]\n",
            "Archive:  /content/src.zip\n",
            "  inflating: /content/src/src/_classification_performance_evaluator.py  \n",
            "  inflating: /content/src/src/base_evaluator.py  \n",
            "  inflating: /content/src/src/constants.py  \n",
            "  inflating: /content/src/src/evaluate_prequential.py  \n",
            "  inflating: /content/src/src/evaluation_data_buffer.py  \n",
            "  inflating: /content/src/src/measure_collection.py  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!zip -r /content/file.zip /content/csv_files\\nfrom google.colab import files\\nfiles.download(\"/content/file.zip\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqTR_N3fpMON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516e5705-9d32-42c3-b75e-6fe0a1f74f04"
      },
      "source": [
        "#!pip uninstall keras\n",
        "#!pip uninstall tensorflow\n",
        "\n",
        "!pip install keras==2.3.1\n",
        "!pip install tensorflow==2.1.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\r\u001b[K     |▉                               | 10kB 14.5MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 5.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 6.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.15.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
            "\u001b[?25hInstalling collected packages: keras-applications, keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.3.1 keras-applications-1.0.8\n",
            "Collecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 29kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.33.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.18.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 40.2MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.2)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 38.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.1.0) (50.3.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.3.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.4.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=38fe88d1ece496ef5283316f55a0219f1f4a2deb50174e3d925f080688bd87f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, gast, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvDISgy5CUOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d83dec89-e859-4271-9279-485b27f5d844"
      },
      "source": [
        "# General Imports \n",
        "import pandas as pd\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "import warnings\n",
        "from pandas.core.common import SettingWithCopyWarning\n",
        "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme(style='darkgrid')\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "# Imports for incremental learner\n",
        "from skmultiflow.data import DataStream\n",
        "from skmultiflow.trees import HoeffdingTreeRegressor\n",
        "from src.evaluate_prequential import EvaluatePrequential\n",
        "from skmultiflow.meta import AdaptiveRandomForestRegressor\n",
        "from skmultiflow.trees import HoeffdingAdaptiveTreeRegressor\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.linear_model import PassiveAggressiveRegressor\n",
        "\n",
        "# Imports for static Learner\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from time import perf_counter as pc_timer\n",
        "from functools import wraps\n",
        "\n",
        "import keras\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "#from tensorflow.keras import Sequential\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# For significance tests\n",
        "from scipy.stats import normaltest\n",
        "from scipy import stats \n",
        "# pd.set_option('display.max_colwidth', 500)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHwu0MJOCm68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "08f5781f-52e3-47aa-f964-83301562df4e"
      },
      "source": [
        "#url = 'https://drive.google.com/file/d/1e7NsptfEFLG2gGLykYlrzjNbDJLiRbGm/view?usp=sharing'\n",
        "url = 'https://drive.google.com/file/d/1VH-nkePskK3gT6U5qkoOP-0hFT4beszC/view?usp=sharing'\n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "df = pd.read_csv(path)\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dateRep</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "      <th>cases</th>\n",
              "      <th>deaths</th>\n",
              "      <th>countriesAndTerritories</th>\n",
              "      <th>geoId</th>\n",
              "      <th>countryterritoryCode</th>\n",
              "      <th>popData2019</th>\n",
              "      <th>continentExp</th>\n",
              "      <th>Cumulative_number_for_14_days_of_COVID-19_cases_per_100000</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>02/11/2020</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>2020</td>\n",
              "      <td>132</td>\n",
              "      <td>5</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>3.767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01/11/2020</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>2020</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>3.575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31/10/2020</td>\n",
              "      <td>31</td>\n",
              "      <td>10</td>\n",
              "      <td>2020</td>\n",
              "      <td>157</td>\n",
              "      <td>4</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>3.554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30/10/2020</td>\n",
              "      <td>30</td>\n",
              "      <td>10</td>\n",
              "      <td>2020</td>\n",
              "      <td>123</td>\n",
              "      <td>3</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>3.265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>29/10/2020</td>\n",
              "      <td>29</td>\n",
              "      <td>10</td>\n",
              "      <td>2020</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>AF</td>\n",
              "      <td>AFG</td>\n",
              "      <td>38041757.000</td>\n",
              "      <td>Asia</td>\n",
              "      <td>2.942</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      dateRep  ...  Cumulative_number_for_14_days_of_COVID-19_cases_per_100000\n",
              "0  02/11/2020  ...                                              3.767         \n",
              "1  01/11/2020  ...                                              3.575         \n",
              "2  31/10/2020  ...                                              3.554         \n",
              "3  30/10/2020  ...                                              3.265         \n",
              "4  29/10/2020  ...                                              2.942         \n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6NQxu_LuoTV"
      },
      "source": [
        "# Grouping countries together for analysis\n",
        "total_countries = df['countriesAndTerritories'].unique()\n",
        "df_grouped = df.groupby('countriesAndTerritories')\n",
        "pretrain_days = [30,60,90,120,150,180]  # List of pretrain days\n",
        "valid_countries = []\n",
        "decimal = 3  # Specify the scale of decimal places \n",
        "error_metrics = ['MAE','MAPE', 'RMSE']\n",
        "\n",
        "# Setting path variables for both experiments\n",
        "csv_processed_path = '/content/csv_files/processed'\n",
        "csv_processed_with_null_path = '/content/csv_files/processed_null'\n",
        "exp1_path = '/content/Result/exp1'\n",
        "exp2_path = '/content/Result/exp2'\n",
        "exp1_runtime_path = '/content/Result/exp1/runtime'\n",
        "exp2_runtime_path = '/content/Result/exp2/runtime'\n",
        "exp1_summary_path = '/content/Result/exp1/summary'\n",
        "exp2_summary_path = '/content/Result/exp2/summary'\n",
        "\n",
        "\n",
        "# Top countries to select for experiment 1\n",
        "Number_of_countries = 2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oubf_WqnATmM"
      },
      "source": [
        "## Feature Set with Individual Countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0vj8668gNrP"
      },
      "source": [
        "# Create lags\n",
        "def create_features_with_lags(df):\n",
        "  for i in range(89, 0, -1):  # Loop in reverse order for creating ordered lags eg: cases_t-10, cases_t-9... cases_t-1. t=current cases\n",
        "    df[f'cases_t-{i}'] = df['cases'].shift(i, axis=0)\n",
        "  return df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G23w6M6HdyEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a89d8163-ea09-4b7e-ce98-ba8666a161cd"
      },
      "source": [
        "# Pre-Processing dataset and saving them into csv's.\n",
        "for country in total_countries:\n",
        "  df = df_grouped.get_group(country)\n",
        "\n",
        "  # Selecting required features\n",
        "  df= df[['dateRep','cases','countriesAndTerritories']]\n",
        "\n",
        "  # Rename features\n",
        "  df.rename(columns={'countriesAndTerritories':'country', 'dateRep':'date'}, inplace=True)\n",
        "\n",
        "  # Convert to date, sort and set index\n",
        "  df['date'] = pd.to_datetime(df['date'],format='%d/%m/%Y')\n",
        "  df.sort_values('date', inplace=True)\n",
        "  df.set_index('date', inplace=True)\n",
        "\n",
        "  # Adding feature\n",
        "  df['day_no']= pd.Series([i for i in range(1,len(df)+1)], index=df.index)\n",
        "\n",
        "  # Reordering features\n",
        "  df = df[['day_no', 'country','cases']]\n",
        "\n",
        "  # Adding features through lags\n",
        "  df = create_features_with_lags(df)\n",
        "\n",
        "  # Creating target with last 10 days cases\n",
        "  df['target'] = df.iloc[:,[2]+[i*-1 for i in range(1,10)]].mean(axis=1)\n",
        "\n",
        "  # Dropping mid columns\n",
        "  drop_columns = list(df.loc[:,'cases_t-39':'cases_t-1'].columns)  #list(df.loc[:,'cases_t-38':'cases_t-1'].columns)\n",
        "  df.drop(drop_columns, axis=1, inplace=True)\n",
        "\n",
        "  # Country name\n",
        "  filename = df['country'].unique()[0]\n",
        "\n",
        "  # Saving file\n",
        "  df.to_csv(f'{csv_processed_with_null_path}/{filename}.csv')\n",
        "\n",
        "  # Dropping null records\n",
        "  df.dropna(how='any', axis=0, inplace=True)\n",
        "\n",
        "  # Valid countries that have records more than max of pretrain\n",
        "  if len(df)>max(pretrain_days):\n",
        "    valid_countries.append(country)  \n",
        "    df.to_csv(f'{csv_processed_path}/{filename}.csv')\n",
        "  \n",
        "print('Done!')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMZCoXFwPsHA"
      },
      "source": [
        "## Total cases of top selected countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTQjGxITEeMD"
      },
      "source": [
        "# Added just for plots. Remove later\n",
        "Number_of_countries = 25"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDWlSV8OJUQP"
      },
      "source": [
        "# Replaces underscore from country names\n",
        "def format_names(list_countries):\n",
        "  updated_country_list = []\n",
        "  for country_name in list_countries:\n",
        "    updated_country_list.append(country_name.replace(\"_\",\" \"))\n",
        "  return updated_country_list"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQfWz6LlAyHf"
      },
      "source": [
        "# A dictionary of all countries\n",
        "dict_countries = Counter(valid_countries)\n",
        "\n",
        "for country in valid_countries:\n",
        "  dict_countries[country] = df_grouped.get_group(country)['cases'].sum()\n",
        "\n",
        "# Select top_countries and order(Ascending/Decending) \n",
        "top_countries = sorted(dict_countries.items(), key=lambda dict_countries: dict_countries[1], reverse=True) [0:Number_of_countries]\n",
        "\n",
        "# Creating dataframe of top selected countries\n",
        "df_top_countries = pd.DataFrame.from_dict(dict(top_countries), orient='index', columns=['Total Cases'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_U3MIv16lLfX",
        "outputId": "a61975e7-6cb8-4d52-f8ed-50f5919836a1"
      },
      "source": [
        "# Plotting graph\n",
        "sns.set_theme(style='white')\n",
        "plt.figure(figsize=(12,16))\n",
        "\n",
        "#plt.bar(df_top_countries.index, df_top_countries['Total Cases'].values)\n",
        "top_countries_list = format_names(df_top_countries.index)\n",
        "plt.barh(df_top_countries.index[::-1], df_top_countries['Total Cases'].values[::-1]) # Reversing the order to have heighest values at the top of bar chart\n",
        "\n",
        "#plt.axvline(Number_of_countries-0.5, 0,1, ls='--', c='black')\n",
        "#plt.axhline(y=(Number_of_countries-0.5), ls='--', c='black')\n",
        "\n",
        "plt.title(f'Top {len(top_countries)} Countries with Most Cases')\n",
        "\n",
        "plt.xscale('log')\n",
        "ax = plt.axes() # for updating \n",
        "ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "\n",
        "#plt.gcf().axes[0].yaxis.get_major_formatter().set_scientific(False)  # To get labels in plain text \n",
        "#plt.xticks(rotation=90)\n",
        "plt.margins(y=0)\n",
        "ax.tick_params(axis='both', which='major', labelsize=14)\n",
        "plt.xlabel('Number of Cases')\n",
        "plt.ylabel('Countries')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x1152 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbgnoDsBEsoQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "65de8068-9250-485f-b711-f6fbc14ce80d"
      },
      "source": [
        "\"\"\"\n",
        "# Plotting graph\n",
        "sns.set_theme(style='white')\n",
        "plt.figure(figsize=(12,16), dpi=90)\n",
        "\n",
        "#plt.bar(df_top_countries.index, df_top_countries['Total Cases'].values)\n",
        "top_countries_list = format_names(df_top_countries.index)\n",
        "plt.barh(top_countries_list, df_top_countries['Total Cases'].values)\n",
        "\n",
        "#plt.axvline(Number_of_countries-0.5, 0,1, ls='--', c='black')\n",
        "plt.axhline(y=(Number_of_countries-0.5), ls='--', c='black')\n",
        "\n",
        "plt.title(f'Top {len(top_countries)} Countries with Most Cases')\n",
        "\n",
        "plt.xscale('log')\n",
        "ax = plt.axes() # for updating \n",
        "ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "\n",
        "#plt.gcf().axes[0].yaxis.get_major_formatter().set_scientific(False)  # To get labels in plain text \n",
        "#plt.xticks(rotation=90)\n",
        "plt.margins(y=0)\n",
        "plt.xlabel('Number of Cases')\n",
        "plt.ylabel('Countries')\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Plotting graph\\nsns.set_theme(style='white')\\nplt.figure(figsize=(12,16), dpi=90)\\n\\n#plt.bar(df_top_countries.index, df_top_countries['Total Cases'].values)\\ntop_countries_list = format_names(df_top_countries.index)\\nplt.barh(top_countries_list, df_top_countries['Total Cases'].values)\\n\\n#plt.axvline(Number_of_countries-0.5, 0,1, ls='--', c='black')\\nplt.axhline(y=(Number_of_countries-0.5), ls='--', c='black')\\n\\nplt.title(f'Top {len(top_countries)} Countries with Most Cases')\\n\\nplt.xscale('log')\\nax = plt.axes() # for updating \\nax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\\n\\n#plt.gcf().axes[0].yaxis.get_major_formatter().set_scientific(False)  # To get labels in plain text \\n#plt.xticks(rotation=90)\\nplt.margins(y=0)\\nplt.xlabel('Number of Cases')\\nplt.ylabel('Countries')\\nplt.show()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0TQucg2Pwu1"
      },
      "source": [
        "## Average cases of top selected countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kOjiJOSRCtG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "outputId": "203927fd-f0c2-4291-9e0b-a0868f5480b3"
      },
      "source": [
        "dict_countries_avg = Counter(total_countries)\n",
        "\n",
        "for country in dict_countries.keys():\n",
        "  dict_countries_avg[country] = df_grouped.get_group(country)['cases'].mean()\n",
        "\n",
        "# Average cases for all countries\n",
        "df_avg_cases_countries = pd.DataFrame.from_dict(dict_countries_avg, orient='index', columns=['Average'])\n",
        "\n",
        "# List of top selected countries\n",
        "top_countries = list(df_top_countries.index)\n",
        "\n",
        "# Average of selected top countries\n",
        "avg_df = df_avg_cases_countries[df_avg_cases_countries.index.isin(top_countries)]\n",
        "avg_df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Belgium</th>\n",
              "      <td>1431.568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Brazil</th>\n",
              "      <td>18005.536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Canada</th>\n",
              "      <td>768.964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Czechia</th>\n",
              "      <td>1109.234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ecuador</th>\n",
              "      <td>558.396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>France</th>\n",
              "      <td>4590.633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Germany</th>\n",
              "      <td>1769.568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>India</th>\n",
              "      <td>26805.580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Indonesia</th>\n",
              "      <td>1371.375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iran</th>\n",
              "      <td>2014.581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iraq</th>\n",
              "      <td>1553.229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Israel</th>\n",
              "      <td>1033.216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Italy</th>\n",
              "      <td>2303.036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mexico</th>\n",
              "      <td>3097.973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nepal</th>\n",
              "      <td>588.363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Netherlands</th>\n",
              "      <td>1167.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pakistan</th>\n",
              "      <td>1105.917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Philippines</th>\n",
              "      <td>1260.240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Romania</th>\n",
              "      <td>806.088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Russia</th>\n",
              "      <td>5314.224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Spain</th>\n",
              "      <td>3862.143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Switzerland</th>\n",
              "      <td>499.117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>United_Arab_Emirates</th>\n",
              "      <td>443.401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>United_Kingdom</th>\n",
              "      <td>3360.110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>United_States_of_America</th>\n",
              "      <td>29894.032</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           Average\n",
              "Belgium                   1431.568\n",
              "Brazil                   18005.536\n",
              "Canada                     768.964\n",
              "Czechia                   1109.234\n",
              "Ecuador                    558.396\n",
              "France                    4590.633\n",
              "Germany                   1769.568\n",
              "India                    26805.580\n",
              "Indonesia                 1371.375\n",
              "Iran                      2014.581\n",
              "Iraq                      1553.229\n",
              "Israel                    1033.216\n",
              "Italy                     2303.036\n",
              "Mexico                    3097.973\n",
              "Nepal                      588.363\n",
              "Netherlands               1167.003\n",
              "Pakistan                  1105.917\n",
              "Philippines               1260.240\n",
              "Romania                    806.088\n",
              "Russia                    5314.224\n",
              "Spain                     3862.143\n",
              "Switzerland                499.117\n",
              "United_Arab_Emirates       443.401\n",
              "United_Kingdom            3360.110\n",
              "United_States_of_America 29894.032"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkabfg44B2iE"
      },
      "source": [
        "# Common Methods for All Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEKtRmy4m6A5"
      },
      "source": [
        "## Testing Script\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s76dImmm-Gf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "2ecee620-a4a2-402c-9c23-13f996bea172"
      },
      "source": [
        "\"\"\"\n",
        "def reshape_dataframe(*data: np.ndarray):\n",
        "    # This function adds an extra dimension which is necessary in the LSTM\n",
        "    arr = []\n",
        "    for d in data:\n",
        "        arr.append(np.reshape(np.array(d), (d.shape[0], 1, d.shape[1])))\n",
        "    return arr\n",
        "\n",
        "\n",
        "def train_test_lstm(regressor, X_train_lstm, y_train, X_val_lstm, y_val, X_test_lstm, patience, epochs):\n",
        "    regressor.compile(loss='mae', optimizer='adagrad', metrics=['mse', 'mae'])\n",
        "\n",
        "    history = regressor.fit(\n",
        "        X_train_lstm,\n",
        "        y_train,\n",
        "        validation_data=(X_val_lstm, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=10,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss',\n",
        "                                 mode='min',\n",
        "                                 patience=patience)])\n",
        "\n",
        "    return regressor.predict(X_test_lstm)\n",
        "\n",
        "\n",
        "def get_validation_set(df_train, batch_size=10):\n",
        "    lst_idx = -1\n",
        "    total_batches = len(df_train) // batch_size\n",
        "    train_set, val_set = [], []\n",
        "\n",
        "    for cur_batch in range(total_batches):\n",
        "        start = lst_idx + 1\n",
        "        end = start + batch_size\n",
        "        if cur_batch % 2 == 0:\n",
        "            train_set.append(df_train.iloc[start:end])\n",
        "        else:\n",
        "            val_set.append(df_train.iloc[start:end])\n",
        "\n",
        "        lst_idx = end - 1  # adjusting last index because we add 1 in starting\n",
        "\n",
        "    return pd.concat(train_set, ignore_index=True), pd.concat(val_set, ignore_index=True)\n",
        "\n",
        "\n",
        "def define_lstm_model(x_train_lstm, layers, activations, patience):\n",
        "    # Start defining the model\n",
        "    input_shape = x_train_lstm.shape\n",
        "\n",
        "    # Definining model first with LSTM n layers\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(layers[0], input_shape=input_shape[1:], activation=activations[0], return_sequences=True))\n",
        "\n",
        "    # Adding middle layers\n",
        "    for l in range(1, len(layers)-1):\n",
        "      model.add(LSTM(layers[l], activation=activations[l], return_sequences=True))\n",
        "      model.add(Dropout(0.2))\n",
        "\n",
        "    # Add last Dense and LSTMs layers\n",
        "    if len(layers) > 1:\n",
        "      model.add(Dense(layers[-1], activation=activations[-1]))\n",
        "      model.add(Dropout(0.2))\n",
        "      model.add(LSTM(layers[-1], activation=activations[-1]))\n",
        "\n",
        "    model.add(Dense(1))  # output layer. Since we have only 1 output value\n",
        "    # End defining model\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def mean_absolute_percentage_error(actual, predicted):\n",
        "    '''\n",
        "    Mean absolute percentage error (MAPE).\n",
        "    :return error\n",
        "    '''\n",
        "    actual =  np.array(actual)\n",
        "    predicted = np.array(predicted)\n",
        "\n",
        "    mask = actual != 0\n",
        "    return (np.fabs(actual - predicted) / np.fabs(actual))[mask].mean()\n",
        "\n",
        "\n",
        "def get_scores(y_true, model_predictions, days):\n",
        "    mdl_evaluation_scores = {}\n",
        "    mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE', 'MAE', 'MAPE']\n",
        "    mdl_evaluation_scores['PretrainDays'] = [days] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "\n",
        "    for model in model_predictions:\n",
        "        y_pred = model_predictions[model]\n",
        "        if model == 'LSTM':\n",
        "            rmse = mean_squared_error(y_true[:, np.newaxis], y_pred, squared=False)\n",
        "            mae = mean_absolute_error(y_true[:, np.newaxis], y_pred)\n",
        "            mape = mean_absolute_percentage_error(y_true[:, np.newaxis], y_pred)\n",
        "        else:\n",
        "            rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "            mae = mean_absolute_error(y_true, y_pred)\n",
        "            mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "\n",
        "        mdl_evaluation_scores[model] = [rmse, mae, mape]\n",
        "    return pd.DataFrame(mdl_evaluation_scores)\n",
        "\n",
        "\n",
        "def normalize_dataset(*dataframes):\n",
        "    arr = []\n",
        "    for df in dataframes:\n",
        "        arr.append(StandardScaler().fit_transform(df))\n",
        "    return arr\n",
        "\n",
        "\n",
        "df = pd.read_csv(r'/content/csv_files/processed/United_States_of_America.csv')\n",
        "model_predictions = {\n",
        "        'LSTM': []\n",
        "        #'MLPRegressor': [],\n",
        "        #'LinearRegression': []\n",
        "    }\n",
        "\n",
        "layers_list = [[50, 30, 20, 10]]\n",
        "epoch_list = [100]\n",
        "\n",
        "for epochs in epoch_list:\n",
        "    for layers in layers_list:\n",
        "        #layers = [51, 30, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\n",
        "        #epochs = 500\n",
        "        activations = ['tanh', 'tanh', 'relu']\n",
        "        patience = 20\n",
        "        train = df.iloc[:150, :]\n",
        "        test = df.iloc[150:180, :]  # Testing on set one month ahead only, hence day+30.\n",
        "\n",
        "        X_train, y_train = train.iloc[:, 4:-1], train.iloc[:, -1]\n",
        "        X_test, y_test = test.iloc[:, 4:-1], test.iloc[:, -1]\n",
        "\n",
        "        # Seperating validation set from test set\n",
        "        train_df, val_df = get_validation_set(train, batch_size=10)\n",
        "\n",
        "        # Splitting test and validation into dependent and independent sets\n",
        "        X_train_batch, y_train_batch = train_df.iloc[:, 4:-1], train_df.iloc[:, -1]  # Consist only odd batches\n",
        "        X_val_batch, y_val_batch = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\n",
        "\n",
        "        # Normalizing dataset\n",
        "        X_train_lstm_norm, X_test_lstm_norm, X_val_lstm_norm = normalize_dataset(X_train_batch, X_test, X_val_batch)\n",
        "\n",
        "        # Reshaping the dataframes\n",
        "        X_train_lstm, X_val_lstm, X_test_lstm = reshape_dataframe(X_train_lstm_norm, X_val_lstm_norm, X_test_lstm_norm)\n",
        "\n",
        "        # Defining models\n",
        "        lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\n",
        "        model_predictions['LSTM'] = train_test_lstm(lstm_model, X_train_lstm, y_train_batch, X_val_lstm, y_val_batch, X_test_lstm, patience, epochs)\n",
        "        print(model_predictions['LSTM'])\n",
        "\n",
        "        mdl_evaluation_df = get_scores(y_test, model_predictions, 60)\n",
        "        print(\"***********************************************************\")\n",
        "        print(f\"Epochs = {epochs}, Layers = {layers}\")\n",
        "        print(\"=========================\")\n",
        "        print(mdl_evaluation_df)\n",
        "        print(\"***********************************************************\")\n",
        "\"\"\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef reshape_dataframe(*data: np.ndarray):\\n    # This function adds an extra dimension which is necessary in the LSTM\\n    arr = []\\n    for d in data:\\n        arr.append(np.reshape(np.array(d), (d.shape[0], 1, d.shape[1])))\\n    return arr\\n\\n\\ndef train_test_lstm(regressor, X_train_lstm, y_train, X_val_lstm, y_val, X_test_lstm, patience, epochs):\\n    regressor.compile(loss=\\'mae\\', optimizer=\\'adagrad\\', metrics=[\\'mse\\', \\'mae\\'])\\n\\n    history = regressor.fit(\\n        X_train_lstm,\\n        y_train,\\n        validation_data=(X_val_lstm, y_val),\\n        epochs=epochs,\\n        batch_size=10,\\n        callbacks=[EarlyStopping(monitor=\\'val_loss\\',\\n                                 mode=\\'min\\',\\n                                 patience=patience)])\\n\\n    return regressor.predict(X_test_lstm)\\n\\n\\ndef get_validation_set(df_train, batch_size=10):\\n    lst_idx = -1\\n    total_batches = len(df_train) // batch_size\\n    train_set, val_set = [], []\\n\\n    for cur_batch in range(total_batches):\\n        start = lst_idx + 1\\n        end = start + batch_size\\n        if cur_batch % 2 == 0:\\n            train_set.append(df_train.iloc[start:end])\\n        else:\\n            val_set.append(df_train.iloc[start:end])\\n\\n        lst_idx = end - 1  # adjusting last index because we add 1 in starting\\n\\n    return pd.concat(train_set, ignore_index=True), pd.concat(val_set, ignore_index=True)\\n\\n\\ndef define_lstm_model(x_train_lstm, layers, activations, patience):\\n    # Start defining the model\\n    input_shape = x_train_lstm.shape\\n\\n    # Definining model first with LSTM n layers\\n    model = Sequential()\\n    model.add(LSTM(layers[0], input_shape=input_shape[1:], activation=activations[0], return_sequences=True))\\n\\n    # Adding middle layers\\n    for l in range(1, len(layers)-1):\\n      model.add(LSTM(layers[l], activation=activations[l], return_sequences=True))\\n      model.add(Dropout(0.2))\\n\\n    # Add last Dense and LSTMs layers\\n    if len(layers) > 1:\\n      model.add(Dense(layers[-1], activation=activations[-1]))\\n      model.add(Dropout(0.2))\\n      model.add(LSTM(layers[-1], activation=activations[-1]))\\n\\n    model.add(Dense(1))  # output layer. Since we have only 1 output value\\n    # End defining model\\n\\n    return model\\n\\n\\ndef mean_absolute_percentage_error(actual, predicted):\\n    \\'\\'\\'\\n    Mean absolute percentage error (MAPE).\\n    :return error\\n    \\'\\'\\'\\n    actual =  np.array(actual)\\n    predicted = np.array(predicted)\\n\\n    mask = actual != 0\\n    return (np.fabs(actual - predicted) / np.fabs(actual))[mask].mean()\\n\\n\\ndef get_scores(y_true, model_predictions, days):\\n    mdl_evaluation_scores = {}\\n    mdl_evaluation_scores[\\'EvaluationMeasurement\\'] = [\\'RMSE\\', \\'MAE\\', \\'MAPE\\']\\n    mdl_evaluation_scores[\\'PretrainDays\\'] = [days] * len(mdl_evaluation_scores[\\'EvaluationMeasurement\\'])\\n\\n    for model in model_predictions:\\n        y_pred = model_predictions[model]\\n        if model == \\'LSTM\\':\\n            rmse = mean_squared_error(y_true[:, np.newaxis], y_pred, squared=False)\\n            mae = mean_absolute_error(y_true[:, np.newaxis], y_pred)\\n            mape = mean_absolute_percentage_error(y_true[:, np.newaxis], y_pred)\\n        else:\\n            rmse = mean_squared_error(y_true, y_pred, squared=False)\\n            mae = mean_absolute_error(y_true, y_pred)\\n            mape = mean_absolute_percentage_error(y_true, y_pred)\\n\\n        mdl_evaluation_scores[model] = [rmse, mae, mape]\\n    return pd.DataFrame(mdl_evaluation_scores)\\n\\n\\ndef normalize_dataset(*dataframes):\\n    arr = []\\n    for df in dataframes:\\n        arr.append(StandardScaler().fit_transform(df))\\n    return arr\\n\\n\\ndf = pd.read_csv(r\\'/content/csv_files/processed/United_States_of_America.csv\\')\\nmodel_predictions = {\\n        \\'LSTM\\': []\\n        #\\'MLPRegressor\\': [],\\n        #\\'LinearRegression\\': []\\n    }\\n\\nlayers_list = [[50, 30, 20, 10]]\\nepoch_list = [100]\\n\\nfor epochs in epoch_list:\\n    for layers in layers_list:\\n        #layers = [51, 30, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\\n        #epochs = 500\\n        activations = [\\'tanh\\', \\'tanh\\', \\'relu\\']\\n        patience = 20\\n        train = df.iloc[:150, :]\\n        test = df.iloc[150:180, :]  # Testing on set one month ahead only, hence day+30.\\n\\n        X_train, y_train = train.iloc[:, 4:-1], train.iloc[:, -1]\\n        X_test, y_test = test.iloc[:, 4:-1], test.iloc[:, -1]\\n\\n        # Seperating validation set from test set\\n        train_df, val_df = get_validation_set(train, batch_size=10)\\n\\n        # Splitting test and validation into dependent and independent sets\\n        X_train_batch, y_train_batch = train_df.iloc[:, 4:-1], train_df.iloc[:, -1]  # Consist only odd batches\\n        X_val_batch, y_val_batch = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\\n\\n        # Normalizing dataset\\n        X_train_lstm_norm, X_test_lstm_norm, X_val_lstm_norm = normalize_dataset(X_train_batch, X_test, X_val_batch)\\n\\n        # Reshaping the dataframes\\n        X_train_lstm, X_val_lstm, X_test_lstm = reshape_dataframe(X_train_lstm_norm, X_val_lstm_norm, X_test_lstm_norm)\\n\\n        # Defining models\\n        lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\\n        model_predictions[\\'LSTM\\'] = train_test_lstm(lstm_model, X_train_lstm, y_train_batch, X_val_lstm, y_val_batch, X_test_lstm, patience, epochs)\\n        print(model_predictions[\\'LSTM\\'])\\n\\n        mdl_evaluation_df = get_scores(y_test, model_predictions, 60)\\n        print(\"***********************************************************\")\\n        print(f\"Epochs = {epochs}, Layers = {layers}\")\\n        print(\"=========================\")\\n        print(mdl_evaluation_df)\\n        print(\"***********************************************************\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jkIXUcMCpwy"
      },
      "source": [
        "## Common Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONXUYHUmNJka"
      },
      "source": [
        "#### Updated Number of countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiLed1lngDEB"
      },
      "source": [
        "Number_of_countries = 2"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2wFG6bg0-0W"
      },
      "source": [
        "# Global variables for countries\n",
        "countries = top_countries[0:Number_of_countries]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHYQO1XBALXe"
      },
      "source": [
        "# Return a combined dataframe for a each error statistics(MAE,RMSE,MAPE etc) along with the newly added mean row.\n",
        "def get_metric_with_mean(result: pd.DataFrame, error_metric: str)->pd.DataFrame:\n",
        "  df_grouped = result.groupby('EvaluationMeasurement')\n",
        "  df = df_grouped.get_group(error_metric).reset_index(drop=True)\n",
        "  df = df.append(df.describe().loc['mean'])\n",
        "  return df"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AVGVaKLwhHu"
      },
      "source": [
        "def calc_mean_to_max_error(df, max_of_pretrain_days, max_of_df):\n",
        "  i=-1\n",
        "  for row_num in range(len(df)-1):  # Go before mean row\n",
        "    i += 1\n",
        "    for col_num in df.columns[2:]:\n",
        "      df.loc[row_num,col_num] = df.loc[row_num,col_num]/max_of_pretrain_days[i] \n",
        "  \n",
        "  for col in df.columns[2:]:\n",
        "      df.loc['mean',col] = df.loc['mean',col]/max_of_df\n",
        "\n",
        "  return df"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgvjK5nJkUaG"
      },
      "source": [
        "# Note: Do not change the filenames, since they are later being used for visualizations \n",
        "def save_runtime(df,path,country=None,static_learner=True,alternate_batch=False, transpose=False):\n",
        "  df = df.apply(pd.to_numeric,errors='ignore') # Converting the dataframe to numeric\n",
        "  df = df.round(decimal) # Setting the precision\n",
        "  \n",
        "  # if transpose flag is set to true\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "\n",
        "  if country==None:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/combined25country_runtime_static.tex')\n",
        "      df.to_csv(f'{path}/combined25country_runtime_static.csv')\n",
        "    else:\n",
        "      if alternate_batch:\n",
        "         df.to_latex(f'{path}/combined25country_runtime_incremental_alternate_batch.tex')\n",
        "         df.to_csv(f'{path}/combined25country_runtime_incremental_alternate_batch.csv')\n",
        "      else:\n",
        "        df.to_latex(f'{path}/combined25country_runtime_incremental.tex')\n",
        "        df.to_csv(f'{path}/combined25country_runtime_incremental.csv')\n",
        "  else:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/{country}_runtime_static.tex')\n",
        "      df.to_csv(f'{path}/{country}_runtime_static.csv')\n",
        "    else:\n",
        "      df.to_latex(f'{path}/{country}_runtime_incremental.tex')\n",
        "      df.to_csv(f'{path}/{country}_runtime_incremental.csv')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJCVpF7oT7r0"
      },
      "source": [
        "# Note: Do not change the filenames, since they are later being used for visualizations \n",
        "def save_summary_table(df,path,country=False,static_learner=True,alternate_batch=False, transpose=False):\n",
        "\n",
        "  df = df.apply(pd.to_numeric,errors='ignore') # Converting the dataframe to numeric\n",
        "  df = df.round(decimal) # Setting the precision\n",
        "  \n",
        "  # if transpose flag is set to true\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "\n",
        "  if country:\n",
        "    metric = df.loc['EvaluationMeasurement'].unique()[0]\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/top_countries_{metric}_summary_table_static.tex')\n",
        "      df.to_csv(f'{path}/top_countries_{metric}_summary_table_static.csv')\n",
        "    else:\n",
        "      df.to_latex(f'{path}/top_countries_{metric}_summary_table_incremental.tex')\n",
        "      df.to_csv(f'{path}/top_countries_{metric}_summary_table_incremental.csv')\n",
        "    \n",
        "  else:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/combined25country_summary_table_static.tex')\n",
        "      df.to_csv(f'{path}/combined25country_summary_table_static.csv')\n",
        "    else:\n",
        "      if alternate_batch:\n",
        "         df.to_latex(f'{path}/combined25country_summary_table_incremental_alternate_batch.tex')\n",
        "         df.to_csv(f'{path}/combined25country_summary_table_incremental_alternate_batch.csv')\n",
        "      else:\n",
        "        df.to_latex(f'{path}/combined25country_summary_table_incremental.tex')\n",
        "        df.to_csv(f'{path}/combined25country_summary_table_incremental.csv')\n",
        "    "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnS1UsFdKRbW"
      },
      "source": [
        "# Note: Do not change the filenames since they are later being used for visualizations\n",
        "def save_metrics(df, path, country=None, static_learner=True, alternate_batch=False, transpose=False): \n",
        "  df = df.apply(pd.to_numeric,errors='ignore') # Converting the dataframe to numeric\n",
        "  df = df.round(decimal) # Setting the precision\n",
        "  \n",
        "  # if transpose flag is set to true\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "\n",
        "  metric_type = df.loc['EvaluationMeasurement'].unique()[0]\n",
        "  if country==None:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/combined25country_{metric_type}_static.tex')\n",
        "      df.to_csv(f'{path}/combined25country_{metric_type}_static.csv')\n",
        "    else:\n",
        "      if alternate_batch:\n",
        "         df.to_latex(f'{path}/combined25country_{metric_type}_incremental_alternate_batch.tex')\n",
        "         df.to_csv(f'{path}/combined25country_{metric_type}_incremental_alternate_batch.csv')\n",
        "      else:\n",
        "        df.to_latex(f'{path}/combined25country_{metric_type}_incremental.tex')\n",
        "        df.to_csv(f'{path}/combined25country_{metric_type}_incremental.csv')\n",
        "  else:\n",
        "    if static_learner:\n",
        "      df.to_latex(f'{path}/{country}_{metric_type}_static.tex')\n",
        "      df.to_csv(f'{path}/{country}_{metric_type}_static.csv')\n",
        "    else:\n",
        "      df.to_latex(f'{path}/{country}_{metric_type}_incremental.tex')\n",
        "      df.to_csv(f'{path}/{country}_{metric_type}_incremental.csv')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQFR3NKzuSK-"
      },
      "source": [
        "def save_combined_summary_table(df, path, static_learner=False,transpose=False):\n",
        "  df = df.apply(pd.to_numeric,errors='ignore')\n",
        "  df = df.round(decimal)\n",
        "  if transpose:\n",
        "    df = df.transpose()\n",
        "  \n",
        "  if static_learner:\n",
        "    save_path = f'{path}/summary_table_combined_mean_static'\n",
        "  else:\n",
        "    save_path = f'{path}/summary_table_combined_mean_incremental'\n",
        "\n",
        "  df.to_csv(f'{save_path}.csv')\n",
        "  df.to_latex(f'{save_path}.tex')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUrUegDEPqof"
      },
      "source": [
        "def display_runtime_per_country(results_runtime,countries):\n",
        "  for i in range(len(countries)):\n",
        "    print(f'_____________Running Time for {countries[i]}________________')\n",
        "    print(results_runtime[i].to_string())\n",
        "    print('\\n')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI4IiENZtLFG"
      },
      "source": [
        "def calc_save_err_metric_countrywise(countries, error_metrics, results, max_of_pretrain_per_country, max_cases_per_country, path, static_learner, transpose):\n",
        "  countrywise_error_scores={}\n",
        "  for i in range(len(countries)):\n",
        "    country_error_score = []\n",
        "    for error_metric in error_metrics:\n",
        "      \n",
        "      df_error_metric = get_metric_with_mean(results[i], error_metric=error_metric)\n",
        "\n",
        "      #if error_metric != 'MAPE':\n",
        "      #  df_error_metric = calc_mean_to_max_error(df_error_metric, max_of_pretrain_per_country[i], max_cases_per_country[i])\n",
        "\n",
        "      country_error_score.append(df_error_metric)\n",
        "      display_countrywise_scores(countries[i],df_error_metric)\n",
        "\n",
        "      # Transposing the metrics while saving\n",
        "      save_metrics(df_error_metric, path=path, country=countries[i], static_learner=static_learner, transpose=transpose)\n",
        "      \n",
        "    countrywise_error_scores[countries[i]] = pd.concat(country_error_score,ignore_index=True)\n",
        "    \n",
        "  return countrywise_error_scores"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e4uCM0f65tm"
      },
      "source": [
        "def calc_save_err_metric_combined(error_metrics, results, max_of_pretrain_days, max_selected_countries, path, static_learner, alternate_batch, transpose):\n",
        "  combined_err_metric = []\n",
        "  for error_metric in error_metrics:\n",
        "    df_error_metric = get_metric_with_mean(results, error_metric=error_metric)\n",
        "\n",
        "    #if error_metric != 'MAPE':\n",
        "    #  df_error_metric = calc_mean_to_max_error(df_error_metric, max_of_pretrain_days, max_selected_countries)\n",
        "\n",
        "    # Transposing the metrics while saving\n",
        "    save_metrics(df_error_metric, path=path, static_learner=static_learner, alternate_batch=alternate_batch, transpose=transpose)\n",
        "    \n",
        "    combined_err_metric.append(df_error_metric)\n",
        "  return (pd.concat(combined_err_metric, ignore_index=True))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7nnCdGu8QVD"
      },
      "source": [
        "def get_summary_table(df_result, df_runtime_result, error_metrics, static_learner=True):\n",
        "  sum_metric=[]\n",
        "  measure_col_name = 'Metric'\n",
        "  \n",
        "  # Setting start row and column for static and incremental learner\n",
        "  for metric in error_metrics:\n",
        "    start_row = 'mean'\n",
        "    if static_learner:\n",
        "      start_col='RandomForest'\n",
        "    else:\n",
        "      start_col='HT_Reg'\n",
        "\n",
        "    df_metric = get_metric_with_mean(df_result, metric)\n",
        "    df_row = pd.DataFrame([df_metric.loc[start_row][start_col:]])\n",
        "    \n",
        "    df_row[measure_col_name] = str(metric)    \n",
        "    sum_metric.append(df_row)\n",
        "\n",
        "  # Adding run time\n",
        "  df_runtime_row = pd.DataFrame([df_runtime_result.describe().loc[start_row][start_col:]])\n",
        "  df_runtime_row[measure_col_name]='Time(sec)'\n",
        "  sum_metric.append(df_runtime_row)\n",
        "\n",
        "  df_summary = pd.concat(sum_metric, ignore_index=True)\n",
        "  df_summary.set_index(measure_col_name, inplace=True)\n",
        "\n",
        "  return df_summary"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLrQqgWXAPgi"
      },
      "source": [
        "def get_summary_table_countrywise(df_result_dict, error_metrics, static_learner=True):  #df_runtime_result,\n",
        "  summary_metric=[]\n",
        "  measure_col_name = f'Country({str(error_metrics[0])})'\n",
        "  eval_measure_col = 'EvaluationMeasurement'\n",
        "  start_row = 'mean'\n",
        "  if static_learner:\n",
        "    start_col='RandomForest'\n",
        "  else:\n",
        "    start_col='HT_Reg'\n",
        "\n",
        "  for country in df_result_dict.keys():\n",
        "    df_result = df_result_dict[country]\n",
        "\n",
        "    # Setting start row and column for static and incremental learner\n",
        "    for metric in error_metrics:      \n",
        "      df_metric = get_metric_with_mean(df_result, metric)\n",
        "      df_row = pd.DataFrame([df_metric.loc[start_row][start_col:]])\n",
        "      df_row[eval_measure_col] = metric\n",
        "      df_row[measure_col_name] = country\n",
        "      summary_metric.append(df_row)\n",
        "\n",
        "  df_summary = pd.concat(summary_metric, ignore_index=True)\n",
        "  df_summary.set_index(measure_col_name, inplace=True)\n",
        "\n",
        "  return df_summary"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSX0nYK3SW2Q"
      },
      "source": [
        "def get_sum_table_combined_mean(countrywise_error_score_incremental,results_runtime, static_learner=False):\n",
        "  sum_table_combined_mean=[]\n",
        "  measure_col_name = 'Metric'\n",
        "  start_row = 'mean'\n",
        "  if static_learner:\n",
        "    start_col = 'RandomForest'\n",
        "  else:\n",
        "    start_col= 'HT_Reg'\n",
        "\n",
        "  for metric in error_metrics:\n",
        "    df_sum_cur_metric = get_summary_table_countrywise(countrywise_error_score_incremental, [metric], static_learner=static_learner)\n",
        "    df_row = pd.DataFrame([df_sum_cur_metric.describe().loc[start_row]])\n",
        "\n",
        "    df_row[measure_col_name] = metric\n",
        "    sum_table_combined_mean.append(df_row)\n",
        "\n",
        "  # Adding run time\n",
        "  df_runtime = pd.concat(results_runtime, ignore_index=True).describe().loc[start_row][start_col:]\n",
        "  df_runtime_row = pd.DataFrame([df_runtime])\n",
        "  df_runtime_row[measure_col_name]='Time(sec)'\n",
        "  sum_table_combined_mean.append(df_runtime_row)\n",
        "\n",
        "  # Concating results to one dataframe\n",
        "  sum_table_combined_mean = pd.concat(sum_table_combined_mean, ignore_index=True)\n",
        "  sum_table_combined_mean.set_index(measure_col_name, inplace=True)\n",
        "  return sum_table_combined_mean"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AKhvSFwAzUj"
      },
      "source": [
        "def check_significance(target_pop, competitor_pop, significance_at: float):\n",
        "    \"\"\"\n",
        "    Comparing algorithms per batch or per country pairs (exp 2 or 1 respectively), \n",
        "      so for each pair, we compare the significance of the best algo to all of the the other algos.\n",
        "    Ttest performed if the distribution is normal, otherwise we perform a non-parametric test.\n",
        "    \"\"\"\n",
        "    model_pop, population = target_pop, competitor_pop  \n",
        "    \n",
        "    # Normality tests\n",
        "    if len(model_pop) >= 8:  # skew test not valid for smaller populations\n",
        "      value_mdl, p_mdl = normaltest(model_pop.values)\n",
        "      value_pop, p_pop = normaltest(population.values)\n",
        "      if (p_mdl >= 0.05) & (p_pop >= 0.05):\n",
        "  #       print('It is likely that both populations are normal. Thus, running T-Test...')\n",
        "          tset, pval = stats.ttest_ind(model_pop, population)\n",
        "          if pval < significance_at:    # alpha value is 0.05 or 5%\n",
        "              significant = 'Significant (Ttest)'\n",
        "          else:\n",
        "              significant = 'Not Significant (Ttest)'\n",
        "      else:\n",
        "  #         print('It is unlikely that the result is normal. Thus, running Wilcoxon test...')\n",
        "          if np.sum(np.subtract(list(model_pop), list(competitor_pop))) != 0.0:  # if values are identical the test will crash, but we now it's not significant\n",
        "              tset, pval = stats.wilcoxon(model_pop, population)\n",
        "              if pval < significance_at:    # alpha value is 0.05 or 5%\n",
        "                  significant = 'Significant (Wilcox Test)'\n",
        "              else:\n",
        "                  significant = 'Not Significant (Wilcox Test)'\n",
        "          else:\n",
        "  #             print('Warning: results are identical')\n",
        "              tset, pval = stats.ttest_ind(model_pop, population)\n",
        "              significant = 'Not Significant (Wilcox Test)'\n",
        "    else:\n",
        "      print('Population too small.')\n",
        "      if np.sum(np.subtract(list(model_pop), list(competitor_pop))) != 0.0:  # if values are identical the test will crash, but we now it's not significant\n",
        "          tset, pval = stats.wilcoxon(model_pop, population)\n",
        "          if pval < significance_at:    # alpha value is 0.05 or 5%\n",
        "              significant = 'Significant (Wilcox Test)'\n",
        "          else:\n",
        "              significant = 'Not Significant (Wilcox Test)'\n",
        "    return pval, significant "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZYx2PYrRBMV"
      },
      "source": [
        "## Combining Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g11ZDONMQvHT"
      },
      "source": [
        "def sortby_date_and_set_index(df):\n",
        "  df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y')\n",
        "  df.sort_values('date', inplace=True)\n",
        "  df.set_index('date', inplace=True)\n",
        "  return df"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZr-9CCDwIVo"
      },
      "source": [
        "def get_dataset_with_target(countries, df_grouped):\n",
        "  \n",
        "  # Empty list to store Dataframes of each country\n",
        "  frames = []\n",
        "  \n",
        "  for country in countries:\n",
        "    \n",
        "    df = df_grouped.get_group(country)\n",
        "\n",
        "    # Creating feature 'day_no'\n",
        "    df['day_no']= pd.Series([i for i in range(1,len(df)+1)], index=df.index)\n",
        "\n",
        "    # Reordering features\n",
        "    df = df[['day_no', 'country','cases']]\n",
        "\n",
        "    # Adding features through lags\n",
        "    df = create_features_with_lags(df)\n",
        "\n",
        "    # Creating target with last 10 days cases\n",
        "    df['target'] = df.iloc[:,[2]+[i*-1 for i in range(1,10)]].mean(axis=1)\n",
        "\n",
        "    # Dropping null columns\n",
        "    df.dropna(how='any', axis=0, inplace=True)\n",
        "\n",
        "    # Dropping mid columns\n",
        "    drop_columns = list(df.loc[:,'cases_t-38':'cases_t-1'].columns)\n",
        "    df.drop(drop_columns, axis=1, inplace=True)\n",
        "\n",
        "    frames.append(df)\n",
        "\n",
        "  return (pd.concat(frames, ignore_index=True))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sit5iHT_0UWl"
      },
      "source": [
        "def reshape_dataframe(*data: np.ndarray):\n",
        "    # This function adds an extra dimension which is necessary in the LSTM\n",
        "    arr = []\n",
        "    for d in data:\n",
        "        arr.append(np.reshape(np.array(d), (d.shape[0], 1, d.shape[1])))\n",
        "    return arr"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1buCiuYoliN"
      },
      "source": [
        "def get_countries_sortedby_cases(valid_countries, df_grouped):\n",
        "  # A dictionary of all countries\n",
        "  dict_countries = Counter(valid_countries)\n",
        "\n",
        "  for country in dict_countries.keys():\n",
        "    dict_countries[country] = df_grouped.get_group(country)['cases'].sum()\n",
        "\n",
        "  # Sorting countries based on number of cases\n",
        "  countries_sortedby_cases = sorted(dict_countries.items(), key=lambda dict_countries: dict_countries[1], reverse=True)\n",
        "\n",
        "  # Creating dataframe \n",
        "  df_countries_sortedbycases = pd.DataFrame.from_dict(dict(countries_sortedby_cases), orient='index', columns=['Total Cases'])\n",
        "  \n",
        "  return df_countries_sortedbycases"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvuRHEVArNJ0"
      },
      "source": [
        "# Getting a list of valid countries\n",
        "def get_countries_with_valid_size(df):\n",
        "  total_countries = list(df_grouped.groups.keys())\n",
        "\n",
        "  # A list for countries with required datasize\n",
        "  valid_countries = []\n",
        "\n",
        "  # List of countries with more than 230 records. Because, max training size = 150, lags removed = 50, prediction = 30.\n",
        "  for country in total_countries:\n",
        "    if len(df_grouped.get_group(country)) >= 230:\n",
        "      valid_countries.append(country)\n",
        "\n",
        "  return valid_countries"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tqHxnYhwsbk"
      },
      "source": [
        "def preprocess_dataset(df):  \n",
        "  # Selecting required features\n",
        "  df= df[['dateRep','cases','countriesAndTerritories']]\n",
        "\n",
        "  # Rename features\n",
        "  df.rename(columns={'countriesAndTerritories':'country', 'dateRep':'date'}, inplace=True)\n",
        "\n",
        "  # Convert to date, sort and set index\n",
        "  df = sortby_date_and_set_index(df)\n",
        "\n",
        "  return df"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6nW5zkHuFLn"
      },
      "source": [
        "# Calculating maximum of dataframe for every pretrain size\n",
        "def calc_max_of_pretrain_days(pretrain_days,df)->list:\n",
        "  max_of_pretrain_days = []\n",
        "  \n",
        "  for day in pretrain_days:\n",
        "    df_subset = create_subset(df,day)\n",
        "    max_of_pretrain_days.append(df_subset['cases'].max())\n",
        "  \n",
        "  return max_of_pretrain_days"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze3Ju2mOFltA"
      },
      "source": [
        "def display_scores(results):\n",
        "  #print(f'_________________________________{country}____________________________________________')\n",
        "  df_MAE = get_metric_with_mean(results,'MAE' )\n",
        "  df_RMSE = get_metric_with_mean(results,'RMSE')\n",
        "  df_MAPE = get_metric_with_mean(results,'MAPE')\n",
        "  print('MAE Score')\n",
        "  print(df_MAE.to_string())\n",
        "  print('-----------------------------------------------------------------------------------')\n",
        "  print('RMSE Score')\n",
        "  print(df_RMSE.to_string())\n",
        "  print('-----------------------------------------------------------------------------------')\n",
        "  print('MAPE Score')\n",
        "  print(df_MAPE.to_string())\n",
        "  print('\\n\\n')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZbb27ZOpQmT"
      },
      "source": [
        "## Alternate Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SKzw1NdWenN"
      },
      "source": [
        "def get_alternate_batch_records_idx(batch_size,total_records): \n",
        "  total_batches = total_records//batch_size\n",
        "  current_batch=1\n",
        "  start_idx = 0\n",
        "  end_idx = batch_size\n",
        "  idx_list = []\n",
        "  \n",
        "  while current_batch <= total_batches:\n",
        "    if current_batch%2!=0:\n",
        "      idx_list.extend([x for x in range(start_idx,end_idx)])\n",
        "      start_idx = idx_list[-1]+(batch_size+1)\n",
        "      end_idx = start_idx + batch_size\n",
        "    current_batch += 1\n",
        "\n",
        "  return idx_list"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1Sdy4o7oufO"
      },
      "source": [
        "def create_alternate_batch_subset(df,days,batch_size):\n",
        "  df_grouped = df.groupby('country')\n",
        "  countries = df['country'].unique()\n",
        "  frame1,frame2 = [],[]\n",
        "\n",
        "  for country in countries:\n",
        "    df_cur_country = df_grouped.get_group(country)\n",
        "\n",
        "    df1 = df_cur_country.iloc[0:days//2]\n",
        "    df2 = df_cur_country.iloc[days:days+30]  # Adding 30 for a testing batch that is one month ahead\n",
        "    \n",
        "    # Selecting alternate batches\n",
        "    idx = get_alternate_batch_records_idx(batch_size,total_records=len(df2))\n",
        "    df2 = df2.iloc[idx]\n",
        "\n",
        "    # Appending dataframes\n",
        "    frame1.append(df1)\n",
        "    frame2.append(df2)\n",
        "\n",
        "  r1 = pd.concat(frame1, ignore_index=True)\n",
        "  r2 = pd.concat(frame2, ignore_index=True)\n",
        "  r = r1.append(r2, ignore_index=True)\n",
        "  \n",
        "  return (r)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPViJY9uDZ0q"
      },
      "source": [
        "## Incremental Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9DZn8PjUter"
      },
      "source": [
        "def instantiate_regressors():\n",
        "  ht_reg = HoeffdingTreeRegressor()\n",
        "  hat_reg = HoeffdingAdaptiveTreeRegressor()\n",
        "  arf_reg = AdaptiveRandomForestRegressor()\n",
        "  pa_reg = PassiveAggressiveRegressor(max_iter=1, random_state=0, tol=1e-3)\n",
        "\n",
        "  model = [ht_reg, hat_reg, arf_reg, pa_reg]\n",
        "  model_names = ['HT_Reg', 'HAT_Reg', 'ARF_Reg', 'PA_Reg']\n",
        "\n",
        "  return model, model_names"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NwhtdOBXBUy"
      },
      "source": [
        "def get_error_scores_per_model(evaluator, mdl_evaluation_scores)-> pd.DataFrame:\n",
        "  \n",
        "  for i in range(len(evaluator.model_names)):\n",
        "    # Desired error metrics\n",
        "    mse = evaluator.mean_eval_measurements[i].get_mean_square_error()\n",
        "    mae = evaluator.mean_eval_measurements[i].get_average_error()\n",
        "    mape = evaluator.mean_eval_measurements[i].get_mean_absolute_percentage_error()\n",
        "    rmse = sqrt(mse)\n",
        "\n",
        "    # Dictionary of errors per model\n",
        "    mdl_evaluation_scores[str(evaluator.model_names[i])] = [rmse, mae, mape]\n",
        "\n",
        "  return(pd.DataFrame(mdl_evaluation_scores))\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSq_iuypN2BE"
      },
      "source": [
        "def get_running_time_per_model_incremental_learner(evaluator,day):\n",
        "    cols = ['PretrainDays']  # Adding pretrain as first column\n",
        "    cols += evaluator.model_names  # Adding remaining columns of different algorithm\n",
        "    running_time = []\n",
        "    running_time.append(day)\n",
        "    for i in range(len(evaluator.model_names)):\n",
        "        running_time.append(evaluator.running_time_measurements[i]._total_time)\n",
        "\n",
        "    return (pd.DataFrame([running_time],columns=cols))  # Passing running_time as a list of list to insert it as a row"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VbOOY4WfUzT"
      },
      "source": [
        "def display_countrywise_scores(country,df_error_metric):\n",
        "  print(f'_________________________________{country}____________________________________________')\n",
        "  print(df_error_metric.to_string())\n",
        "  print('\\n\\n')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC8vD-Dl3uNs"
      },
      "source": [
        "# Create a dataframe of all countries with pre-train size = pretrain days and test&train size = pretrain days\n",
        "def create_subset(result,days):\n",
        "  result_grouped = result.groupby('country')\n",
        "  countries = result['country'].unique()\n",
        "  frame1,frame2 = [],[]\n",
        "  for country in countries:\n",
        "    df = result_grouped.get_group(country) \n",
        "    df1 = df.iloc[0:days]\n",
        "    df2 = df.iloc[days:days+30]\n",
        "    frame1.append(df1)\n",
        "    frame2.append(df2)\n",
        "\n",
        "  r1 = pd.concat(frame1, ignore_index=True)\n",
        "  r2 = pd.concat(frame2, ignore_index=True)\n",
        "  r = r1.append(r2, ignore_index=True)\n",
        "  \n",
        "  return (r)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGzbZRfVDc-c"
      },
      "source": [
        "## Static Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKVRq7O9onX7"
      },
      "source": [
        "def mean_absolute_percentage_error(actual, predicted):\n",
        "    \"\"\"\n",
        "    Mean absolute percentage error (MAPE).\n",
        "    :return error\n",
        "    \"\"\"\n",
        "    actual =  np.array(actual) \n",
        "    predicted = np.array(predicted) \n",
        "\n",
        "    mask = actual != 0\n",
        "    return (np.fabs(actual - predicted) / np.fabs(actual))[mask].mean()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olpCsifni4p-"
      },
      "source": [
        "def get_scores(y_true, model_predictions, days):\n",
        "    mdl_evaluation_scores = {}\n",
        "    mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE', 'MAE', 'MAPE']\n",
        "    mdl_evaluation_scores['PretrainDays'] = [days] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "\n",
        "    for model in model_predictions:\n",
        "        y_pred = model_predictions[model]\n",
        "        if model == 'LSTM':\n",
        "            rmse = mean_squared_error(y_true[:, np.newaxis], y_pred, squared=False)\n",
        "            mae = mean_absolute_error(y_true[:, np.newaxis], y_pred)\n",
        "            mape = mean_absolute_percentage_error(y_true[:, np.newaxis], y_pred)\n",
        "        else:\n",
        "            rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "            mae = mean_absolute_error(y_true, y_pred)\n",
        "            mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "\n",
        "        mdl_evaluation_scores[model] = [rmse, mae, mape]\n",
        "    return pd.DataFrame(mdl_evaluation_scores)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBKHhy5I0sWy"
      },
      "source": [
        "def get_running_time_per_model_static_learner(model_predictions,total_execution_time):\n",
        "    cols = ['PretrainDays']\n",
        "    cols += model_predictions.keys()\n",
        "    return pd.DataFrame(total_execution_time, columns=cols)\n",
        "\n",
        "\n",
        "def measure(wrapped_func):\n",
        "    @wraps(wrapped_func)\n",
        "    def _time_it(*args, **kwargs):\n",
        "        start = pc_timer()\n",
        "        try:\n",
        "            model_predictions = wrapped_func(*args, **kwargs)\n",
        "        finally:\n",
        "            end_ = pc_timer() - start\n",
        "            return model_predictions, end_\n",
        "    return _time_it\n",
        "\n",
        "\n",
        "@measure\n",
        "def train_test_model(regressor, X_train, y_train, X_test):\n",
        "    regressor.fit(X_train, y_train)\n",
        "    return regressor.predict(X_test)\n",
        "\n",
        "\n",
        "@measure\n",
        "def train_test_lstm(regressor, X_train_lstm, y_train, X_val_lstm, y_val, X_test_lstm, patience, epochs, batch_size_lstm):\n",
        "    regressor.compile(loss='mae', optimizer='adagrad', metrics=['mse', 'mae'])\n",
        "\n",
        "    history = regressor.fit(\n",
        "        X_train_lstm,\n",
        "        y_train,\n",
        "        validation_data=(X_val_lstm, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size_lstm,\n",
        "        callbacks=[EarlyStopping(monitor='val_loss',\n",
        "                                 mode='min',\n",
        "                                 patience=patience)])\n",
        "\n",
        "    return regressor.predict(X_test_lstm)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB-GhumBoRRH"
      },
      "source": [
        "def define_lstm_model(x_train_lstm, layers, activations, patience):\n",
        "    # Start defining the model\n",
        "    input_shape = x_train_lstm.shape\n",
        "\n",
        "    # Definining model first with LSTM n layers\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(layers[0], input_shape=input_shape[1:], activation=activations[0], return_sequences=True))\n",
        "\n",
        "    # Adding middle layers\n",
        "    for l in range(1, len(layers)-1):\n",
        "      model.add(LSTM(layers[l], activation=activations[l], return_sequences=True))\n",
        "      model.add(Dropout(0.2))\n",
        "\n",
        "    # Add last Dense and LSTMs layers\n",
        "    if len(layers) > 1:\n",
        "      model.add(Dense(layers[-1], activation=activations[-1]))\n",
        "      model.add(Dropout(0.2))\n",
        "      model.add(LSTM(layers[-1], activation=activations[-1]))\n",
        "\n",
        "    model.add(Dense(1))  # output layer. Since we have only 1 output value\n",
        "    # End defining model\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjnk6w9_cqG6"
      },
      "source": [
        "def normalize_dataset(*dataframes):\n",
        "    arr = []\n",
        "    for df in dataframes:\n",
        "        arr.append(StandardScaler().fit_transform(df))\n",
        "    return arr"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GQGxLLNw1z5"
      },
      "source": [
        "def get_validation_set(df_train, batch_size=10):\n",
        "    lst_idx = -1\n",
        "    total_batches = len(df_train) // batch_size\n",
        "    train_set, val_set = [], []\n",
        "\n",
        "    for cur_batch in range(total_batches):\n",
        "        start = lst_idx + 1\n",
        "        end = start + batch_size\n",
        "        if cur_batch % 2 == 0:\n",
        "            train_set.append(df_train.iloc[start:end])\n",
        "        else:\n",
        "            val_set.append(df_train.iloc[start:end])\n",
        "\n",
        "        lst_idx = end - 1  # adjusting last index because we add 1 in starting\n",
        "\n",
        "    return pd.concat(train_set, ignore_index=True), pd.concat(val_set, ignore_index=True)\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMyZ5jcy_m6j"
      },
      "source": [
        "# Experiment 1\n",
        "Training and testing with five countries individually. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDnpy-TycyY9"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "* cases(t): Number of cases on current day(Column='cases') \n",
        "\n",
        "* cases(t-n): Number of cases 'n' days before current day 't'\n",
        "\n",
        "* 30 day gap: Training from day number t-89 to t-39(50 days). Then a gap of 30 days and then creating target by averaging t to t-9(10 Days).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeJJB0sdNnIq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "9048c902-a27b-41ae-e9ae-15fc698da623"
      },
      "source": [
        "# Sample set for understanding dataset\n",
        "sample_df = pd.read_csv(f'{csv_processed_path}/United_States_of_America.csv')\n",
        "sample_df.tail(5)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>day_no</th>\n",
              "      <th>country</th>\n",
              "      <th>cases</th>\n",
              "      <th>cases_t-89</th>\n",
              "      <th>cases_t-88</th>\n",
              "      <th>cases_t-87</th>\n",
              "      <th>cases_t-86</th>\n",
              "      <th>cases_t-85</th>\n",
              "      <th>cases_t-84</th>\n",
              "      <th>cases_t-83</th>\n",
              "      <th>cases_t-82</th>\n",
              "      <th>cases_t-81</th>\n",
              "      <th>cases_t-80</th>\n",
              "      <th>cases_t-79</th>\n",
              "      <th>cases_t-78</th>\n",
              "      <th>cases_t-77</th>\n",
              "      <th>cases_t-76</th>\n",
              "      <th>cases_t-75</th>\n",
              "      <th>cases_t-74</th>\n",
              "      <th>cases_t-73</th>\n",
              "      <th>cases_t-72</th>\n",
              "      <th>cases_t-71</th>\n",
              "      <th>cases_t-70</th>\n",
              "      <th>cases_t-69</th>\n",
              "      <th>cases_t-68</th>\n",
              "      <th>cases_t-67</th>\n",
              "      <th>cases_t-66</th>\n",
              "      <th>cases_t-65</th>\n",
              "      <th>cases_t-64</th>\n",
              "      <th>cases_t-63</th>\n",
              "      <th>cases_t-62</th>\n",
              "      <th>cases_t-61</th>\n",
              "      <th>cases_t-60</th>\n",
              "      <th>cases_t-59</th>\n",
              "      <th>cases_t-58</th>\n",
              "      <th>cases_t-57</th>\n",
              "      <th>cases_t-56</th>\n",
              "      <th>cases_t-55</th>\n",
              "      <th>cases_t-54</th>\n",
              "      <th>cases_t-53</th>\n",
              "      <th>cases_t-52</th>\n",
              "      <th>cases_t-51</th>\n",
              "      <th>cases_t-50</th>\n",
              "      <th>cases_t-49</th>\n",
              "      <th>cases_t-48</th>\n",
              "      <th>cases_t-47</th>\n",
              "      <th>cases_t-46</th>\n",
              "      <th>cases_t-45</th>\n",
              "      <th>cases_t-44</th>\n",
              "      <th>cases_t-43</th>\n",
              "      <th>cases_t-42</th>\n",
              "      <th>cases_t-41</th>\n",
              "      <th>cases_t-40</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>2020-10-29</td>\n",
              "      <td>304</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>78371</td>\n",
              "      <td>67023.000</td>\n",
              "      <td>58407.000</td>\n",
              "      <td>47511.000</td>\n",
              "      <td>45607.000</td>\n",
              "      <td>57525.000</td>\n",
              "      <td>52804.000</td>\n",
              "      <td>59755.000</td>\n",
              "      <td>58150.000</td>\n",
              "      <td>56221.000</td>\n",
              "      <td>46847.000</td>\n",
              "      <td>49530.000</td>\n",
              "      <td>46813.000</td>\n",
              "      <td>55941.000</td>\n",
              "      <td>51094.000</td>\n",
              "      <td>64838.000</td>\n",
              "      <td>48085.000</td>\n",
              "      <td>42104.000</td>\n",
              "      <td>35056.000</td>\n",
              "      <td>44091.000</td>\n",
              "      <td>47426.000</td>\n",
              "      <td>44005.000</td>\n",
              "      <td>49880.000</td>\n",
              "      <td>44378.000</td>\n",
              "      <td>34506.000</td>\n",
              "      <td>38298.000</td>\n",
              "      <td>38119.000</td>\n",
              "      <td>42848.000</td>\n",
              "      <td>45909.000</td>\n",
              "      <td>49654.000</td>\n",
              "      <td>44143.000</td>\n",
              "      <td>35581.000</td>\n",
              "      <td>33850.000</td>\n",
              "      <td>44639.000</td>\n",
              "      <td>38754.000</td>\n",
              "      <td>36249.000</td>\n",
              "      <td>51071.000</td>\n",
              "      <td>44140.000</td>\n",
              "      <td>30555.000</td>\n",
              "      <td>24250.000</td>\n",
              "      <td>27122.000</td>\n",
              "      <td>31927.000</td>\n",
              "      <td>37507.000</td>\n",
              "      <td>48061.000</td>\n",
              "      <td>40820.000</td>\n",
              "      <td>33871.000</td>\n",
              "      <td>34841.000</td>\n",
              "      <td>51473.000</td>\n",
              "      <td>24598.000</td>\n",
              "      <td>43567.000</td>\n",
              "      <td>50209.000</td>\n",
              "      <td>70342.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>2020-10-30</td>\n",
              "      <td>305</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>88130</td>\n",
              "      <td>58407.000</td>\n",
              "      <td>47511.000</td>\n",
              "      <td>45607.000</td>\n",
              "      <td>57525.000</td>\n",
              "      <td>52804.000</td>\n",
              "      <td>59755.000</td>\n",
              "      <td>58150.000</td>\n",
              "      <td>56221.000</td>\n",
              "      <td>46847.000</td>\n",
              "      <td>49530.000</td>\n",
              "      <td>46813.000</td>\n",
              "      <td>55941.000</td>\n",
              "      <td>51094.000</td>\n",
              "      <td>64838.000</td>\n",
              "      <td>48085.000</td>\n",
              "      <td>42104.000</td>\n",
              "      <td>35056.000</td>\n",
              "      <td>44091.000</td>\n",
              "      <td>47426.000</td>\n",
              "      <td>44005.000</td>\n",
              "      <td>49880.000</td>\n",
              "      <td>44378.000</td>\n",
              "      <td>34506.000</td>\n",
              "      <td>38298.000</td>\n",
              "      <td>38119.000</td>\n",
              "      <td>42848.000</td>\n",
              "      <td>45909.000</td>\n",
              "      <td>49654.000</td>\n",
              "      <td>44143.000</td>\n",
              "      <td>35581.000</td>\n",
              "      <td>33850.000</td>\n",
              "      <td>44639.000</td>\n",
              "      <td>38754.000</td>\n",
              "      <td>36249.000</td>\n",
              "      <td>51071.000</td>\n",
              "      <td>44140.000</td>\n",
              "      <td>30555.000</td>\n",
              "      <td>24250.000</td>\n",
              "      <td>27122.000</td>\n",
              "      <td>31927.000</td>\n",
              "      <td>37507.000</td>\n",
              "      <td>48061.000</td>\n",
              "      <td>40820.000</td>\n",
              "      <td>33871.000</td>\n",
              "      <td>34841.000</td>\n",
              "      <td>51473.000</td>\n",
              "      <td>24598.000</td>\n",
              "      <td>43567.000</td>\n",
              "      <td>50209.000</td>\n",
              "      <td>40295.000</td>\n",
              "      <td>73139.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>2020-10-31</td>\n",
              "      <td>306</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>101273</td>\n",
              "      <td>47511.000</td>\n",
              "      <td>45607.000</td>\n",
              "      <td>57525.000</td>\n",
              "      <td>52804.000</td>\n",
              "      <td>59755.000</td>\n",
              "      <td>58150.000</td>\n",
              "      <td>56221.000</td>\n",
              "      <td>46847.000</td>\n",
              "      <td>49530.000</td>\n",
              "      <td>46813.000</td>\n",
              "      <td>55941.000</td>\n",
              "      <td>51094.000</td>\n",
              "      <td>64838.000</td>\n",
              "      <td>48085.000</td>\n",
              "      <td>42104.000</td>\n",
              "      <td>35056.000</td>\n",
              "      <td>44091.000</td>\n",
              "      <td>47426.000</td>\n",
              "      <td>44005.000</td>\n",
              "      <td>49880.000</td>\n",
              "      <td>44378.000</td>\n",
              "      <td>34506.000</td>\n",
              "      <td>38298.000</td>\n",
              "      <td>38119.000</td>\n",
              "      <td>42848.000</td>\n",
              "      <td>45909.000</td>\n",
              "      <td>49654.000</td>\n",
              "      <td>44143.000</td>\n",
              "      <td>35581.000</td>\n",
              "      <td>33850.000</td>\n",
              "      <td>44639.000</td>\n",
              "      <td>38754.000</td>\n",
              "      <td>36249.000</td>\n",
              "      <td>51071.000</td>\n",
              "      <td>44140.000</td>\n",
              "      <td>30555.000</td>\n",
              "      <td>24250.000</td>\n",
              "      <td>27122.000</td>\n",
              "      <td>31927.000</td>\n",
              "      <td>37507.000</td>\n",
              "      <td>48061.000</td>\n",
              "      <td>40820.000</td>\n",
              "      <td>33871.000</td>\n",
              "      <td>34841.000</td>\n",
              "      <td>51473.000</td>\n",
              "      <td>24598.000</td>\n",
              "      <td>43567.000</td>\n",
              "      <td>50209.000</td>\n",
              "      <td>40295.000</td>\n",
              "      <td>39852.000</td>\n",
              "      <td>77412.300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>2020-11-01</td>\n",
              "      <td>307</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>78934</td>\n",
              "      <td>45607.000</td>\n",
              "      <td>57525.000</td>\n",
              "      <td>52804.000</td>\n",
              "      <td>59755.000</td>\n",
              "      <td>58150.000</td>\n",
              "      <td>56221.000</td>\n",
              "      <td>46847.000</td>\n",
              "      <td>49530.000</td>\n",
              "      <td>46813.000</td>\n",
              "      <td>55941.000</td>\n",
              "      <td>51094.000</td>\n",
              "      <td>64838.000</td>\n",
              "      <td>48085.000</td>\n",
              "      <td>42104.000</td>\n",
              "      <td>35056.000</td>\n",
              "      <td>44091.000</td>\n",
              "      <td>47426.000</td>\n",
              "      <td>44005.000</td>\n",
              "      <td>49880.000</td>\n",
              "      <td>44378.000</td>\n",
              "      <td>34506.000</td>\n",
              "      <td>38298.000</td>\n",
              "      <td>38119.000</td>\n",
              "      <td>42848.000</td>\n",
              "      <td>45909.000</td>\n",
              "      <td>49654.000</td>\n",
              "      <td>44143.000</td>\n",
              "      <td>35581.000</td>\n",
              "      <td>33850.000</td>\n",
              "      <td>44639.000</td>\n",
              "      <td>38754.000</td>\n",
              "      <td>36249.000</td>\n",
              "      <td>51071.000</td>\n",
              "      <td>44140.000</td>\n",
              "      <td>30555.000</td>\n",
              "      <td>24250.000</td>\n",
              "      <td>27122.000</td>\n",
              "      <td>31927.000</td>\n",
              "      <td>37507.000</td>\n",
              "      <td>48061.000</td>\n",
              "      <td>40820.000</td>\n",
              "      <td>33871.000</td>\n",
              "      <td>34841.000</td>\n",
              "      <td>51473.000</td>\n",
              "      <td>24598.000</td>\n",
              "      <td>43567.000</td>\n",
              "      <td>50209.000</td>\n",
              "      <td>40295.000</td>\n",
              "      <td>39852.000</td>\n",
              "      <td>53153.000</td>\n",
              "      <td>79007.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>2020-11-02</td>\n",
              "      <td>308</td>\n",
              "      <td>United_States_of_America</td>\n",
              "      <td>81001</td>\n",
              "      <td>57525.000</td>\n",
              "      <td>52804.000</td>\n",
              "      <td>59755.000</td>\n",
              "      <td>58150.000</td>\n",
              "      <td>56221.000</td>\n",
              "      <td>46847.000</td>\n",
              "      <td>49530.000</td>\n",
              "      <td>46813.000</td>\n",
              "      <td>55941.000</td>\n",
              "      <td>51094.000</td>\n",
              "      <td>64838.000</td>\n",
              "      <td>48085.000</td>\n",
              "      <td>42104.000</td>\n",
              "      <td>35056.000</td>\n",
              "      <td>44091.000</td>\n",
              "      <td>47426.000</td>\n",
              "      <td>44005.000</td>\n",
              "      <td>49880.000</td>\n",
              "      <td>44378.000</td>\n",
              "      <td>34506.000</td>\n",
              "      <td>38298.000</td>\n",
              "      <td>38119.000</td>\n",
              "      <td>42848.000</td>\n",
              "      <td>45909.000</td>\n",
              "      <td>49654.000</td>\n",
              "      <td>44143.000</td>\n",
              "      <td>35581.000</td>\n",
              "      <td>33850.000</td>\n",
              "      <td>44639.000</td>\n",
              "      <td>38754.000</td>\n",
              "      <td>36249.000</td>\n",
              "      <td>51071.000</td>\n",
              "      <td>44140.000</td>\n",
              "      <td>30555.000</td>\n",
              "      <td>24250.000</td>\n",
              "      <td>27122.000</td>\n",
              "      <td>31927.000</td>\n",
              "      <td>37507.000</td>\n",
              "      <td>48061.000</td>\n",
              "      <td>40820.000</td>\n",
              "      <td>33871.000</td>\n",
              "      <td>34841.000</td>\n",
              "      <td>51473.000</td>\n",
              "      <td>24598.000</td>\n",
              "      <td>43567.000</td>\n",
              "      <td>50209.000</td>\n",
              "      <td>40295.000</td>\n",
              "      <td>39852.000</td>\n",
              "      <td>53153.000</td>\n",
              "      <td>38307.000</td>\n",
              "      <td>79902.200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           date  day_no  ... cases_t-40    target\n",
              "214  2020-10-29     304  ...  50209.000 70342.900\n",
              "215  2020-10-30     305  ...  40295.000 73139.900\n",
              "216  2020-10-31     306  ...  39852.000 77412.300\n",
              "217  2020-11-01     307  ...  53153.000 79007.900\n",
              "218  2020-11-02     308  ...  38307.000 79902.200\n",
              "\n",
              "[5 rows x 55 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJiGVxO_hsNv"
      },
      "source": [
        "## Incremental Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j53C2ZstT9tI"
      },
      "source": [
        "def scikit_multiflow(df, pretrain_days):\n",
        "\n",
        "  # Creating a stream from dataframe\n",
        "  stream = DataStream(np.array(df.iloc[:,4:-1]), y=np.array(df.iloc[:,-1])) # Selecting features x=[t-89:t-39] and y=[target]. TODO: Drop columns with name \n",
        "\n",
        "  model, model_names = instantiate_regressors()\n",
        "\n",
        "  frames, running_time_frames = [], []\n",
        "\n",
        "  # Setup the evaluator\n",
        "  for day in pretrain_days:\n",
        "\n",
        "      pretrain_days = day\n",
        "      max_samples = pretrain_days + 30  #Testing on set one month ahead only\n",
        "\n",
        "      evaluator = EvaluatePrequential(show_plot=False,\n",
        "                                    pretrain_size=day,\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'], \n",
        "                                    max_samples=max_samples)\n",
        "      # Run evaluation\n",
        "      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
        "\n",
        "      # Dictionary to store each iteration error scores\n",
        "      mdl_evaluation_scores = {}\n",
        "\n",
        "      # Adding Evaluation Measurements and pretraining days\n",
        "      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE']\n",
        "      mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\n",
        "\n",
        "      # Errors of each model on a specific pre-train days\n",
        "      frames.append(mdl_evaluation_df)\n",
        "\n",
        "      # Run time for each algorithm\n",
        "      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
        "\n",
        "  # Final Run Time DataFrame\n",
        "  running_time_df = pd.concat(running_time_frames,ignore_index=True)\n",
        "\n",
        "  # Final Evaluation Score Dataframe\n",
        "  evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
        "  return evaluation_scores_df, running_time_df"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUw_30g9T4fS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3010370a-6b4b-40bb-806a-56d6abda692c"
      },
      "source": [
        "# Training all countries\n",
        "results_incremental = []\n",
        "results_runtime_incremental = []\n",
        "max_of_pretrain_per_country = []\n",
        "max_cases_per_country = []\n",
        "\n",
        "for country in countries:\n",
        "  # Read each country  \n",
        "  df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\n",
        "\n",
        "  # Get evaluation scores and running time for country\n",
        "  evaluation_scores_df, running_time_df = scikit_multiflow(df_country,pretrain_days)\n",
        "\n",
        "  # Appending evaluation scores and runtime for each country\n",
        "  results_incremental.append(evaluation_scores_df)\n",
        "  results_runtime_incremental.append(running_time_df)\n",
        "\n",
        "  # Get max of each pretrain subset and for each country dataset\n",
        "  max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days,df_country))\n",
        "  max_cases_per_country.append(df_country['cases'].max())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.25s]\n",
            "Processed samples: 60\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 10238155.8984\n",
            "HT_Reg - MAPE          : 0.1077\n",
            "HT_Reg - MAE          : 2726.229562\n",
            "HAT_Reg - MSE          : 5523865.6454\n",
            "HAT_Reg - MAPE          : 0.0818\n",
            "HAT_Reg - MAE          : 2039.594063\n",
            "ARF_Reg - MSE          : 392726616.5186\n",
            "ARF_Reg - MAPE          : 0.7492\n",
            "ARF_Reg - MAE          : 18533.652319\n",
            "PA_Reg - MSE          : 62927183059.5860\n",
            "PA_Reg - MAPE          : 8.2249\n",
            "PA_Reg - MAE          : 193965.777290\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [2.09s]\n",
            "Processed samples: 90\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 458139484.8674\n",
            "HT_Reg - MAPE          : 0.7729\n",
            "HT_Reg - MAE          : 16774.342114\n",
            "HAT_Reg - MSE          : 445932376.2100\n",
            "HAT_Reg - MAPE          : 0.7507\n",
            "HAT_Reg - MAE          : 16309.741901\n",
            "ARF_Reg - MSE          : 537317993.7079\n",
            "ARF_Reg - MAPE          : 0.9360\n",
            "ARF_Reg - MAE          : 20714.414748\n",
            "PA_Reg - MSE          : 857534137659.9487\n",
            "PA_Reg - MAPE          : 30.3646\n",
            "PA_Reg - MAE          : 670387.455840\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.09s]\n",
            "Processed samples: 120\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 271284915.1998\n",
            "HT_Reg - MAPE          : 0.2448\n",
            "HT_Reg - MAE          : 14299.672847\n",
            "HAT_Reg - MSE          : 271270629.0843\n",
            "HAT_Reg - MAPE          : 0.2448\n",
            "HAT_Reg - MAE          : 14298.642646\n",
            "ARF_Reg - MSE          : 279449999.6470\n",
            "ARF_Reg - MAPE          : 0.2625\n",
            "ARF_Reg - MAE          : 14976.154171\n",
            "PA_Reg - MSE          : 396646488510.2900\n",
            "PA_Reg - MAPE          : 9.7370\n",
            "PA_Reg - MAE          : 472027.576987\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.80s]\n",
            "Processed samples: 150\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 151813061.6230\n",
            "HT_Reg - MAPE          : 0.1761\n",
            "HT_Reg - MAE          : 10347.791139\n",
            "HAT_Reg - MSE          : 151813060.8824\n",
            "HAT_Reg - MAPE          : 0.1761\n",
            "HAT_Reg - MAE          : 10347.791117\n",
            "ARF_Reg - MSE          : 103700640.5258\n",
            "ARF_Reg - MAPE          : 0.1181\n",
            "ARF_Reg - MAE          : 7274.473014\n",
            "PA_Reg - MSE          : 1376206705602.8467\n",
            "PA_Reg - MAPE          : 14.4547\n",
            "PA_Reg - MAE          : 799895.355738\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.91s]\n",
            "Processed samples: 180\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 7351419.0495\n",
            "HT_Reg - MAPE          : 0.0472\n",
            "HT_Reg - MAE          : 1875.441790\n",
            "HAT_Reg - MSE          : 7351419.0482\n",
            "HAT_Reg - MAPE          : 0.0472\n",
            "HAT_Reg - MAE          : 1875.441790\n",
            "ARF_Reg - MSE          : 1033715287.4557\n",
            "ARF_Reg - MAPE          : 0.6832\n",
            "ARF_Reg - MAE          : 26903.883983\n",
            "PA_Reg - MSE          : 6993624384607.5488\n",
            "PA_Reg - MAPE          : 42.8033\n",
            "PA_Reg - MAE          : 1717538.679714\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [6.45s]\n",
            "Processed samples: 210\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 5997546.5477\n",
            "HT_Reg - MAPE          : 0.0359\n",
            "HT_Reg - MAE          : 1833.207878\n",
            "HAT_Reg - MSE          : 5997546.5476\n",
            "HAT_Reg - MAPE          : 0.0359\n",
            "HAT_Reg - MAE          : 1833.207878\n",
            "ARF_Reg - MSE          : 402215291.6371\n",
            "ARF_Reg - MAPE          : 0.3139\n",
            "ARF_Reg - MAE          : 14185.386854\n",
            "PA_Reg - MSE          : 931983797906.3052\n",
            "PA_Reg - MAPE          : 15.7020\n",
            "PA_Reg - MAE          : 760302.267275\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 30 sample(s).\n",
            "Evaluating...\n",
            " #################### [98%] [1.19s]\n",
            "Processed samples: 60\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 367460.8194\n",
            "HT_Reg - MAPE          : 0.1550\n",
            "HT_Reg - MAE          : 480.274936\n",
            "HAT_Reg - MSE          : 319923.0970\n",
            "HAT_Reg - MAPE          : 0.1419\n",
            "HAT_Reg - MAE          : 451.069340\n",
            "ARF_Reg - MSE          : 11978046.2840\n",
            "ARF_Reg - MAPE          : 0.4809\n",
            "ARF_Reg - MAE          : 2124.242425\n",
            "PA_Reg - MSE          : 8016123.9259\n",
            "PA_Reg - MAPE          : 0.5818\n",
            "PA_Reg - MAE          : 2173.221425\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 60 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [1.78s]\n",
            "Processed samples: 90\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 710066.2152\n",
            "HT_Reg - MAPE          : 0.0698\n",
            "HT_Reg - MAE          : 633.741497\n",
            "HAT_Reg - MSE          : 1940929.6137\n",
            "HAT_Reg - MAPE          : 0.1107\n",
            "HAT_Reg - MAE          : 1064.744450\n",
            "ARF_Reg - MSE          : 223750686.9219\n",
            "ARF_Reg - MAPE          : 1.1400\n",
            "ARF_Reg - MAE          : 12158.305488\n",
            "PA_Reg - MSE          : 236162038.7136\n",
            "PA_Reg - MAPE          : 1.1823\n",
            "PA_Reg - MAE          : 11499.900172\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 90 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [3.01s]\n",
            "Processed samples: 120\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 443424.8061\n",
            "HT_Reg - MAPE          : 0.0162\n",
            "HT_Reg - MAE          : 466.675824\n",
            "HAT_Reg - MSE          : 429318.8467\n",
            "HAT_Reg - MAPE          : 0.0165\n",
            "HAT_Reg - MAE          : 477.457500\n",
            "ARF_Reg - MSE          : 753969408.2711\n",
            "ARF_Reg - MAPE          : 0.8604\n",
            "ARF_Reg - MAE          : 20884.030795\n",
            "PA_Reg - MSE          : 730746921.5458\n",
            "PA_Reg - MAPE          : 0.6739\n",
            "PA_Reg - MAE          : 14422.914500\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 120 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [4.77s]\n",
            "Processed samples: 150\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 2071219.6660\n",
            "HT_Reg - MAPE          : 0.0215\n",
            "HT_Reg - MAE          : 1189.428293\n",
            "HAT_Reg - MSE          : 2237829.6221\n",
            "HAT_Reg - MAPE          : 0.0217\n",
            "HAT_Reg - MAE          : 1206.630476\n",
            "ARF_Reg - MSE          : 4188016668.5439\n",
            "ARF_Reg - MAPE          : 0.6893\n",
            "ARF_Reg - MAE          : 42417.679928\n",
            "PA_Reg - MSE          : 1082997387.3860\n",
            "PA_Reg - MAPE          : 0.4927\n",
            "PA_Reg - MAE          : 27117.757161\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 150 sample(s).\n",
            "Evaluating...\n",
            " #################### [99%] [6.43s]\n",
            "Processed samples: 180\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 3896263543.7609\n",
            "HT_Reg - MAPE          : 0.3338\n",
            "HT_Reg - MAE          : 30154.034100\n",
            "HAT_Reg - MSE          : 4290933007.5036\n",
            "HAT_Reg - MAPE          : 0.3656\n",
            "HAT_Reg - MAE          : 33074.661600\n",
            "ARF_Reg - MSE          : 12902292984.6168\n",
            "ARF_Reg - MAPE          : 1.0828\n",
            "ARF_Reg - MAE          : 92399.713694\n",
            "PA_Reg - MSE          : 16585628537.0634\n",
            "PA_Reg - MAPE          : 1.1462\n",
            "PA_Reg - MAE          : 97351.772011\n",
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 180 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [7.30s]\n",
            "Processed samples: 210\n",
            "Mean performance:\n",
            "HT_Reg - MSE          : 8294492465.5087\n",
            "HT_Reg - MAPE          : 0.9343\n",
            "HT_Reg - MAE          : 58607.928371\n",
            "HAT_Reg - MSE          : 8796172990.5712\n",
            "HAT_Reg - MAPE          : 0.9799\n",
            "HAT_Reg - MAE          : 61807.778781\n",
            "ARF_Reg - MSE          : 13697575780.1880\n",
            "ARF_Reg - MAPE          : 1.3751\n",
            "ARF_Reg - MAE          : 99523.870179\n",
            "PA_Reg - MSE          : 223922675101.0760\n",
            "PA_Reg - MAPE          : 3.5222\n",
            "PA_Reg - MAE          : 266261.871120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HQdWByW7GTn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37bba01a-18f3-49b3-9076-82b2e91c5564"
      },
      "source": [
        "# Save the running time for each country\n",
        "for i in range(len(countries)):\n",
        "  save_runtime(results_runtime_incremental[i], path=exp1_runtime_path, country = countries[i], static_learner=False)\n",
        "\n",
        "# Display countrywise running time complexity\n",
        "display_runtime_per_country(results_runtime_incremental, countries)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_____________Running Time for United_States_of_America________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.044    0.061    1.075   0.054\n",
            "1            60   0.068    0.088    1.898   0.035\n",
            "2            90   0.147    0.175    2.736   0.037\n",
            "3           120   0.206    0.235    4.348   0.032\n",
            "4           150   0.284    0.317    4.572   0.029\n",
            "5           180   0.340    0.381    5.715   0.031\n",
            "\n",
            "\n",
            "_____________Running Time for India________________\n",
            "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0            30   0.035    0.045    1.062   0.033\n",
            "1            60   0.059    0.077    1.608   0.031\n",
            "2            90   0.117    0.146    2.737   0.037\n",
            "3           120   0.183    0.217    4.365   0.032\n",
            "4           150   0.247    0.290    6.053   0.035\n",
            "5           180   0.306    0.359    6.637   0.031\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMH2CG_nuloA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aff71096-87fc-4d81-817a-cfc452e4b76f"
      },
      "source": [
        "countrywise_error_score_incremental = calc_save_err_metric_countrywise(countries, error_metrics, results_incremental, max_of_pretrain_per_country, max_cases_per_country, path=exp1_path, static_learner=False, transpose=True)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________United_States_of_America____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg      PA_Reg\n",
            "0                      MAE        30.000  2726.230  2039.594 18533.652  193965.777\n",
            "1                      MAE        60.000 16774.342 16309.742 20714.415  670387.456\n",
            "2                      MAE        90.000 14299.673 14298.643 14976.154  472027.577\n",
            "3                      MAE       120.000 10347.791 10347.791  7274.473  799895.356\n",
            "4                      MAE       150.000  1875.442  1875.442 26903.884 1717538.680\n",
            "5                      MAE       180.000  1833.208  1833.208 14185.387  760302.267\n",
            "mean                   NaN       105.000  7976.114  7784.070 17097.994  769019.519\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_States_of_America____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   0.108    0.082    0.749   8.225\n",
            "1                     MAPE        60.000   0.773    0.751    0.936  30.365\n",
            "2                     MAPE        90.000   0.245    0.245    0.263   9.737\n",
            "3                     MAPE       120.000   0.176    0.176    0.118  14.455\n",
            "4                     MAPE       150.000   0.047    0.047    0.683  42.803\n",
            "5                     MAPE       180.000   0.036    0.036    0.314  15.702\n",
            "mean                   NaN       105.000   0.231    0.223    0.510  20.214\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_States_of_America____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg      PA_Reg\n",
            "0                     RMSE        30.000  3199.712  2350.291 19817.331  250852.911\n",
            "1                     RMSE        60.000 21404.193 21117.111 23180.121  926031.391\n",
            "2                     RMSE        90.000 16470.729 16470.295 16716.758  629798.768\n",
            "3                     RMSE       120.000 12321.244 12321.244 10183.351 1173118.368\n",
            "4                     RMSE       150.000  2711.350  2711.350 32151.443 2644546.159\n",
            "5                     RMSE       180.000  2448.989  2448.989 20055.306  965393.079\n",
            "mean                   NaN       105.000  9759.370  9569.880 20350.718 1098290.113\n",
            "\n",
            "\n",
            "\n",
            "_________________________________India____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg     PA_Reg\n",
            "0                      MAE        30.000   480.275   451.069  2124.242   2173.221\n",
            "1                      MAE        60.000   633.741  1064.744 12158.305  11499.900\n",
            "2                      MAE        90.000   466.676   477.458 20884.031  14422.915\n",
            "3                      MAE       120.000  1189.428  1206.630 42417.680  27117.757\n",
            "4                      MAE       150.000 30154.034 33074.662 92399.714  97351.772\n",
            "5                      MAE       180.000 58607.928 61807.779 99523.870 266261.871\n",
            "mean                   NaN       105.000 15255.347 16347.057 44917.974  69804.573\n",
            "\n",
            "\n",
            "\n",
            "_________________________________India____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
            "0                     MAPE        30.000   0.155    0.142    0.481   0.582\n",
            "1                     MAPE        60.000   0.070    0.111    1.140   1.182\n",
            "2                     MAPE        90.000   0.016    0.016    0.860   0.674\n",
            "3                     MAPE       120.000   0.022    0.022    0.689   0.493\n",
            "4                     MAPE       150.000   0.334    0.366    1.083   1.146\n",
            "5                     MAPE       180.000   0.934    0.980    1.375   3.522\n",
            "mean                   NaN       105.000   0.255    0.273    0.938   1.266\n",
            "\n",
            "\n",
            "\n",
            "_________________________________India____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg    ARF_Reg     PA_Reg\n",
            "0                     RMSE        30.000   606.185   565.617   3460.931   2831.276\n",
            "1                     RMSE        60.000   842.654  1393.172  14958.298  15367.565\n",
            "2                     RMSE        90.000   665.901   655.224  27458.503  27032.331\n",
            "3                     RMSE       120.000  1439.173  1495.938  64714.888  32908.926\n",
            "4                     RMSE       150.000 62420.057 65505.214 113588.261 128785.203\n",
            "5                     RMSE       180.000 91074.104 93787.915 117036.643 473204.686\n",
            "mean                   NaN       105.000 26174.679 27233.847  56869.587 113354.998\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6hi5rPZF4Uf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "fc05b25e-dda6-4924-b5cc-ade8d7bc359a"
      },
      "source": [
        "# Get summary table for each country for specified metric\n",
        "summary_table_countrywise_incremental = get_summary_table_countrywise(countrywise_error_score_incremental, ['MAPE'], static_learner=False)\n",
        "\n",
        "# Saving the summary table\n",
        "save_summary_table(summary_table_countrywise_incremental, exp1_summary_path,country=True, static_learner=False,alternate_batch=False,transpose=True)\n",
        "\n",
        "summary_table_countrywise_incremental"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "      <th>EvaluationMeasurement</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Country(MAPE)</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>United_States_of_America</th>\n",
              "      <td>0.231</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.510</td>\n",
              "      <td>20.214</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>India</th>\n",
              "      <td>0.255</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.938</td>\n",
              "      <td>1.266</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          HT_Reg  HAT_Reg  ...  PA_Reg  EvaluationMeasurement\n",
              "Country(MAPE)                              ...                               \n",
              "United_States_of_America   0.231    0.223  ...  20.214                   MAPE\n",
              "India                      0.255    0.273  ...   1.266                   MAPE\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8xs_fZnD28j"
      },
      "source": [
        ""
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jEmM1GgdAqq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c37813f0-1974-4929-ca6c-a98790a22a30"
      },
      "source": [
        "sum_inc_countrywise_mean = get_sum_table_combined_mean(countrywise_error_score_incremental,results_runtime_incremental)\n",
        "save_combined_summary_table(sum_inc_countrywise_mean, exp1_summary_path, static_learner=False, transpose=True) \n",
        "sum_inc_countrywise_mean"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Metric</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MAE</th>\n",
              "      <td>11615.731</td>\n",
              "      <td>12065.563</td>\n",
              "      <td>31007.984</td>\n",
              "      <td>419412.046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MAPE</th>\n",
              "      <td>0.243</td>\n",
              "      <td>0.248</td>\n",
              "      <td>0.724</td>\n",
              "      <td>10.740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RMSE</th>\n",
              "      <td>17967.024</td>\n",
              "      <td>18401.863</td>\n",
              "      <td>38610.153</td>\n",
              "      <td>605822.555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time(sec)</th>\n",
              "      <td>0.170</td>\n",
              "      <td>0.199</td>\n",
              "      <td>3.567</td>\n",
              "      <td>0.035</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             HT_Reg   HAT_Reg   ARF_Reg     PA_Reg\n",
              "Metric                                            \n",
              "MAE       11615.731 12065.563 31007.984 419412.046\n",
              "MAPE          0.243     0.248     0.724     10.740\n",
              "RMSE      17967.024 18401.863 38610.153 605822.555\n",
              "Time(sec)     0.170     0.199     3.567      0.035"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEBzePUFh8FO"
      },
      "source": [
        "## Static Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdlwyL_f04aH"
      },
      "source": [
        "def scikit_learn(df, training_days):\n",
        "    frames = []\n",
        "    model_predictions = {\n",
        "        'RandomForest': [],\n",
        "        'GradientBoosting': [],\n",
        "        'LinearSVR': [],\n",
        "        'DecisionTree': [],\n",
        "        'BayesianRidge': [],\n",
        "        'LSTM': []\n",
        "        #'MLPRegressor': [],\n",
        "        #'LinearRegression': []\n",
        "    }\n",
        "    total_execution_time = []\n",
        "            \n",
        "    # LSTM (TODO: feel free to put the whole LSTM definition in a funtion)\n",
        "    # params (others like epoch and batch size are also hardcoded in train_test_lstm())\n",
        "    layers = [50, 30, 20, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\n",
        "    activations = ['tanh', 'tanh', 'relu']\n",
        "    epochs = 500\n",
        "    patience = 20\n",
        "    batch_size_lstm = 10\n",
        "\n",
        "    for day in training_days:\n",
        "        \n",
        "        cur_exec_time = [day]  # Keeping runing time for each pre-train set\n",
        "\n",
        "        train = df.iloc[:day, :]\n",
        "        test = df.iloc[day:day + 30, :]  # Testing on set one month ahead only, hence day+30.\n",
        "        \n",
        "        # training and test sets for all models except LSTM\n",
        "        X_train, y_train = train.iloc[:, 4:-1], train.iloc[:, -1]\n",
        "        X_test, y_test = test.iloc[:, 4:-1], test.iloc[:, -1]\n",
        "\n",
        "        # Seperating validation set from test set\n",
        "        train_df, val_df = get_validation_set(train, batch_size=10)\n",
        "\n",
        "        # Splitting test and validation into dependent and independent sets\n",
        "        X_train_batch, y_train_batch = train_df.iloc[:, 4:-1], train_df.iloc[:, -1]  # Consist only odd batches\n",
        "        X_val_batch, y_val_batch = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\n",
        "\n",
        "        # Normalizing dataset\n",
        "        X_train_lstm_norm, X_test_lstm_norm, X_val_lstm_norm = normalize_dataset(X_train_batch, X_test, X_val_batch)\n",
        "\n",
        "        # Reshaping the dataframes\n",
        "        X_train_lstm, X_val_lstm, X_test_lstm = reshape_dataframe(X_train_lstm_norm, X_val_lstm_norm, X_test_lstm_norm)\n",
        "       \n",
        "\n",
        "        rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "        model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train,y_train,X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "        gb_reg = GradientBoostingRegressor(random_state=0)\n",
        "        model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "        lsv_reg = LinearSVR(random_state=0, tol=1e-5)\n",
        "        model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "        dt_reg = DecisionTreeRegressor(random_state=0)\n",
        "        model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "        br_reg = BayesianRidge()\n",
        "        model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        \n",
        "        lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\n",
        "        model_predictions['LSTM'],exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train_batch, X_val_lstm, y_val_batch, X_test_lstm, patience, epochs, batch_size_lstm)\n",
        "        cur_exec_time.append(exec_time)\n",
        "\n",
        "        \n",
        "        mdl_evaluation_df = get_scores(y_test, model_predictions, day)\n",
        "        total_execution_time.append(cur_exec_time)\n",
        "        frames.append(mdl_evaluation_df)\n",
        "\n",
        "    evaluation_score_df = pd.concat(frames, ignore_index=True)\n",
        "    running_time_df = get_running_time_per_model_static_learner(model_predictions,total_execution_time)\n",
        "    return evaluation_score_df, running_time_df\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIMqSQZDjq1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd48a955-0a33-4dcf-e009-86a23dfbdddc"
      },
      "source": [
        "# Training all countries\n",
        "results_static = []\n",
        "results_runtime_static = []\n",
        "max_of_pretrain_per_country = []\n",
        "max_cases_per_country = []\n",
        "\n",
        "for country in countries:\n",
        "  # Read country wise csv file\n",
        "  df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\n",
        "\n",
        "  # Evaluation scores and running time of each algorithm over different pre-training days\n",
        "  evaluation_scores_df, running_time_df = scikit_learn(df_country, pretrain_days)\n",
        "\n",
        "  # Append result of each pretrain size in results\n",
        "  results_static.append(evaluation_scores_df)\n",
        "\n",
        "  # Appending every country runtime \n",
        "  results_runtime_static.append(running_time_df)\n",
        "\n",
        "  # Calculating max cases per country based on pre-train size\n",
        "  max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days,df_country))\n",
        "\n",
        "  # Maximum case of each country\n",
        "  max_cases_per_country.append(df_country['cases'].max())"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20 samples, validate on 10 samples\n",
            "Epoch 1/500\n",
            "20/20 [==============================] - 2s 86ms/step - loss: 24206.4590 - mse: 624614592.0000 - mae: 24206.4590 - val_loss: 30089.3652 - val_mse: 906656384.0000 - val_mae: 30089.3652\n",
            "Epoch 2/500\n",
            "20/20 [==============================] - 0s 976us/step - loss: 24206.4375 - mse: 624613504.0000 - mae: 24206.4375 - val_loss: 30089.3496 - val_mse: 906655360.0000 - val_mae: 30089.3496\n",
            "Epoch 3/500\n",
            "20/20 [==============================] - 0s 886us/step - loss: 24206.4209 - mse: 624612736.0000 - mae: 24206.4219 - val_loss: 30089.3340 - val_mse: 906654528.0000 - val_mae: 30089.3340\n",
            "Epoch 4/500\n",
            "20/20 [==============================] - 0s 766us/step - loss: 24206.4072 - mse: 624612032.0000 - mae: 24206.4082 - val_loss: 30089.3223 - val_mse: 906653696.0000 - val_mae: 30089.3223\n",
            "Epoch 5/500\n",
            "20/20 [==============================] - 0s 885us/step - loss: 24206.3916 - mse: 624611328.0000 - mae: 24206.3906 - val_loss: 30089.3066 - val_mse: 906652800.0000 - val_mae: 30089.3066\n",
            "Epoch 6/500\n",
            "20/20 [==============================] - 0s 892us/step - loss: 24206.3760 - mse: 624610688.0000 - mae: 24206.3750 - val_loss: 30089.2910 - val_mse: 906651840.0000 - val_mae: 30089.2910\n",
            "Epoch 7/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24206.3496 - mse: 624609280.0000 - mae: 24206.3496 - val_loss: 30089.2656 - val_mse: 906650240.0000 - val_mae: 30089.2656\n",
            "Epoch 8/500\n",
            "20/20 [==============================] - 0s 832us/step - loss: 24206.3154 - mse: 624608064.0000 - mae: 24206.3164 - val_loss: 30089.2246 - val_mse: 906647936.0000 - val_mae: 30089.2246\n",
            "Epoch 9/500\n",
            "20/20 [==============================] - 0s 883us/step - loss: 24206.1475 - mse: 624601408.0000 - mae: 24206.1465 - val_loss: 30089.1191 - val_mse: 906641728.0000 - val_mae: 30089.1191\n",
            "Epoch 10/500\n",
            "20/20 [==============================] - 0s 880us/step - loss: 24205.8242 - mse: 624587776.0000 - mae: 24205.8242 - val_loss: 30088.8477 - val_mse: 906625536.0000 - val_mae: 30088.8477\n",
            "Epoch 11/500\n",
            "20/20 [==============================] - 0s 935us/step - loss: 24205.1045 - mse: 624563392.0000 - mae: 24205.1055 - val_loss: 30088.3555 - val_mse: 906596736.0000 - val_mae: 30088.3555\n",
            "Epoch 12/500\n",
            "20/20 [==============================] - 0s 938us/step - loss: 24203.9775 - mse: 624514944.0000 - mae: 24203.9785 - val_loss: 30087.5312 - val_mse: 906548032.0000 - val_mae: 30087.5312\n",
            "Epoch 13/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24202.7373 - mse: 624471872.0000 - mae: 24202.7383 - val_loss: 30086.4785 - val_mse: 906485568.0000 - val_mae: 30086.4785\n",
            "Epoch 14/500\n",
            "20/20 [==============================] - 0s 889us/step - loss: 24200.7344 - mse: 624372352.0000 - mae: 24200.7344 - val_loss: 30084.9883 - val_mse: 906396864.0000 - val_mae: 30084.9883\n",
            "Epoch 15/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24198.7803 - mse: 624284800.0000 - mae: 24198.7793 - val_loss: 30083.1758 - val_mse: 906288960.0000 - val_mae: 30083.1758\n",
            "Epoch 16/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24196.3271 - mse: 624183424.0000 - mae: 24196.3262 - val_loss: 30080.8691 - val_mse: 906151232.0000 - val_mae: 30080.8691\n",
            "Epoch 17/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24191.8779 - mse: 623986112.0000 - mae: 24191.8789 - val_loss: 30077.9688 - val_mse: 905978048.0000 - val_mae: 30077.9688\n",
            "Epoch 18/500\n",
            "20/20 [==============================] - 0s 987us/step - loss: 24190.3438 - mse: 623899392.0000 - mae: 24190.3438 - val_loss: 30075.0254 - val_mse: 905801920.0000 - val_mae: 30075.0254\n",
            "Epoch 19/500\n",
            "20/20 [==============================] - 0s 776us/step - loss: 24189.1553 - mse: 623838080.0000 - mae: 24189.1543 - val_loss: 30072.1992 - val_mse: 905632768.0000 - val_mae: 30072.1992\n",
            "Epoch 20/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24178.3154 - mse: 623309248.0000 - mae: 24178.3164 - val_loss: 30068.4102 - val_mse: 905405824.0000 - val_mae: 30068.4102\n",
            "Epoch 21/500\n",
            "20/20 [==============================] - 0s 842us/step - loss: 24179.7041 - mse: 623386176.0000 - mae: 24179.7051 - val_loss: 30065.0410 - val_mse: 905203904.0000 - val_mae: 30065.0410\n",
            "Epoch 22/500\n",
            "20/20 [==============================] - 0s 988us/step - loss: 24171.7715 - mse: 623044160.0000 - mae: 24171.7715 - val_loss: 30061.2227 - val_mse: 904974848.0000 - val_mae: 30061.2227\n",
            "Epoch 23/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24160.3438 - mse: 622484864.0000 - mae: 24160.3438 - val_loss: 30056.6504 - val_mse: 904701120.0000 - val_mae: 30056.6504\n",
            "Epoch 24/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24157.1865 - mse: 622300928.0000 - mae: 24157.1855 - val_loss: 30051.9160 - val_mse: 904417280.0000 - val_mae: 30051.9160\n",
            "Epoch 25/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24146.7314 - mse: 621788288.0000 - mae: 24146.7305 - val_loss: 30046.8438 - val_mse: 904113280.0000 - val_mae: 30046.8438\n",
            "Epoch 26/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24144.5303 - mse: 621714112.0000 - mae: 24144.5293 - val_loss: 30041.7246 - val_mse: 903806336.0000 - val_mae: 30041.7246\n",
            "Epoch 27/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24132.0723 - mse: 621132224.0000 - mae: 24132.0723 - val_loss: 30036.1309 - val_mse: 903471296.0000 - val_mae: 30036.1309\n",
            "Epoch 28/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24131.4570 - mse: 621268992.0000 - mae: 24131.4570 - val_loss: 30030.8027 - val_mse: 903152000.0000 - val_mae: 30030.8027\n",
            "Epoch 29/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24121.9971 - mse: 620695232.0000 - mae: 24121.9961 - val_loss: 30024.9023 - val_mse: 902798464.0000 - val_mae: 30024.9023\n",
            "Epoch 30/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24126.8906 - mse: 620813952.0000 - mae: 24126.8906 - val_loss: 30019.4570 - val_mse: 902472320.0000 - val_mae: 30019.4570\n",
            "Epoch 31/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24121.6221 - mse: 620548608.0000 - mae: 24121.6211 - val_loss: 30013.7402 - val_mse: 902129792.0000 - val_mae: 30013.7402\n",
            "Epoch 32/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24106.4443 - mse: 619913728.0000 - mae: 24106.4434 - val_loss: 30007.2695 - val_mse: 901742272.0000 - val_mae: 30007.2695\n",
            "Epoch 33/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24088.7070 - mse: 619143296.0000 - mae: 24088.7070 - val_loss: 30000.1562 - val_mse: 901316096.0000 - val_mae: 30000.1562\n",
            "Epoch 34/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24087.3740 - mse: 618981376.0000 - mae: 24087.3750 - val_loss: 29992.9102 - val_mse: 900882304.0000 - val_mae: 29992.9102\n",
            "Epoch 35/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24089.7148 - mse: 619107776.0000 - mae: 24089.7148 - val_loss: 29986.1035 - val_mse: 900475008.0000 - val_mae: 29986.1035\n",
            "Epoch 36/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24065.0576 - mse: 618043520.0000 - mae: 24065.0566 - val_loss: 29978.3125 - val_mse: 900008768.0000 - val_mae: 29978.3125\n",
            "Epoch 37/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24069.3174 - mse: 617945472.0000 - mae: 24069.3164 - val_loss: 29970.8320 - val_mse: 899561280.0000 - val_mae: 29970.8320\n",
            "Epoch 38/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24030.2656 - mse: 616318720.0000 - mae: 24030.2656 - val_loss: 29961.7598 - val_mse: 899018752.0000 - val_mae: 29961.7598\n",
            "Epoch 39/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24064.8799 - mse: 618043904.0000 - mae: 24064.8789 - val_loss: 29954.4160 - val_mse: 898579648.0000 - val_mae: 29954.4160\n",
            "Epoch 40/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24005.8984 - mse: 615121472.0000 - mae: 24005.8984 - val_loss: 29944.5820 - val_mse: 897992000.0000 - val_mae: 29944.5820\n",
            "Epoch 41/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 24000.2139 - mse: 614981632.0000 - mae: 24000.2148 - val_loss: 29935.0820 - val_mse: 897424064.0000 - val_mae: 29935.0820\n",
            "Epoch 42/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23980.7998 - mse: 613960256.0000 - mae: 23980.8008 - val_loss: 29924.8223 - val_mse: 896811392.0000 - val_mae: 29924.8223\n",
            "Epoch 43/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23988.8828 - mse: 614183488.0000 - mae: 23988.8809 - val_loss: 29915.0996 - val_mse: 896230784.0000 - val_mae: 29915.0996\n",
            "Epoch 44/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23980.6914 - mse: 613877760.0000 - mae: 23980.6914 - val_loss: 29905.3379 - val_mse: 895648128.0000 - val_mae: 29905.3379\n",
            "Epoch 45/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23980.0186 - mse: 613782912.0000 - mae: 23980.0195 - val_loss: 29895.6836 - val_mse: 895072256.0000 - val_mae: 29895.6836\n",
            "Epoch 46/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23971.2656 - mse: 613548672.0000 - mae: 23971.2656 - val_loss: 29886.0566 - val_mse: 894497920.0000 - val_mae: 29886.0566\n",
            "Epoch 47/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23962.6738 - mse: 612962560.0000 - mae: 23962.6738 - val_loss: 29875.9434 - val_mse: 893894784.0000 - val_mae: 29875.9434\n",
            "Epoch 48/500\n",
            "20/20 [==============================] - 0s 947us/step - loss: 23940.4551 - mse: 611908608.0000 - mae: 23940.4570 - val_loss: 29865.1621 - val_mse: 893252096.0000 - val_mae: 29865.1621\n",
            "Epoch 49/500\n",
            "20/20 [==============================] - 0s 982us/step - loss: 23909.4561 - mse: 610561792.0000 - mae: 23909.4570 - val_loss: 29853.2129 - val_mse: 892540224.0000 - val_mae: 29853.2129\n",
            "Epoch 50/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23927.5947 - mse: 611115200.0000 - mae: 23927.5938 - val_loss: 29842.2070 - val_mse: 891884736.0000 - val_mae: 29842.2070\n",
            "Epoch 51/500\n",
            "20/20 [==============================] - 0s 957us/step - loss: 23879.3662 - mse: 608693504.0000 - mae: 23879.3652 - val_loss: 29829.5781 - val_mse: 891132736.0000 - val_mae: 29829.5781\n",
            "Epoch 52/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23850.7119 - mse: 608043264.0000 - mae: 23850.7129 - val_loss: 29816.1250 - val_mse: 890332160.0000 - val_mae: 29816.1250\n",
            "Epoch 53/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23870.6641 - mse: 608737600.0000 - mae: 23870.6641 - val_loss: 29803.5820 - val_mse: 889586048.0000 - val_mae: 29803.5820\n",
            "Epoch 54/500\n",
            "20/20 [==============================] - 0s 891us/step - loss: 23820.6377 - mse: 606194176.0000 - mae: 23820.6367 - val_loss: 29789.6973 - val_mse: 888760960.0000 - val_mae: 29789.6973\n",
            "Epoch 55/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23846.8809 - mse: 607911808.0000 - mae: 23846.8809 - val_loss: 29776.8867 - val_mse: 887999680.0000 - val_mae: 29776.8867\n",
            "Epoch 56/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23833.9297 - mse: 607100608.0000 - mae: 23833.9277 - val_loss: 29763.6445 - val_mse: 887212928.0000 - val_mae: 29763.6445\n",
            "Epoch 57/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23772.7568 - mse: 604016320.0000 - mae: 23772.7559 - val_loss: 29748.5527 - val_mse: 886316928.0000 - val_mae: 29748.5527\n",
            "Epoch 58/500\n",
            "20/20 [==============================] - 0s 942us/step - loss: 23810.4844 - mse: 605857088.0000 - mae: 23810.4844 - val_loss: 29735.3008 - val_mse: 885530496.0000 - val_mae: 29735.3008\n",
            "Epoch 59/500\n",
            "20/20 [==============================] - 0s 955us/step - loss: 23793.9893 - mse: 605117568.0000 - mae: 23793.9883 - val_loss: 29721.4941 - val_mse: 884711744.0000 - val_mae: 29721.4941\n",
            "Epoch 60/500\n",
            "20/20 [==============================] - 0s 950us/step - loss: 23761.6084 - mse: 603477696.0000 - mae: 23761.6074 - val_loss: 29706.7656 - val_mse: 883838656.0000 - val_mae: 29706.7656\n",
            "Epoch 61/500\n",
            "20/20 [==============================] - 0s 968us/step - loss: 23695.9424 - mse: 601113472.0000 - mae: 23695.9414 - val_loss: 29690.4941 - val_mse: 882874496.0000 - val_mae: 29690.4941\n",
            "Epoch 62/500\n",
            "20/20 [==============================] - 0s 981us/step - loss: 23677.3252 - mse: 599346112.0000 - mae: 23677.3242 - val_loss: 29674.2129 - val_mse: 881910400.0000 - val_mae: 29674.2129\n",
            "Epoch 63/500\n",
            "20/20 [==============================] - 0s 934us/step - loss: 23745.6689 - mse: 602532736.0000 - mae: 23745.6680 - val_loss: 29659.6445 - val_mse: 881048192.0000 - val_mae: 29659.6445\n",
            "Epoch 64/500\n",
            "20/20 [==============================] - 0s 880us/step - loss: 23661.2021 - mse: 598384512.0000 - mae: 23661.2012 - val_loss: 29642.4375 - val_mse: 880030208.0000 - val_mae: 29642.4375\n",
            "Epoch 65/500\n",
            "20/20 [==============================] - 0s 980us/step - loss: 23679.1611 - mse: 600077312.0000 - mae: 23679.1602 - val_loss: 29626.2344 - val_mse: 879072384.0000 - val_mae: 29626.2344\n",
            "Epoch 66/500\n",
            "20/20 [==============================] - 0s 948us/step - loss: 23631.4697 - mse: 597408896.0000 - mae: 23631.4688 - val_loss: 29609.2969 - val_mse: 878071808.0000 - val_mae: 29609.2969\n",
            "Epoch 67/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23625.1475 - mse: 597851520.0000 - mae: 23625.1465 - val_loss: 29591.9531 - val_mse: 877047616.0000 - val_mae: 29591.9531\n",
            "Epoch 68/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23602.2871 - mse: 595889024.0000 - mae: 23602.2871 - val_loss: 29574.2656 - val_mse: 876003712.0000 - val_mae: 29574.2656\n",
            "Epoch 69/500\n",
            "20/20 [==============================] - 0s 934us/step - loss: 23613.5459 - mse: 596230784.0000 - mae: 23613.5469 - val_loss: 29557.0254 - val_mse: 874987008.0000 - val_mae: 29557.0254\n",
            "Epoch 70/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23617.7754 - mse: 596823168.0000 - mae: 23617.7754 - val_loss: 29540.0879 - val_mse: 873988736.0000 - val_mae: 29540.0879\n",
            "Epoch 71/500\n",
            "20/20 [==============================] - 0s 875us/step - loss: 23518.7100 - mse: 591688832.0000 - mae: 23518.7090 - val_loss: 29520.7715 - val_mse: 872850752.0000 - val_mae: 29520.7715\n",
            "Epoch 72/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23564.9746 - mse: 594904384.0000 - mae: 23564.9746 - val_loss: 29503.1777 - val_mse: 871815040.0000 - val_mae: 29503.1777\n",
            "Epoch 73/500\n",
            "20/20 [==============================] - 0s 995us/step - loss: 23446.6670 - mse: 588196480.0000 - mae: 23446.6680 - val_loss: 29482.4785 - val_mse: 870597440.0000 - val_mae: 29482.4785\n",
            "Epoch 74/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23542.0469 - mse: 593186048.0000 - mae: 23542.0469 - val_loss: 29464.5430 - val_mse: 869543104.0000 - val_mae: 29464.5430\n",
            "Epoch 75/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23453.0732 - mse: 589438400.0000 - mae: 23453.0723 - val_loss: 29444.7305 - val_mse: 868379264.0000 - val_mae: 29444.7305\n",
            "Epoch 76/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23419.2529 - mse: 587051840.0000 - mae: 23419.2539 - val_loss: 29423.6055 - val_mse: 867138880.0000 - val_mae: 29423.6055\n",
            "Epoch 77/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23383.3037 - mse: 587428160.0000 - mae: 23383.3027 - val_loss: 29402.0195 - val_mse: 865872576.0000 - val_mae: 29402.0195\n",
            "Epoch 78/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23362.8867 - mse: 583583552.0000 - mae: 23362.8867 - val_loss: 29379.7910 - val_mse: 864569472.0000 - val_mae: 29379.7910\n",
            "Epoch 79/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23452.1875 - mse: 588561792.0000 - mae: 23452.1875 - val_loss: 29360.1680 - val_mse: 863420288.0000 - val_mae: 29360.1680\n",
            "Epoch 80/500\n",
            "20/20 [==============================] - 0s 978us/step - loss: 23370.7578 - mse: 585492160.0000 - mae: 23370.7578 - val_loss: 29338.7441 - val_mse: 862166400.0000 - val_mae: 29338.7441\n",
            "Epoch 81/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23294.1299 - mse: 582404224.0000 - mae: 23294.1289 - val_loss: 29315.7305 - val_mse: 860820800.0000 - val_mae: 29315.7305\n",
            "Epoch 82/500\n",
            "20/20 [==============================] - 0s 964us/step - loss: 23259.9355 - mse: 581041792.0000 - mae: 23259.9336 - val_loss: 29292.2070 - val_mse: 859446080.0000 - val_mae: 29292.2070\n",
            "Epoch 83/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23270.9424 - mse: 580076224.0000 - mae: 23270.9434 - val_loss: 29268.8555 - val_mse: 858082304.0000 - val_mae: 29268.8555\n",
            "Epoch 84/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23188.3867 - mse: 577348096.0000 - mae: 23188.3867 - val_loss: 29244.0156 - val_mse: 856633152.0000 - val_mae: 29244.0156\n",
            "Epoch 85/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23149.8008 - mse: 574572672.0000 - mae: 23149.8008 - val_loss: 29218.5625 - val_mse: 855149376.0000 - val_mae: 29218.5625\n",
            "Epoch 86/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23222.1611 - mse: 578701184.0000 - mae: 23222.1602 - val_loss: 29195.3340 - val_mse: 853796608.0000 - val_mae: 29195.3340\n",
            "Epoch 87/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23141.6699 - mse: 573819520.0000 - mae: 23141.6680 - val_loss: 29170.0312 - val_mse: 852324544.0000 - val_mae: 29170.0312\n",
            "Epoch 88/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23111.5010 - mse: 573322560.0000 - mae: 23111.5000 - val_loss: 29144.5215 - val_mse: 850841536.0000 - val_mae: 29144.5215\n",
            "Epoch 89/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23116.4219 - mse: 573607936.0000 - mae: 23116.4219 - val_loss: 29119.4375 - val_mse: 849384640.0000 - val_mae: 29119.4375\n",
            "Epoch 90/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23147.5381 - mse: 575923520.0000 - mae: 23147.5371 - val_loss: 29095.6777 - val_mse: 848005760.0000 - val_mae: 29095.6777\n",
            "Epoch 91/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23136.5146 - mse: 575468416.0000 - mae: 23136.5156 - val_loss: 29071.4004 - val_mse: 846597888.0000 - val_mae: 29071.4004\n",
            "Epoch 92/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22992.9688 - mse: 568216896.0000 - mae: 22992.9688 - val_loss: 29044.4883 - val_mse: 845038592.0000 - val_mae: 29044.4883\n",
            "Epoch 93/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22911.9453 - mse: 563262592.0000 - mae: 22911.9453 - val_loss: 29016.0840 - val_mse: 843394752.0000 - val_mae: 29016.0840\n",
            "Epoch 94/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22884.9883 - mse: 564200320.0000 - mae: 22884.9883 - val_loss: 28987.6445 - val_mse: 841750208.0000 - val_mae: 28987.6445\n",
            "Epoch 95/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 23015.3408 - mse: 568127360.0000 - mae: 23015.3398 - val_loss: 28961.8066 - val_mse: 840257472.0000 - val_mae: 28961.8066\n",
            "Epoch 96/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22823.1016 - mse: 559179136.0000 - mae: 22823.1016 - val_loss: 28932.8340 - val_mse: 838585728.0000 - val_mae: 28932.8340\n",
            "Epoch 97/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22948.6250 - mse: 565672576.0000 - mae: 22948.6250 - val_loss: 28906.2598 - val_mse: 837053568.0000 - val_mae: 28906.2598\n",
            "Epoch 98/500\n",
            "20/20 [==============================] - 0s 904us/step - loss: 22761.4365 - mse: 556312704.0000 - mae: 22761.4355 - val_loss: 28876.1250 - val_mse: 835317888.0000 - val_mae: 28876.1250\n",
            "Epoch 99/500\n",
            "20/20 [==============================] - 0s 963us/step - loss: 22956.2998 - mse: 566666688.0000 - mae: 22956.3008 - val_loss: 28850.6680 - val_mse: 833853568.0000 - val_mae: 28850.6680\n",
            "Epoch 100/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22775.0938 - mse: 557320896.0000 - mae: 22775.0938 - val_loss: 28821.7441 - val_mse: 832191104.0000 - val_mae: 28821.7441\n",
            "Epoch 101/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22754.6006 - mse: 557526656.0000 - mae: 22754.5996 - val_loss: 28792.3066 - val_mse: 830500736.0000 - val_mae: 28792.3066\n",
            "Epoch 102/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22731.4541 - mse: 555364032.0000 - mae: 22731.4531 - val_loss: 28762.4961 - val_mse: 828790400.0000 - val_mae: 28762.4961\n",
            "Epoch 103/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22621.4697 - mse: 550308160.0000 - mae: 22621.4707 - val_loss: 28731.5312 - val_mse: 827016128.0000 - val_mae: 28731.5312\n",
            "Epoch 104/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22751.4355 - mse: 556526208.0000 - mae: 22751.4355 - val_loss: 28703.0625 - val_mse: 825386944.0000 - val_mae: 28703.0625\n",
            "Epoch 105/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22443.4346 - mse: 543059968.0000 - mae: 22443.4336 - val_loss: 28668.7500 - val_mse: 823425280.0000 - val_mae: 28668.7500\n",
            "Epoch 106/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22608.2520 - mse: 549939328.0000 - mae: 22608.2520 - val_loss: 28638.4746 - val_mse: 821696192.0000 - val_mae: 28638.4746\n",
            "Epoch 107/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22420.4414 - mse: 540187968.0000 - mae: 22420.4414 - val_loss: 28604.3965 - val_mse: 819751936.0000 - val_mae: 28604.3965\n",
            "Epoch 108/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22447.4707 - mse: 542892224.0000 - mae: 22447.4707 - val_loss: 28571.1660 - val_mse: 817857984.0000 - val_mae: 28571.1660\n",
            "Epoch 109/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22494.9229 - mse: 544831616.0000 - mae: 22494.9238 - val_loss: 28538.7695 - val_mse: 816013888.0000 - val_mae: 28538.7695\n",
            "Epoch 110/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22348.7051 - mse: 536966656.0000 - mae: 22348.7051 - val_loss: 28504.0684 - val_mse: 814040512.0000 - val_mae: 28504.0684\n",
            "Epoch 111/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 22307.1631 - mse: 537660160.0000 - mae: 22307.1621 - val_loss: 28468.9434 - val_mse: 812045952.0000 - val_mae: 28468.9434\n",
            "Epoch 112/500\n",
            "20/20 [==============================] - 0s 971us/step - loss: 22164.0859 - mse: 530834528.0000 - mae: 22164.0859 - val_loss: 28432.4121 - val_mse: 809974336.0000 - val_mae: 28432.4121\n",
            "Epoch 113/500\n",
            "20/20 [==============================] - 0s 999us/step - loss: 22289.7109 - mse: 535712064.0000 - mae: 22289.7109 - val_loss: 28398.2969 - val_mse: 808041472.0000 - val_mae: 28398.2969\n",
            "Epoch 114/500\n",
            "20/20 [==============================] - 0s 944us/step - loss: 22297.9619 - mse: 536000096.0000 - mae: 22297.9629 - val_loss: 28364.2656 - val_mse: 806115968.0000 - val_mae: 28364.2656\n",
            "Epoch 115/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22264.5381 - mse: 536492800.0000 - mae: 22264.5371 - val_loss: 28329.8242 - val_mse: 804170048.0000 - val_mae: 28329.8242\n",
            "Epoch 116/500\n",
            "20/20 [==============================] - 0s 965us/step - loss: 22234.0068 - mse: 533793792.0000 - mae: 22234.0059 - val_loss: 28295.1836 - val_mse: 802214592.0000 - val_mae: 28295.1836\n",
            "Epoch 117/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22343.8994 - mse: 538399872.0000 - mae: 22343.9004 - val_loss: 28262.1465 - val_mse: 800352128.0000 - val_mae: 28262.1465\n",
            "Epoch 118/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22077.9443 - mse: 524524640.0000 - mae: 22077.9434 - val_loss: 28224.5742 - val_mse: 798236672.0000 - val_mae: 28224.5742\n",
            "Epoch 119/500\n",
            "20/20 [==============================] - 0s 973us/step - loss: 22089.8027 - mse: 527367424.0000 - mae: 22089.8027 - val_loss: 28187.9121 - val_mse: 796175360.0000 - val_mae: 28187.9121\n",
            "Epoch 120/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22117.4297 - mse: 529491040.0000 - mae: 22117.4277 - val_loss: 28151.5508 - val_mse: 794134400.0000 - val_mae: 28151.5508\n",
            "Epoch 121/500\n",
            "20/20 [==============================] - 0s 953us/step - loss: 21906.3398 - mse: 517880896.0000 - mae: 21906.3398 - val_loss: 28111.9316 - val_mse: 791913600.0000 - val_mae: 28111.9316\n",
            "Epoch 122/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22106.6787 - mse: 528185088.0000 - mae: 22106.6777 - val_loss: 28075.8320 - val_mse: 789891840.0000 - val_mae: 28075.8320\n",
            "Epoch 123/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21993.2256 - mse: 522002944.0000 - mae: 21993.2246 - val_loss: 28038.2070 - val_mse: 787787776.0000 - val_mae: 28038.2070\n",
            "Epoch 124/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22009.3770 - mse: 524653472.0000 - mae: 22009.3789 - val_loss: 28000.9785 - val_mse: 785709312.0000 - val_mae: 28000.9785\n",
            "Epoch 125/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21618.0352 - mse: 505658560.0000 - mae: 21618.0352 - val_loss: 27957.7598 - val_mse: 783298752.0000 - val_mae: 27957.7598\n",
            "Epoch 126/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 21777.0918 - mse: 513370560.0000 - mae: 21777.0918 - val_loss: 27918.1973 - val_mse: 781096320.0000 - val_mae: 27918.1973\n",
            "Epoch 127/500\n",
            "20/20 [==============================] - 0s 894us/step - loss: 21799.3154 - mse: 513457152.0000 - mae: 21799.3164 - val_loss: 27878.7910 - val_mse: 778904768.0000 - val_mae: 27878.7910\n",
            "Epoch 128/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 22042.0459 - mse: 527331232.0000 - mae: 22042.0449 - val_loss: 27843.3125 - val_mse: 776935104.0000 - val_mae: 27843.3125\n",
            "Epoch 129/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21855.0098 - mse: 515532096.0000 - mae: 21855.0098 - val_loss: 27804.6250 - val_mse: 774789696.0000 - val_mae: 27804.6250\n",
            "Epoch 130/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21671.2266 - mse: 508945728.0000 - mae: 21671.2266 - val_loss: 27764.5879 - val_mse: 772572864.0000 - val_mae: 27764.5879\n",
            "Epoch 131/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21699.5332 - mse: 512803840.0000 - mae: 21699.5332 - val_loss: 27725.3496 - val_mse: 770404736.0000 - val_mae: 27725.3496\n",
            "Epoch 132/500\n",
            "20/20 [==============================] - 0s 968us/step - loss: 21711.2168 - mse: 514307584.0000 - mae: 21711.2168 - val_loss: 27686.3281 - val_mse: 768250496.0000 - val_mae: 27686.3281\n",
            "Epoch 133/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21437.4609 - mse: 500757504.0000 - mae: 21437.4609 - val_loss: 27643.7227 - val_mse: 765902592.0000 - val_mae: 27643.7227\n",
            "Epoch 134/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21367.0547 - mse: 495472832.0000 - mae: 21367.0547 - val_loss: 27600.4648 - val_mse: 763522752.0000 - val_mae: 27600.4648\n",
            "Epoch 135/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21377.4424 - mse: 494670688.0000 - mae: 21377.4434 - val_loss: 27556.5879 - val_mse: 761111680.0000 - val_mae: 27556.5879\n",
            "Epoch 136/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21503.9834 - mse: 504777216.0000 - mae: 21503.9844 - val_loss: 27514.7695 - val_mse: 758816064.0000 - val_mae: 27514.7695\n",
            "Epoch 137/500\n",
            "20/20 [==============================] - 0s 890us/step - loss: 21233.4307 - mse: 491631616.0000 - mae: 21233.4316 - val_loss: 27470.3652 - val_mse: 756384576.0000 - val_mae: 27470.3652\n",
            "Epoch 138/500\n",
            "20/20 [==============================] - 0s 880us/step - loss: 21275.5557 - mse: 490972096.0000 - mae: 21275.5566 - val_loss: 27426.2070 - val_mse: 753969792.0000 - val_mae: 27426.2070\n",
            "Epoch 139/500\n",
            "20/20 [==============================] - 0s 991us/step - loss: 21637.8262 - mse: 506343104.0000 - mae: 21637.8262 - val_loss: 27388.1367 - val_mse: 751891392.0000 - val_mae: 27388.1367\n",
            "Epoch 140/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20972.8555 - mse: 478466048.0000 - mae: 20972.8555 - val_loss: 27340.4648 - val_mse: 749292672.0000 - val_mae: 27340.4648\n",
            "Epoch 141/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21087.9951 - mse: 485798912.0000 - mae: 21087.9961 - val_loss: 27295.2500 - val_mse: 746832896.0000 - val_mae: 27295.2500\n",
            "Epoch 142/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21164.9482 - mse: 488804768.0000 - mae: 21164.9492 - val_loss: 27250.9746 - val_mse: 744426688.0000 - val_mae: 27250.9746\n",
            "Epoch 143/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21126.8496 - mse: 488126784.0000 - mae: 21126.8496 - val_loss: 27206.4492 - val_mse: 742010816.0000 - val_mae: 27206.4492\n",
            "Epoch 144/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20930.6484 - mse: 476825408.0000 - mae: 20930.6484 - val_loss: 27159.2461 - val_mse: 739454528.0000 - val_mae: 27159.2461\n",
            "Epoch 145/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21034.4834 - mse: 483207744.0000 - mae: 21034.4844 - val_loss: 27113.2656 - val_mse: 736967552.0000 - val_mae: 27113.2656\n",
            "Epoch 146/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 21031.0107 - mse: 484012544.0000 - mae: 21031.0117 - val_loss: 27068.4746 - val_mse: 734551232.0000 - val_mae: 27068.4746\n",
            "Epoch 147/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20842.6680 - mse: 469286496.0000 - mae: 20842.6680 - val_loss: 27020.5352 - val_mse: 731967744.0000 - val_mae: 27020.5352\n",
            "Epoch 148/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20656.5469 - mse: 464720832.0000 - mae: 20656.5469 - val_loss: 26971.3066 - val_mse: 729320832.0000 - val_mae: 26971.3066\n",
            "Epoch 149/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20782.6162 - mse: 472867584.0000 - mae: 20782.6152 - val_loss: 26923.8867 - val_mse: 726775488.0000 - val_mae: 26923.8867\n",
            "Epoch 150/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 20667.2168 - mse: 465982560.0000 - mae: 20667.2168 - val_loss: 26874.3184 - val_mse: 724117760.0000 - val_mae: 26874.3184\n",
            "Epoch 151/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20878.4883 - mse: 479672320.0000 - mae: 20878.4883 - val_loss: 26828.9434 - val_mse: 721691328.0000 - val_mae: 26828.9434\n",
            "Epoch 152/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20690.9785 - mse: 467810144.0000 - mae: 20690.9785 - val_loss: 26780.7930 - val_mse: 719119872.0000 - val_mae: 26780.7930\n",
            "Epoch 153/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20818.6035 - mse: 474061120.0000 - mae: 20818.6035 - val_loss: 26734.8711 - val_mse: 716671168.0000 - val_mae: 26734.8711\n",
            "Epoch 154/500\n",
            "20/20 [==============================] - 0s 997us/step - loss: 20265.6162 - mse: 447981312.0000 - mae: 20265.6152 - val_loss: 26681.8555 - val_mse: 713850240.0000 - val_mae: 26681.8555\n",
            "Epoch 155/500\n",
            "20/20 [==============================] - 0s 987us/step - loss: 20325.9404 - mse: 455893920.0000 - mae: 20325.9414 - val_loss: 26630.1465 - val_mse: 711104320.0000 - val_mae: 26630.1465\n",
            "Epoch 156/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20741.0459 - mse: 472344160.0000 - mae: 20741.0469 - val_loss: 26584.6406 - val_mse: 708693504.0000 - val_mae: 26584.6406\n",
            "Epoch 157/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20068.1299 - mse: 439714144.0000 - mae: 20068.1309 - val_loss: 26529.8848 - val_mse: 705796928.0000 - val_mae: 26529.8848\n",
            "Epoch 158/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20381.2734 - mse: 457845568.0000 - mae: 20381.2754 - val_loss: 26479.7461 - val_mse: 703148992.0000 - val_mae: 26479.7461\n",
            "Epoch 159/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20342.4277 - mse: 450765120.0000 - mae: 20342.4277 - val_loss: 26428.5840 - val_mse: 700451008.0000 - val_mae: 26428.5840\n",
            "Epoch 160/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 20008.4502 - mse: 437751200.0000 - mae: 20008.4492 - val_loss: 26374.5977 - val_mse: 697611072.0000 - val_mae: 26374.5977\n",
            "Epoch 161/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19875.0840 - mse: 437758464.0000 - mae: 19875.0840 - val_loss: 26319.9121 - val_mse: 694742784.0000 - val_mae: 26319.9121\n",
            "Epoch 162/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19974.8789 - mse: 439674688.0000 - mae: 19974.8789 - val_loss: 26266.3008 - val_mse: 691935616.0000 - val_mae: 26266.3008\n",
            "Epoch 163/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19790.3408 - mse: 424958400.0000 - mae: 19790.3398 - val_loss: 26210.6914 - val_mse: 689028736.0000 - val_mae: 26210.6914\n",
            "Epoch 164/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19913.2842 - mse: 440233152.0000 - mae: 19913.2852 - val_loss: 26157.8320 - val_mse: 686272832.0000 - val_mae: 26157.8320\n",
            "Epoch 165/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19747.5068 - mse: 432647328.0000 - mae: 19747.5059 - val_loss: 26102.8223 - val_mse: 683411328.0000 - val_mae: 26102.8223\n",
            "Epoch 166/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 19831.9072 - mse: 432177056.0000 - mae: 19831.9062 - val_loss: 26049.0254 - val_mse: 680618624.0000 - val_mae: 26049.0254\n",
            "Epoch 167/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 19291.2090 - mse: 411807872.0000 - mae: 19291.2090 - val_loss: 25990.1914 - val_mse: 677569920.0000 - val_mae: 25990.1914\n",
            "Epoch 168/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 19911.1670 - mse: 436150368.0000 - mae: 19911.1660 - val_loss: 25937.5117 - val_mse: 674845184.0000 - val_mae: 25937.5117\n",
            "Epoch 169/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19789.5400 - mse: 432611488.0000 - mae: 19789.5410 - val_loss: 25883.6133 - val_mse: 672064256.0000 - val_mae: 25883.6133\n",
            "Epoch 170/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19425.9043 - mse: 417062816.0000 - mae: 19425.9043 - val_loss: 25825.1211 - val_mse: 669050688.0000 - val_mae: 25825.1211\n",
            "Epoch 171/500\n",
            "20/20 [==============================] - 0s 928us/step - loss: 19489.1289 - mse: 421246304.0000 - mae: 19489.1289 - val_loss: 25768.3086 - val_mse: 666132352.0000 - val_mae: 25768.3086\n",
            "Epoch 172/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19214.5820 - mse: 407427712.0000 - mae: 19214.5820 - val_loss: 25708.7246 - val_mse: 663077504.0000 - val_mae: 25708.7246\n",
            "Epoch 173/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 20175.3818 - mse: 453141760.0000 - mae: 20175.3809 - val_loss: 25661.1328 - val_mse: 660644160.0000 - val_mae: 25661.1328\n",
            "Epoch 174/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19388.4883 - mse: 415643328.0000 - mae: 19388.4883 - val_loss: 25605.0586 - val_mse: 657782592.0000 - val_mae: 25605.0586\n",
            "Epoch 175/500\n",
            "20/20 [==============================] - 0s 880us/step - loss: 19063.9795 - mse: 404170944.0000 - mae: 19063.9805 - val_loss: 25544.5391 - val_mse: 654699712.0000 - val_mae: 25544.5391\n",
            "Epoch 176/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19636.8164 - mse: 428798272.0000 - mae: 19636.8164 - val_loss: 25491.2910 - val_mse: 651995904.0000 - val_mae: 25491.2910\n",
            "Epoch 177/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19040.5176 - mse: 402914240.0000 - mae: 19040.5195 - val_loss: 25432.2695 - val_mse: 649003904.0000 - val_mae: 25432.2695\n",
            "Epoch 178/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 19262.1270 - mse: 415678016.0000 - mae: 19262.1289 - val_loss: 25376.0273 - val_mse: 646159424.0000 - val_mae: 25376.0273\n",
            "Epoch 179/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18882.4434 - mse: 400944192.0000 - mae: 18882.4434 - val_loss: 25315.4590 - val_mse: 643104256.0000 - val_mae: 25315.4590\n",
            "Epoch 180/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18971.3730 - mse: 405617216.0000 - mae: 18971.3711 - val_loss: 25256.5801 - val_mse: 640140416.0000 - val_mae: 25256.5801\n",
            "Epoch 181/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18919.7812 - mse: 398902464.0000 - mae: 18919.7812 - val_loss: 25196.7676 - val_mse: 637136640.0000 - val_mae: 25196.7676\n",
            "Epoch 182/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 19224.8291 - mse: 408101440.0000 - mae: 19224.8281 - val_loss: 25141.0684 - val_mse: 634346368.0000 - val_mae: 25141.0684\n",
            "Epoch 183/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18642.7617 - mse: 390457536.0000 - mae: 18642.7617 - val_loss: 25078.3887 - val_mse: 631211392.0000 - val_mae: 25078.3887\n",
            "Epoch 184/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18677.4131 - mse: 388022464.0000 - mae: 18677.4121 - val_loss: 25016.7598 - val_mse: 628137088.0000 - val_mae: 25016.7598\n",
            "Epoch 185/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18915.9502 - mse: 395157760.0000 - mae: 18915.9492 - val_loss: 24957.9062 - val_mse: 625208512.0000 - val_mae: 24957.9062\n",
            "Epoch 186/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18736.1602 - mse: 397470944.0000 - mae: 18736.1602 - val_loss: 24897.5156 - val_mse: 622210368.0000 - val_mae: 24897.5156\n",
            "Epoch 187/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18455.2119 - mse: 382800704.0000 - mae: 18455.2129 - val_loss: 24834.7812 - val_mse: 619106496.0000 - val_mae: 24834.7812\n",
            "Epoch 188/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18627.4463 - mse: 386943712.0000 - mae: 18627.4473 - val_loss: 24774.1406 - val_mse: 616110848.0000 - val_mae: 24774.1406\n",
            "Epoch 189/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18704.8701 - mse: 390628832.0000 - mae: 18704.8711 - val_loss: 24714.8086 - val_mse: 613188288.0000 - val_mae: 24714.8086\n",
            "Epoch 190/500\n",
            "20/20 [==============================] - 0s 918us/step - loss: 18223.3350 - mse: 375044672.0000 - mae: 18223.3340 - val_loss: 24650.9551 - val_mse: 610051584.0000 - val_mae: 24650.9551\n",
            "Epoch 191/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18243.1904 - mse: 383396672.0000 - mae: 18243.1914 - val_loss: 24587.4941 - val_mse: 606942208.0000 - val_mae: 24587.4941\n",
            "Epoch 192/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18242.7646 - mse: 375691168.0000 - mae: 18242.7656 - val_loss: 24523.7715 - val_mse: 603827008.0000 - val_mae: 24523.7715\n",
            "Epoch 193/500\n",
            "20/20 [==============================] - 0s 987us/step - loss: 18368.4150 - mse: 369735328.0000 - mae: 18368.4160 - val_loss: 24461.4492 - val_mse: 600786752.0000 - val_mae: 24461.4492\n",
            "Epoch 194/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18004.7461 - mse: 362021984.0000 - mae: 18004.7461 - val_loss: 24396.3496 - val_mse: 597621440.0000 - val_mae: 24396.3496\n",
            "Epoch 195/500\n",
            "20/20 [==============================] - 0s 917us/step - loss: 17687.4551 - mse: 345392160.0000 - mae: 17687.4570 - val_loss: 24327.6562 - val_mse: 594288832.0000 - val_mae: 24327.6562\n",
            "Epoch 196/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 18142.0859 - mse: 373689792.0000 - mae: 18142.0879 - val_loss: 24264.7578 - val_mse: 591248512.0000 - val_mae: 24264.7578\n",
            "Epoch 197/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17804.8828 - mse: 356748128.0000 - mae: 17804.8828 - val_loss: 24197.3320 - val_mse: 587993280.0000 - val_mae: 24197.3320\n",
            "Epoch 198/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17554.6895 - mse: 344238400.0000 - mae: 17554.6914 - val_loss: 24128.2656 - val_mse: 584670208.0000 - val_mae: 24128.2656\n",
            "Epoch 199/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17927.4766 - mse: 355497984.0000 - mae: 17927.4766 - val_loss: 24063.8848 - val_mse: 581582336.0000 - val_mae: 24063.8848\n",
            "Epoch 200/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17585.1143 - mse: 351150496.0000 - mae: 17585.1133 - val_loss: 23996.1914 - val_mse: 578344576.0000 - val_mae: 23996.1914\n",
            "Epoch 201/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17870.6943 - mse: 366581984.0000 - mae: 17870.6934 - val_loss: 23932.7773 - val_mse: 575320576.0000 - val_mae: 23932.7773\n",
            "Epoch 202/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 17630.5244 - mse: 348765248.0000 - mae: 17630.5254 - val_loss: 23866.6758 - val_mse: 572177984.0000 - val_mae: 23866.6758\n",
            "Epoch 203/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17467.7266 - mse: 342100480.0000 - mae: 17467.7266 - val_loss: 23798.2129 - val_mse: 568929792.0000 - val_mae: 23798.2129\n",
            "Epoch 204/500\n",
            "20/20 [==============================] - 0s 956us/step - loss: 17185.1606 - mse: 331682752.0000 - mae: 17185.1602 - val_loss: 23727.5840 - val_mse: 565588480.0000 - val_mae: 23727.5840\n",
            "Epoch 205/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17142.0933 - mse: 332681472.0000 - mae: 17142.0938 - val_loss: 23657.7207 - val_mse: 562292736.0000 - val_mae: 23657.7207\n",
            "Epoch 206/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 17149.1299 - mse: 332906240.0000 - mae: 17149.1309 - val_loss: 23587.9355 - val_mse: 559011648.0000 - val_mae: 23587.9355\n",
            "Epoch 207/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16494.2285 - mse: 305341536.0000 - mae: 16494.2285 - val_loss: 23512.7188 - val_mse: 555488064.0000 - val_mae: 23512.7188\n",
            "Epoch 208/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16756.2144 - mse: 314561632.0000 - mae: 16756.2148 - val_loss: 23439.9746 - val_mse: 552088640.0000 - val_mae: 23439.9746\n",
            "Epoch 209/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 17622.5615 - mse: 347462016.0000 - mae: 17622.5605 - val_loss: 23375.5469 - val_mse: 549088192.0000 - val_mae: 23375.5469\n",
            "Epoch 210/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16706.8877 - mse: 327508672.0000 - mae: 16706.8867 - val_loss: 23303.4844 - val_mse: 545742336.0000 - val_mae: 23303.4844\n",
            "Epoch 211/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16962.7798 - mse: 313320384.0000 - mae: 16962.7793 - val_loss: 23232.9336 - val_mse: 542472960.0000 - val_mae: 23232.9336\n",
            "Epoch 212/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16938.4570 - mse: 329171552.0000 - mae: 16938.4570 - val_loss: 23162.7734 - val_mse: 539231808.0000 - val_mae: 23162.7734\n",
            "Epoch 213/500\n",
            "20/20 [==============================] - 0s 959us/step - loss: 17001.3926 - mse: 332386880.0000 - mae: 17001.3926 - val_loss: 23093.1934 - val_mse: 536028320.0000 - val_mae: 23093.1934\n",
            "Epoch 214/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16967.0400 - mse: 337307328.0000 - mae: 16967.0410 - val_loss: 23024.3242 - val_mse: 532868512.0000 - val_mae: 23024.3242\n",
            "Epoch 215/500\n",
            "20/20 [==============================] - 0s 954us/step - loss: 16442.9707 - mse: 317003584.0000 - mae: 16442.9727 - val_loss: 22950.7480 - val_mse: 529502144.0000 - val_mae: 22950.7480\n",
            "Epoch 216/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16436.7534 - mse: 305410400.0000 - mae: 16436.7539 - val_loss: 22877.4258 - val_mse: 526158400.0000 - val_mae: 22877.4258\n",
            "Epoch 217/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15456.6328 - mse: 289101248.0000 - mae: 15456.6328 - val_loss: 22796.1289 - val_mse: 522467008.0000 - val_mae: 22796.1289\n",
            "Epoch 218/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16796.5879 - mse: 342200928.0000 - mae: 16796.5879 - val_loss: 22727.6055 - val_mse: 519365312.0000 - val_mae: 22727.6055\n",
            "Epoch 219/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15946.5542 - mse: 303650240.0000 - mae: 15946.5547 - val_loss: 22651.4863 - val_mse: 515931392.0000 - val_mae: 22651.4863\n",
            "Epoch 220/500\n",
            "20/20 [==============================] - 0s 972us/step - loss: 15949.0396 - mse: 301740256.0000 - mae: 15949.0391 - val_loss: 22575.3086 - val_mse: 512505408.0000 - val_mae: 22575.3086\n",
            "Epoch 221/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16122.4351 - mse: 299157152.0000 - mae: 16122.4346 - val_loss: 22501.4102 - val_mse: 509192256.0000 - val_mae: 22501.4102\n",
            "Epoch 222/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16591.8984 - mse: 309704992.0000 - mae: 16591.8984 - val_loss: 22431.6289 - val_mse: 506072416.0000 - val_mae: 22431.6289\n",
            "Epoch 223/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15698.5254 - mse: 286846240.0000 - mae: 15698.5254 - val_loss: 22353.7578 - val_mse: 502603104.0000 - val_mae: 22353.7578\n",
            "Epoch 224/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16132.4819 - mse: 305925312.0000 - mae: 16132.4814 - val_loss: 22280.9609 - val_mse: 499370944.0000 - val_mae: 22280.9609\n",
            "Epoch 225/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15909.3506 - mse: 290481472.0000 - mae: 15909.3496 - val_loss: 22206.5430 - val_mse: 496079456.0000 - val_mae: 22206.5430\n",
            "Epoch 226/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15345.9092 - mse: 269638304.0000 - mae: 15345.9092 - val_loss: 22126.8770 - val_mse: 492565312.0000 - val_mae: 22126.8770\n",
            "Epoch 227/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 16146.4048 - mse: 297796960.0000 - mae: 16146.4043 - val_loss: 22053.3320 - val_mse: 489330592.0000 - val_mae: 22053.3320\n",
            "Epoch 228/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 15233.1201 - mse: 265141216.0000 - mae: 15233.1201 - val_loss: 21973.4141 - val_mse: 485830144.0000 - val_mae: 21973.4141\n",
            "Epoch 229/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 16195.3301 - mse: 297887168.0000 - mae: 16195.3311 - val_loss: 21902.7070 - val_mse: 482747072.0000 - val_mae: 21902.7070\n",
            "Epoch 230/500\n",
            "20/20 [==============================] - 0s 964us/step - loss: 15077.9531 - mse: 271805312.0000 - mae: 15077.9531 - val_loss: 21821.9082 - val_mse: 479234560.0000 - val_mae: 21821.9082\n",
            "Epoch 231/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 15483.5537 - mse: 281902528.0000 - mae: 15483.5527 - val_loss: 21744.7812 - val_mse: 475891200.0000 - val_mae: 21744.7812\n",
            "Epoch 232/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 14531.1255 - mse: 246138160.0000 - mae: 14531.1250 - val_loss: 21660.7129 - val_mse: 472265024.0000 - val_mae: 21660.7129\n",
            "Epoch 233/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 14600.9717 - mse: 258597168.0000 - mae: 14600.9717 - val_loss: 21577.3633 - val_mse: 468681920.0000 - val_mae: 21577.3633\n",
            "Epoch 234/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 15685.9297 - mse: 291423680.0000 - mae: 15685.9297 - val_loss: 21503.3379 - val_mse: 465511264.0000 - val_mae: 21503.3379\n",
            "Epoch 235/500\n",
            "20/20 [==============================] - 0s 822us/step - loss: 15468.8945 - mse: 295230848.0000 - mae: 15468.8936 - val_loss: 21427.7715 - val_mse: 462284352.0000 - val_mae: 21427.7715\n",
            "Epoch 236/500\n",
            "20/20 [==============================] - 0s 928us/step - loss: 14352.8618 - mse: 261841568.0000 - mae: 14352.8623 - val_loss: 21342.9766 - val_mse: 458676032.0000 - val_mae: 21342.9766\n",
            "Epoch 237/500\n",
            "20/20 [==============================] - 0s 902us/step - loss: 15089.8130 - mse: 266958768.0000 - mae: 15089.8125 - val_loss: 21265.3223 - val_mse: 455387584.0000 - val_mae: 21265.3223\n",
            "Epoch 238/500\n",
            "20/20 [==============================] - 0s 923us/step - loss: 14923.9409 - mse: 264934608.0000 - mae: 14923.9404 - val_loss: 21185.9941 - val_mse: 452039680.0000 - val_mae: 21185.9941\n",
            "Epoch 239/500\n",
            "20/20 [==============================] - 0s 954us/step - loss: 15578.9146 - mse: 279732480.0000 - mae: 15578.9160 - val_loss: 21111.6934 - val_mse: 448912064.0000 - val_mae: 21111.6934\n",
            "Epoch 240/500\n",
            "20/20 [==============================] - 0s 971us/step - loss: 14623.9502 - mse: 250041296.0000 - mae: 14623.9502 - val_loss: 21030.4043 - val_mse: 445504256.0000 - val_mae: 21030.4043\n",
            "Epoch 241/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 14054.9722 - mse: 230891744.0000 - mae: 14054.9717 - val_loss: 20944.8242 - val_mse: 441933888.0000 - val_mae: 20944.8242\n",
            "Epoch 242/500\n",
            "20/20 [==============================] - 0s 858us/step - loss: 13322.2417 - mse: 220715264.0000 - mae: 13322.2402 - val_loss: 20862.6484 - val_mse: 438514336.0000 - val_mae: 20862.6484\n",
            "Epoch 243/500\n",
            "20/20 [==============================] - 0s 708us/step - loss: 13679.9619 - mse: 229853648.0000 - mae: 13679.9629 - val_loss: 20774.6094 - val_mse: 434871456.0000 - val_mae: 20774.6094\n",
            "Epoch 244/500\n",
            "20/20 [==============================] - 0s 882us/step - loss: 15492.8608 - mse: 282763072.0000 - mae: 15492.8613 - val_loss: 20702.0605 - val_mse: 431879872.0000 - val_mae: 20702.0605\n",
            "Epoch 245/500\n",
            "20/20 [==============================] - 0s 979us/step - loss: 14419.5493 - mse: 260406944.0000 - mae: 14419.5498 - val_loss: 20621.3809 - val_mse: 428566688.0000 - val_mae: 20621.3809\n",
            "Epoch 246/500\n",
            "20/20 [==============================] - 0s 870us/step - loss: 14811.1055 - mse: 259448992.0000 - mae: 14811.1064 - val_loss: 20553.6113 - val_mse: 425792576.0000 - val_mae: 20553.6113\n",
            "Epoch 247/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13770.5669 - mse: 231339168.0000 - mae: 13770.5674 - val_loss: 20476.5410 - val_mse: 422646368.0000 - val_mae: 20476.5410\n",
            "Epoch 248/500\n",
            "20/20 [==============================] - 0s 960us/step - loss: 14143.8306 - mse: 237352496.0000 - mae: 14143.8311 - val_loss: 20403.0605 - val_mse: 419658464.0000 - val_mae: 20403.0605\n",
            "Epoch 249/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13303.3525 - mse: 208452288.0000 - mae: 13303.3535 - val_loss: 20312.5684 - val_mse: 415991360.0000 - val_mae: 20312.5684\n",
            "Epoch 250/500\n",
            "20/20 [==============================] - 0s 968us/step - loss: 14077.9468 - mse: 242519552.0000 - mae: 14077.9473 - val_loss: 20229.9766 - val_mse: 412661408.0000 - val_mae: 20229.9766\n",
            "Epoch 251/500\n",
            "20/20 [==============================] - 0s 952us/step - loss: 13688.3203 - mse: 229215696.0000 - mae: 13688.3203 - val_loss: 20153.7461 - val_mse: 409601056.0000 - val_mae: 20153.7461\n",
            "Epoch 252/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 13608.2373 - mse: 220584144.0000 - mae: 13608.2373 - val_loss: 20068.0625 - val_mse: 406173856.0000 - val_mae: 20068.0625\n",
            "Epoch 253/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 12725.7646 - mse: 208004736.0000 - mae: 12725.7646 - val_loss: 19985.1211 - val_mse: 402870784.0000 - val_mae: 19985.1211\n",
            "Epoch 254/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13180.3799 - mse: 213290736.0000 - mae: 13180.3799 - val_loss: 19897.0195 - val_mse: 399379840.0000 - val_mae: 19897.0195\n",
            "Epoch 255/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 13838.4302 - mse: 225435104.0000 - mae: 13838.4316 - val_loss: 19813.6367 - val_mse: 396086976.0000 - val_mae: 19813.6367\n",
            "Epoch 256/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13163.3989 - mse: 209054816.0000 - mae: 13163.3984 - val_loss: 19734.7051 - val_mse: 392983232.0000 - val_mae: 19734.7051\n",
            "Epoch 257/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 13634.5327 - mse: 228750800.0000 - mae: 13634.5332 - val_loss: 19660.5059 - val_mse: 390075776.0000 - val_mae: 19660.5059\n",
            "Epoch 258/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13143.6973 - mse: 215074720.0000 - mae: 13143.6973 - val_loss: 19581.7246 - val_mse: 387001728.0000 - val_mae: 19581.7246\n",
            "Epoch 259/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13165.0332 - mse: 218651648.0000 - mae: 13165.0332 - val_loss: 19503.7812 - val_mse: 383974880.0000 - val_mae: 19503.7812\n",
            "Epoch 260/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13071.3564 - mse: 211411216.0000 - mae: 13071.3564 - val_loss: 19425.2402 - val_mse: 380935680.0000 - val_mae: 19425.2402\n",
            "Epoch 261/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12988.4355 - mse: 214169296.0000 - mae: 12988.4355 - val_loss: 19347.9238 - val_mse: 377951328.0000 - val_mae: 19347.9238\n",
            "Epoch 262/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12451.9517 - mse: 193729632.0000 - mae: 12451.9512 - val_loss: 19275.3242 - val_mse: 375158848.0000 - val_mae: 19275.3242\n",
            "Epoch 263/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13696.7954 - mse: 234947488.0000 - mae: 13696.7949 - val_loss: 19202.2461 - val_mse: 372365152.0000 - val_mae: 19202.2461\n",
            "Epoch 264/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 12785.1997 - mse: 195566848.0000 - mae: 12785.2002 - val_loss: 19112.8711 - val_mse: 368962112.0000 - val_mae: 19112.8711\n",
            "Epoch 265/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13465.4634 - mse: 223202016.0000 - mae: 13465.4629 - val_loss: 19047.2695 - val_mse: 366471680.0000 - val_mae: 19047.2695\n",
            "Epoch 266/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12977.4248 - mse: 209977200.0000 - mae: 12977.4248 - val_loss: 18968.4141 - val_mse: 363491136.0000 - val_mae: 18968.4141\n",
            "Epoch 267/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12325.4692 - mse: 184829696.0000 - mae: 12325.4688 - val_loss: 18885.5215 - val_mse: 360367808.0000 - val_mae: 18885.5215\n",
            "Epoch 268/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12665.5908 - mse: 210880416.0000 - mae: 12665.5908 - val_loss: 18804.9570 - val_mse: 357348000.0000 - val_mae: 18804.9570\n",
            "Epoch 269/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12156.0669 - mse: 182900064.0000 - mae: 12156.0674 - val_loss: 18712.9023 - val_mse: 353917664.0000 - val_mae: 18712.9023\n",
            "Epoch 270/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12595.0859 - mse: 200965776.0000 - mae: 12595.0859 - val_loss: 18633.7012 - val_mse: 350976704.0000 - val_mae: 18633.7012\n",
            "Epoch 271/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 13159.6367 - mse: 205619872.0000 - mae: 13159.6377 - val_loss: 18557.2070 - val_mse: 348146720.0000 - val_mae: 18557.2070\n",
            "Epoch 272/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12481.0156 - mse: 186293408.0000 - mae: 12481.0156 - val_loss: 18467.4297 - val_mse: 344841664.0000 - val_mae: 18467.4297\n",
            "Epoch 273/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11346.4546 - mse: 165873120.0000 - mae: 11346.4551 - val_loss: 18370.1523 - val_mse: 341285184.0000 - val_mae: 18370.1523\n",
            "Epoch 274/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10416.9170 - mse: 141707168.0000 - mae: 10416.9170 - val_loss: 18266.9141 - val_mse: 337529184.0000 - val_mae: 18266.9141\n",
            "Epoch 275/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 12163.7422 - mse: 189887056.0000 - mae: 12163.7422 - val_loss: 18200.3398 - val_mse: 335113504.0000 - val_mae: 18200.3398\n",
            "Epoch 276/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12279.0430 - mse: 198291616.0000 - mae: 12279.0430 - val_loss: 18111.3594 - val_mse: 331905088.0000 - val_mae: 18111.3594\n",
            "Epoch 277/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11293.2261 - mse: 163827152.0000 - mae: 11293.2266 - val_loss: 18015.6426 - val_mse: 328469440.0000 - val_mae: 18015.6426\n",
            "Epoch 278/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12674.6709 - mse: 204938016.0000 - mae: 12674.6709 - val_loss: 17938.2070 - val_mse: 325704800.0000 - val_mae: 17938.2070\n",
            "Epoch 279/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12479.8550 - mse: 192221584.0000 - mae: 12479.8545 - val_loss: 17859.4570 - val_mse: 322905760.0000 - val_mae: 17859.4570\n",
            "Epoch 280/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12386.4746 - mse: 197201888.0000 - mae: 12386.4746 - val_loss: 17791.0352 - val_mse: 320480192.0000 - val_mae: 17791.0352\n",
            "Epoch 281/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11884.2485 - mse: 190349856.0000 - mae: 11884.2480 - val_loss: 17729.2324 - val_mse: 318297888.0000 - val_mae: 17729.2324\n",
            "Epoch 282/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12239.7661 - mse: 181313696.0000 - mae: 12239.7656 - val_loss: 17658.9980 - val_mse: 315822144.0000 - val_mae: 17658.9980\n",
            "Epoch 283/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11303.4473 - mse: 153448352.0000 - mae: 11303.4473 - val_loss: 17574.2383 - val_mse: 312854784.0000 - val_mae: 17574.2383\n",
            "Epoch 284/500\n",
            "20/20 [==============================] - 0s 949us/step - loss: 11555.1162 - mse: 170049536.0000 - mae: 11555.1162 - val_loss: 17489.7129 - val_mse: 309908800.0000 - val_mae: 17489.7129\n",
            "Epoch 285/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9793.4302 - mse: 122867560.0000 - mae: 9793.4297 - val_loss: 17391.7344 - val_mse: 306509792.0000 - val_mae: 17391.7344\n",
            "Epoch 286/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11809.9521 - mse: 182706736.0000 - mae: 11809.9512 - val_loss: 17319.8164 - val_mse: 304024640.0000 - val_mae: 17319.8164\n",
            "Epoch 287/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10441.1445 - mse: 139365312.0000 - mae: 10441.1436 - val_loss: 17236.3633 - val_mse: 301156448.0000 - val_mae: 17236.3633\n",
            "Epoch 288/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12432.9727 - mse: 191198496.0000 - mae: 12432.9727 - val_loss: 17158.7402 - val_mse: 298504800.0000 - val_mae: 17158.7402\n",
            "Epoch 289/500\n",
            "20/20 [==============================] - 0s 942us/step - loss: 11111.4707 - mse: 163663696.0000 - mae: 11111.4707 - val_loss: 17093.4180 - val_mse: 296274240.0000 - val_mae: 17093.4180\n",
            "Epoch 290/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10739.5229 - mse: 150270112.0000 - mae: 10739.5225 - val_loss: 17003.5820 - val_mse: 293228320.0000 - val_mae: 17003.5820\n",
            "Epoch 291/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 10399.9539 - mse: 142555360.0000 - mae: 10399.9541 - val_loss: 16914.4609 - val_mse: 290221152.0000 - val_mae: 16914.4609\n",
            "Epoch 292/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11030.9480 - mse: 164694624.0000 - mae: 11030.9482 - val_loss: 16837.2695 - val_mse: 287630080.0000 - val_mae: 16837.2695\n",
            "Epoch 293/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10472.6924 - mse: 157548640.0000 - mae: 10472.6924 - val_loss: 16783.4023 - val_mse: 285826944.0000 - val_mae: 16783.4023\n",
            "Epoch 294/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9742.0112 - mse: 130243864.0000 - mae: 9742.0107 - val_loss: 16708.8379 - val_mse: 283343008.0000 - val_mae: 16708.8379\n",
            "Epoch 295/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12058.9722 - mse: 170914144.0000 - mae: 12058.9717 - val_loss: 16637.5977 - val_mse: 280980384.0000 - val_mae: 16637.5977\n",
            "Epoch 296/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10629.4629 - mse: 155446784.0000 - mae: 10629.4629 - val_loss: 16569.2949 - val_mse: 278717664.0000 - val_mae: 16569.2949\n",
            "Epoch 297/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11074.5073 - mse: 167011424.0000 - mae: 11074.5078 - val_loss: 16474.8555 - val_mse: 275618368.0000 - val_mae: 16474.8555\n",
            "Epoch 298/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11524.0215 - mse: 174594208.0000 - mae: 11524.0215 - val_loss: 16412.1699 - val_mse: 273567424.0000 - val_mae: 16412.1699\n",
            "Epoch 299/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 12255.0361 - mse: 199667664.0000 - mae: 12255.0361 - val_loss: 16355.8340 - val_mse: 271725216.0000 - val_mae: 16355.8340\n",
            "Epoch 300/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11358.5059 - mse: 165103456.0000 - mae: 11358.5059 - val_loss: 16283.1094 - val_mse: 269364928.0000 - val_mae: 16283.1094\n",
            "Epoch 301/500\n",
            "20/20 [==============================] - 0s 999us/step - loss: 10035.3262 - mse: 126349264.0000 - mae: 10035.3262 - val_loss: 16189.4326 - val_mse: 266342496.0000 - val_mae: 16189.4326\n",
            "Epoch 302/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11301.9199 - mse: 161784336.0000 - mae: 11301.9199 - val_loss: 16114.6230 - val_mse: 263941584.0000 - val_mae: 16114.6230\n",
            "Epoch 303/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9813.0435 - mse: 123712768.0000 - mae: 9813.0439 - val_loss: 16039.5908 - val_mse: 261539584.0000 - val_mae: 16039.5908\n",
            "Epoch 304/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9620.2065 - mse: 115203328.0000 - mae: 9620.2061 - val_loss: 15946.4502 - val_mse: 258582272.0000 - val_mae: 15946.4502\n",
            "Epoch 305/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11070.3208 - mse: 152235120.0000 - mae: 11070.3203 - val_loss: 15871.1738 - val_mse: 256204640.0000 - val_mae: 15871.1738\n",
            "Epoch 306/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9450.7266 - mse: 118234544.0000 - mae: 9450.7266 - val_loss: 15817.5059 - val_mse: 254508768.0000 - val_mae: 15817.5059\n",
            "Epoch 307/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9375.4429 - mse: 113735344.0000 - mae: 9375.4434 - val_loss: 15739.2471 - val_mse: 252052512.0000 - val_mae: 15739.2471\n",
            "Epoch 308/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11662.4272 - mse: 172598304.0000 - mae: 11662.4268 - val_loss: 15666.6855 - val_mse: 249787056.0000 - val_mae: 15666.6855\n",
            "Epoch 309/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10004.8037 - mse: 136304816.0000 - mae: 10004.8027 - val_loss: 15573.8096 - val_mse: 246903424.0000 - val_mae: 15573.8096\n",
            "Epoch 310/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10231.3721 - mse: 139221536.0000 - mae: 10231.3721 - val_loss: 15503.7861 - val_mse: 244736608.0000 - val_mae: 15503.7861\n",
            "Epoch 311/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10199.8696 - mse: 135940864.0000 - mae: 10199.8691 - val_loss: 15442.7637 - val_mse: 242856112.0000 - val_mae: 15442.7637\n",
            "Epoch 312/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10165.1270 - mse: 129914960.0000 - mae: 10165.1270 - val_loss: 15363.0732 - val_mse: 240414256.0000 - val_mae: 15363.0732\n",
            "Epoch 313/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8648.8940 - mse: 103806064.0000 - mae: 8648.8936 - val_loss: 15285.3438 - val_mse: 238043872.0000 - val_mae: 15285.3438\n",
            "Epoch 314/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 11220.4712 - mse: 163211216.0000 - mae: 11220.4717 - val_loss: 15234.3066 - val_mse: 236489776.0000 - val_mae: 15234.3066\n",
            "Epoch 315/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10001.3577 - mse: 133821272.0000 - mae: 10001.3574 - val_loss: 15177.1592 - val_mse: 234758880.0000 - val_mae: 15177.1592\n",
            "Epoch 316/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10626.7939 - mse: 147736480.0000 - mae: 10626.7939 - val_loss: 15109.1855 - val_mse: 232711040.0000 - val_mae: 15109.1855\n",
            "Epoch 317/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10024.9800 - mse: 134998176.0000 - mae: 10024.9795 - val_loss: 15049.7783 - val_mse: 230926128.0000 - val_mae: 15049.7783\n",
            "Epoch 318/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9646.4155 - mse: 125321704.0000 - mae: 9646.4160 - val_loss: 14965.3516 - val_mse: 228404256.0000 - val_mae: 14965.3516\n",
            "Epoch 319/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 9514.0947 - mse: 112796584.0000 - mae: 9514.0957 - val_loss: 14899.5645 - val_mse: 226445744.0000 - val_mae: 14899.5645\n",
            "Epoch 320/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 9650.7881 - mse: 123894488.0000 - mae: 9650.7871 - val_loss: 14815.4580 - val_mse: 223961984.0000 - val_mae: 14815.4580\n",
            "Epoch 321/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10341.2041 - mse: 131720368.0000 - mae: 10341.2051 - val_loss: 14747.0254 - val_mse: 221945904.0000 - val_mae: 14747.0254\n",
            "Epoch 322/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7360.6082 - mse: 81406928.0000 - mae: 7360.6079 - val_loss: 14660.9512 - val_mse: 219426384.0000 - val_mae: 14660.9512\n",
            "Epoch 323/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 9942.4609 - mse: 131559272.0000 - mae: 9942.4609 - val_loss: 14570.1191 - val_mse: 216792480.0000 - val_mae: 14570.1191\n",
            "Epoch 324/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10649.7363 - mse: 152712032.0000 - mae: 10649.7363 - val_loss: 14513.8955 - val_mse: 215167488.0000 - val_mae: 14513.8955\n",
            "Epoch 325/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7381.5505 - mse: 79396032.0000 - mae: 7381.5508 - val_loss: 14450.1406 - val_mse: 213325584.0000 - val_mae: 14450.1406\n",
            "Epoch 326/500\n",
            "20/20 [==============================] - 0s 997us/step - loss: 9126.5996 - mse: 113428976.0000 - mae: 9126.5996 - val_loss: 14383.3457 - val_mse: 211409280.0000 - val_mae: 14383.3457\n",
            "Epoch 327/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9535.1685 - mse: 116152784.0000 - mae: 9535.1689 - val_loss: 14330.9141 - val_mse: 209907456.0000 - val_mae: 14330.9141\n",
            "Epoch 328/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7557.0276 - mse: 71112312.0000 - mae: 7557.0283 - val_loss: 14241.8965 - val_mse: 207377408.0000 - val_mae: 14241.8965\n",
            "Epoch 329/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 9667.2432 - mse: 126908400.0000 - mae: 9667.2441 - val_loss: 14150.2168 - val_mse: 204798976.0000 - val_mae: 14150.2168\n",
            "Epoch 330/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8406.9224 - mse: 93687472.0000 - mae: 8406.9219 - val_loss: 14070.0859 - val_mse: 202552672.0000 - val_mae: 14070.0859\n",
            "Epoch 331/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9495.1987 - mse: 109731816.0000 - mae: 9495.1982 - val_loss: 14006.0176 - val_mse: 200762528.0000 - val_mae: 14006.0176\n",
            "Epoch 332/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9012.5098 - mse: 97380960.0000 - mae: 9012.5098 - val_loss: 13927.2139 - val_mse: 198573376.0000 - val_mae: 13927.2139\n",
            "Epoch 333/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 9208.7891 - mse: 124939032.0000 - mae: 9208.7891 - val_loss: 13860.7500 - val_mse: 196734464.0000 - val_mae: 13860.7500\n",
            "Epoch 334/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 10210.2778 - mse: 132856272.0000 - mae: 10210.2783 - val_loss: 13796.7002 - val_mse: 194973472.0000 - val_mae: 13796.7002\n",
            "Epoch 335/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9275.3479 - mse: 113931072.0000 - mae: 9275.3477 - val_loss: 13731.9316 - val_mse: 193202576.0000 - val_mae: 13731.9316\n",
            "Epoch 336/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9898.7920 - mse: 121289008.0000 - mae: 9898.7920 - val_loss: 13659.5469 - val_mse: 191229536.0000 - val_mae: 13659.5469\n",
            "Epoch 337/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8758.1528 - mse: 113067824.0000 - mae: 8758.1533 - val_loss: 13582.9844 - val_mse: 189160736.0000 - val_mae: 13582.9844\n",
            "Epoch 338/500\n",
            "20/20 [==============================] - 0s 954us/step - loss: 9712.9980 - mse: 120800208.0000 - mae: 9712.9980 - val_loss: 13519.8545 - val_mse: 187458464.0000 - val_mae: 13519.8545\n",
            "Epoch 339/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8353.1943 - mse: 95397416.0000 - mae: 8353.1943 - val_loss: 13448.7871 - val_mse: 185555360.0000 - val_mae: 13448.7871\n",
            "Epoch 340/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8278.9275 - mse: 103344576.0000 - mae: 8278.9277 - val_loss: 13376.4668 - val_mse: 183623088.0000 - val_mae: 13376.4668\n",
            "Epoch 341/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7248.1062 - mse: 70368192.0000 - mae: 7248.1064 - val_loss: 13324.4248 - val_mse: 182234800.0000 - val_mae: 13324.4248\n",
            "Epoch 342/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7456.7383 - mse: 78458448.0000 - mae: 7456.7383 - val_loss: 13250.5566 - val_mse: 180282624.0000 - val_mae: 13250.5566\n",
            "Epoch 343/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8626.8269 - mse: 101453656.0000 - mae: 8626.8262 - val_loss: 13186.1357 - val_mse: 178586016.0000 - val_mae: 13186.1357\n",
            "Epoch 344/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8193.6611 - mse: 85857144.0000 - mae: 8193.6611 - val_loss: 13115.9062 - val_mse: 176745456.0000 - val_mae: 13115.9062\n",
            "Epoch 345/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7965.5234 - mse: 88041920.0000 - mae: 7965.5234 - val_loss: 13057.0088 - val_mse: 175208992.0000 - val_mae: 13057.0088\n",
            "Epoch 346/500\n",
            "20/20 [==============================] - 0s 990us/step - loss: 8651.0701 - mse: 99997264.0000 - mae: 8651.0703 - val_loss: 12989.1436 - val_mse: 173448800.0000 - val_mae: 12989.1436\n",
            "Epoch 347/500\n",
            "20/20 [==============================] - 0s 943us/step - loss: 10989.1860 - mse: 146298464.0000 - mae: 10989.1855 - val_loss: 12947.4199 - val_mse: 172366336.0000 - val_mae: 12947.4199\n",
            "Epoch 348/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7905.8882 - mse: 84869608.0000 - mae: 7905.8882 - val_loss: 12862.0273 - val_mse: 170175600.0000 - val_mae: 12862.0273\n",
            "Epoch 349/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7491.1770 - mse: 80621376.0000 - mae: 7491.1768 - val_loss: 12786.9775 - val_mse: 168262144.0000 - val_mae: 12786.9775\n",
            "Epoch 350/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7067.7971 - mse: 72098640.0000 - mae: 7067.7969 - val_loss: 12687.4941 - val_mse: 165747264.0000 - val_mae: 12687.4941\n",
            "Epoch 351/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7231.6089 - mse: 80874944.0000 - mae: 7231.6094 - val_loss: 12624.8857 - val_mse: 164165072.0000 - val_mae: 12624.8857\n",
            "Epoch 352/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8196.8354 - mse: 89924088.0000 - mae: 8196.8350 - val_loss: 12601.3887 - val_mse: 163565568.0000 - val_mae: 12601.3887\n",
            "Epoch 353/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8464.9270 - mse: 85890936.0000 - mae: 8464.9268 - val_loss: 12534.6191 - val_mse: 161893792.0000 - val_mae: 12534.6191\n",
            "Epoch 354/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7863.1147 - mse: 85053968.0000 - mae: 7863.1147 - val_loss: 12473.0879 - val_mse: 160359392.0000 - val_mae: 12473.0879\n",
            "Epoch 355/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8413.3435 - mse: 93920512.0000 - mae: 8413.3438 - val_loss: 12416.5703 - val_mse: 158956384.0000 - val_mae: 12416.5703\n",
            "Epoch 356/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8579.9988 - mse: 102874480.0000 - mae: 8579.9980 - val_loss: 12348.7734 - val_mse: 157287824.0000 - val_mae: 12348.7734\n",
            "Epoch 357/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8590.7700 - mse: 99792112.0000 - mae: 8590.7705 - val_loss: 12292.0400 - val_mse: 155893632.0000 - val_mae: 12292.0400\n",
            "Epoch 358/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7895.1465 - mse: 93449008.0000 - mae: 7895.1460 - val_loss: 12229.8291 - val_mse: 154374960.0000 - val_mae: 12229.8291\n",
            "Epoch 359/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8011.7222 - mse: 91310936.0000 - mae: 8011.7217 - val_loss: 12188.7334 - val_mse: 153368480.0000 - val_mae: 12188.7334\n",
            "Epoch 360/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7871.0911 - mse: 87136328.0000 - mae: 7871.0908 - val_loss: 12137.3262 - val_mse: 152115872.0000 - val_mae: 12137.3262\n",
            "Epoch 361/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7448.5215 - mse: 83590272.0000 - mae: 7448.5220 - val_loss: 12076.6924 - val_mse: 150651808.0000 - val_mae: 12076.6924\n",
            "Epoch 362/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7081.1748 - mse: 62802316.0000 - mae: 7081.1748 - val_loss: 12013.7725 - val_mse: 149141248.0000 - val_mae: 12013.7725\n",
            "Epoch 363/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6263.4875 - mse: 50173908.0000 - mae: 6263.4873 - val_loss: 11928.7656 - val_mse: 147120592.0000 - val_mae: 11928.7656\n",
            "Epoch 364/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7655.5676 - mse: 72351376.0000 - mae: 7655.5674 - val_loss: 11854.5127 - val_mse: 145367264.0000 - val_mae: 11854.5127\n",
            "Epoch 365/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8956.9829 - mse: 100080272.0000 - mae: 8956.9824 - val_loss: 11803.0342 - val_mse: 144155552.0000 - val_mae: 11803.0342\n",
            "Epoch 366/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7122.2029 - mse: 72974704.0000 - mae: 7122.2031 - val_loss: 11744.7090 - val_mse: 142785232.0000 - val_mae: 11744.7090\n",
            "Epoch 367/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6218.4817 - mse: 54006164.0000 - mae: 6218.4814 - val_loss: 11686.9482 - val_mse: 141439360.0000 - val_mae: 11686.9482\n",
            "Epoch 368/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8377.2368 - mse: 81811568.0000 - mae: 8377.2363 - val_loss: 11628.8184 - val_mse: 140088160.0000 - val_mae: 11628.8184\n",
            "Epoch 369/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7337.1299 - mse: 74380176.0000 - mae: 7337.1299 - val_loss: 11565.0674 - val_mse: 138617680.0000 - val_mae: 11565.0674\n",
            "Epoch 370/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6843.7156 - mse: 63930316.0000 - mae: 6843.7158 - val_loss: 11485.4648 - val_mse: 136793216.0000 - val_mae: 11485.4648\n",
            "Epoch 371/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6862.4290 - mse: 74112488.0000 - mae: 6862.4287 - val_loss: 11406.9395 - val_mse: 135006368.0000 - val_mae: 11406.9395\n",
            "Epoch 372/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7921.8059 - mse: 97371344.0000 - mae: 7921.8062 - val_loss: 11382.1699 - val_mse: 134434752.0000 - val_mae: 11382.1699\n",
            "Epoch 373/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6608.6094 - mse: 59624600.0000 - mae: 6608.6094 - val_loss: 11314.0107 - val_mse: 132896400.0000 - val_mae: 11314.0107\n",
            "Epoch 374/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7500.0593 - mse: 73213672.0000 - mae: 7500.0596 - val_loss: 11243.7773 - val_mse: 131321392.0000 - val_mae: 11243.7773\n",
            "Epoch 375/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7009.8347 - mse: 70165608.0000 - mae: 7009.8345 - val_loss: 11146.4502 - val_mse: 129158992.0000 - val_mae: 11146.4502\n",
            "Epoch 376/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7950.7913 - mse: 88782208.0000 - mae: 7950.7905 - val_loss: 11077.4863 - val_mse: 127637032.0000 - val_mae: 11077.4863\n",
            "Epoch 377/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8543.6812 - mse: 101861608.0000 - mae: 8543.6816 - val_loss: 11021.6143 - val_mse: 126406632.0000 - val_mae: 11021.6143\n",
            "Epoch 378/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6390.2563 - mse: 53155832.0000 - mae: 6390.2563 - val_loss: 10956.5225 - val_mse: 124978712.0000 - val_mae: 10956.5225\n",
            "Epoch 379/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8022.4985 - mse: 83908616.0000 - mae: 8022.4985 - val_loss: 10897.4580 - val_mse: 123694288.0000 - val_mae: 10897.4580\n",
            "Epoch 380/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6735.6094 - mse: 70458880.0000 - mae: 6735.6094 - val_loss: 10875.9482 - val_mse: 123217768.0000 - val_mae: 10875.9482\n",
            "Epoch 381/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7938.9431 - mse: 90029584.0000 - mae: 7938.9429 - val_loss: 10846.2168 - val_mse: 122568048.0000 - val_mae: 10846.2168\n",
            "Epoch 382/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8160.4268 - mse: 78724392.0000 - mae: 8160.4268 - val_loss: 10814.7695 - val_mse: 121880944.0000 - val_mae: 10814.7695\n",
            "Epoch 383/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7635.7168 - mse: 73954680.0000 - mae: 7635.7173 - val_loss: 10772.7402 - val_mse: 120978768.0000 - val_mae: 10772.7402\n",
            "Epoch 384/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7710.1494 - mse: 91980496.0000 - mae: 7710.1494 - val_loss: 10699.9707 - val_mse: 119424512.0000 - val_mae: 10699.9707\n",
            "Epoch 385/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7037.6018 - mse: 69499848.0000 - mae: 7037.6016 - val_loss: 10657.4463 - val_mse: 118516840.0000 - val_mae: 10657.4463\n",
            "Epoch 386/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8292.7415 - mse: 93777120.0000 - mae: 8292.7412 - val_loss: 10621.8691 - val_mse: 117765760.0000 - val_mae: 10621.8691\n",
            "Epoch 387/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7887.6985 - mse: 81430032.0000 - mae: 7887.6982 - val_loss: 10588.1748 - val_mse: 117047568.0000 - val_mae: 10588.1748\n",
            "Epoch 388/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7516.2166 - mse: 72791360.0000 - mae: 7516.2163 - val_loss: 10541.0625 - val_mse: 116051888.0000 - val_mae: 10541.0625\n",
            "Epoch 389/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9341.1025 - mse: 116297960.0000 - mae: 9341.1025 - val_loss: 10490.7285 - val_mse: 114998376.0000 - val_mae: 10490.7285\n",
            "Epoch 390/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7117.1619 - mse: 76058984.0000 - mae: 7117.1616 - val_loss: 10442.0547 - val_mse: 113980456.0000 - val_mae: 10442.0547\n",
            "Epoch 391/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7515.3984 - mse: 77338600.0000 - mae: 7515.3984 - val_loss: 10395.1123 - val_mse: 113004144.0000 - val_mae: 10395.1123\n",
            "Epoch 392/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8622.8848 - mse: 100034792.0000 - mae: 8622.8848 - val_loss: 10329.5215 - val_mse: 111649944.0000 - val_mae: 10329.5215\n",
            "Epoch 393/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7412.1094 - mse: 77261040.0000 - mae: 7412.1094 - val_loss: 10309.6211 - val_mse: 111229976.0000 - val_mae: 10309.6211\n",
            "Epoch 394/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6357.7754 - mse: 55059968.0000 - mae: 6357.7754 - val_loss: 10241.2090 - val_mse: 109832784.0000 - val_mae: 10241.2090\n",
            "Epoch 395/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5342.7817 - mse: 36825196.0000 - mae: 5342.7817 - val_loss: 10144.8027 - val_mse: 107884208.0000 - val_mae: 10144.8027\n",
            "Epoch 396/500\n",
            "20/20 [==============================] - 0s 956us/step - loss: 7325.9465 - mse: 71505104.0000 - mae: 7325.9468 - val_loss: 10085.7480 - val_mse: 106696208.0000 - val_mae: 10085.7480\n",
            "Epoch 397/500\n",
            "20/20 [==============================] - 0s 988us/step - loss: 7266.7910 - mse: 65989016.0000 - mae: 7266.7905 - val_loss: 10025.6348 - val_mse: 105493072.0000 - val_mae: 10025.6348\n",
            "Epoch 398/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8424.2495 - mse: 88886896.0000 - mae: 8424.2500 - val_loss: 10005.0449 - val_mse: 105080720.0000 - val_mae: 10005.0449\n",
            "Epoch 399/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7126.0588 - mse: 71052976.0000 - mae: 7126.0596 - val_loss: 10000.6328 - val_mse: 104983144.0000 - val_mae: 10000.6328\n",
            "Epoch 400/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8383.9724 - mse: 92332880.0000 - mae: 8383.9717 - val_loss: 9980.9170 - val_mse: 104584688.0000 - val_mae: 9980.9170\n",
            "Epoch 401/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6509.0247 - mse: 53477824.0000 - mae: 6509.0244 - val_loss: 9939.0732 - val_mse: 103751264.0000 - val_mae: 9939.0732\n",
            "Epoch 402/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7395.4817 - mse: 68080088.0000 - mae: 7395.4814 - val_loss: 9909.1230 - val_mse: 103157848.0000 - val_mae: 9909.1230\n",
            "Epoch 403/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6925.8716 - mse: 64498696.0000 - mae: 6925.8721 - val_loss: 9866.9131 - val_mse: 102322152.0000 - val_mae: 9866.9131\n",
            "Epoch 404/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8060.3967 - mse: 85106096.0000 - mae: 8060.3970 - val_loss: 9820.6777 - val_mse: 101412096.0000 - val_mae: 9820.6777\n",
            "Epoch 405/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5724.4780 - mse: 58749184.0000 - mae: 5724.4780 - val_loss: 9828.3135 - val_mse: 101543344.0000 - val_mae: 9828.3135\n",
            "Epoch 406/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6852.9309 - mse: 69966864.0000 - mae: 6852.9312 - val_loss: 9801.2031 - val_mse: 101010448.0000 - val_mae: 9801.2031\n",
            "Epoch 407/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5820.2556 - mse: 57006408.0000 - mae: 5820.2554 - val_loss: 9707.3955 - val_mse: 99197464.0000 - val_mae: 9707.3955\n",
            "Epoch 408/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7349.7634 - mse: 77585592.0000 - mae: 7349.7632 - val_loss: 9643.9873 - val_mse: 97977432.0000 - val_mae: 9643.9873\n",
            "Epoch 409/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8088.9751 - mse: 80448176.0000 - mae: 8088.9751 - val_loss: 9605.0752 - val_mse: 97235864.0000 - val_mae: 9605.0752\n",
            "Epoch 410/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7247.1946 - mse: 72479824.0000 - mae: 7247.1943 - val_loss: 9560.9336 - val_mse: 96390032.0000 - val_mae: 9560.9336\n",
            "Epoch 411/500\n",
            "20/20 [==============================] - 0s 981us/step - loss: 5295.8660 - mse: 49847904.0000 - mae: 5295.8662 - val_loss: 9488.6611 - val_mse: 95021344.0000 - val_mae: 9488.6611\n",
            "Epoch 412/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6810.0120 - mse: 69561544.0000 - mae: 6810.0117 - val_loss: 9422.8545 - val_mse: 93784216.0000 - val_mae: 9422.8545\n",
            "Epoch 413/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7390.5237 - mse: 71177616.0000 - mae: 7390.5234 - val_loss: 9396.3887 - val_mse: 93282344.0000 - val_mae: 9396.3887\n",
            "Epoch 414/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7178.6499 - mse: 70641816.0000 - mae: 7178.6499 - val_loss: 9320.9258 - val_mse: 91883096.0000 - val_mae: 9320.9258\n",
            "Epoch 415/500\n",
            "20/20 [==============================] - 0s 786us/step - loss: 5769.4160 - mse: 55789716.0000 - mae: 5769.4160 - val_loss: 9299.8770 - val_mse: 91478320.0000 - val_mae: 9299.8770\n",
            "Epoch 416/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7272.8245 - mse: 71652584.0000 - mae: 7272.8242 - val_loss: 9296.3027 - val_mse: 91401944.0000 - val_mae: 9296.3027\n",
            "Epoch 417/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6861.1802 - mse: 73023712.0000 - mae: 6861.1797 - val_loss: 9262.4639 - val_mse: 90771728.0000 - val_mae: 9262.4639\n",
            "Epoch 418/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7465.3840 - mse: 72716016.0000 - mae: 7465.3843 - val_loss: 9219.1475 - val_mse: 89978872.0000 - val_mae: 9219.1475\n",
            "Epoch 419/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7794.8250 - mse: 74794600.0000 - mae: 7794.8252 - val_loss: 9173.8457 - val_mse: 89145984.0000 - val_mae: 9173.8457\n",
            "Epoch 420/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8328.7451 - mse: 96880216.0000 - mae: 8328.7451 - val_loss: 9118.0586 - val_mse: 88130696.0000 - val_mae: 9118.0586\n",
            "Epoch 421/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6558.8657 - mse: 57632884.0000 - mae: 6558.8657 - val_loss: 9069.4473 - val_mse: 87257088.0000 - val_mae: 9069.4473\n",
            "Epoch 422/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6682.9387 - mse: 62163040.0000 - mae: 6682.9390 - val_loss: 9025.5771 - val_mse: 86468720.0000 - val_mae: 9025.5771\n",
            "Epoch 423/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5363.8000 - mse: 47533488.0000 - mae: 5363.7998 - val_loss: 8970.9395 - val_mse: 85485872.0000 - val_mae: 8970.9395\n",
            "Epoch 424/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7104.0461 - mse: 77334656.0000 - mae: 7104.0459 - val_loss: 8939.3271 - val_mse: 84914392.0000 - val_mae: 8939.3271\n",
            "Epoch 425/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7775.4919 - mse: 89807344.0000 - mae: 7775.4922 - val_loss: 8879.0381 - val_mse: 83843160.0000 - val_mae: 8879.0381\n",
            "Epoch 426/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6074.0164 - mse: 58160480.0000 - mae: 6074.0166 - val_loss: 8867.6035 - val_mse: 83646640.0000 - val_mae: 8867.6035\n",
            "Epoch 427/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7285.2246 - mse: 73936856.0000 - mae: 7285.2251 - val_loss: 8838.3389 - val_mse: 83125392.0000 - val_mae: 8838.3389\n",
            "Epoch 428/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7997.0610 - mse: 81300992.0000 - mae: 7997.0610 - val_loss: 8823.0605 - val_mse: 82845800.0000 - val_mae: 8823.0605\n",
            "Epoch 429/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7530.5872 - mse: 74750936.0000 - mae: 7530.5874 - val_loss: 8789.3613 - val_mse: 82249488.0000 - val_mae: 8789.3613\n",
            "Epoch 430/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8233.7178 - mse: 98025816.0000 - mae: 8233.7178 - val_loss: 8741.5381 - val_mse: 81414352.0000 - val_mae: 8741.5381\n",
            "Epoch 431/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7373.0266 - mse: 66327592.0000 - mae: 7373.0264 - val_loss: 8681.0762 - val_mse: 80365200.0000 - val_mae: 8681.0762\n",
            "Epoch 432/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5964.6809 - mse: 60640408.0000 - mae: 5964.6807 - val_loss: 8663.4268 - val_mse: 80052248.0000 - val_mae: 8663.4268\n",
            "Epoch 433/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5923.3325 - mse: 46811636.0000 - mae: 5923.3330 - val_loss: 8610.8037 - val_mse: 79145920.0000 - val_mae: 8610.8037\n",
            "Epoch 434/500\n",
            "20/20 [==============================] - 0s 933us/step - loss: 7899.7305 - mse: 80009872.0000 - mae: 7899.7305 - val_loss: 8578.5469 - val_mse: 78588136.0000 - val_mae: 8578.5469\n",
            "Epoch 435/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8159.9541 - mse: 95087912.0000 - mae: 8159.9546 - val_loss: 8566.6445 - val_mse: 78374776.0000 - val_mae: 8566.6445\n",
            "Epoch 436/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8333.5549 - mse: 83854672.0000 - mae: 8333.5547 - val_loss: 8570.8213 - val_mse: 78430320.0000 - val_mae: 8570.8213\n",
            "Epoch 437/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5764.0999 - mse: 45427828.0000 - mae: 5764.1001 - val_loss: 8529.6895 - val_mse: 77726968.0000 - val_mae: 8529.6895\n",
            "Epoch 438/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7295.3674 - mse: 75302384.0000 - mae: 7295.3672 - val_loss: 8479.3711 - val_mse: 76871768.0000 - val_mae: 8479.3711\n",
            "Epoch 439/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7527.1843 - mse: 78733040.0000 - mae: 7527.1846 - val_loss: 8432.3613 - val_mse: 76077776.0000 - val_mae: 8432.3613\n",
            "Epoch 440/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7312.0122 - mse: 69584680.0000 - mae: 7312.0127 - val_loss: 8412.8076 - val_mse: 75739088.0000 - val_mae: 8412.8076\n",
            "Epoch 441/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7611.6477 - mse: 76288360.0000 - mae: 7611.6475 - val_loss: 8412.8477 - val_mse: 75726512.0000 - val_mae: 8412.8477\n",
            "Epoch 442/500\n",
            "20/20 [==============================] - 0s 922us/step - loss: 7444.2617 - mse: 85353048.0000 - mae: 7444.2617 - val_loss: 8407.9434 - val_mse: 75644288.0000 - val_mae: 8407.9434\n",
            "Epoch 443/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6662.4216 - mse: 54750336.0000 - mae: 6662.4219 - val_loss: 8372.0049 - val_mse: 75040416.0000 - val_mae: 8372.0049\n",
            "Epoch 444/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7667.4216 - mse: 79073880.0000 - mae: 7667.4219 - val_loss: 8354.0859 - val_mse: 74737096.0000 - val_mae: 8354.0859\n",
            "Epoch 445/500\n",
            "20/20 [==============================] - 0s 907us/step - loss: 8009.4033 - mse: 87618504.0000 - mae: 8009.4033 - val_loss: 8326.0713 - val_mse: 74265232.0000 - val_mae: 8326.0713\n",
            "Epoch 446/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7307.6482 - mse: 75606528.0000 - mae: 7307.6484 - val_loss: 8297.4141 - val_mse: 73790992.0000 - val_mae: 8297.4141\n",
            "Epoch 447/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6348.6750 - mse: 67933808.0000 - mae: 6348.6748 - val_loss: 8296.0527 - val_mse: 73764496.0000 - val_mae: 8296.0527\n",
            "Epoch 448/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6914.0859 - mse: 67608336.0000 - mae: 6914.0859 - val_loss: 8301.5273 - val_mse: 73848472.0000 - val_mae: 8301.5273\n",
            "Epoch 449/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6253.3633 - mse: 55565416.0000 - mae: 6253.3633 - val_loss: 8273.6973 - val_mse: 73389168.0000 - val_mae: 8273.6973\n",
            "Epoch 450/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5312.3770 - mse: 45469696.0000 - mae: 5312.3770 - val_loss: 8221.7090 - val_mse: 72533024.0000 - val_mae: 8221.7090\n",
            "Epoch 451/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7659.7861 - mse: 83383856.0000 - mae: 7659.7861 - val_loss: 8222.5488 - val_mse: 72530480.0000 - val_mae: 8222.5488\n",
            "Epoch 452/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8019.2102 - mse: 95136784.0000 - mae: 8019.2100 - val_loss: 8258.7988 - val_mse: 73119400.0000 - val_mae: 8258.7988\n",
            "Epoch 453/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 9075.9468 - mse: 94999880.0000 - mae: 9075.9473 - val_loss: 8285.2217 - val_mse: 73537600.0000 - val_mae: 8285.2217\n",
            "Epoch 454/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5920.6047 - mse: 53078396.0000 - mae: 5920.6045 - val_loss: 8275.4307 - val_mse: 73367624.0000 - val_mae: 8275.4307\n",
            "Epoch 455/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5958.7644 - mse: 47291468.0000 - mae: 5958.7646 - val_loss: 8223.8193 - val_mse: 72520976.0000 - val_mae: 8223.8193\n",
            "Epoch 456/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5768.4153 - mse: 50579780.0000 - mae: 5768.4150 - val_loss: 8165.3062 - val_mse: 71567264.0000 - val_mae: 8165.3062\n",
            "Epoch 457/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8243.5991 - mse: 99261104.0000 - mae: 8243.5996 - val_loss: 8128.6748 - val_mse: 70981656.0000 - val_mae: 8128.6748\n",
            "Epoch 458/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6188.2439 - mse: 56257128.0000 - mae: 6188.2437 - val_loss: 8101.4868 - val_mse: 70532528.0000 - val_mae: 8101.4868\n",
            "Epoch 459/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8391.0249 - mse: 97277280.0000 - mae: 8391.0254 - val_loss: 8078.1729 - val_mse: 70156096.0000 - val_mae: 8078.1729\n",
            "Epoch 460/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5268.4507 - mse: 38576524.0000 - mae: 5268.4507 - val_loss: 8098.6836 - val_mse: 70472240.0000 - val_mae: 8098.6836\n",
            "Epoch 461/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 4960.5226 - mse: 46692412.0000 - mae: 4960.5225 - val_loss: 8120.7173 - val_mse: 70821976.0000 - val_mae: 8120.7173\n",
            "Epoch 462/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7121.2998 - mse: 75417648.0000 - mae: 7121.2998 - val_loss: 8109.4780 - val_mse: 70639760.0000 - val_mae: 8109.4780\n",
            "Epoch 463/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6077.8618 - mse: 56604172.0000 - mae: 6077.8618 - val_loss: 8105.1963 - val_mse: 70566960.0000 - val_mae: 8105.1963\n",
            "Epoch 464/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 5566.8630 - mse: 51700820.0000 - mae: 5566.8628 - val_loss: 8079.2290 - val_mse: 70141528.0000 - val_mae: 8079.2290\n",
            "Epoch 465/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8087.9985 - mse: 83833536.0000 - mae: 8087.9985 - val_loss: 8026.5913 - val_mse: 69298064.0000 - val_mae: 8026.5913\n",
            "Epoch 466/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5852.4954 - mse: 62527512.0000 - mae: 5852.4951 - val_loss: 8005.6929 - val_mse: 68959504.0000 - val_mae: 8005.6929\n",
            "Epoch 467/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6026.8562 - mse: 52962936.0000 - mae: 6026.8564 - val_loss: 7967.1562 - val_mse: 68337584.0000 - val_mae: 7967.1562\n",
            "Epoch 468/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6894.9773 - mse: 69629016.0000 - mae: 6894.9775 - val_loss: 8018.5049 - val_mse: 69132456.0000 - val_mae: 8018.5049\n",
            "Epoch 469/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8576.8809 - mse: 96621040.0000 - mae: 8576.8809 - val_loss: 7999.1250 - val_mse: 68830704.0000 - val_mae: 7999.1250\n",
            "Epoch 470/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7066.6589 - mse: 77809960.0000 - mae: 7066.6592 - val_loss: 7969.3853 - val_mse: 68351984.0000 - val_mae: 7969.3853\n",
            "Epoch 471/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7335.4973 - mse: 80538088.0000 - mae: 7335.4971 - val_loss: 7931.3232 - val_mse: 67751024.0000 - val_mae: 7931.3232\n",
            "Epoch 472/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5906.3938 - mse: 52083424.0000 - mae: 5906.3936 - val_loss: 7881.0479 - val_mse: 66961760.0000 - val_mae: 7881.0479\n",
            "Epoch 473/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 4467.1333 - mse: 34868924.0000 - mae: 4467.1338 - val_loss: 7898.5312 - val_mse: 67219520.0000 - val_mae: 7898.5312\n",
            "Epoch 474/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 7931.7859 - mse: 82916600.0000 - mae: 7931.7861 - val_loss: 7888.8740 - val_mse: 67062328.0000 - val_mae: 7888.8740\n",
            "Epoch 475/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8383.4158 - mse: 101523952.0000 - mae: 8383.4160 - val_loss: 7851.0796 - val_mse: 66468824.0000 - val_mae: 7851.0796\n",
            "Epoch 476/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6647.3145 - mse: 73095936.0000 - mae: 6647.3140 - val_loss: 7834.5508 - val_mse: 66199176.0000 - val_mae: 7834.5508\n",
            "Epoch 477/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6896.4238 - mse: 68443912.0000 - mae: 6896.4233 - val_loss: 7835.5923 - val_mse: 66215924.0000 - val_mae: 7835.5923\n",
            "Epoch 478/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5580.2712 - mse: 50988864.0000 - mae: 5580.2710 - val_loss: 7850.8662 - val_mse: 66450168.0000 - val_mae: 7850.8662\n",
            "Epoch 479/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 5542.7050 - mse: 51131544.0000 - mae: 5542.7051 - val_loss: 7910.5010 - val_mse: 67375800.0000 - val_mae: 7910.5010\n",
            "Epoch 480/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7648.6345 - mse: 87872392.0000 - mae: 7648.6343 - val_loss: 7905.3096 - val_mse: 67288136.0000 - val_mae: 7905.3096\n",
            "Epoch 481/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7652.9583 - mse: 77378792.0000 - mae: 7652.9580 - val_loss: 7867.9648 - val_mse: 66700608.0000 - val_mae: 7867.9648\n",
            "Epoch 482/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 8057.6599 - mse: 80254640.0000 - mae: 8057.6592 - val_loss: 7863.4155 - val_mse: 66620312.0000 - val_mae: 7863.4155\n",
            "Epoch 483/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7567.8035 - mse: 78810496.0000 - mae: 7567.8032 - val_loss: 7835.0010 - val_mse: 66184024.0000 - val_mae: 7835.0010\n",
            "Epoch 484/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 5896.8049 - mse: 55388288.0000 - mae: 5896.8052 - val_loss: 7782.5283 - val_mse: 65369460.0000 - val_mae: 7782.5283\n",
            "Epoch 485/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5291.4812 - mse: 52528960.0000 - mae: 5291.4814 - val_loss: 7739.7217 - val_mse: 64705592.0000 - val_mae: 7739.7217\n",
            "Epoch 486/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5931.1355 - mse: 48910968.0000 - mae: 5931.1357 - val_loss: 7821.5493 - val_mse: 65969484.0000 - val_mae: 7821.5493\n",
            "Epoch 487/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 4494.3242 - mse: 35528544.0000 - mae: 4494.3242 - val_loss: 7814.0859 - val_mse: 65842728.0000 - val_mae: 7814.0859\n",
            "Epoch 488/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6922.6008 - mse: 68204552.0000 - mae: 6922.6006 - val_loss: 7787.5249 - val_mse: 65420216.0000 - val_mae: 7787.5249\n",
            "Epoch 489/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6104.3013 - mse: 56049716.0000 - mae: 6104.3018 - val_loss: 7769.6846 - val_mse: 65141620.0000 - val_mae: 7769.6846\n",
            "Epoch 490/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5370.8965 - mse: 45949176.0000 - mae: 5370.8965 - val_loss: 7716.2178 - val_mse: 64317972.0000 - val_mae: 7716.2178\n",
            "Epoch 491/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6388.0076 - mse: 60352064.0000 - mae: 6388.0078 - val_loss: 7728.2686 - val_mse: 64490636.0000 - val_mae: 7728.2686\n",
            "Epoch 492/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6184.2400 - mse: 62183616.0000 - mae: 6184.2397 - val_loss: 7711.0234 - val_mse: 64223208.0000 - val_mae: 7711.0234\n",
            "Epoch 493/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8185.3691 - mse: 95875200.0000 - mae: 8185.3687 - val_loss: 7735.6577 - val_mse: 64581208.0000 - val_mae: 7735.6577\n",
            "Epoch 494/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6187.4434 - mse: 69729792.0000 - mae: 6187.4434 - val_loss: 7728.9346 - val_mse: 64464232.0000 - val_mae: 7728.9346\n",
            "Epoch 495/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 7239.2986 - mse: 63540492.0000 - mae: 7239.2983 - val_loss: 7713.3506 - val_mse: 64217908.0000 - val_mae: 7713.3506\n",
            "Epoch 496/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6447.4316 - mse: 63369560.0000 - mae: 6447.4316 - val_loss: 7695.5376 - val_mse: 63937512.0000 - val_mae: 7695.5376\n",
            "Epoch 497/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 6332.0989 - mse: 65570572.0000 - mae: 6332.0986 - val_loss: 7680.4609 - val_mse: 63706144.0000 - val_mae: 7680.4609\n",
            "Epoch 498/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 8082.2854 - mse: 85172976.0000 - mae: 8082.2861 - val_loss: 7698.1187 - val_mse: 63959768.0000 - val_mae: 7698.1187\n",
            "Epoch 499/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 6966.6899 - mse: 73423656.0000 - mae: 6966.6899 - val_loss: 7719.3115 - val_mse: 64280936.0000 - val_mae: 7719.3115\n",
            "Epoch 500/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 5665.7646 - mse: 47631856.0000 - mae: 5665.7646 - val_loss: 7697.5781 - val_mse: 63951064.0000 - val_mae: 7697.5781\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 30 samples, validate on 30 samples\n",
            "Epoch 1/500\n",
            "30/30 [==============================] - 2s 61ms/step - loss: 24466.5072 - mse: 625249472.0000 - mae: 24466.5078 - val_loss: 26857.1940 - val_mse: 732522368.0000 - val_mae: 26857.1934\n",
            "Epoch 2/500\n",
            "30/30 [==============================] - 0s 785us/step - loss: 24466.4505 - mse: 625246784.0000 - mae: 24466.4492 - val_loss: 26857.1029 - val_mse: 732517312.0000 - val_mae: 26857.1035\n",
            "Epoch 3/500\n",
            "30/30 [==============================] - 0s 790us/step - loss: 24466.3346 - mse: 625240768.0000 - mae: 24466.3340 - val_loss: 26856.6966 - val_mse: 732493952.0000 - val_mae: 26856.6953\n",
            "Epoch 4/500\n",
            "30/30 [==============================] - 0s 722us/step - loss: 24465.4375 - mse: 625197184.0000 - mae: 24465.4375 - val_loss: 26854.9310 - val_mse: 732391488.0000 - val_mae: 26854.9336\n",
            "Epoch 5/500\n",
            "30/30 [==============================] - 0s 776us/step - loss: 24463.5658 - mse: 625109824.0000 - mae: 24463.5664 - val_loss: 26852.1849 - val_mse: 732232128.0000 - val_mae: 26852.1855\n",
            "Epoch 6/500\n",
            "30/30 [==============================] - 0s 757us/step - loss: 24461.2103 - mse: 624992640.0000 - mae: 24461.2109 - val_loss: 26849.0605 - val_mse: 732051072.0000 - val_mae: 26849.0625\n",
            "Epoch 7/500\n",
            "30/30 [==============================] - 0s 796us/step - loss: 24456.9896 - mse: 624779712.0000 - mae: 24456.9902 - val_loss: 26845.0781 - val_mse: 731820224.0000 - val_mae: 26845.0801\n",
            "Epoch 8/500\n",
            "30/30 [==============================] - 0s 669us/step - loss: 24452.6797 - mse: 624571712.0000 - mae: 24452.6797 - val_loss: 26840.5143 - val_mse: 731555968.0000 - val_mae: 26840.5117\n",
            "Epoch 9/500\n",
            "30/30 [==============================] - 0s 802us/step - loss: 24448.3190 - mse: 624342784.0000 - mae: 24448.3184 - val_loss: 26835.4590 - val_mse: 731263232.0000 - val_mae: 26835.4609\n",
            "Epoch 10/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24439.9492 - mse: 624002432.0000 - mae: 24439.9492 - val_loss: 26829.0091 - val_mse: 730889792.0000 - val_mae: 26829.0078\n",
            "Epoch 11/500\n",
            "30/30 [==============================] - 0s 712us/step - loss: 24432.1862 - mse: 623572672.0000 - mae: 24432.1875 - val_loss: 26821.9447 - val_mse: 730481216.0000 - val_mae: 26821.9453\n",
            "Epoch 12/500\n",
            "30/30 [==============================] - 0s 718us/step - loss: 24430.5605 - mse: 623524800.0000 - mae: 24430.5605 - val_loss: 26815.3509 - val_mse: 730100096.0000 - val_mae: 26815.3496\n",
            "Epoch 13/500\n",
            "30/30 [==============================] - 0s 856us/step - loss: 24416.5885 - mse: 622848768.0000 - mae: 24416.5879 - val_loss: 26807.2480 - val_mse: 729632192.0000 - val_mae: 26807.2500\n",
            "Epoch 14/500\n",
            "30/30 [==============================] - 0s 770us/step - loss: 24417.0020 - mse: 622876736.0000 - mae: 24417.0020 - val_loss: 26799.7077 - val_mse: 729197248.0000 - val_mae: 26799.7090\n",
            "Epoch 15/500\n",
            "30/30 [==============================] - 0s 774us/step - loss: 24412.9863 - mse: 622557760.0000 - mae: 24412.9883 - val_loss: 26791.8509 - val_mse: 728744320.0000 - val_mae: 26791.8496\n",
            "Epoch 16/500\n",
            "30/30 [==============================] - 0s 830us/step - loss: 24397.4844 - mse: 621879168.0000 - mae: 24397.4824 - val_loss: 26782.2943 - val_mse: 728194176.0000 - val_mae: 26782.2930\n",
            "Epoch 17/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24385.7142 - mse: 621311168.0000 - mae: 24385.7129 - val_loss: 26771.7923 - val_mse: 727590144.0000 - val_mae: 26771.7910\n",
            "Epoch 18/500\n",
            "30/30 [==============================] - 0s 878us/step - loss: 24367.7786 - mse: 620428608.0000 - mae: 24367.7793 - val_loss: 26760.0332 - val_mse: 726914624.0000 - val_mae: 26760.0332\n",
            "Epoch 19/500\n",
            "30/30 [==============================] - 0s 824us/step - loss: 24360.3470 - mse: 620144512.0000 - mae: 24360.3457 - val_loss: 26748.0723 - val_mse: 726229248.0000 - val_mae: 26748.0723\n",
            "Epoch 20/500\n",
            "30/30 [==============================] - 0s 935us/step - loss: 24348.1445 - mse: 619756992.0000 - mae: 24348.1465 - val_loss: 26735.2396 - val_mse: 725495104.0000 - val_mae: 26735.2402\n",
            "Epoch 21/500\n",
            "30/30 [==============================] - 0s 864us/step - loss: 24332.3932 - mse: 618972224.0000 - mae: 24332.3926 - val_loss: 26721.7786 - val_mse: 724726336.0000 - val_mae: 26721.7773\n",
            "Epoch 22/500\n",
            "30/30 [==============================] - 0s 832us/step - loss: 24330.4596 - mse: 618726592.0000 - mae: 24330.4609 - val_loss: 26708.4466 - val_mse: 723967808.0000 - val_mae: 26708.4473\n",
            "Epoch 23/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24300.3874 - mse: 617102784.0000 - mae: 24300.3867 - val_loss: 26693.1999 - val_mse: 723102656.0000 - val_mae: 26693.1992\n",
            "Epoch 24/500\n",
            "30/30 [==============================] - 0s 943us/step - loss: 24300.3633 - mse: 617160064.0000 - mae: 24300.3633 - val_loss: 26678.2878 - val_mse: 722260224.0000 - val_mae: 26678.2871\n",
            "Epoch 25/500\n",
            "30/30 [==============================] - 0s 791us/step - loss: 24265.3210 - mse: 615319488.0000 - mae: 24265.3203 - val_loss: 26661.1589 - val_mse: 721290944.0000 - val_mae: 26661.1582\n",
            "Epoch 26/500\n",
            "30/30 [==============================] - 0s 762us/step - loss: 24243.7891 - mse: 614542336.0000 - mae: 24243.7891 - val_loss: 26642.0345 - val_mse: 720215104.0000 - val_mae: 26642.0332\n",
            "Epoch 27/500\n",
            "30/30 [==============================] - 0s 912us/step - loss: 24234.6706 - mse: 614170816.0000 - mae: 24234.6699 - val_loss: 26623.0059 - val_mse: 719148800.0000 - val_mae: 26623.0078\n",
            "Epoch 28/500\n",
            "30/30 [==============================] - 0s 766us/step - loss: 24200.3145 - mse: 612397824.0000 - mae: 24200.3145 - val_loss: 26601.8158 - val_mse: 717967360.0000 - val_mae: 26601.8164\n",
            "Epoch 29/500\n",
            "30/30 [==============================] - 0s 775us/step - loss: 24170.4329 - mse: 610988800.0000 - mae: 24170.4336 - val_loss: 26579.7650 - val_mse: 716737088.0000 - val_mae: 26579.7637\n",
            "Epoch 30/500\n",
            "30/30 [==============================] - 0s 854us/step - loss: 24155.2135 - mse: 610103872.0000 - mae: 24155.2129 - val_loss: 26556.7253 - val_mse: 715459136.0000 - val_mae: 26556.7246\n",
            "Epoch 31/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 24143.2650 - mse: 609977152.0000 - mae: 24143.2637 - val_loss: 26533.8477 - val_mse: 714192448.0000 - val_mae: 26533.8457\n",
            "Epoch 32/500\n",
            "30/30 [==============================] - 0s 930us/step - loss: 24104.5026 - mse: 607435776.0000 - mae: 24104.5020 - val_loss: 26509.1745 - val_mse: 712828096.0000 - val_mae: 26509.1758\n",
            "Epoch 33/500\n",
            "30/30 [==============================] - 0s 932us/step - loss: 24060.7865 - mse: 605634944.0000 - mae: 24060.7871 - val_loss: 26482.8581 - val_mse: 711374784.0000 - val_mae: 26482.8574\n",
            "Epoch 34/500\n",
            "30/30 [==============================] - 0s 869us/step - loss: 24064.7832 - mse: 606026368.0000 - mae: 24064.7832 - val_loss: 26458.0885 - val_mse: 710005760.0000 - val_mae: 26458.0879\n",
            "Epoch 35/500\n",
            "30/30 [==============================] - 0s 837us/step - loss: 24059.4831 - mse: 605533952.0000 - mae: 24059.4824 - val_loss: 26433.1712 - val_mse: 708638144.0000 - val_mae: 26433.1699\n",
            "Epoch 36/500\n",
            "30/30 [==============================] - 0s 862us/step - loss: 24000.6582 - mse: 602470528.0000 - mae: 24000.6582 - val_loss: 26405.4023 - val_mse: 707113664.0000 - val_mae: 26405.4004\n",
            "Epoch 37/500\n",
            "30/30 [==============================] - 0s 834us/step - loss: 24016.2565 - mse: 603092288.0000 - mae: 24016.2559 - val_loss: 26379.0632 - val_mse: 705673920.0000 - val_mae: 26379.0625\n",
            "Epoch 38/500\n",
            "30/30 [==============================] - 0s 937us/step - loss: 23915.2930 - mse: 598259776.0000 - mae: 23915.2930 - val_loss: 26349.2057 - val_mse: 704032384.0000 - val_mae: 26349.2051\n",
            "Epoch 39/500\n",
            "30/30 [==============================] - 0s 793us/step - loss: 23919.8385 - mse: 598844608.0000 - mae: 23919.8379 - val_loss: 26319.8958 - val_mse: 702433152.0000 - val_mae: 26319.8965\n",
            "Epoch 40/500\n",
            "30/30 [==============================] - 0s 957us/step - loss: 23889.0944 - mse: 597035264.0000 - mae: 23889.0957 - val_loss: 26289.3242 - val_mse: 700765312.0000 - val_mae: 26289.3242\n",
            "Epoch 41/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23801.5820 - mse: 593076032.0000 - mae: 23801.5840 - val_loss: 26256.1953 - val_mse: 698958464.0000 - val_mae: 26256.1934\n",
            "Epoch 42/500\n",
            "30/30 [==============================] - 0s 760us/step - loss: 23785.3548 - mse: 592346048.0000 - mae: 23785.3535 - val_loss: 26222.7617 - val_mse: 697144128.0000 - val_mae: 26222.7617\n",
            "Epoch 43/500\n",
            "30/30 [==============================] - 0s 838us/step - loss: 23825.9375 - mse: 594175872.0000 - mae: 23825.9375 - val_loss: 26191.8242 - val_mse: 695469248.0000 - val_mae: 26191.8223\n",
            "Epoch 44/500\n",
            "30/30 [==============================] - 0s 917us/step - loss: 23740.7546 - mse: 590870144.0000 - mae: 23740.7539 - val_loss: 26157.9147 - val_mse: 693633600.0000 - val_mae: 26157.9141\n",
            "Epoch 45/500\n",
            "30/30 [==============================] - 0s 897us/step - loss: 23698.8105 - mse: 588078016.0000 - mae: 23698.8105 - val_loss: 26122.6035 - val_mse: 691728256.0000 - val_mae: 26122.6035\n",
            "Epoch 46/500\n",
            "30/30 [==============================] - 0s 833us/step - loss: 23656.1576 - mse: 586226240.0000 - mae: 23656.1582 - val_loss: 26085.6042 - val_mse: 689740992.0000 - val_mae: 26085.6035\n",
            "Epoch 47/500\n",
            "30/30 [==============================] - 0s 942us/step - loss: 23633.8112 - mse: 585082240.0000 - mae: 23633.8125 - val_loss: 26049.0729 - val_mse: 687781120.0000 - val_mae: 26049.0723\n",
            "Epoch 48/500\n",
            "30/30 [==============================] - 0s 965us/step - loss: 23637.1439 - mse: 586044032.0000 - mae: 23637.1445 - val_loss: 26013.0299 - val_mse: 685852224.0000 - val_mae: 26013.0293\n",
            "Epoch 49/500\n",
            "30/30 [==============================] - 0s 931us/step - loss: 23520.4362 - mse: 578479040.0000 - mae: 23520.4375 - val_loss: 25972.4876 - val_mse: 683692288.0000 - val_mae: 25972.4902\n",
            "Epoch 50/500\n",
            "30/30 [==============================] - 0s 932us/step - loss: 23430.7819 - mse: 574767936.0000 - mae: 23430.7812 - val_loss: 25928.5130 - val_mse: 681353344.0000 - val_mae: 25928.5117\n",
            "Epoch 51/500\n",
            "30/30 [==============================] - 0s 772us/step - loss: 23479.7812 - mse: 577919936.0000 - mae: 23479.7793 - val_loss: 25885.8229 - val_mse: 679098496.0000 - val_mae: 25885.8223\n",
            "Epoch 52/500\n",
            "30/30 [==============================] - 0s 795us/step - loss: 23389.3555 - mse: 573715904.0000 - mae: 23389.3535 - val_loss: 25838.6510 - val_mse: 676622912.0000 - val_mae: 25838.6523\n",
            "Epoch 53/500\n",
            "30/30 [==============================] - 0s 789us/step - loss: 23304.2233 - mse: 569107712.0000 - mae: 23304.2227 - val_loss: 25789.8945 - val_mse: 674058624.0000 - val_mae: 25789.8965\n",
            "Epoch 54/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23288.4186 - mse: 569295744.0000 - mae: 23288.4180 - val_loss: 25738.8079 - val_mse: 671404416.0000 - val_mae: 25738.8066\n",
            "Epoch 55/500\n",
            "30/30 [==============================] - 0s 999us/step - loss: 23268.0605 - mse: 568203648.0000 - mae: 23268.0605 - val_loss: 25688.6432 - val_mse: 668790400.0000 - val_mae: 25688.6445\n",
            "Epoch 56/500\n",
            "30/30 [==============================] - 0s 845us/step - loss: 23243.2995 - mse: 567865920.0000 - mae: 23243.3008 - val_loss: 25635.3073 - val_mse: 666048896.0000 - val_mae: 25635.3086\n",
            "Epoch 57/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 23209.4030 - mse: 565608384.0000 - mae: 23209.4023 - val_loss: 25580.4154 - val_mse: 663239488.0000 - val_mae: 25580.4160\n",
            "Epoch 58/500\n",
            "30/30 [==============================] - 0s 854us/step - loss: 23039.1868 - mse: 557219456.0000 - mae: 23039.1875 - val_loss: 25519.2415 - val_mse: 660117824.0000 - val_mae: 25519.2422\n",
            "Epoch 59/500\n",
            "30/30 [==============================] - 0s 786us/step - loss: 23107.0970 - mse: 559330624.0000 - mae: 23107.0977 - val_loss: 25460.1536 - val_mse: 657122112.0000 - val_mae: 25460.1543\n",
            "Epoch 60/500\n",
            "30/30 [==============================] - 0s 787us/step - loss: 23013.3776 - mse: 556706624.0000 - mae: 23013.3770 - val_loss: 25399.9447 - val_mse: 654068736.0000 - val_mae: 25399.9453\n",
            "Epoch 61/500\n",
            "30/30 [==============================] - 0s 965us/step - loss: 22937.2715 - mse: 553584320.0000 - mae: 22937.2715 - val_loss: 25335.9740 - val_mse: 650842752.0000 - val_mae: 25335.9746\n",
            "Epoch 62/500\n",
            "30/30 [==============================] - 0s 806us/step - loss: 22832.0059 - mse: 547700800.0000 - mae: 22832.0059 - val_loss: 25267.7982 - val_mse: 647420736.0000 - val_mae: 25267.8008\n",
            "Epoch 63/500\n",
            "30/30 [==============================] - 0s 844us/step - loss: 22880.3750 - mse: 550363072.0000 - mae: 22880.3750 - val_loss: 25203.2559 - val_mse: 644181440.0000 - val_mae: 25203.2559\n",
            "Epoch 64/500\n",
            "30/30 [==============================] - 0s 893us/step - loss: 22672.0879 - mse: 540630464.0000 - mae: 22672.0879 - val_loss: 25127.5020 - val_mse: 640402048.0000 - val_mae: 25127.5020\n",
            "Epoch 65/500\n",
            "30/30 [==============================] - 0s 727us/step - loss: 22671.9447 - mse: 541660736.0000 - mae: 22671.9453 - val_loss: 25050.6270 - val_mse: 636588608.0000 - val_mae: 25050.6289\n",
            "Epoch 66/500\n",
            "30/30 [==============================] - 0s 839us/step - loss: 22550.9023 - mse: 533740704.0000 - mae: 22550.9023 - val_loss: 24974.6530 - val_mse: 632809856.0000 - val_mae: 24974.6543\n",
            "Epoch 67/500\n",
            "30/30 [==============================] - 0s 970us/step - loss: 22655.0026 - mse: 539423360.0000 - mae: 22655.0020 - val_loss: 24904.0312 - val_mse: 629307392.0000 - val_mae: 24904.0312\n",
            "Epoch 68/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22470.1530 - mse: 533110496.0000 - mae: 22470.1543 - val_loss: 24825.9421 - val_mse: 625464064.0000 - val_mae: 24825.9414\n",
            "Epoch 69/500\n",
            "30/30 [==============================] - 0s 874us/step - loss: 22397.0326 - mse: 529135136.0000 - mae: 22397.0312 - val_loss: 24747.5247 - val_mse: 621601600.0000 - val_mae: 24747.5254\n",
            "Epoch 70/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22286.6146 - mse: 523283648.0000 - mae: 22286.6152 - val_loss: 24660.1283 - val_mse: 617350080.0000 - val_mae: 24660.1270\n",
            "Epoch 71/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 22130.2949 - mse: 516551360.0000 - mae: 22130.2949 - val_loss: 24573.1895 - val_mse: 613110080.0000 - val_mae: 24573.1875\n",
            "Epoch 72/500\n",
            "30/30 [==============================] - 0s 876us/step - loss: 22035.0137 - mse: 512497344.0000 - mae: 22035.0137 - val_loss: 24485.1348 - val_mse: 608829440.0000 - val_mae: 24485.1348\n",
            "Epoch 73/500\n",
            "30/30 [==============================] - 0s 948us/step - loss: 21958.1302 - mse: 508781248.0000 - mae: 21958.1309 - val_loss: 24398.2734 - val_mse: 604612160.0000 - val_mae: 24398.2734\n",
            "Epoch 74/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21822.0957 - mse: 502870592.0000 - mae: 21822.0957 - val_loss: 24310.2572 - val_mse: 600339648.0000 - val_mae: 24310.2578\n",
            "Epoch 75/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21838.9141 - mse: 506539904.0000 - mae: 21838.9121 - val_loss: 24225.7383 - val_mse: 596248448.0000 - val_mae: 24225.7383\n",
            "Epoch 76/500\n",
            "30/30 [==============================] - 0s 997us/step - loss: 21711.8704 - mse: 498323648.0000 - mae: 21711.8711 - val_loss: 24138.2630 - val_mse: 592025536.0000 - val_mae: 24138.2617\n",
            "Epoch 77/500\n",
            "30/30 [==============================] - 0s 898us/step - loss: 21712.8438 - mse: 499861856.0000 - mae: 21712.8438 - val_loss: 24051.4668 - val_mse: 587857856.0000 - val_mae: 24051.4668\n",
            "Epoch 78/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21643.7383 - mse: 496924480.0000 - mae: 21643.7383 - val_loss: 23964.1172 - val_mse: 583675264.0000 - val_mae: 23964.1172\n",
            "Epoch 79/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21441.5306 - mse: 487281280.0000 - mae: 21441.5312 - val_loss: 23873.1660 - val_mse: 579335488.0000 - val_mae: 23873.1660\n",
            "Epoch 80/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21391.0527 - mse: 483061856.0000 - mae: 21391.0527 - val_loss: 23782.6289 - val_mse: 575026496.0000 - val_mae: 23782.6289\n",
            "Epoch 81/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21329.4740 - mse: 481897312.0000 - mae: 21329.4727 - val_loss: 23692.1087 - val_mse: 570734656.0000 - val_mae: 23692.1074\n",
            "Epoch 82/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 21311.4277 - mse: 484134880.0000 - mae: 21311.4297 - val_loss: 23602.1172 - val_mse: 566490752.0000 - val_mae: 23602.1172\n",
            "Epoch 83/500\n",
            "30/30 [==============================] - 0s 982us/step - loss: 21125.9883 - mse: 473967648.0000 - mae: 21125.9883 - val_loss: 23508.3750 - val_mse: 562081344.0000 - val_mae: 23508.3750\n",
            "Epoch 84/500\n",
            "30/30 [==============================] - 0s 866us/step - loss: 20886.5924 - mse: 462715680.0000 - mae: 20886.5918 - val_loss: 23410.0898 - val_mse: 557477248.0000 - val_mae: 23410.0918\n",
            "Epoch 85/500\n",
            "30/30 [==============================] - 0s 874us/step - loss: 20756.7181 - mse: 460544896.0000 - mae: 20756.7188 - val_loss: 23310.3776 - val_mse: 552818752.0000 - val_mae: 23310.3770\n",
            "Epoch 86/500\n",
            "30/30 [==============================] - 0s 895us/step - loss: 20799.8874 - mse: 460161184.0000 - mae: 20799.8867 - val_loss: 23213.0326 - val_mse: 548294720.0000 - val_mae: 23213.0332\n",
            "Epoch 87/500\n",
            "30/30 [==============================] - 0s 855us/step - loss: 20117.2103 - mse: 432829664.0000 - mae: 20117.2109 - val_loss: 23100.8418 - val_mse: 543105280.0000 - val_mae: 23100.8418\n",
            "Epoch 88/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 20370.8021 - mse: 443489760.0000 - mae: 20370.8027 - val_loss: 22997.9447 - val_mse: 538359104.0000 - val_mae: 22997.9453\n",
            "Epoch 89/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 20442.2617 - mse: 444696352.0000 - mae: 20442.2617 - val_loss: 22896.3359 - val_mse: 533702720.0000 - val_mae: 22896.3359\n",
            "Epoch 90/500\n",
            "30/30 [==============================] - 0s 905us/step - loss: 20354.0280 - mse: 443070592.0000 - mae: 20354.0273 - val_loss: 22793.9948 - val_mse: 529032480.0000 - val_mae: 22793.9941\n",
            "Epoch 91/500\n",
            "30/30 [==============================] - 0s 898us/step - loss: 20292.2793 - mse: 440941216.0000 - mae: 20292.2793 - val_loss: 22693.0280 - val_mse: 524435072.0000 - val_mae: 22693.0293\n",
            "Epoch 92/500\n",
            "30/30 [==============================] - 0s 919us/step - loss: 19980.8249 - mse: 427275648.0000 - mae: 19980.8242 - val_loss: 22586.9225 - val_mse: 519629216.0000 - val_mae: 22586.9238\n",
            "Epoch 93/500\n",
            "30/30 [==============================] - 0s 869us/step - loss: 19838.2943 - mse: 421121568.0000 - mae: 19838.2949 - val_loss: 22479.7917 - val_mse: 514795936.0000 - val_mae: 22479.7910\n",
            "Epoch 94/500\n",
            "30/30 [==============================] - 0s 810us/step - loss: 19591.4010 - mse: 409595808.0000 - mae: 19591.4004 - val_loss: 22366.7923 - val_mse: 509730144.0000 - val_mae: 22366.7910\n",
            "Epoch 95/500\n",
            "30/30 [==============================] - 0s 817us/step - loss: 19860.8919 - mse: 425629408.0000 - mae: 19860.8926 - val_loss: 22262.3177 - val_mse: 505067040.0000 - val_mae: 22262.3164\n",
            "Epoch 96/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 19962.3587 - mse: 427019968.0000 - mae: 19962.3574 - val_loss: 22161.6868 - val_mse: 500590560.0000 - val_mae: 22161.6875\n",
            "Epoch 97/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 19725.1276 - mse: 417860896.0000 - mae: 19725.1270 - val_loss: 22056.1400 - val_mse: 495925664.0000 - val_mae: 22056.1387\n",
            "Epoch 98/500\n",
            "30/30 [==============================] - 0s 799us/step - loss: 19253.2520 - mse: 400461600.0000 - mae: 19253.2520 - val_loss: 21942.2910 - val_mse: 490918624.0000 - val_mae: 21942.2891\n",
            "Epoch 99/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 19170.8021 - mse: 393575776.0000 - mae: 19170.8027 - val_loss: 21828.3926 - val_mse: 485936544.0000 - val_mae: 21828.3926\n",
            "Epoch 100/500\n",
            "30/30 [==============================] - 0s 850us/step - loss: 19192.7279 - mse: 394715680.0000 - mae: 19192.7285 - val_loss: 21717.0469 - val_mse: 481084736.0000 - val_mae: 21717.0488\n",
            "Epoch 101/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 18571.4710 - mse: 374131104.0000 - mae: 18571.4707 - val_loss: 21594.1283 - val_mse: 475759232.0000 - val_mae: 21594.1289\n",
            "Epoch 102/500\n",
            "30/30 [==============================] - 0s 916us/step - loss: 19138.8229 - mse: 397524864.0000 - mae: 19138.8223 - val_loss: 21483.1862 - val_mse: 470978016.0000 - val_mae: 21483.1875\n",
            "Epoch 103/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 18543.3372 - mse: 370847968.0000 - mae: 18543.3379 - val_loss: 21363.2331 - val_mse: 465839200.0000 - val_mae: 21363.2324\n",
            "Epoch 104/500\n",
            "30/30 [==============================] - 0s 835us/step - loss: 18594.7839 - mse: 373245216.0000 - mae: 18594.7832 - val_loss: 21246.3568 - val_mse: 460857120.0000 - val_mae: 21246.3555\n",
            "Epoch 105/500\n",
            "30/30 [==============================] - 0s 887us/step - loss: 18079.5938 - mse: 358825440.0000 - mae: 18079.5938 - val_loss: 21120.3685 - val_mse: 455518944.0000 - val_mae: 21120.3691\n",
            "Epoch 106/500\n",
            "30/30 [==============================] - 0s 832us/step - loss: 18445.4385 - mse: 369474560.0000 - mae: 18445.4375 - val_loss: 21003.4238 - val_mse: 450595328.0000 - val_mae: 21003.4258\n",
            "Epoch 107/500\n",
            "30/30 [==============================] - 0s 836us/step - loss: 18232.7474 - mse: 364548992.0000 - mae: 18232.7480 - val_loss: 20883.3301 - val_mse: 445567616.0000 - val_mae: 20883.3301\n",
            "Epoch 108/500\n",
            "30/30 [==============================] - 0s 906us/step - loss: 18502.6667 - mse: 376508224.0000 - mae: 18502.6660 - val_loss: 20770.7135 - val_mse: 440878144.0000 - val_mae: 20770.7129\n",
            "Epoch 109/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 18207.6191 - mse: 359759424.0000 - mae: 18207.6191 - val_loss: 20652.4492 - val_mse: 435985792.0000 - val_mae: 20652.4473\n",
            "Epoch 110/500\n",
            "30/30 [==============================] - 0s 936us/step - loss: 17929.7884 - mse: 352530656.0000 - mae: 17929.7871 - val_loss: 20530.8516 - val_mse: 430985248.0000 - val_mae: 20530.8516\n",
            "Epoch 111/500\n",
            "30/30 [==============================] - 0s 832us/step - loss: 18139.3496 - mse: 359241856.0000 - mae: 18139.3496 - val_loss: 20414.2832 - val_mse: 426222720.0000 - val_mae: 20414.2832\n",
            "Epoch 112/500\n",
            "30/30 [==============================] - 0s 799us/step - loss: 17437.3424 - mse: 328867008.0000 - mae: 17437.3418 - val_loss: 20286.3255 - val_mse: 421021152.0000 - val_mae: 20286.3242\n",
            "Epoch 113/500\n",
            "30/30 [==============================] - 0s 935us/step - loss: 17693.8171 - mse: 347271168.0000 - mae: 17693.8164 - val_loss: 20162.9124 - val_mse: 416043744.0000 - val_mae: 20162.9121\n",
            "Epoch 114/500\n",
            "30/30 [==============================] - 0s 891us/step - loss: 17353.6484 - mse: 322508800.0000 - mae: 17353.6484 - val_loss: 20034.8652 - val_mse: 410908128.0000 - val_mae: 20034.8672\n",
            "Epoch 115/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 17255.6950 - mse: 323029824.0000 - mae: 17255.6953 - val_loss: 19906.8005 - val_mse: 405805248.0000 - val_mae: 19906.8008\n",
            "Epoch 116/500\n",
            "30/30 [==============================] - 0s 953us/step - loss: 17178.5775 - mse: 319037632.0000 - mae: 17178.5781 - val_loss: 19779.3079 - val_mse: 400754880.0000 - val_mae: 19779.3086\n",
            "Epoch 117/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 17314.1139 - mse: 333394048.0000 - mae: 17314.1133 - val_loss: 19655.6029 - val_mse: 395888704.0000 - val_mae: 19655.6016\n",
            "Epoch 118/500\n",
            "30/30 [==============================] - 0s 948us/step - loss: 16612.0124 - mse: 310833280.0000 - mae: 16612.0117 - val_loss: 19521.5762 - val_mse: 390649856.0000 - val_mae: 19521.5742\n",
            "Epoch 119/500\n",
            "30/30 [==============================] - 0s 992us/step - loss: 16889.6201 - mse: 312018848.0000 - mae: 16889.6211 - val_loss: 19393.3369 - val_mse: 385671392.0000 - val_mae: 19393.3359\n",
            "Epoch 120/500\n",
            "30/30 [==============================] - 0s 925us/step - loss: 16841.2601 - mse: 321594080.0000 - mae: 16841.2598 - val_loss: 19264.3415 - val_mse: 380700608.0000 - val_mae: 19264.3418\n",
            "Epoch 121/500\n",
            "30/30 [==============================] - 0s 840us/step - loss: 17373.3451 - mse: 333681472.0000 - mae: 17373.3457 - val_loss: 19145.6471 - val_mse: 376156864.0000 - val_mae: 19145.6465\n",
            "Epoch 122/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 16250.0387 - mse: 299721056.0000 - mae: 16250.0381 - val_loss: 19010.5459 - val_mse: 371019744.0000 - val_mae: 19010.5449\n",
            "Epoch 123/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 16485.6742 - mse: 295069568.0000 - mae: 16485.6758 - val_loss: 18880.6917 - val_mse: 366115296.0000 - val_mae: 18880.6914\n",
            "Epoch 124/500\n",
            "30/30 [==============================] - 0s 965us/step - loss: 15812.6527 - mse: 278686816.0000 - mae: 15812.6523 - val_loss: 18740.9255 - val_mse: 360876640.0000 - val_mae: 18740.9258\n",
            "Epoch 125/500\n",
            "30/30 [==============================] - 0s 939us/step - loss: 16225.5381 - mse: 293381472.0000 - mae: 16225.5371 - val_loss: 18608.4551 - val_mse: 355949376.0000 - val_mae: 18608.4551\n",
            "Epoch 126/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 16456.0029 - mse: 304045888.0000 - mae: 16456.0039 - val_loss: 18480.7874 - val_mse: 351233568.0000 - val_mae: 18480.7871\n",
            "Epoch 127/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 15467.0550 - mse: 269012416.0000 - mae: 15467.0537 - val_loss: 18338.5094 - val_mse: 346016480.0000 - val_mae: 18338.5078\n",
            "Epoch 128/500\n",
            "30/30 [==============================] - 0s 953us/step - loss: 15241.9759 - mse: 263254528.0000 - mae: 15241.9756 - val_loss: 18194.6868 - val_mse: 340789728.0000 - val_mae: 18194.6875\n",
            "Epoch 129/500\n",
            "30/30 [==============================] - 0s 897us/step - loss: 15353.5186 - mse: 263378336.0000 - mae: 15353.5186 - val_loss: 18064.3473 - val_mse: 336086752.0000 - val_mae: 18064.3477\n",
            "Epoch 130/500\n",
            "30/30 [==============================] - 0s 889us/step - loss: 15407.6644 - mse: 268474688.0000 - mae: 15407.6650 - val_loss: 17924.0378 - val_mse: 331065152.0000 - val_mae: 17924.0371\n",
            "Epoch 131/500\n",
            "30/30 [==============================] - 0s 856us/step - loss: 14902.9590 - mse: 247536656.0000 - mae: 14902.9580 - val_loss: 17778.5212 - val_mse: 325897248.0000 - val_mae: 17778.5215\n",
            "Epoch 132/500\n",
            "30/30 [==============================] - 0s 909us/step - loss: 15180.3913 - mse: 264013920.0000 - mae: 15180.3916 - val_loss: 17638.4017 - val_mse: 320964288.0000 - val_mae: 17638.4004\n",
            "Epoch 133/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 15380.4339 - mse: 273199520.0000 - mae: 15380.4336 - val_loss: 17513.7640 - val_mse: 316609120.0000 - val_mae: 17513.7617\n",
            "Epoch 134/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 14359.0537 - mse: 230158384.0000 - mae: 14359.0537 - val_loss: 17374.9759 - val_mse: 311798432.0000 - val_mae: 17374.9766\n",
            "Epoch 135/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 14474.6396 - mse: 243802832.0000 - mae: 14474.6396 - val_loss: 17239.4795 - val_mse: 307140128.0000 - val_mae: 17239.4785\n",
            "Epoch 136/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 14983.4616 - mse: 257211328.0000 - mae: 14983.4600 - val_loss: 17100.3392 - val_mse: 302394880.0000 - val_mae: 17100.3379\n",
            "Epoch 137/500\n",
            "30/30 [==============================] - 0s 882us/step - loss: 14048.5062 - mse: 229811888.0000 - mae: 14048.5059 - val_loss: 16975.6888 - val_mse: 298178912.0000 - val_mae: 16975.6895\n",
            "Epoch 138/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 13470.1781 - mse: 201890032.0000 - mae: 13470.1777 - val_loss: 16815.8952 - val_mse: 292817536.0000 - val_mae: 16815.8945\n",
            "Epoch 139/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 13815.4583 - mse: 225843888.0000 - mae: 13815.4580 - val_loss: 16675.4596 - val_mse: 288149152.0000 - val_mae: 16675.4609\n",
            "Epoch 140/500\n",
            "30/30 [==============================] - 0s 913us/step - loss: 14003.1253 - mse: 226766192.0000 - mae: 14003.1250 - val_loss: 16536.3122 - val_mse: 283562912.0000 - val_mae: 16536.3125\n",
            "Epoch 141/500\n",
            "30/30 [==============================] - 0s 908us/step - loss: 14045.2806 - mse: 224444176.0000 - mae: 14045.2803 - val_loss: 16400.8483 - val_mse: 279137728.0000 - val_mae: 16400.8496\n",
            "Epoch 142/500\n",
            "30/30 [==============================] - 0s 860us/step - loss: 13314.8926 - mse: 206930624.0000 - mae: 13314.8926 - val_loss: 16254.8477 - val_mse: 274409504.0000 - val_mae: 16254.8467\n",
            "Epoch 143/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 13638.8854 - mse: 218456176.0000 - mae: 13638.8857 - val_loss: 16126.0560 - val_mse: 270275872.0000 - val_mae: 16126.0566\n",
            "Epoch 144/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 13214.1774 - mse: 205796624.0000 - mae: 13214.1768 - val_loss: 15970.0417 - val_mse: 265313824.0000 - val_mae: 15970.0410\n",
            "Epoch 145/500\n",
            "30/30 [==============================] - 0s 980us/step - loss: 12670.5804 - mse: 183113136.0000 - mae: 12670.5801 - val_loss: 15819.8053 - val_mse: 260582368.0000 - val_mae: 15819.8037\n",
            "Epoch 146/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 13009.1032 - mse: 197106608.0000 - mae: 13009.1035 - val_loss: 15672.6787 - val_mse: 255995136.0000 - val_mae: 15672.6787\n",
            "Epoch 147/500\n",
            "30/30 [==============================] - 0s 959us/step - loss: 12866.7907 - mse: 194955408.0000 - mae: 12866.7910 - val_loss: 15538.0798 - val_mse: 251836624.0000 - val_mae: 15538.0791\n",
            "Epoch 148/500\n",
            "30/30 [==============================] - 0s 911us/step - loss: 12206.5365 - mse: 175594944.0000 - mae: 12206.5361 - val_loss: 15372.0173 - val_mse: 246758160.0000 - val_mae: 15372.0166\n",
            "Epoch 149/500\n",
            "30/30 [==============================] - 0s 891us/step - loss: 12538.5186 - mse: 188321552.0000 - mae: 12538.5186 - val_loss: 15212.9336 - val_mse: 241944160.0000 - val_mae: 15212.9336\n",
            "Epoch 150/500\n",
            "30/30 [==============================] - 0s 920us/step - loss: 12336.4229 - mse: 183359520.0000 - mae: 12336.4229 - val_loss: 15072.7839 - val_mse: 237747648.0000 - val_mae: 15072.7832\n",
            "Epoch 151/500\n",
            "30/30 [==============================] - 0s 911us/step - loss: 12465.3870 - mse: 187115312.0000 - mae: 12465.3877 - val_loss: 14922.8861 - val_mse: 233304512.0000 - val_mae: 14922.8857\n",
            "Epoch 152/500\n",
            "30/30 [==============================] - 0s 985us/step - loss: 12360.9329 - mse: 184347632.0000 - mae: 12360.9336 - val_loss: 14773.8470 - val_mse: 228934480.0000 - val_mae: 14773.8467\n",
            "Epoch 153/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11724.4154 - mse: 169618224.0000 - mae: 11724.4150 - val_loss: 14616.9919 - val_mse: 224382688.0000 - val_mae: 14616.9912\n",
            "Epoch 154/500\n",
            "30/30 [==============================] - 0s 871us/step - loss: 12557.5365 - mse: 187378944.0000 - mae: 12557.5361 - val_loss: 14471.1084 - val_mse: 220196416.0000 - val_mae: 14471.1084\n",
            "Epoch 155/500\n",
            "30/30 [==============================] - 0s 884us/step - loss: 11815.0052 - mse: 174286880.0000 - mae: 11815.0059 - val_loss: 14332.3428 - val_mse: 216254496.0000 - val_mae: 14332.3428\n",
            "Epoch 156/500\n",
            "30/30 [==============================] - 0s 789us/step - loss: 11401.6686 - mse: 156741024.0000 - mae: 11401.6689 - val_loss: 14173.2295 - val_mse: 211782624.0000 - val_mae: 14173.2295\n",
            "Epoch 157/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11632.1657 - mse: 161657280.0000 - mae: 11632.1660 - val_loss: 14018.7699 - val_mse: 207490768.0000 - val_mae: 14018.7686\n",
            "Epoch 158/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11213.0876 - mse: 160868144.0000 - mae: 11213.0879 - val_loss: 13871.9609 - val_mse: 203459584.0000 - val_mae: 13871.9600\n",
            "Epoch 159/500\n",
            "30/30 [==============================] - 0s 937us/step - loss: 11381.8018 - mse: 163767664.0000 - mae: 11381.8018 - val_loss: 13705.7666 - val_mse: 198946960.0000 - val_mae: 13705.7666\n",
            "Epoch 160/500\n",
            "30/30 [==============================] - 0s 953us/step - loss: 10855.9889 - mse: 148388576.0000 - mae: 10855.9893 - val_loss: 13571.5107 - val_mse: 195344240.0000 - val_mae: 13571.5107\n",
            "Epoch 161/500\n",
            "30/30 [==============================] - 0s 914us/step - loss: 11106.9017 - mse: 148930016.0000 - mae: 11106.9023 - val_loss: 13435.7093 - val_mse: 191737536.0000 - val_mae: 13435.7090\n",
            "Epoch 162/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11727.5251 - mse: 156864336.0000 - mae: 11727.5254 - val_loss: 13307.5195 - val_mse: 188367968.0000 - val_mae: 13307.5186\n",
            "Epoch 163/500\n",
            "30/30 [==============================] - 0s 907us/step - loss: 10352.3776 - mse: 138660224.0000 - mae: 10352.3770 - val_loss: 13174.5752 - val_mse: 184904496.0000 - val_mae: 13174.5752\n",
            "Epoch 164/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11405.1458 - mse: 156713008.0000 - mae: 11405.1455 - val_loss: 13056.7275 - val_mse: 181870352.0000 - val_mae: 13056.7275\n",
            "Epoch 165/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10195.9941 - mse: 130078976.0000 - mae: 10195.9941 - val_loss: 12900.8324 - val_mse: 177898528.0000 - val_mae: 12900.8311\n",
            "Epoch 166/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10143.5817 - mse: 123533640.0000 - mae: 10143.5811 - val_loss: 12755.8079 - val_mse: 174249040.0000 - val_mae: 12755.8076\n",
            "Epoch 167/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10831.9948 - mse: 144093904.0000 - mae: 10831.9951 - val_loss: 12610.8271 - val_mse: 170642032.0000 - val_mae: 12610.8271\n",
            "Epoch 168/500\n",
            "30/30 [==============================] - 0s 926us/step - loss: 10082.0514 - mse: 132344952.0000 - mae: 10082.0508 - val_loss: 12496.5924 - val_mse: 167832064.0000 - val_mae: 12496.5928\n",
            "Epoch 169/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 10735.0947 - mse: 137871856.0000 - mae: 10735.0947 - val_loss: 12374.0824 - val_mse: 164846928.0000 - val_mae: 12374.0820\n",
            "Epoch 170/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9965.7904 - mse: 126810848.0000 - mae: 9965.7900 - val_loss: 12208.7799 - val_mse: 160867984.0000 - val_mae: 12208.7793\n",
            "Epoch 171/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9205.5117 - mse: 106283824.0000 - mae: 9205.5117 - val_loss: 12069.8581 - val_mse: 157568976.0000 - val_mae: 12069.8584\n",
            "Epoch 172/500\n",
            "30/30 [==============================] - 0s 910us/step - loss: 9102.5010 - mse: 102025584.0000 - mae: 9102.5010 - val_loss: 11930.1491 - val_mse: 154289136.0000 - val_mae: 11930.1494\n",
            "Epoch 173/500\n",
            "30/30 [==============================] - 0s 792us/step - loss: 9804.4678 - mse: 130570656.0000 - mae: 9804.4688 - val_loss: 11828.1074 - val_mse: 151921904.0000 - val_mae: 11828.1084\n",
            "Epoch 174/500\n",
            "30/30 [==============================] - 0s 843us/step - loss: 9976.1922 - mse: 126946064.0000 - mae: 9976.1924 - val_loss: 11676.4541 - val_mse: 148439088.0000 - val_mae: 11676.4541\n",
            "Epoch 175/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9304.1460 - mse: 114600480.0000 - mae: 9304.1455 - val_loss: 11515.0939 - val_mse: 144785728.0000 - val_mae: 11515.0938\n",
            "Epoch 176/500\n",
            "30/30 [==============================] - 0s 972us/step - loss: 9010.0386 - mse: 107067544.0000 - mae: 9010.0381 - val_loss: 11344.5749 - val_mse: 140982480.0000 - val_mae: 11344.5752\n",
            "Epoch 177/500\n",
            "30/30 [==============================] - 0s 933us/step - loss: 8452.5692 - mse: 91422800.0000 - mae: 8452.5684 - val_loss: 11174.4417 - val_mse: 137245632.0000 - val_mae: 11174.4414\n",
            "Epoch 178/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9519.4941 - mse: 122287840.0000 - mae: 9519.4941 - val_loss: 11034.8086 - val_mse: 134224000.0000 - val_mae: 11034.8086\n",
            "Epoch 179/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9625.2367 - mse: 129164808.0000 - mae: 9625.2363 - val_loss: 10925.4266 - val_mse: 131884848.0000 - val_mae: 10925.4268\n",
            "Epoch 180/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8807.3685 - mse: 99735648.0000 - mae: 8807.3691 - val_loss: 10798.6250 - val_mse: 129205864.0000 - val_mae: 10798.6250\n",
            "Epoch 181/500\n",
            "30/30 [==============================] - 0s 993us/step - loss: 8823.7765 - mse: 106738952.0000 - mae: 8823.7773 - val_loss: 10645.1393 - val_mse: 126005040.0000 - val_mae: 10645.1396\n",
            "Epoch 182/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9062.4766 - mse: 107738344.0000 - mae: 9062.4775 - val_loss: 10512.0501 - val_mse: 123269936.0000 - val_mae: 10512.0498\n",
            "Epoch 183/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9058.9395 - mse: 112908440.0000 - mae: 9058.9395 - val_loss: 10377.8267 - val_mse: 120548520.0000 - val_mae: 10377.8271\n",
            "Epoch 184/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 9172.3765 - mse: 108042448.0000 - mae: 9172.3760 - val_loss: 10243.2489 - val_mse: 117856872.0000 - val_mae: 10243.2490\n",
            "Epoch 185/500\n",
            "30/30 [==============================] - 0s 793us/step - loss: 8711.6540 - mse: 97778160.0000 - mae: 8711.6543 - val_loss: 10092.9621 - val_mse: 114894560.0000 - val_mae: 10092.9629\n",
            "Epoch 186/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8838.6204 - mse: 103369256.0000 - mae: 8838.6211 - val_loss: 9956.7964 - val_mse: 112250592.0000 - val_mae: 9956.7959\n",
            "Epoch 187/500\n",
            "30/30 [==============================] - 0s 841us/step - loss: 8558.7746 - mse: 107388776.0000 - mae: 8558.7754 - val_loss: 9850.9323 - val_mse: 110219544.0000 - val_mae: 9850.9326\n",
            "Epoch 188/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7322.4743 - mse: 75038704.0000 - mae: 7322.4741 - val_loss: 9750.5223 - val_mse: 108314272.0000 - val_mae: 9750.5225\n",
            "Epoch 189/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8045.7448 - mse: 87650136.0000 - mae: 8045.7446 - val_loss: 9650.8843 - val_mse: 106442904.0000 - val_mae: 9650.8848\n",
            "Epoch 190/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7084.0501 - mse: 77579160.0000 - mae: 7084.0498 - val_loss: 9539.5830 - val_mse: 104374192.0000 - val_mae: 9539.5830\n",
            "Epoch 191/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7980.5666 - mse: 89358584.0000 - mae: 7980.5669 - val_loss: 9443.1357 - val_mse: 102606712.0000 - val_mae: 9443.1357\n",
            "Epoch 192/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7731.4650 - mse: 79000072.0000 - mae: 7731.4653 - val_loss: 9344.3177 - val_mse: 100815976.0000 - val_mae: 9344.3174\n",
            "Epoch 193/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7891.8050 - mse: 76470440.0000 - mae: 7891.8052 - val_loss: 9247.9144 - val_mse: 99079000.0000 - val_mae: 9247.9150\n",
            "Epoch 194/500\n",
            "30/30 [==============================] - 0s 958us/step - loss: 6733.6291 - mse: 75040208.0000 - mae: 6733.6294 - val_loss: 9118.7437 - val_mse: 96787168.0000 - val_mae: 9118.7441\n",
            "Epoch 195/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8581.4163 - mse: 100595936.0000 - mae: 8581.4170 - val_loss: 9030.7048 - val_mse: 95242480.0000 - val_mae: 9030.7041\n",
            "Epoch 196/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6987.6875 - mse: 67503048.0000 - mae: 6987.6875 - val_loss: 8879.1108 - val_mse: 92629800.0000 - val_mae: 8879.1104\n",
            "Epoch 197/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6211.9601 - mse: 55519460.0000 - mae: 6211.9600 - val_loss: 8752.4674 - val_mse: 90478080.0000 - val_mae: 8752.4678\n",
            "Epoch 198/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7615.2684 - mse: 81700296.0000 - mae: 7615.2681 - val_loss: 8652.8667 - val_mse: 88812536.0000 - val_mae: 8652.8662\n",
            "Epoch 199/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8037.7806 - mse: 81170616.0000 - mae: 8037.7803 - val_loss: 8537.6536 - val_mse: 86911648.0000 - val_mae: 8537.6543\n",
            "Epoch 200/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8617.6925 - mse: 95842800.0000 - mae: 8617.6924 - val_loss: 8429.9665 - val_mse: 85158584.0000 - val_mae: 8429.9668\n",
            "Epoch 201/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6666.4844 - mse: 62588216.0000 - mae: 6666.4844 - val_loss: 8324.4364 - val_mse: 83462440.0000 - val_mae: 8324.4355\n",
            "Epoch 202/500\n",
            "30/30 [==============================] - 0s 947us/step - loss: 6676.8402 - mse: 58923400.0000 - mae: 6676.8403 - val_loss: 8216.6167 - val_mse: 81752184.0000 - val_mae: 8216.6162\n",
            "Epoch 203/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6397.3592 - mse: 60262160.0000 - mae: 6397.3589 - val_loss: 8123.6273 - val_mse: 80297384.0000 - val_mae: 8123.6270\n",
            "Epoch 204/500\n",
            "30/30 [==============================] - 0s 917us/step - loss: 7579.8488 - mse: 77972344.0000 - mae: 7579.8491 - val_loss: 8013.2080 - val_mse: 78593776.0000 - val_mae: 8013.2085\n",
            "Epoch 205/500\n",
            "30/30 [==============================] - 0s 967us/step - loss: 6631.4640 - mse: 71503168.0000 - mae: 6631.4644 - val_loss: 7876.8613 - val_mse: 76530432.0000 - val_mae: 7876.8613\n",
            "Epoch 206/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5826.4551 - mse: 50517016.0000 - mae: 5826.4551 - val_loss: 7783.9492 - val_mse: 75144872.0000 - val_mae: 7783.9497\n",
            "Epoch 207/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8035.4440 - mse: 82263584.0000 - mae: 8035.4438 - val_loss: 7655.9777 - val_mse: 73261096.0000 - val_mae: 7655.9775\n",
            "Epoch 208/500\n",
            "30/30 [==============================] - 0s 995us/step - loss: 8560.4961 - mse: 92076720.0000 - mae: 8560.4961 - val_loss: 7538.2150 - val_mse: 71558888.0000 - val_mae: 7538.2153\n",
            "Epoch 209/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5885.9992 - mse: 50715604.0000 - mae: 5885.9995 - val_loss: 7402.6548 - val_mse: 69634240.0000 - val_mae: 7402.6553\n",
            "Epoch 210/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 8666.5856 - mse: 89198384.0000 - mae: 8666.5850 - val_loss: 7314.2572 - val_mse: 68397016.0000 - val_mae: 7314.2573\n",
            "Epoch 211/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7094.0075 - mse: 64661604.0000 - mae: 7094.0073 - val_loss: 7216.9100 - val_mse: 67047924.0000 - val_mae: 7216.9092\n",
            "Epoch 212/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7360.2578 - mse: 74995816.0000 - mae: 7360.2578 - val_loss: 7149.0396 - val_mse: 66125200.0000 - val_mae: 7149.0400\n",
            "Epoch 213/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6202.1893 - mse: 54503364.0000 - mae: 6202.1890 - val_loss: 7005.0929 - val_mse: 64206976.0000 - val_mae: 7005.0933\n",
            "Epoch 214/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6634.1680 - mse: 60398532.0000 - mae: 6634.1675 - val_loss: 6912.8933 - val_mse: 62992208.0000 - val_mae: 6912.8936\n",
            "Epoch 215/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7320.3551 - mse: 80457424.0000 - mae: 7320.3550 - val_loss: 6836.3895 - val_mse: 61998108.0000 - val_mae: 6836.3896\n",
            "Epoch 216/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7008.6208 - mse: 64635244.0000 - mae: 7008.6206 - val_loss: 6783.6417 - val_mse: 61313396.0000 - val_mae: 6783.6416\n",
            "Epoch 217/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6944.3716 - mse: 67459856.0000 - mae: 6944.3721 - val_loss: 6705.4810 - val_mse: 60319848.0000 - val_mae: 6705.4810\n",
            "Epoch 218/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7815.7487 - mse: 94473984.0000 - mae: 7815.7490 - val_loss: 6650.0142 - val_mse: 59619556.0000 - val_mae: 6650.0137\n",
            "Epoch 219/500\n",
            "30/30 [==============================] - 0s 925us/step - loss: 7562.2938 - mse: 77994648.0000 - mae: 7562.2939 - val_loss: 6569.5875 - val_mse: 58615560.0000 - val_mae: 6569.5874\n",
            "Epoch 220/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7243.0884 - mse: 70857368.0000 - mae: 7243.0884 - val_loss: 6541.1266 - val_mse: 58263952.0000 - val_mae: 6541.1265\n",
            "Epoch 221/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5783.9297 - mse: 45742300.0000 - mae: 5783.9297 - val_loss: 6449.5158 - val_mse: 57144596.0000 - val_mae: 6449.5156\n",
            "Epoch 222/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7143.3294 - mse: 81738672.0000 - mae: 7143.3291 - val_loss: 6385.9102 - val_mse: 56393976.0000 - val_mae: 6385.9106\n",
            "Epoch 223/500\n",
            "30/30 [==============================] - 0s 895us/step - loss: 6802.6672 - mse: 61287636.0000 - mae: 6802.6670 - val_loss: 6303.0521 - val_mse: 55423752.0000 - val_mae: 6303.0527\n",
            "Epoch 224/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6644.0537 - mse: 60190788.0000 - mae: 6644.0542 - val_loss: 6213.1278 - val_mse: 54380604.0000 - val_mae: 6213.1274\n",
            "Epoch 225/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7690.5659 - mse: 77105048.0000 - mae: 7690.5669 - val_loss: 6170.2982 - val_mse: 53891464.0000 - val_mae: 6170.2983\n",
            "Epoch 226/500\n",
            "30/30 [==============================] - 0s 936us/step - loss: 6309.9860 - mse: 57397660.0000 - mae: 6309.9858 - val_loss: 6073.7607 - val_mse: 52786136.0000 - val_mae: 6073.7603\n",
            "Epoch 227/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6659.9951 - mse: 62057940.0000 - mae: 6659.9946 - val_loss: 6018.6945 - val_mse: 52169584.0000 - val_mae: 6018.6948\n",
            "Epoch 228/500\n",
            "30/30 [==============================] - 0s 1000us/step - loss: 6340.8671 - mse: 68247296.0000 - mae: 6340.8672 - val_loss: 5998.4706 - val_mse: 51947656.0000 - val_mae: 5998.4702\n",
            "Epoch 229/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6245.5993 - mse: 56247468.0000 - mae: 6245.5996 - val_loss: 5944.0642 - val_mse: 51338896.0000 - val_mae: 5944.0640\n",
            "Epoch 230/500\n",
            "30/30 [==============================] - 0s 902us/step - loss: 7578.8783 - mse: 76439400.0000 - mae: 7578.8779 - val_loss: 5913.1413 - val_mse: 50991488.0000 - val_mae: 5913.1416\n",
            "Epoch 231/500\n",
            "30/30 [==============================] - 0s 919us/step - loss: 6715.6981 - mse: 59628324.0000 - mae: 6715.6978 - val_loss: 5840.3680 - val_mse: 50204948.0000 - val_mae: 5840.3677\n",
            "Epoch 232/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7072.5239 - mse: 63345644.0000 - mae: 7072.5239 - val_loss: 5757.6875 - val_mse: 49310700.0000 - val_mae: 5757.6875\n",
            "Epoch 233/500\n",
            "30/30 [==============================] - 0s 991us/step - loss: 6044.4014 - mse: 51182660.0000 - mae: 6044.4014 - val_loss: 5696.7459 - val_mse: 48651460.0000 - val_mae: 5696.7456\n",
            "Epoch 234/500\n",
            "30/30 [==============================] - 0s 991us/step - loss: 6374.5577 - mse: 64366812.0000 - mae: 6374.5576 - val_loss: 5623.4612 - val_mse: 47872424.0000 - val_mae: 5623.4614\n",
            "Epoch 235/500\n",
            "30/30 [==============================] - 0s 932us/step - loss: 6269.7067 - mse: 56486860.0000 - mae: 6269.7065 - val_loss: 5538.1428 - val_mse: 46935092.0000 - val_mae: 5538.1426\n",
            "Epoch 236/500\n",
            "30/30 [==============================] - 0s 1000us/step - loss: 5776.0549 - mse: 50032820.0000 - mae: 5776.0552 - val_loss: 5528.7891 - val_mse: 46829140.0000 - val_mae: 5528.7891\n",
            "Epoch 237/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5735.7853 - mse: 55968064.0000 - mae: 5735.7856 - val_loss: 5454.3101 - val_mse: 46021924.0000 - val_mae: 5454.3105\n",
            "Epoch 238/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5743.3740 - mse: 50525900.0000 - mae: 5743.3740 - val_loss: 5405.1354 - val_mse: 45486648.0000 - val_mae: 5405.1353\n",
            "Epoch 239/500\n",
            "30/30 [==============================] - 0s 919us/step - loss: 5495.4632 - mse: 50345124.0000 - mae: 5495.4634 - val_loss: 5396.5031 - val_mse: 45394184.0000 - val_mae: 5396.5029\n",
            "Epoch 240/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5335.1948 - mse: 45803008.0000 - mae: 5335.1948 - val_loss: 5336.4510 - val_mse: 44755164.0000 - val_mae: 5336.4512\n",
            "Epoch 241/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6872.3459 - mse: 56859716.0000 - mae: 6872.3457 - val_loss: 5291.4275 - val_mse: 44257748.0000 - val_mae: 5291.4272\n",
            "Epoch 242/500\n",
            "30/30 [==============================] - 0s 903us/step - loss: 5716.8427 - mse: 50901240.0000 - mae: 5716.8428 - val_loss: 5251.6606 - val_mse: 43809740.0000 - val_mae: 5251.6606\n",
            "Epoch 243/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6760.8997 - mse: 74378680.0000 - mae: 6760.8994 - val_loss: 5221.8394 - val_mse: 43465944.0000 - val_mae: 5221.8394\n",
            "Epoch 244/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6354.0760 - mse: 55037544.0000 - mae: 6354.0762 - val_loss: 5127.2516 - val_mse: 42413904.0000 - val_mae: 5127.2520\n",
            "Epoch 245/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6149.0850 - mse: 52280828.0000 - mae: 6149.0850 - val_loss: 5120.7652 - val_mse: 42336220.0000 - val_mae: 5120.7651\n",
            "Epoch 246/500\n",
            "30/30 [==============================] - 0s 944us/step - loss: 5294.9445 - mse: 46844068.0000 - mae: 5294.9448 - val_loss: 5069.8652 - val_mse: 41744624.0000 - val_mae: 5069.8657\n",
            "Epoch 247/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5280.5513 - mse: 40944384.0000 - mae: 5280.5513 - val_loss: 5024.3694 - val_mse: 41190592.0000 - val_mae: 5024.3691\n",
            "Epoch 248/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6953.8641 - mse: 67449864.0000 - mae: 6953.8643 - val_loss: 4991.7127 - val_mse: 40732312.0000 - val_mae: 4991.7129\n",
            "Epoch 249/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6984.2897 - mse: 74730544.0000 - mae: 6984.2896 - val_loss: 4971.8681 - val_mse: 40467892.0000 - val_mae: 4971.8682\n",
            "Epoch 250/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6276.7560 - mse: 53662680.0000 - mae: 6276.7563 - val_loss: 4944.9120 - val_mse: 40109516.0000 - val_mae: 4944.9121\n",
            "Epoch 251/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7205.1494 - mse: 71456112.0000 - mae: 7205.1494 - val_loss: 4938.4328 - val_mse: 40015016.0000 - val_mae: 4938.4326\n",
            "Epoch 252/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6263.6992 - mse: 60385720.0000 - mae: 6263.6997 - val_loss: 4936.8516 - val_mse: 39998468.0000 - val_mae: 4936.8516\n",
            "Epoch 253/500\n",
            "30/30 [==============================] - 0s 939us/step - loss: 5801.1408 - mse: 47452588.0000 - mae: 5801.1406 - val_loss: 4938.5537 - val_mse: 40029604.0000 - val_mae: 4938.5542\n",
            "Epoch 254/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7685.0885 - mse: 74688184.0000 - mae: 7685.0884 - val_loss: 4925.4898 - val_mse: 39860548.0000 - val_mae: 4925.4902\n",
            "Epoch 255/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4835.4085 - mse: 44408196.0000 - mae: 4835.4082 - val_loss: 4939.3208 - val_mse: 40044884.0000 - val_mae: 4939.3208\n",
            "Epoch 256/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6243.0532 - mse: 63726280.0000 - mae: 6243.0532 - val_loss: 4904.7714 - val_mse: 39578692.0000 - val_mae: 4904.7715\n",
            "Epoch 257/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7148.4860 - mse: 73274872.0000 - mae: 7148.4858 - val_loss: 4930.5863 - val_mse: 39927844.0000 - val_mae: 4930.5864\n",
            "Epoch 258/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5825.8164 - mse: 49500708.0000 - mae: 5825.8169 - val_loss: 4905.3266 - val_mse: 39594556.0000 - val_mae: 4905.3267\n",
            "Epoch 259/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4555.5072 - mse: 39781840.0000 - mae: 4555.5073 - val_loss: 4899.2477 - val_mse: 39510044.0000 - val_mae: 4899.2480\n",
            "Epoch 260/500\n",
            "30/30 [==============================] - 0s 935us/step - loss: 5871.5363 - mse: 64545684.0000 - mae: 5871.5366 - val_loss: 4905.8062 - val_mse: 39597260.0000 - val_mae: 4905.8062\n",
            "Epoch 261/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6727.9482 - mse: 68997816.0000 - mae: 6727.9478 - val_loss: 4922.7310 - val_mse: 39813096.0000 - val_mae: 4922.7314\n",
            "Epoch 262/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6316.7928 - mse: 59970396.0000 - mae: 6316.7925 - val_loss: 4946.0773 - val_mse: 40126208.0000 - val_mae: 4946.0771\n",
            "Epoch 263/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5527.6499 - mse: 45723340.0000 - mae: 5527.6499 - val_loss: 4937.2052 - val_mse: 40004148.0000 - val_mae: 4937.2051\n",
            "Epoch 264/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6594.5509 - mse: 60868464.0000 - mae: 6594.5513 - val_loss: 4901.7029 - val_mse: 39520860.0000 - val_mae: 4901.7031\n",
            "Epoch 265/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5647.9333 - mse: 56750484.0000 - mae: 5647.9331 - val_loss: 4885.3455 - val_mse: 39308668.0000 - val_mae: 4885.3452\n",
            "Epoch 266/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5670.0864 - mse: 48767188.0000 - mae: 5670.0864 - val_loss: 4809.8743 - val_mse: 38332956.0000 - val_mae: 4809.8745\n",
            "Epoch 267/500\n",
            "30/30 [==============================] - 0s 898us/step - loss: 6067.8802 - mse: 52180976.0000 - mae: 6067.8804 - val_loss: 4776.1136 - val_mse: 37827720.0000 - val_mae: 4776.1138\n",
            "Epoch 268/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 5447.9832 - mse: 59334340.0000 - mae: 5447.9834 - val_loss: 4754.8063 - val_mse: 37485224.0000 - val_mae: 4754.8062\n",
            "Epoch 269/500\n",
            "30/30 [==============================] - 0s 944us/step - loss: 6250.0145 - mse: 61771484.0000 - mae: 6250.0146 - val_loss: 4746.5859 - val_mse: 37361944.0000 - val_mae: 4746.5859\n",
            "Epoch 270/500\n",
            "30/30 [==============================] - 0s 980us/step - loss: 6871.4294 - mse: 71923720.0000 - mae: 6871.4292 - val_loss: 4769.6659 - val_mse: 37733356.0000 - val_mae: 4769.6660\n",
            "Epoch 271/500\n",
            "30/30 [==============================] - 0s 905us/step - loss: 5166.9624 - mse: 46167144.0000 - mae: 5166.9624 - val_loss: 4743.7159 - val_mse: 37330280.0000 - val_mae: 4743.7158\n",
            "Epoch 272/500\n",
            "30/30 [==============================] - 0s 949us/step - loss: 8026.1466 - mse: 82750616.0000 - mae: 8026.1470 - val_loss: 4716.7091 - val_mse: 36915816.0000 - val_mae: 4716.7095\n",
            "Epoch 273/500\n",
            "30/30 [==============================] - 0s 911us/step - loss: 5693.6930 - mse: 57096540.0000 - mae: 5693.6934 - val_loss: 4698.7735 - val_mse: 36628304.0000 - val_mae: 4698.7734\n",
            "Epoch 274/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6214.8766 - mse: 54105672.0000 - mae: 6214.8770 - val_loss: 4667.6775 - val_mse: 36104428.0000 - val_mae: 4667.6777\n",
            "Epoch 275/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5331.1506 - mse: 40713976.0000 - mae: 5331.1504 - val_loss: 4655.8399 - val_mse: 35856300.0000 - val_mae: 4655.8403\n",
            "Epoch 276/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5094.2254 - mse: 41489420.0000 - mae: 5094.2256 - val_loss: 4659.0771 - val_mse: 35929648.0000 - val_mae: 4659.0771\n",
            "Epoch 277/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6258.7085 - mse: 55530456.0000 - mae: 6258.7085 - val_loss: 4653.3576 - val_mse: 35812968.0000 - val_mae: 4653.3579\n",
            "Epoch 278/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6264.3312 - mse: 60107980.0000 - mae: 6264.3311 - val_loss: 4639.8717 - val_mse: 35541312.0000 - val_mae: 4639.8721\n",
            "Epoch 279/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6469.5290 - mse: 54918904.0000 - mae: 6469.5293 - val_loss: 4633.4561 - val_mse: 35415456.0000 - val_mae: 4633.4556\n",
            "Epoch 280/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5814.9320 - mse: 58155392.0000 - mae: 5814.9316 - val_loss: 4652.2639 - val_mse: 35796728.0000 - val_mae: 4652.2642\n",
            "Epoch 281/500\n",
            "30/30 [==============================] - 0s 908us/step - loss: 5920.7593 - mse: 48345344.0000 - mae: 5920.7593 - val_loss: 4631.5387 - val_mse: 35386212.0000 - val_mae: 4631.5386\n",
            "Epoch 282/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6570.9759 - mse: 63122056.0000 - mae: 6570.9761 - val_loss: 4633.1428 - val_mse: 35424216.0000 - val_mae: 4633.1426\n",
            "Epoch 283/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6512.3848 - mse: 65398272.0000 - mae: 6512.3848 - val_loss: 4629.5673 - val_mse: 35341984.0000 - val_mae: 4629.5674\n",
            "Epoch 284/500\n",
            "30/30 [==============================] - 0s 924us/step - loss: 6452.2700 - mse: 63351316.0000 - mae: 6452.2700 - val_loss: 4626.6866 - val_mse: 35277288.0000 - val_mae: 4626.6865\n",
            "Epoch 285/500\n",
            "30/30 [==============================] - 0s 983us/step - loss: 5892.2686 - mse: 57587836.0000 - mae: 5892.2686 - val_loss: 4636.3702 - val_mse: 35476592.0000 - val_mae: 4636.3701\n",
            "Epoch 286/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5749.1004 - mse: 49101200.0000 - mae: 5749.1006 - val_loss: 4647.3899 - val_mse: 35687236.0000 - val_mae: 4647.3901\n",
            "Epoch 287/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6592.2611 - mse: 74017552.0000 - mae: 6592.2612 - val_loss: 4643.2668 - val_mse: 35594640.0000 - val_mae: 4643.2666\n",
            "Epoch 288/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5425.7163 - mse: 52366936.0000 - mae: 5425.7168 - val_loss: 4661.1038 - val_mse: 35964476.0000 - val_mae: 4661.1040\n",
            "Epoch 289/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6324.0441 - mse: 64771388.0000 - mae: 6324.0444 - val_loss: 4663.3245 - val_mse: 36008112.0000 - val_mae: 4663.3247\n",
            "Epoch 290/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5858.2509 - mse: 53052624.0000 - mae: 5858.2505 - val_loss: 4662.9624 - val_mse: 36001972.0000 - val_mae: 4662.9624\n",
            "Epoch 291/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 4906.0145 - mse: 35382088.0000 - mae: 4906.0146 - val_loss: 4657.8823 - val_mse: 35901384.0000 - val_mae: 4657.8823\n",
            "Epoch 292/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 7183.0254 - mse: 74696296.0000 - mae: 7183.0249 - val_loss: 4663.1501 - val_mse: 36005964.0000 - val_mae: 4663.1504\n",
            "Epoch 293/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6790.1436 - mse: 72814880.0000 - mae: 6790.1431 - val_loss: 4654.4261 - val_mse: 35841792.0000 - val_mae: 4654.4263\n",
            "Epoch 294/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 5223.2567 - mse: 33618052.0000 - mae: 5223.2563 - val_loss: 4656.6571 - val_mse: 35878584.0000 - val_mae: 4656.6572\n",
            "Epoch 295/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6869.2529 - mse: 71838568.0000 - mae: 6869.2529 - val_loss: 4647.2973 - val_mse: 35679880.0000 - val_mae: 4647.2974\n",
            "Epoch 296/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6926.1799 - mse: 69371664.0000 - mae: 6926.1797 - val_loss: 4648.4991 - val_mse: 35712660.0000 - val_mae: 4648.4995\n",
            "Epoch 297/500\n",
            "30/30 [==============================] - 0s 913us/step - loss: 5212.9761 - mse: 44744344.0000 - mae: 5212.9761 - val_loss: 4655.8792 - val_mse: 35871624.0000 - val_mae: 4655.8794\n",
            "Epoch 298/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6428.7734 - mse: 57106568.0000 - mae: 6428.7729 - val_loss: 4662.4346 - val_mse: 36012040.0000 - val_mae: 4662.4346\n",
            "Epoch 299/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5648.7493 - mse: 48596276.0000 - mae: 5648.7490 - val_loss: 4674.9717 - val_mse: 36248556.0000 - val_mae: 4674.9717\n",
            "Epoch 300/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5348.8273 - mse: 42826504.0000 - mae: 5348.8271 - val_loss: 4644.8169 - val_mse: 35666320.0000 - val_mae: 4644.8169\n",
            "Epoch 301/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6246.3776 - mse: 61724208.0000 - mae: 6246.3774 - val_loss: 4636.2793 - val_mse: 35501876.0000 - val_mae: 4636.2793\n",
            "Epoch 302/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6280.0028 - mse: 62999612.0000 - mae: 6280.0024 - val_loss: 4640.8488 - val_mse: 35607180.0000 - val_mae: 4640.8491\n",
            "Epoch 303/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 5691.6147 - mse: 54606320.0000 - mae: 5691.6147 - val_loss: 4640.5611 - val_mse: 35595468.0000 - val_mae: 4640.5615\n",
            "Epoch 304/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 6791.0482 - mse: 70654704.0000 - mae: 6791.0483 - val_loss: 4628.3470 - val_mse: 35342420.0000 - val_mae: 4628.3467\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50 samples, validate on 40 samples\n",
            "Epoch 1/500\n",
            "50/50 [==============================] - 2s 36ms/step - loss: 24047.3449 - mse: 597862336.0000 - mae: 24047.3457 - val_loss: 25553.9263 - val_mse: 666561536.0000 - val_mae: 25553.9258\n",
            "Epoch 2/500\n",
            "50/50 [==============================] - 0s 653us/step - loss: 24047.2871 - mse: 597859456.0000 - mae: 24047.2871 - val_loss: 25553.8306 - val_mse: 666556288.0000 - val_mae: 25553.8320\n",
            "Epoch 3/500\n",
            "50/50 [==============================] - 0s 716us/step - loss: 24047.0934 - mse: 597849792.0000 - mae: 24047.0918 - val_loss: 25553.4033 - val_mse: 666532160.0000 - val_mae: 25553.4023\n",
            "Epoch 4/500\n",
            "50/50 [==============================] - 0s 716us/step - loss: 24046.0656 - mse: 597799488.0000 - mae: 24046.0645 - val_loss: 25551.9575 - val_mse: 666450944.0000 - val_mae: 25551.9570\n",
            "Epoch 5/500\n",
            "50/50 [==============================] - 0s 680us/step - loss: 24044.8473 - mse: 597743552.0000 - mae: 24044.8477 - val_loss: 25549.9736 - val_mse: 666340992.0000 - val_mae: 25549.9746\n",
            "Epoch 6/500\n",
            "50/50 [==============================] - 0s 688us/step - loss: 24042.5172 - mse: 597629120.0000 - mae: 24042.5176 - val_loss: 25547.2217 - val_mse: 666189888.0000 - val_mae: 25547.2227\n",
            "Epoch 7/500\n",
            "50/50 [==============================] - 0s 713us/step - loss: 24040.3246 - mse: 597524672.0000 - mae: 24040.3242 - val_loss: 25544.1636 - val_mse: 666024064.0000 - val_mae: 25544.1641\n",
            "Epoch 8/500\n",
            "50/50 [==============================] - 0s 670us/step - loss: 24036.4813 - mse: 597343104.0000 - mae: 24036.4824 - val_loss: 25540.5645 - val_mse: 665831296.0000 - val_mae: 25540.5664\n",
            "Epoch 9/500\n",
            "50/50 [==============================] - 0s 638us/step - loss: 24033.4785 - mse: 597189504.0000 - mae: 24033.4766 - val_loss: 25536.8120 - val_mse: 665632256.0000 - val_mae: 25536.8125\n",
            "Epoch 10/500\n",
            "50/50 [==============================] - 0s 652us/step - loss: 24028.2316 - mse: 596944832.0000 - mae: 24028.2324 - val_loss: 25532.6260 - val_mse: 665409152.0000 - val_mae: 25532.6250\n",
            "Epoch 11/500\n",
            "50/50 [==============================] - 0s 650us/step - loss: 24024.8219 - mse: 596753920.0000 - mae: 24024.8203 - val_loss: 25528.2905 - val_mse: 665177984.0000 - val_mae: 25528.2910\n",
            "Epoch 12/500\n",
            "50/50 [==============================] - 0s 664us/step - loss: 24020.2574 - mse: 596531456.0000 - mae: 24020.2559 - val_loss: 25523.5127 - val_mse: 664924544.0000 - val_mae: 25523.5137\n",
            "Epoch 13/500\n",
            "50/50 [==============================] - 0s 721us/step - loss: 24014.8469 - mse: 596290048.0000 - mae: 24014.8477 - val_loss: 25518.1108 - val_mse: 664636160.0000 - val_mae: 25518.1113\n",
            "Epoch 14/500\n",
            "50/50 [==============================] - 0s 631us/step - loss: 24012.4801 - mse: 596163392.0000 - mae: 24012.4805 - val_loss: 25511.4380 - val_mse: 664278464.0000 - val_mae: 25511.4375\n",
            "Epoch 15/500\n",
            "50/50 [==============================] - 0s 635us/step - loss: 24005.5000 - mse: 595820608.0000 - mae: 24005.5000 - val_loss: 25504.2651 - val_mse: 663894848.0000 - val_mae: 25504.2656\n",
            "Epoch 16/500\n",
            "50/50 [==============================] - 0s 651us/step - loss: 23996.0148 - mse: 595394816.0000 - mae: 23996.0156 - val_loss: 25496.5151 - val_mse: 663480512.0000 - val_mae: 25496.5156\n",
            "Epoch 17/500\n",
            "50/50 [==============================] - 0s 661us/step - loss: 23989.2289 - mse: 595077312.0000 - mae: 23989.2266 - val_loss: 25488.1255 - val_mse: 663033728.0000 - val_mae: 25488.1250\n",
            "Epoch 18/500\n",
            "50/50 [==============================] - 0s 660us/step - loss: 23987.5285 - mse: 594960576.0000 - mae: 23987.5273 - val_loss: 25479.8545 - val_mse: 662593728.0000 - val_mae: 25479.8555\n",
            "Epoch 19/500\n",
            "50/50 [==============================] - 0s 613us/step - loss: 23975.7938 - mse: 594397248.0000 - mae: 23975.7930 - val_loss: 25469.6309 - val_mse: 662047808.0000 - val_mae: 25469.6309\n",
            "Epoch 20/500\n",
            "50/50 [==============================] - 0s 626us/step - loss: 23948.4391 - mse: 593012032.0000 - mae: 23948.4395 - val_loss: 25456.8774 - val_mse: 661366400.0000 - val_mae: 25456.8789\n",
            "Epoch 21/500\n",
            "50/50 [==============================] - 0s 659us/step - loss: 23946.6195 - mse: 593091968.0000 - mae: 23946.6191 - val_loss: 25444.9570 - val_mse: 660732416.0000 - val_mae: 25444.9570\n",
            "Epoch 22/500\n",
            "50/50 [==============================] - 0s 663us/step - loss: 23934.6648 - mse: 592427520.0000 - mae: 23934.6641 - val_loss: 25432.3618 - val_mse: 660063744.0000 - val_mae: 25432.3633\n",
            "Epoch 23/500\n",
            "50/50 [==============================] - 0s 690us/step - loss: 23919.2172 - mse: 591764224.0000 - mae: 23919.2168 - val_loss: 25418.4800 - val_mse: 659325312.0000 - val_mae: 25418.4805\n",
            "Epoch 24/500\n",
            "50/50 [==============================] - 0s 694us/step - loss: 23905.0559 - mse: 590918016.0000 - mae: 23905.0547 - val_loss: 25404.0732 - val_mse: 658560256.0000 - val_mae: 25404.0742\n",
            "Epoch 25/500\n",
            "50/50 [==============================] - 0s 702us/step - loss: 23900.7207 - mse: 590705152.0000 - mae: 23900.7207 - val_loss: 25389.7388 - val_mse: 657800064.0000 - val_mae: 25389.7383\n",
            "Epoch 26/500\n",
            "50/50 [==============================] - 0s 690us/step - loss: 23884.3000 - mse: 589998592.0000 - mae: 23884.3008 - val_loss: 25374.4868 - val_mse: 656992384.0000 - val_mae: 25374.4863\n",
            "Epoch 27/500\n",
            "50/50 [==============================] - 0s 715us/step - loss: 23860.2012 - mse: 588811776.0000 - mae: 23860.2031 - val_loss: 25358.2290 - val_mse: 656131392.0000 - val_mae: 25358.2285\n",
            "Epoch 28/500\n",
            "50/50 [==============================] - 0s 635us/step - loss: 23838.0812 - mse: 587970816.0000 - mae: 23838.0820 - val_loss: 25340.6401 - val_mse: 655200128.0000 - val_mae: 25340.6406\n",
            "Epoch 29/500\n",
            "50/50 [==============================] - 0s 630us/step - loss: 23824.7898 - mse: 586883072.0000 - mae: 23824.7891 - val_loss: 25322.2832 - val_mse: 654234944.0000 - val_mae: 25322.2852\n",
            "Epoch 30/500\n",
            "50/50 [==============================] - 0s 797us/step - loss: 23799.4902 - mse: 585976640.0000 - mae: 23799.4902 - val_loss: 25302.8398 - val_mse: 653213312.0000 - val_mae: 25302.8398\n",
            "Epoch 31/500\n",
            "50/50 [==============================] - 0s 618us/step - loss: 23788.3609 - mse: 585446528.0000 - mae: 23788.3594 - val_loss: 25283.2344 - val_mse: 652185984.0000 - val_mae: 25283.2344\n",
            "Epoch 32/500\n",
            "50/50 [==============================] - 0s 824us/step - loss: 23775.4121 - mse: 584841536.0000 - mae: 23775.4121 - val_loss: 25262.9717 - val_mse: 651124672.0000 - val_mae: 25262.9727\n",
            "Epoch 33/500\n",
            "50/50 [==============================] - 0s 626us/step - loss: 23743.4078 - mse: 583378368.0000 - mae: 23743.4082 - val_loss: 25241.3633 - val_mse: 649986752.0000 - val_mae: 25241.3633\n",
            "Epoch 34/500\n",
            "50/50 [==============================] - 0s 597us/step - loss: 23723.4191 - mse: 582407104.0000 - mae: 23723.4180 - val_loss: 25219.1440 - val_mse: 648824256.0000 - val_mae: 25219.1445\n",
            "Epoch 35/500\n",
            "50/50 [==============================] - 0s 696us/step - loss: 23705.8258 - mse: 581749376.0000 - mae: 23705.8281 - val_loss: 25196.5630 - val_mse: 647635840.0000 - val_mae: 25196.5625\n",
            "Epoch 36/500\n",
            "50/50 [==============================] - 0s 696us/step - loss: 23672.6895 - mse: 579943488.0000 - mae: 23672.6895 - val_loss: 25172.2231 - val_mse: 646361344.0000 - val_mae: 25172.2246\n",
            "Epoch 37/500\n",
            "50/50 [==============================] - 0s 729us/step - loss: 23658.1059 - mse: 579219840.0000 - mae: 23658.1055 - val_loss: 25147.9316 - val_mse: 645091840.0000 - val_mae: 25147.9336\n",
            "Epoch 38/500\n",
            "50/50 [==============================] - 0s 800us/step - loss: 23634.0207 - mse: 578070784.0000 - mae: 23634.0195 - val_loss: 25122.8179 - val_mse: 643780608.0000 - val_mae: 25122.8164\n",
            "Epoch 39/500\n",
            "50/50 [==============================] - 0s 632us/step - loss: 23647.3160 - mse: 578519936.0000 - mae: 23647.3184 - val_loss: 25099.0415 - val_mse: 642540672.0000 - val_mae: 25099.0430\n",
            "Epoch 40/500\n",
            "50/50 [==============================] - 0s 719us/step - loss: 23560.9668 - mse: 574721728.0000 - mae: 23560.9668 - val_loss: 25071.3032 - val_mse: 641093696.0000 - val_mae: 25071.3027\n",
            "Epoch 41/500\n",
            "50/50 [==============================] - 0s 736us/step - loss: 23528.0207 - mse: 573304640.0000 - mae: 23528.0195 - val_loss: 25042.1309 - val_mse: 639574912.0000 - val_mae: 25042.1309\n",
            "Epoch 42/500\n",
            "50/50 [==============================] - 0s 716us/step - loss: 23562.1512 - mse: 574518528.0000 - mae: 23562.1523 - val_loss: 25015.6450 - val_mse: 638198784.0000 - val_mae: 25015.6445\n",
            "Epoch 43/500\n",
            "50/50 [==============================] - 0s 696us/step - loss: 23564.7188 - mse: 575002688.0000 - mae: 23564.7207 - val_loss: 24989.7383 - val_mse: 636852032.0000 - val_mae: 24989.7383\n",
            "Epoch 44/500\n",
            "50/50 [==============================] - 0s 728us/step - loss: 23470.3352 - mse: 570058112.0000 - mae: 23470.3359 - val_loss: 24959.4268 - val_mse: 635285184.0000 - val_mae: 24959.4277\n",
            "Epoch 45/500\n",
            "50/50 [==============================] - 0s 733us/step - loss: 23424.5438 - mse: 568256320.0000 - mae: 23424.5449 - val_loss: 24927.7783 - val_mse: 633651328.0000 - val_mae: 24927.7773\n",
            "Epoch 46/500\n",
            "50/50 [==============================] - 0s 807us/step - loss: 23372.5676 - mse: 565483456.0000 - mae: 23372.5684 - val_loss: 24894.3701 - val_mse: 631929664.0000 - val_mae: 24894.3691\n",
            "Epoch 47/500\n",
            "50/50 [==============================] - 0s 892us/step - loss: 23315.4516 - mse: 563305728.0000 - mae: 23315.4531 - val_loss: 24859.2778 - val_mse: 630122048.0000 - val_mae: 24859.2773\n",
            "Epoch 48/500\n",
            "50/50 [==============================] - 0s 738us/step - loss: 23334.0418 - mse: 564036608.0000 - mae: 23334.0430 - val_loss: 24826.1016 - val_mse: 628420160.0000 - val_mae: 24826.1016\n",
            "Epoch 49/500\n",
            "50/50 [==============================] - 0s 737us/step - loss: 23327.8656 - mse: 563717952.0000 - mae: 23327.8652 - val_loss: 24793.0469 - val_mse: 626725184.0000 - val_mae: 24793.0449\n",
            "Epoch 50/500\n",
            "50/50 [==============================] - 0s 732us/step - loss: 23309.8191 - mse: 562394496.0000 - mae: 23309.8203 - val_loss: 24759.9395 - val_mse: 625028416.0000 - val_mae: 24759.9395\n",
            "Epoch 51/500\n",
            "50/50 [==============================] - 0s 736us/step - loss: 23190.5062 - mse: 557303040.0000 - mae: 23190.5059 - val_loss: 24722.4805 - val_mse: 623113216.0000 - val_mae: 24722.4805\n",
            "Epoch 52/500\n",
            "50/50 [==============================] - 0s 804us/step - loss: 23178.8246 - mse: 556603776.0000 - mae: 23178.8242 - val_loss: 24685.2534 - val_mse: 621213056.0000 - val_mae: 24685.2539\n",
            "Epoch 53/500\n",
            "50/50 [==============================] - 0s 741us/step - loss: 23176.8551 - mse: 556312768.0000 - mae: 23176.8555 - val_loss: 24648.6846 - val_mse: 619348288.0000 - val_mae: 24648.6855\n",
            "Epoch 54/500\n",
            "50/50 [==============================] - 0s 708us/step - loss: 23179.2191 - mse: 556278784.0000 - mae: 23179.2207 - val_loss: 24612.6934 - val_mse: 617516416.0000 - val_mae: 24612.6934\n",
            "Epoch 55/500\n",
            "50/50 [==============================] - 0s 789us/step - loss: 23093.9891 - mse: 553298880.0000 - mae: 23093.9902 - val_loss: 24574.2759 - val_mse: 615569024.0000 - val_mae: 24574.2754\n",
            "Epoch 56/500\n",
            "50/50 [==============================] - 0s 863us/step - loss: 23085.1371 - mse: 552674176.0000 - mae: 23085.1348 - val_loss: 24536.2031 - val_mse: 613636864.0000 - val_mae: 24536.2031\n",
            "Epoch 57/500\n",
            "50/50 [==============================] - 0s 875us/step - loss: 23038.2313 - mse: 550141184.0000 - mae: 23038.2305 - val_loss: 24496.8198 - val_mse: 611643008.0000 - val_mae: 24496.8203\n",
            "Epoch 58/500\n",
            "50/50 [==============================] - 0s 705us/step - loss: 22997.7059 - mse: 548645504.0000 - mae: 22997.7051 - val_loss: 24456.5669 - val_mse: 609613632.0000 - val_mae: 24456.5664\n",
            "Epoch 59/500\n",
            "50/50 [==============================] - 0s 807us/step - loss: 22945.4801 - mse: 546044288.0000 - mae: 22945.4805 - val_loss: 24415.4292 - val_mse: 607544384.0000 - val_mae: 24415.4297\n",
            "Epoch 60/500\n",
            "50/50 [==============================] - 0s 699us/step - loss: 22853.1141 - mse: 542099008.0000 - mae: 22853.1152 - val_loss: 24371.2759 - val_mse: 605322624.0000 - val_mae: 24371.2754\n",
            "Epoch 61/500\n",
            "50/50 [==============================] - 0s 701us/step - loss: 22817.5586 - mse: 540374080.0000 - mae: 22817.5566 - val_loss: 24326.5405 - val_mse: 603074176.0000 - val_mae: 24326.5410\n",
            "Epoch 62/500\n",
            "50/50 [==============================] - 0s 723us/step - loss: 22819.8664 - mse: 540122112.0000 - mae: 22819.8652 - val_loss: 24282.8125 - val_mse: 600883328.0000 - val_mae: 24282.8145\n",
            "Epoch 63/500\n",
            "50/50 [==============================] - 0s 709us/step - loss: 22838.5258 - mse: 540867776.0000 - mae: 22838.5254 - val_loss: 24240.6616 - val_mse: 598773824.0000 - val_mae: 24240.6621\n",
            "Epoch 64/500\n",
            "50/50 [==============================] - 0s 819us/step - loss: 22717.5230 - mse: 535655136.0000 - mae: 22717.5234 - val_loss: 24194.7461 - val_mse: 596476608.0000 - val_mae: 24194.7461\n",
            "Epoch 65/500\n",
            "50/50 [==============================] - 0s 799us/step - loss: 22710.9938 - mse: 535568256.0000 - mae: 22710.9941 - val_loss: 24149.2046 - val_mse: 594202624.0000 - val_mae: 24149.2070\n",
            "Epoch 66/500\n",
            "50/50 [==============================] - 0s 725us/step - loss: 22669.7855 - mse: 533024448.0000 - mae: 22669.7852 - val_loss: 24103.1562 - val_mse: 591917760.0000 - val_mae: 24103.1562\n",
            "Epoch 67/500\n",
            "50/50 [==============================] - 0s 689us/step - loss: 22531.1094 - mse: 526778848.0000 - mae: 22531.1094 - val_loss: 24053.3276 - val_mse: 589443200.0000 - val_mae: 24053.3281\n",
            "Epoch 68/500\n",
            "50/50 [==============================] - 0s 714us/step - loss: 22529.0246 - mse: 526521888.0000 - mae: 22529.0254 - val_loss: 24003.4292 - val_mse: 586976512.0000 - val_mae: 24003.4277\n",
            "Epoch 69/500\n",
            "50/50 [==============================] - 0s 772us/step - loss: 22456.1090 - mse: 524332736.0000 - mae: 22456.1094 - val_loss: 23952.8696 - val_mse: 584481856.0000 - val_mae: 23952.8691\n",
            "Epoch 70/500\n",
            "50/50 [==============================] - 0s 797us/step - loss: 22392.4016 - mse: 521808512.0000 - mae: 22392.4023 - val_loss: 23900.9370 - val_mse: 581927808.0000 - val_mae: 23900.9375\n",
            "Epoch 71/500\n",
            "50/50 [==============================] - 0s 767us/step - loss: 22289.9203 - mse: 516400672.0000 - mae: 22289.9199 - val_loss: 23846.7446 - val_mse: 579262528.0000 - val_mae: 23846.7461\n",
            "Epoch 72/500\n",
            "50/50 [==============================] - 0s 703us/step - loss: 22403.1711 - mse: 522104512.0000 - mae: 22403.1699 - val_loss: 23796.8921 - val_mse: 576818624.0000 - val_mae: 23796.8926\n",
            "Epoch 73/500\n",
            "50/50 [==============================] - 0s 761us/step - loss: 22330.4988 - mse: 518269568.0000 - mae: 22330.5000 - val_loss: 23745.5176 - val_mse: 574313280.0000 - val_mae: 23745.5176\n",
            "Epoch 74/500\n",
            "50/50 [==============================] - 0s 691us/step - loss: 22262.6434 - mse: 515282624.0000 - mae: 22262.6445 - val_loss: 23692.9321 - val_mse: 571751488.0000 - val_mae: 23692.9336\n",
            "Epoch 75/500\n",
            "50/50 [==============================] - 0s 836us/step - loss: 22072.7684 - mse: 507591840.0000 - mae: 22072.7695 - val_loss: 23635.2290 - val_mse: 568948544.0000 - val_mae: 23635.2305\n",
            "Epoch 76/500\n",
            "50/50 [==============================] - 0s 845us/step - loss: 22069.1449 - mse: 506725952.0000 - mae: 22069.1445 - val_loss: 23578.9395 - val_mse: 566224512.0000 - val_mae: 23578.9395\n",
            "Epoch 77/500\n",
            "50/50 [==============================] - 0s 850us/step - loss: 22021.1602 - mse: 503691104.0000 - mae: 22021.1602 - val_loss: 23521.6528 - val_mse: 563457280.0000 - val_mae: 23521.6523\n",
            "Epoch 78/500\n",
            "50/50 [==============================] - 0s 877us/step - loss: 21892.6793 - mse: 498442240.0000 - mae: 21892.6797 - val_loss: 23461.2368 - val_mse: 560540544.0000 - val_mae: 23461.2383\n",
            "Epoch 79/500\n",
            "50/50 [==============================] - 0s 908us/step - loss: 21803.2660 - mse: 495200000.0000 - mae: 21803.2656 - val_loss: 23397.9429 - val_mse: 557496320.0000 - val_mae: 23397.9434\n",
            "Epoch 80/500\n",
            "50/50 [==============================] - 0s 901us/step - loss: 22061.4570 - mse: 506458080.0000 - mae: 22061.4570 - val_loss: 23340.5249 - val_mse: 554733760.0000 - val_mae: 23340.5234\n",
            "Epoch 81/500\n",
            "50/50 [==============================] - 0s 746us/step - loss: 21746.6078 - mse: 493183072.0000 - mae: 21746.6074 - val_loss: 23275.4160 - val_mse: 551614848.0000 - val_mae: 23275.4160\n",
            "Epoch 82/500\n",
            "50/50 [==============================] - 0s 702us/step - loss: 21846.0234 - mse: 496952160.0000 - mae: 21846.0254 - val_loss: 23213.2065 - val_mse: 548655488.0000 - val_mae: 23213.2070\n",
            "Epoch 83/500\n",
            "50/50 [==============================] - 0s 632us/step - loss: 21779.5172 - mse: 496402592.0000 - mae: 21779.5176 - val_loss: 23150.2300 - val_mse: 545656384.0000 - val_mae: 23150.2305\n",
            "Epoch 84/500\n",
            "50/50 [==============================] - 0s 780us/step - loss: 21684.6691 - mse: 490342272.0000 - mae: 21684.6699 - val_loss: 23084.8511 - val_mse: 542556992.0000 - val_mae: 23084.8496\n",
            "Epoch 85/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 21548.5434 - mse: 483045376.0000 - mae: 21548.5430 - val_loss: 23016.1792 - val_mse: 539308160.0000 - val_mae: 23016.1797\n",
            "Epoch 86/500\n",
            "50/50 [==============================] - 0s 873us/step - loss: 21453.3805 - mse: 480450848.0000 - mae: 21453.3809 - val_loss: 22945.8774 - val_mse: 535992832.0000 - val_mae: 22945.8770\n",
            "Epoch 87/500\n",
            "50/50 [==============================] - 0s 758us/step - loss: 21455.8629 - mse: 479268480.0000 - mae: 21455.8633 - val_loss: 22875.9746 - val_mse: 532712864.0000 - val_mae: 22875.9746\n",
            "Epoch 88/500\n",
            "50/50 [==============================] - 0s 744us/step - loss: 21336.2316 - mse: 475137216.0000 - mae: 21336.2324 - val_loss: 22804.9893 - val_mse: 529391424.0000 - val_mae: 22804.9902\n",
            "Epoch 89/500\n",
            "50/50 [==============================] - 0s 678us/step - loss: 21225.3641 - mse: 470721696.0000 - mae: 21225.3652 - val_loss: 22731.2563 - val_mse: 525948480.0000 - val_mae: 22731.2559\n",
            "Epoch 90/500\n",
            "50/50 [==============================] - 0s 755us/step - loss: 21319.5488 - mse: 472848832.0000 - mae: 21319.5508 - val_loss: 22659.9702 - val_mse: 522631776.0000 - val_mae: 22659.9688\n",
            "Epoch 91/500\n",
            "50/50 [==============================] - 0s 735us/step - loss: 21165.5719 - mse: 466615040.0000 - mae: 21165.5723 - val_loss: 22586.5190 - val_mse: 519231488.0000 - val_mae: 22586.5195\n",
            "Epoch 92/500\n",
            "50/50 [==============================] - 0s 812us/step - loss: 20893.0422 - mse: 457379424.0000 - mae: 20893.0430 - val_loss: 22506.7568 - val_mse: 515548736.0000 - val_mae: 22506.7559\n",
            "Epoch 93/500\n",
            "50/50 [==============================] - 0s 757us/step - loss: 20988.3305 - mse: 461703744.0000 - mae: 20988.3301 - val_loss: 22431.2036 - val_mse: 512080128.0000 - val_mae: 22431.2051\n",
            "Epoch 94/500\n",
            "50/50 [==============================] - 0s 882us/step - loss: 20822.8906 - mse: 453676032.0000 - mae: 20822.8906 - val_loss: 22351.0024 - val_mse: 508392768.0000 - val_mae: 22351.0039\n",
            "Epoch 95/500\n",
            "50/50 [==============================] - 0s 739us/step - loss: 20953.1832 - mse: 460667936.0000 - mae: 20953.1816 - val_loss: 22274.5767 - val_mse: 504901952.0000 - val_mae: 22274.5762\n",
            "Epoch 96/500\n",
            "50/50 [==============================] - 0s 756us/step - loss: 20693.6223 - mse: 449388992.0000 - mae: 20693.6211 - val_loss: 22193.9951 - val_mse: 501243840.0000 - val_mae: 22193.9961\n",
            "Epoch 97/500\n",
            "50/50 [==============================] - 0s 727us/step - loss: 20770.7941 - mse: 452981056.0000 - mae: 20770.7930 - val_loss: 22115.4453 - val_mse: 497685696.0000 - val_mae: 22115.4453\n",
            "Epoch 98/500\n",
            "50/50 [==============================] - 0s 752us/step - loss: 20692.2164 - mse: 447432544.0000 - mae: 20692.2168 - val_loss: 22035.5146 - val_mse: 494090144.0000 - val_mae: 22035.5156\n",
            "Epoch 99/500\n",
            "50/50 [==============================] - 0s 827us/step - loss: 20630.5500 - mse: 445847456.0000 - mae: 20630.5508 - val_loss: 21954.7676 - val_mse: 490463136.0000 - val_mae: 21954.7695\n",
            "Epoch 100/500\n",
            "50/50 [==============================] - 0s 734us/step - loss: 20474.9336 - mse: 439006592.0000 - mae: 20474.9336 - val_loss: 21872.2065 - val_mse: 486763840.0000 - val_mae: 21872.2070\n",
            "Epoch 101/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 20468.5680 - mse: 441199072.0000 - mae: 20468.5684 - val_loss: 21790.4106 - val_mse: 483117248.0000 - val_mae: 21790.4102\n",
            "Epoch 102/500\n",
            "50/50 [==============================] - 0s 717us/step - loss: 20435.0418 - mse: 435285536.0000 - mae: 20435.0430 - val_loss: 21709.0718 - val_mse: 479507136.0000 - val_mae: 21709.0723\n",
            "Epoch 103/500\n",
            "50/50 [==============================] - 0s 738us/step - loss: 20297.1320 - mse: 431321344.0000 - mae: 20297.1328 - val_loss: 21624.7407 - val_mse: 475783168.0000 - val_mae: 21624.7402\n",
            "Epoch 104/500\n",
            "50/50 [==============================] - 0s 736us/step - loss: 20253.7148 - mse: 430556320.0000 - mae: 20253.7148 - val_loss: 21540.2539 - val_mse: 472057152.0000 - val_mae: 21540.2539\n",
            "Epoch 105/500\n",
            "50/50 [==============================] - 0s 670us/step - loss: 20451.0824 - mse: 437905088.0000 - mae: 20451.0820 - val_loss: 21460.7729 - val_mse: 468567872.0000 - val_mae: 21460.7715\n",
            "Epoch 106/500\n",
            "50/50 [==============================] - 0s 756us/step - loss: 20018.0078 - mse: 419203616.0000 - mae: 20018.0078 - val_loss: 21372.8115 - val_mse: 464721408.0000 - val_mae: 21372.8105\n",
            "Epoch 107/500\n",
            "50/50 [==============================] - 0s 790us/step - loss: 19906.5613 - mse: 417263040.0000 - mae: 19906.5605 - val_loss: 21283.4692 - val_mse: 460833120.0000 - val_mae: 21283.4707\n",
            "Epoch 108/500\n",
            "50/50 [==============================] - 0s 751us/step - loss: 19793.4219 - mse: 414503040.0000 - mae: 19793.4219 - val_loss: 21191.9712 - val_mse: 456873728.0000 - val_mae: 21191.9707\n",
            "Epoch 109/500\n",
            "50/50 [==============================] - 0s 831us/step - loss: 19464.9125 - mse: 398540896.0000 - mae: 19464.9141 - val_loss: 21091.3169 - val_mse: 452523680.0000 - val_mae: 21091.3164\n",
            "Epoch 110/500\n",
            "50/50 [==============================] - 0s 937us/step - loss: 19378.7203 - mse: 395581184.0000 - mae: 19378.7207 - val_loss: 20989.8291 - val_mse: 448161120.0000 - val_mae: 20989.8301\n",
            "Epoch 111/500\n",
            "50/50 [==============================] - 0s 794us/step - loss: 19396.1555 - mse: 399355168.0000 - mae: 19396.1562 - val_loss: 20889.2764 - val_mse: 443868928.0000 - val_mae: 20889.2773\n",
            "Epoch 112/500\n",
            "50/50 [==============================] - 0s 710us/step - loss: 19477.8293 - mse: 398733920.0000 - mae: 19477.8301 - val_loss: 20791.5522 - val_mse: 439716096.0000 - val_mae: 20791.5527\n",
            "Epoch 113/500\n",
            "50/50 [==============================] - 0s 757us/step - loss: 19416.3031 - mse: 399502304.0000 - mae: 19416.3027 - val_loss: 20692.9429 - val_mse: 435547968.0000 - val_mae: 20692.9414\n",
            "Epoch 114/500\n",
            "50/50 [==============================] - 0s 653us/step - loss: 19252.1672 - mse: 391314848.0000 - mae: 19252.1680 - val_loss: 20593.3994 - val_mse: 431356736.0000 - val_mae: 20593.4004\n",
            "Epoch 115/500\n",
            "50/50 [==============================] - 0s 794us/step - loss: 18510.5461 - mse: 367200160.0000 - mae: 18510.5449 - val_loss: 20479.8687 - val_mse: 426605760.0000 - val_mae: 20479.8691\n",
            "Epoch 116/500\n",
            "50/50 [==============================] - 0s 892us/step - loss: 18693.2820 - mse: 373230880.0000 - mae: 18693.2812 - val_loss: 20371.6445 - val_mse: 422101408.0000 - val_mae: 20371.6445\n",
            "Epoch 117/500\n",
            "50/50 [==============================] - 0s 880us/step - loss: 19385.9887 - mse: 398368672.0000 - mae: 19385.9883 - val_loss: 20278.2856 - val_mse: 418244288.0000 - val_mae: 20278.2871\n",
            "Epoch 118/500\n",
            "50/50 [==============================] - 0s 791us/step - loss: 18746.8848 - mse: 374200064.0000 - mae: 18746.8828 - val_loss: 20172.0225 - val_mse: 413868480.0000 - val_mae: 20172.0215\n",
            "Epoch 119/500\n",
            "50/50 [==============================] - 0s 691us/step - loss: 18657.4266 - mse: 370677856.0000 - mae: 18657.4277 - val_loss: 20064.4668 - val_mse: 409469088.0000 - val_mae: 20064.4648\n",
            "Epoch 120/500\n",
            "50/50 [==============================] - 0s 639us/step - loss: 18939.2045 - mse: 382899904.0000 - mae: 18939.2051 - val_loss: 19963.2422 - val_mse: 405346464.0000 - val_mae: 19963.2422\n",
            "Epoch 121/500\n",
            "50/50 [==============================] - 0s 778us/step - loss: 18215.5086 - mse: 354772128.0000 - mae: 18215.5078 - val_loss: 19850.0205 - val_mse: 400761536.0000 - val_mae: 19850.0215\n",
            "Epoch 122/500\n",
            "50/50 [==============================] - 0s 794us/step - loss: 18259.3734 - mse: 356266848.0000 - mae: 18259.3730 - val_loss: 19739.3633 - val_mse: 396314720.0000 - val_mae: 19739.3633\n",
            "Epoch 123/500\n",
            "50/50 [==============================] - 0s 773us/step - loss: 18445.7387 - mse: 363265408.0000 - mae: 18445.7383 - val_loss: 19631.9321 - val_mse: 392016064.0000 - val_mae: 19631.9336\n",
            "Epoch 124/500\n",
            "50/50 [==============================] - 0s 762us/step - loss: 18305.7680 - mse: 355103584.0000 - mae: 18305.7676 - val_loss: 19522.9810 - val_mse: 387680928.0000 - val_mae: 19522.9805\n",
            "Epoch 125/500\n",
            "50/50 [==============================] - 0s 729us/step - loss: 17959.4562 - mse: 345214496.0000 - mae: 17959.4570 - val_loss: 19408.5371 - val_mse: 383154080.0000 - val_mae: 19408.5371\n",
            "Epoch 126/500\n",
            "50/50 [==============================] - 0s 884us/step - loss: 17577.9207 - mse: 333198656.0000 - mae: 17577.9219 - val_loss: 19288.5571 - val_mse: 378444576.0000 - val_mae: 19288.5586\n",
            "Epoch 127/500\n",
            "50/50 [==============================] - 0s 777us/step - loss: 18081.6734 - mse: 351248256.0000 - mae: 18081.6719 - val_loss: 19177.9810 - val_mse: 374135392.0000 - val_mae: 19177.9805\n",
            "Epoch 128/500\n",
            "50/50 [==============================] - 0s 718us/step - loss: 17485.7277 - mse: 328176320.0000 - mae: 17485.7266 - val_loss: 19058.7266 - val_mse: 369510816.0000 - val_mae: 19058.7266\n",
            "Epoch 129/500\n",
            "50/50 [==============================] - 0s 945us/step - loss: 17806.4053 - mse: 340716384.0000 - mae: 17806.4043 - val_loss: 18944.9170 - val_mse: 365126496.0000 - val_mae: 18944.9180\n",
            "Epoch 130/500\n",
            "50/50 [==============================] - 0s 786us/step - loss: 17852.9090 - mse: 343181472.0000 - mae: 17852.9082 - val_loss: 18832.6953 - val_mse: 360827328.0000 - val_mae: 18832.6973\n",
            "Epoch 131/500\n",
            "50/50 [==============================] - 0s 691us/step - loss: 17295.1730 - mse: 329038240.0000 - mae: 17295.1719 - val_loss: 18711.9763 - val_mse: 356235008.0000 - val_mae: 18711.9746\n",
            "Epoch 132/500\n",
            "50/50 [==============================] - 0s 906us/step - loss: 17446.1270 - mse: 328683808.0000 - mae: 17446.1270 - val_loss: 18594.9551 - val_mse: 351807808.0000 - val_mae: 18594.9570\n",
            "Epoch 133/500\n",
            "50/50 [==============================] - 0s 732us/step - loss: 17192.1895 - mse: 320634016.0000 - mae: 17192.1895 - val_loss: 18473.6582 - val_mse: 347258816.0000 - val_mae: 18473.6602\n",
            "Epoch 134/500\n",
            "50/50 [==============================] - 0s 781us/step - loss: 16624.1311 - mse: 298775232.0000 - mae: 16624.1309 - val_loss: 18344.2632 - val_mse: 342441536.0000 - val_mae: 18344.2617\n",
            "Epoch 135/500\n",
            "50/50 [==============================] - 0s 782us/step - loss: 16449.3578 - mse: 301268256.0000 - mae: 16449.3574 - val_loss: 18222.0007 - val_mse: 337919616.0000 - val_mae: 18222.0000\n",
            "Epoch 136/500\n",
            "50/50 [==============================] - 0s 795us/step - loss: 16669.0414 - mse: 299829888.0000 - mae: 16669.0410 - val_loss: 18096.6040 - val_mse: 333317472.0000 - val_mae: 18096.6055\n",
            "Epoch 137/500\n",
            "50/50 [==============================] - 0s 853us/step - loss: 16742.4443 - mse: 301532320.0000 - mae: 16742.4434 - val_loss: 17972.2500 - val_mse: 328785024.0000 - val_mae: 17972.2500\n",
            "Epoch 138/500\n",
            "50/50 [==============================] - 0s 805us/step - loss: 16593.6080 - mse: 295087008.0000 - mae: 16593.6094 - val_loss: 17847.0188 - val_mse: 324258784.0000 - val_mae: 17847.0195\n",
            "Epoch 139/500\n",
            "50/50 [==============================] - 0s 872us/step - loss: 16639.0453 - mse: 303737920.0000 - mae: 16639.0469 - val_loss: 17730.6460 - val_mse: 320078016.0000 - val_mae: 17730.6465\n",
            "Epoch 140/500\n",
            "50/50 [==============================] - 0s 826us/step - loss: 16211.7154 - mse: 285341088.0000 - mae: 16211.7148 - val_loss: 17608.1274 - val_mse: 315710944.0000 - val_mae: 17608.1270\n",
            "Epoch 141/500\n",
            "50/50 [==============================] - 0s 842us/step - loss: 16080.7068 - mse: 284869664.0000 - mae: 16080.7061 - val_loss: 17476.9375 - val_mse: 311066432.0000 - val_mae: 17476.9375\n",
            "Epoch 142/500\n",
            "50/50 [==============================] - 0s 744us/step - loss: 15786.5953 - mse: 274641792.0000 - mae: 15786.5947 - val_loss: 17341.7634 - val_mse: 306321344.0000 - val_mae: 17341.7617\n",
            "Epoch 143/500\n",
            "50/50 [==============================] - 0s 768us/step - loss: 16243.5545 - mse: 289062528.0000 - mae: 16243.5547 - val_loss: 17215.0435 - val_mse: 301909472.0000 - val_mae: 17215.0430\n",
            "Epoch 144/500\n",
            "50/50 [==============================] - 0s 727us/step - loss: 15833.5836 - mse: 276722272.0000 - mae: 15833.5820 - val_loss: 17090.9685 - val_mse: 297622592.0000 - val_mae: 17090.9688\n",
            "Epoch 145/500\n",
            "50/50 [==============================] - 0s 820us/step - loss: 15970.5084 - mse: 285051904.0000 - mae: 15970.5088 - val_loss: 16960.8186 - val_mse: 293159744.0000 - val_mae: 16960.8184\n",
            "Epoch 146/500\n",
            "50/50 [==============================] - 0s 873us/step - loss: 15286.2955 - mse: 264017568.0000 - mae: 15286.2959 - val_loss: 16829.4604 - val_mse: 288695616.0000 - val_mae: 16829.4609\n",
            "Epoch 147/500\n",
            "50/50 [==============================] - 0s 856us/step - loss: 15829.2227 - mse: 277792032.0000 - mae: 15829.2227 - val_loss: 16708.9490 - val_mse: 284630720.0000 - val_mae: 16708.9492\n",
            "Epoch 148/500\n",
            "50/50 [==============================] - 0s 789us/step - loss: 15096.1092 - mse: 253240832.0000 - mae: 15096.1104 - val_loss: 16569.0010 - val_mse: 279951264.0000 - val_mae: 16569.0000\n",
            "Epoch 149/500\n",
            "50/50 [==============================] - 0s 724us/step - loss: 14644.6693 - mse: 244858288.0000 - mae: 14644.6699 - val_loss: 16430.6648 - val_mse: 275366912.0000 - val_mae: 16430.6660\n",
            "Epoch 150/500\n",
            "50/50 [==============================] - 0s 712us/step - loss: 14989.6279 - mse: 250903984.0000 - mae: 14989.6279 - val_loss: 16305.8167 - val_mse: 271267904.0000 - val_mae: 16305.8154\n",
            "Epoch 151/500\n",
            "50/50 [==============================] - 0s 788us/step - loss: 14372.9906 - mse: 233430912.0000 - mae: 14372.9902 - val_loss: 16158.0051 - val_mse: 266455904.0000 - val_mae: 16158.0059\n",
            "Epoch 152/500\n",
            "50/50 [==============================] - 0s 849us/step - loss: 15045.8793 - mse: 254450032.0000 - mae: 15045.8799 - val_loss: 16028.6531 - val_mse: 262284032.0000 - val_mae: 16028.6533\n",
            "Epoch 153/500\n",
            "50/50 [==============================] - 0s 857us/step - loss: 14496.3086 - mse: 237090208.0000 - mae: 14496.3086 - val_loss: 15917.2651 - val_mse: 258719488.0000 - val_mae: 15917.2656\n",
            "Epoch 154/500\n",
            "50/50 [==============================] - 0s 725us/step - loss: 15200.5574 - mse: 255362400.0000 - mae: 15200.5576 - val_loss: 15783.4944 - val_mse: 254472240.0000 - val_mae: 15783.4941\n",
            "Epoch 155/500\n",
            "50/50 [==============================] - 0s 871us/step - loss: 14306.9426 - mse: 230677280.0000 - mae: 14306.9424 - val_loss: 15662.6497 - val_mse: 250669232.0000 - val_mae: 15662.6504\n",
            "Epoch 156/500\n",
            "50/50 [==============================] - 0s 701us/step - loss: 14217.8092 - mse: 228239872.0000 - mae: 14217.8086 - val_loss: 15542.1069 - val_mse: 246906416.0000 - val_mae: 15542.1064\n",
            "Epoch 157/500\n",
            "50/50 [==============================] - 0s 742us/step - loss: 13617.9746 - mse: 214947024.0000 - mae: 13617.9746 - val_loss: 15396.7058 - val_mse: 242407984.0000 - val_mae: 15396.7061\n",
            "Epoch 158/500\n",
            "50/50 [==============================] - 0s 718us/step - loss: 14156.5162 - mse: 225650688.0000 - mae: 14156.5166 - val_loss: 15266.6743 - val_mse: 238422528.0000 - val_mae: 15266.6748\n",
            "Epoch 159/500\n",
            "50/50 [==============================] - 0s 897us/step - loss: 14516.1396 - mse: 235877984.0000 - mae: 14516.1396 - val_loss: 15134.5596 - val_mse: 234410080.0000 - val_mae: 15134.5596\n",
            "Epoch 160/500\n",
            "50/50 [==============================] - 0s 737us/step - loss: 14352.5568 - mse: 228359296.0000 - mae: 14352.5576 - val_loss: 15006.8730 - val_mse: 230567504.0000 - val_mae: 15006.8730\n",
            "Epoch 161/500\n",
            "50/50 [==============================] - 0s 741us/step - loss: 13408.5057 - mse: 203720864.0000 - mae: 13408.5049 - val_loss: 14868.2917 - val_mse: 226437680.0000 - val_mae: 14868.2920\n",
            "Epoch 162/500\n",
            "50/50 [==============================] - 0s 706us/step - loss: 13193.3984 - mse: 202304512.0000 - mae: 13193.3984 - val_loss: 14746.2502 - val_mse: 222832128.0000 - val_mae: 14746.2500\n",
            "Epoch 163/500\n",
            "50/50 [==============================] - 0s 736us/step - loss: 12904.6379 - mse: 199094176.0000 - mae: 12904.6377 - val_loss: 14593.9688 - val_mse: 218380080.0000 - val_mae: 14593.9688\n",
            "Epoch 164/500\n",
            "50/50 [==============================] - 0s 701us/step - loss: 13067.0051 - mse: 202194656.0000 - mae: 13067.0049 - val_loss: 14453.5764 - val_mse: 214317792.0000 - val_mae: 14453.5762\n",
            "Epoch 165/500\n",
            "50/50 [==============================] - 0s 853us/step - loss: 13195.6000 - mse: 198296672.0000 - mae: 13195.5996 - val_loss: 14306.5447 - val_mse: 210111152.0000 - val_mae: 14306.5439\n",
            "Epoch 166/500\n",
            "50/50 [==============================] - 0s 941us/step - loss: 13639.8986 - mse: 211404928.0000 - mae: 13639.8984 - val_loss: 14174.5935 - val_mse: 206373376.0000 - val_mae: 14174.5938\n",
            "Epoch 167/500\n",
            "50/50 [==============================] - 0s 768us/step - loss: 13224.3369 - mse: 200524480.0000 - mae: 13224.3379 - val_loss: 14023.5256 - val_mse: 202142000.0000 - val_mae: 14023.5254\n",
            "Epoch 168/500\n",
            "50/50 [==============================] - 0s 686us/step - loss: 13374.1436 - mse: 206137856.0000 - mae: 13374.1426 - val_loss: 13881.6926 - val_mse: 198209696.0000 - val_mae: 13881.6934\n",
            "Epoch 169/500\n",
            "50/50 [==============================] - 0s 669us/step - loss: 12914.8965 - mse: 194327552.0000 - mae: 12914.8965 - val_loss: 13742.6934 - val_mse: 194403936.0000 - val_mae: 13742.6934\n",
            "Epoch 170/500\n",
            "50/50 [==============================] - 0s 742us/step - loss: 12791.3475 - mse: 187691744.0000 - mae: 12791.3477 - val_loss: 13596.2849 - val_mse: 190434160.0000 - val_mae: 13596.2842\n",
            "Epoch 171/500\n",
            "50/50 [==============================] - 0s 908us/step - loss: 12174.5350 - mse: 179411504.0000 - mae: 12174.5352 - val_loss: 13451.6167 - val_mse: 186558448.0000 - val_mae: 13451.6172\n",
            "Epoch 172/500\n",
            "50/50 [==============================] - 0s 978us/step - loss: 12485.6758 - mse: 181569296.0000 - mae: 12485.6758 - val_loss: 13316.5652 - val_mse: 182977632.0000 - val_mae: 13316.5654\n",
            "Epoch 173/500\n",
            "50/50 [==============================] - 0s 787us/step - loss: 12682.6561 - mse: 193598992.0000 - mae: 12682.6553 - val_loss: 13203.2913 - val_mse: 179998000.0000 - val_mae: 13203.2910\n",
            "Epoch 174/500\n",
            "50/50 [==============================] - 0s 771us/step - loss: 12647.9469 - mse: 192764192.0000 - mae: 12647.9463 - val_loss: 13079.4131 - val_mse: 176769584.0000 - val_mae: 13079.4121\n",
            "Epoch 175/500\n",
            "50/50 [==============================] - 0s 725us/step - loss: 11905.4715 - mse: 165877808.0000 - mae: 11905.4717 - val_loss: 12936.9321 - val_mse: 173107376.0000 - val_mae: 12936.9316\n",
            "Epoch 176/500\n",
            "50/50 [==============================] - 0s 760us/step - loss: 12352.8598 - mse: 179580176.0000 - mae: 12352.8604 - val_loss: 12802.5168 - val_mse: 169689024.0000 - val_mae: 12802.5176\n",
            "Epoch 177/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 11193.9066 - mse: 149536608.0000 - mae: 11193.9062 - val_loss: 12653.0300 - val_mse: 165929312.0000 - val_mae: 12653.0303\n",
            "Epoch 178/500\n",
            "50/50 [==============================] - 0s 691us/step - loss: 11794.1918 - mse: 168899184.0000 - mae: 11794.1914 - val_loss: 12504.7883 - val_mse: 162254256.0000 - val_mae: 12504.7881\n",
            "Epoch 179/500\n",
            "50/50 [==============================] - 0s 876us/step - loss: 10666.6268 - mse: 140506704.0000 - mae: 10666.6279 - val_loss: 12363.9272 - val_mse: 158798960.0000 - val_mae: 12363.9277\n",
            "Epoch 180/500\n",
            "50/50 [==============================] - 0s 836us/step - loss: 11690.0816 - mse: 168720256.0000 - mae: 11690.0811 - val_loss: 12226.8833 - val_mse: 155474768.0000 - val_mae: 12226.8828\n",
            "Epoch 181/500\n",
            "50/50 [==============================] - 0s 915us/step - loss: 11462.6328 - mse: 155103856.0000 - mae: 11462.6328 - val_loss: 12095.9600 - val_mse: 152324704.0000 - val_mae: 12095.9609\n",
            "Epoch 182/500\n",
            "50/50 [==============================] - 0s 794us/step - loss: 11112.2676 - mse: 150878448.0000 - mae: 11112.2676 - val_loss: 11961.4106 - val_mse: 149121408.0000 - val_mae: 11961.4102\n",
            "Epoch 183/500\n",
            "50/50 [==============================] - 0s 885us/step - loss: 10407.6795 - mse: 134115736.0000 - mae: 10407.6797 - val_loss: 11806.1323 - val_mse: 145484928.0000 - val_mae: 11806.1328\n",
            "Epoch 184/500\n",
            "50/50 [==============================] - 0s 847us/step - loss: 11144.0205 - mse: 153431472.0000 - mae: 11144.0195 - val_loss: 11644.2170 - val_mse: 141771584.0000 - val_mae: 11644.2168\n",
            "Epoch 185/500\n",
            "50/50 [==============================] - 0s 756us/step - loss: 10695.1662 - mse: 140769056.0000 - mae: 10695.1660 - val_loss: 11516.5212 - val_mse: 138847456.0000 - val_mae: 11516.5205\n",
            "Epoch 186/500\n",
            "50/50 [==============================] - 0s 725us/step - loss: 11083.7789 - mse: 150245296.0000 - mae: 11083.7783 - val_loss: 11384.4790 - val_mse: 135872192.0000 - val_mae: 11384.4785\n",
            "Epoch 187/500\n",
            "50/50 [==============================] - 0s 723us/step - loss: 11351.9242 - mse: 153147616.0000 - mae: 11351.9238 - val_loss: 11264.5562 - val_mse: 133191504.0000 - val_mae: 11264.5566\n",
            "Epoch 188/500\n",
            "50/50 [==============================] - 0s 660us/step - loss: 10334.0826 - mse: 133860600.0000 - mae: 10334.0820 - val_loss: 11118.2017 - val_mse: 129982056.0000 - val_mae: 11118.2021\n",
            "Epoch 189/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 10115.1297 - mse: 128935856.0000 - mae: 10115.1289 - val_loss: 10962.0762 - val_mse: 126612176.0000 - val_mae: 10962.0762\n",
            "Epoch 190/500\n",
            "50/50 [==============================] - 0s 855us/step - loss: 10793.6676 - mse: 146479664.0000 - mae: 10793.6680 - val_loss: 10837.2759 - val_mse: 123926656.0000 - val_mae: 10837.2764\n",
            "Epoch 191/500\n",
            "50/50 [==============================] - 0s 739us/step - loss: 9005.9206 - mse: 108086496.0000 - mae: 9005.9209 - val_loss: 10699.6165 - val_mse: 121007872.0000 - val_mae: 10699.6162\n",
            "Epoch 192/500\n",
            "50/50 [==============================] - 0s 760us/step - loss: 9466.5885 - mse: 116182664.0000 - mae: 9466.5889 - val_loss: 10544.9644 - val_mse: 117802000.0000 - val_mae: 10544.9639\n",
            "Epoch 193/500\n",
            "50/50 [==============================] - 0s 792us/step - loss: 10746.9691 - mse: 140699552.0000 - mae: 10746.9697 - val_loss: 10427.5872 - val_mse: 115371952.0000 - val_mae: 10427.5869\n",
            "Epoch 194/500\n",
            "50/50 [==============================] - 0s 806us/step - loss: 10777.6590 - mse: 143496288.0000 - mae: 10777.6602 - val_loss: 10303.4050 - val_mse: 112848856.0000 - val_mae: 10303.4053\n",
            "Epoch 195/500\n",
            "50/50 [==============================] - 0s 887us/step - loss: 9740.0635 - mse: 121832080.0000 - mae: 9740.0635 - val_loss: 10172.5581 - val_mse: 110218920.0000 - val_mae: 10172.5576\n",
            "Epoch 196/500\n",
            "50/50 [==============================] - 0s 942us/step - loss: 9511.8199 - mse: 120206496.0000 - mae: 9511.8203 - val_loss: 10068.2495 - val_mse: 108116776.0000 - val_mae: 10068.2490\n",
            "Epoch 197/500\n",
            "50/50 [==============================] - 0s 813us/step - loss: 9497.6534 - mse: 115073544.0000 - mae: 9497.6533 - val_loss: 9944.3325 - val_mse: 105673200.0000 - val_mae: 9944.3330\n",
            "Epoch 198/500\n",
            "50/50 [==============================] - 0s 931us/step - loss: 9427.2968 - mse: 110870352.0000 - mae: 9427.2979 - val_loss: 9839.2021 - val_mse: 103605264.0000 - val_mae: 9839.2021\n",
            "Epoch 199/500\n",
            "50/50 [==============================] - 0s 896us/step - loss: 9208.3479 - mse: 111179912.0000 - mae: 9208.3477 - val_loss: 9708.3203 - val_mse: 101085112.0000 - val_mae: 9708.3203\n",
            "Epoch 200/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 9114.3466 - mse: 115663536.0000 - mae: 9114.3467 - val_loss: 9593.3195 - val_mse: 98879752.0000 - val_mae: 9593.3184\n",
            "Epoch 201/500\n",
            "50/50 [==============================] - 0s 926us/step - loss: 10100.5854 - mse: 136901120.0000 - mae: 10100.5850 - val_loss: 9533.0020 - val_mse: 97679528.0000 - val_mae: 9533.0020\n",
            "Epoch 202/500\n",
            "50/50 [==============================] - 0s 876us/step - loss: 9553.8941 - mse: 116875696.0000 - mae: 9553.8945 - val_loss: 9453.6560 - val_mse: 96135408.0000 - val_mae: 9453.6562\n",
            "Epoch 203/500\n",
            "50/50 [==============================] - 0s 822us/step - loss: 9418.5923 - mse: 116597480.0000 - mae: 9418.5928 - val_loss: 9337.1758 - val_mse: 93975472.0000 - val_mae: 9337.1768\n",
            "Epoch 204/500\n",
            "50/50 [==============================] - 0s 741us/step - loss: 8755.9245 - mse: 101130712.0000 - mae: 8755.9238 - val_loss: 9238.7993 - val_mse: 92142728.0000 - val_mae: 9238.7988\n",
            "Epoch 205/500\n",
            "50/50 [==============================] - 0s 963us/step - loss: 8850.0465 - mse: 106674664.0000 - mae: 8850.0459 - val_loss: 9142.6558 - val_mse: 90374968.0000 - val_mae: 9142.6562\n",
            "Epoch 206/500\n",
            "50/50 [==============================] - 0s 764us/step - loss: 8852.8398 - mse: 101284608.0000 - mae: 8852.8398 - val_loss: 9012.1335 - val_mse: 88032256.0000 - val_mae: 9012.1338\n",
            "Epoch 207/500\n",
            "50/50 [==============================] - 0s 799us/step - loss: 9511.7038 - mse: 109375552.0000 - mae: 9511.7041 - val_loss: 8895.3604 - val_mse: 85970000.0000 - val_mae: 8895.3604\n",
            "Epoch 208/500\n",
            "50/50 [==============================] - 0s 687us/step - loss: 8835.9742 - mse: 109334648.0000 - mae: 8835.9746 - val_loss: 8799.0430 - val_mse: 84267320.0000 - val_mae: 8799.0430\n",
            "Epoch 209/500\n",
            "50/50 [==============================] - 0s 685us/step - loss: 9062.0804 - mse: 106161008.0000 - mae: 9062.0811 - val_loss: 8715.4567 - val_mse: 82772544.0000 - val_mae: 8715.4561\n",
            "Epoch 210/500\n",
            "50/50 [==============================] - 0s 792us/step - loss: 9329.7713 - mse: 112880432.0000 - mae: 9329.7715 - val_loss: 8605.8506 - val_mse: 80897216.0000 - val_mae: 8605.8496\n",
            "Epoch 211/500\n",
            "50/50 [==============================] - 0s 814us/step - loss: 7783.1215 - mse: 80315976.0000 - mae: 7783.1211 - val_loss: 8507.6094 - val_mse: 79210432.0000 - val_mae: 8507.6094\n",
            "Epoch 212/500\n",
            "50/50 [==============================] - 0s 829us/step - loss: 8216.1440 - mse: 90673568.0000 - mae: 8216.1436 - val_loss: 8373.5562 - val_mse: 76993552.0000 - val_mae: 8373.5566\n",
            "Epoch 213/500\n",
            "50/50 [==============================] - 0s 769us/step - loss: 8461.0064 - mse: 90861792.0000 - mae: 8461.0059 - val_loss: 8302.0442 - val_mse: 75763472.0000 - val_mae: 8302.0439\n",
            "Epoch 214/500\n",
            "50/50 [==============================] - 0s 872us/step - loss: 8723.3883 - mse: 102645784.0000 - mae: 8723.3877 - val_loss: 8209.6140 - val_mse: 74237592.0000 - val_mae: 8209.6143\n",
            "Epoch 215/500\n",
            "50/50 [==============================] - 0s 786us/step - loss: 8781.0864 - mse: 99453888.0000 - mae: 8781.0859 - val_loss: 8078.9933 - val_mse: 72142552.0000 - val_mae: 8078.9937\n",
            "Epoch 216/500\n",
            "50/50 [==============================] - 0s 774us/step - loss: 8700.4390 - mse: 105146088.0000 - mae: 8700.4385 - val_loss: 7990.1367 - val_mse: 70700976.0000 - val_mae: 7990.1377\n",
            "Epoch 217/500\n",
            "50/50 [==============================] - 0s 798us/step - loss: 8270.9017 - mse: 97926432.0000 - mae: 8270.9014 - val_loss: 7898.1638 - val_mse: 69213240.0000 - val_mae: 7898.1641\n",
            "Epoch 218/500\n",
            "50/50 [==============================] - 0s 760us/step - loss: 8825.2564 - mse: 102638104.0000 - mae: 8825.2568 - val_loss: 7798.5571 - val_mse: 67650368.0000 - val_mae: 7798.5571\n",
            "Epoch 219/500\n",
            "50/50 [==============================] - 0s 875us/step - loss: 7566.6938 - mse: 73704328.0000 - mae: 7566.6938 - val_loss: 7700.9811 - val_mse: 66106356.0000 - val_mae: 7700.9814\n",
            "Epoch 220/500\n",
            "50/50 [==============================] - 0s 800us/step - loss: 7902.3363 - mse: 86217440.0000 - mae: 7902.3364 - val_loss: 7596.2188 - val_mse: 64484576.0000 - val_mae: 7596.2188\n",
            "Epoch 221/500\n",
            "50/50 [==============================] - 0s 840us/step - loss: 7117.3742 - mse: 77729744.0000 - mae: 7117.3745 - val_loss: 7460.6342 - val_mse: 62491432.0000 - val_mae: 7460.6343\n",
            "Epoch 222/500\n",
            "50/50 [==============================] - 0s 808us/step - loss: 8087.6702 - mse: 98323744.0000 - mae: 8087.6699 - val_loss: 7410.4548 - val_mse: 61684328.0000 - val_mae: 7410.4546\n",
            "Epoch 223/500\n",
            "50/50 [==============================] - 0s 881us/step - loss: 8733.4599 - mse: 99545904.0000 - mae: 8733.4600 - val_loss: 7307.3656 - val_mse: 60176600.0000 - val_mae: 7307.3657\n",
            "Epoch 224/500\n",
            "50/50 [==============================] - 0s 987us/step - loss: 7628.0396 - mse: 80588904.0000 - mae: 7628.0386 - val_loss: 7201.3920 - val_mse: 58642656.0000 - val_mae: 7201.3921\n",
            "Epoch 225/500\n",
            "50/50 [==============================] - 0s 994us/step - loss: 8431.1177 - mse: 99219552.0000 - mae: 8431.1172 - val_loss: 7097.8463 - val_mse: 57183252.0000 - val_mae: 7097.8467\n",
            "Epoch 226/500\n",
            "50/50 [==============================] - 0s 884us/step - loss: 7442.4870 - mse: 76967640.0000 - mae: 7442.4868 - val_loss: 6997.2970 - val_mse: 55773800.0000 - val_mae: 6997.2969\n",
            "Epoch 227/500\n",
            "50/50 [==============================] - 0s 933us/step - loss: 7637.8104 - mse: 81741920.0000 - mae: 7637.8101 - val_loss: 6893.3951 - val_mse: 54314888.0000 - val_mae: 6893.3955\n",
            "Epoch 228/500\n",
            "50/50 [==============================] - 0s 841us/step - loss: 7477.9445 - mse: 69861384.0000 - mae: 7477.9448 - val_loss: 6823.6943 - val_mse: 53265496.0000 - val_mae: 6823.6943\n",
            "Epoch 229/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 7211.3568 - mse: 75696240.0000 - mae: 7211.3569 - val_loss: 6708.1479 - val_mse: 51650412.0000 - val_mae: 6708.1484\n",
            "Epoch 230/500\n",
            "50/50 [==============================] - 0s 820us/step - loss: 8717.1533 - mse: 103421192.0000 - mae: 8717.1523 - val_loss: 6655.1134 - val_mse: 50850184.0000 - val_mae: 6655.1133\n",
            "Epoch 231/500\n",
            "50/50 [==============================] - 0s 709us/step - loss: 7519.9033 - mse: 77313600.0000 - mae: 7519.9033 - val_loss: 6584.3430 - val_mse: 49831360.0000 - val_mae: 6584.3428\n",
            "Epoch 232/500\n",
            "50/50 [==============================] - 0s 772us/step - loss: 7336.0904 - mse: 73222080.0000 - mae: 7336.0898 - val_loss: 6517.6658 - val_mse: 48870452.0000 - val_mae: 6517.6655\n",
            "Epoch 233/500\n",
            "50/50 [==============================] - 0s 872us/step - loss: 8212.6595 - mse: 91061176.0000 - mae: 8212.6602 - val_loss: 6407.0154 - val_mse: 47316256.0000 - val_mae: 6407.0156\n",
            "Epoch 234/500\n",
            "50/50 [==============================] - 0s 742us/step - loss: 7758.2613 - mse: 81526040.0000 - mae: 7758.2612 - val_loss: 6343.1469 - val_mse: 46401528.0000 - val_mae: 6343.1470\n",
            "Epoch 235/500\n",
            "50/50 [==============================] - 0s 881us/step - loss: 7064.3430 - mse: 71150816.0000 - mae: 7064.3438 - val_loss: 6293.9764 - val_mse: 45655176.0000 - val_mae: 6293.9761\n",
            "Epoch 236/500\n",
            "50/50 [==============================] - 0s 874us/step - loss: 7191.4936 - mse: 80387224.0000 - mae: 7191.4937 - val_loss: 6200.3687 - val_mse: 44304200.0000 - val_mae: 6200.3687\n",
            "Epoch 237/500\n",
            "50/50 [==============================] - 0s 876us/step - loss: 7069.3292 - mse: 66429220.0000 - mae: 7069.3296 - val_loss: 6129.4526 - val_mse: 43261452.0000 - val_mae: 6129.4526\n",
            "Epoch 238/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 7794.3557 - mse: 89923792.0000 - mae: 7794.3564 - val_loss: 6110.9539 - val_mse: 42935948.0000 - val_mae: 6110.9541\n",
            "Epoch 239/500\n",
            "50/50 [==============================] - 0s 821us/step - loss: 7526.6999 - mse: 81030664.0000 - mae: 7526.7002 - val_loss: 6064.8337 - val_mse: 42266312.0000 - val_mae: 6064.8335\n",
            "Epoch 240/500\n",
            "50/50 [==============================] - 0s 922us/step - loss: 7369.9339 - mse: 73190656.0000 - mae: 7369.9331 - val_loss: 5995.4227 - val_mse: 41274516.0000 - val_mae: 5995.4229\n",
            "Epoch 241/500\n",
            "50/50 [==============================] - 0s 835us/step - loss: 7420.6446 - mse: 83992864.0000 - mae: 7420.6450 - val_loss: 5962.9159 - val_mse: 40781868.0000 - val_mae: 5962.9155\n",
            "Epoch 242/500\n",
            "50/50 [==============================] - 0s 844us/step - loss: 7355.2174 - mse: 82470192.0000 - mae: 7355.2173 - val_loss: 5928.1996 - val_mse: 40294792.0000 - val_mae: 5928.1992\n",
            "Epoch 243/500\n",
            "50/50 [==============================] - 0s 859us/step - loss: 7430.4178 - mse: 79628640.0000 - mae: 7430.4180 - val_loss: 5881.5654 - val_mse: 39634860.0000 - val_mae: 5881.5654\n",
            "Epoch 244/500\n",
            "50/50 [==============================] - 0s 884us/step - loss: 6514.5142 - mse: 65209200.0000 - mae: 6514.5137 - val_loss: 5861.3416 - val_mse: 39325832.0000 - val_mae: 5861.3413\n",
            "Epoch 245/500\n",
            "50/50 [==============================] - 0s 806us/step - loss: 6917.9956 - mse: 64606056.0000 - mae: 6917.9956 - val_loss: 5805.7782 - val_mse: 38562872.0000 - val_mae: 5805.7783\n",
            "Epoch 246/500\n",
            "50/50 [==============================] - 0s 863us/step - loss: 6483.0803 - mse: 59756256.0000 - mae: 6483.0806 - val_loss: 5729.6558 - val_mse: 37543220.0000 - val_mae: 5729.6553\n",
            "Epoch 247/500\n",
            "50/50 [==============================] - 0s 846us/step - loss: 6534.7080 - mse: 65672556.0000 - mae: 6534.7080 - val_loss: 5683.6857 - val_mse: 36915208.0000 - val_mae: 5683.6860\n",
            "Epoch 248/500\n",
            "50/50 [==============================] - 0s 906us/step - loss: 7654.5751 - mse: 87191512.0000 - mae: 7654.5757 - val_loss: 5610.8575 - val_mse: 35910952.0000 - val_mae: 5610.8579\n",
            "Epoch 249/500\n",
            "50/50 [==============================] - 0s 905us/step - loss: 6585.1247 - mse: 67847608.0000 - mae: 6585.1250 - val_loss: 5558.9287 - val_mse: 35208804.0000 - val_mae: 5558.9287\n",
            "Epoch 250/500\n",
            "50/50 [==============================] - 0s 816us/step - loss: 6322.5359 - mse: 57308056.0000 - mae: 6322.5361 - val_loss: 5508.0649 - val_mse: 34522676.0000 - val_mae: 5508.0649\n",
            "Epoch 251/500\n",
            "50/50 [==============================] - 0s 910us/step - loss: 7177.8897 - mse: 70640096.0000 - mae: 7177.8901 - val_loss: 5446.0737 - val_mse: 33685280.0000 - val_mae: 5446.0737\n",
            "Epoch 252/500\n",
            "50/50 [==============================] - 0s 936us/step - loss: 6069.3160 - mse: 57768600.0000 - mae: 6069.3164 - val_loss: 5368.5798 - val_mse: 32677108.0000 - val_mae: 5368.5796\n",
            "Epoch 253/500\n",
            "50/50 [==============================] - 0s 877us/step - loss: 6884.7288 - mse: 72019864.0000 - mae: 6884.7285 - val_loss: 5324.5067 - val_mse: 32098964.0000 - val_mae: 5324.5068\n",
            "Epoch 254/500\n",
            "50/50 [==============================] - 0s 886us/step - loss: 6869.9645 - mse: 65958668.0000 - mae: 6869.9644 - val_loss: 5242.9870 - val_mse: 31064390.0000 - val_mae: 5242.9868\n",
            "Epoch 255/500\n",
            "50/50 [==============================] - 0s 800us/step - loss: 6523.2721 - mse: 57959768.0000 - mae: 6523.2725 - val_loss: 5182.6694 - val_mse: 30298784.0000 - val_mae: 5182.6689\n",
            "Epoch 256/500\n",
            "50/50 [==============================] - 0s 763us/step - loss: 6476.8103 - mse: 66487240.0000 - mae: 6476.8101 - val_loss: 5137.0211 - val_mse: 29737132.0000 - val_mae: 5137.0210\n",
            "Epoch 257/500\n",
            "50/50 [==============================] - 0s 932us/step - loss: 6707.0957 - mse: 71642672.0000 - mae: 6707.0957 - val_loss: 5127.3154 - val_mse: 29608112.0000 - val_mae: 5127.3154\n",
            "Epoch 258/500\n",
            "50/50 [==============================] - 0s 846us/step - loss: 5823.9905 - mse: 48929832.0000 - mae: 5823.9902 - val_loss: 5072.2750 - val_mse: 28945798.0000 - val_mae: 5072.2749\n",
            "Epoch 259/500\n",
            "50/50 [==============================] - 0s 904us/step - loss: 6935.4060 - mse: 64404236.0000 - mae: 6935.4058 - val_loss: 5012.3966 - val_mse: 28232822.0000 - val_mae: 5012.3970\n",
            "Epoch 260/500\n",
            "50/50 [==============================] - 0s 778us/step - loss: 6258.2070 - mse: 65480924.0000 - mae: 6258.2075 - val_loss: 5006.4717 - val_mse: 28170860.0000 - val_mae: 5006.4717\n",
            "Epoch 261/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6777.4843 - mse: 69994344.0000 - mae: 6777.4844 - val_loss: 4990.8754 - val_mse: 27983286.0000 - val_mae: 4990.8760\n",
            "Epoch 262/500\n",
            "50/50 [==============================] - 0s 956us/step - loss: 7344.3185 - mse: 73051160.0000 - mae: 7344.3184 - val_loss: 4943.6854 - val_mse: 27436492.0000 - val_mae: 4943.6851\n",
            "Epoch 263/500\n",
            "50/50 [==============================] - 0s 954us/step - loss: 6500.1474 - mse: 66672528.0000 - mae: 6500.1475 - val_loss: 4898.7476 - val_mse: 26904912.0000 - val_mae: 4898.7476\n",
            "Epoch 264/500\n",
            "50/50 [==============================] - 0s 908us/step - loss: 6433.0725 - mse: 60557036.0000 - mae: 6433.0723 - val_loss: 4843.3968 - val_mse: 26275608.0000 - val_mae: 4843.3970\n",
            "Epoch 265/500\n",
            "50/50 [==============================] - 0s 901us/step - loss: 5264.1493 - mse: 39146080.0000 - mae: 5264.1494 - val_loss: 4828.5673 - val_mse: 26102084.0000 - val_mae: 4828.5674\n",
            "Epoch 266/500\n",
            "50/50 [==============================] - 0s 919us/step - loss: 6790.0884 - mse: 76249240.0000 - mae: 6790.0879 - val_loss: 4783.4937 - val_mse: 25609828.0000 - val_mae: 4783.4937\n",
            "Epoch 267/500\n",
            "50/50 [==============================] - 0s 905us/step - loss: 7164.2564 - mse: 77576256.0000 - mae: 7164.2568 - val_loss: 4782.3660 - val_mse: 25601044.0000 - val_mae: 4782.3662\n",
            "Epoch 268/500\n",
            "50/50 [==============================] - 0s 966us/step - loss: 6802.6468 - mse: 67762424.0000 - mae: 6802.6470 - val_loss: 4744.7185 - val_mse: 25192976.0000 - val_mae: 4744.7188\n",
            "Epoch 269/500\n",
            "50/50 [==============================] - 0s 880us/step - loss: 7575.5555 - mse: 79626504.0000 - mae: 7575.5557 - val_loss: 4708.7661 - val_mse: 24801700.0000 - val_mae: 4708.7666\n",
            "Epoch 270/500\n",
            "50/50 [==============================] - 0s 832us/step - loss: 7014.2884 - mse: 81664080.0000 - mae: 7014.2881 - val_loss: 4679.7292 - val_mse: 24489930.0000 - val_mae: 4679.7295\n",
            "Epoch 271/500\n",
            "50/50 [==============================] - 0s 789us/step - loss: 6217.3587 - mse: 60273420.0000 - mae: 6217.3589 - val_loss: 4677.6017 - val_mse: 24483306.0000 - val_mae: 4677.6016\n",
            "Epoch 272/500\n",
            "50/50 [==============================] - 0s 897us/step - loss: 6631.9793 - mse: 66365956.0000 - mae: 6631.9785 - val_loss: 4621.6732 - val_mse: 23870740.0000 - val_mae: 4621.6729\n",
            "Epoch 273/500\n",
            "50/50 [==============================] - 0s 761us/step - loss: 6652.2822 - mse: 69479960.0000 - mae: 6652.2827 - val_loss: 4606.8223 - val_mse: 23724662.0000 - val_mae: 4606.8228\n",
            "Epoch 274/500\n",
            "50/50 [==============================] - 0s 791us/step - loss: 6229.4526 - mse: 59549220.0000 - mae: 6229.4526 - val_loss: 4605.0031 - val_mse: 23723862.0000 - val_mae: 4605.0029\n",
            "Epoch 275/500\n",
            "50/50 [==============================] - 0s 848us/step - loss: 5907.2575 - mse: 53621300.0000 - mae: 5907.2573 - val_loss: 4569.3828 - val_mse: 23347060.0000 - val_mae: 4569.3828\n",
            "Epoch 276/500\n",
            "50/50 [==============================] - 0s 893us/step - loss: 7351.7571 - mse: 74148984.0000 - mae: 7351.7573 - val_loss: 4547.2900 - val_mse: 23129336.0000 - val_mae: 4547.2900\n",
            "Epoch 277/500\n",
            "50/50 [==============================] - 0s 858us/step - loss: 5854.7559 - mse: 55266992.0000 - mae: 5854.7563 - val_loss: 4546.9059 - val_mse: 23131094.0000 - val_mae: 4546.9058\n",
            "Epoch 278/500\n",
            "50/50 [==============================] - 0s 819us/step - loss: 7069.5663 - mse: 73827352.0000 - mae: 7069.5664 - val_loss: 4546.0577 - val_mse: 23133334.0000 - val_mae: 4546.0576\n",
            "Epoch 279/500\n",
            "50/50 [==============================] - 0s 814us/step - loss: 5430.8386 - mse: 52304628.0000 - mae: 5430.8389 - val_loss: 4540.3672 - val_mse: 23089786.0000 - val_mae: 4540.3672\n",
            "Epoch 280/500\n",
            "50/50 [==============================] - 0s 820us/step - loss: 6419.0080 - mse: 59298372.0000 - mae: 6419.0083 - val_loss: 4529.3255 - val_mse: 23014044.0000 - val_mae: 4529.3257\n",
            "Epoch 281/500\n",
            "50/50 [==============================] - 0s 941us/step - loss: 6412.6745 - mse: 57947192.0000 - mae: 6412.6748 - val_loss: 4488.4583 - val_mse: 22573428.0000 - val_mae: 4488.4580\n",
            "Epoch 282/500\n",
            "50/50 [==============================] - 0s 710us/step - loss: 6508.7146 - mse: 63946968.0000 - mae: 6508.7148 - val_loss: 4516.9141 - val_mse: 22918828.0000 - val_mae: 4516.9141\n",
            "Epoch 283/500\n",
            "50/50 [==============================] - 0s 741us/step - loss: 6781.9087 - mse: 74745280.0000 - mae: 6781.9087 - val_loss: 4491.6834 - val_mse: 22673350.0000 - val_mae: 4491.6836\n",
            "Epoch 284/500\n",
            "50/50 [==============================] - 0s 918us/step - loss: 7104.2249 - mse: 71206480.0000 - mae: 7104.2251 - val_loss: 4460.4415 - val_mse: 22367420.0000 - val_mae: 4460.4419\n",
            "Epoch 285/500\n",
            "50/50 [==============================] - 0s 814us/step - loss: 6830.2855 - mse: 67108816.0000 - mae: 6830.2856 - val_loss: 4415.5717 - val_mse: 21915540.0000 - val_mae: 4415.5718\n",
            "Epoch 286/500\n",
            "50/50 [==============================] - 0s 679us/step - loss: 6375.7684 - mse: 59381564.0000 - mae: 6375.7681 - val_loss: 4412.3211 - val_mse: 21887148.0000 - val_mae: 4412.3213\n",
            "Epoch 287/500\n",
            "50/50 [==============================] - 0s 758us/step - loss: 5935.3506 - mse: 53138264.0000 - mae: 5935.3506 - val_loss: 4385.5327 - val_mse: 21665178.0000 - val_mae: 4385.5327\n",
            "Epoch 288/500\n",
            "50/50 [==============================] - 0s 690us/step - loss: 5676.0859 - mse: 48340392.0000 - mae: 5676.0864 - val_loss: 4361.3347 - val_mse: 21461190.0000 - val_mae: 4361.3350\n",
            "Epoch 289/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6861.4228 - mse: 67787472.0000 - mae: 6861.4224 - val_loss: 4354.7900 - val_mse: 21439374.0000 - val_mae: 4354.7900\n",
            "Epoch 290/500\n",
            "50/50 [==============================] - 0s 847us/step - loss: 6951.1411 - mse: 69274544.0000 - mae: 6951.1411 - val_loss: 4312.6710 - val_mse: 21035804.0000 - val_mae: 4312.6709\n",
            "Epoch 291/500\n",
            "50/50 [==============================] - 0s 761us/step - loss: 6123.5061 - mse: 61722940.0000 - mae: 6123.5063 - val_loss: 4260.5651 - val_mse: 20499886.0000 - val_mae: 4260.5649\n",
            "Epoch 292/500\n",
            "50/50 [==============================] - 0s 745us/step - loss: 6853.8243 - mse: 69884616.0000 - mae: 6853.8242 - val_loss: 4229.7676 - val_mse: 20204578.0000 - val_mae: 4229.7681\n",
            "Epoch 293/500\n",
            "50/50 [==============================] - 0s 841us/step - loss: 7378.2741 - mse: 75737256.0000 - mae: 7378.2739 - val_loss: 4219.6752 - val_mse: 20146770.0000 - val_mae: 4219.6753\n",
            "Epoch 294/500\n",
            "50/50 [==============================] - 0s 789us/step - loss: 6428.9191 - mse: 59540204.0000 - mae: 6428.9189 - val_loss: 4175.4737 - val_mse: 19757146.0000 - val_mae: 4175.4736\n",
            "Epoch 295/500\n",
            "50/50 [==============================] - 0s 770us/step - loss: 6680.0679 - mse: 71808160.0000 - mae: 6680.0684 - val_loss: 4175.8817 - val_mse: 19819060.0000 - val_mae: 4175.8818\n",
            "Epoch 296/500\n",
            "50/50 [==============================] - 0s 764us/step - loss: 5879.2469 - mse: 46046708.0000 - mae: 5879.2461 - val_loss: 4103.3584 - val_mse: 19129980.0000 - val_mae: 4103.3584\n",
            "Epoch 297/500\n",
            "50/50 [==============================] - 0s 846us/step - loss: 6387.4564 - mse: 64476604.0000 - mae: 6387.4561 - val_loss: 4054.1248 - val_mse: 18702400.0000 - val_mae: 4054.1245\n",
            "Epoch 298/500\n",
            "50/50 [==============================] - 0s 810us/step - loss: 6928.4339 - mse: 71968032.0000 - mae: 6928.4336 - val_loss: 4052.2181 - val_mse: 18713910.0000 - val_mae: 4052.2180\n",
            "Epoch 299/500\n",
            "50/50 [==============================] - 0s 902us/step - loss: 6323.2198 - mse: 59094616.0000 - mae: 6323.2202 - val_loss: 4050.4590 - val_mse: 18718590.0000 - val_mae: 4050.4590\n",
            "Epoch 300/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6553.1433 - mse: 72759656.0000 - mae: 6553.1426 - val_loss: 4039.7939 - val_mse: 18673036.0000 - val_mae: 4039.7942\n",
            "Epoch 301/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5613.4611 - mse: 59508808.0000 - mae: 5613.4614 - val_loss: 4043.7234 - val_mse: 18755016.0000 - val_mae: 4043.7234\n",
            "Epoch 302/500\n",
            "50/50 [==============================] - 0s 708us/step - loss: 6114.6896 - mse: 54459888.0000 - mae: 6114.6895 - val_loss: 4029.2819 - val_mse: 18680064.0000 - val_mae: 4029.2820\n",
            "Epoch 303/500\n",
            "50/50 [==============================] - 0s 746us/step - loss: 5131.2109 - mse: 42774696.0000 - mae: 5131.2109 - val_loss: 4044.8817 - val_mse: 18876506.0000 - val_mae: 4044.8816\n",
            "Epoch 304/500\n",
            "50/50 [==============================] - 0s 794us/step - loss: 5909.8372 - mse: 51559356.0000 - mae: 5909.8374 - val_loss: 4066.1461 - val_mse: 19200180.0000 - val_mae: 4066.1460\n",
            "Epoch 305/500\n",
            "50/50 [==============================] - 0s 930us/step - loss: 6057.2305 - mse: 50931016.0000 - mae: 6057.2305 - val_loss: 4077.8644 - val_mse: 19380786.0000 - val_mae: 4077.8645\n",
            "Epoch 306/500\n",
            "50/50 [==============================] - 0s 881us/step - loss: 5918.2742 - mse: 55623056.0000 - mae: 5918.2744 - val_loss: 4016.2365 - val_mse: 18795200.0000 - val_mae: 4016.2363\n",
            "Epoch 307/500\n",
            "50/50 [==============================] - 0s 777us/step - loss: 6504.4592 - mse: 64008040.0000 - mae: 6504.4590 - val_loss: 3971.7588 - val_mse: 18385004.0000 - val_mae: 3971.7585\n",
            "Epoch 308/500\n",
            "50/50 [==============================] - 0s 886us/step - loss: 6105.8635 - mse: 57409248.0000 - mae: 6105.8638 - val_loss: 3969.4384 - val_mse: 18432602.0000 - val_mae: 3969.4382\n",
            "Epoch 309/500\n",
            "50/50 [==============================] - 0s 866us/step - loss: 5523.8175 - mse: 50319536.0000 - mae: 5523.8174 - val_loss: 3987.2824 - val_mse: 18672762.0000 - val_mae: 3987.2825\n",
            "Epoch 310/500\n",
            "50/50 [==============================] - 0s 885us/step - loss: 4926.6140 - mse: 38294068.0000 - mae: 4926.6143 - val_loss: 4006.8114 - val_mse: 18890004.0000 - val_mae: 4006.8118\n",
            "Epoch 311/500\n",
            "50/50 [==============================] - 0s 968us/step - loss: 6359.3855 - mse: 61312992.0000 - mae: 6359.3848 - val_loss: 3988.6351 - val_mse: 18691956.0000 - val_mae: 3988.6353\n",
            "Epoch 312/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5627.0014 - mse: 50609060.0000 - mae: 5627.0015 - val_loss: 3960.1436 - val_mse: 18426814.0000 - val_mae: 3960.1438\n",
            "Epoch 313/500\n",
            "50/50 [==============================] - 0s 968us/step - loss: 6395.1390 - mse: 72218176.0000 - mae: 6395.1387 - val_loss: 3958.3903 - val_mse: 18465836.0000 - val_mae: 3958.3901\n",
            "Epoch 314/500\n",
            "50/50 [==============================] - 0s 798us/step - loss: 6180.5703 - mse: 58117756.0000 - mae: 6180.5708 - val_loss: 3940.1671 - val_mse: 18340578.0000 - val_mae: 3940.1672\n",
            "Epoch 315/500\n",
            "50/50 [==============================] - 0s 840us/step - loss: 6384.8993 - mse: 62291840.0000 - mae: 6384.8994 - val_loss: 3926.4078 - val_mse: 18271722.0000 - val_mae: 3926.4077\n",
            "Epoch 316/500\n",
            "50/50 [==============================] - 0s 776us/step - loss: 6024.5718 - mse: 57535252.0000 - mae: 6024.5713 - val_loss: 3899.1730 - val_mse: 18060478.0000 - val_mae: 3899.1731\n",
            "Epoch 317/500\n",
            "50/50 [==============================] - 0s 868us/step - loss: 6455.8218 - mse: 61375232.0000 - mae: 6455.8213 - val_loss: 3903.4617 - val_mse: 18160744.0000 - val_mae: 3903.4617\n",
            "Epoch 318/500\n",
            "50/50 [==============================] - 0s 973us/step - loss: 6319.3489 - mse: 65583244.0000 - mae: 6319.3486 - val_loss: 3900.4543 - val_mse: 18193190.0000 - val_mae: 3900.4543\n",
            "Epoch 319/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6482.9287 - mse: 63499724.0000 - mae: 6482.9287 - val_loss: 3872.0133 - val_mse: 17972218.0000 - val_mae: 3872.0132\n",
            "Epoch 320/500\n",
            "50/50 [==============================] - 0s 851us/step - loss: 6201.8368 - mse: 61059480.0000 - mae: 6201.8369 - val_loss: 3878.4510 - val_mse: 18040534.0000 - val_mae: 3878.4512\n",
            "Epoch 321/500\n",
            "50/50 [==============================] - 0s 944us/step - loss: 6063.7228 - mse: 57302324.0000 - mae: 6063.7231 - val_loss: 3838.0977 - val_mse: 17693746.0000 - val_mae: 3838.0977\n",
            "Epoch 322/500\n",
            "50/50 [==============================] - 0s 788us/step - loss: 5671.6235 - mse: 50825332.0000 - mae: 5671.6235 - val_loss: 3790.0791 - val_mse: 17291272.0000 - val_mae: 3790.0789\n",
            "Epoch 323/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5887.4662 - mse: 55113488.0000 - mae: 5887.4663 - val_loss: 3801.0295 - val_mse: 17478948.0000 - val_mae: 3801.0298\n",
            "Epoch 324/500\n",
            "50/50 [==============================] - 0s 815us/step - loss: 6499.0178 - mse: 59821216.0000 - mae: 6499.0181 - val_loss: 3773.2253 - val_mse: 17274386.0000 - val_mae: 3773.2253\n",
            "Epoch 325/500\n",
            "50/50 [==============================] - 0s 833us/step - loss: 5424.3998 - mse: 43238932.0000 - mae: 5424.3999 - val_loss: 3781.6412 - val_mse: 17397128.0000 - val_mae: 3781.6414\n",
            "Epoch 326/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5354.7493 - mse: 44465872.0000 - mae: 5354.7495 - val_loss: 3747.8718 - val_mse: 17084850.0000 - val_mae: 3747.8718\n",
            "Epoch 327/500\n",
            "50/50 [==============================] - 0s 969us/step - loss: 5923.6127 - mse: 54148704.0000 - mae: 5923.6123 - val_loss: 3756.1732 - val_mse: 17151848.0000 - val_mae: 3756.1733\n",
            "Epoch 328/500\n",
            "50/50 [==============================] - 0s 847us/step - loss: 6138.1751 - mse: 61450496.0000 - mae: 6138.1758 - val_loss: 3737.7027 - val_mse: 17009668.0000 - val_mae: 3737.7026\n",
            "Epoch 329/500\n",
            "50/50 [==============================] - 0s 990us/step - loss: 7185.6625 - mse: 87694144.0000 - mae: 7185.6626 - val_loss: 3761.5534 - val_mse: 17346828.0000 - val_mae: 3761.5535\n",
            "Epoch 330/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6707.8954 - mse: 74949456.0000 - mae: 6707.8950 - val_loss: 3787.2299 - val_mse: 17668528.0000 - val_mae: 3787.2297\n",
            "Epoch 331/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6512.2326 - mse: 64193284.0000 - mae: 6512.2324 - val_loss: 3785.9532 - val_mse: 17681356.0000 - val_mae: 3785.9531\n",
            "Epoch 332/500\n",
            "50/50 [==============================] - 0s 814us/step - loss: 6522.8308 - mse: 63787376.0000 - mae: 6522.8306 - val_loss: 3764.1832 - val_mse: 17552484.0000 - val_mae: 3764.1831\n",
            "Epoch 333/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6075.7350 - mse: 60203940.0000 - mae: 6075.7349 - val_loss: 3746.5316 - val_mse: 17437286.0000 - val_mae: 3746.5317\n",
            "Epoch 334/500\n",
            "50/50 [==============================] - 0s 882us/step - loss: 5299.4741 - mse: 43190272.0000 - mae: 5299.4736 - val_loss: 3735.1956 - val_mse: 17339900.0000 - val_mae: 3735.1958\n",
            "Epoch 335/500\n",
            "50/50 [==============================] - 0s 952us/step - loss: 7381.7053 - mse: 76702184.0000 - mae: 7381.7051 - val_loss: 3691.2831 - val_mse: 16917548.0000 - val_mae: 3691.2832\n",
            "Epoch 336/500\n",
            "50/50 [==============================] - 0s 855us/step - loss: 6325.6243 - mse: 62274668.0000 - mae: 6325.6245 - val_loss: 3661.0748 - val_mse: 16674650.0000 - val_mae: 3661.0747\n",
            "Epoch 337/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6021.7816 - mse: 55569268.0000 - mae: 6021.7812 - val_loss: 3637.0389 - val_mse: 16529598.0000 - val_mae: 3637.0391\n",
            "Epoch 338/500\n",
            "50/50 [==============================] - 0s 941us/step - loss: 5636.1779 - mse: 52099048.0000 - mae: 5636.1782 - val_loss: 3604.1901 - val_mse: 16324240.0000 - val_mae: 3604.1902\n",
            "Epoch 339/500\n",
            "50/50 [==============================] - 0s 899us/step - loss: 5529.8651 - mse: 47682548.0000 - mae: 5529.8652 - val_loss: 3567.0408 - val_mse: 15980216.0000 - val_mae: 3567.0405\n",
            "Epoch 340/500\n",
            "50/50 [==============================] - 0s 973us/step - loss: 6201.7611 - mse: 60104272.0000 - mae: 6201.7612 - val_loss: 3583.2621 - val_mse: 16143781.0000 - val_mae: 3583.2622\n",
            "Epoch 341/500\n",
            "50/50 [==============================] - 0s 899us/step - loss: 6185.1595 - mse: 55253672.0000 - mae: 6185.1592 - val_loss: 3582.4178 - val_mse: 16169672.0000 - val_mae: 3582.4180\n",
            "Epoch 342/500\n",
            "50/50 [==============================] - 0s 824us/step - loss: 5476.9763 - mse: 50129908.0000 - mae: 5476.9761 - val_loss: 3568.3521 - val_mse: 16078486.0000 - val_mae: 3568.3521\n",
            "Epoch 343/500\n",
            "50/50 [==============================] - 0s 966us/step - loss: 7308.3294 - mse: 79364152.0000 - mae: 7308.3286 - val_loss: 3572.6993 - val_mse: 16109998.0000 - val_mae: 3572.6992\n",
            "Epoch 344/500\n",
            "50/50 [==============================] - 0s 781us/step - loss: 6160.0880 - mse: 57026876.0000 - mae: 6160.0879 - val_loss: 3542.1785 - val_mse: 15863470.0000 - val_mae: 3542.1785\n",
            "Epoch 345/500\n",
            "50/50 [==============================] - 0s 799us/step - loss: 5316.5916 - mse: 46669124.0000 - mae: 5316.5913 - val_loss: 3552.4027 - val_mse: 15964040.0000 - val_mae: 3552.4028\n",
            "Epoch 346/500\n",
            "50/50 [==============================] - 0s 873us/step - loss: 6606.1742 - mse: 57964092.0000 - mae: 6606.1743 - val_loss: 3569.6451 - val_mse: 16117946.0000 - val_mae: 3569.6450\n",
            "Epoch 347/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5168.1193 - mse: 41082136.0000 - mae: 5168.1191 - val_loss: 3581.2039 - val_mse: 16178136.0000 - val_mae: 3581.2039\n",
            "Epoch 348/500\n",
            "50/50 [==============================] - 0s 878us/step - loss: 6450.6572 - mse: 65569944.0000 - mae: 6450.6567 - val_loss: 3612.5635 - val_mse: 16517058.0000 - val_mae: 3612.5632\n",
            "Epoch 349/500\n",
            "50/50 [==============================] - 0s 961us/step - loss: 7005.0589 - mse: 69242672.0000 - mae: 7005.0596 - val_loss: 3599.8146 - val_mse: 16437872.0000 - val_mae: 3599.8149\n",
            "Epoch 350/500\n",
            "50/50 [==============================] - 0s 984us/step - loss: 6071.9665 - mse: 57045608.0000 - mae: 6071.9663 - val_loss: 3587.3591 - val_mse: 16404883.0000 - val_mae: 3587.3594\n",
            "Epoch 351/500\n",
            "50/50 [==============================] - 0s 900us/step - loss: 5375.3446 - mse: 43821456.0000 - mae: 5375.3452 - val_loss: 3608.6230 - val_mse: 16679582.0000 - val_mae: 3608.6230\n",
            "Epoch 352/500\n",
            "50/50 [==============================] - 0s 965us/step - loss: 5393.6251 - mse: 43910312.0000 - mae: 5393.6250 - val_loss: 3562.5419 - val_mse: 16324941.0000 - val_mae: 3562.5417\n",
            "Epoch 353/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6960.3606 - mse: 72678464.0000 - mae: 6960.3608 - val_loss: 3545.3061 - val_mse: 16232494.0000 - val_mae: 3545.3062\n",
            "Epoch 354/500\n",
            "50/50 [==============================] - 0s 993us/step - loss: 5420.9313 - mse: 49427656.0000 - mae: 5420.9312 - val_loss: 3561.8511 - val_mse: 16326952.0000 - val_mae: 3561.8511\n",
            "Epoch 355/500\n",
            "50/50 [==============================] - 0s 875us/step - loss: 5258.9276 - mse: 45515196.0000 - mae: 5258.9277 - val_loss: 3548.6208 - val_mse: 16288560.0000 - val_mae: 3548.6206\n",
            "Epoch 356/500\n",
            "50/50 [==============================] - 0s 965us/step - loss: 6402.8010 - mse: 58648704.0000 - mae: 6402.8013 - val_loss: 3550.9921 - val_mse: 16340749.0000 - val_mae: 3550.9922\n",
            "Epoch 357/500\n",
            "50/50 [==============================] - 0s 973us/step - loss: 5695.5590 - mse: 46941016.0000 - mae: 5695.5586 - val_loss: 3542.9030 - val_mse: 16308541.0000 - val_mae: 3542.9031\n",
            "Epoch 358/500\n",
            "50/50 [==============================] - 0s 976us/step - loss: 6724.0776 - mse: 68470888.0000 - mae: 6724.0781 - val_loss: 3525.8482 - val_mse: 16272214.0000 - val_mae: 3525.8481\n",
            "Epoch 359/500\n",
            "50/50 [==============================] - 0s 956us/step - loss: 6419.1444 - mse: 56956012.0000 - mae: 6419.1450 - val_loss: 3522.4383 - val_mse: 16340530.0000 - val_mae: 3522.4382\n",
            "Epoch 360/500\n",
            "50/50 [==============================] - 0s 901us/step - loss: 6233.1194 - mse: 69196192.0000 - mae: 6233.1191 - val_loss: 3523.3978 - val_mse: 16367922.0000 - val_mae: 3523.3977\n",
            "Epoch 361/500\n",
            "50/50 [==============================] - 0s 912us/step - loss: 5758.9674 - mse: 51305488.0000 - mae: 5758.9673 - val_loss: 3503.8325 - val_mse: 16212435.0000 - val_mae: 3503.8325\n",
            "Epoch 362/500\n",
            "50/50 [==============================] - 0s 903us/step - loss: 5429.3100 - mse: 48137288.0000 - mae: 5429.3101 - val_loss: 3461.4072 - val_mse: 15869235.0000 - val_mae: 3461.4070\n",
            "Epoch 363/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 4761.2329 - mse: 44341120.0000 - mae: 4761.2329 - val_loss: 3437.1265 - val_mse: 15660342.0000 - val_mae: 3437.1265\n",
            "Epoch 364/500\n",
            "50/50 [==============================] - 0s 901us/step - loss: 5287.0215 - mse: 46744576.0000 - mae: 5287.0215 - val_loss: 3465.8908 - val_mse: 15927992.0000 - val_mae: 3465.8911\n",
            "Epoch 365/500\n",
            "50/50 [==============================] - 0s 888us/step - loss: 5606.3498 - mse: 49650980.0000 - mae: 5606.3501 - val_loss: 3424.5099 - val_mse: 15589330.0000 - val_mae: 3424.5098\n",
            "Epoch 366/500\n",
            "50/50 [==============================] - 0s 867us/step - loss: 5782.8583 - mse: 45692412.0000 - mae: 5782.8579 - val_loss: 3404.7210 - val_mse: 15485696.0000 - val_mae: 3404.7212\n",
            "Epoch 367/500\n",
            "50/50 [==============================] - 0s 837us/step - loss: 5073.3226 - mse: 49487264.0000 - mae: 5073.3223 - val_loss: 3404.9696 - val_mse: 15549165.0000 - val_mae: 3404.9695\n",
            "Epoch 368/500\n",
            "50/50 [==============================] - 0s 788us/step - loss: 5890.4813 - mse: 53086512.0000 - mae: 5890.4814 - val_loss: 3422.2001 - val_mse: 15697752.0000 - val_mae: 3422.2000\n",
            "Epoch 369/500\n",
            "50/50 [==============================] - 0s 931us/step - loss: 5575.2549 - mse: 47025064.0000 - mae: 5575.2549 - val_loss: 3430.4967 - val_mse: 15867051.0000 - val_mae: 3430.4968\n",
            "Epoch 370/500\n",
            "50/50 [==============================] - 0s 906us/step - loss: 6448.4316 - mse: 64958832.0000 - mae: 6448.4316 - val_loss: 3434.5784 - val_mse: 15928966.0000 - val_mae: 3434.5781\n",
            "Epoch 371/500\n",
            "50/50 [==============================] - 0s 947us/step - loss: 5856.2252 - mse: 48060256.0000 - mae: 5856.2256 - val_loss: 3404.6452 - val_mse: 15748243.0000 - val_mae: 3404.6453\n",
            "Epoch 372/500\n",
            "50/50 [==============================] - 0s 918us/step - loss: 5654.1087 - mse: 49282924.0000 - mae: 5654.1089 - val_loss: 3403.7732 - val_mse: 15785795.0000 - val_mae: 3403.7734\n",
            "Epoch 373/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 7618.8331 - mse: 79614912.0000 - mae: 7618.8330 - val_loss: 3407.4148 - val_mse: 15878680.0000 - val_mae: 3407.4148\n",
            "Epoch 374/500\n",
            "50/50 [==============================] - 0s 840us/step - loss: 6325.9503 - mse: 58968976.0000 - mae: 6325.9502 - val_loss: 3366.0554 - val_mse: 15551832.0000 - val_mae: 3366.0554\n",
            "Epoch 375/500\n",
            "50/50 [==============================] - 0s 926us/step - loss: 5553.9199 - mse: 48107788.0000 - mae: 5553.9199 - val_loss: 3333.1436 - val_mse: 15347376.0000 - val_mae: 3333.1433\n",
            "Epoch 376/500\n",
            "50/50 [==============================] - 0s 864us/step - loss: 6587.6586 - mse: 66305780.0000 - mae: 6587.6587 - val_loss: 3342.9780 - val_mse: 15444616.0000 - val_mae: 3342.9778\n",
            "Epoch 377/500\n",
            "50/50 [==============================] - 0s 935us/step - loss: 4892.2075 - mse: 35399044.0000 - mae: 4892.2075 - val_loss: 3350.9978 - val_mse: 15579470.0000 - val_mae: 3350.9980\n",
            "Epoch 378/500\n",
            "50/50 [==============================] - 0s 819us/step - loss: 5566.1969 - mse: 49271776.0000 - mae: 5566.1968 - val_loss: 3333.4837 - val_mse: 15579691.0000 - val_mae: 3333.4836\n",
            "Epoch 379/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6814.7223 - mse: 67146880.0000 - mae: 6814.7227 - val_loss: 3322.9372 - val_mse: 15586570.0000 - val_mae: 3322.9370\n",
            "Epoch 380/500\n",
            "50/50 [==============================] - 0s 914us/step - loss: 7089.1719 - mse: 72861480.0000 - mae: 7089.1719 - val_loss: 3319.3212 - val_mse: 15609925.0000 - val_mae: 3319.3210\n",
            "Epoch 381/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6544.6232 - mse: 59729220.0000 - mae: 6544.6230 - val_loss: 3322.7081 - val_mse: 15756221.0000 - val_mae: 3322.7083\n",
            "Epoch 382/500\n",
            "50/50 [==============================] - 0s 972us/step - loss: 6611.7697 - mse: 60158080.0000 - mae: 6611.7695 - val_loss: 3322.8296 - val_mse: 15773006.0000 - val_mae: 3322.8296\n",
            "Epoch 383/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6699.4479 - mse: 67141024.0000 - mae: 6699.4473 - val_loss: 3283.1216 - val_mse: 15424283.0000 - val_mae: 3283.1216\n",
            "Epoch 384/500\n",
            "50/50 [==============================] - 0s 856us/step - loss: 7523.8076 - mse: 79251808.0000 - mae: 7523.8081 - val_loss: 3294.3827 - val_mse: 15550235.0000 - val_mae: 3294.3828\n",
            "Epoch 385/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6440.5737 - mse: 65748144.0000 - mae: 6440.5737 - val_loss: 3332.6204 - val_mse: 15894302.0000 - val_mae: 3332.6204\n",
            "Epoch 386/500\n",
            "50/50 [==============================] - 0s 742us/step - loss: 5848.3261 - mse: 52926800.0000 - mae: 5848.3262 - val_loss: 3339.6635 - val_mse: 15907829.0000 - val_mae: 3339.6636\n",
            "Epoch 387/500\n",
            "50/50 [==============================] - 0s 875us/step - loss: 6700.0957 - mse: 72676416.0000 - mae: 6700.0962 - val_loss: 3336.9198 - val_mse: 15934914.0000 - val_mae: 3336.9199\n",
            "Epoch 388/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6182.6817 - mse: 58670396.0000 - mae: 6182.6816 - val_loss: 3313.0529 - val_mse: 15832669.0000 - val_mae: 3313.0532\n",
            "Epoch 389/500\n",
            "50/50 [==============================] - 0s 874us/step - loss: 6243.3332 - mse: 61590476.0000 - mae: 6243.3330 - val_loss: 3313.3518 - val_mse: 15823843.0000 - val_mae: 3313.3521\n",
            "Epoch 390/500\n",
            "50/50 [==============================] - 0s 838us/step - loss: 7785.0562 - mse: 91081704.0000 - mae: 7785.0562 - val_loss: 3302.7440 - val_mse: 15793477.0000 - val_mae: 3302.7441\n",
            "Epoch 391/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 7040.3718 - mse: 71564456.0000 - mae: 7040.3721 - val_loss: 3284.8335 - val_mse: 15794726.0000 - val_mae: 3284.8335\n",
            "Epoch 392/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 7245.9120 - mse: 68496752.0000 - mae: 7245.9126 - val_loss: 3285.9044 - val_mse: 15845296.0000 - val_mae: 3285.9043\n",
            "Epoch 393/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5642.1619 - mse: 45881332.0000 - mae: 5642.1621 - val_loss: 3269.9115 - val_mse: 15685842.0000 - val_mae: 3269.9116\n",
            "Epoch 394/500\n",
            "50/50 [==============================] - 0s 798us/step - loss: 6144.2038 - mse: 63128460.0000 - mae: 6144.2036 - val_loss: 3294.1997 - val_mse: 15966706.0000 - val_mae: 3294.1997\n",
            "Epoch 395/500\n",
            "50/50 [==============================] - 0s 843us/step - loss: 4730.9148 - mse: 35157584.0000 - mae: 4730.9150 - val_loss: 3281.5517 - val_mse: 15879283.0000 - val_mae: 3281.5515\n",
            "Epoch 396/500\n",
            "50/50 [==============================] - 0s 944us/step - loss: 6616.4067 - mse: 61119484.0000 - mae: 6616.4067 - val_loss: 3252.2637 - val_mse: 15645115.0000 - val_mae: 3252.2637\n",
            "Epoch 397/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6697.8617 - mse: 70688416.0000 - mae: 6697.8613 - val_loss: 3262.6157 - val_mse: 15752115.0000 - val_mae: 3262.6157\n",
            "Epoch 398/500\n",
            "50/50 [==============================] - 0s 931us/step - loss: 4851.6033 - mse: 35801904.0000 - mae: 4851.6030 - val_loss: 3243.1917 - val_mse: 15651362.0000 - val_mae: 3243.1919\n",
            "Epoch 399/500\n",
            "50/50 [==============================] - 0s 850us/step - loss: 4719.2349 - mse: 36002568.0000 - mae: 4719.2349 - val_loss: 3226.1288 - val_mse: 15481994.0000 - val_mae: 3226.1287\n",
            "Epoch 400/500\n",
            "50/50 [==============================] - 0s 871us/step - loss: 5113.0285 - mse: 40543380.0000 - mae: 5113.0283 - val_loss: 3218.5605 - val_mse: 15481986.0000 - val_mae: 3218.5605\n",
            "Epoch 401/500\n",
            "50/50 [==============================] - 0s 923us/step - loss: 5325.3058 - mse: 45577984.0000 - mae: 5325.3057 - val_loss: 3217.7119 - val_mse: 15461746.0000 - val_mae: 3217.7122\n",
            "Epoch 402/500\n",
            "50/50 [==============================] - 0s 890us/step - loss: 4131.1868 - mse: 27658956.0000 - mae: 4131.1865 - val_loss: 3241.8545 - val_mse: 15521525.0000 - val_mae: 3241.8545\n",
            "Epoch 403/500\n",
            "50/50 [==============================] - 0s 857us/step - loss: 6655.7303 - mse: 64555908.0000 - mae: 6655.7300 - val_loss: 3265.3048 - val_mse: 15689046.0000 - val_mae: 3265.3047\n",
            "Epoch 404/500\n",
            "50/50 [==============================] - 0s 771us/step - loss: 5427.8872 - mse: 42312836.0000 - mae: 5427.8877 - val_loss: 3308.0750 - val_mse: 16142238.0000 - val_mae: 3308.0754\n",
            "Epoch 405/500\n",
            "50/50 [==============================] - 0s 796us/step - loss: 5663.5209 - mse: 44858824.0000 - mae: 5663.5215 - val_loss: 3253.7761 - val_mse: 15676568.0000 - val_mae: 3253.7761\n",
            "Epoch 406/500\n",
            "50/50 [==============================] - 0s 816us/step - loss: 6051.7686 - mse: 54518932.0000 - mae: 6051.7686 - val_loss: 3262.9758 - val_mse: 15891691.0000 - val_mae: 3262.9758\n",
            "Epoch 407/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5649.5479 - mse: 62371920.0000 - mae: 5649.5483 - val_loss: 3277.6064 - val_mse: 16006682.0000 - val_mae: 3277.6062\n",
            "Epoch 408/500\n",
            "50/50 [==============================] - 0s 866us/step - loss: 5701.4889 - mse: 53445664.0000 - mae: 5701.4888 - val_loss: 3271.7210 - val_mse: 16105427.0000 - val_mae: 3271.7209\n",
            "Epoch 409/500\n",
            "50/50 [==============================] - 0s 824us/step - loss: 5715.7743 - mse: 54708676.0000 - mae: 5715.7744 - val_loss: 3262.5051 - val_mse: 16100178.0000 - val_mae: 3262.5051\n",
            "Epoch 410/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6294.4929 - mse: 55576300.0000 - mae: 6294.4932 - val_loss: 3231.0583 - val_mse: 15842090.0000 - val_mae: 3231.0581\n",
            "Epoch 411/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5996.6883 - mse: 53450272.0000 - mae: 5996.6880 - val_loss: 3219.6268 - val_mse: 15680147.0000 - val_mae: 3219.6267\n",
            "Epoch 412/500\n",
            "50/50 [==============================] - 0s 979us/step - loss: 4933.2215 - mse: 41223760.0000 - mae: 4933.2217 - val_loss: 3194.1813 - val_mse: 15439411.0000 - val_mae: 3194.1814\n",
            "Epoch 413/500\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 6172.4479 - mse: 58560424.0000 - mae: 6172.4482 - val_loss: 3226.8866 - val_mse: 15534672.0000 - val_mae: 3226.8867\n",
            "Epoch 414/500\n",
            "50/50 [==============================] - 0s 876us/step - loss: 5371.4004 - mse: 44741912.0000 - mae: 5371.3999 - val_loss: 3229.9567 - val_mse: 15695248.0000 - val_mae: 3229.9565\n",
            "Epoch 415/500\n",
            "50/50 [==============================] - 0s 907us/step - loss: 5151.0979 - mse: 46010308.0000 - mae: 5151.0977 - val_loss: 3266.0929 - val_mse: 16131619.0000 - val_mae: 3266.0930\n",
            "Epoch 416/500\n",
            "50/50 [==============================] - 0s 778us/step - loss: 5455.3476 - mse: 43998084.0000 - mae: 5455.3477 - val_loss: 3227.6871 - val_mse: 15789358.0000 - val_mae: 3227.6873\n",
            "Epoch 417/500\n",
            "50/50 [==============================] - 0s 872us/step - loss: 5052.4045 - mse: 41852676.0000 - mae: 5052.4043 - val_loss: 3240.0202 - val_mse: 15985885.0000 - val_mae: 3240.0203\n",
            "Epoch 418/500\n",
            "50/50 [==============================] - 0s 838us/step - loss: 6022.1034 - mse: 48916256.0000 - mae: 6022.1030 - val_loss: 3251.2173 - val_mse: 16163795.0000 - val_mae: 3251.2173\n",
            "Epoch 419/500\n",
            "50/50 [==============================] - 0s 750us/step - loss: 5868.7641 - mse: 64102604.0000 - mae: 5868.7637 - val_loss: 3210.6386 - val_mse: 15875398.0000 - val_mae: 3210.6387\n",
            "Epoch 420/500\n",
            "50/50 [==============================] - 0s 929us/step - loss: 6927.1440 - mse: 71800416.0000 - mae: 6927.1436 - val_loss: 3220.9093 - val_mse: 16031317.0000 - val_mae: 3220.9092\n",
            "Epoch 421/500\n",
            "50/50 [==============================] - 0s 852us/step - loss: 5821.7873 - mse: 53025404.0000 - mae: 5821.7876 - val_loss: 3217.7745 - val_mse: 16093422.0000 - val_mae: 3217.7747\n",
            "Epoch 422/500\n",
            "50/50 [==============================] - 0s 869us/step - loss: 6291.7247 - mse: 60180864.0000 - mae: 6291.7251 - val_loss: 3250.3680 - val_mse: 16308053.0000 - val_mae: 3250.3679\n",
            "Epoch 423/500\n",
            "50/50 [==============================] - 0s 893us/step - loss: 6209.6453 - mse: 66015712.0000 - mae: 6209.6455 - val_loss: 3258.5597 - val_mse: 16440501.0000 - val_mae: 3258.5598\n",
            "Epoch 424/500\n",
            "50/50 [==============================] - 0s 984us/step - loss: 5321.2064 - mse: 43344052.0000 - mae: 5321.2061 - val_loss: 3207.5895 - val_mse: 16032019.0000 - val_mae: 3207.5894\n",
            "Epoch 425/500\n",
            "50/50 [==============================] - 0s 816us/step - loss: 5796.0432 - mse: 54654880.0000 - mae: 5796.0430 - val_loss: 3237.3801 - val_mse: 16245024.0000 - val_mae: 3237.3801\n",
            "Epoch 426/500\n",
            "50/50 [==============================] - 0s 942us/step - loss: 6423.3865 - mse: 68621640.0000 - mae: 6423.3862 - val_loss: 3226.5666 - val_mse: 16180742.0000 - val_mae: 3226.5667\n",
            "Epoch 427/500\n",
            "50/50 [==============================] - 0s 835us/step - loss: 5555.8655 - mse: 49757968.0000 - mae: 5555.8657 - val_loss: 3191.9857 - val_mse: 15820933.0000 - val_mae: 3191.9856\n",
            "Epoch 428/500\n",
            "50/50 [==============================] - 0s 865us/step - loss: 6167.8289 - mse: 60765940.0000 - mae: 6167.8286 - val_loss: 3191.1881 - val_mse: 15798635.0000 - val_mae: 3191.1882\n",
            "Epoch 429/500\n",
            "50/50 [==============================] - 0s 865us/step - loss: 5972.0833 - mse: 50998276.0000 - mae: 5972.0830 - val_loss: 3188.6126 - val_mse: 15814582.0000 - val_mae: 3188.6125\n",
            "Epoch 430/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6038.8385 - mse: 62856856.0000 - mae: 6038.8389 - val_loss: 3228.5829 - val_mse: 16192419.0000 - val_mae: 3228.5828\n",
            "Epoch 431/500\n",
            "50/50 [==============================] - 0s 922us/step - loss: 7281.8342 - mse: 79511504.0000 - mae: 7281.8345 - val_loss: 3231.5252 - val_mse: 16248560.0000 - val_mae: 3231.5254\n",
            "Epoch 432/500\n",
            "50/50 [==============================] - 0s 849us/step - loss: 6203.6395 - mse: 62168380.0000 - mae: 6203.6401 - val_loss: 3231.4101 - val_mse: 16311434.0000 - val_mae: 3231.4102\n",
            "Epoch 433/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6090.5107 - mse: 53887120.0000 - mae: 6090.5107 - val_loss: 3209.8300 - val_mse: 16113971.0000 - val_mae: 3209.8298\n",
            "Epoch 434/500\n",
            "50/50 [==============================] - 0s 974us/step - loss: 4859.3967 - mse: 38188160.0000 - mae: 4859.3970 - val_loss: 3189.3103 - val_mse: 15964797.0000 - val_mae: 3189.3101\n",
            "Epoch 435/500\n",
            "50/50 [==============================] - 0s 919us/step - loss: 5473.2284 - mse: 51294772.0000 - mae: 5473.2280 - val_loss: 3198.3379 - val_mse: 16135197.0000 - val_mae: 3198.3379\n",
            "Epoch 436/500\n",
            "50/50 [==============================] - 0s 807us/step - loss: 5182.0927 - mse: 38494248.0000 - mae: 5182.0923 - val_loss: 3176.6408 - val_mse: 15942362.0000 - val_mae: 3176.6409\n",
            "Epoch 437/500\n",
            "50/50 [==============================] - 0s 849us/step - loss: 5738.7396 - mse: 53963488.0000 - mae: 5738.7393 - val_loss: 3148.0093 - val_mse: 15632133.0000 - val_mae: 3148.0093\n",
            "Epoch 438/500\n",
            "50/50 [==============================] - 0s 965us/step - loss: 5667.4113 - mse: 60728616.0000 - mae: 5667.4111 - val_loss: 3173.2106 - val_mse: 15854926.0000 - val_mae: 3173.2104\n",
            "Epoch 439/500\n",
            "50/50 [==============================] - 0s 997us/step - loss: 5768.4476 - mse: 53585484.0000 - mae: 5768.4473 - val_loss: 3158.2659 - val_mse: 15733277.0000 - val_mae: 3158.2661\n",
            "Epoch 440/500\n",
            "50/50 [==============================] - 0s 853us/step - loss: 5114.0750 - mse: 43541560.0000 - mae: 5114.0752 - val_loss: 3165.4460 - val_mse: 15772046.0000 - val_mae: 3165.4460\n",
            "Epoch 441/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6000.6812 - mse: 49456564.0000 - mae: 6000.6812 - val_loss: 3168.9154 - val_mse: 15829067.0000 - val_mae: 3168.9155\n",
            "Epoch 442/500\n",
            "50/50 [==============================] - 0s 901us/step - loss: 5307.9002 - mse: 43779808.0000 - mae: 5307.8999 - val_loss: 3159.9934 - val_mse: 15779632.0000 - val_mae: 3159.9934\n",
            "Epoch 443/500\n",
            "50/50 [==============================] - 0s 966us/step - loss: 5858.5955 - mse: 53052476.0000 - mae: 5858.5957 - val_loss: 3144.8445 - val_mse: 15766395.0000 - val_mae: 3144.8447\n",
            "Epoch 444/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6077.5696 - mse: 59360588.0000 - mae: 6077.5698 - val_loss: 3147.4412 - val_mse: 15871974.0000 - val_mae: 3147.4414\n",
            "Epoch 445/500\n",
            "50/50 [==============================] - 0s 931us/step - loss: 5918.8649 - mse: 51047704.0000 - mae: 5918.8652 - val_loss: 3156.8937 - val_mse: 15987482.0000 - val_mae: 3156.8938\n",
            "Epoch 446/500\n",
            "50/50 [==============================] - 0s 947us/step - loss: 5499.3736 - mse: 48214784.0000 - mae: 5499.3735 - val_loss: 3138.0343 - val_mse: 15777302.0000 - val_mae: 3138.0344\n",
            "Epoch 447/500\n",
            "50/50 [==============================] - 0s 993us/step - loss: 5916.6132 - mse: 49149024.0000 - mae: 5916.6133 - val_loss: 3136.5318 - val_mse: 15720771.0000 - val_mae: 3136.5317\n",
            "Epoch 448/500\n",
            "50/50 [==============================] - 0s 912us/step - loss: 5719.8522 - mse: 52929780.0000 - mae: 5719.8525 - val_loss: 3149.9835 - val_mse: 15813963.0000 - val_mae: 3149.9834\n",
            "Epoch 449/500\n",
            "50/50 [==============================] - 0s 939us/step - loss: 6192.2212 - mse: 63946040.0000 - mae: 6192.2212 - val_loss: 3153.9664 - val_mse: 15864398.0000 - val_mae: 3153.9663\n",
            "Epoch 450/500\n",
            "50/50 [==============================] - 0s 911us/step - loss: 6117.4803 - mse: 58793728.0000 - mae: 6117.4800 - val_loss: 3174.4302 - val_mse: 16092616.0000 - val_mae: 3174.4304\n",
            "Epoch 451/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5582.2251 - mse: 51084620.0000 - mae: 5582.2251 - val_loss: 3160.8292 - val_mse: 15982922.0000 - val_mae: 3160.8293\n",
            "Epoch 452/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 7773.0844 - mse: 84995568.0000 - mae: 7773.0845 - val_loss: 3168.3234 - val_mse: 16077598.0000 - val_mae: 3168.3235\n",
            "Epoch 453/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6859.7399 - mse: 70633200.0000 - mae: 6859.7402 - val_loss: 3185.5254 - val_mse: 16308192.0000 - val_mae: 3185.5254\n",
            "Epoch 454/500\n",
            "50/50 [==============================] - 0s 990us/step - loss: 6746.7846 - mse: 73789648.0000 - mae: 6746.7852 - val_loss: 3164.0853 - val_mse: 16133509.0000 - val_mae: 3164.0854\n",
            "Epoch 455/500\n",
            "50/50 [==============================] - 0s 935us/step - loss: 5575.4796 - mse: 47699840.0000 - mae: 5575.4800 - val_loss: 3166.4739 - val_mse: 16244157.0000 - val_mae: 3166.4739\n",
            "Epoch 456/500\n",
            "50/50 [==============================] - 0s 857us/step - loss: 6237.5416 - mse: 57061636.0000 - mae: 6237.5410 - val_loss: 3160.2128 - val_mse: 16234251.0000 - val_mae: 3160.2129\n",
            "Epoch 457/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6706.3578 - mse: 64244028.0000 - mae: 6706.3574 - val_loss: 3186.3487 - val_mse: 16470864.0000 - val_mae: 3186.3484\n",
            "Epoch 458/500\n",
            "50/50 [==============================] - 0s 769us/step - loss: 5804.5962 - mse: 48800716.0000 - mae: 5804.5962 - val_loss: 3168.8411 - val_mse: 16356010.0000 - val_mae: 3168.8413\n",
            "Epoch 459/500\n",
            "50/50 [==============================] - 0s 850us/step - loss: 5906.4342 - mse: 56299096.0000 - mae: 5906.4346 - val_loss: 3169.3637 - val_mse: 16383850.0000 - val_mae: 3169.3638\n",
            "Epoch 460/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6308.1238 - mse: 59288432.0000 - mae: 6308.1235 - val_loss: 3150.5962 - val_mse: 16206427.0000 - val_mae: 3150.5962\n",
            "Epoch 461/500\n",
            "50/50 [==============================] - 0s 894us/step - loss: 5369.4415 - mse: 41386736.0000 - mae: 5369.4419 - val_loss: 3145.9819 - val_mse: 16159090.0000 - val_mae: 3145.9819\n",
            "Epoch 462/500\n",
            "50/50 [==============================] - 0s 972us/step - loss: 4704.6427 - mse: 41344816.0000 - mae: 4704.6426 - val_loss: 3117.8922 - val_mse: 15906014.0000 - val_mae: 3117.8921\n",
            "Epoch 463/500\n",
            "50/50 [==============================] - 0s 862us/step - loss: 6734.3621 - mse: 67574328.0000 - mae: 6734.3623 - val_loss: 3140.2243 - val_mse: 16163064.0000 - val_mae: 3140.2241\n",
            "Epoch 464/500\n",
            "50/50 [==============================] - 0s 977us/step - loss: 5624.4038 - mse: 54682312.0000 - mae: 5624.4038 - val_loss: 3150.7700 - val_mse: 16319578.0000 - val_mae: 3150.7700\n",
            "Epoch 465/500\n",
            "50/50 [==============================] - 0s 932us/step - loss: 6584.7458 - mse: 61065400.0000 - mae: 6584.7456 - val_loss: 3159.8463 - val_mse: 16458405.0000 - val_mae: 3159.8464\n",
            "Epoch 466/500\n",
            "50/50 [==============================] - 0s 844us/step - loss: 5547.6165 - mse: 45188116.0000 - mae: 5547.6162 - val_loss: 3164.4941 - val_mse: 16512547.0000 - val_mae: 3164.4941\n",
            "Epoch 467/500\n",
            "50/50 [==============================] - 0s 939us/step - loss: 6202.8945 - mse: 59877644.0000 - mae: 6202.8950 - val_loss: 3162.6826 - val_mse: 16476195.0000 - val_mae: 3162.6829\n",
            "Epoch 468/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5984.4780 - mse: 57750504.0000 - mae: 5984.4780 - val_loss: 3151.7460 - val_mse: 16395378.0000 - val_mae: 3151.7461\n",
            "Epoch 469/500\n",
            "50/50 [==============================] - 0s 931us/step - loss: 5230.7183 - mse: 44465592.0000 - mae: 5230.7183 - val_loss: 3164.6332 - val_mse: 16469365.0000 - val_mae: 3164.6333\n",
            "Epoch 470/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6046.5789 - mse: 57044532.0000 - mae: 6046.5786 - val_loss: 3152.5451 - val_mse: 16358854.0000 - val_mae: 3152.5452\n",
            "Epoch 471/500\n",
            "50/50 [==============================] - 0s 924us/step - loss: 7193.8512 - mse: 71997984.0000 - mae: 7193.8511 - val_loss: 3169.2411 - val_mse: 16585530.0000 - val_mae: 3169.2415\n",
            "Epoch 472/500\n",
            "50/50 [==============================] - 0s 797us/step - loss: 5986.9925 - mse: 57690496.0000 - mae: 5986.9927 - val_loss: 3153.3902 - val_mse: 16372243.0000 - val_mae: 3153.3901\n",
            "Epoch 473/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5372.9794 - mse: 46432336.0000 - mae: 5372.9795 - val_loss: 3139.5748 - val_mse: 16250944.0000 - val_mae: 3139.5747\n",
            "Epoch 474/500\n",
            "50/50 [==============================] - 0s 878us/step - loss: 5522.2155 - mse: 44581564.0000 - mae: 5522.2148 - val_loss: 3122.4663 - val_mse: 16107365.0000 - val_mae: 3122.4663\n",
            "Epoch 475/500\n",
            "50/50 [==============================] - 0s 844us/step - loss: 5731.7078 - mse: 52005600.0000 - mae: 5731.7075 - val_loss: 3132.0788 - val_mse: 16183387.0000 - val_mae: 3132.0789\n",
            "Epoch 476/500\n",
            "50/50 [==============================] - 0s 850us/step - loss: 5628.8938 - mse: 47866072.0000 - mae: 5628.8936 - val_loss: 3113.2000 - val_mse: 15953232.0000 - val_mae: 3113.2000\n",
            "Epoch 477/500\n",
            "50/50 [==============================] - 0s 804us/step - loss: 6226.5495 - mse: 52395744.0000 - mae: 6226.5498 - val_loss: 3110.4914 - val_mse: 15974120.0000 - val_mae: 3110.4915\n",
            "Epoch 478/500\n",
            "50/50 [==============================] - 0s 819us/step - loss: 6946.3497 - mse: 71468824.0000 - mae: 6946.3501 - val_loss: 3104.9736 - val_mse: 15928107.0000 - val_mae: 3104.9739\n",
            "Epoch 479/500\n",
            "50/50 [==============================] - 0s 913us/step - loss: 5949.5770 - mse: 49674620.0000 - mae: 5949.5767 - val_loss: 3062.4070 - val_mse: 15502784.0000 - val_mae: 3062.4070\n",
            "Epoch 480/500\n",
            "50/50 [==============================] - 0s 782us/step - loss: 5328.5484 - mse: 52948956.0000 - mae: 5328.5488 - val_loss: 3054.2637 - val_mse: 15425587.0000 - val_mae: 3054.2637\n",
            "Epoch 481/500\n",
            "50/50 [==============================] - 0s 848us/step - loss: 5699.3156 - mse: 54413608.0000 - mae: 5699.3164 - val_loss: 3045.2502 - val_mse: 15350269.0000 - val_mae: 3045.2502\n",
            "Epoch 482/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6055.8168 - mse: 57990400.0000 - mae: 6055.8169 - val_loss: 3029.7906 - val_mse: 15145382.0000 - val_mae: 3029.7905\n",
            "Epoch 483/500\n",
            "50/50 [==============================] - 0s 833us/step - loss: 7100.5322 - mse: 71062744.0000 - mae: 7100.5327 - val_loss: 2996.8065 - val_mse: 14799760.0000 - val_mae: 2996.8066\n",
            "Epoch 484/500\n",
            "50/50 [==============================] - 0s 794us/step - loss: 6206.0270 - mse: 62688988.0000 - mae: 6206.0269 - val_loss: 3004.3399 - val_mse: 14922602.0000 - val_mae: 3004.3401\n",
            "Epoch 485/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6302.0341 - mse: 54636676.0000 - mae: 6302.0342 - val_loss: 3022.0239 - val_mse: 14967019.0000 - val_mae: 3022.0239\n",
            "Epoch 486/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 7090.2701 - mse: 72570088.0000 - mae: 7090.2700 - val_loss: 3027.2989 - val_mse: 15054789.0000 - val_mae: 3027.2988\n",
            "Epoch 487/500\n",
            "50/50 [==============================] - 0s 912us/step - loss: 5737.6119 - mse: 54227052.0000 - mae: 5737.6118 - val_loss: 3020.1477 - val_mse: 15052458.0000 - val_mae: 3020.1477\n",
            "Epoch 488/500\n",
            "50/50 [==============================] - 0s 959us/step - loss: 6062.6877 - mse: 62607028.0000 - mae: 6062.6875 - val_loss: 3011.9555 - val_mse: 14897547.0000 - val_mae: 3011.9556\n",
            "Epoch 489/500\n",
            "50/50 [==============================] - 0s 958us/step - loss: 5954.8201 - mse: 55060712.0000 - mae: 5954.8198 - val_loss: 3030.2397 - val_mse: 15115574.0000 - val_mae: 3030.2397\n",
            "Epoch 490/500\n",
            "50/50 [==============================] - 0s 810us/step - loss: 5270.7814 - mse: 42940864.0000 - mae: 5270.7812 - val_loss: 3023.7024 - val_mse: 15136130.0000 - val_mae: 3023.7026\n",
            "Epoch 491/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 7296.2965 - mse: 80132160.0000 - mae: 7296.2964 - val_loss: 3043.2731 - val_mse: 15314722.0000 - val_mae: 3043.2732\n",
            "Epoch 492/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 5149.8453 - mse: 39313540.0000 - mae: 5149.8457 - val_loss: 3050.8908 - val_mse: 15391347.0000 - val_mae: 3050.8909\n",
            "Epoch 493/500\n",
            "50/50 [==============================] - 0s 929us/step - loss: 5771.0029 - mse: 45947688.0000 - mae: 5771.0029 - val_loss: 3050.4048 - val_mse: 15350998.0000 - val_mae: 3050.4048\n",
            "Epoch 494/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6374.1348 - mse: 59698756.0000 - mae: 6374.1348 - val_loss: 3054.6703 - val_mse: 15469506.0000 - val_mae: 3054.6704\n",
            "Epoch 495/500\n",
            "50/50 [==============================] - 0s 949us/step - loss: 5829.9232 - mse: 55377212.0000 - mae: 5829.9233 - val_loss: 3074.7235 - val_mse: 15689989.0000 - val_mae: 3074.7236\n",
            "Epoch 496/500\n",
            "50/50 [==============================] - 0s 824us/step - loss: 5513.8101 - mse: 50763652.0000 - mae: 5513.8101 - val_loss: 3071.2752 - val_mse: 15657866.0000 - val_mae: 3071.2754\n",
            "Epoch 497/500\n",
            "50/50 [==============================] - 0s 899us/step - loss: 5902.0262 - mse: 57799624.0000 - mae: 5902.0264 - val_loss: 3058.3045 - val_mse: 15541237.0000 - val_mae: 3058.3047\n",
            "Epoch 498/500\n",
            "50/50 [==============================] - 0s 936us/step - loss: 5764.4374 - mse: 50617808.0000 - mae: 5764.4375 - val_loss: 3094.3551 - val_mse: 15918342.0000 - val_mae: 3094.3550\n",
            "Epoch 499/500\n",
            "50/50 [==============================] - 0s 989us/step - loss: 6257.6876 - mse: 58344740.0000 - mae: 6257.6875 - val_loss: 3091.3802 - val_mse: 15878872.0000 - val_mae: 3091.3801\n",
            "Epoch 500/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 6155.7555 - mse: 60108532.0000 - mae: 6155.7559 - val_loss: 3073.2194 - val_mse: 15704877.0000 - val_mae: 3073.2192\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60 samples, validate on 60 samples\n",
            "Epoch 1/500\n",
            "60/60 [==============================] - 2s 31ms/step - loss: 29102.6322 - mse: 994167616.0000 - mae: 29102.6328 - val_loss: 34679.7454 - val_mse: 1438888192.0000 - val_mae: 34679.7461\n",
            "Epoch 2/500\n",
            "60/60 [==============================] - 0s 712us/step - loss: 29102.3854 - mse: 994153920.0000 - mae: 29102.3848 - val_loss: 34678.9860 - val_mse: 1438840064.0000 - val_mae: 34678.9844\n",
            "Epoch 3/500\n",
            "60/60 [==============================] - 0s 645us/step - loss: 29099.8747 - mse: 994013568.0000 - mae: 29099.8750 - val_loss: 34673.4147 - val_mse: 1438478976.0000 - val_mae: 34673.4141\n",
            "Epoch 4/500\n",
            "60/60 [==============================] - 0s 713us/step - loss: 29092.4642 - mse: 993632576.0000 - mae: 29092.4629 - val_loss: 34662.8197 - val_mse: 1437768576.0000 - val_mae: 34662.8203\n",
            "Epoch 5/500\n",
            "60/60 [==============================] - 0s 651us/step - loss: 29081.5013 - mse: 992990016.0000 - mae: 29081.5020 - val_loss: 34649.5166 - val_mse: 1436861568.0000 - val_mae: 34649.5156\n",
            "Epoch 6/500\n",
            "60/60 [==============================] - 0s 667us/step - loss: 29067.4938 - mse: 992128768.0000 - mae: 29067.4922 - val_loss: 34634.4206 - val_mse: 1435822592.0000 - val_mae: 34634.4219\n",
            "Epoch 7/500\n",
            "60/60 [==============================] - 0s 742us/step - loss: 29047.5684 - mse: 990994048.0000 - mae: 29047.5703 - val_loss: 34617.0023 - val_mse: 1434623488.0000 - val_mae: 34617.0039\n",
            "Epoch 8/500\n",
            "60/60 [==============================] - 0s 719us/step - loss: 29032.4701 - mse: 990095744.0000 - mae: 29032.4707 - val_loss: 34596.7972 - val_mse: 1433233792.0000 - val_mae: 34596.7969\n",
            "Epoch 9/500\n",
            "60/60 [==============================] - 0s 725us/step - loss: 29016.5690 - mse: 989330752.0000 - mae: 29016.5664 - val_loss: 34575.8997 - val_mse: 1431795456.0000 - val_mae: 34575.8984\n",
            "Epoch 10/500\n",
            "60/60 [==============================] - 0s 742us/step - loss: 28994.0928 - mse: 987860864.0000 - mae: 28994.0918 - val_loss: 34553.7389 - val_mse: 1430266624.0000 - val_mae: 34553.7383\n",
            "Epoch 11/500\n",
            "60/60 [==============================] - 0s 715us/step - loss: 28959.7214 - mse: 985825984.0000 - mae: 28959.7207 - val_loss: 34528.4629 - val_mse: 1428524544.0000 - val_mae: 34528.4609\n",
            "Epoch 12/500\n",
            "60/60 [==============================] - 0s 675us/step - loss: 28926.4508 - mse: 984318272.0000 - mae: 28926.4492 - val_loss: 34501.2858 - val_mse: 1426652928.0000 - val_mae: 34501.2852\n",
            "Epoch 13/500\n",
            "60/60 [==============================] - 0s 659us/step - loss: 28910.0107 - mse: 982973568.0000 - mae: 28910.0098 - val_loss: 34474.5267 - val_mse: 1424809472.0000 - val_mae: 34474.5234\n",
            "Epoch 14/500\n",
            "60/60 [==============================] - 0s 672us/step - loss: 28887.6712 - mse: 981749760.0000 - mae: 28887.6738 - val_loss: 34446.7891 - val_mse: 1422899072.0000 - val_mae: 34446.7891\n",
            "Epoch 15/500\n",
            "60/60 [==============================] - 0s 727us/step - loss: 28849.7292 - mse: 979310272.0000 - mae: 28849.7285 - val_loss: 34416.7191 - val_mse: 1420828544.0000 - val_mae: 34416.7188\n",
            "Epoch 16/500\n",
            "60/60 [==============================] - 0s 770us/step - loss: 28838.4668 - mse: 979111104.0000 - mae: 28838.4668 - val_loss: 34386.2386 - val_mse: 1418732672.0000 - val_mae: 34386.2383\n",
            "Epoch 17/500\n",
            "60/60 [==============================] - 0s 735us/step - loss: 28798.2324 - mse: 977047744.0000 - mae: 28798.2324 - val_loss: 34352.8486 - val_mse: 1416439680.0000 - val_mae: 34352.8516\n",
            "Epoch 18/500\n",
            "60/60 [==============================] - 0s 873us/step - loss: 28768.0967 - mse: 974908352.0000 - mae: 28768.0957 - val_loss: 34317.2819 - val_mse: 1413997952.0000 - val_mae: 34317.2852\n",
            "Epoch 19/500\n",
            "60/60 [==============================] - 0s 789us/step - loss: 28734.9255 - mse: 972792768.0000 - mae: 28734.9258 - val_loss: 34279.0000 - val_mse: 1411372416.0000 - val_mae: 34279.0000\n",
            "Epoch 20/500\n",
            "60/60 [==============================] - 0s 647us/step - loss: 28713.3379 - mse: 971803328.0000 - mae: 28713.3398 - val_loss: 34240.8105 - val_mse: 1408757632.0000 - val_mae: 34240.8086\n",
            "Epoch 21/500\n",
            "60/60 [==============================] - 0s 794us/step - loss: 28669.0540 - mse: 968934080.0000 - mae: 28669.0566 - val_loss: 34199.6771 - val_mse: 1405942144.0000 - val_mae: 34199.6758\n",
            "Epoch 22/500\n",
            "60/60 [==============================] - 0s 905us/step - loss: 28599.0798 - mse: 964252608.0000 - mae: 28599.0801 - val_loss: 34154.5758 - val_mse: 1402858112.0000 - val_mae: 34154.5742\n",
            "Epoch 23/500\n",
            "60/60 [==============================] - 0s 783us/step - loss: 28547.7933 - mse: 962319296.0000 - mae: 28547.7910 - val_loss: 34107.4912 - val_mse: 1399644160.0000 - val_mae: 34107.4922\n",
            "Epoch 24/500\n",
            "60/60 [==============================] - 0s 833us/step - loss: 28526.9417 - mse: 960173248.0000 - mae: 28526.9414 - val_loss: 34060.3740 - val_mse: 1396432256.0000 - val_mae: 34060.3711\n",
            "Epoch 25/500\n",
            "60/60 [==============================] - 0s 829us/step - loss: 28502.8060 - mse: 959631808.0000 - mae: 28502.8066 - val_loss: 34013.2093 - val_mse: 1393221632.0000 - val_mae: 34013.2070\n",
            "Epoch 26/500\n",
            "60/60 [==============================] - 0s 703us/step - loss: 28420.0589 - mse: 954895232.0000 - mae: 28420.0605 - val_loss: 33961.7861 - val_mse: 1389725056.0000 - val_mae: 33961.7891\n",
            "Epoch 27/500\n",
            "60/60 [==============================] - 0s 703us/step - loss: 28370.9355 - mse: 950752384.0000 - mae: 28370.9355 - val_loss: 33908.4170 - val_mse: 1386101376.0000 - val_mae: 33908.4180\n",
            "Epoch 28/500\n",
            "60/60 [==============================] - 0s 830us/step - loss: 28340.4398 - mse: 950292928.0000 - mae: 28340.4395 - val_loss: 33854.9014 - val_mse: 1382475648.0000 - val_mae: 33854.8984\n",
            "Epoch 29/500\n",
            "60/60 [==============================] - 0s 728us/step - loss: 28247.8499 - mse: 944395840.0000 - mae: 28247.8477 - val_loss: 33797.6514 - val_mse: 1378601856.0000 - val_mae: 33797.6484\n",
            "Epoch 30/500\n",
            "60/60 [==============================] - 0s 696us/step - loss: 28209.4850 - mse: 942026368.0000 - mae: 28209.4863 - val_loss: 33739.7773 - val_mse: 1374692480.0000 - val_mae: 33739.7773\n",
            "Epoch 31/500\n",
            "60/60 [==============================] - 0s 763us/step - loss: 28136.6556 - mse: 938937728.0000 - mae: 28136.6562 - val_loss: 33679.0664 - val_mse: 1370599936.0000 - val_mae: 33679.0664\n",
            "Epoch 32/500\n",
            "60/60 [==============================] - 0s 818us/step - loss: 28138.7878 - mse: 940868544.0000 - mae: 28138.7871 - val_loss: 33619.5618 - val_mse: 1366595200.0000 - val_mae: 33619.5625\n",
            "Epoch 33/500\n",
            "60/60 [==============================] - 0s 705us/step - loss: 28009.4473 - mse: 930917312.0000 - mae: 28009.4453 - val_loss: 33552.6214 - val_mse: 1362098816.0000 - val_mae: 33552.6211\n",
            "Epoch 34/500\n",
            "60/60 [==============================] - 0s 616us/step - loss: 27955.4671 - mse: 929124480.0000 - mae: 27955.4688 - val_loss: 33483.9167 - val_mse: 1357492480.0000 - val_mae: 33483.9180\n",
            "Epoch 35/500\n",
            "60/60 [==============================] - 0s 699us/step - loss: 27800.6657 - mse: 921355328.0000 - mae: 27800.6660 - val_loss: 33409.1862 - val_mse: 1352493440.0000 - val_mae: 33409.1836\n",
            "Epoch 36/500\n",
            "60/60 [==============================] - 0s 713us/step - loss: 27840.6475 - mse: 923544256.0000 - mae: 27840.6484 - val_loss: 33337.7422 - val_mse: 1347724928.0000 - val_mae: 33337.7422\n",
            "Epoch 37/500\n",
            "60/60 [==============================] - 0s 650us/step - loss: 27653.6227 - mse: 910447488.0000 - mae: 27653.6211 - val_loss: 33256.5228 - val_mse: 1342315392.0000 - val_mae: 33256.5234\n",
            "Epoch 38/500\n",
            "60/60 [==============================] - 0s 752us/step - loss: 27682.4163 - mse: 914940864.0000 - mae: 27682.4180 - val_loss: 33178.5752 - val_mse: 1337137024.0000 - val_mae: 33178.5742\n",
            "Epoch 39/500\n",
            "60/60 [==============================] - 0s 691us/step - loss: 27620.7210 - mse: 910994624.0000 - mae: 27620.7207 - val_loss: 33098.8662 - val_mse: 1331854208.0000 - val_mae: 33098.8672\n",
            "Epoch 40/500\n",
            "60/60 [==============================] - 0s 725us/step - loss: 27424.9274 - mse: 899751040.0000 - mae: 27424.9277 - val_loss: 33009.9541 - val_mse: 1325976064.0000 - val_mae: 33009.9531\n",
            "Epoch 41/500\n",
            "60/60 [==============================] - 0s 714us/step - loss: 27375.0107 - mse: 898214848.0000 - mae: 27375.0098 - val_loss: 32920.1146 - val_mse: 1320052864.0000 - val_mae: 32920.1133\n",
            "Epoch 42/500\n",
            "60/60 [==============================] - 0s 657us/step - loss: 27406.7441 - mse: 898563008.0000 - mae: 27406.7441 - val_loss: 32833.5928 - val_mse: 1314363392.0000 - val_mae: 32833.5898\n",
            "Epoch 43/500\n",
            "60/60 [==============================] - 0s 664us/step - loss: 27132.1807 - mse: 884777792.0000 - mae: 27132.1816 - val_loss: 32736.2370 - val_mse: 1307979776.0000 - val_mae: 32736.2383\n",
            "Epoch 44/500\n",
            "60/60 [==============================] - 0s 582us/step - loss: 26950.2204 - mse: 873966656.0000 - mae: 26950.2188 - val_loss: 32632.9512 - val_mse: 1301228032.0000 - val_mae: 32632.9492\n",
            "Epoch 45/500\n",
            "60/60 [==============================] - 0s 616us/step - loss: 27045.4365 - mse: 878534336.0000 - mae: 27045.4355 - val_loss: 32537.1559 - val_mse: 1294984448.0000 - val_mae: 32537.1543\n",
            "Epoch 46/500\n",
            "60/60 [==============================] - 0s 676us/step - loss: 26905.8320 - mse: 871251520.0000 - mae: 26905.8301 - val_loss: 32436.9242 - val_mse: 1288471296.0000 - val_mae: 32436.9258\n",
            "Epoch 47/500\n",
            "60/60 [==============================] - 0s 687us/step - loss: 26666.9567 - mse: 864569600.0000 - mae: 26666.9590 - val_loss: 32329.1729 - val_mse: 1281492224.0000 - val_mae: 32329.1758\n",
            "Epoch 48/500\n",
            "60/60 [==============================] - 0s 658us/step - loss: 26610.0892 - mse: 856399424.0000 - mae: 26610.0879 - val_loss: 32221.2279 - val_mse: 1274523392.0000 - val_mae: 32221.2285\n",
            "Epoch 49/500\n",
            "60/60 [==============================] - 0s 722us/step - loss: 26625.8324 - mse: 857238400.0000 - mae: 26625.8340 - val_loss: 32116.7461 - val_mse: 1267800320.0000 - val_mae: 32116.7461\n",
            "Epoch 50/500\n",
            "60/60 [==============================] - 0s 722us/step - loss: 26482.5739 - mse: 851462272.0000 - mae: 26482.5742 - val_loss: 32008.3226 - val_mse: 1260847232.0000 - val_mae: 32008.3242\n",
            "Epoch 51/500\n",
            "60/60 [==============================] - 0s 721us/step - loss: 26486.9342 - mse: 852766336.0000 - mae: 26486.9336 - val_loss: 31901.8512 - val_mse: 1254042496.0000 - val_mae: 31901.8535\n",
            "Epoch 52/500\n",
            "60/60 [==============================] - 0s 807us/step - loss: 26269.5065 - mse: 837491392.0000 - mae: 26269.5039 - val_loss: 31789.3444 - val_mse: 1246876032.0000 - val_mae: 31789.3438\n",
            "Epoch 53/500\n",
            "60/60 [==============================] - 0s 782us/step - loss: 26257.5902 - mse: 838718720.0000 - mae: 26257.5898 - val_loss: 31677.4206 - val_mse: 1239771904.0000 - val_mae: 31677.4199\n",
            "Epoch 54/500\n",
            "60/60 [==============================] - 0s 694us/step - loss: 26080.5505 - mse: 833731136.0000 - mae: 26080.5508 - val_loss: 31561.6169 - val_mse: 1232448384.0000 - val_mae: 31561.6172\n",
            "Epoch 55/500\n",
            "60/60 [==============================] - 0s 785us/step - loss: 26050.9635 - mse: 829047680.0000 - mae: 26050.9648 - val_loss: 31447.1530 - val_mse: 1225235968.0000 - val_mae: 31447.1543\n",
            "Epoch 56/500\n",
            "60/60 [==============================] - 0s 834us/step - loss: 25891.1706 - mse: 818398528.0000 - mae: 25891.1699 - val_loss: 31328.0540 - val_mse: 1217758720.0000 - val_mae: 31328.0547\n",
            "Epoch 57/500\n",
            "60/60 [==============================] - 0s 694us/step - loss: 25714.7529 - mse: 806483520.0000 - mae: 25714.7539 - val_loss: 31205.5482 - val_mse: 1210096768.0000 - val_mae: 31205.5508\n",
            "Epoch 58/500\n",
            "60/60 [==============================] - 0s 784us/step - loss: 25698.4596 - mse: 807127744.0000 - mae: 25698.4609 - val_loss: 31083.9424 - val_mse: 1202521344.0000 - val_mae: 31083.9414\n",
            "Epoch 59/500\n",
            "60/60 [==============================] - 0s 714us/step - loss: 25458.6774 - mse: 795610432.0000 - mae: 25458.6777 - val_loss: 30956.1035 - val_mse: 1194589440.0000 - val_mae: 30956.1035\n",
            "Epoch 60/500\n",
            "60/60 [==============================] - 0s 749us/step - loss: 25263.8932 - mse: 788319680.0000 - mae: 25263.8945 - val_loss: 30824.2165 - val_mse: 1186440576.0000 - val_mae: 30824.2168\n",
            "Epoch 61/500\n",
            "60/60 [==============================] - 0s 640us/step - loss: 25085.3125 - mse: 784443392.0000 - mae: 25085.3125 - val_loss: 30688.6003 - val_mse: 1178097920.0000 - val_mae: 30688.5996\n",
            "Epoch 62/500\n",
            "60/60 [==============================] - 0s 760us/step - loss: 25152.5693 - mse: 778011264.0000 - mae: 25152.5684 - val_loss: 30557.0514 - val_mse: 1170040192.0000 - val_mae: 30557.0508\n",
            "Epoch 63/500\n",
            "60/60 [==============================] - 0s 707us/step - loss: 25054.0241 - mse: 779717632.0000 - mae: 25054.0254 - val_loss: 30423.9290 - val_mse: 1161922048.0000 - val_mae: 30423.9297\n",
            "Epoch 64/500\n",
            "60/60 [==============================] - 0s 687us/step - loss: 24605.4408 - mse: 753847424.0000 - mae: 24605.4414 - val_loss: 30279.6735 - val_mse: 1153164672.0000 - val_mae: 30279.6738\n",
            "Epoch 65/500\n",
            "60/60 [==============================] - 0s 716us/step - loss: 24716.3389 - mse: 759568064.0000 - mae: 24716.3379 - val_loss: 30141.1745 - val_mse: 1144795648.0000 - val_mae: 30141.1758\n",
            "Epoch 66/500\n",
            "60/60 [==============================] - 0s 660us/step - loss: 24461.7223 - mse: 741883328.0000 - mae: 24461.7207 - val_loss: 29996.0309 - val_mse: 1136066432.0000 - val_mae: 29996.0312\n",
            "Epoch 67/500\n",
            "60/60 [==============================] - 0s 659us/step - loss: 24581.1237 - mse: 748168256.0000 - mae: 24581.1211 - val_loss: 29856.8177 - val_mse: 1127733120.0000 - val_mae: 29856.8164\n",
            "Epoch 68/500\n",
            "60/60 [==============================] - 0s 658us/step - loss: 24055.4443 - mse: 729869120.0000 - mae: 24055.4434 - val_loss: 29703.9688 - val_mse: 1118628864.0000 - val_mae: 29703.9668\n",
            "Epoch 69/500\n",
            "60/60 [==============================] - 0s 638us/step - loss: 24146.8141 - mse: 736462912.0000 - mae: 24146.8145 - val_loss: 29556.9993 - val_mse: 1109918720.0000 - val_mae: 29557.0000\n",
            "Epoch 70/500\n",
            "60/60 [==============================] - 0s 718us/step - loss: 23755.5964 - mse: 712392320.0000 - mae: 23755.5957 - val_loss: 29400.3786 - val_mse: 1100684160.0000 - val_mae: 29400.3789\n",
            "Epoch 71/500\n",
            "60/60 [==============================] - 0s 645us/step - loss: 23434.3604 - mse: 692666624.0000 - mae: 23434.3613 - val_loss: 29237.7111 - val_mse: 1091144832.0000 - val_mae: 29237.7109\n",
            "Epoch 72/500\n",
            "60/60 [==============================] - 0s 687us/step - loss: 23842.9163 - mse: 720017024.0000 - mae: 23842.9160 - val_loss: 29089.0146 - val_mse: 1082471424.0000 - val_mae: 29089.0137\n",
            "Epoch 73/500\n",
            "60/60 [==============================] - 0s 636us/step - loss: 23637.2855 - mse: 708393216.0000 - mae: 23637.2852 - val_loss: 28936.3716 - val_mse: 1073613568.0000 - val_mae: 28936.3711\n",
            "Epoch 74/500\n",
            "60/60 [==============================] - 0s 736us/step - loss: 23454.7350 - mse: 701756672.0000 - mae: 23454.7324 - val_loss: 28780.4823 - val_mse: 1064615232.0000 - val_mae: 28780.4824\n",
            "Epoch 75/500\n",
            "60/60 [==============================] - 0s 672us/step - loss: 23187.7777 - mse: 680714176.0000 - mae: 23187.7773 - val_loss: 28619.0197 - val_mse: 1055346688.0000 - val_mae: 28619.0195\n",
            "Epoch 76/500\n",
            "60/60 [==============================] - 0s 760us/step - loss: 22842.0405 - mse: 671730368.0000 - mae: 22842.0410 - val_loss: 28450.9373 - val_mse: 1045753600.0000 - val_mae: 28450.9375\n",
            "Epoch 77/500\n",
            "60/60 [==============================] - 0s 651us/step - loss: 23314.6367 - mse: 692539520.0000 - mae: 23314.6348 - val_loss: 28297.2767 - val_mse: 1037033088.0000 - val_mae: 28297.2793\n",
            "Epoch 78/500\n",
            "60/60 [==============================] - 0s 739us/step - loss: 22758.4056 - mse: 662132864.0000 - mae: 22758.4062 - val_loss: 28131.2137 - val_mse: 1027661568.0000 - val_mae: 28131.2129\n",
            "Epoch 79/500\n",
            "60/60 [==============================] - 0s 682us/step - loss: 22499.0130 - mse: 656803456.0000 - mae: 22499.0137 - val_loss: 27960.8726 - val_mse: 1018106048.0000 - val_mae: 27960.8730\n",
            "Epoch 80/500\n",
            "60/60 [==============================] - 0s 683us/step - loss: 22099.5228 - mse: 637187008.0000 - mae: 22099.5234 - val_loss: 27781.7834 - val_mse: 1008122816.0000 - val_mae: 27781.7832\n",
            "Epoch 81/500\n",
            "60/60 [==============================] - 0s 704us/step - loss: 22408.2083 - mse: 664635968.0000 - mae: 22408.2070 - val_loss: 27613.0614 - val_mse: 998776000.0000 - val_mae: 27613.0625\n",
            "Epoch 82/500\n",
            "60/60 [==============================] - 0s 687us/step - loss: 22067.8747 - mse: 642744064.0000 - mae: 22067.8750 - val_loss: 27438.5801 - val_mse: 989170432.0000 - val_mae: 27438.5801\n",
            "Epoch 83/500\n",
            "60/60 [==============================] - 0s 821us/step - loss: 21604.1224 - mse: 628354752.0000 - mae: 21604.1211 - val_loss: 27254.8407 - val_mse: 979120512.0000 - val_mae: 27254.8398\n",
            "Epoch 84/500\n",
            "60/60 [==============================] - 0s 633us/step - loss: 21878.6455 - mse: 632461504.0000 - mae: 21878.6445 - val_loss: 27079.1156 - val_mse: 969572224.0000 - val_mae: 27079.1172\n",
            "Epoch 85/500\n",
            "60/60 [==============================] - 0s 622us/step - loss: 20820.7894 - mse: 592577408.0000 - mae: 20820.7910 - val_loss: 26881.8013 - val_mse: 958924480.0000 - val_mae: 26881.8027\n",
            "Epoch 86/500\n",
            "60/60 [==============================] - 0s 705us/step - loss: 21364.7018 - mse: 605816000.0000 - mae: 21364.7012 - val_loss: 26698.9919 - val_mse: 949128832.0000 - val_mae: 26698.9922\n",
            "Epoch 87/500\n",
            "60/60 [==============================] - 0s 719us/step - loss: 20916.4590 - mse: 592525760.0000 - mae: 20916.4590 - val_loss: 26508.4282 - val_mse: 938988800.0000 - val_mae: 26508.4277\n",
            "Epoch 88/500\n",
            "60/60 [==============================] - 0s 717us/step - loss: 20485.2676 - mse: 565968192.0000 - mae: 20485.2676 - val_loss: 26310.9390 - val_mse: 928556928.0000 - val_mae: 26310.9375\n",
            "Epoch 89/500\n",
            "60/60 [==============================] - 0s 708us/step - loss: 20070.3879 - mse: 566277888.0000 - mae: 20070.3867 - val_loss: 26106.3913 - val_mse: 917834752.0000 - val_mae: 26106.3926\n",
            "Epoch 90/500\n",
            "60/60 [==============================] - 0s 685us/step - loss: 20412.5270 - mse: 562238144.0000 - mae: 20412.5273 - val_loss: 25911.6217 - val_mse: 907702272.0000 - val_mae: 25911.6211\n",
            "Epoch 91/500\n",
            "60/60 [==============================] - 0s 823us/step - loss: 20084.4823 - mse: 558834688.0000 - mae: 20084.4824 - val_loss: 25711.5648 - val_mse: 897374336.0000 - val_mae: 25711.5645\n",
            "Epoch 92/500\n",
            "60/60 [==============================] - 0s 816us/step - loss: 20447.9170 - mse: 568884992.0000 - mae: 20447.9160 - val_loss: 25522.1935 - val_mse: 887671488.0000 - val_mae: 25522.1914\n",
            "Epoch 93/500\n",
            "60/60 [==============================] - 0s 839us/step - loss: 19396.5140 - mse: 533991616.0000 - mae: 19396.5117 - val_loss: 25312.1458 - val_mse: 876993216.0000 - val_mae: 25312.1465\n",
            "Epoch 94/500\n",
            "60/60 [==============================] - 0s 834us/step - loss: 19820.4085 - mse: 539104704.0000 - mae: 19820.4082 - val_loss: 25112.6872 - val_mse: 866935040.0000 - val_mae: 25112.6875\n",
            "Epoch 95/500\n",
            "60/60 [==============================] - 0s 701us/step - loss: 19454.6826 - mse: 542890368.0000 - mae: 19454.6836 - val_loss: 24908.1051 - val_mse: 856701248.0000 - val_mae: 24908.1035\n",
            "Epoch 96/500\n",
            "60/60 [==============================] - 0s 859us/step - loss: 18628.5239 - mse: 488979776.0000 - mae: 18628.5234 - val_loss: 24689.7629 - val_mse: 845871232.0000 - val_mae: 24689.7617\n",
            "Epoch 97/500\n",
            "60/60 [==============================] - 0s 789us/step - loss: 18569.1307 - mse: 497244096.0000 - mae: 18569.1309 - val_loss: 24479.9167 - val_mse: 835552448.0000 - val_mae: 24479.9160\n",
            "Epoch 98/500\n",
            "60/60 [==============================] - 0s 757us/step - loss: 18633.6968 - mse: 507008736.0000 - mae: 18633.6973 - val_loss: 24277.0918 - val_mse: 825662592.0000 - val_mae: 24277.0918\n",
            "Epoch 99/500\n",
            "60/60 [==============================] - 0s 735us/step - loss: 18507.1325 - mse: 495003264.0000 - mae: 18507.1328 - val_loss: 24070.1354 - val_mse: 815656192.0000 - val_mae: 24070.1348\n",
            "Epoch 100/500\n",
            "60/60 [==============================] - 0s 799us/step - loss: 18110.0070 - mse: 494987104.0000 - mae: 18110.0078 - val_loss: 23848.9207 - val_mse: 805055296.0000 - val_mae: 23848.9199\n",
            "Epoch 101/500\n",
            "60/60 [==============================] - 0s 718us/step - loss: 18592.1035 - mse: 487159168.0000 - mae: 18592.1035 - val_loss: 23646.8761 - val_mse: 795458496.0000 - val_mae: 23646.8750\n",
            "Epoch 102/500\n",
            "60/60 [==============================] - 0s 697us/step - loss: 17826.4575 - mse: 477269920.0000 - mae: 17826.4590 - val_loss: 23440.8631 - val_mse: 785757248.0000 - val_mae: 23440.8633\n",
            "Epoch 103/500\n",
            "60/60 [==============================] - 0s 681us/step - loss: 18590.8003 - mse: 515155232.0000 - mae: 18590.8008 - val_loss: 23251.1473 - val_mse: 776898688.0000 - val_mae: 23251.1484\n",
            "Epoch 104/500\n",
            "60/60 [==============================] - 0s 776us/step - loss: 17665.3584 - mse: 464754240.0000 - mae: 17665.3574 - val_loss: 23045.6473 - val_mse: 767384128.0000 - val_mae: 23045.6465\n",
            "Epoch 105/500\n",
            "60/60 [==============================] - 0s 846us/step - loss: 17168.1501 - mse: 439295936.0000 - mae: 17168.1484 - val_loss: 22832.0788 - val_mse: 757585472.0000 - val_mae: 22832.0801\n",
            "Epoch 106/500\n",
            "60/60 [==============================] - 0s 781us/step - loss: 17930.6344 - mse: 448546912.0000 - mae: 17930.6348 - val_loss: 22626.4727 - val_mse: 748238400.0000 - val_mae: 22626.4727\n",
            "Epoch 107/500\n",
            "60/60 [==============================] - 0s 763us/step - loss: 17609.0226 - mse: 449083680.0000 - mae: 17609.0215 - val_loss: 22423.2946 - val_mse: 739084672.0000 - val_mae: 22423.2949\n",
            "Epoch 108/500\n",
            "60/60 [==============================] - 0s 784us/step - loss: 17085.0418 - mse: 460951360.0000 - mae: 17085.0410 - val_loss: 22212.3883 - val_mse: 729670464.0000 - val_mae: 22212.3867\n",
            "Epoch 109/500\n",
            "60/60 [==============================] - 0s 689us/step - loss: 16812.5140 - mse: 433650784.0000 - mae: 16812.5137 - val_loss: 21990.1535 - val_mse: 719846592.0000 - val_mae: 21990.1543\n",
            "Epoch 110/500\n",
            "60/60 [==============================] - 0s 754us/step - loss: 17085.2160 - mse: 461863136.0000 - mae: 17085.2148 - val_loss: 21778.2192 - val_mse: 710570304.0000 - val_mae: 21778.2188\n",
            "Epoch 111/500\n",
            "60/60 [==============================] - 0s 656us/step - loss: 15560.4269 - mse: 403066400.0000 - mae: 15560.4277 - val_loss: 21553.7218 - val_mse: 700841984.0000 - val_mae: 21553.7227\n",
            "Epoch 112/500\n",
            "60/60 [==============================] - 0s 760us/step - loss: 15911.1659 - mse: 414262496.0000 - mae: 15911.1670 - val_loss: 21310.1619 - val_mse: 690401600.0000 - val_mae: 21310.1621\n",
            "Epoch 113/500\n",
            "60/60 [==============================] - 0s 747us/step - loss: 15984.4305 - mse: 396110240.0000 - mae: 15984.4316 - val_loss: 21060.4631 - val_mse: 679821376.0000 - val_mae: 21060.4629\n",
            "Epoch 114/500\n",
            "60/60 [==============================] - 0s 767us/step - loss: 15955.6471 - mse: 425763968.0000 - mae: 15955.6475 - val_loss: 20828.8831 - val_mse: 670120512.0000 - val_mae: 20828.8828\n",
            "Epoch 115/500\n",
            "60/60 [==============================] - 0s 833us/step - loss: 15754.3446 - mse: 401919040.0000 - mae: 15754.3438 - val_loss: 20596.1218 - val_mse: 660477952.0000 - val_mae: 20596.1230\n",
            "Epoch 116/500\n",
            "60/60 [==============================] - 0s 883us/step - loss: 15407.1227 - mse: 396624704.0000 - mae: 15407.1230 - val_loss: 20369.5009 - val_mse: 651193984.0000 - val_mae: 20369.5000\n",
            "Epoch 117/500\n",
            "60/60 [==============================] - 0s 784us/step - loss: 15605.4049 - mse: 375754848.0000 - mae: 15605.4043 - val_loss: 20139.0776 - val_mse: 641859392.0000 - val_mae: 20139.0762\n",
            "Epoch 118/500\n",
            "60/60 [==============================] - 0s 729us/step - loss: 14257.3960 - mse: 336983360.0000 - mae: 14257.3955 - val_loss: 19875.6218 - val_mse: 631316928.0000 - val_mae: 19875.6211\n",
            "Epoch 119/500\n",
            "60/60 [==============================] - 0s 711us/step - loss: 14895.3070 - mse: 385460576.0000 - mae: 14895.3076 - val_loss: 19642.4601 - val_mse: 622102528.0000 - val_mae: 19642.4609\n",
            "Epoch 120/500\n",
            "60/60 [==============================] - 0s 831us/step - loss: 14388.8353 - mse: 333599776.0000 - mae: 14388.8350 - val_loss: 19420.5484 - val_mse: 613433344.0000 - val_mae: 19420.5488\n",
            "Epoch 121/500\n",
            "60/60 [==============================] - 0s 769us/step - loss: 13806.9469 - mse: 325965728.0000 - mae: 13806.9482 - val_loss: 19181.2992 - val_mse: 604197312.0000 - val_mae: 19181.3008\n",
            "Epoch 122/500\n",
            "60/60 [==============================] - 0s 772us/step - loss: 14400.9062 - mse: 362571424.0000 - mae: 14400.9062 - val_loss: 18960.1667 - val_mse: 595762624.0000 - val_mae: 18960.1660\n",
            "Epoch 123/500\n",
            "60/60 [==============================] - 0s 793us/step - loss: 13632.4354 - mse: 328314048.0000 - mae: 13632.4355 - val_loss: 18720.6200 - val_mse: 586735872.0000 - val_mae: 18720.6211\n",
            "Epoch 124/500\n",
            "60/60 [==============================] - 0s 755us/step - loss: 13663.1548 - mse: 338016256.0000 - mae: 13663.1543 - val_loss: 18481.0651 - val_mse: 577823616.0000 - val_mae: 18481.0664\n",
            "Epoch 125/500\n",
            "60/60 [==============================] - 0s 845us/step - loss: 13217.4171 - mse: 298755616.0000 - mae: 13217.4180 - val_loss: 18252.3092 - val_mse: 569420352.0000 - val_mae: 18252.3105\n",
            "Epoch 126/500\n",
            "60/60 [==============================] - 0s 820us/step - loss: 12883.2306 - mse: 303944384.0000 - mae: 12883.2314 - val_loss: 18038.4332 - val_mse: 561657984.0000 - val_mae: 18038.4336\n",
            "Epoch 127/500\n",
            "60/60 [==============================] - 0s 679us/step - loss: 13835.8285 - mse: 327730656.0000 - mae: 13835.8291 - val_loss: 17820.0781 - val_mse: 553827584.0000 - val_mae: 17820.0762\n",
            "Epoch 128/500\n",
            "60/60 [==============================] - 0s 716us/step - loss: 11954.8387 - mse: 279701504.0000 - mae: 11954.8389 - val_loss: 17603.4061 - val_mse: 546151808.0000 - val_mae: 17603.4043\n",
            "Epoch 129/500\n",
            "60/60 [==============================] - 0s 717us/step - loss: 12296.0084 - mse: 310604384.0000 - mae: 12296.0088 - val_loss: 17383.5489 - val_mse: 538459392.0000 - val_mae: 17383.5488\n",
            "Epoch 130/500\n",
            "60/60 [==============================] - 0s 725us/step - loss: 12010.1854 - mse: 290718080.0000 - mae: 12010.1855 - val_loss: 17181.4458 - val_mse: 531473248.0000 - val_mae: 17181.4453\n",
            "Epoch 131/500\n",
            "60/60 [==============================] - 0s 774us/step - loss: 12809.2982 - mse: 330011040.0000 - mae: 12809.2979 - val_loss: 17011.0252 - val_mse: 525645952.0000 - val_mae: 17011.0254\n",
            "Epoch 132/500\n",
            "60/60 [==============================] - 0s 768us/step - loss: 11624.3099 - mse: 282346112.0000 - mae: 11624.3105 - val_loss: 16775.9053 - val_mse: 517701632.0000 - val_mae: 16775.9062\n",
            "Epoch 133/500\n",
            "60/60 [==============================] - 0s 689us/step - loss: 12528.7412 - mse: 295884768.0000 - mae: 12528.7412 - val_loss: 16556.8011 - val_mse: 510397856.0000 - val_mae: 16556.8027\n",
            "Epoch 134/500\n",
            "60/60 [==============================] - 0s 750us/step - loss: 12035.0381 - mse: 271977056.0000 - mae: 12035.0371 - val_loss: 16299.7460 - val_mse: 501951360.0000 - val_mae: 16299.7461\n",
            "Epoch 135/500\n",
            "60/60 [==============================] - 0s 706us/step - loss: 11835.4941 - mse: 282056096.0000 - mae: 11835.4941 - val_loss: 16126.7067 - val_mse: 496339840.0000 - val_mae: 16126.7061\n",
            "Epoch 136/500\n",
            "60/60 [==============================] - 0s 684us/step - loss: 10976.0518 - mse: 242312096.0000 - mae: 10976.0518 - val_loss: 15920.9867 - val_mse: 489746336.0000 - val_mae: 15920.9873\n",
            "Epoch 137/500\n",
            "60/60 [==============================] - 0s 901us/step - loss: 10607.6417 - mse: 227259696.0000 - mae: 10607.6416 - val_loss: 15747.6665 - val_mse: 484256704.0000 - val_mae: 15747.6670\n",
            "Epoch 138/500\n",
            "60/60 [==============================] - 0s 722us/step - loss: 11661.7085 - mse: 305067968.0000 - mae: 11661.7080 - val_loss: 15640.5643 - val_mse: 480894304.0000 - val_mae: 15640.5645\n",
            "Epoch 139/500\n",
            "60/60 [==============================] - 0s 807us/step - loss: 11373.9624 - mse: 278210432.0000 - mae: 11373.9629 - val_loss: 15463.6136 - val_mse: 475389824.0000 - val_mae: 15463.6133\n",
            "Epoch 140/500\n",
            "60/60 [==============================] - 0s 854us/step - loss: 11272.2863 - mse: 239520096.0000 - mae: 11272.2861 - val_loss: 15288.1409 - val_mse: 469993408.0000 - val_mae: 15288.1416\n",
            "Epoch 141/500\n",
            "60/60 [==============================] - 0s 810us/step - loss: 10537.9565 - mse: 257773920.0000 - mae: 10537.9561 - val_loss: 15132.7299 - val_mse: 465265152.0000 - val_mae: 15132.7295\n",
            "Epoch 142/500\n",
            "60/60 [==============================] - 0s 793us/step - loss: 12499.5430 - mse: 293291808.0000 - mae: 12499.5430 - val_loss: 14955.3010 - val_mse: 459926400.0000 - val_mae: 14955.3008\n",
            "Epoch 143/500\n",
            "60/60 [==============================] - 0s 875us/step - loss: 11913.5767 - mse: 301955488.0000 - mae: 11913.5762 - val_loss: 14786.2777 - val_mse: 454898784.0000 - val_mae: 14786.2773\n",
            "Epoch 144/500\n",
            "60/60 [==============================] - 0s 784us/step - loss: 10994.7896 - mse: 263582752.0000 - mae: 10994.7900 - val_loss: 14608.0767 - val_mse: 449660480.0000 - val_mae: 14608.0762\n",
            "Epoch 145/500\n",
            "60/60 [==============================] - 0s 883us/step - loss: 10547.1222 - mse: 244066384.0000 - mae: 10547.1230 - val_loss: 14529.8793 - val_mse: 447381472.0000 - val_mae: 14529.8789\n",
            "Epoch 146/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 11084.8856 - mse: 240977728.0000 - mae: 11084.8857 - val_loss: 14402.4684 - val_mse: 443694720.0000 - val_mae: 14402.4688\n",
            "Epoch 147/500\n",
            "60/60 [==============================] - 0s 737us/step - loss: 10376.8511 - mse: 224012320.0000 - mae: 10376.8506 - val_loss: 14194.0296 - val_mse: 437733856.0000 - val_mae: 14194.0293\n",
            "Epoch 148/500\n",
            "60/60 [==============================] - 0s 824us/step - loss: 11327.8091 - mse: 271402752.0000 - mae: 11327.8096 - val_loss: 14125.7514 - val_mse: 435799840.0000 - val_mae: 14125.7510\n",
            "Epoch 149/500\n",
            "60/60 [==============================] - 0s 794us/step - loss: 10942.8133 - mse: 248650544.0000 - mae: 10942.8135 - val_loss: 14013.6162 - val_mse: 432643968.0000 - val_mae: 14013.6162\n",
            "Epoch 150/500\n",
            "60/60 [==============================] - 0s 774us/step - loss: 9922.7842 - mse: 233902112.0000 - mae: 9922.7842 - val_loss: 13894.2319 - val_mse: 429311904.0000 - val_mae: 13894.2314\n",
            "Epoch 151/500\n",
            "60/60 [==============================] - 0s 726us/step - loss: 10454.5826 - mse: 224207840.0000 - mae: 10454.5820 - val_loss: 13753.8693 - val_mse: 425430880.0000 - val_mae: 13753.8701\n",
            "Epoch 152/500\n",
            "60/60 [==============================] - 0s 799us/step - loss: 9462.7684 - mse: 194999392.0000 - mae: 9462.7686 - val_loss: 13641.0362 - val_mse: 422243392.0000 - val_mae: 13641.0352\n",
            "Epoch 153/500\n",
            "60/60 [==============================] - 0s 752us/step - loss: 11185.3207 - mse: 244310944.0000 - mae: 11185.3193 - val_loss: 13523.2923 - val_mse: 418817984.0000 - val_mae: 13523.2920\n",
            "Epoch 154/500\n",
            "60/60 [==============================] - 0s 689us/step - loss: 10629.7633 - mse: 232682320.0000 - mae: 10629.7637 - val_loss: 13416.5986 - val_mse: 415670560.0000 - val_mae: 13416.5986\n",
            "Epoch 155/500\n",
            "60/60 [==============================] - 0s 843us/step - loss: 10662.1617 - mse: 243185856.0000 - mae: 10662.1611 - val_loss: 13319.9975 - val_mse: 412741536.0000 - val_mae: 13319.9980\n",
            "Epoch 156/500\n",
            "60/60 [==============================] - 0s 786us/step - loss: 10450.1429 - mse: 245299104.0000 - mae: 10450.1426 - val_loss: 13268.7442 - val_mse: 411123904.0000 - val_mae: 13268.7441\n",
            "Epoch 157/500\n",
            "60/60 [==============================] - 0s 749us/step - loss: 10470.9992 - mse: 214973152.0000 - mae: 10470.9990 - val_loss: 13183.5754 - val_mse: 408430720.0000 - val_mae: 13183.5752\n",
            "Epoch 158/500\n",
            "60/60 [==============================] - 0s 707us/step - loss: 10579.8564 - mse: 268163952.0000 - mae: 10579.8564 - val_loss: 13137.6583 - val_mse: 406987552.0000 - val_mae: 13137.6582\n",
            "Epoch 159/500\n",
            "60/60 [==============================] - 0s 778us/step - loss: 11249.4128 - mse: 241858656.0000 - mae: 11249.4131 - val_loss: 13029.8675 - val_mse: 403462080.0000 - val_mae: 13029.8662\n",
            "Epoch 160/500\n",
            "60/60 [==============================] - 0s 791us/step - loss: 10479.1945 - mse: 222851104.0000 - mae: 10479.1943 - val_loss: 12959.6841 - val_mse: 401102560.0000 - val_mae: 12959.6846\n",
            "Epoch 161/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 9688.9564 - mse: 217492640.0000 - mae: 9688.9561 - val_loss: 12876.6869 - val_mse: 398200416.0000 - val_mae: 12876.6865\n",
            "Epoch 162/500\n",
            "60/60 [==============================] - 0s 773us/step - loss: 10049.6965 - mse: 206312448.0000 - mae: 10049.6963 - val_loss: 12806.4802 - val_mse: 395601248.0000 - val_mae: 12806.4805\n",
            "Epoch 163/500\n",
            "60/60 [==============================] - 0s 953us/step - loss: 10487.3958 - mse: 201066448.0000 - mae: 10487.3955 - val_loss: 12749.5629 - val_mse: 393339040.0000 - val_mae: 12749.5625\n",
            "Epoch 164/500\n",
            "60/60 [==============================] - 0s 930us/step - loss: 9997.2463 - mse: 206372432.0000 - mae: 9997.2471 - val_loss: 12676.9546 - val_mse: 390225248.0000 - val_mae: 12676.9541\n",
            "Epoch 165/500\n",
            "60/60 [==============================] - 0s 745us/step - loss: 11278.4225 - mse: 255429392.0000 - mae: 11278.4229 - val_loss: 12653.1938 - val_mse: 389171392.0000 - val_mae: 12653.1934\n",
            "Epoch 166/500\n",
            "60/60 [==============================] - 0s 914us/step - loss: 10722.7138 - mse: 223156224.0000 - mae: 10722.7139 - val_loss: 12608.5876 - val_mse: 387109440.0000 - val_mae: 12608.5879\n",
            "Epoch 167/500\n",
            "60/60 [==============================] - 0s 792us/step - loss: 10445.9775 - mse: 232472464.0000 - mae: 10445.9775 - val_loss: 12572.6894 - val_mse: 385459200.0000 - val_mae: 12572.6895\n",
            "Epoch 168/500\n",
            "60/60 [==============================] - 0s 757us/step - loss: 9855.2534 - mse: 215342224.0000 - mae: 9855.2529 - val_loss: 12513.2078 - val_mse: 382477472.0000 - val_mae: 12513.2080\n",
            "Epoch 169/500\n",
            "60/60 [==============================] - 0s 771us/step - loss: 10189.4521 - mse: 214830432.0000 - mae: 10189.4521 - val_loss: 12498.2771 - val_mse: 381704416.0000 - val_mae: 12498.2773\n",
            "Epoch 170/500\n",
            "60/60 [==============================] - 0s 809us/step - loss: 11099.9819 - mse: 266842528.0000 - mae: 11099.9814 - val_loss: 12451.1864 - val_mse: 379280320.0000 - val_mae: 12451.1865\n",
            "Epoch 171/500\n",
            "60/60 [==============================] - 0s 889us/step - loss: 10379.3332 - mse: 210854256.0000 - mae: 10379.3330 - val_loss: 12414.3949 - val_mse: 377400384.0000 - val_mae: 12414.3945\n",
            "Epoch 172/500\n",
            "60/60 [==============================] - 0s 825us/step - loss: 10610.4861 - mse: 228264144.0000 - mae: 10610.4873 - val_loss: 12386.3864 - val_mse: 375977536.0000 - val_mae: 12386.3867\n",
            "Epoch 173/500\n",
            "60/60 [==============================] - 0s 814us/step - loss: 9892.5013 - mse: 211006720.0000 - mae: 9892.5010 - val_loss: 12356.6393 - val_mse: 374474560.0000 - val_mae: 12356.6396\n",
            "Epoch 174/500\n",
            "60/60 [==============================] - 0s 840us/step - loss: 10010.1122 - mse: 195716416.0000 - mae: 10010.1123 - val_loss: 12328.8649 - val_mse: 372972160.0000 - val_mae: 12328.8643\n",
            "Epoch 175/500\n",
            "60/60 [==============================] - 0s 792us/step - loss: 11260.4852 - mse: 250318784.0000 - mae: 11260.4844 - val_loss: 12293.6206 - val_mse: 370919232.0000 - val_mae: 12293.6211\n",
            "Epoch 176/500\n",
            "60/60 [==============================] - 0s 858us/step - loss: 11126.1976 - mse: 249099872.0000 - mae: 11126.1973 - val_loss: 12299.9044 - val_mse: 371283840.0000 - val_mae: 12299.9043\n",
            "Epoch 177/500\n",
            "60/60 [==============================] - 0s 832us/step - loss: 10251.5720 - mse: 222617872.0000 - mae: 10251.5723 - val_loss: 12271.5105 - val_mse: 369639008.0000 - val_mae: 12271.5107\n",
            "Epoch 178/500\n",
            "60/60 [==============================] - 0s 806us/step - loss: 10170.1750 - mse: 213348672.0000 - mae: 10170.1748 - val_loss: 12235.4169 - val_mse: 367318016.0000 - val_mae: 12235.4170\n",
            "Epoch 179/500\n",
            "60/60 [==============================] - 0s 872us/step - loss: 10134.5660 - mse: 208342032.0000 - mae: 10134.5654 - val_loss: 12232.6117 - val_mse: 367125216.0000 - val_mae: 12232.6113\n",
            "Epoch 180/500\n",
            "60/60 [==============================] - 0s 794us/step - loss: 10788.1222 - mse: 224190736.0000 - mae: 10788.1230 - val_loss: 12253.3771 - val_mse: 368528192.0000 - val_mae: 12253.3770\n",
            "Epoch 181/500\n",
            "60/60 [==============================] - 0s 790us/step - loss: 10520.1752 - mse: 231082400.0000 - mae: 10520.1748 - val_loss: 12234.0629 - val_mse: 367224512.0000 - val_mae: 12234.0625\n",
            "Epoch 182/500\n",
            "60/60 [==============================] - 0s 841us/step - loss: 10286.3086 - mse: 238683248.0000 - mae: 10286.3096 - val_loss: 12217.9186 - val_mse: 366118336.0000 - val_mae: 12217.9189\n",
            "Epoch 183/500\n",
            "60/60 [==============================] - 0s 718us/step - loss: 9124.7679 - mse: 186574176.0000 - mae: 9124.7676 - val_loss: 12197.8856 - val_mse: 364751936.0000 - val_mae: 12197.8857\n",
            "Epoch 184/500\n",
            "60/60 [==============================] - 0s 795us/step - loss: 9979.9825 - mse: 213245776.0000 - mae: 9979.9834 - val_loss: 12191.9760 - val_mse: 364350176.0000 - val_mae: 12191.9746\n",
            "Epoch 185/500\n",
            "60/60 [==============================] - 0s 893us/step - loss: 9825.5325 - mse: 219646576.0000 - mae: 9825.5322 - val_loss: 12165.7619 - val_mse: 362576064.0000 - val_mae: 12165.7617\n",
            "Epoch 186/500\n",
            "60/60 [==============================] - 0s 710us/step - loss: 10263.5269 - mse: 214990480.0000 - mae: 10263.5273 - val_loss: 12160.8598 - val_mse: 362245600.0000 - val_mae: 12160.8604\n",
            "Epoch 187/500\n",
            "60/60 [==============================] - 0s 786us/step - loss: 10891.9756 - mse: 225250144.0000 - mae: 10891.9746 - val_loss: 12165.7054 - val_mse: 362572000.0000 - val_mae: 12165.7051\n",
            "Epoch 188/500\n",
            "60/60 [==============================] - 0s 805us/step - loss: 9633.9793 - mse: 218122272.0000 - mae: 9633.9795 - val_loss: 12142.3924 - val_mse: 361004672.0000 - val_mae: 12142.3926\n",
            "Epoch 189/500\n",
            "60/60 [==============================] - 0s 747us/step - loss: 10563.7722 - mse: 218828448.0000 - mae: 10563.7725 - val_loss: 12142.7074 - val_mse: 361025600.0000 - val_mae: 12142.7080\n",
            "Epoch 190/500\n",
            "60/60 [==============================] - 0s 804us/step - loss: 10215.6748 - mse: 227330656.0000 - mae: 10215.6748 - val_loss: 12113.3509 - val_mse: 359065696.0000 - val_mae: 12113.3506\n",
            "Epoch 191/500\n",
            "60/60 [==============================] - 0s 880us/step - loss: 10062.9735 - mse: 236027856.0000 - mae: 10062.9736 - val_loss: 12115.9886 - val_mse: 359241024.0000 - val_mae: 12115.9883\n",
            "Epoch 192/500\n",
            "60/60 [==============================] - 0s 851us/step - loss: 9917.9486 - mse: 216855792.0000 - mae: 9917.9492 - val_loss: 12114.6378 - val_mse: 359151072.0000 - val_mae: 12114.6377\n",
            "Epoch 193/500\n",
            "60/60 [==============================] - 0s 776us/step - loss: 11107.5592 - mse: 242034336.0000 - mae: 11107.5596 - val_loss: 12111.8252 - val_mse: 358963872.0000 - val_mae: 12111.8252\n",
            "Epoch 194/500\n",
            "60/60 [==============================] - 0s 833us/step - loss: 11259.3039 - mse: 225757248.0000 - mae: 11259.3037 - val_loss: 12115.7685 - val_mse: 359225952.0000 - val_mae: 12115.7686\n",
            "Epoch 195/500\n",
            "60/60 [==============================] - 0s 842us/step - loss: 10135.4144 - mse: 188578512.0000 - mae: 10135.4150 - val_loss: 12091.7259 - val_mse: 357631360.0000 - val_mae: 12091.7246\n",
            "Epoch 196/500\n",
            "60/60 [==============================] - 0s 804us/step - loss: 10112.4759 - mse: 237228816.0000 - mae: 10112.4756 - val_loss: 12069.1931 - val_mse: 356146272.0000 - val_mae: 12069.1934\n",
            "Epoch 197/500\n",
            "60/60 [==============================] - 0s 811us/step - loss: 10718.4263 - mse: 222313472.0000 - mae: 10718.4268 - val_loss: 12064.6859 - val_mse: 355850208.0000 - val_mae: 12064.6865\n",
            "Epoch 198/500\n",
            "60/60 [==============================] - 0s 741us/step - loss: 9982.5094 - mse: 194134320.0000 - mae: 9982.5098 - val_loss: 12036.0443 - val_mse: 353977984.0000 - val_mae: 12036.0439\n",
            "Epoch 199/500\n",
            "60/60 [==============================] - 0s 731us/step - loss: 10363.5753 - mse: 233889520.0000 - mae: 10363.5752 - val_loss: 12015.7064 - val_mse: 352657600.0000 - val_mae: 12015.7061\n",
            "Epoch 200/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 10032.4746 - mse: 194992416.0000 - mae: 10032.4746 - val_loss: 12008.9993 - val_mse: 352223712.0000 - val_mae: 12009.0000\n",
            "Epoch 201/500\n",
            "60/60 [==============================] - 0s 814us/step - loss: 9307.0470 - mse: 168849072.0000 - mae: 9307.0469 - val_loss: 12007.5738 - val_mse: 352131296.0000 - val_mae: 12007.5742\n",
            "Epoch 202/500\n",
            "60/60 [==============================] - 0s 717us/step - loss: 9532.0649 - mse: 207919168.0000 - mae: 9532.0654 - val_loss: 12023.9529 - val_mse: 353191520.0000 - val_mae: 12023.9521\n",
            "Epoch 203/500\n",
            "60/60 [==============================] - 0s 770us/step - loss: 11107.0750 - mse: 262131456.0000 - mae: 11107.0752 - val_loss: 12034.5393 - val_mse: 353879264.0000 - val_mae: 12034.5400\n",
            "Epoch 204/500\n",
            "60/60 [==============================] - 0s 695us/step - loss: 10834.6797 - mse: 220828400.0000 - mae: 10834.6807 - val_loss: 12020.4401 - val_mse: 352963552.0000 - val_mae: 12020.4404\n",
            "Epoch 205/500\n",
            "60/60 [==============================] - 0s 884us/step - loss: 10622.9111 - mse: 247607328.0000 - mae: 10622.9102 - val_loss: 12007.1330 - val_mse: 352102400.0000 - val_mae: 12007.1338\n",
            "Epoch 206/500\n",
            "60/60 [==============================] - 0s 754us/step - loss: 10037.3005 - mse: 201946448.0000 - mae: 10037.2998 - val_loss: 11988.3784 - val_mse: 350894400.0000 - val_mae: 11988.3779\n",
            "Epoch 207/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 11304.6676 - mse: 244259584.0000 - mae: 11304.6670 - val_loss: 11998.8119 - val_mse: 351565408.0000 - val_mae: 11998.8125\n",
            "Epoch 208/500\n",
            "60/60 [==============================] - 0s 808us/step - loss: 11268.7568 - mse: 256883712.0000 - mae: 11268.7559 - val_loss: 12013.2726 - val_mse: 352498880.0000 - val_mae: 12013.2725\n",
            "Epoch 209/500\n",
            "60/60 [==============================] - 0s 687us/step - loss: 10838.5566 - mse: 239784624.0000 - mae: 10838.5566 - val_loss: 12002.6809 - val_mse: 351814752.0000 - val_mae: 12002.6816\n",
            "Epoch 210/500\n",
            "60/60 [==============================] - 0s 757us/step - loss: 11016.0479 - mse: 263663760.0000 - mae: 11016.0479 - val_loss: 12019.6253 - val_mse: 352910048.0000 - val_mae: 12019.6250\n",
            "Epoch 211/500\n",
            "60/60 [==============================] - 0s 765us/step - loss: 10162.1277 - mse: 195864160.0000 - mae: 10162.1279 - val_loss: 11983.9440 - val_mse: 350609056.0000 - val_mae: 11983.9434\n",
            "Epoch 212/500\n",
            "60/60 [==============================] - 0s 778us/step - loss: 9536.2291 - mse: 168666384.0000 - mae: 9536.2295 - val_loss: 11976.6922 - val_mse: 350144192.0000 - val_mae: 11976.6914\n",
            "Epoch 213/500\n",
            "60/60 [==============================] - 0s 823us/step - loss: 9573.5824 - mse: 200140352.0000 - mae: 9573.5820 - val_loss: 11974.3630 - val_mse: 349994880.0000 - val_mae: 11974.3623\n",
            "Epoch 214/500\n",
            "60/60 [==============================] - 0s 965us/step - loss: 9789.9046 - mse: 226420288.0000 - mae: 9789.9053 - val_loss: 11972.8025 - val_mse: 349894880.0000 - val_mae: 11972.8018\n",
            "Epoch 215/500\n",
            "60/60 [==============================] - 0s 824us/step - loss: 9999.4450 - mse: 218594544.0000 - mae: 9999.4463 - val_loss: 11992.7548 - val_mse: 351174528.0000 - val_mae: 11992.7539\n",
            "Epoch 216/500\n",
            "60/60 [==============================] - 0s 735us/step - loss: 11063.0921 - mse: 212551952.0000 - mae: 11063.0928 - val_loss: 12010.7850 - val_mse: 352337024.0000 - val_mae: 12010.7842\n",
            "Epoch 217/500\n",
            "60/60 [==============================] - 0s 793us/step - loss: 10269.0145 - mse: 252543568.0000 - mae: 10269.0146 - val_loss: 11995.7885 - val_mse: 351369376.0000 - val_mae: 11995.7881\n",
            "Epoch 218/500\n",
            "60/60 [==============================] - 0s 796us/step - loss: 8804.3929 - mse: 165713424.0000 - mae: 8804.3926 - val_loss: 11967.5428 - val_mse: 349558176.0000 - val_mae: 11967.5430\n",
            "Epoch 219/500\n",
            "60/60 [==============================] - 0s 733us/step - loss: 10131.4854 - mse: 216944912.0000 - mae: 10131.4854 - val_loss: 11981.9102 - val_mse: 350477536.0000 - val_mae: 11981.9102\n",
            "Epoch 220/500\n",
            "60/60 [==============================] - 0s 709us/step - loss: 10466.9744 - mse: 226461600.0000 - mae: 10466.9746 - val_loss: 11978.4682 - val_mse: 350256704.0000 - val_mae: 11978.4688\n",
            "Epoch 221/500\n",
            "60/60 [==============================] - 0s 883us/step - loss: 10402.6820 - mse: 192722352.0000 - mae: 10402.6826 - val_loss: 11960.9275 - val_mse: 349135584.0000 - val_mae: 11960.9268\n",
            "Epoch 222/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 10121.3648 - mse: 204645552.0000 - mae: 10121.3643 - val_loss: 11971.0573 - val_mse: 349782144.0000 - val_mae: 11971.0576\n",
            "Epoch 223/500\n",
            "60/60 [==============================] - 0s 761us/step - loss: 9632.2701 - mse: 212758800.0000 - mae: 9632.2695 - val_loss: 11958.1030 - val_mse: 348955360.0000 - val_mae: 11958.1035\n",
            "Epoch 224/500\n",
            "60/60 [==============================] - 0s 763us/step - loss: 10166.6074 - mse: 222381840.0000 - mae: 10166.6084 - val_loss: 11933.6144 - val_mse: 347400864.0000 - val_mae: 11933.6143\n",
            "Epoch 225/500\n",
            "60/60 [==============================] - 0s 953us/step - loss: 9259.5927 - mse: 180148944.0000 - mae: 9259.5928 - val_loss: 11919.5967 - val_mse: 346515840.0000 - val_mae: 11919.5967\n",
            "Epoch 226/500\n",
            "60/60 [==============================] - 0s 743us/step - loss: 10394.6272 - mse: 224874064.0000 - mae: 10394.6270 - val_loss: 11908.5441 - val_mse: 345820480.0000 - val_mae: 11908.5439\n",
            "Epoch 227/500\n",
            "60/60 [==============================] - 0s 958us/step - loss: 10262.3729 - mse: 218420720.0000 - mae: 10262.3730 - val_loss: 11880.3118 - val_mse: 344054368.0000 - val_mae: 11880.3125\n",
            "Epoch 228/500\n",
            "60/60 [==============================] - 0s 801us/step - loss: 10376.2694 - mse: 237735408.0000 - mae: 10376.2695 - val_loss: 11890.4651 - val_mse: 344687680.0000 - val_mae: 11890.4648\n",
            "Epoch 229/500\n",
            "60/60 [==============================] - 0s 781us/step - loss: 9172.7737 - mse: 175401280.0000 - mae: 9172.7744 - val_loss: 11900.2814 - val_mse: 345301856.0000 - val_mae: 11900.2812\n",
            "Epoch 230/500\n",
            "60/60 [==============================] - 0s 775us/step - loss: 10116.1038 - mse: 204410048.0000 - mae: 10116.1045 - val_loss: 11912.3907 - val_mse: 346061792.0000 - val_mae: 11912.3906\n",
            "Epoch 231/500\n",
            "60/60 [==============================] - 0s 733us/step - loss: 10900.5384 - mse: 222840576.0000 - mae: 10900.5381 - val_loss: 11914.3798 - val_mse: 346186816.0000 - val_mae: 11914.3789\n",
            "Epoch 232/500\n",
            "60/60 [==============================] - 0s 877us/step - loss: 9214.6191 - mse: 182529072.0000 - mae: 9214.6191 - val_loss: 11922.8869 - val_mse: 346722656.0000 - val_mae: 11922.8877\n",
            "Epoch 233/500\n",
            "60/60 [==============================] - 0s 833us/step - loss: 10648.1255 - mse: 223475808.0000 - mae: 10648.1250 - val_loss: 11923.6230 - val_mse: 346768960.0000 - val_mae: 11923.6230\n",
            "Epoch 234/500\n",
            "60/60 [==============================] - 0s 804us/step - loss: 10965.4764 - mse: 231791440.0000 - mae: 10965.4756 - val_loss: 11924.0011 - val_mse: 346792576.0000 - val_mae: 11924.0000\n",
            "Epoch 235/500\n",
            "60/60 [==============================] - 0s 816us/step - loss: 9609.8760 - mse: 188714528.0000 - mae: 9609.8750 - val_loss: 11908.4831 - val_mse: 345815648.0000 - val_mae: 11908.4834\n",
            "Epoch 236/500\n",
            "60/60 [==============================] - 0s 910us/step - loss: 10952.1833 - mse: 225120832.0000 - mae: 10952.1836 - val_loss: 11913.1244 - val_mse: 346107200.0000 - val_mae: 11913.1250\n",
            "Epoch 237/500\n",
            "60/60 [==============================] - 0s 928us/step - loss: 9404.5688 - mse: 200448576.0000 - mae: 9404.5684 - val_loss: 11917.5804 - val_mse: 346387456.0000 - val_mae: 11917.5811\n",
            "Epoch 238/500\n",
            "60/60 [==============================] - 0s 779us/step - loss: 10037.8441 - mse: 195612224.0000 - mae: 10037.8438 - val_loss: 11919.4882 - val_mse: 346507456.0000 - val_mae: 11919.4873\n",
            "Epoch 239/500\n",
            "60/60 [==============================] - 0s 755us/step - loss: 10029.4854 - mse: 175210432.0000 - mae: 10029.4854 - val_loss: 11917.7661 - val_mse: 346398912.0000 - val_mae: 11917.7656\n",
            "Epoch 240/500\n",
            "60/60 [==============================] - 0s 935us/step - loss: 9643.0091 - mse: 218418976.0000 - mae: 9643.0098 - val_loss: 11904.3171 - val_mse: 345553504.0000 - val_mae: 11904.3164\n",
            "Epoch 241/500\n",
            "60/60 [==============================] - 0s 710us/step - loss: 10371.0512 - mse: 221311424.0000 - mae: 10371.0508 - val_loss: 11898.9182 - val_mse: 345215040.0000 - val_mae: 11898.9189\n",
            "Epoch 242/500\n",
            "60/60 [==============================] - 0s 828us/step - loss: 10481.2747 - mse: 226101280.0000 - mae: 10481.2744 - val_loss: 11906.5707 - val_mse: 345694688.0000 - val_mae: 11906.5713\n",
            "Epoch 243/500\n",
            "60/60 [==============================] - 0s 797us/step - loss: 10427.3241 - mse: 218671680.0000 - mae: 10427.3242 - val_loss: 11901.8986 - val_mse: 345401568.0000 - val_mae: 11901.8975\n",
            "Epoch 244/500\n",
            "60/60 [==============================] - 0s 969us/step - loss: 10731.0288 - mse: 202005440.0000 - mae: 10731.0293 - val_loss: 11901.0028 - val_mse: 345345216.0000 - val_mae: 11901.0020\n",
            "Epoch 245/500\n",
            "60/60 [==============================] - 0s 701us/step - loss: 9600.9526 - mse: 183043424.0000 - mae: 9600.9521 - val_loss: 11866.6172 - val_mse: 343200800.0000 - val_mae: 11866.6162\n",
            "Epoch 246/500\n",
            "60/60 [==============================] - 0s 983us/step - loss: 10202.9456 - mse: 231532864.0000 - mae: 10202.9463 - val_loss: 11855.4406 - val_mse: 342508256.0000 - val_mae: 11855.4414\n",
            "Epoch 247/500\n",
            "60/60 [==============================] - 0s 776us/step - loss: 9660.2206 - mse: 196854048.0000 - mae: 9660.2207 - val_loss: 11861.6713 - val_mse: 342893984.0000 - val_mae: 11861.6709\n",
            "Epoch 248/500\n",
            "60/60 [==============================] - 0s 728us/step - loss: 9668.4823 - mse: 186559280.0000 - mae: 9668.4824 - val_loss: 11851.2298 - val_mse: 342247904.0000 - val_mae: 11851.2305\n",
            "Epoch 249/500\n",
            "60/60 [==============================] - 0s 764us/step - loss: 10881.5518 - mse: 244115632.0000 - mae: 10881.5518 - val_loss: 11838.6697 - val_mse: 341473344.0000 - val_mae: 11838.6699\n",
            "Epoch 250/500\n",
            "60/60 [==============================] - 0s 681us/step - loss: 10438.7877 - mse: 209374416.0000 - mae: 10438.7871 - val_loss: 11856.9583 - val_mse: 342602016.0000 - val_mae: 11856.9580\n",
            "Epoch 251/500\n",
            "60/60 [==============================] - 0s 774us/step - loss: 10575.0044 - mse: 217298640.0000 - mae: 10575.0039 - val_loss: 11840.8859 - val_mse: 341609536.0000 - val_mae: 11840.8857\n",
            "Epoch 252/500\n",
            "60/60 [==============================] - 0s 968us/step - loss: 11148.7608 - mse: 219704080.0000 - mae: 11148.7617 - val_loss: 11844.7237 - val_mse: 341845984.0000 - val_mae: 11844.7236\n",
            "Epoch 253/500\n",
            "60/60 [==============================] - 0s 879us/step - loss: 10571.9770 - mse: 226041744.0000 - mae: 10571.9775 - val_loss: 11847.0828 - val_mse: 341991328.0000 - val_mae: 11847.0820\n",
            "Epoch 254/500\n",
            "60/60 [==============================] - 0s 804us/step - loss: 10406.8864 - mse: 235605744.0000 - mae: 10406.8857 - val_loss: 11836.7842 - val_mse: 341356704.0000 - val_mae: 11836.7832\n",
            "Epoch 255/500\n",
            "60/60 [==============================] - 0s 834us/step - loss: 9358.5230 - mse: 182019088.0000 - mae: 9358.5225 - val_loss: 11830.4777 - val_mse: 340968992.0000 - val_mae: 11830.4785\n",
            "Epoch 256/500\n",
            "60/60 [==============================] - 0s 814us/step - loss: 9737.7376 - mse: 183666544.0000 - mae: 9737.7373 - val_loss: 11839.0304 - val_mse: 341494624.0000 - val_mae: 11839.0303\n",
            "Epoch 257/500\n",
            "60/60 [==============================] - 0s 937us/step - loss: 9777.4159 - mse: 199599248.0000 - mae: 9777.4170 - val_loss: 11819.5808 - val_mse: 340300800.0000 - val_mae: 11819.5811\n",
            "Epoch 258/500\n",
            "60/60 [==============================] - 0s 764us/step - loss: 9819.0203 - mse: 177327952.0000 - mae: 9819.0205 - val_loss: 11823.6856 - val_mse: 340552096.0000 - val_mae: 11823.6855\n",
            "Epoch 259/500\n",
            "60/60 [==============================] - 0s 836us/step - loss: 10471.1537 - mse: 217793600.0000 - mae: 10471.1533 - val_loss: 11821.3066 - val_mse: 340406272.0000 - val_mae: 11821.3066\n",
            "Epoch 260/500\n",
            "60/60 [==============================] - 0s 727us/step - loss: 8912.6128 - mse: 176947440.0000 - mae: 8912.6123 - val_loss: 11818.0726 - val_mse: 340208256.0000 - val_mae: 11818.0732\n",
            "Epoch 261/500\n",
            "60/60 [==============================] - 0s 804us/step - loss: 10324.9390 - mse: 221451152.0000 - mae: 10324.9385 - val_loss: 11825.1627 - val_mse: 340642272.0000 - val_mae: 11825.1621\n",
            "Epoch 262/500\n",
            "60/60 [==============================] - 0s 821us/step - loss: 9653.5483 - mse: 209999120.0000 - mae: 9653.5488 - val_loss: 11834.6937 - val_mse: 341227232.0000 - val_mae: 11834.6934\n",
            "Epoch 263/500\n",
            "60/60 [==============================] - 0s 848us/step - loss: 9532.9587 - mse: 177544192.0000 - mae: 9532.9580 - val_loss: 11825.5305 - val_mse: 340664608.0000 - val_mae: 11825.5303\n",
            "Epoch 264/500\n",
            "60/60 [==============================] - 0s 903us/step - loss: 9752.4622 - mse: 220399344.0000 - mae: 9752.4619 - val_loss: 11832.6823 - val_mse: 341103456.0000 - val_mae: 11832.6826\n",
            "Epoch 265/500\n",
            "60/60 [==============================] - 0s 769us/step - loss: 9122.6842 - mse: 165927600.0000 - mae: 9122.6846 - val_loss: 11816.6980 - val_mse: 340123488.0000 - val_mae: 11816.6982\n",
            "Epoch 266/500\n",
            "60/60 [==============================] - 0s 913us/step - loss: 10578.4740 - mse: 216944672.0000 - mae: 10578.4746 - val_loss: 11829.6005 - val_mse: 340913984.0000 - val_mae: 11829.5996\n",
            "Epoch 267/500\n",
            "60/60 [==============================] - 0s 710us/step - loss: 10086.6523 - mse: 210060000.0000 - mae: 10086.6523 - val_loss: 11819.2520 - val_mse: 340279488.0000 - val_mae: 11819.2520\n",
            "Epoch 268/500\n",
            "60/60 [==============================] - 0s 802us/step - loss: 11697.7389 - mse: 251184032.0000 - mae: 11697.7393 - val_loss: 11827.5643 - val_mse: 340788832.0000 - val_mae: 11827.5645\n",
            "Epoch 269/500\n",
            "60/60 [==============================] - 0s 990us/step - loss: 8650.2771 - mse: 148395952.0000 - mae: 8650.2773 - val_loss: 11836.2636 - val_mse: 341323168.0000 - val_mae: 11836.2637\n",
            "Epoch 270/500\n",
            "60/60 [==============================] - 0s 764us/step - loss: 10844.5160 - mse: 233261392.0000 - mae: 10844.5166 - val_loss: 11843.2284 - val_mse: 341751840.0000 - val_mae: 11843.2285\n",
            "Epoch 271/500\n",
            "60/60 [==============================] - 0s 790us/step - loss: 11883.9662 - mse: 264878352.0000 - mae: 11883.9658 - val_loss: 11843.6618 - val_mse: 341778528.0000 - val_mae: 11843.6611\n",
            "Epoch 272/500\n",
            "60/60 [==============================] - 0s 923us/step - loss: 10240.2021 - mse: 214532096.0000 - mae: 10240.2021 - val_loss: 11819.0093 - val_mse: 340264384.0000 - val_mae: 11819.0098\n",
            "Epoch 273/500\n",
            "60/60 [==============================] - 0s 763us/step - loss: 9399.4880 - mse: 221185232.0000 - mae: 9399.4873 - val_loss: 11832.0059 - val_mse: 341061120.0000 - val_mae: 11832.0059\n",
            "Epoch 274/500\n",
            "60/60 [==============================] - 0s 927us/step - loss: 11763.8809 - mse: 265133888.0000 - mae: 11763.8809 - val_loss: 11842.6449 - val_mse: 341715680.0000 - val_mae: 11842.6445\n",
            "Epoch 275/500\n",
            "60/60 [==============================] - 0s 759us/step - loss: 10289.0163 - mse: 229544688.0000 - mae: 10289.0166 - val_loss: 11838.4120 - val_mse: 341454880.0000 - val_mae: 11838.4121\n",
            "Epoch 276/500\n",
            "60/60 [==============================] - 0s 758us/step - loss: 10386.5322 - mse: 199688288.0000 - mae: 10386.5322 - val_loss: 11848.2826 - val_mse: 342063104.0000 - val_mae: 11848.2822\n",
            "Epoch 277/500\n",
            "60/60 [==============================] - 0s 683us/step - loss: 9411.2834 - mse: 200267264.0000 - mae: 9411.2832 - val_loss: 11838.6848 - val_mse: 341471456.0000 - val_mae: 11838.6855\n",
            "Epoch 278/500\n",
            "60/60 [==============================] - 0s 769us/step - loss: 11247.8611 - mse: 245324128.0000 - mae: 11247.8604 - val_loss: 11849.5328 - val_mse: 342140192.0000 - val_mae: 11849.5322\n",
            "Epoch 279/500\n",
            "60/60 [==============================] - 0s 724us/step - loss: 10566.5731 - mse: 233286752.0000 - mae: 10566.5732 - val_loss: 11855.7304 - val_mse: 342523104.0000 - val_mae: 11855.7314\n",
            "Epoch 280/500\n",
            "60/60 [==============================] - 0s 907us/step - loss: 10582.5786 - mse: 224974368.0000 - mae: 10582.5781 - val_loss: 11836.8352 - val_mse: 341357504.0000 - val_mae: 11836.8350\n",
            "Epoch 281/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 8864.5484 - mse: 182388304.0000 - mae: 8864.5488 - val_loss: 11844.0933 - val_mse: 341804320.0000 - val_mae: 11844.0928\n",
            "Epoch 282/500\n",
            "60/60 [==============================] - 0s 762us/step - loss: 10258.1187 - mse: 189280080.0000 - mae: 10258.1191 - val_loss: 11850.7793 - val_mse: 342216832.0000 - val_mae: 11850.7793\n",
            "Epoch 283/500\n",
            "60/60 [==============================] - 0s 908us/step - loss: 10713.7918 - mse: 251732208.0000 - mae: 10713.7920 - val_loss: 11860.8703 - val_mse: 342840896.0000 - val_mae: 11860.8711\n",
            "Epoch 284/500\n",
            "60/60 [==============================] - 0s 823us/step - loss: 9306.0748 - mse: 175186800.0000 - mae: 9306.0752 - val_loss: 11851.1246 - val_mse: 342237920.0000 - val_mae: 11851.1250\n",
            "Epoch 285/500\n",
            "60/60 [==============================] - 0s 895us/step - loss: 11075.9552 - mse: 229099920.0000 - mae: 11075.9551 - val_loss: 11848.5765 - val_mse: 342080544.0000 - val_mae: 11848.5771\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 80 samples, validate on 70 samples\n",
            "Epoch 1/500\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 35870.6692 - mse: 1551455872.0000 - mae: 35870.6680 - val_loss: 37657.9049 - val_mse: 1674816512.0000 - val_mae: 37657.9062\n",
            "Epoch 2/500\n",
            "80/80 [==============================] - 0s 611us/step - loss: 35868.2598 - mse: 1551233408.0000 - mae: 35868.2578 - val_loss: 37651.3184 - val_mse: 1674217984.0000 - val_mae: 37651.3164\n",
            "Epoch 3/500\n",
            "80/80 [==============================] - 0s 634us/step - loss: 35859.4966 - mse: 1550482560.0000 - mae: 35859.4922 - val_loss: 37639.0670 - val_mse: 1673136512.0000 - val_mae: 37639.0664\n",
            "Epoch 4/500\n",
            "80/80 [==============================] - 0s 572us/step - loss: 35842.4529 - mse: 1549064832.0000 - mae: 35842.4570 - val_loss: 37619.6663 - val_mse: 1671475072.0000 - val_mae: 37619.6680\n",
            "Epoch 5/500\n",
            "80/80 [==============================] - 0s 574us/step - loss: 35819.6001 - mse: 1547043328.0000 - mae: 35819.6016 - val_loss: 37594.4660 - val_mse: 1669333120.0000 - val_mae: 37594.4648\n",
            "Epoch 6/500\n",
            "80/80 [==============================] - 0s 550us/step - loss: 35795.1602 - mse: 1545147136.0000 - mae: 35795.1602 - val_loss: 37563.8044 - val_mse: 1666761856.0000 - val_mae: 37563.8086\n",
            "Epoch 7/500\n",
            "80/80 [==============================] - 0s 571us/step - loss: 35757.5649 - mse: 1541995136.0000 - mae: 35757.5664 - val_loss: 37526.4414 - val_mse: 1663668096.0000 - val_mae: 37526.4414\n",
            "Epoch 8/500\n",
            "80/80 [==============================] - 0s 637us/step - loss: 35713.2715 - mse: 1538425088.0000 - mae: 35713.2734 - val_loss: 37482.0312 - val_mse: 1660016640.0000 - val_mae: 37482.0312\n",
            "Epoch 9/500\n",
            "80/80 [==============================] - 0s 600us/step - loss: 35668.6987 - mse: 1534871424.0000 - mae: 35668.6953 - val_loss: 37431.1049 - val_mse: 1655868416.0000 - val_mae: 37431.1055\n",
            "Epoch 10/500\n",
            "80/80 [==============================] - 0s 690us/step - loss: 35615.5371 - mse: 1530873984.0000 - mae: 35615.5391 - val_loss: 37373.9657 - val_mse: 1651243008.0000 - val_mae: 37373.9648\n",
            "Epoch 11/500\n",
            "80/80 [==============================] - 0s 743us/step - loss: 35577.5088 - mse: 1528482048.0000 - mae: 35577.5078 - val_loss: 37312.8792 - val_mse: 1646356992.0000 - val_mae: 37312.8789\n",
            "Epoch 12/500\n",
            "80/80 [==============================] - 0s 633us/step - loss: 35490.3713 - mse: 1521998848.0000 - mae: 35490.3672 - val_loss: 37239.8613 - val_mse: 1640552320.0000 - val_mae: 37239.8594\n",
            "Epoch 13/500\n",
            "80/80 [==============================] - 0s 599us/step - loss: 35427.7896 - mse: 1516036736.0000 - mae: 35427.7891 - val_loss: 37161.4852 - val_mse: 1634359680.0000 - val_mae: 37161.4844\n",
            "Epoch 14/500\n",
            "80/80 [==============================] - 0s 684us/step - loss: 35312.5129 - mse: 1507875456.0000 - mae: 35312.5156 - val_loss: 37069.0605 - val_mse: 1627104128.0000 - val_mae: 37069.0625\n",
            "Epoch 15/500\n",
            "80/80 [==============================] - 0s 621us/step - loss: 35268.5850 - mse: 1504015104.0000 - mae: 35268.5859 - val_loss: 36976.2503 - val_mse: 1619845632.0000 - val_mae: 36976.2500\n",
            "Epoch 16/500\n",
            "80/80 [==============================] - 0s 633us/step - loss: 35164.5549 - mse: 1497776384.0000 - mae: 35164.5508 - val_loss: 36873.4350 - val_mse: 1611877632.0000 - val_mae: 36873.4375\n",
            "Epoch 17/500\n",
            "80/80 [==============================] - 0s 603us/step - loss: 35047.8779 - mse: 1486771200.0000 - mae: 35047.8750 - val_loss: 36761.7799 - val_mse: 1603259520.0000 - val_mae: 36761.7773\n",
            "Epoch 18/500\n",
            "80/80 [==============================] - 0s 600us/step - loss: 34911.1184 - mse: 1477707520.0000 - mae: 34911.1172 - val_loss: 36639.6378 - val_mse: 1593874304.0000 - val_mae: 36639.6406\n",
            "Epoch 19/500\n",
            "80/80 [==============================] - 0s 632us/step - loss: 34826.0488 - mse: 1470486528.0000 - mae: 34826.0508 - val_loss: 36514.2243 - val_mse: 1584293632.0000 - val_mae: 36514.2266\n",
            "Epoch 20/500\n",
            "80/80 [==============================] - 0s 684us/step - loss: 34680.2678 - mse: 1460419200.0000 - mae: 34680.2695 - val_loss: 36378.8867 - val_mse: 1573995648.0000 - val_mae: 36378.8867\n",
            "Epoch 21/500\n",
            "80/80 [==============================] - 0s 628us/step - loss: 34557.7146 - mse: 1453185792.0000 - mae: 34557.7148 - val_loss: 36236.5689 - val_mse: 1563228288.0000 - val_mae: 36236.5703\n",
            "Epoch 22/500\n",
            "80/80 [==============================] - 0s 606us/step - loss: 34374.9458 - mse: 1440167680.0000 - mae: 34374.9453 - val_loss: 36082.0114 - val_mse: 1551611904.0000 - val_mae: 36082.0117\n",
            "Epoch 23/500\n",
            "80/80 [==============================] - 0s 604us/step - loss: 34225.1921 - mse: 1425236736.0000 - mae: 34225.1914 - val_loss: 35921.0458 - val_mse: 1539574528.0000 - val_mae: 35921.0430\n",
            "Epoch 24/500\n",
            "80/80 [==============================] - 0s 594us/step - loss: 34049.1006 - mse: 1414878720.0000 - mae: 34049.1016 - val_loss: 35751.0452 - val_mse: 1526938880.0000 - val_mae: 35751.0469\n",
            "Epoch 25/500\n",
            "80/80 [==============================] - 0s 533us/step - loss: 33889.7273 - mse: 1404871424.0000 - mae: 33889.7266 - val_loss: 35575.8608 - val_mse: 1513985152.0000 - val_mae: 35575.8594\n",
            "Epoch 26/500\n",
            "80/80 [==============================] - 0s 700us/step - loss: 33703.9004 - mse: 1391817856.0000 - mae: 33703.8984 - val_loss: 35390.8002 - val_mse: 1500397056.0000 - val_mae: 35390.8008\n",
            "Epoch 27/500\n",
            "80/80 [==============================] - 0s 600us/step - loss: 33558.7488 - mse: 1379551104.0000 - mae: 33558.7461 - val_loss: 35201.8384 - val_mse: 1486598016.0000 - val_mae: 35201.8359\n",
            "Epoch 28/500\n",
            "80/80 [==============================] - 0s 599us/step - loss: 33404.4727 - mse: 1369556096.0000 - mae: 33404.4766 - val_loss: 35006.8733 - val_mse: 1472471680.0000 - val_mae: 35006.8750\n",
            "Epoch 29/500\n",
            "80/80 [==============================] - 0s 625us/step - loss: 33264.8096 - mse: 1357880960.0000 - mae: 33264.8086 - val_loss: 34807.2377 - val_mse: 1458103680.0000 - val_mae: 34807.2383\n",
            "Epoch 30/500\n",
            "80/80 [==============================] - 0s 548us/step - loss: 32934.6260 - mse: 1337879168.0000 - mae: 32934.6250 - val_loss: 34590.9623 - val_mse: 1442642304.0000 - val_mae: 34590.9609\n",
            "Epoch 31/500\n",
            "80/80 [==============================] - 0s 603us/step - loss: 32763.4160 - mse: 1324833024.0000 - mae: 32763.4180 - val_loss: 34370.0723 - val_mse: 1426954240.0000 - val_mae: 34370.0703\n",
            "Epoch 32/500\n",
            "80/80 [==============================] - 0s 816us/step - loss: 32522.8464 - mse: 1314001024.0000 - mae: 32522.8438 - val_loss: 34139.8421 - val_mse: 1410732544.0000 - val_mae: 34139.8438\n",
            "Epoch 33/500\n",
            "80/80 [==============================] - 0s 643us/step - loss: 32311.0793 - mse: 1295648000.0000 - mae: 32311.0820 - val_loss: 33902.5326 - val_mse: 1394146432.0000 - val_mae: 33902.5312\n",
            "Epoch 34/500\n",
            "80/80 [==============================] - 0s 707us/step - loss: 31992.3887 - mse: 1279432704.0000 - mae: 31992.3906 - val_loss: 33652.1702 - val_mse: 1376775552.0000 - val_mae: 33652.1680\n",
            "Epoch 35/500\n",
            "80/80 [==============================] - 0s 636us/step - loss: 31886.5342 - mse: 1267263104.0000 - mae: 31886.5352 - val_loss: 33404.4023 - val_mse: 1359702528.0000 - val_mae: 33404.3984\n",
            "Epoch 36/500\n",
            "80/80 [==============================] - 0s 604us/step - loss: 31549.8049 - mse: 1248937984.0000 - mae: 31549.8086 - val_loss: 33141.6431 - val_mse: 1341773952.0000 - val_mae: 33141.6445\n",
            "Epoch 37/500\n",
            "80/80 [==============================] - 0s 613us/step - loss: 31068.8972 - mse: 1213277312.0000 - mae: 31068.9004 - val_loss: 32859.2743 - val_mse: 1322672768.0000 - val_mae: 32859.2734\n",
            "Epoch 38/500\n",
            "80/80 [==============================] - 0s 580us/step - loss: 31029.5688 - mse: 1218587904.0000 - mae: 31029.5684 - val_loss: 32582.1039 - val_mse: 1304095232.0000 - val_mae: 32582.1035\n",
            "Epoch 39/500\n",
            "80/80 [==============================] - 0s 553us/step - loss: 30806.5005 - mse: 1204215040.0000 - mae: 30806.5000 - val_loss: 32299.2098 - val_mse: 1285329024.0000 - val_mae: 32299.2070\n",
            "Epoch 40/500\n",
            "80/80 [==============================] - 0s 618us/step - loss: 30367.4570 - mse: 1183196800.0000 - mae: 30367.4590 - val_loss: 32000.8415 - val_mse: 1265713664.0000 - val_mae: 32000.8438\n",
            "Epoch 41/500\n",
            "80/80 [==============================] - 0s 598us/step - loss: 30220.4312 - mse: 1161954048.0000 - mae: 30220.4316 - val_loss: 31701.6256 - val_mse: 1246228352.0000 - val_mae: 31701.6250\n",
            "Epoch 42/500\n",
            "80/80 [==============================] - 0s 547us/step - loss: 29662.1697 - mse: 1136817664.0000 - mae: 29662.1680 - val_loss: 31382.4262 - val_mse: 1225644416.0000 - val_mae: 31382.4258\n",
            "Epoch 43/500\n",
            "80/80 [==============================] - 0s 712us/step - loss: 29335.0889 - mse: 1098847616.0000 - mae: 29335.0898 - val_loss: 31056.5269 - val_mse: 1204837248.0000 - val_mae: 31056.5293\n",
            "Epoch 44/500\n",
            "80/80 [==============================] - 0s 584us/step - loss: 28950.8230 - mse: 1098430720.0000 - mae: 28950.8242 - val_loss: 30718.4369 - val_mse: 1183526144.0000 - val_mae: 30718.4355\n",
            "Epoch 45/500\n",
            "80/80 [==============================] - 0s 606us/step - loss: 28621.5623 - mse: 1055932736.0000 - mae: 28621.5625 - val_loss: 30374.0290 - val_mse: 1162046592.0000 - val_mae: 30374.0293\n",
            "Epoch 46/500\n",
            "80/80 [==============================] - 0s 637us/step - loss: 28286.4663 - mse: 1039605056.0000 - mae: 28286.4648 - val_loss: 30022.9722 - val_mse: 1140403584.0000 - val_mae: 30022.9707\n",
            "Epoch 47/500\n",
            "80/80 [==============================] - 0s 708us/step - loss: 28347.1125 - mse: 1049455296.0000 - mae: 28347.1133 - val_loss: 29682.1317 - val_mse: 1119639296.0000 - val_mae: 29682.1328\n",
            "Epoch 48/500\n",
            "80/80 [==============================] - 0s 747us/step - loss: 28308.3691 - mse: 1045016576.0000 - mae: 28308.3691 - val_loss: 29345.4979 - val_mse: 1099386112.0000 - val_mae: 29345.4980\n",
            "Epoch 49/500\n",
            "80/80 [==============================] - 0s 741us/step - loss: 27589.3025 - mse: 1005595840.0000 - mae: 27589.3027 - val_loss: 28983.8239 - val_mse: 1077890432.0000 - val_mae: 28983.8242\n",
            "Epoch 50/500\n",
            "80/80 [==============================] - 0s 672us/step - loss: 26789.3694 - mse: 965006144.0000 - mae: 26789.3711 - val_loss: 28596.7962 - val_mse: 1055178304.0000 - val_mae: 28596.7969\n",
            "Epoch 51/500\n",
            "80/80 [==============================] - 0s 695us/step - loss: 26813.0129 - mse: 959971136.0000 - mae: 26813.0117 - val_loss: 28219.5579 - val_mse: 1033343424.0000 - val_mae: 28219.5566\n",
            "Epoch 52/500\n",
            "80/80 [==============================] - 0s 629us/step - loss: 26380.8270 - mse: 951992512.0000 - mae: 26380.8242 - val_loss: 27830.4978 - val_mse: 1011148992.0000 - val_mae: 27830.4980\n",
            "Epoch 53/500\n",
            "80/80 [==============================] - 0s 627us/step - loss: 26052.2163 - mse: 918689088.0000 - mae: 26052.2188 - val_loss: 27438.5314 - val_mse: 989054336.0000 - val_mae: 27438.5312\n",
            "Epoch 54/500\n",
            "80/80 [==============================] - 0s 655us/step - loss: 25243.2720 - mse: 889141568.0000 - mae: 25243.2715 - val_loss: 27013.2633 - val_mse: 965483840.0000 - val_mae: 27013.2617\n",
            "Epoch 55/500\n",
            "80/80 [==============================] - 0s 602us/step - loss: 25102.0538 - mse: 878653568.0000 - mae: 25102.0547 - val_loss: 26587.2766 - val_mse: 942241728.0000 - val_mae: 26587.2754\n",
            "Epoch 56/500\n",
            "80/80 [==============================] - 0s 636us/step - loss: 24644.0786 - mse: 853470080.0000 - mae: 24644.0801 - val_loss: 26160.3313 - val_mse: 919299520.0000 - val_mae: 26160.3301\n",
            "Epoch 57/500\n",
            "80/80 [==============================] - 0s 671us/step - loss: 24159.2317 - mse: 833743552.0000 - mae: 24159.2305 - val_loss: 25711.0459 - val_mse: 895588544.0000 - val_mae: 25711.0469\n",
            "Epoch 58/500\n",
            "80/80 [==============================] - 0s 673us/step - loss: 23379.1716 - mse: 778935552.0000 - mae: 23379.1699 - val_loss: 25253.9883 - val_mse: 871873024.0000 - val_mae: 25253.9902\n",
            "Epoch 59/500\n",
            "80/80 [==============================] - 0s 636us/step - loss: 22867.5051 - mse: 755043136.0000 - mae: 22867.5039 - val_loss: 24786.0778 - val_mse: 848043584.0000 - val_mae: 24786.0781\n",
            "Epoch 60/500\n",
            "80/80 [==============================] - 0s 693us/step - loss: 22518.6179 - mse: 736947136.0000 - mae: 22518.6172 - val_loss: 24331.7690 - val_mse: 825336832.0000 - val_mae: 24331.7695\n",
            "Epoch 61/500\n",
            "80/80 [==============================] - 0s 750us/step - loss: 22331.3185 - mse: 757475072.0000 - mae: 22331.3164 - val_loss: 23877.3743 - val_mse: 803079872.0000 - val_mae: 23877.3750\n",
            "Epoch 62/500\n",
            "80/80 [==============================] - 0s 654us/step - loss: 22078.2849 - mse: 733180928.0000 - mae: 22078.2852 - val_loss: 23433.3338 - val_mse: 781690240.0000 - val_mae: 23433.3340\n",
            "Epoch 63/500\n",
            "80/80 [==============================] - 0s 672us/step - loss: 21487.9922 - mse: 716267648.0000 - mae: 21487.9922 - val_loss: 22945.4278 - val_mse: 758684416.0000 - val_mae: 22945.4277\n",
            "Epoch 64/500\n",
            "80/80 [==============================] - 0s 686us/step - loss: 21323.1742 - mse: 685395776.0000 - mae: 21323.1758 - val_loss: 22467.6228 - val_mse: 736628032.0000 - val_mae: 22467.6211\n",
            "Epoch 65/500\n",
            "80/80 [==============================] - 0s 679us/step - loss: 20364.6617 - mse: 659992704.0000 - mae: 20364.6621 - val_loss: 21981.9902 - val_mse: 714639168.0000 - val_mae: 21981.9902\n",
            "Epoch 66/500\n",
            "80/80 [==============================] - 0s 719us/step - loss: 20214.4723 - mse: 644601664.0000 - mae: 20214.4727 - val_loss: 21529.6127 - val_mse: 694630080.0000 - val_mae: 21529.6152\n",
            "Epoch 67/500\n",
            "80/80 [==============================] - 0s 684us/step - loss: 19884.5717 - mse: 630827328.0000 - mae: 19884.5723 - val_loss: 21096.4036 - val_mse: 675869632.0000 - val_mae: 21096.4043\n",
            "Epoch 68/500\n",
            "80/80 [==============================] - 0s 697us/step - loss: 19796.5969 - mse: 626241152.0000 - mae: 19796.5977 - val_loss: 20638.0175 - val_mse: 656444352.0000 - val_mae: 20638.0176\n",
            "Epoch 69/500\n",
            "80/80 [==============================] - 0s 714us/step - loss: 18049.3831 - mse: 559803008.0000 - mae: 18049.3848 - val_loss: 20160.7298 - val_mse: 636590528.0000 - val_mae: 20160.7285\n",
            "Epoch 70/500\n",
            "80/80 [==============================] - 0s 704us/step - loss: 19036.8676 - mse: 587320128.0000 - mae: 19036.8691 - val_loss: 19706.8883 - val_mse: 618182976.0000 - val_mae: 19706.8887\n",
            "Epoch 71/500\n",
            "80/80 [==============================] - 0s 632us/step - loss: 18661.3079 - mse: 564652160.0000 - mae: 18661.3066 - val_loss: 19282.6519 - val_mse: 601340736.0000 - val_mae: 19282.6523\n",
            "Epoch 72/500\n",
            "80/80 [==============================] - 0s 602us/step - loss: 18133.4094 - mse: 532637280.0000 - mae: 18133.4102 - val_loss: 18940.3528 - val_mse: 587997312.0000 - val_mae: 18940.3535\n",
            "Epoch 73/500\n",
            "80/80 [==============================] - 0s 602us/step - loss: 17648.6062 - mse: 518799296.0000 - mae: 17648.6074 - val_loss: 18532.5710 - val_mse: 572489856.0000 - val_mae: 18532.5723\n",
            "Epoch 74/500\n",
            "80/80 [==============================] - 0s 635us/step - loss: 17477.2832 - mse: 518260064.0000 - mae: 17477.2832 - val_loss: 18119.1299 - val_mse: 556719872.0000 - val_mae: 18119.1309\n",
            "Epoch 75/500\n",
            "80/80 [==============================] - 0s 628us/step - loss: 18605.1705 - mse: 563198272.0000 - mae: 18605.1699 - val_loss: 17735.6087 - val_mse: 540928960.0000 - val_mae: 17735.6094\n",
            "Epoch 76/500\n",
            "80/80 [==============================] - 0s 646us/step - loss: 17844.0841 - mse: 549670080.0000 - mae: 17844.0840 - val_loss: 17456.8007 - val_mse: 528525728.0000 - val_mae: 17456.8008\n",
            "Epoch 77/500\n",
            "80/80 [==============================] - 0s 613us/step - loss: 16824.7637 - mse: 482692704.0000 - mae: 16824.7617 - val_loss: 17200.7198 - val_mse: 515560480.0000 - val_mae: 17200.7207\n",
            "Epoch 78/500\n",
            "80/80 [==============================] - 0s 615us/step - loss: 16185.9713 - mse: 463891872.0000 - mae: 16185.9717 - val_loss: 16953.9066 - val_mse: 501980000.0000 - val_mae: 16953.9062\n",
            "Epoch 79/500\n",
            "80/80 [==============================] - 0s 610us/step - loss: 16560.1436 - mse: 481522112.0000 - mae: 16560.1445 - val_loss: 16728.9572 - val_mse: 489794752.0000 - val_mae: 16728.9570\n",
            "Epoch 80/500\n",
            "80/80 [==============================] - 0s 620us/step - loss: 16572.4520 - mse: 452343360.0000 - mae: 16572.4512 - val_loss: 16523.6912 - val_mse: 478871424.0000 - val_mae: 16523.6914\n",
            "Epoch 81/500\n",
            "80/80 [==============================] - 0s 580us/step - loss: 15841.0309 - mse: 425844064.0000 - mae: 15841.0312 - val_loss: 16345.2703 - val_mse: 469423296.0000 - val_mae: 16345.2695\n",
            "Epoch 82/500\n",
            "80/80 [==============================] - 0s 545us/step - loss: 15965.7671 - mse: 465628064.0000 - mae: 15965.7656 - val_loss: 16152.5876 - val_mse: 459513984.0000 - val_mae: 16152.5879\n",
            "Epoch 83/500\n",
            "80/80 [==============================] - 0s 591us/step - loss: 14800.3119 - mse: 392182560.0000 - mae: 14800.3125 - val_loss: 15943.9077 - val_mse: 448730016.0000 - val_mae: 15943.9072\n",
            "Epoch 84/500\n",
            "80/80 [==============================] - 0s 641us/step - loss: 16170.0771 - mse: 478341952.0000 - mae: 16170.0762 - val_loss: 15751.8537 - val_mse: 438832928.0000 - val_mae: 15751.8535\n",
            "Epoch 85/500\n",
            "80/80 [==============================] - 0s 711us/step - loss: 15021.6687 - mse: 403231680.0000 - mae: 15021.6689 - val_loss: 15543.5544 - val_mse: 428419840.0000 - val_mae: 15543.5537\n",
            "Epoch 86/500\n",
            "80/80 [==============================] - 0s 717us/step - loss: 15326.1160 - mse: 416393664.0000 - mae: 15326.1172 - val_loss: 15386.4004 - val_mse: 420656928.0000 - val_mae: 15386.4004\n",
            "Epoch 87/500\n",
            "80/80 [==============================] - 0s 645us/step - loss: 14451.9425 - mse: 380634560.0000 - mae: 14451.9424 - val_loss: 15182.3770 - val_mse: 410934144.0000 - val_mae: 15182.3770\n",
            "Epoch 88/500\n",
            "80/80 [==============================] - 0s 710us/step - loss: 15056.4319 - mse: 384628704.0000 - mae: 15056.4316 - val_loss: 15035.6710 - val_mse: 403977728.0000 - val_mae: 15035.6719\n",
            "Epoch 89/500\n",
            "80/80 [==============================] - 0s 590us/step - loss: 15665.4606 - mse: 435630848.0000 - mae: 15665.4609 - val_loss: 14878.2529 - val_mse: 396704384.0000 - val_mae: 14878.2520\n",
            "Epoch 90/500\n",
            "80/80 [==============================] - 0s 643us/step - loss: 15820.9681 - mse: 423790528.0000 - mae: 15820.9688 - val_loss: 14749.5915 - val_mse: 390965216.0000 - val_mae: 14749.5918\n",
            "Epoch 91/500\n",
            "80/80 [==============================] - 0s 640us/step - loss: 14507.1466 - mse: 391275936.0000 - mae: 14507.1455 - val_loss: 14540.4778 - val_mse: 381678528.0000 - val_mae: 14540.4785\n",
            "Epoch 92/500\n",
            "80/80 [==============================] - 0s 732us/step - loss: 14476.2147 - mse: 358969728.0000 - mae: 14476.2158 - val_loss: 14381.4933 - val_mse: 374656608.0000 - val_mae: 14381.4941\n",
            "Epoch 93/500\n",
            "80/80 [==============================] - 0s 822us/step - loss: 15123.3118 - mse: 407524512.0000 - mae: 15123.3105 - val_loss: 14252.1972 - val_mse: 368777728.0000 - val_mae: 14252.1963\n",
            "Epoch 94/500\n",
            "80/80 [==============================] - 0s 654us/step - loss: 13968.4247 - mse: 361051360.0000 - mae: 13968.4248 - val_loss: 14115.5306 - val_mse: 362743584.0000 - val_mae: 14115.5303\n",
            "Epoch 95/500\n",
            "80/80 [==============================] - 0s 627us/step - loss: 14278.5754 - mse: 342071264.0000 - mae: 14278.5752 - val_loss: 13940.8356 - val_mse: 355647328.0000 - val_mae: 13940.8359\n",
            "Epoch 96/500\n",
            "80/80 [==============================] - 0s 674us/step - loss: 14355.3796 - mse: 373865824.0000 - mae: 14355.3799 - val_loss: 13804.5938 - val_mse: 351581568.0000 - val_mae: 13804.5938\n",
            "Epoch 97/500\n",
            "80/80 [==============================] - 0s 633us/step - loss: 13570.7307 - mse: 335594368.0000 - mae: 13570.7314 - val_loss: 13716.9936 - val_mse: 345954240.0000 - val_mae: 13716.9932\n",
            "Epoch 98/500\n",
            "80/80 [==============================] - 0s 669us/step - loss: 14573.6057 - mse: 363114528.0000 - mae: 14573.6064 - val_loss: 13645.1020 - val_mse: 343016640.0000 - val_mae: 13645.1016\n",
            "Epoch 99/500\n",
            "80/80 [==============================] - 0s 563us/step - loss: 13543.8685 - mse: 329959488.0000 - mae: 13543.8691 - val_loss: 13517.2556 - val_mse: 337980960.0000 - val_mae: 13517.2549\n",
            "Epoch 100/500\n",
            "80/80 [==============================] - 0s 553us/step - loss: 14180.7969 - mse: 342335424.0000 - mae: 14180.7969 - val_loss: 13326.3563 - val_mse: 332497440.0000 - val_mae: 13326.3564\n",
            "Epoch 101/500\n",
            "80/80 [==============================] - 0s 665us/step - loss: 13405.9996 - mse: 325280064.0000 - mae: 13406.0000 - val_loss: 13400.2884 - val_mse: 331279616.0000 - val_mae: 13400.2881\n",
            "Epoch 102/500\n",
            "80/80 [==============================] - 0s 811us/step - loss: 14537.8806 - mse: 355495360.0000 - mae: 14537.8809 - val_loss: 13226.8960 - val_mse: 327283616.0000 - val_mae: 13226.8965\n",
            "Epoch 103/500\n",
            "80/80 [==============================] - 0s 630us/step - loss: 14047.0970 - mse: 335604512.0000 - mae: 14047.0967 - val_loss: 13093.0542 - val_mse: 322324256.0000 - val_mae: 13093.0547\n",
            "Epoch 104/500\n",
            "80/80 [==============================] - 0s 588us/step - loss: 12555.6340 - mse: 296136416.0000 - mae: 12555.6348 - val_loss: 13014.8455 - val_mse: 318797888.0000 - val_mae: 13014.8467\n",
            "Epoch 105/500\n",
            "80/80 [==============================] - 0s 714us/step - loss: 12606.2363 - mse: 292034272.0000 - mae: 12606.2373 - val_loss: 12938.5283 - val_mse: 314633760.0000 - val_mae: 12938.5283\n",
            "Epoch 106/500\n",
            "80/80 [==============================] - 0s 771us/step - loss: 13324.7751 - mse: 341926272.0000 - mae: 13324.7754 - val_loss: 12835.0442 - val_mse: 310007136.0000 - val_mae: 12835.0439\n",
            "Epoch 107/500\n",
            "80/80 [==============================] - 0s 619us/step - loss: 13552.3234 - mse: 308089152.0000 - mae: 13552.3232 - val_loss: 12734.7209 - val_mse: 305737312.0000 - val_mae: 12734.7217\n",
            "Epoch 108/500\n",
            "80/80 [==============================] - 0s 634us/step - loss: 12958.0366 - mse: 328750624.0000 - mae: 12958.0371 - val_loss: 12654.0883 - val_mse: 301439296.0000 - val_mae: 12654.0889\n",
            "Epoch 109/500\n",
            "80/80 [==============================] - 0s 682us/step - loss: 11940.2781 - mse: 257045456.0000 - mae: 11940.2773 - val_loss: 12577.6071 - val_mse: 297113760.0000 - val_mae: 12577.6084\n",
            "Epoch 110/500\n",
            "80/80 [==============================] - 0s 683us/step - loss: 12968.3550 - mse: 312332576.0000 - mae: 12968.3545 - val_loss: 12537.6412 - val_mse: 295658816.0000 - val_mae: 12537.6406\n",
            "Epoch 111/500\n",
            "80/80 [==============================] - 0s 682us/step - loss: 13274.9863 - mse: 305372096.0000 - mae: 13274.9873 - val_loss: 12444.6015 - val_mse: 291472736.0000 - val_mae: 12444.6016\n",
            "Epoch 112/500\n",
            "80/80 [==============================] - 0s 644us/step - loss: 12427.2128 - mse: 275763648.0000 - mae: 12427.2129 - val_loss: 12406.7129 - val_mse: 289179840.0000 - val_mae: 12406.7139\n",
            "Epoch 113/500\n",
            "80/80 [==============================] - 0s 612us/step - loss: 14268.4211 - mse: 333795744.0000 - mae: 14268.4219 - val_loss: 12356.5783 - val_mse: 286596128.0000 - val_mae: 12356.5781\n",
            "Epoch 114/500\n",
            "80/80 [==============================] - 0s 644us/step - loss: 11512.0540 - mse: 236844624.0000 - mae: 11512.0527 - val_loss: 12279.0601 - val_mse: 283366720.0000 - val_mae: 12279.0596\n",
            "Epoch 115/500\n",
            "80/80 [==============================] - 0s 666us/step - loss: 12853.6857 - mse: 272241472.0000 - mae: 12853.6855 - val_loss: 12243.9525 - val_mse: 281465088.0000 - val_mae: 12243.9521\n",
            "Epoch 116/500\n",
            "80/80 [==============================] - 0s 764us/step - loss: 12945.6509 - mse: 297041696.0000 - mae: 12945.6504 - val_loss: 12194.0485 - val_mse: 279411840.0000 - val_mae: 12194.0479\n",
            "Epoch 117/500\n",
            "80/80 [==============================] - 0s 683us/step - loss: 12618.6390 - mse: 264592544.0000 - mae: 12618.6387 - val_loss: 12151.9976 - val_mse: 277221312.0000 - val_mae: 12151.9980\n",
            "Epoch 118/500\n",
            "80/80 [==============================] - 0s 671us/step - loss: 12568.3724 - mse: 264086608.0000 - mae: 12568.3721 - val_loss: 12097.5285 - val_mse: 274959200.0000 - val_mae: 12097.5283\n",
            "Epoch 119/500\n",
            "80/80 [==============================] - 0s 622us/step - loss: 12484.9605 - mse: 280943872.0000 - mae: 12484.9619 - val_loss: 12061.9696 - val_mse: 273655776.0000 - val_mae: 12061.9697\n",
            "Epoch 120/500\n",
            "80/80 [==============================] - 0s 755us/step - loss: 13484.6458 - mse: 295660032.0000 - mae: 13484.6455 - val_loss: 12018.5672 - val_mse: 271831392.0000 - val_mae: 12018.5674\n",
            "Epoch 121/500\n",
            "80/80 [==============================] - 0s 678us/step - loss: 12852.6462 - mse: 278998432.0000 - mae: 12852.6465 - val_loss: 11947.4759 - val_mse: 269138336.0000 - val_mae: 11947.4756\n",
            "Epoch 122/500\n",
            "80/80 [==============================] - 0s 620us/step - loss: 12684.7173 - mse: 279725216.0000 - mae: 12684.7168 - val_loss: 11893.7615 - val_mse: 267015168.0000 - val_mae: 11893.7617\n",
            "Epoch 123/500\n",
            "80/80 [==============================] - 0s 654us/step - loss: 12488.3287 - mse: 257720320.0000 - mae: 12488.3281 - val_loss: 11851.1289 - val_mse: 264719104.0000 - val_mae: 11851.1289\n",
            "Epoch 124/500\n",
            "80/80 [==============================] - 0s 749us/step - loss: 12698.2461 - mse: 264941872.0000 - mae: 12698.2461 - val_loss: 11819.2117 - val_mse: 263083680.0000 - val_mae: 11819.2129\n",
            "Epoch 125/500\n",
            "80/80 [==============================] - 0s 683us/step - loss: 13688.2421 - mse: 307028544.0000 - mae: 13688.2402 - val_loss: 11775.8371 - val_mse: 261300688.0000 - val_mae: 11775.8379\n",
            "Epoch 126/500\n",
            "80/80 [==============================] - 0s 819us/step - loss: 12598.0635 - mse: 264806240.0000 - mae: 12598.0645 - val_loss: 11740.4474 - val_mse: 259946080.0000 - val_mae: 11740.4482\n",
            "Epoch 127/500\n",
            "80/80 [==============================] - 0s 733us/step - loss: 12074.6359 - mse: 257225344.0000 - mae: 12074.6348 - val_loss: 11694.6252 - val_mse: 257581728.0000 - val_mae: 11694.6250\n",
            "Epoch 128/500\n",
            "80/80 [==============================] - 0s 841us/step - loss: 13037.9448 - mse: 263494400.0000 - mae: 13037.9463 - val_loss: 11659.0343 - val_mse: 256035024.0000 - val_mae: 11659.0342\n",
            "Epoch 129/500\n",
            "80/80 [==============================] - 0s 840us/step - loss: 12215.4398 - mse: 247721648.0000 - mae: 12215.4395 - val_loss: 11621.4651 - val_mse: 254259200.0000 - val_mae: 11621.4658\n",
            "Epoch 130/500\n",
            "80/80 [==============================] - 0s 732us/step - loss: 14365.7468 - mse: 321703296.0000 - mae: 14365.7471 - val_loss: 11611.2291 - val_mse: 253937552.0000 - val_mae: 11611.2305\n",
            "Epoch 131/500\n",
            "80/80 [==============================] - 0s 694us/step - loss: 13378.7159 - mse: 282621344.0000 - mae: 13378.7158 - val_loss: 11564.5232 - val_mse: 252101248.0000 - val_mae: 11564.5234\n",
            "Epoch 132/500\n",
            "80/80 [==============================] - 0s 636us/step - loss: 12642.2281 - mse: 251450656.0000 - mae: 12642.2285 - val_loss: 11554.6369 - val_mse: 251827120.0000 - val_mae: 11554.6367\n",
            "Epoch 133/500\n",
            "80/80 [==============================] - 0s 690us/step - loss: 12413.8422 - mse: 239740848.0000 - mae: 12413.8418 - val_loss: 11525.1697 - val_mse: 250326864.0000 - val_mae: 11525.1689\n",
            "Epoch 134/500\n",
            "80/80 [==============================] - 0s 648us/step - loss: 11937.5840 - mse: 258815440.0000 - mae: 11937.5840 - val_loss: 11448.9347 - val_mse: 247613792.0000 - val_mae: 11448.9346\n",
            "Epoch 135/500\n",
            "80/80 [==============================] - 0s 696us/step - loss: 12915.6136 - mse: 268135088.0000 - mae: 12915.6152 - val_loss: 11423.4092 - val_mse: 246413552.0000 - val_mae: 11423.4092\n",
            "Epoch 136/500\n",
            "80/80 [==============================] - 0s 692us/step - loss: 12791.4993 - mse: 277184832.0000 - mae: 12791.4990 - val_loss: 11372.7923 - val_mse: 244445696.0000 - val_mae: 11372.7930\n",
            "Epoch 137/500\n",
            "80/80 [==============================] - 0s 691us/step - loss: 12952.1748 - mse: 273933120.0000 - mae: 12952.1748 - val_loss: 11336.6976 - val_mse: 243266576.0000 - val_mae: 11336.6982\n",
            "Epoch 138/500\n",
            "80/80 [==============================] - 0s 703us/step - loss: 11333.5659 - mse: 226729216.0000 - mae: 11333.5654 - val_loss: 11288.9965 - val_mse: 241678816.0000 - val_mae: 11288.9961\n",
            "Epoch 139/500\n",
            "80/80 [==============================] - 0s 703us/step - loss: 12746.5746 - mse: 272389568.0000 - mae: 12746.5742 - val_loss: 11281.5519 - val_mse: 241199888.0000 - val_mae: 11281.5518\n",
            "Epoch 140/500\n",
            "80/80 [==============================] - 0s 646us/step - loss: 11866.1074 - mse: 248142768.0000 - mae: 11866.1074 - val_loss: 11239.8926 - val_mse: 239781376.0000 - val_mae: 11239.8926\n",
            "Epoch 141/500\n",
            "80/80 [==============================] - 0s 781us/step - loss: 12112.3667 - mse: 242161696.0000 - mae: 12112.3662 - val_loss: 11197.9240 - val_mse: 238765472.0000 - val_mae: 11197.9229\n",
            "Epoch 142/500\n",
            "80/80 [==============================] - 0s 589us/step - loss: 10859.6502 - mse: 219472464.0000 - mae: 10859.6504 - val_loss: 11136.0835 - val_mse: 235544032.0000 - val_mae: 11136.0830\n",
            "Epoch 143/500\n",
            "80/80 [==============================] - 0s 617us/step - loss: 10932.2158 - mse: 218714416.0000 - mae: 10932.2168 - val_loss: 11082.2184 - val_mse: 233147264.0000 - val_mae: 11082.2178\n",
            "Epoch 144/500\n",
            "80/80 [==============================] - 0s 653us/step - loss: 11443.8878 - mse: 225402320.0000 - mae: 11443.8887 - val_loss: 11024.3469 - val_mse: 230704064.0000 - val_mae: 11024.3467\n",
            "Epoch 145/500\n",
            "80/80 [==============================] - 0s 681us/step - loss: 12398.9614 - mse: 268713920.0000 - mae: 12398.9609 - val_loss: 10982.7685 - val_mse: 229080240.0000 - val_mae: 10982.7686\n",
            "Epoch 146/500\n",
            "80/80 [==============================] - 0s 658us/step - loss: 12346.9808 - mse: 261330944.0000 - mae: 12346.9814 - val_loss: 10917.1686 - val_mse: 226667088.0000 - val_mae: 10917.1689\n",
            "Epoch 147/500\n",
            "80/80 [==============================] - 0s 647us/step - loss: 12503.2531 - mse: 239846656.0000 - mae: 12503.2529 - val_loss: 10887.8048 - val_mse: 225631344.0000 - val_mae: 10887.8047\n",
            "Epoch 148/500\n",
            "80/80 [==============================] - 0s 725us/step - loss: 11898.5890 - mse: 226662560.0000 - mae: 11898.5879 - val_loss: 10857.8302 - val_mse: 224986944.0000 - val_mae: 10857.8301\n",
            "Epoch 149/500\n",
            "80/80 [==============================] - 0s 760us/step - loss: 12948.3531 - mse: 290149504.0000 - mae: 12948.3535 - val_loss: 10809.8624 - val_mse: 223485760.0000 - val_mae: 10809.8623\n",
            "Epoch 150/500\n",
            "80/80 [==============================] - 0s 762us/step - loss: 10850.9811 - mse: 203365744.0000 - mae: 10850.9805 - val_loss: 10815.4207 - val_mse: 222364848.0000 - val_mae: 10815.4209\n",
            "Epoch 151/500\n",
            "80/80 [==============================] - 0s 732us/step - loss: 11406.8051 - mse: 246960176.0000 - mae: 11406.8057 - val_loss: 10772.9349 - val_mse: 221075680.0000 - val_mae: 10772.9355\n",
            "Epoch 152/500\n",
            "80/80 [==============================] - 0s 725us/step - loss: 11970.1198 - mse: 237438416.0000 - mae: 11970.1191 - val_loss: 10705.2360 - val_mse: 219429808.0000 - val_mae: 10705.2363\n",
            "Epoch 153/500\n",
            "80/80 [==============================] - 0s 633us/step - loss: 11707.0320 - mse: 225274240.0000 - mae: 11707.0322 - val_loss: 10628.7357 - val_mse: 217063744.0000 - val_mae: 10628.7354\n",
            "Epoch 154/500\n",
            "80/80 [==============================] - 0s 669us/step - loss: 12807.4948 - mse: 263080352.0000 - mae: 12807.4951 - val_loss: 10642.5653 - val_mse: 216733696.0000 - val_mae: 10642.5664\n",
            "Epoch 155/500\n",
            "80/80 [==============================] - 0s 681us/step - loss: 10182.4212 - mse: 181379552.0000 - mae: 10182.4219 - val_loss: 10599.0786 - val_mse: 215617536.0000 - val_mae: 10599.0781\n",
            "Epoch 156/500\n",
            "80/80 [==============================] - 0s 616us/step - loss: 11767.0134 - mse: 230645072.0000 - mae: 11767.0127 - val_loss: 10560.7361 - val_mse: 214516736.0000 - val_mae: 10560.7363\n",
            "Epoch 157/500\n",
            "80/80 [==============================] - 0s 641us/step - loss: 11349.6876 - mse: 209157184.0000 - mae: 11349.6875 - val_loss: 10542.1705 - val_mse: 212934208.0000 - val_mae: 10542.1719\n",
            "Epoch 158/500\n",
            "80/80 [==============================] - 0s 651us/step - loss: 10835.3020 - mse: 217616464.0000 - mae: 10835.3018 - val_loss: 10508.2016 - val_mse: 212167920.0000 - val_mae: 10508.2021\n",
            "Epoch 159/500\n",
            "80/80 [==============================] - 0s 816us/step - loss: 11554.8599 - mse: 231853872.0000 - mae: 11554.8594 - val_loss: 10473.9839 - val_mse: 210533296.0000 - val_mae: 10473.9844\n",
            "Epoch 160/500\n",
            "80/80 [==============================] - 0s 677us/step - loss: 12964.5923 - mse: 269904256.0000 - mae: 12964.5918 - val_loss: 10469.6190 - val_mse: 210040608.0000 - val_mae: 10469.6201\n",
            "Epoch 161/500\n",
            "80/80 [==============================] - 0s 633us/step - loss: 11642.0731 - mse: 231822896.0000 - mae: 11642.0732 - val_loss: 10418.4943 - val_mse: 209578128.0000 - val_mae: 10418.4951\n",
            "Epoch 162/500\n",
            "80/80 [==============================] - 0s 595us/step - loss: 11710.5040 - mse: 234333648.0000 - mae: 11710.5039 - val_loss: 10405.4549 - val_mse: 208445472.0000 - val_mae: 10405.4551\n",
            "Epoch 163/500\n",
            "80/80 [==============================] - 0s 673us/step - loss: 11958.4260 - mse: 221592736.0000 - mae: 11958.4258 - val_loss: 10383.4034 - val_mse: 207033200.0000 - val_mae: 10383.4033\n",
            "Epoch 164/500\n",
            "80/80 [==============================] - 0s 635us/step - loss: 11600.3768 - mse: 233730608.0000 - mae: 11600.3770 - val_loss: 10416.3135 - val_mse: 206135216.0000 - val_mae: 10416.3135\n",
            "Epoch 165/500\n",
            "80/80 [==============================] - 0s 641us/step - loss: 13283.9178 - mse: 285634272.0000 - mae: 13283.9189 - val_loss: 10409.9249 - val_mse: 205872656.0000 - val_mae: 10409.9248\n",
            "Epoch 166/500\n",
            "80/80 [==============================] - 0s 690us/step - loss: 11875.3170 - mse: 237526864.0000 - mae: 11875.3174 - val_loss: 10426.7559 - val_mse: 205468048.0000 - val_mae: 10426.7559\n",
            "Epoch 167/500\n",
            "80/80 [==============================] - 0s 699us/step - loss: 11458.7424 - mse: 233077072.0000 - mae: 11458.7422 - val_loss: 10410.6699 - val_mse: 205032240.0000 - val_mae: 10410.6699\n",
            "Epoch 168/500\n",
            "80/80 [==============================] - 0s 672us/step - loss: 10706.5897 - mse: 197217040.0000 - mae: 10706.5898 - val_loss: 10339.9569 - val_mse: 202901712.0000 - val_mae: 10339.9570\n",
            "Epoch 169/500\n",
            "80/80 [==============================] - 0s 649us/step - loss: 12477.0941 - mse: 266923872.0000 - mae: 12477.0938 - val_loss: 10311.4244 - val_mse: 202277568.0000 - val_mae: 10311.4238\n",
            "Epoch 170/500\n",
            "80/80 [==============================] - 0s 671us/step - loss: 11592.6873 - mse: 225068544.0000 - mae: 11592.6875 - val_loss: 10310.4288 - val_mse: 201765024.0000 - val_mae: 10310.4287\n",
            "Epoch 171/500\n",
            "80/80 [==============================] - 0s 680us/step - loss: 11173.4044 - mse: 205575104.0000 - mae: 11173.4043 - val_loss: 10273.5613 - val_mse: 200438176.0000 - val_mae: 10273.5605\n",
            "Epoch 172/500\n",
            "80/80 [==============================] - 0s 770us/step - loss: 10308.6997 - mse: 182040096.0000 - mae: 10308.7002 - val_loss: 10210.6130 - val_mse: 198218688.0000 - val_mae: 10210.6123\n",
            "Epoch 173/500\n",
            "80/80 [==============================] - 0s 778us/step - loss: 12011.3136 - mse: 245556016.0000 - mae: 12011.3145 - val_loss: 10197.6487 - val_mse: 197678768.0000 - val_mae: 10197.6484\n",
            "Epoch 174/500\n",
            "80/80 [==============================] - 0s 675us/step - loss: 12073.2623 - mse: 251170528.0000 - mae: 12073.2627 - val_loss: 10229.4634 - val_mse: 197736288.0000 - val_mae: 10229.4629\n",
            "Epoch 175/500\n",
            "80/80 [==============================] - 0s 657us/step - loss: 12022.3958 - mse: 246152928.0000 - mae: 12022.3955 - val_loss: 10173.1913 - val_mse: 196016080.0000 - val_mae: 10173.1914\n",
            "Epoch 176/500\n",
            "80/80 [==============================] - 0s 663us/step - loss: 10456.9415 - mse: 190494752.0000 - mae: 10456.9414 - val_loss: 10166.2395 - val_mse: 195233248.0000 - val_mae: 10166.2393\n",
            "Epoch 177/500\n",
            "80/80 [==============================] - 0s 737us/step - loss: 13239.3673 - mse: 280142912.0000 - mae: 13239.3672 - val_loss: 10180.4913 - val_mse: 195642160.0000 - val_mae: 10180.4912\n",
            "Epoch 178/500\n",
            "80/80 [==============================] - 0s 639us/step - loss: 13486.8950 - mse: 317067456.0000 - mae: 13486.8936 - val_loss: 10136.8402 - val_mse: 194403008.0000 - val_mae: 10136.8398\n",
            "Epoch 179/500\n",
            "80/80 [==============================] - 0s 665us/step - loss: 10496.1998 - mse: 179211904.0000 - mae: 10496.2002 - val_loss: 10172.8705 - val_mse: 193915616.0000 - val_mae: 10172.8711\n",
            "Epoch 180/500\n",
            "80/80 [==============================] - 0s 715us/step - loss: 11395.3494 - mse: 213408832.0000 - mae: 11395.3486 - val_loss: 10205.9466 - val_mse: 194086848.0000 - val_mae: 10205.9463\n",
            "Epoch 181/500\n",
            "80/80 [==============================] - 0s 775us/step - loss: 11857.7561 - mse: 228457440.0000 - mae: 11857.7559 - val_loss: 10189.5920 - val_mse: 194323360.0000 - val_mae: 10189.5918\n",
            "Epoch 182/500\n",
            "80/80 [==============================] - 0s 651us/step - loss: 10535.4506 - mse: 177738960.0000 - mae: 10535.4512 - val_loss: 10186.0535 - val_mse: 193656304.0000 - val_mae: 10186.0527\n",
            "Epoch 183/500\n",
            "80/80 [==============================] - 0s 649us/step - loss: 11791.4524 - mse: 219208160.0000 - mae: 11791.4512 - val_loss: 10151.5229 - val_mse: 193416048.0000 - val_mae: 10151.5234\n",
            "Epoch 184/500\n",
            "80/80 [==============================] - 0s 733us/step - loss: 12288.0151 - mse: 269414912.0000 - mae: 12288.0146 - val_loss: 10121.2012 - val_mse: 192539504.0000 - val_mae: 10121.2002\n",
            "Epoch 185/500\n",
            "80/80 [==============================] - 0s 780us/step - loss: 11882.9003 - mse: 234886912.0000 - mae: 11882.9004 - val_loss: 10085.4834 - val_mse: 191977760.0000 - val_mae: 10085.4844\n",
            "Epoch 186/500\n",
            "80/80 [==============================] - 0s 629us/step - loss: 12003.9824 - mse: 256393296.0000 - mae: 12003.9824 - val_loss: 10022.9036 - val_mse: 190776592.0000 - val_mae: 10022.9033\n",
            "Epoch 187/500\n",
            "80/80 [==============================] - 0s 690us/step - loss: 11533.0247 - mse: 215833504.0000 - mae: 11533.0244 - val_loss: 9993.3820 - val_mse: 189877008.0000 - val_mae: 9993.3818\n",
            "Epoch 188/500\n",
            "80/80 [==============================] - 0s 769us/step - loss: 12351.6563 - mse: 260774688.0000 - mae: 12351.6572 - val_loss: 10014.1765 - val_mse: 189821632.0000 - val_mae: 10014.1768\n",
            "Epoch 189/500\n",
            "80/80 [==============================] - 0s 727us/step - loss: 10939.2423 - mse: 209650592.0000 - mae: 10939.2422 - val_loss: 10042.8573 - val_mse: 189672320.0000 - val_mae: 10042.8574\n",
            "Epoch 190/500\n",
            "80/80 [==============================] - 0s 818us/step - loss: 12265.4184 - mse: 237294688.0000 - mae: 12265.4180 - val_loss: 10022.0261 - val_mse: 189496096.0000 - val_mae: 10022.0264\n",
            "Epoch 191/500\n",
            "80/80 [==============================] - 0s 662us/step - loss: 12887.5299 - mse: 279372064.0000 - mae: 12887.5293 - val_loss: 9996.3171 - val_mse: 188788688.0000 - val_mae: 9996.3174\n",
            "Epoch 192/500\n",
            "80/80 [==============================] - 0s 702us/step - loss: 12692.7885 - mse: 271772416.0000 - mae: 12692.7891 - val_loss: 9994.8071 - val_mse: 188172112.0000 - val_mae: 9994.8076\n",
            "Epoch 193/500\n",
            "80/80 [==============================] - 0s 697us/step - loss: 11934.6616 - mse: 245696816.0000 - mae: 11934.6611 - val_loss: 10025.3780 - val_mse: 187924864.0000 - val_mae: 10025.3779\n",
            "Epoch 194/500\n",
            "80/80 [==============================] - 0s 733us/step - loss: 11386.2862 - mse: 221583136.0000 - mae: 11386.2871 - val_loss: 10040.0059 - val_mse: 187795024.0000 - val_mae: 10040.0068\n",
            "Epoch 195/500\n",
            "80/80 [==============================] - 0s 674us/step - loss: 10216.7786 - mse: 188031760.0000 - mae: 10216.7783 - val_loss: 10044.0084 - val_mse: 187109984.0000 - val_mae: 10044.0088\n",
            "Epoch 196/500\n",
            "80/80 [==============================] - 0s 662us/step - loss: 12295.1189 - mse: 255888000.0000 - mae: 12295.1182 - val_loss: 9990.9451 - val_mse: 185948624.0000 - val_mae: 9990.9443\n",
            "Epoch 197/500\n",
            "80/80 [==============================] - 0s 749us/step - loss: 11876.7166 - mse: 239895904.0000 - mae: 11876.7168 - val_loss: 9955.8850 - val_mse: 184829840.0000 - val_mae: 9955.8857\n",
            "Epoch 198/500\n",
            "80/80 [==============================] - 0s 698us/step - loss: 11941.1141 - mse: 247776512.0000 - mae: 11941.1143 - val_loss: 9977.6219 - val_mse: 184645520.0000 - val_mae: 9977.6221\n",
            "Epoch 199/500\n",
            "80/80 [==============================] - 0s 865us/step - loss: 10518.8896 - mse: 176693408.0000 - mae: 10518.8896 - val_loss: 9930.8153 - val_mse: 184218032.0000 - val_mae: 9930.8154\n",
            "Epoch 200/500\n",
            "80/80 [==============================] - 0s 705us/step - loss: 10076.8919 - mse: 171018192.0000 - mae: 10076.8926 - val_loss: 9916.2983 - val_mse: 183717360.0000 - val_mae: 9916.2979\n",
            "Epoch 201/500\n",
            "80/80 [==============================] - 0s 645us/step - loss: 12044.1477 - mse: 241118496.0000 - mae: 12044.1475 - val_loss: 9935.8623 - val_mse: 183376640.0000 - val_mae: 9935.8623\n",
            "Epoch 202/500\n",
            "80/80 [==============================] - 0s 711us/step - loss: 9828.6277 - mse: 160491392.0000 - mae: 9828.6270 - val_loss: 9916.2389 - val_mse: 182224928.0000 - val_mae: 9916.2383\n",
            "Epoch 203/500\n",
            "80/80 [==============================] - 0s 651us/step - loss: 10737.1035 - mse: 209776480.0000 - mae: 10737.1035 - val_loss: 9864.6592 - val_mse: 180926352.0000 - val_mae: 9864.6592\n",
            "Epoch 204/500\n",
            "80/80 [==============================] - 0s 663us/step - loss: 10909.5109 - mse: 209591648.0000 - mae: 10909.5117 - val_loss: 9867.2851 - val_mse: 180811344.0000 - val_mae: 9867.2852\n",
            "Epoch 205/500\n",
            "80/80 [==============================] - 0s 659us/step - loss: 9985.3615 - mse: 181920928.0000 - mae: 9985.3613 - val_loss: 9833.8012 - val_mse: 179714464.0000 - val_mae: 9833.8018\n",
            "Epoch 206/500\n",
            "80/80 [==============================] - 0s 682us/step - loss: 11760.2894 - mse: 218443040.0000 - mae: 11760.2881 - val_loss: 9855.9655 - val_mse: 180196208.0000 - val_mae: 9855.9658\n",
            "Epoch 207/500\n",
            "80/80 [==============================] - 0s 776us/step - loss: 11529.5117 - mse: 222950048.0000 - mae: 11529.5107 - val_loss: 9883.1486 - val_mse: 180625152.0000 - val_mae: 9883.1494\n",
            "Epoch 208/500\n",
            "80/80 [==============================] - 0s 726us/step - loss: 10431.7528 - mse: 195330512.0000 - mae: 10431.7520 - val_loss: 9853.8361 - val_mse: 179967600.0000 - val_mae: 9853.8359\n",
            "Epoch 209/500\n",
            "80/80 [==============================] - 0s 719us/step - loss: 12395.9250 - mse: 245245344.0000 - mae: 12395.9248 - val_loss: 9858.0085 - val_mse: 179942976.0000 - val_mae: 9858.0078\n",
            "Epoch 210/500\n",
            "80/80 [==============================] - 0s 636us/step - loss: 11690.0930 - mse: 225610288.0000 - mae: 11690.0928 - val_loss: 9868.2161 - val_mse: 179788528.0000 - val_mae: 9868.2158\n",
            "Epoch 211/500\n",
            "80/80 [==============================] - 0s 765us/step - loss: 11290.9063 - mse: 205543280.0000 - mae: 11290.9072 - val_loss: 9866.8465 - val_mse: 179952256.0000 - val_mae: 9866.8467\n",
            "Epoch 212/500\n",
            "80/80 [==============================] - 0s 647us/step - loss: 10311.6136 - mse: 176862944.0000 - mae: 10311.6133 - val_loss: 9837.6183 - val_mse: 178779744.0000 - val_mae: 9837.6182\n",
            "Epoch 213/500\n",
            "80/80 [==============================] - 0s 765us/step - loss: 11401.4186 - mse: 210079264.0000 - mae: 11401.4189 - val_loss: 9826.9124 - val_mse: 178082992.0000 - val_mae: 9826.9121\n",
            "Epoch 214/500\n",
            "80/80 [==============================] - 0s 695us/step - loss: 11348.7080 - mse: 207311008.0000 - mae: 11348.7090 - val_loss: 9810.6914 - val_mse: 177757728.0000 - val_mae: 9810.6914\n",
            "Epoch 215/500\n",
            "80/80 [==============================] - 0s 868us/step - loss: 10146.2075 - mse: 176004832.0000 - mae: 10146.2080 - val_loss: 9836.5614 - val_mse: 178359040.0000 - val_mae: 9836.5605\n",
            "Epoch 216/500\n",
            "80/80 [==============================] - 0s 777us/step - loss: 9477.4361 - mse: 144610848.0000 - mae: 9477.4355 - val_loss: 9835.8927 - val_mse: 178146096.0000 - val_mae: 9835.8926\n",
            "Epoch 217/500\n",
            "80/80 [==============================] - 0s 958us/step - loss: 10960.3637 - mse: 182805808.0000 - mae: 10960.3643 - val_loss: 9844.0031 - val_mse: 178541680.0000 - val_mae: 9844.0039\n",
            "Epoch 218/500\n",
            "80/80 [==============================] - 0s 682us/step - loss: 10285.9714 - mse: 182038880.0000 - mae: 10285.9717 - val_loss: 9790.9471 - val_mse: 177048576.0000 - val_mae: 9790.9473\n",
            "Epoch 219/500\n",
            "80/80 [==============================] - 0s 845us/step - loss: 11499.8212 - mse: 213212640.0000 - mae: 11499.8203 - val_loss: 9773.5394 - val_mse: 176704096.0000 - val_mae: 9773.5391\n",
            "Epoch 220/500\n",
            "80/80 [==============================] - 0s 677us/step - loss: 10168.5323 - mse: 170771056.0000 - mae: 10168.5322 - val_loss: 9769.6215 - val_mse: 176430880.0000 - val_mae: 9769.6211\n",
            "Epoch 221/500\n",
            "80/80 [==============================] - 0s 823us/step - loss: 10828.2585 - mse: 202169840.0000 - mae: 10828.2588 - val_loss: 9755.2580 - val_mse: 175728528.0000 - val_mae: 9755.2578\n",
            "Epoch 222/500\n",
            "80/80 [==============================] - 0s 837us/step - loss: 10501.0523 - mse: 193322464.0000 - mae: 10501.0527 - val_loss: 9792.3004 - val_mse: 176206368.0000 - val_mae: 9792.2998\n",
            "Epoch 223/500\n",
            "80/80 [==============================] - 0s 659us/step - loss: 12054.8244 - mse: 249013680.0000 - mae: 12054.8252 - val_loss: 9838.0468 - val_mse: 177069504.0000 - val_mae: 9838.0469\n",
            "Epoch 224/500\n",
            "80/80 [==============================] - 0s 655us/step - loss: 9935.6505 - mse: 155259472.0000 - mae: 9935.6504 - val_loss: 9808.8222 - val_mse: 176206912.0000 - val_mae: 9808.8223\n",
            "Epoch 225/500\n",
            "80/80 [==============================] - 0s 716us/step - loss: 10984.5671 - mse: 192966912.0000 - mae: 10984.5674 - val_loss: 9783.0996 - val_mse: 175292208.0000 - val_mae: 9783.0996\n",
            "Epoch 226/500\n",
            "80/80 [==============================] - 0s 708us/step - loss: 11763.5531 - mse: 207237424.0000 - mae: 11763.5527 - val_loss: 9751.4296 - val_mse: 174378336.0000 - val_mae: 9751.4307\n",
            "Epoch 227/500\n",
            "80/80 [==============================] - 0s 762us/step - loss: 11514.1516 - mse: 217929520.0000 - mae: 11514.1514 - val_loss: 9750.6483 - val_mse: 173963200.0000 - val_mae: 9750.6484\n",
            "Epoch 228/500\n",
            "80/80 [==============================] - 0s 797us/step - loss: 10465.3442 - mse: 185480800.0000 - mae: 10465.3447 - val_loss: 9730.7971 - val_mse: 173455312.0000 - val_mae: 9730.7969\n",
            "Epoch 229/500\n",
            "80/80 [==============================] - 0s 625us/step - loss: 10199.1575 - mse: 184361568.0000 - mae: 10199.1582 - val_loss: 9686.3158 - val_mse: 172409152.0000 - val_mae: 9686.3164\n",
            "Epoch 230/500\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 11892.6000 - mse: 226587600.0000 - mae: 11892.6006 - val_loss: 9705.0659 - val_mse: 172451344.0000 - val_mae: 9705.0664\n",
            "Epoch 231/500\n",
            "80/80 [==============================] - 0s 804us/step - loss: 12218.0518 - mse: 238406272.0000 - mae: 12218.0518 - val_loss: 9706.0585 - val_mse: 172193872.0000 - val_mae: 9706.0586\n",
            "Epoch 232/500\n",
            "80/80 [==============================] - 0s 623us/step - loss: 10406.6588 - mse: 189418592.0000 - mae: 10406.6582 - val_loss: 9721.9828 - val_mse: 172434880.0000 - val_mae: 9721.9824\n",
            "Epoch 233/500\n",
            "80/80 [==============================] - 0s 680us/step - loss: 11929.3065 - mse: 234116432.0000 - mae: 11929.3066 - val_loss: 9661.7871 - val_mse: 171364768.0000 - val_mae: 9661.7871\n",
            "Epoch 234/500\n",
            "80/80 [==============================] - 0s 681us/step - loss: 11157.4829 - mse: 203184208.0000 - mae: 11157.4824 - val_loss: 9634.2027 - val_mse: 170672112.0000 - val_mae: 9634.2031\n",
            "Epoch 235/500\n",
            "80/80 [==============================] - 0s 756us/step - loss: 10433.4335 - mse: 184241744.0000 - mae: 10433.4336 - val_loss: 9622.4298 - val_mse: 170553520.0000 - val_mae: 9622.4307\n",
            "Epoch 236/500\n",
            "80/80 [==============================] - 0s 778us/step - loss: 11145.0440 - mse: 226267264.0000 - mae: 11145.0439 - val_loss: 9564.0120 - val_mse: 169220032.0000 - val_mae: 9564.0127\n",
            "Epoch 237/500\n",
            "80/80 [==============================] - 0s 888us/step - loss: 11106.3988 - mse: 215812736.0000 - mae: 11106.3994 - val_loss: 9567.9606 - val_mse: 169162400.0000 - val_mae: 9567.9609\n",
            "Epoch 238/500\n",
            "80/80 [==============================] - 0s 762us/step - loss: 11250.3405 - mse: 209284896.0000 - mae: 11250.3398 - val_loss: 9548.6215 - val_mse: 168123552.0000 - val_mae: 9548.6211\n",
            "Epoch 239/500\n",
            "80/80 [==============================] - 0s 688us/step - loss: 10887.9266 - mse: 202522608.0000 - mae: 10887.9268 - val_loss: 9521.0539 - val_mse: 167387728.0000 - val_mae: 9521.0537\n",
            "Epoch 240/500\n",
            "80/80 [==============================] - 0s 759us/step - loss: 11357.3188 - mse: 218152032.0000 - mae: 11357.3184 - val_loss: 9525.6855 - val_mse: 166899840.0000 - val_mae: 9525.6855\n",
            "Epoch 241/500\n",
            "80/80 [==============================] - 0s 702us/step - loss: 10135.9728 - mse: 168829952.0000 - mae: 10135.9736 - val_loss: 9524.1799 - val_mse: 166526256.0000 - val_mae: 9524.1807\n",
            "Epoch 242/500\n",
            "80/80 [==============================] - 0s 684us/step - loss: 11474.0602 - mse: 220215552.0000 - mae: 11474.0605 - val_loss: 9576.3385 - val_mse: 167332304.0000 - val_mae: 9576.3389\n",
            "Epoch 243/500\n",
            "80/80 [==============================] - 0s 757us/step - loss: 11647.9666 - mse: 227825456.0000 - mae: 11647.9668 - val_loss: 9609.6097 - val_mse: 167843952.0000 - val_mae: 9609.6094\n",
            "Epoch 244/500\n",
            "80/80 [==============================] - 0s 833us/step - loss: 9796.3010 - mse: 179657936.0000 - mae: 9796.3018 - val_loss: 9591.1976 - val_mse: 167206304.0000 - val_mae: 9591.1973\n",
            "Epoch 245/500\n",
            "80/80 [==============================] - 0s 907us/step - loss: 10624.5857 - mse: 183513184.0000 - mae: 10624.5850 - val_loss: 9611.7952 - val_mse: 167460224.0000 - val_mae: 9611.7949\n",
            "Epoch 246/500\n",
            "80/80 [==============================] - 0s 733us/step - loss: 10297.6227 - mse: 184159136.0000 - mae: 10297.6221 - val_loss: 9606.0235 - val_mse: 167006640.0000 - val_mae: 9606.0244\n",
            "Epoch 247/500\n",
            "80/80 [==============================] - 0s 687us/step - loss: 10057.2961 - mse: 188671024.0000 - mae: 10057.2969 - val_loss: 9595.8868 - val_mse: 166642576.0000 - val_mae: 9595.8867\n",
            "Epoch 248/500\n",
            "80/80 [==============================] - 0s 778us/step - loss: 10905.2546 - mse: 201489664.0000 - mae: 10905.2549 - val_loss: 9566.0124 - val_mse: 165867104.0000 - val_mae: 9566.0127\n",
            "Epoch 249/500\n",
            "80/80 [==============================] - 0s 821us/step - loss: 12290.4113 - mse: 257184736.0000 - mae: 12290.4121 - val_loss: 9578.1391 - val_mse: 165783872.0000 - val_mae: 9578.1396\n",
            "Epoch 250/500\n",
            "80/80 [==============================] - 0s 824us/step - loss: 9901.9434 - mse: 162316000.0000 - mae: 9901.9434 - val_loss: 9544.3918 - val_mse: 164881808.0000 - val_mae: 9544.3906\n",
            "Epoch 251/500\n",
            "80/80 [==============================] - 0s 732us/step - loss: 11269.0786 - mse: 202766256.0000 - mae: 11269.0801 - val_loss: 9563.2640 - val_mse: 165050160.0000 - val_mae: 9563.2646\n",
            "Epoch 252/500\n",
            "80/80 [==============================] - 0s 754us/step - loss: 10892.0122 - mse: 194542464.0000 - mae: 10892.0127 - val_loss: 9514.1202 - val_mse: 163871856.0000 - val_mae: 9514.1201\n",
            "Epoch 253/500\n",
            "80/80 [==============================] - 0s 744us/step - loss: 10711.4222 - mse: 198993984.0000 - mae: 10711.4229 - val_loss: 9533.1336 - val_mse: 163978528.0000 - val_mae: 9533.1328\n",
            "Epoch 254/500\n",
            "80/80 [==============================] - 0s 817us/step - loss: 9882.2968 - mse: 178895504.0000 - mae: 9882.2969 - val_loss: 9493.9928 - val_mse: 162773328.0000 - val_mae: 9493.9932\n",
            "Epoch 255/500\n",
            "80/80 [==============================] - 0s 703us/step - loss: 10677.8380 - mse: 202645456.0000 - mae: 10677.8389 - val_loss: 9516.2249 - val_mse: 162945824.0000 - val_mae: 9516.2236\n",
            "Epoch 256/500\n",
            "80/80 [==============================] - 0s 925us/step - loss: 10918.4932 - mse: 195772576.0000 - mae: 10918.4932 - val_loss: 9496.6994 - val_mse: 162500352.0000 - val_mae: 9496.6992\n",
            "Epoch 257/500\n",
            "80/80 [==============================] - 0s 662us/step - loss: 10191.6086 - mse: 175261456.0000 - mae: 10191.6094 - val_loss: 9499.5500 - val_mse: 162456912.0000 - val_mae: 9499.5498\n",
            "Epoch 258/500\n",
            "80/80 [==============================] - 0s 694us/step - loss: 10414.3453 - mse: 188172576.0000 - mae: 10414.3447 - val_loss: 9481.4095 - val_mse: 162048512.0000 - val_mae: 9481.4092\n",
            "Epoch 259/500\n",
            "80/80 [==============================] - 0s 859us/step - loss: 10278.9206 - mse: 164043344.0000 - mae: 10278.9199 - val_loss: 9509.8222 - val_mse: 162456992.0000 - val_mae: 9509.8223\n",
            "Epoch 260/500\n",
            "80/80 [==============================] - 0s 739us/step - loss: 9741.2183 - mse: 194461056.0000 - mae: 9741.2188 - val_loss: 9514.5878 - val_mse: 162424240.0000 - val_mae: 9514.5879\n",
            "Epoch 261/500\n",
            "80/80 [==============================] - 0s 711us/step - loss: 9663.0457 - mse: 167524192.0000 - mae: 9663.0449 - val_loss: 9503.1974 - val_mse: 162083728.0000 - val_mae: 9503.1973\n",
            "Epoch 262/500\n",
            "80/80 [==============================] - 0s 828us/step - loss: 10035.2400 - mse: 173935792.0000 - mae: 10035.2402 - val_loss: 9504.3657 - val_mse: 161899920.0000 - val_mae: 9504.3652\n",
            "Epoch 263/500\n",
            "80/80 [==============================] - 0s 740us/step - loss: 9940.5697 - mse: 162385376.0000 - mae: 9940.5684 - val_loss: 9479.8017 - val_mse: 161298496.0000 - val_mae: 9479.8018\n",
            "Epoch 264/500\n",
            "80/80 [==============================] - 0s 732us/step - loss: 10302.9577 - mse: 198105072.0000 - mae: 10302.9580 - val_loss: 9460.6686 - val_mse: 160748832.0000 - val_mae: 9460.6689\n",
            "Epoch 265/500\n",
            "80/80 [==============================] - 0s 838us/step - loss: 9731.2699 - mse: 152350624.0000 - mae: 9731.2695 - val_loss: 9428.6224 - val_mse: 159788640.0000 - val_mae: 9428.6230\n",
            "Epoch 266/500\n",
            "80/80 [==============================] - 0s 750us/step - loss: 12216.6949 - mse: 246110928.0000 - mae: 12216.6943 - val_loss: 9410.2074 - val_mse: 159258448.0000 - val_mae: 9410.2070\n",
            "Epoch 267/500\n",
            "80/80 [==============================] - 0s 662us/step - loss: 10606.2001 - mse: 197721952.0000 - mae: 10606.2002 - val_loss: 9385.7618 - val_mse: 158435488.0000 - val_mae: 9385.7627\n",
            "Epoch 268/500\n",
            "80/80 [==============================] - 0s 793us/step - loss: 10188.1136 - mse: 184027184.0000 - mae: 10188.1143 - val_loss: 9405.1901 - val_mse: 158628080.0000 - val_mae: 9405.1904\n",
            "Epoch 269/500\n",
            "80/80 [==============================] - 0s 721us/step - loss: 11735.7422 - mse: 235339552.0000 - mae: 11735.7422 - val_loss: 9412.4516 - val_mse: 158747984.0000 - val_mae: 9412.4521\n",
            "Epoch 270/500\n",
            "80/80 [==============================] - 0s 739us/step - loss: 10125.1063 - mse: 185604000.0000 - mae: 10125.1064 - val_loss: 9466.6753 - val_mse: 159662672.0000 - val_mae: 9466.6748\n",
            "Epoch 271/500\n",
            "80/80 [==============================] - 0s 698us/step - loss: 9975.9966 - mse: 180594144.0000 - mae: 9975.9961 - val_loss: 9470.3244 - val_mse: 159622304.0000 - val_mae: 9470.3242\n",
            "Epoch 272/500\n",
            "80/80 [==============================] - 0s 703us/step - loss: 11201.0556 - mse: 212053904.0000 - mae: 11201.0557 - val_loss: 9465.2297 - val_mse: 159290288.0000 - val_mae: 9465.2305\n",
            "Epoch 273/500\n",
            "80/80 [==============================] - 0s 795us/step - loss: 10467.0806 - mse: 193656736.0000 - mae: 10467.0811 - val_loss: 9479.3765 - val_mse: 159485888.0000 - val_mae: 9479.3770\n",
            "Epoch 274/500\n",
            "80/80 [==============================] - 0s 784us/step - loss: 9839.5585 - mse: 169324304.0000 - mae: 9839.5576 - val_loss: 9490.8269 - val_mse: 159497888.0000 - val_mae: 9490.8271\n",
            "Epoch 275/500\n",
            "80/80 [==============================] - 0s 866us/step - loss: 10119.6486 - mse: 178009248.0000 - mae: 10119.6484 - val_loss: 9537.3156 - val_mse: 160367120.0000 - val_mae: 9537.3154\n",
            "Epoch 276/500\n",
            "80/80 [==============================] - 0s 706us/step - loss: 10931.8281 - mse: 216211888.0000 - mae: 10931.8281 - val_loss: 9529.3521 - val_mse: 160188672.0000 - val_mae: 9529.3516\n",
            "Epoch 277/500\n",
            "80/80 [==============================] - 0s 749us/step - loss: 10038.8505 - mse: 163583056.0000 - mae: 10038.8496 - val_loss: 9535.0152 - val_mse: 160349120.0000 - val_mae: 9535.0156\n",
            "Epoch 278/500\n",
            "80/80 [==============================] - 0s 820us/step - loss: 11680.2831 - mse: 229580960.0000 - mae: 11680.2832 - val_loss: 9536.0832 - val_mse: 160211040.0000 - val_mae: 9536.0830\n",
            "Epoch 279/500\n",
            "80/80 [==============================] - 0s 923us/step - loss: 11893.3395 - mse: 242675616.0000 - mae: 11893.3398 - val_loss: 9530.4709 - val_mse: 160213168.0000 - val_mae: 9530.4717\n",
            "Epoch 280/500\n",
            "80/80 [==============================] - 0s 948us/step - loss: 12070.9966 - mse: 234579120.0000 - mae: 12070.9971 - val_loss: 9516.0488 - val_mse: 159908224.0000 - val_mae: 9516.0479\n",
            "Epoch 281/500\n",
            "80/80 [==============================] - 0s 706us/step - loss: 10277.8143 - mse: 207198832.0000 - mae: 10277.8145 - val_loss: 9502.3195 - val_mse: 159275360.0000 - val_mae: 9502.3193\n",
            "Epoch 282/500\n",
            "80/80 [==============================] - 0s 688us/step - loss: 9724.7736 - mse: 168296416.0000 - mae: 9724.7744 - val_loss: 9463.5548 - val_mse: 158465344.0000 - val_mae: 9463.5547\n",
            "Epoch 283/500\n",
            "80/80 [==============================] - 0s 894us/step - loss: 10736.6437 - mse: 195954912.0000 - mae: 10736.6436 - val_loss: 9483.8662 - val_mse: 158821904.0000 - val_mae: 9483.8662\n",
            "Epoch 284/500\n",
            "80/80 [==============================] - 0s 724us/step - loss: 10755.0182 - mse: 197313920.0000 - mae: 10755.0186 - val_loss: 9464.6413 - val_mse: 158222496.0000 - val_mae: 9464.6406\n",
            "Epoch 285/500\n",
            "80/80 [==============================] - 0s 695us/step - loss: 10725.3740 - mse: 199661920.0000 - mae: 10725.3740 - val_loss: 9451.0689 - val_mse: 157713536.0000 - val_mae: 9451.0684\n",
            "Epoch 286/500\n",
            "80/80 [==============================] - 0s 674us/step - loss: 10474.6985 - mse: 181756592.0000 - mae: 10474.6982 - val_loss: 9427.8258 - val_mse: 156993680.0000 - val_mae: 9427.8262\n",
            "Epoch 287/500\n",
            "80/80 [==============================] - 0s 737us/step - loss: 9471.2529 - mse: 160701792.0000 - mae: 9471.2529 - val_loss: 9449.5458 - val_mse: 157171504.0000 - val_mae: 9449.5469\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 90 samples, validate on 90 samples\n",
            "Epoch 1/500\n",
            "90/90 [==============================] - 2s 21ms/step - loss: 36156.3741 - mse: 1543867776.0000 - mae: 36156.3750 - val_loss: 38268.4221 - val_mse: 1666703872.0000 - val_mae: 38268.4180\n",
            "Epoch 2/500\n",
            "90/90 [==============================] - 0s 630us/step - loss: 36155.6851 - mse: 1543818112.0000 - mae: 36155.6875 - val_loss: 38266.5326 - val_mse: 1666569344.0000 - val_mae: 38266.5352\n",
            "Epoch 3/500\n",
            "90/90 [==============================] - 0s 572us/step - loss: 36149.4516 - mse: 1543345280.0000 - mae: 36149.4492 - val_loss: 38254.3349 - val_mse: 1665638912.0000 - val_mae: 38254.3320\n",
            "Epoch 4/500\n",
            "90/90 [==============================] - 0s 562us/step - loss: 36129.5028 - mse: 1541902080.0000 - mae: 36129.5039 - val_loss: 38231.0653 - val_mse: 1663857792.0000 - val_mae: 38231.0625\n",
            "Epoch 5/500\n",
            "90/90 [==============================] - 0s 736us/step - loss: 36103.1649 - mse: 1539859840.0000 - mae: 36103.1641 - val_loss: 38200.9970 - val_mse: 1661532032.0000 - val_mae: 38200.9961\n",
            "Epoch 6/500\n",
            "90/90 [==============================] - 0s 606us/step - loss: 36069.6070 - mse: 1537682048.0000 - mae: 36069.6055 - val_loss: 38164.0653 - val_mse: 1658703616.0000 - val_mae: 38164.0625\n",
            "Epoch 7/500\n",
            "90/90 [==============================] - 0s 672us/step - loss: 36027.1864 - mse: 1534386560.0000 - mae: 36027.1875 - val_loss: 38117.3611 - val_mse: 1655133184.0000 - val_mae: 38117.3633\n",
            "Epoch 8/500\n",
            "90/90 [==============================] - 0s 641us/step - loss: 35980.7559 - mse: 1531286016.0000 - mae: 35980.7539 - val_loss: 38064.5011 - val_mse: 1651110528.0000 - val_mae: 38064.5039\n",
            "Epoch 9/500\n",
            "90/90 [==============================] - 0s 588us/step - loss: 35917.5677 - mse: 1526432256.0000 - mae: 35917.5664 - val_loss: 38003.1675 - val_mse: 1646447104.0000 - val_mae: 38003.1680\n",
            "Epoch 10/500\n",
            "90/90 [==============================] - 0s 631us/step - loss: 35849.7222 - mse: 1521701504.0000 - mae: 35849.7227 - val_loss: 37936.0135 - val_mse: 1641341312.0000 - val_mae: 37936.0117\n",
            "Epoch 11/500\n",
            "90/90 [==============================] - 0s 588us/step - loss: 35803.1749 - mse: 1518073344.0000 - mae: 35803.1797 - val_loss: 37867.1704 - val_mse: 1636112128.0000 - val_mae: 37867.1680\n",
            "Epoch 12/500\n",
            "90/90 [==============================] - 0s 578us/step - loss: 35735.3770 - mse: 1513938176.0000 - mae: 35735.3789 - val_loss: 37793.6749 - val_mse: 1630550912.0000 - val_mae: 37793.6758\n",
            "Epoch 13/500\n",
            "90/90 [==============================] - 0s 600us/step - loss: 35635.1610 - mse: 1506749952.0000 - mae: 35635.1602 - val_loss: 37711.1667 - val_mse: 1624316800.0000 - val_mae: 37711.1680\n",
            "Epoch 14/500\n",
            "90/90 [==============================] - 0s 652us/step - loss: 35542.5150 - mse: 1499729024.0000 - mae: 35542.5156 - val_loss: 37623.6586 - val_mse: 1617721216.0000 - val_mae: 37623.6602\n",
            "Epoch 15/500\n",
            "90/90 [==============================] - 0s 634us/step - loss: 35473.7802 - mse: 1495222016.0000 - mae: 35473.7773 - val_loss: 37534.2010 - val_mse: 1610992256.0000 - val_mae: 37534.1992\n",
            "Epoch 16/500\n",
            "90/90 [==============================] - 0s 566us/step - loss: 35337.3201 - mse: 1486334080.0000 - mae: 35337.3164 - val_loss: 37435.2671 - val_mse: 1603571072.0000 - val_mae: 37435.2656\n",
            "Epoch 17/500\n",
            "90/90 [==============================] - 0s 605us/step - loss: 35261.8841 - mse: 1480934912.0000 - mae: 35261.8867 - val_loss: 37335.0156 - val_mse: 1596072320.0000 - val_mae: 37335.0156\n",
            "Epoch 18/500\n",
            "90/90 [==============================] - 0s 588us/step - loss: 35162.4709 - mse: 1472337280.0000 - mae: 35162.4688 - val_loss: 37230.5842 - val_mse: 1588278400.0000 - val_mae: 37230.5820\n",
            "Epoch 19/500\n",
            "90/90 [==============================] - 0s 635us/step - loss: 35112.1875 - mse: 1470257152.0000 - mae: 35112.1875 - val_loss: 37126.9371 - val_mse: 1580567808.0000 - val_mae: 37126.9336\n",
            "Epoch 20/500\n",
            "90/90 [==============================] - 0s 584us/step - loss: 34989.0122 - mse: 1461303168.0000 - mae: 34989.0156 - val_loss: 37016.9399 - val_mse: 1572409088.0000 - val_mae: 37016.9375\n",
            "Epoch 21/500\n",
            "90/90 [==============================] - 0s 646us/step - loss: 34898.2741 - mse: 1455053184.0000 - mae: 34898.2734 - val_loss: 36904.1380 - val_mse: 1564066944.0000 - val_mae: 36904.1367\n",
            "Epoch 22/500\n",
            "90/90 [==============================] - 0s 695us/step - loss: 34730.7684 - mse: 1443325056.0000 - mae: 34730.7695 - val_loss: 36782.6636 - val_mse: 1555111552.0000 - val_mae: 36782.6602\n",
            "Epoch 23/500\n",
            "90/90 [==============================] - 0s 602us/step - loss: 34587.9138 - mse: 1435189760.0000 - mae: 34587.9180 - val_loss: 36655.4275 - val_mse: 1545764864.0000 - val_mae: 36655.4297\n",
            "Epoch 24/500\n",
            "90/90 [==============================] - 0s 557us/step - loss: 34544.6688 - mse: 1430755200.0000 - mae: 34544.6680 - val_loss: 36529.0063 - val_mse: 1536509056.0000 - val_mae: 36529.0078\n",
            "Epoch 25/500\n",
            "90/90 [==============================] - 0s 537us/step - loss: 34283.2765 - mse: 1410286592.0000 - mae: 34283.2734 - val_loss: 36383.1981 - val_mse: 1525871744.0000 - val_mae: 36383.1992\n",
            "Epoch 26/500\n",
            "90/90 [==============================] - 0s 610us/step - loss: 34142.2064 - mse: 1403222016.0000 - mae: 34142.2070 - val_loss: 36230.0464 - val_mse: 1514747648.0000 - val_mae: 36230.0430\n",
            "Epoch 27/500\n",
            "90/90 [==============================] - 0s 666us/step - loss: 34049.6131 - mse: 1395702144.0000 - mae: 34049.6094 - val_loss: 36077.2626 - val_mse: 1503696000.0000 - val_mae: 36077.2656\n",
            "Epoch 28/500\n",
            "90/90 [==============================] - 0s 656us/step - loss: 33956.2899 - mse: 1390867712.0000 - mae: 33956.2930 - val_loss: 35922.6654 - val_mse: 1492562048.0000 - val_mae: 35922.6641\n",
            "Epoch 29/500\n",
            "90/90 [==============================] - 0s 675us/step - loss: 33849.4718 - mse: 1380985728.0000 - mae: 33849.4727 - val_loss: 35765.0323 - val_mse: 1481257600.0000 - val_mae: 35765.0352\n",
            "Epoch 30/500\n",
            "90/90 [==============================] - 0s 642us/step - loss: 33504.4685 - mse: 1359847296.0000 - mae: 33504.4688 - val_loss: 35588.9954 - val_mse: 1468693120.0000 - val_mae: 35588.9961\n",
            "Epoch 31/500\n",
            "90/90 [==============================] - 0s 662us/step - loss: 33329.6194 - mse: 1346787840.0000 - mae: 33329.6172 - val_loss: 35405.7246 - val_mse: 1455678848.0000 - val_mae: 35405.7266\n",
            "Epoch 32/500\n",
            "90/90 [==============================] - 0s 819us/step - loss: 33065.2665 - mse: 1328689536.0000 - mae: 33065.2656 - val_loss: 35206.4106 - val_mse: 1441600128.0000 - val_mae: 35206.4102\n",
            "Epoch 33/500\n",
            "90/90 [==============================] - 0s 622us/step - loss: 32990.0983 - mse: 1325312640.0000 - mae: 32990.1016 - val_loss: 35007.9616 - val_mse: 1427662336.0000 - val_mae: 35007.9609\n",
            "Epoch 34/500\n",
            "90/90 [==============================] - 0s 589us/step - loss: 32717.3307 - mse: 1305160320.0000 - mae: 32717.3340 - val_loss: 34800.2760 - val_mse: 1413159424.0000 - val_mae: 34800.2734\n",
            "Epoch 35/500\n",
            "90/90 [==============================] - 0s 644us/step - loss: 32580.4674 - mse: 1299932544.0000 - mae: 32580.4727 - val_loss: 34590.7852 - val_mse: 1398620416.0000 - val_mae: 34590.7852\n",
            "Epoch 36/500\n",
            "90/90 [==============================] - 0s 693us/step - loss: 32304.6102 - mse: 1279633920.0000 - mae: 32304.6113 - val_loss: 34370.9820 - val_mse: 1383459712.0000 - val_mae: 34370.9844\n",
            "Epoch 37/500\n",
            "90/90 [==============================] - 0s 669us/step - loss: 32036.2548 - mse: 1261492224.0000 - mae: 32036.2559 - val_loss: 34143.6873 - val_mse: 1367883648.0000 - val_mae: 34143.6875\n",
            "Epoch 38/500\n",
            "90/90 [==============================] - 0s 686us/step - loss: 31782.6806 - mse: 1245977344.0000 - mae: 31782.6797 - val_loss: 33908.7658 - val_mse: 1351895424.0000 - val_mae: 33908.7695\n",
            "Epoch 39/500\n",
            "90/90 [==============================] - 0s 754us/step - loss: 31604.9692 - mse: 1233107200.0000 - mae: 31604.9668 - val_loss: 33671.6007 - val_mse: 1335865472.0000 - val_mae: 33671.6016\n",
            "Epoch 40/500\n",
            "90/90 [==============================] - 0s 799us/step - loss: 31484.1391 - mse: 1227918336.0000 - mae: 31484.1387 - val_loss: 33434.7070 - val_mse: 1319966976.0000 - val_mae: 33434.7070\n",
            "Epoch 41/500\n",
            "90/90 [==============================] - 0s 701us/step - loss: 31129.1784 - mse: 1207389312.0000 - mae: 31129.1777 - val_loss: 33185.2114 - val_mse: 1303344640.0000 - val_mae: 33185.2070\n",
            "Epoch 42/500\n",
            "90/90 [==============================] - 0s 668us/step - loss: 31113.4273 - mse: 1208157696.0000 - mae: 31113.4277 - val_loss: 32940.9646 - val_mse: 1287192704.0000 - val_mae: 32940.9648\n",
            "Epoch 43/500\n",
            "90/90 [==============================] - 0s 654us/step - loss: 30701.1102 - mse: 1181849856.0000 - mae: 30701.1113 - val_loss: 32683.9878 - val_mse: 1270327424.0000 - val_mae: 32683.9863\n",
            "Epoch 44/500\n",
            "90/90 [==============================] - 0s 642us/step - loss: 30501.7880 - mse: 1160712320.0000 - mae: 30501.7891 - val_loss: 32424.3201 - val_mse: 1253419136.0000 - val_mae: 32424.3223\n",
            "Epoch 45/500\n",
            "90/90 [==============================] - 0s 597us/step - loss: 30232.6860 - mse: 1154024704.0000 - mae: 30232.6855 - val_loss: 32158.6555 - val_mse: 1236262144.0000 - val_mae: 32158.6562\n",
            "Epoch 46/500\n",
            "90/90 [==============================] - 0s 664us/step - loss: 29691.6465 - mse: 1113231488.0000 - mae: 29691.6465 - val_loss: 31875.4742 - val_mse: 1218127232.0000 - val_mae: 31875.4746\n",
            "Epoch 47/500\n",
            "90/90 [==============================] - 0s 623us/step - loss: 29459.8223 - mse: 1104806656.0000 - mae: 29459.8223 - val_loss: 31588.0523 - val_mse: 1199884800.0000 - val_mae: 31588.0527\n",
            "Epoch 48/500\n",
            "90/90 [==============================] - 0s 663us/step - loss: 29569.5812 - mse: 1120589184.0000 - mae: 29569.5801 - val_loss: 31313.2973 - val_mse: 1182601856.0000 - val_mae: 31313.2969\n",
            "Epoch 49/500\n",
            "90/90 [==============================] - 0s 621us/step - loss: 29002.2695 - mse: 1083956736.0000 - mae: 29002.2676 - val_loss: 31020.1322 - val_mse: 1164327808.0000 - val_mae: 31020.1328\n",
            "Epoch 50/500\n",
            "90/90 [==============================] - 0s 567us/step - loss: 28538.7166 - mse: 1061336960.0000 - mae: 28538.7168 - val_loss: 30714.4707 - val_mse: 1145457792.0000 - val_mae: 30714.4727\n",
            "Epoch 51/500\n",
            "90/90 [==============================] - 0s 601us/step - loss: 28809.6181 - mse: 1071016000.0000 - mae: 28809.6230 - val_loss: 30425.2684 - val_mse: 1127775872.0000 - val_mae: 30425.2676\n",
            "Epoch 52/500\n",
            "90/90 [==============================] - 0s 610us/step - loss: 28345.0113 - mse: 1047142336.0000 - mae: 28345.0117 - val_loss: 30122.2607 - val_mse: 1109429504.0000 - val_mae: 30122.2617\n",
            "Epoch 53/500\n",
            "90/90 [==============================] - 0s 613us/step - loss: 28216.6359 - mse: 1025553856.0000 - mae: 28216.6328 - val_loss: 29819.4349 - val_mse: 1091276672.0000 - val_mae: 29819.4355\n",
            "Epoch 54/500\n",
            "90/90 [==============================] - 0s 579us/step - loss: 27313.1126 - mse: 984665280.0000 - mae: 27313.1113 - val_loss: 29490.9978 - val_mse: 1071796480.0000 - val_mae: 29490.9980\n",
            "Epoch 55/500\n",
            "90/90 [==============================] - 0s 603us/step - loss: 27419.3991 - mse: 989864384.0000 - mae: 27419.4004 - val_loss: 29170.0251 - val_mse: 1052966848.0000 - val_mae: 29170.0254\n",
            "Epoch 56/500\n",
            "90/90 [==============================] - 0s 684us/step - loss: 26808.8223 - mse: 954683968.0000 - mae: 26808.8242 - val_loss: 28835.1061 - val_mse: 1033540032.0000 - val_mae: 28835.1074\n",
            "Epoch 57/500\n",
            "90/90 [==============================] - 0s 700us/step - loss: 26309.2611 - mse: 926755136.0000 - mae: 26309.2578 - val_loss: 28487.0400 - val_mse: 1013587520.0000 - val_mae: 28487.0391\n",
            "Epoch 58/500\n",
            "90/90 [==============================] - 0s 604us/step - loss: 26458.5009 - mse: 957102848.0000 - mae: 26458.5000 - val_loss: 28159.5064 - val_mse: 995033792.0000 - val_mae: 28159.5078\n",
            "Epoch 59/500\n",
            "90/90 [==============================] - 0s 604us/step - loss: 25835.5128 - mse: 921347136.0000 - mae: 25835.5137 - val_loss: 27817.7454 - val_mse: 975903296.0000 - val_mae: 27817.7441\n",
            "Epoch 60/500\n",
            "90/90 [==============================] - 0s 590us/step - loss: 25682.1721 - mse: 901066368.0000 - mae: 25682.1699 - val_loss: 27474.3078 - val_mse: 956914240.0000 - val_mae: 27474.3047\n",
            "Epoch 61/500\n",
            "90/90 [==============================] - 0s 632us/step - loss: 25149.3789 - mse: 879181248.0000 - mae: 25149.3770 - val_loss: 27111.0766 - val_mse: 937087360.0000 - val_mae: 27111.0742\n",
            "Epoch 62/500\n",
            "90/90 [==============================] - 0s 624us/step - loss: 24834.4442 - mse: 857924736.0000 - mae: 24834.4453 - val_loss: 26743.5188 - val_mse: 917292672.0000 - val_mae: 26743.5195\n",
            "Epoch 63/500\n",
            "90/90 [==============================] - 0s 597us/step - loss: 24142.4835 - mse: 827741952.0000 - mae: 24142.4824 - val_loss: 26367.8657 - val_mse: 897340928.0000 - val_mae: 26367.8633\n",
            "Epoch 64/500\n",
            "90/90 [==============================] - 0s 604us/step - loss: 24134.1191 - mse: 801663936.0000 - mae: 24134.1191 - val_loss: 26010.3306 - val_mse: 878612288.0000 - val_mae: 26010.3301\n",
            "Epoch 65/500\n",
            "90/90 [==============================] - 0s 612us/step - loss: 23805.7112 - mse: 819315712.0000 - mae: 23805.7109 - val_loss: 25646.9606 - val_mse: 859841216.0000 - val_mae: 25646.9609\n",
            "Epoch 66/500\n",
            "90/90 [==============================] - 0s 658us/step - loss: 23799.4240 - mse: 804477440.0000 - mae: 23799.4219 - val_loss: 25294.1617 - val_mse: 841868736.0000 - val_mae: 25294.1602\n",
            "Epoch 67/500\n",
            "90/90 [==============================] - 0s 630us/step - loss: 23688.3628 - mse: 802260672.0000 - mae: 23688.3633 - val_loss: 24934.6241 - val_mse: 823809984.0000 - val_mae: 24934.6250\n",
            "Epoch 68/500\n",
            "90/90 [==============================] - 0s 711us/step - loss: 22727.2533 - mse: 755459648.0000 - mae: 22727.2520 - val_loss: 24534.7843 - val_mse: 804030080.0000 - val_mae: 24534.7852\n",
            "Epoch 69/500\n",
            "90/90 [==============================] - 0s 776us/step - loss: 23291.2558 - mse: 774525632.0000 - mae: 23291.2559 - val_loss: 24171.5039 - val_mse: 786336000.0000 - val_mae: 24171.5059\n",
            "Epoch 70/500\n",
            "90/90 [==============================] - 0s 719us/step - loss: 21940.4546 - mse: 719262528.0000 - mae: 21940.4551 - val_loss: 23777.1834 - val_mse: 767428288.0000 - val_mae: 23777.1855\n",
            "Epoch 71/500\n",
            "90/90 [==============================] - 0s 702us/step - loss: 22177.5204 - mse: 725308864.0000 - mae: 22177.5215 - val_loss: 23396.4189 - val_mse: 749465984.0000 - val_mae: 23396.4199\n",
            "Epoch 72/500\n",
            "90/90 [==============================] - 0s 726us/step - loss: 20923.1878 - mse: 702518976.0000 - mae: 20923.1855 - val_loss: 22995.8601 - val_mse: 730883392.0000 - val_mae: 22995.8594\n",
            "Epoch 73/500\n",
            "90/90 [==============================] - 0s 767us/step - loss: 20768.2143 - mse: 687286464.0000 - mae: 20768.2148 - val_loss: 22586.1503 - val_mse: 712207808.0000 - val_mae: 22586.1523\n",
            "Epoch 74/500\n",
            "90/90 [==============================] - 0s 605us/step - loss: 21227.0536 - mse: 688408064.0000 - mae: 21227.0527 - val_loss: 22208.2546 - val_mse: 695279808.0000 - val_mae: 22208.2559\n",
            "Epoch 75/500\n",
            "90/90 [==============================] - 0s 746us/step - loss: 20298.8546 - mse: 640259776.0000 - mae: 20298.8555 - val_loss: 21816.8944 - val_mse: 678049536.0000 - val_mae: 21816.8945\n",
            "Epoch 76/500\n",
            "90/90 [==============================] - 0s 644us/step - loss: 19614.5787 - mse: 611846272.0000 - mae: 19614.5781 - val_loss: 21478.1632 - val_mse: 663381632.0000 - val_mae: 21478.1621\n",
            "Epoch 77/500\n",
            "90/90 [==============================] - 0s 651us/step - loss: 19860.0345 - mse: 641176960.0000 - mae: 19860.0352 - val_loss: 21086.2199 - val_mse: 646698752.0000 - val_mae: 21086.2188\n",
            "Epoch 78/500\n",
            "90/90 [==============================] - 0s 700us/step - loss: 19334.8689 - mse: 603362944.0000 - mae: 19334.8672 - val_loss: 20711.1131 - val_mse: 631019456.0000 - val_mae: 20711.1133\n",
            "Epoch 79/500\n",
            "90/90 [==============================] - 0s 667us/step - loss: 18790.5781 - mse: 595072000.0000 - mae: 18790.5781 - val_loss: 20320.6865 - val_mse: 614999040.0000 - val_mae: 20320.6855\n",
            "Epoch 80/500\n",
            "90/90 [==============================] - 0s 620us/step - loss: 19255.4387 - mse: 607264000.0000 - mae: 19255.4375 - val_loss: 19997.1514 - val_mse: 601950336.0000 - val_mae: 19997.1523\n",
            "Epoch 81/500\n",
            "90/90 [==============================] - 0s 628us/step - loss: 18071.8153 - mse: 569890048.0000 - mae: 18071.8145 - val_loss: 19597.0834 - val_mse: 586109312.0000 - val_mae: 19597.0840\n",
            "Epoch 82/500\n",
            "90/90 [==============================] - 0s 614us/step - loss: 17846.7873 - mse: 560678720.0000 - mae: 17846.7891 - val_loss: 19246.0459 - val_mse: 572471808.0000 - val_mae: 19246.0449\n",
            "Epoch 83/500\n",
            "90/90 [==============================] - 0s 682us/step - loss: 17490.7776 - mse: 523528064.0000 - mae: 17490.7773 - val_loss: 18855.2780 - val_mse: 557580096.0000 - val_mae: 18855.2773\n",
            "Epoch 84/500\n",
            "90/90 [==============================] - 0s 654us/step - loss: 17679.9582 - mse: 507655904.0000 - mae: 17679.9570 - val_loss: 18479.3331 - val_mse: 543540288.0000 - val_mae: 18479.3340\n",
            "Epoch 85/500\n",
            "90/90 [==============================] - 0s 597us/step - loss: 16779.0675 - mse: 490889024.0000 - mae: 16779.0684 - val_loss: 18153.6672 - val_mse: 531605728.0000 - val_mae: 18153.6660\n",
            "Epoch 86/500\n",
            "90/90 [==============================] - 0s 647us/step - loss: 17182.8446 - mse: 522995392.0000 - mae: 17182.8438 - val_loss: 17805.2458 - val_mse: 519072704.0000 - val_mae: 17805.2480\n",
            "Epoch 87/500\n",
            "90/90 [==============================] - 0s 837us/step - loss: 16685.0336 - mse: 505544800.0000 - mae: 16685.0352 - val_loss: 17501.5698 - val_mse: 508347552.0000 - val_mae: 17501.5703\n",
            "Epoch 88/500\n",
            "90/90 [==============================] - 0s 872us/step - loss: 16924.2628 - mse: 502014624.0000 - mae: 16924.2617 - val_loss: 17180.8751 - val_mse: 497055776.0000 - val_mae: 17180.8750\n",
            "Epoch 89/500\n",
            "90/90 [==============================] - 0s 723us/step - loss: 16375.4282 - mse: 453313408.0000 - mae: 16375.4287 - val_loss: 16897.7391 - val_mse: 486773056.0000 - val_mae: 16897.7402\n",
            "Epoch 90/500\n",
            "90/90 [==============================] - 0s 803us/step - loss: 17150.2605 - mse: 482334688.0000 - mae: 17150.2617 - val_loss: 16659.1138 - val_mse: 477788288.0000 - val_mae: 16659.1133\n",
            "Epoch 91/500\n",
            "90/90 [==============================] - 0s 641us/step - loss: 15882.8037 - mse: 461067648.0000 - mae: 15882.8027 - val_loss: 16360.9315 - val_mse: 466095808.0000 - val_mae: 16360.9316\n",
            "Epoch 92/500\n",
            "90/90 [==============================] - 0s 731us/step - loss: 15855.2489 - mse: 432297856.0000 - mae: 15855.2490 - val_loss: 16125.1035 - val_mse: 455952672.0000 - val_mae: 16125.1045\n",
            "Epoch 93/500\n",
            "90/90 [==============================] - 0s 777us/step - loss: 16354.6968 - mse: 483920448.0000 - mae: 16354.6963 - val_loss: 15973.6536 - val_mse: 449092448.0000 - val_mae: 15973.6543\n",
            "Epoch 94/500\n",
            "90/90 [==============================] - 0s 724us/step - loss: 15857.6303 - mse: 448372224.0000 - mae: 15857.6309 - val_loss: 15824.7532 - val_mse: 441886592.0000 - val_mae: 15824.7529\n",
            "Epoch 95/500\n",
            "90/90 [==============================] - 0s 712us/step - loss: 16098.1039 - mse: 465851648.0000 - mae: 16098.1025 - val_loss: 15707.1780 - val_mse: 436240928.0000 - val_mae: 15707.1777\n",
            "Epoch 96/500\n",
            "90/90 [==============================] - 0s 626us/step - loss: 15734.0592 - mse: 444840800.0000 - mae: 15734.0596 - val_loss: 15548.0881 - val_mse: 428048896.0000 - val_mae: 15548.0879\n",
            "Epoch 97/500\n",
            "90/90 [==============================] - 0s 678us/step - loss: 14897.4948 - mse: 410802944.0000 - mae: 14897.4941 - val_loss: 15477.1825 - val_mse: 424237888.0000 - val_mae: 15477.1816\n",
            "Epoch 98/500\n",
            "90/90 [==============================] - 0s 614us/step - loss: 15185.3577 - mse: 399080960.0000 - mae: 15185.3584 - val_loss: 15339.1053 - val_mse: 416895808.0000 - val_mae: 15339.1055\n",
            "Epoch 99/500\n",
            "90/90 [==============================] - 0s 639us/step - loss: 15388.5258 - mse: 432836064.0000 - mae: 15388.5254 - val_loss: 15220.3568 - val_mse: 410682400.0000 - val_mae: 15220.3584\n",
            "Epoch 100/500\n",
            "90/90 [==============================] - 0s 581us/step - loss: 14586.0190 - mse: 391607264.0000 - mae: 14586.0195 - val_loss: 15061.9288 - val_mse: 402526560.0000 - val_mae: 15061.9277\n",
            "Epoch 101/500\n",
            "90/90 [==============================] - 0s 604us/step - loss: 15713.9201 - mse: 443729888.0000 - mae: 15713.9209 - val_loss: 14947.6690 - val_mse: 396753472.0000 - val_mae: 14947.6680\n",
            "Epoch 102/500\n",
            "90/90 [==============================] - 0s 587us/step - loss: 15000.1916 - mse: 403279552.0000 - mae: 15000.1904 - val_loss: 14831.1497 - val_mse: 390944256.0000 - val_mae: 14831.1504\n",
            "Epoch 103/500\n",
            "90/90 [==============================] - 0s 612us/step - loss: 15323.1211 - mse: 406192448.0000 - mae: 15323.1211 - val_loss: 14733.2876 - val_mse: 386143392.0000 - val_mae: 14733.2871\n",
            "Epoch 104/500\n",
            "90/90 [==============================] - 0s 663us/step - loss: 14508.6459 - mse: 388045568.0000 - mae: 14508.6445 - val_loss: 14583.6839 - val_mse: 378910016.0000 - val_mae: 14583.6836\n",
            "Epoch 105/500\n",
            "90/90 [==============================] - 0s 554us/step - loss: 14463.6293 - mse: 366608640.0000 - mae: 14463.6309 - val_loss: 14465.9736 - val_mse: 373323136.0000 - val_mae: 14465.9727\n",
            "Epoch 106/500\n",
            "90/90 [==============================] - 0s 645us/step - loss: 14159.5535 - mse: 351419584.0000 - mae: 14159.5537 - val_loss: 14358.6672 - val_mse: 368314400.0000 - val_mae: 14358.6680\n",
            "Epoch 107/500\n",
            "90/90 [==============================] - 0s 636us/step - loss: 15668.7130 - mse: 407937120.0000 - mae: 15668.7139 - val_loss: 14282.8064 - val_mse: 364819872.0000 - val_mae: 14282.8057\n",
            "Epoch 108/500\n",
            "90/90 [==============================] - 0s 731us/step - loss: 15325.5430 - mse: 403039264.0000 - mae: 15325.5430 - val_loss: 14202.9355 - val_mse: 361176224.0000 - val_mae: 14202.9365\n",
            "Epoch 109/500\n",
            "90/90 [==============================] - 0s 653us/step - loss: 14632.3977 - mse: 369940352.0000 - mae: 14632.3984 - val_loss: 14081.0382 - val_mse: 355689696.0000 - val_mae: 14081.0371\n",
            "Epoch 110/500\n",
            "90/90 [==============================] - 0s 635us/step - loss: 15238.9215 - mse: 391774752.0000 - mae: 15238.9219 - val_loss: 14022.6711 - val_mse: 353106464.0000 - val_mae: 14022.6709\n",
            "Epoch 111/500\n",
            "90/90 [==============================] - 0s 693us/step - loss: 14350.7747 - mse: 360722048.0000 - mae: 14350.7734 - val_loss: 13959.8013 - val_mse: 350347200.0000 - val_mae: 13959.8018\n",
            "Epoch 112/500\n",
            "90/90 [==============================] - 0s 715us/step - loss: 14437.6468 - mse: 385782624.0000 - mae: 14437.6475 - val_loss: 13871.5696 - val_mse: 346507232.0000 - val_mae: 13871.5693\n",
            "Epoch 113/500\n",
            "90/90 [==============================] - 0s 707us/step - loss: 13813.1636 - mse: 344962240.0000 - mae: 13813.1641 - val_loss: 13767.9451 - val_mse: 341897792.0000 - val_mae: 13767.9443\n",
            "Epoch 114/500\n",
            "90/90 [==============================] - 0s 709us/step - loss: 14738.3132 - mse: 391431456.0000 - mae: 14738.3154 - val_loss: 13682.3708 - val_mse: 338073472.0000 - val_mae: 13682.3711\n",
            "Epoch 115/500\n",
            "90/90 [==============================] - 0s 646us/step - loss: 14134.9168 - mse: 347577792.0000 - mae: 14134.9170 - val_loss: 13608.7475 - val_mse: 334744608.0000 - val_mae: 13608.7490\n",
            "Epoch 116/500\n",
            "90/90 [==============================] - 0s 703us/step - loss: 13963.1470 - mse: 341337856.0000 - mae: 13963.1475 - val_loss: 13537.8831 - val_mse: 331581792.0000 - val_mae: 13537.8818\n",
            "Epoch 117/500\n",
            "90/90 [==============================] - 0s 704us/step - loss: 14350.3596 - mse: 384943232.0000 - mae: 14350.3594 - val_loss: 13485.8338 - val_mse: 329286432.0000 - val_mae: 13485.8350\n",
            "Epoch 118/500\n",
            "90/90 [==============================] - 0s 708us/step - loss: 13912.8701 - mse: 334813696.0000 - mae: 13912.8691 - val_loss: 13451.0403 - val_mse: 327771680.0000 - val_mae: 13451.0400\n",
            "Epoch 119/500\n",
            "90/90 [==============================] - 0s 789us/step - loss: 14914.4058 - mse: 375461216.0000 - mae: 14914.4053 - val_loss: 13413.2364 - val_mse: 326118688.0000 - val_mae: 13413.2363\n",
            "Epoch 120/500\n",
            "90/90 [==============================] - 0s 768us/step - loss: 14239.0077 - mse: 353591424.0000 - mae: 14239.0068 - val_loss: 13323.6981 - val_mse: 322255456.0000 - val_mae: 13323.6973\n",
            "Epoch 121/500\n",
            "90/90 [==============================] - 0s 675us/step - loss: 13896.8503 - mse: 318592448.0000 - mae: 13896.8486 - val_loss: 13259.4004 - val_mse: 319511808.0000 - val_mae: 13259.4004\n",
            "Epoch 122/500\n",
            "90/90 [==============================] - 0s 659us/step - loss: 14435.5195 - mse: 363505568.0000 - mae: 14435.5195 - val_loss: 13186.1663 - val_mse: 316303200.0000 - val_mae: 13186.1680\n",
            "Epoch 123/500\n",
            "90/90 [==============================] - 0s 682us/step - loss: 14777.5881 - mse: 361867168.0000 - mae: 14777.5879 - val_loss: 13147.6197 - val_mse: 314643584.0000 - val_mae: 13147.6191\n",
            "Epoch 124/500\n",
            "90/90 [==============================] - 0s 675us/step - loss: 13949.8612 - mse: 325413760.0000 - mae: 13949.8594 - val_loss: 13111.2429 - val_mse: 313080768.0000 - val_mae: 13111.2432\n",
            "Epoch 125/500\n",
            "90/90 [==============================] - 0s 709us/step - loss: 14294.7849 - mse: 337707136.0000 - mae: 14294.7852 - val_loss: 13085.7328 - val_mse: 311981984.0000 - val_mae: 13085.7334\n",
            "Epoch 126/500\n",
            "90/90 [==============================] - 0s 686us/step - loss: 13577.3242 - mse: 320132288.0000 - mae: 13577.3252 - val_loss: 13046.0661 - val_mse: 310205568.0000 - val_mae: 13046.0664\n",
            "Epoch 127/500\n",
            "90/90 [==============================] - 0s 645us/step - loss: 15101.2938 - mse: 373658112.0000 - mae: 15101.2949 - val_loss: 13046.3602 - val_mse: 310254432.0000 - val_mae: 13046.3594\n",
            "Epoch 128/500\n",
            "90/90 [==============================] - 0s 656us/step - loss: 14360.5479 - mse: 348895488.0000 - mae: 14360.5488 - val_loss: 13012.3292 - val_mse: 308694944.0000 - val_mae: 13012.3291\n",
            "Epoch 129/500\n",
            "90/90 [==============================] - 0s 626us/step - loss: 13838.5435 - mse: 340945920.0000 - mae: 13838.5449 - val_loss: 12955.7256 - val_mse: 306073632.0000 - val_mae: 12955.7266\n",
            "Epoch 130/500\n",
            "90/90 [==============================] - 0s 724us/step - loss: 13951.3506 - mse: 320919072.0000 - mae: 13951.3516 - val_loss: 12937.7338 - val_mse: 305250176.0000 - val_mae: 12937.7344\n",
            "Epoch 131/500\n",
            "90/90 [==============================] - 0s 657us/step - loss: 14537.2314 - mse: 354187328.0000 - mae: 14537.2305 - val_loss: 12909.8129 - val_mse: 303963168.0000 - val_mae: 12909.8135\n",
            "Epoch 132/500\n",
            "90/90 [==============================] - 0s 646us/step - loss: 13848.1212 - mse: 348004000.0000 - mae: 13848.1211 - val_loss: 12884.2062 - val_mse: 302819040.0000 - val_mae: 12884.2070\n",
            "Epoch 133/500\n",
            "90/90 [==============================] - 0s 602us/step - loss: 14367.9990 - mse: 341325280.0000 - mae: 14367.9990 - val_loss: 12853.8396 - val_mse: 301336832.0000 - val_mae: 12853.8389\n",
            "Epoch 134/500\n",
            "90/90 [==============================] - 0s 618us/step - loss: 14009.3353 - mse: 334234496.0000 - mae: 14009.3359 - val_loss: 12834.0481 - val_mse: 300449216.0000 - val_mae: 12834.0469\n",
            "Epoch 135/500\n",
            "90/90 [==============================] - 0s 654us/step - loss: 13428.7760 - mse: 299676224.0000 - mae: 13428.7764 - val_loss: 12796.9234 - val_mse: 298713280.0000 - val_mae: 12796.9238\n",
            "Epoch 136/500\n",
            "90/90 [==============================] - 0s 936us/step - loss: 14414.2296 - mse: 333949088.0000 - mae: 14414.2295 - val_loss: 12773.4601 - val_mse: 297703776.0000 - val_mae: 12773.4600\n",
            "Epoch 137/500\n",
            "90/90 [==============================] - 0s 642us/step - loss: 13564.2363 - mse: 312236192.0000 - mae: 13564.2363 - val_loss: 12749.9047 - val_mse: 296705248.0000 - val_mae: 12749.9043\n",
            "Epoch 138/500\n",
            "90/90 [==============================] - 0s 681us/step - loss: 13705.2238 - mse: 300000448.0000 - mae: 13705.2236 - val_loss: 12700.5580 - val_mse: 294399680.0000 - val_mae: 12700.5586\n",
            "Epoch 139/500\n",
            "90/90 [==============================] - 0s 779us/step - loss: 15224.3881 - mse: 388988704.0000 - mae: 15224.3887 - val_loss: 12674.8118 - val_mse: 293448576.0000 - val_mae: 12674.8115\n",
            "Epoch 140/500\n",
            "90/90 [==============================] - 0s 755us/step - loss: 13941.6252 - mse: 340272256.0000 - mae: 13941.6250 - val_loss: 12625.0225 - val_mse: 291376864.0000 - val_mae: 12625.0225\n",
            "Epoch 141/500\n",
            "90/90 [==============================] - 0s 705us/step - loss: 14736.7347 - mse: 363885952.0000 - mae: 14736.7363 - val_loss: 12574.2192 - val_mse: 289393856.0000 - val_mae: 12574.2197\n",
            "Epoch 142/500\n",
            "90/90 [==============================] - 0s 720us/step - loss: 14659.1711 - mse: 364278528.0000 - mae: 14659.1719 - val_loss: 12534.5103 - val_mse: 287187552.0000 - val_mae: 12534.5098\n",
            "Epoch 143/500\n",
            "90/90 [==============================] - 0s 621us/step - loss: 13896.3313 - mse: 345871936.0000 - mae: 13896.3301 - val_loss: 12474.8556 - val_mse: 285039776.0000 - val_mae: 12474.8555\n",
            "Epoch 144/500\n",
            "90/90 [==============================] - 0s 591us/step - loss: 14107.8415 - mse: 335438688.0000 - mae: 14107.8398 - val_loss: 12735.6570 - val_mse: 284661312.0000 - val_mae: 12735.6572\n",
            "Epoch 145/500\n",
            "90/90 [==============================] - 0s 645us/step - loss: 14484.1321 - mse: 353480960.0000 - mae: 14484.1338 - val_loss: 12908.7167 - val_mse: 284472448.0000 - val_mae: 12908.7168\n",
            "Epoch 146/500\n",
            "90/90 [==============================] - 0s 641us/step - loss: 14103.3947 - mse: 324938208.0000 - mae: 14103.3945 - val_loss: 12551.5427 - val_mse: 278813728.0000 - val_mae: 12551.5430\n",
            "Epoch 147/500\n",
            "90/90 [==============================] - 0s 610us/step - loss: 13609.6118 - mse: 304199104.0000 - mae: 13609.6113 - val_loss: 12422.1945 - val_mse: 275899520.0000 - val_mae: 12422.1943\n",
            "Epoch 148/500\n",
            "90/90 [==============================] - 0s 595us/step - loss: 13172.9205 - mse: 296136192.0000 - mae: 13172.9209 - val_loss: 12419.6033 - val_mse: 274059360.0000 - val_mae: 12419.6045\n",
            "Epoch 149/500\n",
            "90/90 [==============================] - 0s 646us/step - loss: 13202.4173 - mse: 281791040.0000 - mae: 13202.4180 - val_loss: 12726.7523 - val_mse: 276609344.0000 - val_mae: 12726.7510\n",
            "Epoch 150/500\n",
            "90/90 [==============================] - 0s 709us/step - loss: 13101.0798 - mse: 281516224.0000 - mae: 13101.0801 - val_loss: 12671.1546 - val_mse: 275861632.0000 - val_mae: 12671.1543\n",
            "Epoch 151/500\n",
            "90/90 [==============================] - 0s 738us/step - loss: 13458.6325 - mse: 310136256.0000 - mae: 13458.6318 - val_loss: 12826.1530 - val_mse: 280915616.0000 - val_mae: 12826.1523\n",
            "Epoch 152/500\n",
            "90/90 [==============================] - 0s 807us/step - loss: 13577.3829 - mse: 317318336.0000 - mae: 13577.3838 - val_loss: 12439.6783 - val_mse: 271634080.0000 - val_mae: 12439.6777\n",
            "Epoch 153/500\n",
            "90/90 [==============================] - 0s 666us/step - loss: 13423.6343 - mse: 321473024.0000 - mae: 13423.6348 - val_loss: 12541.7459 - val_mse: 275804704.0000 - val_mae: 12541.7461\n",
            "Epoch 154/500\n",
            "90/90 [==============================] - 0s 706us/step - loss: 12866.5820 - mse: 289414720.0000 - mae: 12866.5820 - val_loss: 12155.0802 - val_mse: 266504400.0000 - val_mae: 12155.0801\n",
            "Epoch 155/500\n",
            "90/90 [==============================] - 0s 690us/step - loss: 12319.6300 - mse: 272062080.0000 - mae: 12319.6309 - val_loss: 11975.8891 - val_mse: 261742864.0000 - val_mae: 11975.8887\n",
            "Epoch 156/500\n",
            "90/90 [==============================] - 0s 657us/step - loss: 12573.3844 - mse: 273187200.0000 - mae: 12573.3848 - val_loss: 11787.9092 - val_mse: 254806992.0000 - val_mae: 11787.9102\n",
            "Epoch 157/500\n",
            "90/90 [==============================] - 0s 621us/step - loss: 11900.6711 - mse: 268874656.0000 - mae: 11900.6709 - val_loss: 11798.0598 - val_mse: 251368720.0000 - val_mae: 11798.0596\n",
            "Epoch 158/500\n",
            "90/90 [==============================] - 0s 591us/step - loss: 12914.2494 - mse: 298243456.0000 - mae: 12914.2490 - val_loss: 11315.1815 - val_mse: 241758528.0000 - val_mae: 11315.1816\n",
            "Epoch 159/500\n",
            "90/90 [==============================] - 0s 596us/step - loss: 12123.1483 - mse: 263407024.0000 - mae: 12123.1475 - val_loss: 11058.4992 - val_mse: 236089120.0000 - val_mae: 11058.5000\n",
            "Epoch 160/500\n",
            "90/90 [==============================] - 0s 589us/step - loss: 11875.5648 - mse: 251003648.0000 - mae: 11875.5654 - val_loss: 10962.1663 - val_mse: 233370992.0000 - val_mae: 10962.1670\n",
            "Epoch 161/500\n",
            "90/90 [==============================] - 0s 758us/step - loss: 13349.8467 - mse: 315132640.0000 - mae: 13349.8477 - val_loss: 11402.4688 - val_mse: 233657504.0000 - val_mae: 11402.4678\n",
            "Epoch 162/500\n",
            "90/90 [==============================] - 0s 683us/step - loss: 12515.1332 - mse: 267800400.0000 - mae: 12515.1338 - val_loss: 11105.9148 - val_mse: 227706720.0000 - val_mae: 11105.9150\n",
            "Epoch 163/500\n",
            "90/90 [==============================] - 0s 721us/step - loss: 11527.8013 - mse: 231553936.0000 - mae: 11527.8018 - val_loss: 10520.3027 - val_mse: 219993024.0000 - val_mae: 10520.3027\n",
            "Epoch 164/500\n",
            "90/90 [==============================] - 0s 712us/step - loss: 11724.0360 - mse: 248744944.0000 - mae: 11724.0371 - val_loss: 10343.9397 - val_mse: 215202704.0000 - val_mae: 10343.9404\n",
            "Epoch 165/500\n",
            "90/90 [==============================] - 0s 659us/step - loss: 12373.5580 - mse: 259258576.0000 - mae: 12373.5586 - val_loss: 10248.7878 - val_mse: 212783968.0000 - val_mae: 10248.7881\n",
            "Epoch 166/500\n",
            "90/90 [==============================] - 0s 645us/step - loss: 11441.0994 - mse: 242666768.0000 - mae: 11441.0996 - val_loss: 10078.3516 - val_mse: 208485424.0000 - val_mae: 10078.3516\n",
            "Epoch 167/500\n",
            "90/90 [==============================] - 0s 603us/step - loss: 12497.0245 - mse: 289388896.0000 - mae: 12497.0254 - val_loss: 10144.7066 - val_mse: 208487168.0000 - val_mae: 10144.7061\n",
            "Epoch 168/500\n",
            "90/90 [==============================] - 0s 679us/step - loss: 11281.2900 - mse: 237050304.0000 - mae: 11281.2900 - val_loss: 10191.7710 - val_mse: 207866176.0000 - val_mae: 10191.7705\n",
            "Epoch 169/500\n",
            "90/90 [==============================] - 0s 675us/step - loss: 12634.4843 - mse: 276928192.0000 - mae: 12634.4844 - val_loss: 10007.2096 - val_mse: 204121104.0000 - val_mae: 10007.2100\n",
            "Epoch 170/500\n",
            "90/90 [==============================] - 0s 645us/step - loss: 11726.8025 - mse: 245549648.0000 - mae: 11726.8027 - val_loss: 9902.9132 - val_mse: 200523552.0000 - val_mae: 9902.9131\n",
            "Epoch 171/500\n",
            "90/90 [==============================] - 0s 664us/step - loss: 11475.0901 - mse: 242479056.0000 - mae: 11475.0898 - val_loss: 9724.7830 - val_mse: 195932144.0000 - val_mae: 9724.7822\n",
            "Epoch 172/500\n",
            "90/90 [==============================] - 0s 712us/step - loss: 12493.8285 - mse: 283428544.0000 - mae: 12493.8291 - val_loss: 9673.4199 - val_mse: 193639520.0000 - val_mae: 9673.4199\n",
            "Epoch 173/500\n",
            "90/90 [==============================] - 0s 657us/step - loss: 11607.2097 - mse: 245281088.0000 - mae: 11607.2090 - val_loss: 9534.9450 - val_mse: 190130176.0000 - val_mae: 9534.9453\n",
            "Epoch 174/500\n",
            "90/90 [==============================] - 0s 728us/step - loss: 11519.6802 - mse: 218377872.0000 - mae: 11519.6807 - val_loss: 9550.9276 - val_mse: 189195904.0000 - val_mae: 9550.9268\n",
            "Epoch 175/500\n",
            "90/90 [==============================] - 0s 695us/step - loss: 11092.6793 - mse: 214120864.0000 - mae: 11092.6797 - val_loss: 9324.5896 - val_mse: 184168528.0000 - val_mae: 9324.5898\n",
            "Epoch 176/500\n",
            "90/90 [==============================] - 0s 635us/step - loss: 12010.0307 - mse: 265448352.0000 - mae: 12010.0303 - val_loss: 9209.4164 - val_mse: 181005568.0000 - val_mae: 9209.4160\n",
            "Epoch 177/500\n",
            "90/90 [==============================] - 0s 679us/step - loss: 11072.2989 - mse: 210183904.0000 - mae: 11072.2979 - val_loss: 9529.2433 - val_mse: 183446096.0000 - val_mae: 9529.2441\n",
            "Epoch 178/500\n",
            "90/90 [==============================] - 0s 884us/step - loss: 12143.9915 - mse: 256077216.0000 - mae: 12143.9902 - val_loss: 9293.3041 - val_mse: 179243824.0000 - val_mae: 9293.3037\n",
            "Epoch 179/500\n",
            "90/90 [==============================] - 0s 627us/step - loss: 10990.9206 - mse: 233603136.0000 - mae: 10990.9199 - val_loss: 9214.5101 - val_mse: 177594816.0000 - val_mae: 9214.5098\n",
            "Epoch 180/500\n",
            "90/90 [==============================] - 0s 633us/step - loss: 11428.9810 - mse: 218603776.0000 - mae: 11428.9805 - val_loss: 9043.0192 - val_mse: 173650288.0000 - val_mae: 9043.0195\n",
            "Epoch 181/500\n",
            "90/90 [==============================] - 0s 735us/step - loss: 11616.5163 - mse: 223483488.0000 - mae: 11616.5166 - val_loss: 9270.3429 - val_mse: 175993360.0000 - val_mae: 9270.3428\n",
            "Epoch 182/500\n",
            "90/90 [==============================] - 0s 717us/step - loss: 11242.1806 - mse: 226106576.0000 - mae: 11242.1816 - val_loss: 9208.8810 - val_mse: 172949056.0000 - val_mae: 9208.8809\n",
            "Epoch 183/500\n",
            "90/90 [==============================] - 0s 839us/step - loss: 10708.4904 - mse: 203427568.0000 - mae: 10708.4902 - val_loss: 9767.2001 - val_mse: 179736480.0000 - val_mae: 9767.2002\n",
            "Epoch 184/500\n",
            "90/90 [==============================] - 0s 702us/step - loss: 11328.0729 - mse: 220741888.0000 - mae: 11328.0732 - val_loss: 9511.3162 - val_mse: 175565008.0000 - val_mae: 9511.3154\n",
            "Epoch 185/500\n",
            "90/90 [==============================] - 0s 688us/step - loss: 11957.4397 - mse: 243473664.0000 - mae: 11957.4385 - val_loss: 9549.1295 - val_mse: 176076064.0000 - val_mae: 9549.1299\n",
            "Epoch 186/500\n",
            "90/90 [==============================] - 0s 680us/step - loss: 9582.7435 - mse: 165496240.0000 - mae: 9582.7432 - val_loss: 9276.8107 - val_mse: 170125216.0000 - val_mae: 9276.8105\n",
            "Epoch 187/500\n",
            "90/90 [==============================] - 0s 776us/step - loss: 11153.0455 - mse: 228202992.0000 - mae: 11153.0469 - val_loss: 9007.0056 - val_mse: 165251936.0000 - val_mae: 9007.0049\n",
            "Epoch 188/500\n",
            "90/90 [==============================] - 0s 660us/step - loss: 12443.2167 - mse: 271128000.0000 - mae: 12443.2168 - val_loss: 8850.4652 - val_mse: 160888112.0000 - val_mae: 8850.4648\n",
            "Epoch 189/500\n",
            "90/90 [==============================] - 0s 666us/step - loss: 11783.7841 - mse: 240740208.0000 - mae: 11783.7832 - val_loss: 8637.4933 - val_mse: 157428240.0000 - val_mae: 8637.4932\n",
            "Epoch 190/500\n",
            "90/90 [==============================] - 0s 794us/step - loss: 10623.7057 - mse: 205147664.0000 - mae: 10623.7061 - val_loss: 8497.3175 - val_mse: 154543984.0000 - val_mae: 8497.3184\n",
            "Epoch 191/500\n",
            "90/90 [==============================] - 0s 761us/step - loss: 11132.1752 - mse: 207294512.0000 - mae: 11132.1758 - val_loss: 8527.3298 - val_mse: 154423488.0000 - val_mae: 8527.3301\n",
            "Epoch 192/500\n",
            "90/90 [==============================] - 0s 755us/step - loss: 10735.4473 - mse: 203395888.0000 - mae: 10735.4473 - val_loss: 8622.0717 - val_mse: 153691904.0000 - val_mae: 8622.0723\n",
            "Epoch 193/500\n",
            "90/90 [==============================] - 0s 696us/step - loss: 10430.2955 - mse: 200099584.0000 - mae: 10430.2959 - val_loss: 8635.1368 - val_mse: 153170688.0000 - val_mae: 8635.1367\n",
            "Epoch 194/500\n",
            "90/90 [==============================] - 0s 685us/step - loss: 11159.8352 - mse: 234491744.0000 - mae: 11159.8350 - val_loss: 8596.9927 - val_mse: 151524944.0000 - val_mae: 8596.9932\n",
            "Epoch 195/500\n",
            "90/90 [==============================] - 0s 741us/step - loss: 12098.0286 - mse: 233172624.0000 - mae: 12098.0273 - val_loss: 8874.3218 - val_mse: 154397232.0000 - val_mae: 8874.3213\n",
            "Epoch 196/500\n",
            "90/90 [==============================] - 0s 745us/step - loss: 10193.1388 - mse: 194610272.0000 - mae: 10193.1387 - val_loss: 8619.5524 - val_mse: 150376192.0000 - val_mae: 8619.5527\n",
            "Epoch 197/500\n",
            "90/90 [==============================] - 0s 691us/step - loss: 11039.4021 - mse: 201252288.0000 - mae: 11039.4023 - val_loss: 8492.1697 - val_mse: 148679552.0000 - val_mae: 8492.1699\n",
            "Epoch 198/500\n",
            "90/90 [==============================] - 0s 858us/step - loss: 12106.3710 - mse: 252904768.0000 - mae: 12106.3711 - val_loss: 8404.0625 - val_mse: 147041952.0000 - val_mae: 8404.0625\n",
            "Epoch 199/500\n",
            "90/90 [==============================] - 0s 974us/step - loss: 11219.0419 - mse: 202597088.0000 - mae: 11219.0420 - val_loss: 8934.3737 - val_mse: 153472080.0000 - val_mae: 8934.3740\n",
            "Epoch 200/500\n",
            "90/90 [==============================] - 0s 730us/step - loss: 10464.0030 - mse: 186322096.0000 - mae: 10464.0029 - val_loss: 8382.7716 - val_mse: 145164016.0000 - val_mae: 8382.7705\n",
            "Epoch 201/500\n",
            "90/90 [==============================] - 0s 673us/step - loss: 11416.4831 - mse: 228300624.0000 - mae: 11416.4824 - val_loss: 8291.8753 - val_mse: 142273808.0000 - val_mae: 8291.8760\n",
            "Epoch 202/500\n",
            "90/90 [==============================] - 0s 624us/step - loss: 10243.9607 - mse: 184529888.0000 - mae: 10243.9609 - val_loss: 8188.6819 - val_mse: 140478560.0000 - val_mae: 8188.6821\n",
            "Epoch 203/500\n",
            "90/90 [==============================] - 0s 678us/step - loss: 11044.5811 - mse: 197886400.0000 - mae: 11044.5801 - val_loss: 7974.8875 - val_mse: 136769088.0000 - val_mae: 7974.8877\n",
            "Epoch 204/500\n",
            "90/90 [==============================] - 0s 631us/step - loss: 10697.7467 - mse: 188008016.0000 - mae: 10697.7471 - val_loss: 8040.5136 - val_mse: 137172384.0000 - val_mae: 8040.5137\n",
            "Epoch 205/500\n",
            "90/90 [==============================] - 0s 651us/step - loss: 9782.4234 - mse: 170713376.0000 - mae: 9782.4229 - val_loss: 8320.7111 - val_mse: 139797056.0000 - val_mae: 8320.7109\n",
            "Epoch 206/500\n",
            "90/90 [==============================] - 0s 647us/step - loss: 11555.6887 - mse: 222468000.0000 - mae: 11555.6885 - val_loss: 7790.3277 - val_mse: 132793096.0000 - val_mae: 7790.3271\n",
            "Epoch 207/500\n",
            "90/90 [==============================] - 0s 689us/step - loss: 11642.8780 - mse: 246718448.0000 - mae: 11642.8779 - val_loss: 7810.9924 - val_mse: 132312088.0000 - val_mae: 7810.9922\n",
            "Epoch 208/500\n",
            "90/90 [==============================] - 0s 823us/step - loss: 10831.9026 - mse: 209496912.0000 - mae: 10831.9023 - val_loss: 7826.0549 - val_mse: 131811320.0000 - val_mae: 7826.0547\n",
            "Epoch 209/500\n",
            "90/90 [==============================] - 0s 753us/step - loss: 9594.7012 - mse: 174466608.0000 - mae: 9594.7002 - val_loss: 7846.5390 - val_mse: 131159600.0000 - val_mae: 7846.5391\n",
            "Epoch 210/500\n",
            "90/90 [==============================] - 0s 654us/step - loss: 10198.0355 - mse: 188615648.0000 - mae: 10198.0352 - val_loss: 8082.2448 - val_mse: 134675232.0000 - val_mae: 8082.2446\n",
            "Epoch 211/500\n",
            "90/90 [==============================] - 0s 704us/step - loss: 10760.3139 - mse: 202977728.0000 - mae: 10760.3135 - val_loss: 7618.1753 - val_mse: 128594008.0000 - val_mae: 7618.1748\n",
            "Epoch 212/500\n",
            "90/90 [==============================] - 0s 683us/step - loss: 9613.8957 - mse: 175205312.0000 - mae: 9613.8955 - val_loss: 7483.6371 - val_mse: 125631864.0000 - val_mae: 7483.6367\n",
            "Epoch 213/500\n",
            "90/90 [==============================] - 0s 756us/step - loss: 9915.9918 - mse: 172773728.0000 - mae: 9915.9912 - val_loss: 7432.5944 - val_mse: 124671568.0000 - val_mae: 7432.5942\n",
            "Epoch 214/500\n",
            "90/90 [==============================] - 0s 798us/step - loss: 10092.5614 - mse: 171703232.0000 - mae: 10092.5615 - val_loss: 7346.2345 - val_mse: 123360112.0000 - val_mae: 7346.2349\n",
            "Epoch 215/500\n",
            "90/90 [==============================] - 0s 780us/step - loss: 10082.3039 - mse: 181934688.0000 - mae: 10082.3037 - val_loss: 7498.5819 - val_mse: 124110816.0000 - val_mae: 7498.5820\n",
            "Epoch 216/500\n",
            "90/90 [==============================] - 0s 851us/step - loss: 10106.9482 - mse: 194052384.0000 - mae: 10106.9482 - val_loss: 7558.7829 - val_mse: 123939376.0000 - val_mae: 7558.7827\n",
            "Epoch 217/500\n",
            "90/90 [==============================] - 0s 693us/step - loss: 10773.4386 - mse: 205550064.0000 - mae: 10773.4385 - val_loss: 7537.1733 - val_mse: 122936024.0000 - val_mae: 7537.1729\n",
            "Epoch 218/500\n",
            "90/90 [==============================] - 0s 642us/step - loss: 9471.1307 - mse: 157289872.0000 - mae: 9471.1309 - val_loss: 7742.0948 - val_mse: 124666472.0000 - val_mae: 7742.0942\n",
            "Epoch 219/500\n",
            "90/90 [==============================] - 0s 623us/step - loss: 10176.0859 - mse: 190360592.0000 - mae: 10176.0859 - val_loss: 7371.7928 - val_mse: 120960920.0000 - val_mae: 7371.7925\n",
            "Epoch 220/500\n",
            "90/90 [==============================] - 0s 715us/step - loss: 9926.3496 - mse: 170113392.0000 - mae: 9926.3496 - val_loss: 7465.0593 - val_mse: 121260888.0000 - val_mae: 7465.0591\n",
            "Epoch 221/500\n",
            "90/90 [==============================] - 0s 665us/step - loss: 11168.1832 - mse: 213702208.0000 - mae: 11168.1836 - val_loss: 7460.9149 - val_mse: 121493456.0000 - val_mae: 7460.9150\n",
            "Epoch 222/500\n",
            "90/90 [==============================] - 0s 628us/step - loss: 11361.4150 - mse: 231027568.0000 - mae: 11361.4150 - val_loss: 7714.3009 - val_mse: 123832184.0000 - val_mae: 7714.3008\n",
            "Epoch 223/500\n",
            "90/90 [==============================] - 0s 635us/step - loss: 10337.7916 - mse: 202449808.0000 - mae: 10337.7920 - val_loss: 8154.1872 - val_mse: 128250360.0000 - val_mae: 8154.1875\n",
            "Epoch 224/500\n",
            "90/90 [==============================] - 0s 683us/step - loss: 10551.0004 - mse: 181665616.0000 - mae: 10551.0000 - val_loss: 7555.8236 - val_mse: 121350984.0000 - val_mae: 7555.8237\n",
            "Epoch 225/500\n",
            "90/90 [==============================] - 0s 761us/step - loss: 10107.0573 - mse: 179703600.0000 - mae: 10107.0576 - val_loss: 7501.3160 - val_mse: 120048912.0000 - val_mae: 7501.3159\n",
            "Epoch 226/500\n",
            "90/90 [==============================] - 0s 804us/step - loss: 9770.2027 - mse: 165628480.0000 - mae: 9770.2031 - val_loss: 7781.1803 - val_mse: 123149536.0000 - val_mae: 7781.1797\n",
            "Epoch 227/500\n",
            "90/90 [==============================] - 0s 690us/step - loss: 9399.4380 - mse: 148129312.0000 - mae: 9399.4375 - val_loss: 7654.8565 - val_mse: 121346552.0000 - val_mae: 7654.8569\n",
            "Epoch 228/500\n",
            "90/90 [==============================] - 0s 660us/step - loss: 10480.1285 - mse: 184929568.0000 - mae: 10480.1289 - val_loss: 7566.9113 - val_mse: 119889696.0000 - val_mae: 7566.9111\n",
            "Epoch 229/500\n",
            "90/90 [==============================] - 0s 740us/step - loss: 11001.8029 - mse: 191855936.0000 - mae: 11001.8037 - val_loss: 7919.5061 - val_mse: 123118272.0000 - val_mae: 7919.5054\n",
            "Epoch 230/500\n",
            "90/90 [==============================] - 0s 858us/step - loss: 10349.9934 - mse: 178905776.0000 - mae: 10349.9941 - val_loss: 7366.2335 - val_mse: 116335024.0000 - val_mae: 7366.2334\n",
            "Epoch 231/500\n",
            "90/90 [==============================] - 0s 644us/step - loss: 10565.3593 - mse: 194877264.0000 - mae: 10565.3594 - val_loss: 7292.5837 - val_mse: 114925344.0000 - val_mae: 7292.5840\n",
            "Epoch 232/500\n",
            "90/90 [==============================] - 0s 712us/step - loss: 10183.7680 - mse: 184682592.0000 - mae: 10183.7676 - val_loss: 7181.6460 - val_mse: 112747984.0000 - val_mae: 7181.6465\n",
            "Epoch 233/500\n",
            "90/90 [==============================] - 0s 615us/step - loss: 11177.0113 - mse: 207819936.0000 - mae: 11177.0117 - val_loss: 7237.6137 - val_mse: 113559744.0000 - val_mae: 7237.6133\n",
            "Epoch 234/500\n",
            "90/90 [==============================] - 0s 720us/step - loss: 10365.4364 - mse: 186148416.0000 - mae: 10365.4365 - val_loss: 7178.5752 - val_mse: 113177648.0000 - val_mae: 7178.5752\n",
            "Epoch 235/500\n",
            "90/90 [==============================] - 0s 690us/step - loss: 10075.8455 - mse: 169734864.0000 - mae: 10075.8457 - val_loss: 7076.6819 - val_mse: 112051840.0000 - val_mae: 7076.6821\n",
            "Epoch 236/500\n",
            "90/90 [==============================] - 0s 680us/step - loss: 9128.2437 - mse: 169908608.0000 - mae: 9128.2441 - val_loss: 7256.2994 - val_mse: 113480952.0000 - val_mae: 7256.2988\n",
            "Epoch 237/500\n",
            "90/90 [==============================] - 0s 776us/step - loss: 9570.8559 - mse: 168306528.0000 - mae: 9570.8555 - val_loss: 7377.0891 - val_mse: 113234216.0000 - val_mae: 7377.0889\n",
            "Epoch 238/500\n",
            "90/90 [==============================] - 0s 719us/step - loss: 11039.0543 - mse: 215380928.0000 - mae: 11039.0537 - val_loss: 7495.3728 - val_mse: 114246248.0000 - val_mae: 7495.3721\n",
            "Epoch 239/500\n",
            "90/90 [==============================] - 0s 682us/step - loss: 9006.4403 - mse: 167507936.0000 - mae: 9006.4414 - val_loss: 7250.9061 - val_mse: 110759552.0000 - val_mae: 7250.9058\n",
            "Epoch 240/500\n",
            "90/90 [==============================] - 0s 700us/step - loss: 11029.2691 - mse: 229829488.0000 - mae: 11029.2695 - val_loss: 7724.1081 - val_mse: 116912720.0000 - val_mae: 7724.1074\n",
            "Epoch 241/500\n",
            "90/90 [==============================] - 0s 694us/step - loss: 10768.2523 - mse: 199766944.0000 - mae: 10768.2510 - val_loss: 7203.0065 - val_mse: 110648488.0000 - val_mae: 7203.0068\n",
            "Epoch 242/500\n",
            "90/90 [==============================] - 0s 696us/step - loss: 11185.8688 - mse: 205772256.0000 - mae: 11185.8691 - val_loss: 7448.7522 - val_mse: 112457304.0000 - val_mae: 7448.7529\n",
            "Epoch 243/500\n",
            "90/90 [==============================] - 0s 690us/step - loss: 10962.9865 - mse: 196426752.0000 - mae: 10962.9863 - val_loss: 7595.6406 - val_mse: 114116344.0000 - val_mae: 7595.6411\n",
            "Epoch 244/500\n",
            "90/90 [==============================] - 0s 809us/step - loss: 11165.7045 - mse: 198442912.0000 - mae: 11165.7051 - val_loss: 7476.2344 - val_mse: 112165568.0000 - val_mae: 7476.2339\n",
            "Epoch 245/500\n",
            "90/90 [==============================] - 0s 686us/step - loss: 10243.7509 - mse: 174004560.0000 - mae: 10243.7510 - val_loss: 7292.1611 - val_mse: 109124008.0000 - val_mae: 7292.1606\n",
            "Epoch 246/500\n",
            "90/90 [==============================] - 0s 658us/step - loss: 10420.5551 - mse: 193579488.0000 - mae: 10420.5547 - val_loss: 7270.9076 - val_mse: 108615920.0000 - val_mae: 7270.9077\n",
            "Epoch 247/500\n",
            "90/90 [==============================] - 0s 715us/step - loss: 11347.5217 - mse: 193500336.0000 - mae: 11347.5215 - val_loss: 7416.1106 - val_mse: 110324016.0000 - val_mae: 7416.1104\n",
            "Epoch 248/500\n",
            "90/90 [==============================] - 0s 753us/step - loss: 10697.0661 - mse: 210088672.0000 - mae: 10697.0664 - val_loss: 7354.1174 - val_mse: 109634928.0000 - val_mae: 7354.1172\n",
            "Epoch 249/500\n",
            "90/90 [==============================] - 0s 703us/step - loss: 9480.3836 - mse: 166495488.0000 - mae: 9480.3838 - val_loss: 7615.7973 - val_mse: 112229600.0000 - val_mae: 7615.7974\n",
            "Epoch 250/500\n",
            "90/90 [==============================] - 0s 713us/step - loss: 11113.6347 - mse: 216164496.0000 - mae: 11113.6338 - val_loss: 7466.4557 - val_mse: 110363760.0000 - val_mae: 7466.4556\n",
            "Epoch 251/500\n",
            "90/90 [==============================] - 0s 701us/step - loss: 10067.3458 - mse: 175691616.0000 - mae: 10067.3447 - val_loss: 7666.0176 - val_mse: 113186336.0000 - val_mae: 7666.0176\n",
            "Epoch 252/500\n",
            "90/90 [==============================] - 0s 688us/step - loss: 9107.5422 - mse: 158374656.0000 - mae: 9107.5420 - val_loss: 7245.1233 - val_mse: 107976784.0000 - val_mae: 7245.1230\n",
            "Epoch 253/500\n",
            "90/90 [==============================] - 0s 777us/step - loss: 9264.7604 - mse: 159178800.0000 - mae: 9264.7598 - val_loss: 7208.5310 - val_mse: 107625416.0000 - val_mae: 7208.5317\n",
            "Epoch 254/500\n",
            "90/90 [==============================] - 0s 798us/step - loss: 10489.5144 - mse: 173203344.0000 - mae: 10489.5137 - val_loss: 7468.4757 - val_mse: 109771152.0000 - val_mae: 7468.4756\n",
            "Epoch 255/500\n",
            "90/90 [==============================] - 0s 761us/step - loss: 9996.8540 - mse: 193265488.0000 - mae: 9996.8545 - val_loss: 7270.0035 - val_mse: 106864392.0000 - val_mae: 7270.0029\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 20 samples, validate on 10 samples\n",
            "Epoch 1/500\n",
            "20/20 [==============================] - 2s 100ms/step - loss: 738.9736 - mse: 829285.1250 - mae: 738.9736 - val_loss: 711.0170 - val_mse: 527265.8750 - val_mae: 711.0170\n",
            "Epoch 2/500\n",
            "20/20 [==============================] - 0s 988us/step - loss: 738.9530 - mse: 829252.1875 - mae: 738.9530 - val_loss: 710.9996 - val_mse: 527241.3125 - val_mae: 710.9996\n",
            "Epoch 3/500\n",
            "20/20 [==============================] - 0s 858us/step - loss: 738.9351 - mse: 829228.1875 - mae: 738.9351 - val_loss: 710.9828 - val_mse: 527217.3750 - val_mae: 710.9828\n",
            "Epoch 4/500\n",
            "20/20 [==============================] - 0s 917us/step - loss: 738.9178 - mse: 829201.0000 - mae: 738.9178 - val_loss: 710.9669 - val_mse: 527194.6875 - val_mae: 710.9669\n",
            "Epoch 5/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 738.9032 - mse: 829182.3750 - mae: 738.9032 - val_loss: 710.9512 - val_mse: 527172.3750 - val_mae: 710.9512\n",
            "Epoch 6/500\n",
            "20/20 [==============================] - 0s 826us/step - loss: 738.8841 - mse: 829154.5625 - mae: 738.8841 - val_loss: 710.9335 - val_mse: 527147.5000 - val_mae: 710.9335\n",
            "Epoch 7/500\n",
            "20/20 [==============================] - 0s 889us/step - loss: 738.8619 - mse: 829124.5625 - mae: 738.8619 - val_loss: 710.9124 - val_mse: 527118.1250 - val_mae: 710.9124\n",
            "Epoch 8/500\n",
            "20/20 [==============================] - 0s 915us/step - loss: 738.8280 - mse: 829084.6875 - mae: 738.8280 - val_loss: 710.8835 - val_mse: 527078.8125 - val_mae: 710.8835\n",
            "Epoch 9/500\n",
            "20/20 [==============================] - 0s 909us/step - loss: 738.7670 - mse: 829027.8125 - mae: 738.7670 - val_loss: 710.8344 - val_mse: 527014.5625 - val_mae: 710.8344\n",
            "Epoch 10/500\n",
            "20/20 [==============================] - 0s 996us/step - loss: 738.5511 - mse: 828876.7500 - mae: 738.5511 - val_loss: 710.7181 - val_mse: 526868.9375 - val_mae: 710.7181\n",
            "Epoch 11/500\n",
            "20/20 [==============================] - 0s 747us/step - loss: 737.8629 - mse: 828469.1875 - mae: 737.8629 - val_loss: 710.4247 - val_mse: 526510.8750 - val_mae: 710.4247\n",
            "Epoch 12/500\n",
            "20/20 [==============================] - 0s 795us/step - loss: 737.1604 - mse: 828205.3750 - mae: 737.1604 - val_loss: 709.9199 - val_mse: 525904.8125 - val_mae: 709.9199\n",
            "Epoch 13/500\n",
            "20/20 [==============================] - 0s 829us/step - loss: 735.7249 - mse: 827169.0000 - mae: 735.7250 - val_loss: 709.1144 - val_mse: 524930.6250 - val_mae: 709.1144\n",
            "Epoch 14/500\n",
            "20/20 [==============================] - 0s 816us/step - loss: 732.6595 - mse: 825319.1250 - mae: 732.6595 - val_loss: 707.8795 - val_mse: 523441.5938 - val_mae: 707.8795\n",
            "Epoch 15/500\n",
            "20/20 [==============================] - 0s 893us/step - loss: 732.1310 - mse: 823989.3125 - mae: 732.1310 - val_loss: 706.6610 - val_mse: 521919.0938 - val_mae: 706.6610\n",
            "Epoch 16/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 726.5538 - mse: 820474.4375 - mae: 726.5538 - val_loss: 704.9073 - val_mse: 519715.4375 - val_mae: 704.9073\n",
            "Epoch 17/500\n",
            "20/20 [==============================] - 0s 965us/step - loss: 725.2003 - mse: 817285.4375 - mae: 725.2003 - val_loss: 703.0851 - val_mse: 517338.3125 - val_mae: 703.0851\n",
            "Epoch 18/500\n",
            "20/20 [==============================] - 0s 956us/step - loss: 722.7110 - mse: 814809.6250 - mae: 722.7111 - val_loss: 701.1041 - val_mse: 514774.8438 - val_mae: 701.1041\n",
            "Epoch 19/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 719.7509 - mse: 810618.3750 - mae: 719.7509 - val_loss: 698.8944 - val_mse: 511851.3125 - val_mae: 698.8944\n",
            "Epoch 20/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 717.7577 - mse: 807478.1875 - mae: 717.7577 - val_loss: 696.5875 - val_mse: 508751.5938 - val_mae: 696.5875\n",
            "Epoch 21/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 714.8208 - mse: 801994.3125 - mae: 714.8208 - val_loss: 694.0334 - val_mse: 505348.3125 - val_mae: 694.0334\n",
            "Epoch 22/500\n",
            "20/20 [==============================] - 0s 883us/step - loss: 706.6683 - mse: 791571.0625 - mae: 706.6683 - val_loss: 690.8959 - val_mse: 501092.3125 - val_mae: 690.8959\n",
            "Epoch 23/500\n",
            "20/20 [==============================] - 0s 840us/step - loss: 698.3162 - mse: 781675.8750 - mae: 698.3162 - val_loss: 687.3924 - val_mse: 496344.7500 - val_mae: 687.3924\n",
            "Epoch 24/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 698.2524 - mse: 781919.6250 - mae: 698.2524 - val_loss: 684.0856 - val_mse: 491907.4062 - val_mae: 684.0856\n",
            "Epoch 25/500\n",
            "20/20 [==============================] - 0s 849us/step - loss: 692.8875 - mse: 775307.3125 - mae: 692.8875 - val_loss: 680.5148 - val_mse: 487130.0938 - val_mae: 680.5148\n",
            "Epoch 26/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 688.4380 - mse: 765366.5000 - mae: 688.4380 - val_loss: 676.7832 - val_mse: 482100.6875 - val_mae: 676.7832\n",
            "Epoch 27/500\n",
            "20/20 [==============================] - 0s 951us/step - loss: 684.1272 - mse: 762082.1250 - mae: 684.1271 - val_loss: 672.9341 - val_mse: 476983.0625 - val_mae: 672.9341\n",
            "Epoch 28/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 680.8385 - mse: 751695.6875 - mae: 680.8385 - val_loss: 668.9135 - val_mse: 471607.6875 - val_mae: 668.9135\n",
            "Epoch 29/500\n",
            "20/20 [==============================] - 0s 991us/step - loss: 674.2511 - mse: 741905.8750 - mae: 674.2511 - val_loss: 664.6984 - val_mse: 465972.3125 - val_mae: 664.6984\n",
            "Epoch 30/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 675.5093 - mse: 745095.3125 - mae: 675.5094 - val_loss: 660.6969 - val_mse: 460658.5938 - val_mae: 660.6969\n",
            "Epoch 31/500\n",
            "20/20 [==============================] - 0s 828us/step - loss: 665.3373 - mse: 737062.1250 - mae: 665.3373 - val_loss: 656.8278 - val_mse: 455552.0938 - val_mae: 656.8278\n",
            "Epoch 32/500\n",
            "20/20 [==============================] - 0s 919us/step - loss: 659.4154 - mse: 716708.0625 - mae: 659.4154 - val_loss: 652.5320 - val_mse: 449877.9375 - val_mae: 652.5320\n",
            "Epoch 33/500\n",
            "20/20 [==============================] - 0s 897us/step - loss: 649.1491 - mse: 709497.0000 - mae: 649.1491 - val_loss: 649.0317 - val_mse: 445222.6562 - val_mae: 649.0317\n",
            "Epoch 34/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 650.0705 - mse: 715722.8750 - mae: 650.0705 - val_loss: 644.6801 - val_mse: 439588.1562 - val_mae: 644.6801\n",
            "Epoch 35/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 640.8455 - mse: 701367.8125 - mae: 640.8455 - val_loss: 640.3413 - val_mse: 434001.1875 - val_mae: 640.3413\n",
            "Epoch 36/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 635.4277 - mse: 701086.8750 - mae: 635.4277 - val_loss: 636.1238 - val_mse: 428603.5938 - val_mae: 636.1238\n",
            "Epoch 37/500\n",
            "20/20 [==============================] - 0s 950us/step - loss: 636.0848 - mse: 693163.7500 - mae: 636.0848 - val_loss: 633.1327 - val_mse: 424681.1875 - val_mae: 633.1327\n",
            "Epoch 38/500\n",
            "20/20 [==============================] - 0s 915us/step - loss: 628.0650 - mse: 683361.8750 - mae: 628.0650 - val_loss: 628.5106 - val_mse: 418869.9688 - val_mae: 628.5106\n",
            "Epoch 39/500\n",
            "20/20 [==============================] - 0s 902us/step - loss: 614.3253 - mse: 647119.1250 - mae: 614.3253 - val_loss: 623.8351 - val_mse: 412853.8125 - val_mae: 623.8351\n",
            "Epoch 40/500\n",
            "20/20 [==============================] - 0s 799us/step - loss: 634.8197 - mse: 679592.3125 - mae: 634.8197 - val_loss: 620.8416 - val_mse: 408995.3125 - val_mae: 620.8416\n",
            "Epoch 41/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 624.3403 - mse: 657053.6250 - mae: 624.3402 - val_loss: 617.0695 - val_mse: 404156.2812 - val_mae: 617.0695\n",
            "Epoch 42/500\n",
            "20/20 [==============================] - 0s 952us/step - loss: 606.4739 - mse: 649891.1250 - mae: 606.4739 - val_loss: 611.7943 - val_mse: 397658.3438 - val_mae: 611.7943\n",
            "Epoch 43/500\n",
            "20/20 [==============================] - 0s 971us/step - loss: 616.3150 - mse: 651964.8125 - mae: 616.3151 - val_loss: 607.1224 - val_mse: 391876.9062 - val_mae: 607.1224\n",
            "Epoch 44/500\n",
            "20/20 [==============================] - 0s 999us/step - loss: 618.7752 - mse: 654120.1875 - mae: 618.7752 - val_loss: 603.8157 - val_mse: 387726.3125 - val_mae: 603.8157\n",
            "Epoch 45/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 611.4575 - mse: 643051.3750 - mae: 611.4575 - val_loss: 600.2812 - val_mse: 383309.3125 - val_mae: 600.2812\n",
            "Epoch 46/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 605.1985 - mse: 621815.3125 - mae: 605.1985 - val_loss: 596.4283 - val_mse: 378517.9062 - val_mae: 596.4283\n",
            "Epoch 47/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 603.3458 - mse: 631396.6250 - mae: 603.3458 - val_loss: 592.7913 - val_mse: 374008.1250 - val_mae: 592.7913\n",
            "Epoch 48/500\n",
            "20/20 [==============================] - 0s 996us/step - loss: 598.2109 - mse: 635855.4375 - mae: 598.2109 - val_loss: 588.3807 - val_mse: 368731.2188 - val_mae: 588.3807\n",
            "Epoch 49/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 602.7758 - mse: 623117.5000 - mae: 602.7759 - val_loss: 584.3039 - val_mse: 363814.9688 - val_mae: 584.3039\n",
            "Epoch 50/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 597.2706 - mse: 628993.8750 - mae: 597.2706 - val_loss: 580.9786 - val_mse: 359751.5938 - val_mae: 580.9786\n",
            "Epoch 51/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 578.7912 - mse: 581370.5000 - mae: 578.7912 - val_loss: 579.5736 - val_mse: 357579.9062 - val_mae: 579.5736\n",
            "Epoch 52/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 591.6243 - mse: 603520.3750 - mae: 591.6243 - val_loss: 577.5985 - val_mse: 354840.9062 - val_mae: 577.5985\n",
            "Epoch 53/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 583.2922 - mse: 598402.3125 - mae: 583.2922 - val_loss: 576.4921 - val_mse: 352876.6562 - val_mae: 576.4921\n",
            "Epoch 54/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 580.6654 - mse: 588057.3750 - mae: 580.6654 - val_loss: 577.4600 - val_mse: 353244.0000 - val_mae: 577.4600\n",
            "Epoch 55/500\n",
            "20/20 [==============================] - 0s 894us/step - loss: 568.1062 - mse: 571890.8750 - mae: 568.1063 - val_loss: 571.9440 - val_mse: 346834.0938 - val_mae: 571.9440\n",
            "Epoch 56/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 574.6395 - mse: 584725.2500 - mae: 574.6395 - val_loss: 566.8125 - val_mse: 341086.7500 - val_mae: 566.8125\n",
            "Epoch 57/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 578.4308 - mse: 592613.0625 - mae: 578.4308 - val_loss: 562.0835 - val_mse: 335790.7500 - val_mae: 562.0835\n",
            "Epoch 58/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 554.5780 - mse: 532006.5000 - mae: 554.5779 - val_loss: 564.3666 - val_mse: 337376.5625 - val_mae: 564.3666\n",
            "Epoch 59/500\n",
            "20/20 [==============================] - 0s 631us/step - loss: 556.5164 - mse: 545476.1250 - mae: 556.5165 - val_loss: 561.1567 - val_mse: 333548.3125 - val_mae: 561.1567\n",
            "Epoch 60/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 574.7164 - mse: 548945.8125 - mae: 574.7164 - val_loss: 554.2700 - val_mse: 326041.1875 - val_mae: 554.2700\n",
            "Epoch 61/500\n",
            "20/20 [==============================] - 0s 987us/step - loss: 577.2628 - mse: 592375.6250 - mae: 577.2628 - val_loss: 558.0958 - val_mse: 329401.7500 - val_mae: 558.0958\n",
            "Epoch 62/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 547.6823 - mse: 537770.8750 - mae: 547.6823 - val_loss: 550.0717 - val_mse: 320721.8125 - val_mae: 550.0717\n",
            "Epoch 63/500\n",
            "20/20 [==============================] - 0s 903us/step - loss: 548.0773 - mse: 539114.5625 - mae: 548.0773 - val_loss: 543.1149 - val_mse: 313220.3438 - val_mae: 543.1149\n",
            "Epoch 64/500\n",
            "20/20 [==============================] - 0s 999us/step - loss: 566.3330 - mse: 538517.3125 - mae: 566.3330 - val_loss: 542.8621 - val_mse: 312491.4375 - val_mae: 542.8621\n",
            "Epoch 65/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 537.1761 - mse: 524332.5000 - mae: 537.1761 - val_loss: 540.4432 - val_mse: 309568.0000 - val_mae: 540.4432\n",
            "Epoch 66/500\n",
            "20/20 [==============================] - 0s 930us/step - loss: 533.4505 - mse: 523983.0000 - mae: 533.4505 - val_loss: 541.5779 - val_mse: 310239.6562 - val_mae: 541.5779\n",
            "Epoch 67/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 500.6633 - mse: 495526.0000 - mae: 500.6633 - val_loss: 532.4569 - val_mse: 300519.0000 - val_mae: 532.4569\n",
            "Epoch 68/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 524.8566 - mse: 519813.5938 - mae: 524.8566 - val_loss: 530.4831 - val_mse: 298190.5625 - val_mae: 530.4831\n",
            "Epoch 69/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 539.9551 - mse: 512776.1875 - mae: 539.9551 - val_loss: 530.6431 - val_mse: 297841.0000 - val_mae: 530.6431\n",
            "Epoch 70/500\n",
            "20/20 [==============================] - 0s 925us/step - loss: 547.9510 - mse: 512391.1875 - mae: 547.9510 - val_loss: 525.2684 - val_mse: 292173.7812 - val_mae: 525.2684\n",
            "Epoch 71/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 526.3152 - mse: 497074.3125 - mae: 526.3152 - val_loss: 522.5300 - val_mse: 289004.4062 - val_mae: 522.5300\n",
            "Epoch 72/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 502.3342 - mse: 468554.6875 - mae: 502.3342 - val_loss: 515.9589 - val_mse: 282176.9375 - val_mae: 515.9589\n",
            "Epoch 73/500\n",
            "20/20 [==============================] - 0s 933us/step - loss: 518.6307 - mse: 473598.5000 - mae: 518.6307 - val_loss: 519.6432 - val_mse: 285308.1562 - val_mae: 519.6432\n",
            "Epoch 74/500\n",
            "20/20 [==============================] - 0s 973us/step - loss: 530.9610 - mse: 478210.0938 - mae: 530.9611 - val_loss: 512.6534 - val_mse: 278103.5625 - val_mae: 512.6534\n",
            "Epoch 75/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 534.4776 - mse: 503676.0000 - mae: 534.4775 - val_loss: 509.1393 - val_mse: 274121.8125 - val_mae: 509.1393\n",
            "Epoch 76/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 501.9083 - mse: 486121.0938 - mae: 501.9083 - val_loss: 503.4656 - val_mse: 268239.8125 - val_mae: 503.4656\n",
            "Epoch 77/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 492.4815 - mse: 454391.4062 - mae: 492.4815 - val_loss: 500.9539 - val_mse: 265329.4375 - val_mae: 500.9539\n",
            "Epoch 78/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 496.4525 - mse: 450403.8125 - mae: 496.4525 - val_loss: 505.1662 - val_mse: 268789.5312 - val_mae: 505.1662\n",
            "Epoch 79/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 469.0776 - mse: 397725.6562 - mae: 469.0776 - val_loss: 502.1197 - val_mse: 265225.7812 - val_mae: 502.1197\n",
            "Epoch 80/500\n",
            "20/20 [==============================] - 0s 959us/step - loss: 484.8842 - mse: 422186.3125 - mae: 484.8842 - val_loss: 497.3859 - val_mse: 260258.3750 - val_mae: 497.3859\n",
            "Epoch 81/500\n",
            "20/20 [==============================] - 0s 935us/step - loss: 460.7399 - mse: 388657.1875 - mae: 460.7399 - val_loss: 489.9982 - val_mse: 252691.0312 - val_mae: 489.9982\n",
            "Epoch 82/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 468.2452 - mse: 396242.9688 - mae: 468.2452 - val_loss: 492.6466 - val_mse: 254915.2031 - val_mae: 492.6466\n",
            "Epoch 83/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 505.2527 - mse: 472977.7500 - mae: 505.2527 - val_loss: 487.1960 - val_mse: 249268.3750 - val_mae: 487.1960\n",
            "Epoch 84/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 508.7613 - mse: 461968.5938 - mae: 508.7613 - val_loss: 483.7271 - val_mse: 245570.7031 - val_mae: 483.7271\n",
            "Epoch 85/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 454.4587 - mse: 381809.4375 - mae: 454.4587 - val_loss: 479.5688 - val_mse: 241433.3750 - val_mae: 479.5688\n",
            "Epoch 86/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 496.7118 - mse: 426930.4062 - mae: 496.7118 - val_loss: 484.0526 - val_mse: 245015.6562 - val_mae: 484.0526\n",
            "Epoch 87/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 452.3018 - mse: 384249.5312 - mae: 452.3018 - val_loss: 476.7692 - val_mse: 237890.4219 - val_mae: 476.7692\n",
            "Epoch 88/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 494.3850 - mse: 421999.6875 - mae: 494.3850 - val_loss: 475.6852 - val_mse: 236332.8438 - val_mae: 475.6852\n",
            "Epoch 89/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 462.7098 - mse: 404927.1875 - mae: 462.7098 - val_loss: 473.0502 - val_mse: 233507.0469 - val_mae: 473.0502\n",
            "Epoch 90/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 472.4131 - mse: 406330.8125 - mae: 472.4131 - val_loss: 467.2724 - val_mse: 227686.7969 - val_mae: 467.2724\n",
            "Epoch 91/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 446.7081 - mse: 367178.7500 - mae: 446.7081 - val_loss: 467.9553 - val_mse: 227801.0312 - val_mae: 467.9553\n",
            "Epoch 92/500\n",
            "20/20 [==============================] - 0s 842us/step - loss: 473.3034 - mse: 388256.4375 - mae: 473.3034 - val_loss: 468.5226 - val_mse: 227556.5469 - val_mae: 468.5226\n",
            "Epoch 93/500\n",
            "20/20 [==============================] - 0s 847us/step - loss: 447.0511 - mse: 368806.9062 - mae: 447.0511 - val_loss: 465.5963 - val_mse: 224265.7500 - val_mae: 465.5963\n",
            "Epoch 94/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 427.8998 - mse: 341217.2500 - mae: 427.8998 - val_loss: 456.8952 - val_mse: 216116.4531 - val_mae: 456.8952\n",
            "Epoch 95/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 416.1398 - mse: 317266.6875 - mae: 416.1398 - val_loss: 455.1668 - val_mse: 214217.4062 - val_mae: 455.1668\n",
            "Epoch 96/500\n",
            "20/20 [==============================] - 0s 962us/step - loss: 451.7038 - mse: 379783.9062 - mae: 451.7038 - val_loss: 459.2431 - val_mse: 217350.6250 - val_mae: 459.2431\n",
            "Epoch 97/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 440.3750 - mse: 360813.6875 - mae: 440.3750 - val_loss: 457.9362 - val_mse: 215642.5000 - val_mae: 457.9362\n",
            "Epoch 98/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 415.3956 - mse: 342175.6875 - mae: 415.3956 - val_loss: 450.9717 - val_mse: 208858.4531 - val_mae: 450.9717\n",
            "Epoch 99/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 399.4661 - mse: 298189.5938 - mae: 399.4661 - val_loss: 449.1122 - val_mse: 207003.5781 - val_mae: 449.1122\n",
            "Epoch 100/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 419.2478 - mse: 307429.6562 - mae: 419.2478 - val_loss: 445.7794 - val_mse: 203856.4688 - val_mae: 445.7794\n",
            "Epoch 101/500\n",
            "20/20 [==============================] - 0s 981us/step - loss: 452.3129 - mse: 356371.1562 - mae: 452.3129 - val_loss: 440.9373 - val_mse: 199268.8438 - val_mae: 440.9373\n",
            "Epoch 102/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 437.4159 - mse: 352790.4062 - mae: 437.4159 - val_loss: 440.2854 - val_mse: 198267.7656 - val_mae: 440.2854\n",
            "Epoch 103/500\n",
            "20/20 [==============================] - 0s 834us/step - loss: 427.7357 - mse: 335268.6562 - mae: 427.7357 - val_loss: 440.4255 - val_mse: 198107.0938 - val_mae: 440.4255\n",
            "Epoch 104/500\n",
            "20/20 [==============================] - 0s 977us/step - loss: 436.0493 - mse: 340265.1875 - mae: 436.0493 - val_loss: 436.2932 - val_mse: 194203.8438 - val_mae: 436.2932\n",
            "Epoch 105/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 397.4501 - mse: 281595.1875 - mae: 397.4501 - val_loss: 439.2127 - val_mse: 196578.9219 - val_mae: 439.2127\n",
            "Epoch 106/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 403.5831 - mse: 299156.5625 - mae: 403.5831 - val_loss: 435.5070 - val_mse: 193477.9688 - val_mae: 435.5070\n",
            "Epoch 107/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 437.7178 - mse: 345073.0938 - mae: 437.7178 - val_loss: 434.7933 - val_mse: 192738.9062 - val_mae: 434.7933\n",
            "Epoch 108/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 407.3434 - mse: 310836.5000 - mae: 407.3434 - val_loss: 428.1621 - val_mse: 187116.8438 - val_mae: 428.1621\n",
            "Epoch 109/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 384.4426 - mse: 275350.5000 - mae: 384.4426 - val_loss: 428.8121 - val_mse: 187975.8438 - val_mae: 428.8121\n",
            "Epoch 110/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 454.2736 - mse: 376890.0000 - mae: 454.2736 - val_loss: 429.7072 - val_mse: 189117.4688 - val_mae: 429.7072\n",
            "Epoch 111/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 403.9130 - mse: 287381.5938 - mae: 403.9130 - val_loss: 428.5630 - val_mse: 188141.2500 - val_mae: 428.5630\n",
            "Epoch 112/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 347.4027 - mse: 226946.5312 - mae: 347.4027 - val_loss: 422.7052 - val_mse: 183232.0938 - val_mae: 422.7052\n",
            "Epoch 113/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 354.4997 - mse: 233168.6562 - mae: 354.4997 - val_loss: 420.1114 - val_mse: 181506.1875 - val_mae: 420.1114\n",
            "Epoch 114/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 331.8452 - mse: 226698.5469 - mae: 331.8452 - val_loss: 411.2276 - val_mse: 174203.5312 - val_mae: 411.2276\n",
            "Epoch 115/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 366.6515 - mse: 274038.3125 - mae: 366.6515 - val_loss: 411.5674 - val_mse: 174805.4062 - val_mae: 411.5674\n",
            "Epoch 116/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 356.8067 - mse: 242414.7031 - mae: 356.8066 - val_loss: 406.5898 - val_mse: 170827.3438 - val_mae: 406.5898\n",
            "Epoch 117/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 342.8920 - mse: 216618.5938 - mae: 342.8919 - val_loss: 403.8567 - val_mse: 169338.2969 - val_mae: 403.8567\n",
            "Epoch 118/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 309.2403 - mse: 186673.7188 - mae: 309.2403 - val_loss: 390.1612 - val_mse: 158333.0938 - val_mae: 390.1612\n",
            "Epoch 119/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 382.0400 - mse: 269320.5625 - mae: 382.0400 - val_loss: 389.8630 - val_mse: 159145.2812 - val_mae: 389.8630\n",
            "Epoch 120/500\n",
            "20/20 [==============================] - 0s 979us/step - loss: 353.5542 - mse: 218628.6719 - mae: 353.5543 - val_loss: 395.6926 - val_mse: 165295.5469 - val_mae: 395.6926\n",
            "Epoch 121/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 331.0818 - mse: 211302.0469 - mae: 331.0818 - val_loss: 392.1248 - val_mse: 162852.9531 - val_mae: 392.1248\n",
            "Epoch 122/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 379.1752 - mse: 267910.1875 - mae: 379.1752 - val_loss: 384.6082 - val_mse: 157562.4219 - val_mae: 384.6082\n",
            "Epoch 123/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 342.8297 - mse: 219253.0469 - mae: 342.8297 - val_loss: 374.9312 - val_mse: 150351.8281 - val_mae: 374.9312\n",
            "Epoch 124/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 358.5905 - mse: 233101.2969 - mae: 358.5905 - val_loss: 371.0192 - val_mse: 148097.2031 - val_mae: 371.0192\n",
            "Epoch 125/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 355.7874 - mse: 246583.7812 - mae: 355.7874 - val_loss: 369.0120 - val_mse: 147579.0781 - val_mae: 369.0120\n",
            "Epoch 126/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 274.0812 - mse: 151349.9219 - mae: 274.0812 - val_loss: 368.0809 - val_mse: 148068.6562 - val_mae: 368.0809\n",
            "Epoch 127/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 348.7691 - mse: 222420.8750 - mae: 348.7691 - val_loss: 367.3748 - val_mse: 148646.5781 - val_mae: 367.3748\n",
            "Epoch 128/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 250.6249 - mse: 118530.3750 - mae: 250.6248 - val_loss: 363.2714 - val_mse: 147184.9844 - val_mae: 363.2714\n",
            "Epoch 129/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 274.9241 - mse: 132550.4688 - mae: 274.9241 - val_loss: 357.7429 - val_mse: 144034.5781 - val_mae: 357.7429\n",
            "Epoch 130/500\n",
            "20/20 [==============================] - 0s 964us/step - loss: 281.1186 - mse: 165290.6875 - mae: 281.1186 - val_loss: 353.7761 - val_mse: 142690.2500 - val_mae: 353.7761\n",
            "Epoch 131/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 352.5880 - mse: 235743.7500 - mae: 352.5880 - val_loss: 354.0118 - val_mse: 144802.0469 - val_mae: 354.0118\n",
            "Epoch 132/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 295.7542 - mse: 183324.8906 - mae: 295.7542 - val_loss: 347.8458 - val_mse: 140940.5312 - val_mae: 347.8458\n",
            "Epoch 133/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 272.3703 - mse: 157644.3750 - mae: 272.3703 - val_loss: 345.0114 - val_mse: 140808.0000 - val_mae: 345.0114\n",
            "Epoch 134/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 305.7628 - mse: 176665.5000 - mae: 305.7628 - val_loss: 337.5556 - val_mse: 136391.6094 - val_mae: 337.5556\n",
            "Epoch 135/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 337.9309 - mse: 229300.1562 - mae: 337.9309 - val_loss: 336.0462 - val_mse: 137313.5781 - val_mae: 336.0462\n",
            "Epoch 136/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 300.6781 - mse: 191215.2344 - mae: 300.6781 - val_loss: 328.8523 - val_mse: 133425.9062 - val_mae: 328.8523\n",
            "Epoch 137/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 290.0725 - mse: 160750.1719 - mae: 290.0725 - val_loss: 320.3671 - val_mse: 128836.0469 - val_mae: 320.3671\n",
            "Epoch 138/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 255.5955 - mse: 128009.2344 - mae: 255.5955 - val_loss: 317.8902 - val_mse: 129721.6094 - val_mae: 317.8902\n",
            "Epoch 139/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 229.1453 - mse: 111401.3125 - mae: 229.1453 - val_loss: 315.3293 - val_mse: 130830.1406 - val_mae: 315.3293\n",
            "Epoch 140/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 244.7770 - mse: 125493.2891 - mae: 244.7770 - val_loss: 314.9073 - val_mse: 133565.7812 - val_mae: 314.9073\n",
            "Epoch 141/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 206.3812 - mse: 86140.8984 - mae: 206.3812 - val_loss: 307.0384 - val_mse: 130587.0625 - val_mae: 307.0384\n",
            "Epoch 142/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 289.1226 - mse: 170234.8438 - mae: 289.1226 - val_loss: 303.9580 - val_mse: 130294.6016 - val_mae: 303.9580\n",
            "Epoch 143/500\n",
            "20/20 [==============================] - 0s 979us/step - loss: 301.3053 - mse: 192104.0781 - mae: 301.3053 - val_loss: 305.2940 - val_mse: 134375.8281 - val_mae: 305.2940\n",
            "Epoch 144/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 258.4795 - mse: 140992.7656 - mae: 258.4796 - val_loss: 295.2782 - val_mse: 128352.7891 - val_mae: 295.2782\n",
            "Epoch 145/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 286.7624 - mse: 198614.6406 - mae: 286.7624 - val_loss: 279.9149 - val_mse: 118736.7031 - val_mae: 279.9149\n",
            "Epoch 146/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 267.6790 - mse: 133797.0938 - mae: 267.6790 - val_loss: 275.2554 - val_mse: 116458.8359 - val_mae: 275.2554\n",
            "Epoch 147/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 244.0879 - mse: 121122.1641 - mae: 244.0879 - val_loss: 282.4455 - val_mse: 122969.6484 - val_mae: 282.4455\n",
            "Epoch 148/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 223.2742 - mse: 97671.4375 - mae: 223.2742 - val_loss: 285.8801 - val_mse: 126899.3906 - val_mae: 285.8801\n",
            "Epoch 149/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 184.7772 - mse: 81298.2031 - mae: 184.7772 - val_loss: 285.7936 - val_mse: 127526.2109 - val_mae: 285.7936\n",
            "Epoch 150/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 283.1835 - mse: 179614.9219 - mae: 283.1835 - val_loss: 284.2265 - val_mse: 127243.6484 - val_mae: 284.2265\n",
            "Epoch 151/500\n",
            "20/20 [==============================] - 0s 888us/step - loss: 202.5937 - mse: 106370.5781 - mae: 202.5937 - val_loss: 287.5710 - val_mse: 131629.5000 - val_mae: 287.5710\n",
            "Epoch 152/500\n",
            "20/20 [==============================] - 0s 993us/step - loss: 262.7472 - mse: 154669.7656 - mae: 262.7472 - val_loss: 282.3835 - val_mse: 127664.7031 - val_mae: 282.3835\n",
            "Epoch 153/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 192.1876 - mse: 66203.5391 - mae: 192.1876 - val_loss: 289.6907 - val_mse: 134397.9688 - val_mae: 289.6907\n",
            "Epoch 154/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 189.3147 - mse: 76072.5781 - mae: 189.3147 - val_loss: 284.0980 - val_mse: 128150.1875 - val_mae: 284.0980\n",
            "Epoch 155/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 163.0060 - mse: 51973.2773 - mae: 163.0060 - val_loss: 285.3925 - val_mse: 126868.1875 - val_mae: 285.3925\n",
            "Epoch 156/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 191.8999 - mse: 87348.8281 - mae: 191.8999 - val_loss: 288.6615 - val_mse: 129987.0234 - val_mae: 288.6615\n",
            "Epoch 157/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 183.6053 - mse: 82259.0469 - mae: 183.6053 - val_loss: 291.3088 - val_mse: 131861.7031 - val_mae: 291.3088\n",
            "Epoch 158/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 242.2995 - mse: 136920.5938 - mae: 242.2995 - val_loss: 289.7953 - val_mse: 128559.6875 - val_mae: 289.7953\n",
            "Epoch 159/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 194.5315 - mse: 88237.7812 - mae: 194.5315 - val_loss: 286.2445 - val_mse: 123781.2656 - val_mae: 286.2445\n",
            "Epoch 160/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 229.2931 - mse: 133187.8906 - mae: 229.2930 - val_loss: 289.4835 - val_mse: 125998.2266 - val_mae: 289.4835\n",
            "Epoch 161/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 182.5236 - mse: 107362.3281 - mae: 182.5236 - val_loss: 292.1978 - val_mse: 127649.6016 - val_mae: 292.1978\n",
            "Epoch 162/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 179.4124 - mse: 66874.2891 - mae: 179.4124 - val_loss: 299.6658 - val_mse: 131261.8281 - val_mae: 299.6658\n",
            "Epoch 163/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 186.3905 - mse: 98267.6484 - mae: 186.3905 - val_loss: 303.2010 - val_mse: 131427.5781 - val_mae: 303.2010\n",
            "Epoch 164/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 135.0676 - mse: 35672.8242 - mae: 135.0676 - val_loss: 310.1619 - val_mse: 135813.6406 - val_mae: 310.1619\n",
            "Epoch 165/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 150.4168 - mse: 57864.4609 - mae: 150.4168 - val_loss: 301.2844 - val_mse: 129219.4141 - val_mae: 301.2844\n",
            "Epoch 166/500\n",
            "20/20 [==============================] - 0s 1ms/step - loss: 198.1137 - mse: 95048.2031 - mae: 198.1137 - val_loss: 302.3746 - val_mse: 126405.2500 - val_mae: 302.3746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 30 samples, validate on 30 samples\n",
            "Epoch 1/500\n",
            "30/30 [==============================] - 2s 58ms/step - loss: 1637.3267 - mse: 4521314.5000 - mae: 1637.3267 - val_loss: 2644.3728 - val_mse: 10815954.0000 - val_mae: 2644.3726\n",
            "Epoch 2/500\n",
            "30/30 [==============================] - 0s 778us/step - loss: 1637.2820 - mse: 4521166.0000 - mae: 1637.2820 - val_loss: 2644.3269 - val_mse: 10815739.0000 - val_mae: 2644.3269\n",
            "Epoch 3/500\n",
            "30/30 [==============================] - 0s 827us/step - loss: 1637.2223 - mse: 4521024.0000 - mae: 1637.2224 - val_loss: 2644.1948 - val_mse: 10815286.0000 - val_mae: 2644.1948\n",
            "Epoch 4/500\n",
            "30/30 [==============================] - 0s 910us/step - loss: 1636.9770 - mse: 4520646.5000 - mae: 1636.9769 - val_loss: 2643.2267 - val_mse: 10812595.0000 - val_mae: 2643.2266\n",
            "Epoch 5/500\n",
            "30/30 [==============================] - 0s 857us/step - loss: 1635.1793 - mse: 4517695.0000 - mae: 1635.1793 - val_loss: 2639.4412 - val_mse: 10802165.0000 - val_mae: 2639.4412\n",
            "Epoch 6/500\n",
            "30/30 [==============================] - 0s 839us/step - loss: 1631.2979 - mse: 4511977.5000 - mae: 1631.2977 - val_loss: 2634.3582 - val_mse: 10787977.0000 - val_mae: 2634.3584\n",
            "Epoch 7/500\n",
            "30/30 [==============================] - 0s 894us/step - loss: 1626.2846 - mse: 4504207.5000 - mae: 1626.2847 - val_loss: 2629.5096 - val_mse: 10774358.0000 - val_mae: 2629.5095\n",
            "Epoch 8/500\n",
            "30/30 [==============================] - 0s 780us/step - loss: 1621.7020 - mse: 4499109.0000 - mae: 1621.7020 - val_loss: 2624.3938 - val_mse: 10759833.0000 - val_mae: 2624.3938\n",
            "Epoch 9/500\n",
            "30/30 [==============================] - 0s 902us/step - loss: 1617.5917 - mse: 4489932.5000 - mae: 1617.5917 - val_loss: 2619.2322 - val_mse: 10744831.0000 - val_mae: 2619.2322\n",
            "Epoch 10/500\n",
            "30/30 [==============================] - 0s 923us/step - loss: 1612.2710 - mse: 4485250.5000 - mae: 1612.2710 - val_loss: 2613.5579 - val_mse: 10728173.0000 - val_mae: 2613.5579\n",
            "Epoch 11/500\n",
            "30/30 [==============================] - 0s 919us/step - loss: 1604.1662 - mse: 4468704.0000 - mae: 1604.1661 - val_loss: 2607.2076 - val_mse: 10708643.0000 - val_mae: 2607.2075\n",
            "Epoch 12/500\n",
            "30/30 [==============================] - 0s 840us/step - loss: 1602.5138 - mse: 4464051.0000 - mae: 1602.5138 - val_loss: 2601.2323 - val_mse: 10689401.0000 - val_mae: 2601.2322\n",
            "Epoch 13/500\n",
            "30/30 [==============================] - 0s 840us/step - loss: 1594.5204 - mse: 4445807.0000 - mae: 1594.5203 - val_loss: 2594.2285 - val_mse: 10665546.0000 - val_mae: 2594.2283\n",
            "Epoch 14/500\n",
            "30/30 [==============================] - 0s 873us/step - loss: 1588.6541 - mse: 4442301.5000 - mae: 1588.6542 - val_loss: 2586.8690 - val_mse: 10640812.0000 - val_mae: 2586.8689\n",
            "Epoch 15/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1575.7364 - mse: 4421678.5000 - mae: 1575.7363 - val_loss: 2579.0371 - val_mse: 10614021.0000 - val_mae: 2579.0369\n",
            "Epoch 16/500\n",
            "30/30 [==============================] - 0s 920us/step - loss: 1567.9040 - mse: 4394750.5000 - mae: 1567.9039 - val_loss: 2570.4827 - val_mse: 10583137.0000 - val_mae: 2570.4829\n",
            "Epoch 17/500\n",
            "30/30 [==============================] - 0s 918us/step - loss: 1558.5344 - mse: 4377853.5000 - mae: 1558.5345 - val_loss: 2561.3485 - val_mse: 10550339.0000 - val_mae: 2561.3484\n",
            "Epoch 18/500\n",
            "30/30 [==============================] - 0s 962us/step - loss: 1559.4058 - mse: 4379637.5000 - mae: 1559.4059 - val_loss: 2553.2231 - val_mse: 10520193.0000 - val_mae: 2553.2231\n",
            "Epoch 19/500\n",
            "30/30 [==============================] - 0s 831us/step - loss: 1541.4305 - mse: 4332025.5000 - mae: 1541.4305 - val_loss: 2544.2229 - val_mse: 10487029.0000 - val_mae: 2544.2229\n",
            "Epoch 20/500\n",
            "30/30 [==============================] - 0s 673us/step - loss: 1535.2535 - mse: 4324668.5000 - mae: 1535.2535 - val_loss: 2535.2213 - val_mse: 10454183.0000 - val_mae: 2535.2214\n",
            "Epoch 21/500\n",
            "30/30 [==============================] - 0s 851us/step - loss: 1534.9862 - mse: 4297462.0000 - mae: 1534.9862 - val_loss: 2526.2326 - val_mse: 10419761.0000 - val_mae: 2526.2327\n",
            "Epoch 22/500\n",
            "30/30 [==============================] - 0s 794us/step - loss: 1529.8062 - mse: 4272798.0000 - mae: 1529.8063 - val_loss: 2518.3634 - val_mse: 10388866.0000 - val_mae: 2518.3635\n",
            "Epoch 23/500\n",
            "30/30 [==============================] - 0s 828us/step - loss: 1526.3897 - mse: 4259955.5000 - mae: 1526.3899 - val_loss: 2510.9944 - val_mse: 10359525.0000 - val_mae: 2510.9944\n",
            "Epoch 24/500\n",
            "30/30 [==============================] - 0s 809us/step - loss: 1528.9226 - mse: 4264040.5000 - mae: 1528.9226 - val_loss: 2503.1902 - val_mse: 10328143.0000 - val_mae: 2503.1902\n",
            "Epoch 25/500\n",
            "30/30 [==============================] - 0s 800us/step - loss: 1507.0882 - mse: 4221705.5000 - mae: 1507.0883 - val_loss: 2496.0755 - val_mse: 10299232.0000 - val_mae: 2496.0754\n",
            "Epoch 26/500\n",
            "30/30 [==============================] - 0s 772us/step - loss: 1524.8309 - mse: 4217026.5000 - mae: 1524.8309 - val_loss: 2487.5606 - val_mse: 10265205.0000 - val_mae: 2487.5608\n",
            "Epoch 27/500\n",
            "30/30 [==============================] - 0s 885us/step - loss: 1500.1300 - mse: 4153211.7500 - mae: 1500.1301 - val_loss: 2479.8413 - val_mse: 10233664.0000 - val_mae: 2479.8413\n",
            "Epoch 28/500\n",
            "30/30 [==============================] - 0s 782us/step - loss: 1491.4302 - mse: 4150379.5000 - mae: 1491.4302 - val_loss: 2469.7008 - val_mse: 10194345.0000 - val_mae: 2469.7007\n",
            "Epoch 29/500\n",
            "30/30 [==============================] - 0s 854us/step - loss: 1495.3866 - mse: 4158891.2500 - mae: 1495.3866 - val_loss: 2460.2304 - val_mse: 10157727.0000 - val_mae: 2460.2302\n",
            "Epoch 30/500\n",
            "30/30 [==============================] - 0s 771us/step - loss: 1473.2476 - mse: 4068579.7500 - mae: 1473.2477 - val_loss: 2452.2200 - val_mse: 10124243.0000 - val_mae: 2452.2197\n",
            "Epoch 31/500\n",
            "30/30 [==============================] - 0s 744us/step - loss: 1485.8562 - mse: 4105318.5000 - mae: 1485.8562 - val_loss: 2441.9585 - val_mse: 10084079.0000 - val_mae: 2441.9583\n",
            "Epoch 32/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1474.2149 - mse: 4071351.7500 - mae: 1474.2148 - val_loss: 2433.8082 - val_mse: 10051034.0000 - val_mae: 2433.8083\n",
            "Epoch 33/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1482.9680 - mse: 4033977.0000 - mae: 1482.9680 - val_loss: 2424.7540 - val_mse: 10014532.0000 - val_mae: 2424.7539\n",
            "Epoch 34/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1465.3741 - mse: 4002043.2500 - mae: 1465.3743 - val_loss: 2416.3617 - val_mse: 9979834.0000 - val_mae: 2416.3618\n",
            "Epoch 35/500\n",
            "30/30 [==============================] - 0s 903us/step - loss: 1459.1766 - mse: 3953630.5000 - mae: 1459.1765 - val_loss: 2408.2525 - val_mse: 9946450.0000 - val_mae: 2408.2527\n",
            "Epoch 36/500\n",
            "30/30 [==============================] - 0s 902us/step - loss: 1452.6263 - mse: 3971344.0000 - mae: 1452.6263 - val_loss: 2398.9717 - val_mse: 9909441.0000 - val_mae: 2398.9719\n",
            "Epoch 37/500\n",
            "30/30 [==============================] - 0s 891us/step - loss: 1437.3806 - mse: 3919715.7500 - mae: 1437.3806 - val_loss: 2390.1209 - val_mse: 9873702.0000 - val_mae: 2390.1208\n",
            "Epoch 38/500\n",
            "30/30 [==============================] - 0s 789us/step - loss: 1465.9727 - mse: 3952824.0000 - mae: 1465.9727 - val_loss: 2387.7287 - val_mse: 9860666.0000 - val_mae: 2387.7288\n",
            "Epoch 39/500\n",
            "30/30 [==============================] - 0s 823us/step - loss: 1459.9867 - mse: 3915772.7500 - mae: 1459.9867 - val_loss: 2378.9849 - val_mse: 9825570.0000 - val_mae: 2378.9849\n",
            "Epoch 40/500\n",
            "30/30 [==============================] - 0s 836us/step - loss: 1423.0829 - mse: 3874474.7500 - mae: 1423.0828 - val_loss: 2371.4762 - val_mse: 9794630.0000 - val_mae: 2371.4761\n",
            "Epoch 41/500\n",
            "30/30 [==============================] - 0s 838us/step - loss: 1431.0196 - mse: 3825784.0000 - mae: 1431.0195 - val_loss: 2361.1341 - val_mse: 9752564.0000 - val_mae: 2361.1340\n",
            "Epoch 42/500\n",
            "30/30 [==============================] - 0s 952us/step - loss: 1418.8222 - mse: 3807795.7500 - mae: 1418.8223 - val_loss: 2351.7920 - val_mse: 9714924.0000 - val_mae: 2351.7920\n",
            "Epoch 43/500\n",
            "30/30 [==============================] - 0s 896us/step - loss: 1405.1912 - mse: 3758827.7500 - mae: 1405.1912 - val_loss: 2340.8933 - val_mse: 9672308.0000 - val_mae: 2340.8933\n",
            "Epoch 44/500\n",
            "30/30 [==============================] - 0s 907us/step - loss: 1425.0162 - mse: 3786924.2500 - mae: 1425.0161 - val_loss: 2333.6148 - val_mse: 9642145.0000 - val_mae: 2333.6147\n",
            "Epoch 45/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1412.3805 - mse: 3685939.5000 - mae: 1412.3805 - val_loss: 2323.3110 - val_mse: 9600511.0000 - val_mae: 2323.3110\n",
            "Epoch 46/500\n",
            "30/30 [==============================] - 0s 976us/step - loss: 1434.4926 - mse: 3761188.2500 - mae: 1434.4927 - val_loss: 2314.6974 - val_mse: 9565149.0000 - val_mae: 2314.6975\n",
            "Epoch 47/500\n",
            "30/30 [==============================] - 0s 945us/step - loss: 1422.1118 - mse: 3716197.0000 - mae: 1422.1117 - val_loss: 2305.5126 - val_mse: 9527643.0000 - val_mae: 2305.5125\n",
            "Epoch 48/500\n",
            "30/30 [==============================] - 0s 877us/step - loss: 1397.3046 - mse: 3683565.0000 - mae: 1397.3047 - val_loss: 2293.0865 - val_mse: 9479402.0000 - val_mae: 2293.0864\n",
            "Epoch 49/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1429.7088 - mse: 3763774.7500 - mae: 1429.7089 - val_loss: 2285.9523 - val_mse: 9449947.0000 - val_mae: 2285.9521\n",
            "Epoch 50/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1394.8297 - mse: 3628022.5000 - mae: 1394.8297 - val_loss: 2276.6507 - val_mse: 9411601.0000 - val_mae: 2276.6506\n",
            "Epoch 51/500\n",
            "30/30 [==============================] - 0s 936us/step - loss: 1417.3884 - mse: 3650012.0000 - mae: 1417.3883 - val_loss: 2270.5535 - val_mse: 9384201.0000 - val_mae: 2270.5537\n",
            "Epoch 52/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1405.9471 - mse: 3621988.5000 - mae: 1405.9471 - val_loss: 2261.5822 - val_mse: 9346420.0000 - val_mae: 2261.5820\n",
            "Epoch 53/500\n",
            "30/30 [==============================] - 0s 846us/step - loss: 1400.8986 - mse: 3612184.0000 - mae: 1400.8987 - val_loss: 2254.6627 - val_mse: 9315510.0000 - val_mae: 2254.6628\n",
            "Epoch 54/500\n",
            "30/30 [==============================] - 0s 815us/step - loss: 1354.3822 - mse: 3470669.7500 - mae: 1354.3822 - val_loss: 2243.3809 - val_mse: 9265749.0000 - val_mae: 2243.3811\n",
            "Epoch 55/500\n",
            "30/30 [==============================] - 0s 964us/step - loss: 1378.1576 - mse: 3548715.2500 - mae: 1378.1576 - val_loss: 2234.0087 - val_mse: 9224265.0000 - val_mae: 2234.0088\n",
            "Epoch 56/500\n",
            "30/30 [==============================] - 0s 913us/step - loss: 1367.4302 - mse: 3449479.0000 - mae: 1367.4302 - val_loss: 2223.6559 - val_mse: 9178747.0000 - val_mae: 2223.6558\n",
            "Epoch 57/500\n",
            "30/30 [==============================] - 0s 904us/step - loss: 1352.4401 - mse: 3413801.5000 - mae: 1352.4401 - val_loss: 2214.2594 - val_mse: 9137161.0000 - val_mae: 2214.2593\n",
            "Epoch 58/500\n",
            "30/30 [==============================] - 0s 941us/step - loss: 1340.8051 - mse: 3342827.0000 - mae: 1340.8052 - val_loss: 2204.8328 - val_mse: 9092134.0000 - val_mae: 2204.8328\n",
            "Epoch 59/500\n",
            "30/30 [==============================] - 0s 902us/step - loss: 1347.2883 - mse: 3357024.5000 - mae: 1347.2883 - val_loss: 2194.4720 - val_mse: 9043282.0000 - val_mae: 2194.4722\n",
            "Epoch 60/500\n",
            "30/30 [==============================] - 0s 777us/step - loss: 1388.0906 - mse: 3527704.5000 - mae: 1388.0906 - val_loss: 2185.4031 - val_mse: 9000998.0000 - val_mae: 2185.4031\n",
            "Epoch 61/500\n",
            "30/30 [==============================] - 0s 866us/step - loss: 1368.9305 - mse: 3406195.5000 - mae: 1368.9304 - val_loss: 2177.6418 - val_mse: 8963234.0000 - val_mae: 2177.6418\n",
            "Epoch 62/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1340.4227 - mse: 3301366.5000 - mae: 1340.4226 - val_loss: 2167.3013 - val_mse: 8915601.0000 - val_mae: 2167.3013\n",
            "Epoch 63/500\n",
            "30/30 [==============================] - 0s 891us/step - loss: 1359.0914 - mse: 3341013.0000 - mae: 1359.0914 - val_loss: 2159.3357 - val_mse: 8877446.0000 - val_mae: 2159.3357\n",
            "Epoch 64/500\n",
            "30/30 [==============================] - 0s 779us/step - loss: 1346.1455 - mse: 3300373.2500 - mae: 1346.1454 - val_loss: 2149.1124 - val_mse: 8830255.0000 - val_mae: 2149.1123\n",
            "Epoch 65/500\n",
            "30/30 [==============================] - 0s 831us/step - loss: 1327.2968 - mse: 3219094.7500 - mae: 1327.2969 - val_loss: 2139.5948 - val_mse: 8786012.0000 - val_mae: 2139.5950\n",
            "Epoch 66/500\n",
            "30/30 [==============================] - 0s 947us/step - loss: 1313.1024 - mse: 3240809.0000 - mae: 1313.1023 - val_loss: 2130.3003 - val_mse: 8740045.0000 - val_mae: 2130.3000\n",
            "Epoch 67/500\n",
            "30/30 [==============================] - 0s 824us/step - loss: 1342.2333 - mse: 3164393.5000 - mae: 1342.2333 - val_loss: 2122.0984 - val_mse: 8697563.0000 - val_mae: 2122.0984\n",
            "Epoch 68/500\n",
            "30/30 [==============================] - 0s 922us/step - loss: 1298.4192 - mse: 3132279.2500 - mae: 1298.4193 - val_loss: 2112.3691 - val_mse: 8649343.0000 - val_mae: 2112.3689\n",
            "Epoch 69/500\n",
            "30/30 [==============================] - 0s 857us/step - loss: 1335.7535 - mse: 3233231.0000 - mae: 1335.7534 - val_loss: 2105.0402 - val_mse: 8609151.0000 - val_mae: 2105.0400\n",
            "Epoch 70/500\n",
            "30/30 [==============================] - 0s 872us/step - loss: 1355.7531 - mse: 3116089.0000 - mae: 1355.7532 - val_loss: 2098.2644 - val_mse: 8571163.0000 - val_mae: 2098.2644\n",
            "Epoch 71/500\n",
            "30/30 [==============================] - 0s 875us/step - loss: 1281.8985 - mse: 3000020.2500 - mae: 1281.8984 - val_loss: 2088.6605 - val_mse: 8519961.0000 - val_mae: 2088.6604\n",
            "Epoch 72/500\n",
            "30/30 [==============================] - 0s 865us/step - loss: 1284.1237 - mse: 3072726.2500 - mae: 1284.1237 - val_loss: 2079.6410 - val_mse: 8471132.0000 - val_mae: 2079.6409\n",
            "Epoch 73/500\n",
            "30/30 [==============================] - 0s 779us/step - loss: 1276.1816 - mse: 2994079.7500 - mae: 1276.1815 - val_loss: 2069.9263 - val_mse: 8415394.0000 - val_mae: 2069.9265\n",
            "Epoch 74/500\n",
            "30/30 [==============================] - 0s 865us/step - loss: 1281.9346 - mse: 2986395.0000 - mae: 1281.9347 - val_loss: 2061.5011 - val_mse: 8366674.5000 - val_mae: 2061.5012\n",
            "Epoch 75/500\n",
            "30/30 [==============================] - 0s 917us/step - loss: 1301.7475 - mse: 3152958.7500 - mae: 1301.7476 - val_loss: 2054.0546 - val_mse: 8324434.0000 - val_mae: 2054.0547\n",
            "Epoch 76/500\n",
            "30/30 [==============================] - 0s 881us/step - loss: 1297.4750 - mse: 3039086.7500 - mae: 1297.4750 - val_loss: 2046.5151 - val_mse: 8281470.5000 - val_mae: 2046.5151\n",
            "Epoch 77/500\n",
            "30/30 [==============================] - 0s 966us/step - loss: 1311.7060 - mse: 3026400.0000 - mae: 1311.7059 - val_loss: 2039.9370 - val_mse: 8240401.0000 - val_mae: 2039.9370\n",
            "Epoch 78/500\n",
            "30/30 [==============================] - 0s 870us/step - loss: 1303.6161 - mse: 2978443.5000 - mae: 1303.6161 - val_loss: 2032.9564 - val_mse: 8195517.0000 - val_mae: 2032.9564\n",
            "Epoch 79/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1285.2468 - mse: 2939953.5000 - mae: 1285.2468 - val_loss: 2028.1169 - val_mse: 8163968.5000 - val_mae: 2028.1169\n",
            "Epoch 80/500\n",
            "30/30 [==============================] - 0s 824us/step - loss: 1258.3939 - mse: 2833671.0000 - mae: 1258.3940 - val_loss: 2019.9664 - val_mse: 8112835.0000 - val_mae: 2019.9664\n",
            "Epoch 81/500\n",
            "30/30 [==============================] - 0s 995us/step - loss: 1281.0973 - mse: 3010467.2500 - mae: 1281.0973 - val_loss: 2012.1661 - val_mse: 8064678.0000 - val_mae: 2012.1661\n",
            "Epoch 82/500\n",
            "30/30 [==============================] - 0s 975us/step - loss: 1324.2391 - mse: 3112112.7500 - mae: 1324.2390 - val_loss: 2007.0946 - val_mse: 8029466.0000 - val_mae: 2007.0946\n",
            "Epoch 83/500\n",
            "30/30 [==============================] - 0s 902us/step - loss: 1270.0777 - mse: 2947170.7500 - mae: 1270.0778 - val_loss: 2000.6678 - val_mse: 7984647.5000 - val_mae: 2000.6678\n",
            "Epoch 84/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1307.2258 - mse: 2887861.2500 - mae: 1307.2258 - val_loss: 1995.7388 - val_mse: 7946116.5000 - val_mae: 1995.7388\n",
            "Epoch 85/500\n",
            "30/30 [==============================] - 0s 827us/step - loss: 1271.8084 - mse: 2884535.0000 - mae: 1271.8083 - val_loss: 1994.9325 - val_mse: 7938488.5000 - val_mae: 1994.9324\n",
            "Epoch 86/500\n",
            "30/30 [==============================] - 0s 786us/step - loss: 1292.0810 - mse: 2845177.2500 - mae: 1292.0809 - val_loss: 1990.0756 - val_mse: 7899509.5000 - val_mae: 1990.0756\n",
            "Epoch 87/500\n",
            "30/30 [==============================] - 0s 878us/step - loss: 1258.9264 - mse: 2792542.5000 - mae: 1258.9263 - val_loss: 1984.3484 - val_mse: 7853988.5000 - val_mae: 1984.3484\n",
            "Epoch 88/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1288.4584 - mse: 2768455.5000 - mae: 1288.4584 - val_loss: 1979.3360 - val_mse: 7814160.0000 - val_mae: 1979.3359\n",
            "Epoch 89/500\n",
            "30/30 [==============================] - 0s 849us/step - loss: 1223.4638 - mse: 2675465.5000 - mae: 1223.4637 - val_loss: 1974.5527 - val_mse: 7776437.5000 - val_mae: 1974.5526\n",
            "Epoch 90/500\n",
            "30/30 [==============================] - 0s 887us/step - loss: 1277.4806 - mse: 2753983.0000 - mae: 1277.4805 - val_loss: 1970.8361 - val_mse: 7746720.5000 - val_mae: 1970.8359\n",
            "Epoch 91/500\n",
            "30/30 [==============================] - 0s 798us/step - loss: 1263.1005 - mse: 2749850.7500 - mae: 1263.1005 - val_loss: 1966.1745 - val_mse: 7704903.5000 - val_mae: 1966.1746\n",
            "Epoch 92/500\n",
            "30/30 [==============================] - 0s 971us/step - loss: 1292.0510 - mse: 2821466.2500 - mae: 1292.0510 - val_loss: 1962.1360 - val_mse: 7667847.5000 - val_mae: 1962.1361\n",
            "Epoch 93/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1279.8191 - mse: 2800967.5000 - mae: 1279.8190 - val_loss: 1960.6173 - val_mse: 7651839.5000 - val_mae: 1960.6173\n",
            "Epoch 94/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1272.2125 - mse: 2620221.5000 - mae: 1272.2125 - val_loss: 1957.7380 - val_mse: 7620486.5000 - val_mae: 1957.7380\n",
            "Epoch 95/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1192.5531 - mse: 2557812.2500 - mae: 1192.5531 - val_loss: 1954.1135 - val_mse: 7580682.0000 - val_mae: 1954.1134\n",
            "Epoch 96/500\n",
            "30/30 [==============================] - 0s 942us/step - loss: 1298.5216 - mse: 2796736.7500 - mae: 1298.5215 - val_loss: 1951.9833 - val_mse: 7557793.5000 - val_mae: 1951.9832\n",
            "Epoch 97/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1209.7146 - mse: 2525372.7500 - mae: 1209.7146 - val_loss: 1949.5240 - val_mse: 7531492.5000 - val_mae: 1949.5239\n",
            "Epoch 98/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1160.2714 - mse: 2473384.0000 - mae: 1160.2714 - val_loss: 1946.3635 - val_mse: 7497406.0000 - val_mae: 1946.3635\n",
            "Epoch 99/500\n",
            "30/30 [==============================] - 0s 929us/step - loss: 1254.3007 - mse: 2737084.5000 - mae: 1254.3008 - val_loss: 1942.7529 - val_mse: 7458055.0000 - val_mae: 1942.7528\n",
            "Epoch 100/500\n",
            "30/30 [==============================] - 0s 800us/step - loss: 1209.6620 - mse: 2501769.0000 - mae: 1209.6620 - val_loss: 1940.2728 - val_mse: 7432183.5000 - val_mae: 1940.2728\n",
            "Epoch 101/500\n",
            "30/30 [==============================] - 0s 889us/step - loss: 1196.3008 - mse: 2602213.0000 - mae: 1196.3008 - val_loss: 1938.6563 - val_mse: 7415736.0000 - val_mae: 1938.6564\n",
            "Epoch 102/500\n",
            "30/30 [==============================] - 0s 870us/step - loss: 1173.9384 - mse: 2559097.2500 - mae: 1173.9382 - val_loss: 1933.9777 - val_mse: 7365343.5000 - val_mae: 1933.9777\n",
            "Epoch 103/500\n",
            "30/30 [==============================] - 0s 919us/step - loss: 1201.4885 - mse: 2432780.5000 - mae: 1201.4884 - val_loss: 1932.9122 - val_mse: 7355106.5000 - val_mae: 1932.9122\n",
            "Epoch 104/500\n",
            "30/30 [==============================] - 0s 967us/step - loss: 1208.5206 - mse: 2521032.2500 - mae: 1208.5206 - val_loss: 1930.3170 - val_mse: 7328303.5000 - val_mae: 1930.3170\n",
            "Epoch 105/500\n",
            "30/30 [==============================] - 0s 834us/step - loss: 1232.5436 - mse: 2530578.7500 - mae: 1232.5435 - val_loss: 1927.3371 - val_mse: 7297122.5000 - val_mae: 1927.3373\n",
            "Epoch 106/500\n",
            "30/30 [==============================] - 0s 912us/step - loss: 1157.9007 - mse: 2409053.5000 - mae: 1157.9008 - val_loss: 1925.3122 - val_mse: 7276627.0000 - val_mae: 1925.3123\n",
            "Epoch 107/500\n",
            "30/30 [==============================] - 0s 859us/step - loss: 1173.6056 - mse: 2318390.7500 - mae: 1173.6055 - val_loss: 1923.3154 - val_mse: 7256851.5000 - val_mae: 1923.3156\n",
            "Epoch 108/500\n",
            "30/30 [==============================] - 0s 886us/step - loss: 1206.5167 - mse: 2535941.5000 - mae: 1206.5167 - val_loss: 1923.0031 - val_mse: 7254734.0000 - val_mae: 1923.0032\n",
            "Epoch 109/500\n",
            "30/30 [==============================] - 0s 840us/step - loss: 1204.5567 - mse: 2376229.7500 - mae: 1204.5568 - val_loss: 1919.4173 - val_mse: 7217756.5000 - val_mae: 1919.4174\n",
            "Epoch 110/500\n",
            "30/30 [==============================] - 0s 997us/step - loss: 1198.1484 - mse: 2541348.7500 - mae: 1198.1484 - val_loss: 1915.0870 - val_mse: 7172793.0000 - val_mae: 1915.0870\n",
            "Epoch 111/500\n",
            "30/30 [==============================] - 0s 854us/step - loss: 1246.0942 - mse: 2609173.2500 - mae: 1246.0941 - val_loss: 1915.0288 - val_mse: 7173387.0000 - val_mae: 1915.0288\n",
            "Epoch 112/500\n",
            "30/30 [==============================] - 0s 938us/step - loss: 1218.8674 - mse: 2598820.5000 - mae: 1218.8674 - val_loss: 1914.0706 - val_mse: 7164203.0000 - val_mae: 1914.0707\n",
            "Epoch 113/500\n",
            "30/30 [==============================] - 0s 915us/step - loss: 1120.0061 - mse: 2146885.2500 - mae: 1120.0060 - val_loss: 1911.6862 - val_mse: 7140337.5000 - val_mae: 1911.6862\n",
            "Epoch 114/500\n",
            "30/30 [==============================] - 0s 945us/step - loss: 1192.2514 - mse: 2420803.7500 - mae: 1192.2513 - val_loss: 1909.1827 - val_mse: 7115269.0000 - val_mae: 1909.1829\n",
            "Epoch 115/500\n",
            "30/30 [==============================] - 0s 806us/step - loss: 1162.8273 - mse: 2340780.5000 - mae: 1162.8274 - val_loss: 1908.3303 - val_mse: 7107499.5000 - val_mae: 1908.3303\n",
            "Epoch 116/500\n",
            "30/30 [==============================] - 0s 818us/step - loss: 1209.0794 - mse: 2444352.0000 - mae: 1209.0795 - val_loss: 1905.2188 - val_mse: 7076289.0000 - val_mae: 1905.2189\n",
            "Epoch 117/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1133.6038 - mse: 2141487.5000 - mae: 1133.6038 - val_loss: 1902.1229 - val_mse: 7045788.5000 - val_mae: 1902.1229\n",
            "Epoch 118/500\n",
            "30/30 [==============================] - 0s 928us/step - loss: 1264.6589 - mse: 2545535.0000 - mae: 1264.6589 - val_loss: 1900.7215 - val_mse: 7032568.5000 - val_mae: 1900.7214\n",
            "Epoch 119/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1186.5542 - mse: 2338921.5000 - mae: 1186.5542 - val_loss: 1900.1560 - val_mse: 7028015.5000 - val_mae: 1900.1560\n",
            "Epoch 120/500\n",
            "30/30 [==============================] - 0s 857us/step - loss: 1233.4298 - mse: 2412729.5000 - mae: 1233.4298 - val_loss: 1901.1139 - val_mse: 7038962.0000 - val_mae: 1901.1138\n",
            "Epoch 121/500\n",
            "30/30 [==============================] - 0s 774us/step - loss: 1255.7207 - mse: 2533167.5000 - mae: 1255.7207 - val_loss: 1901.3269 - val_mse: 7042288.0000 - val_mae: 1901.3269\n",
            "Epoch 122/500\n",
            "30/30 [==============================] - 0s 905us/step - loss: 1132.7795 - mse: 2162615.7500 - mae: 1132.7794 - val_loss: 1898.9350 - val_mse: 7019159.0000 - val_mae: 1898.9349\n",
            "Epoch 123/500\n",
            "30/30 [==============================] - 0s 882us/step - loss: 1222.5219 - mse: 2515109.7500 - mae: 1222.5219 - val_loss: 1898.7717 - val_mse: 7018791.0000 - val_mae: 1898.7717\n",
            "Epoch 124/500\n",
            "30/30 [==============================] - 0s 839us/step - loss: 1100.6082 - mse: 2167679.7500 - mae: 1100.6082 - val_loss: 1896.9634 - val_mse: 7001834.0000 - val_mae: 1896.9634\n",
            "Epoch 125/500\n",
            "30/30 [==============================] - 0s 839us/step - loss: 1231.6462 - mse: 2550153.0000 - mae: 1231.6462 - val_loss: 1895.8314 - val_mse: 6991651.0000 - val_mae: 1895.8314\n",
            "Epoch 126/500\n",
            "30/30 [==============================] - 0s 909us/step - loss: 1110.6312 - mse: 2325728.5000 - mae: 1110.6312 - val_loss: 1891.9280 - val_mse: 6953219.0000 - val_mae: 1891.9280\n",
            "Epoch 127/500\n",
            "30/30 [==============================] - 0s 868us/step - loss: 1237.9806 - mse: 2465549.0000 - mae: 1237.9806 - val_loss: 1891.8252 - val_mse: 6953544.0000 - val_mae: 1891.8253\n",
            "Epoch 128/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1209.7973 - mse: 2540073.5000 - mae: 1209.7972 - val_loss: 1892.3384 - val_mse: 6960094.5000 - val_mae: 1892.3384\n",
            "Epoch 129/500\n",
            "30/30 [==============================] - 0s 871us/step - loss: 1195.4432 - mse: 2423613.5000 - mae: 1195.4432 - val_loss: 1889.2142 - val_mse: 6929801.0000 - val_mae: 1889.2142\n",
            "Epoch 130/500\n",
            "30/30 [==============================] - 0s 970us/step - loss: 1148.1378 - mse: 2290360.5000 - mae: 1148.1378 - val_loss: 1886.9452 - val_mse: 6908676.5000 - val_mae: 1886.9453\n",
            "Epoch 131/500\n",
            "30/30 [==============================] - 0s 989us/step - loss: 1167.1630 - mse: 2298169.0000 - mae: 1167.1630 - val_loss: 1884.8305 - val_mse: 6889169.0000 - val_mae: 1884.8304\n",
            "Epoch 132/500\n",
            "30/30 [==============================] - 0s 803us/step - loss: 1203.8408 - mse: 2410282.7500 - mae: 1203.8409 - val_loss: 1884.4981 - val_mse: 6887189.0000 - val_mae: 1884.4982\n",
            "Epoch 133/500\n",
            "30/30 [==============================] - 0s 852us/step - loss: 1179.0992 - mse: 2245827.7500 - mae: 1179.0992 - val_loss: 1882.5897 - val_mse: 6869769.0000 - val_mae: 1882.5897\n",
            "Epoch 134/500\n",
            "30/30 [==============================] - 0s 932us/step - loss: 1092.0060 - mse: 2129794.2500 - mae: 1092.0060 - val_loss: 1880.8635 - val_mse: 6854158.5000 - val_mae: 1880.8635\n",
            "Epoch 135/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1303.8437 - mse: 2704816.5000 - mae: 1303.8438 - val_loss: 1880.6091 - val_mse: 6852808.5000 - val_mae: 1880.6091\n",
            "Epoch 136/500\n",
            "30/30 [==============================] - 0s 956us/step - loss: 1260.2916 - mse: 2610570.5000 - mae: 1260.2916 - val_loss: 1879.1043 - val_mse: 6839290.0000 - val_mae: 1879.1041\n",
            "Epoch 137/500\n",
            "30/30 [==============================] - 0s 855us/step - loss: 1117.7264 - mse: 2060853.1250 - mae: 1117.7263 - val_loss: 1876.3110 - val_mse: 6813656.0000 - val_mae: 1876.3109\n",
            "Epoch 138/500\n",
            "30/30 [==============================] - 0s 791us/step - loss: 1204.7288 - mse: 2314368.5000 - mae: 1204.7288 - val_loss: 1874.9602 - val_mse: 6801787.0000 - val_mae: 1874.9602\n",
            "Epoch 139/500\n",
            "30/30 [==============================] - 0s 776us/step - loss: 1233.0512 - mse: 2461456.7500 - mae: 1233.0513 - val_loss: 1873.8754 - val_mse: 6792568.5000 - val_mae: 1873.8755\n",
            "Epoch 140/500\n",
            "30/30 [==============================] - 0s 814us/step - loss: 1182.7319 - mse: 2284318.2500 - mae: 1182.7319 - val_loss: 1872.0372 - val_mse: 6776337.5000 - val_mae: 1872.0372\n",
            "Epoch 141/500\n",
            "30/30 [==============================] - 0s 896us/step - loss: 1176.9251 - mse: 2194189.0000 - mae: 1176.9252 - val_loss: 1869.3036 - val_mse: 6751584.5000 - val_mae: 1869.3035\n",
            "Epoch 142/500\n",
            "30/30 [==============================] - 0s 887us/step - loss: 1091.1135 - mse: 1980631.7500 - mae: 1091.1135 - val_loss: 1866.4017 - val_mse: 6725690.0000 - val_mae: 1866.4017\n",
            "Epoch 143/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1169.4557 - mse: 2212798.2500 - mae: 1169.4556 - val_loss: 1864.4920 - val_mse: 6709147.5000 - val_mae: 1864.4919\n",
            "Epoch 144/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1192.9728 - mse: 2337265.0000 - mae: 1192.9728 - val_loss: 1860.8671 - val_mse: 6676145.5000 - val_mae: 1860.8672\n",
            "Epoch 145/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1055.3579 - mse: 1835487.1250 - mae: 1055.3579 - val_loss: 1857.9653 - val_mse: 6650475.5000 - val_mae: 1857.9653\n",
            "Epoch 146/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1065.1681 - mse: 1973856.2500 - mae: 1065.1681 - val_loss: 1858.5535 - val_mse: 6657377.0000 - val_mae: 1858.5535\n",
            "Epoch 147/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1139.9627 - mse: 2225677.5000 - mae: 1139.9628 - val_loss: 1857.7667 - val_mse: 6650973.5000 - val_mae: 1857.7667\n",
            "Epoch 148/500\n",
            "30/30 [==============================] - 0s 975us/step - loss: 1211.9305 - mse: 2219959.5000 - mae: 1211.9304 - val_loss: 1857.0355 - val_mse: 6645470.0000 - val_mae: 1857.0354\n",
            "Epoch 149/500\n",
            "30/30 [==============================] - 0s 977us/step - loss: 1266.9939 - mse: 2592487.0000 - mae: 1266.9939 - val_loss: 1857.0253 - val_mse: 6646489.5000 - val_mae: 1857.0253\n",
            "Epoch 150/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1176.8014 - mse: 2194765.5000 - mae: 1176.8013 - val_loss: 1856.9196 - val_mse: 6646695.0000 - val_mae: 1856.9197\n",
            "Epoch 151/500\n",
            "30/30 [==============================] - 0s 930us/step - loss: 1167.0235 - mse: 2386559.0000 - mae: 1167.0236 - val_loss: 1856.0346 - val_mse: 6639321.0000 - val_mae: 1856.0347\n",
            "Epoch 152/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1179.7965 - mse: 2189632.5000 - mae: 1179.7966 - val_loss: 1856.3254 - val_mse: 6642997.5000 - val_mae: 1856.3254\n",
            "Epoch 153/500\n",
            "30/30 [==============================] - 0s 943us/step - loss: 1205.3488 - mse: 2297551.5000 - mae: 1205.3489 - val_loss: 1857.5258 - val_mse: 6655478.0000 - val_mae: 1857.5258\n",
            "Epoch 154/500\n",
            "30/30 [==============================] - 0s 975us/step - loss: 1248.7479 - mse: 2351750.2500 - mae: 1248.7479 - val_loss: 1858.4772 - val_mse: 6665555.5000 - val_mae: 1858.4772\n",
            "Epoch 155/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1194.3637 - mse: 2355938.7500 - mae: 1194.3638 - val_loss: 1857.7615 - val_mse: 6659510.5000 - val_mae: 1857.7616\n",
            "Epoch 156/500\n",
            "30/30 [==============================] - 0s 925us/step - loss: 1218.9663 - mse: 2409877.0000 - mae: 1218.9664 - val_loss: 1856.7137 - val_mse: 6650332.5000 - val_mae: 1856.7137\n",
            "Epoch 157/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1183.6793 - mse: 2265214.5000 - mae: 1183.6792 - val_loss: 1857.0692 - val_mse: 6654541.0000 - val_mae: 1857.0692\n",
            "Epoch 158/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1176.6837 - mse: 2165867.5000 - mae: 1176.6836 - val_loss: 1856.5060 - val_mse: 6649962.0000 - val_mae: 1856.5060\n",
            "Epoch 159/500\n",
            "30/30 [==============================] - 0s 959us/step - loss: 1218.9781 - mse: 2446276.0000 - mae: 1218.9781 - val_loss: 1854.6704 - val_mse: 6633058.5000 - val_mae: 1854.6704\n",
            "Epoch 160/500\n",
            "30/30 [==============================] - 0s 945us/step - loss: 1190.2511 - mse: 2181505.7500 - mae: 1190.2511 - val_loss: 1852.5376 - val_mse: 6613611.5000 - val_mae: 1852.5376\n",
            "Epoch 161/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1143.8856 - mse: 2216535.0000 - mae: 1143.8856 - val_loss: 1852.5553 - val_mse: 6614571.5000 - val_mae: 1852.5553\n",
            "Epoch 162/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1193.6030 - mse: 2371710.2500 - mae: 1193.6030 - val_loss: 1850.5913 - val_mse: 6596623.5000 - val_mae: 1850.5913\n",
            "Epoch 163/500\n",
            "30/30 [==============================] - 0s 860us/step - loss: 1191.5867 - mse: 2279784.0000 - mae: 1191.5867 - val_loss: 1850.3553 - val_mse: 6595097.5000 - val_mae: 1850.3555\n",
            "Epoch 164/500\n",
            "30/30 [==============================] - 0s 978us/step - loss: 1118.8299 - mse: 2175937.5000 - mae: 1118.8300 - val_loss: 1848.3123 - val_mse: 6576509.0000 - val_mae: 1848.3124\n",
            "Epoch 165/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1174.9950 - mse: 2203492.7500 - mae: 1174.9950 - val_loss: 1848.3374 - val_mse: 6577543.5000 - val_mae: 1848.3374\n",
            "Epoch 166/500\n",
            "30/30 [==============================] - 0s 990us/step - loss: 1243.9040 - mse: 2316057.0000 - mae: 1243.9039 - val_loss: 1846.7322 - val_mse: 6563218.0000 - val_mae: 1846.7323\n",
            "Epoch 167/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1225.1732 - mse: 2434945.5000 - mae: 1225.1732 - val_loss: 1845.8775 - val_mse: 6555831.5000 - val_mae: 1845.8776\n",
            "Epoch 168/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1221.5789 - mse: 2247280.2500 - mae: 1221.5789 - val_loss: 1843.3071 - val_mse: 6532481.0000 - val_mae: 1843.3071\n",
            "Epoch 169/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1134.2007 - mse: 2091270.8750 - mae: 1134.2008 - val_loss: 1844.9196 - val_mse: 6548416.5000 - val_mae: 1844.9197\n",
            "Epoch 170/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1292.4476 - mse: 2599431.2500 - mae: 1292.4476 - val_loss: 1848.2851 - val_mse: 6580667.5000 - val_mae: 1848.2852\n",
            "Epoch 171/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1203.1344 - mse: 2280119.0000 - mae: 1203.1344 - val_loss: 1849.8049 - val_mse: 6595732.5000 - val_mae: 1849.8048\n",
            "Epoch 172/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1195.3378 - mse: 2388416.0000 - mae: 1195.3378 - val_loss: 1850.6846 - val_mse: 6604750.5000 - val_mae: 1850.6847\n",
            "Epoch 173/500\n",
            "30/30 [==============================] - 0s 996us/step - loss: 1086.0125 - mse: 2020488.2500 - mae: 1086.0125 - val_loss: 1847.5925 - val_mse: 6576227.0000 - val_mae: 1847.5924\n",
            "Epoch 174/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1098.0987 - mse: 2157873.0000 - mae: 1098.0988 - val_loss: 1844.6698 - val_mse: 6549256.0000 - val_mae: 1844.6698\n",
            "Epoch 175/500\n",
            "30/30 [==============================] - 0s 986us/step - loss: 1196.6045 - mse: 2238026.5000 - mae: 1196.6044 - val_loss: 1844.1681 - val_mse: 6545163.5000 - val_mae: 1844.1680\n",
            "Epoch 176/500\n",
            "30/30 [==============================] - 0s 908us/step - loss: 1130.8723 - mse: 2017934.6250 - mae: 1130.8723 - val_loss: 1842.9905 - val_mse: 6534736.0000 - val_mae: 1842.9905\n",
            "Epoch 177/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1146.4226 - mse: 2120808.2500 - mae: 1146.4226 - val_loss: 1842.0705 - val_mse: 6526781.0000 - val_mae: 1842.0706\n",
            "Epoch 178/500\n",
            "30/30 [==============================] - 0s 954us/step - loss: 1129.7686 - mse: 2050308.5000 - mae: 1129.7687 - val_loss: 1841.1495 - val_mse: 6519006.0000 - val_mae: 1841.1495\n",
            "Epoch 179/500\n",
            "30/30 [==============================] - 0s 1000us/step - loss: 1146.5411 - mse: 2089746.8750 - mae: 1146.5411 - val_loss: 1838.5436 - val_mse: 6495425.0000 - val_mae: 1838.5436\n",
            "Epoch 180/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1215.1691 - mse: 2292532.0000 - mae: 1215.1692 - val_loss: 1839.0382 - val_mse: 6500675.5000 - val_mae: 1839.0382\n",
            "Epoch 181/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1214.4317 - mse: 2346864.0000 - mae: 1214.4318 - val_loss: 1835.7873 - val_mse: 6471053.5000 - val_mae: 1835.7874\n",
            "Epoch 182/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1178.3530 - mse: 2298443.5000 - mae: 1178.3530 - val_loss: 1835.3221 - val_mse: 6467426.5000 - val_mae: 1835.3220\n",
            "Epoch 183/500\n",
            "30/30 [==============================] - 0s 987us/step - loss: 1199.1224 - mse: 2299593.2500 - mae: 1199.1224 - val_loss: 1833.5699 - val_mse: 6451853.0000 - val_mae: 1833.5698\n",
            "Epoch 184/500\n",
            "30/30 [==============================] - 0s 903us/step - loss: 1173.9967 - mse: 2088479.6250 - mae: 1173.9967 - val_loss: 1833.0010 - val_mse: 6447287.0000 - val_mae: 1833.0011\n",
            "Epoch 185/500\n",
            "30/30 [==============================] - 0s 866us/step - loss: 1179.7916 - mse: 2169202.0000 - mae: 1179.7916 - val_loss: 1831.8014 - val_mse: 6436838.0000 - val_mae: 1831.8014\n",
            "Epoch 186/500\n",
            "30/30 [==============================] - 0s 995us/step - loss: 1183.0318 - mse: 2209189.0000 - mae: 1183.0317 - val_loss: 1828.0657 - val_mse: 6403202.5000 - val_mae: 1828.0657\n",
            "Epoch 187/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1150.6267 - mse: 2056151.5000 - mae: 1150.6267 - val_loss: 1828.8608 - val_mse: 6410952.0000 - val_mae: 1828.8608\n",
            "Epoch 188/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1127.3460 - mse: 2119252.0000 - mae: 1127.3459 - val_loss: 1829.6027 - val_mse: 6418261.5000 - val_mae: 1829.6027\n",
            "Epoch 189/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1263.5331 - mse: 2546340.7500 - mae: 1263.5331 - val_loss: 1828.6007 - val_mse: 6409443.5000 - val_mae: 1828.6007\n",
            "Epoch 190/500\n",
            "30/30 [==============================] - 0s 884us/step - loss: 1101.9595 - mse: 1929698.2500 - mae: 1101.9595 - val_loss: 1827.7737 - val_mse: 6402347.0000 - val_mae: 1827.7737\n",
            "Epoch 191/500\n",
            "30/30 [==============================] - 0s 930us/step - loss: 1208.3761 - mse: 2315644.0000 - mae: 1208.3761 - val_loss: 1827.7494 - val_mse: 6402447.5000 - val_mae: 1827.7495\n",
            "Epoch 192/500\n",
            "30/30 [==============================] - 0s 874us/step - loss: 1200.9997 - mse: 2383744.2500 - mae: 1200.9998 - val_loss: 1828.3934 - val_mse: 6408627.5000 - val_mae: 1828.3934\n",
            "Epoch 193/500\n",
            "30/30 [==============================] - 0s 909us/step - loss: 1130.1095 - mse: 2002333.2500 - mae: 1130.1096 - val_loss: 1828.9801 - val_mse: 6414359.0000 - val_mae: 1828.9802\n",
            "Epoch 194/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1178.1727 - mse: 2110473.5000 - mae: 1178.1726 - val_loss: 1828.6603 - val_mse: 6411742.0000 - val_mae: 1828.6604\n",
            "Epoch 195/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1239.8584 - mse: 2364479.5000 - mae: 1239.8583 - val_loss: 1826.5470 - val_mse: 6392696.5000 - val_mae: 1826.5470\n",
            "Epoch 196/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1206.2435 - mse: 2249260.5000 - mae: 1206.2435 - val_loss: 1825.7257 - val_mse: 6385438.0000 - val_mae: 1825.7257\n",
            "Epoch 197/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1148.4719 - mse: 2090145.2500 - mae: 1148.4720 - val_loss: 1823.6676 - val_mse: 6366928.0000 - val_mae: 1823.6677\n",
            "Epoch 198/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1233.0461 - mse: 2293844.2500 - mae: 1233.0461 - val_loss: 1824.0708 - val_mse: 6370862.0000 - val_mae: 1824.0708\n",
            "Epoch 199/500\n",
            "30/30 [==============================] - 0s 996us/step - loss: 1313.1048 - mse: 2633015.5000 - mae: 1313.1049 - val_loss: 1824.2669 - val_mse: 6372854.5000 - val_mae: 1824.2670\n",
            "Epoch 200/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1163.6050 - mse: 2143022.2500 - mae: 1163.6050 - val_loss: 1823.7488 - val_mse: 6368366.0000 - val_mae: 1823.7488\n",
            "Epoch 201/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1144.6142 - mse: 2067441.8750 - mae: 1144.6141 - val_loss: 1822.3704 - val_mse: 6356024.0000 - val_mae: 1822.3706\n",
            "Epoch 202/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1211.1087 - mse: 2339331.0000 - mae: 1211.1088 - val_loss: 1821.7030 - val_mse: 6350129.5000 - val_mae: 1821.7030\n",
            "Epoch 203/500\n",
            "30/30 [==============================] - 0s 979us/step - loss: 1212.5627 - mse: 2317336.5000 - mae: 1212.5627 - val_loss: 1823.5454 - val_mse: 6367087.5000 - val_mae: 1823.5454\n",
            "Epoch 204/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1128.3510 - mse: 2041242.8750 - mae: 1128.3511 - val_loss: 1823.8525 - val_mse: 6370096.5000 - val_mae: 1823.8524\n",
            "Epoch 205/500\n",
            "30/30 [==============================] - 0s 953us/step - loss: 1193.0271 - mse: 2159809.0000 - mae: 1193.0271 - val_loss: 1822.9213 - val_mse: 6361790.5000 - val_mae: 1822.9214\n",
            "Epoch 206/500\n",
            "30/30 [==============================] - 0s 890us/step - loss: 1111.6672 - mse: 2014202.2500 - mae: 1111.6672 - val_loss: 1823.0621 - val_mse: 6363288.5000 - val_mae: 1823.0621\n",
            "Epoch 207/500\n",
            "30/30 [==============================] - 0s 948us/step - loss: 1179.3660 - mse: 2162358.5000 - mae: 1179.3660 - val_loss: 1823.8098 - val_mse: 6370293.5000 - val_mae: 1823.8099\n",
            "Epoch 208/500\n",
            "30/30 [==============================] - 0s 965us/step - loss: 1179.6178 - mse: 2444055.5000 - mae: 1179.6178 - val_loss: 1824.8540 - val_mse: 6379999.0000 - val_mae: 1824.8540\n",
            "Epoch 209/500\n",
            "30/30 [==============================] - 0s 906us/step - loss: 1190.8795 - mse: 2140274.5000 - mae: 1190.8794 - val_loss: 1825.4055 - val_mse: 6385253.5000 - val_mae: 1825.4055\n",
            "Epoch 210/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1202.1470 - mse: 2237747.2500 - mae: 1202.1471 - val_loss: 1823.9107 - val_mse: 6371694.0000 - val_mae: 1823.9106\n",
            "Epoch 211/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1145.5705 - mse: 1940614.3750 - mae: 1145.5706 - val_loss: 1821.9386 - val_mse: 6353858.0000 - val_mae: 1821.9386\n",
            "Epoch 212/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1204.1593 - mse: 2370476.2500 - mae: 1204.1594 - val_loss: 1820.5864 - val_mse: 6341645.5000 - val_mae: 1820.5864\n",
            "Epoch 213/500\n",
            "30/30 [==============================] - 0s 962us/step - loss: 1232.7782 - mse: 2474368.5000 - mae: 1232.7782 - val_loss: 1821.0658 - val_mse: 6346116.5000 - val_mae: 1821.0658\n",
            "Epoch 214/500\n",
            "30/30 [==============================] - 0s 962us/step - loss: 1252.8723 - mse: 2363524.5000 - mae: 1252.8724 - val_loss: 1820.7199 - val_mse: 6343103.0000 - val_mae: 1820.7200\n",
            "Epoch 215/500\n",
            "30/30 [==============================] - 0s 958us/step - loss: 1102.1300 - mse: 2063006.5000 - mae: 1102.1300 - val_loss: 1819.9910 - val_mse: 6336596.5000 - val_mae: 1819.9910\n",
            "Epoch 216/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1120.2599 - mse: 2147374.7500 - mae: 1120.2599 - val_loss: 1817.9083 - val_mse: 6317825.0000 - val_mae: 1817.9083\n",
            "Epoch 217/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1113.7903 - mse: 2120592.7500 - mae: 1113.7904 - val_loss: 1819.8126 - val_mse: 6335215.5000 - val_mae: 1819.8125\n",
            "Epoch 218/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1302.0509 - mse: 2448226.2500 - mae: 1302.0508 - val_loss: 1819.2687 - val_mse: 6330392.0000 - val_mae: 1819.2688\n",
            "Epoch 219/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1200.7759 - mse: 2454344.0000 - mae: 1200.7759 - val_loss: 1818.8199 - val_mse: 6326425.5000 - val_mae: 1818.8198\n",
            "Epoch 220/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1207.4194 - mse: 2300508.2500 - mae: 1207.4193 - val_loss: 1818.9020 - val_mse: 6327266.0000 - val_mae: 1818.9019\n",
            "Epoch 221/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1083.2702 - mse: 1894671.3750 - mae: 1083.2701 - val_loss: 1817.8316 - val_mse: 6317686.0000 - val_mae: 1817.8315\n",
            "Epoch 222/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1175.1612 - mse: 2245270.7500 - mae: 1175.1613 - val_loss: 1817.6262 - val_mse: 6315922.5000 - val_mae: 1817.6262\n",
            "Epoch 223/500\n",
            "30/30 [==============================] - 0s 931us/step - loss: 1169.3726 - mse: 2133544.7500 - mae: 1169.3727 - val_loss: 1816.8014 - val_mse: 6308549.5000 - val_mae: 1816.8014\n",
            "Epoch 224/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1193.8919 - mse: 2236025.7500 - mae: 1193.8920 - val_loss: 1818.4240 - val_mse: 6323358.0000 - val_mae: 1818.4240\n",
            "Epoch 225/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1205.7742 - mse: 2263079.5000 - mae: 1205.7742 - val_loss: 1820.6034 - val_mse: 6343299.5000 - val_mae: 1820.6034\n",
            "Epoch 226/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1146.4081 - mse: 2001328.2500 - mae: 1146.4082 - val_loss: 1819.4513 - val_mse: 6332929.5000 - val_mae: 1819.4513\n",
            "Epoch 227/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1097.0779 - mse: 1961859.7500 - mae: 1097.0779 - val_loss: 1818.3640 - val_mse: 6323147.0000 - val_mae: 1818.3640\n",
            "Epoch 228/500\n",
            "30/30 [==============================] - 0s 922us/step - loss: 1232.1487 - mse: 2375451.7500 - mae: 1232.1487 - val_loss: 1816.4861 - val_mse: 6306222.0000 - val_mae: 1816.4862\n",
            "Epoch 229/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1200.8069 - mse: 2361866.2500 - mae: 1200.8069 - val_loss: 1815.1151 - val_mse: 6293914.5000 - val_mae: 1815.1151\n",
            "Epoch 230/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1141.5288 - mse: 1974073.2500 - mae: 1141.5289 - val_loss: 1815.2504 - val_mse: 6295258.5000 - val_mae: 1815.2504\n",
            "Epoch 231/500\n",
            "30/30 [==============================] - 0s 891us/step - loss: 1252.8316 - mse: 2401583.7500 - mae: 1252.8317 - val_loss: 1818.0283 - val_mse: 6320523.0000 - val_mae: 1818.0282\n",
            "Epoch 232/500\n",
            "30/30 [==============================] - 0s 923us/step - loss: 1190.9082 - mse: 2377040.0000 - mae: 1190.9082 - val_loss: 1816.7510 - val_mse: 6309000.0000 - val_mae: 1816.7511\n",
            "Epoch 233/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1164.2677 - mse: 2156354.5000 - mae: 1164.2677 - val_loss: 1816.5644 - val_mse: 6307421.0000 - val_mae: 1816.5645\n",
            "Epoch 234/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1216.1764 - mse: 2277930.7500 - mae: 1216.1764 - val_loss: 1815.3630 - val_mse: 6296621.0000 - val_mae: 1815.3630\n",
            "Epoch 235/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1183.7885 - mse: 2176068.5000 - mae: 1183.7886 - val_loss: 1814.0497 - val_mse: 6284856.0000 - val_mae: 1814.0497\n",
            "Epoch 236/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1145.8945 - mse: 2117711.2500 - mae: 1145.8945 - val_loss: 1816.8621 - val_mse: 6310373.5000 - val_mae: 1816.8621\n",
            "Epoch 237/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1214.5525 - mse: 2352330.5000 - mae: 1214.5526 - val_loss: 1816.3201 - val_mse: 6305528.0000 - val_mae: 1816.3202\n",
            "Epoch 238/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1158.5525 - mse: 2184623.2500 - mae: 1158.5526 - val_loss: 1813.9393 - val_mse: 6284060.5000 - val_mae: 1813.9393\n",
            "Epoch 239/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1264.7330 - mse: 2253418.2500 - mae: 1264.7330 - val_loss: 1815.1112 - val_mse: 6294733.0000 - val_mae: 1815.1112\n",
            "Epoch 240/500\n",
            "30/30 [==============================] - 0s 955us/step - loss: 1229.9904 - mse: 2494909.5000 - mae: 1229.9904 - val_loss: 1814.8827 - val_mse: 6292709.0000 - val_mae: 1814.8827\n",
            "Epoch 241/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1235.7361 - mse: 2339413.7500 - mae: 1235.7362 - val_loss: 1815.5266 - val_mse: 6298602.0000 - val_mae: 1815.5266\n",
            "Epoch 242/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1138.4352 - mse: 2208058.5000 - mae: 1138.4352 - val_loss: 1815.8030 - val_mse: 6301187.0000 - val_mae: 1815.8029\n",
            "Epoch 243/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1104.1299 - mse: 2034758.7500 - mae: 1104.1298 - val_loss: 1815.6630 - val_mse: 6300010.5000 - val_mae: 1815.6628\n",
            "Epoch 244/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1209.3735 - mse: 2326813.2500 - mae: 1209.3734 - val_loss: 1815.9600 - val_mse: 6302755.5000 - val_mae: 1815.9601\n",
            "Epoch 245/500\n",
            "30/30 [==============================] - 0s 846us/step - loss: 1118.0055 - mse: 2061315.2500 - mae: 1118.0055 - val_loss: 1815.2693 - val_mse: 6296544.5000 - val_mae: 1815.2693\n",
            "Epoch 246/500\n",
            "30/30 [==============================] - 0s 884us/step - loss: 1114.6913 - mse: 2065061.3750 - mae: 1114.6913 - val_loss: 1813.9547 - val_mse: 6284744.0000 - val_mae: 1813.9547\n",
            "Epoch 247/500\n",
            "30/30 [==============================] - 0s 967us/step - loss: 1199.8309 - mse: 1950805.8750 - mae: 1199.8307 - val_loss: 1814.0129 - val_mse: 6285342.0000 - val_mae: 1814.0131\n",
            "Epoch 248/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1148.0622 - mse: 2107383.5000 - mae: 1148.0623 - val_loss: 1812.9937 - val_mse: 6276234.5000 - val_mae: 1812.9937\n",
            "Epoch 249/500\n",
            "30/30 [==============================] - 0s 882us/step - loss: 1177.8362 - mse: 2255597.7500 - mae: 1177.8362 - val_loss: 1811.4528 - val_mse: 6262450.0000 - val_mae: 1811.4529\n",
            "Epoch 250/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1228.1632 - mse: 2341605.7500 - mae: 1228.1632 - val_loss: 1811.1222 - val_mse: 6259509.5000 - val_mae: 1811.1222\n",
            "Epoch 251/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1279.8793 - mse: 2485114.2500 - mae: 1279.8793 - val_loss: 1809.8203 - val_mse: 6247910.0000 - val_mae: 1809.8203\n",
            "Epoch 252/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1061.3451 - mse: 1832942.1250 - mae: 1061.3451 - val_loss: 1809.4500 - val_mse: 6244686.5000 - val_mae: 1809.4498\n",
            "Epoch 253/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1292.1351 - mse: 2515329.0000 - mae: 1292.1351 - val_loss: 1809.7731 - val_mse: 6247597.5000 - val_mae: 1809.7732\n",
            "Epoch 254/500\n",
            "30/30 [==============================] - 0s 965us/step - loss: 1216.6221 - mse: 2298266.2500 - mae: 1216.6222 - val_loss: 1810.0286 - val_mse: 6249914.5000 - val_mae: 1810.0286\n",
            "Epoch 255/500\n",
            "30/30 [==============================] - 0s 932us/step - loss: 1225.0435 - mse: 2248130.5000 - mae: 1225.0435 - val_loss: 1809.9474 - val_mse: 6249223.5000 - val_mae: 1809.9474\n",
            "Epoch 256/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1090.8303 - mse: 1980649.3750 - mae: 1090.8303 - val_loss: 1809.8621 - val_mse: 6248533.5000 - val_mae: 1809.8621\n",
            "Epoch 257/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1021.4213 - mse: 1940047.5000 - mae: 1021.4213 - val_loss: 1808.5478 - val_mse: 6236791.5000 - val_mae: 1808.5477\n",
            "Epoch 258/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1230.1761 - mse: 2472160.0000 - mae: 1230.1760 - val_loss: 1808.2994 - val_mse: 6234598.5000 - val_mae: 1808.2994\n",
            "Epoch 259/500\n",
            "30/30 [==============================] - 0s 957us/step - loss: 1135.7330 - mse: 2094109.6250 - mae: 1135.7329 - val_loss: 1807.8044 - val_mse: 6230248.5000 - val_mae: 1807.8044\n",
            "Epoch 260/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1226.9345 - mse: 2302783.0000 - mae: 1226.9344 - val_loss: 1808.1309 - val_mse: 6233185.0000 - val_mae: 1808.1309\n",
            "Epoch 261/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1135.2160 - mse: 1959541.3750 - mae: 1135.2159 - val_loss: 1807.0529 - val_mse: 6223589.5000 - val_mae: 1807.0529\n",
            "Epoch 262/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1213.6345 - mse: 2273879.0000 - mae: 1213.6344 - val_loss: 1807.9495 - val_mse: 6231670.5000 - val_mae: 1807.9495\n",
            "Epoch 263/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1192.9504 - mse: 2379182.5000 - mae: 1192.9504 - val_loss: 1807.3892 - val_mse: 6226682.0000 - val_mae: 1807.3893\n",
            "Epoch 264/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1133.7466 - mse: 2062452.7500 - mae: 1133.7466 - val_loss: 1807.7273 - val_mse: 6229737.5000 - val_mae: 1807.7273\n",
            "Epoch 265/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 1200.7381 - mse: 2196479.7500 - mae: 1200.7380 - val_loss: 1808.5120 - val_mse: 6236793.0000 - val_mae: 1808.5120\n",
            "Epoch 266/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1169.6852 - mse: 2363907.7500 - mae: 1169.6852 - val_loss: 1808.4919 - val_mse: 6236676.5000 - val_mae: 1808.4918\n",
            "Epoch 267/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1171.2619 - mse: 2298193.7500 - mae: 1171.2618 - val_loss: 1808.3522 - val_mse: 6235491.5000 - val_mae: 1808.3522\n",
            "Epoch 268/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1184.1084 - mse: 2165940.5000 - mae: 1184.1083 - val_loss: 1807.2834 - val_mse: 6225955.0000 - val_mae: 1807.2833\n",
            "Epoch 269/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1159.7626 - mse: 2126755.7500 - mae: 1159.7626 - val_loss: 1806.5242 - val_mse: 6219208.5000 - val_mae: 1806.5242\n",
            "Epoch 270/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1157.7340 - mse: 2119691.0000 - mae: 1157.7341 - val_loss: 1804.9018 - val_mse: 6204831.5000 - val_mae: 1804.9019\n",
            "Epoch 271/500\n",
            "30/30 [==============================] - 0s 937us/step - loss: 1128.4480 - mse: 1974212.0000 - mae: 1128.4480 - val_loss: 1805.2014 - val_mse: 6207581.0000 - val_mae: 1805.2014\n",
            "Epoch 272/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1187.4885 - mse: 2122590.7500 - mae: 1187.4885 - val_loss: 1807.8660 - val_mse: 6231453.0000 - val_mae: 1807.8660\n",
            "Epoch 273/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1134.8456 - mse: 2045591.7500 - mae: 1134.8456 - val_loss: 1806.8986 - val_mse: 6222917.5000 - val_mae: 1806.8987\n",
            "Epoch 274/500\n",
            "30/30 [==============================] - 0s 887us/step - loss: 1064.1793 - mse: 1892901.2500 - mae: 1064.1793 - val_loss: 1804.0244 - val_mse: 6197315.0000 - val_mae: 1804.0243\n",
            "Epoch 275/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1176.0008 - mse: 2186050.7500 - mae: 1176.0007 - val_loss: 1802.7452 - val_mse: 6186000.5000 - val_mae: 1802.7450\n",
            "Epoch 276/500\n",
            "30/30 [==============================] - 0s 814us/step - loss: 1243.3781 - mse: 2376071.2500 - mae: 1243.3782 - val_loss: 1803.3394 - val_mse: 6191285.5000 - val_mae: 1803.3394\n",
            "Epoch 277/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1136.5000 - mse: 2134953.7500 - mae: 1136.5000 - val_loss: 1801.8711 - val_mse: 6178296.5000 - val_mae: 1801.8711\n",
            "Epoch 278/500\n",
            "30/30 [==============================] - 0s 959us/step - loss: 1170.7599 - mse: 2103108.2500 - mae: 1170.7599 - val_loss: 1801.1628 - val_mse: 6172068.5000 - val_mae: 1801.1627\n",
            "Epoch 279/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1202.7597 - mse: 2315365.2500 - mae: 1202.7598 - val_loss: 1800.6525 - val_mse: 6167575.5000 - val_mae: 1800.6526\n",
            "Epoch 280/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1191.1629 - mse: 2311507.5000 - mae: 1191.1628 - val_loss: 1800.0366 - val_mse: 6162211.5000 - val_mae: 1800.0366\n",
            "Epoch 281/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1169.1430 - mse: 2021309.1250 - mae: 1169.1429 - val_loss: 1801.3793 - val_mse: 6174078.0000 - val_mae: 1801.3793\n",
            "Epoch 282/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1124.9759 - mse: 2034324.8750 - mae: 1124.9760 - val_loss: 1802.2076 - val_mse: 6181488.5000 - val_mae: 1802.2075\n",
            "Epoch 283/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1122.5698 - mse: 2071721.3750 - mae: 1122.5698 - val_loss: 1799.8036 - val_mse: 6160269.5000 - val_mae: 1799.8036\n",
            "Epoch 284/500\n",
            "30/30 [==============================] - 0s 902us/step - loss: 1139.3508 - mse: 2038950.3750 - mae: 1139.3508 - val_loss: 1799.4092 - val_mse: 6156874.0000 - val_mae: 1799.4091\n",
            "Epoch 285/500\n",
            "30/30 [==============================] - 0s 924us/step - loss: 1073.3228 - mse: 2013109.6250 - mae: 1073.3228 - val_loss: 1799.3517 - val_mse: 6156454.0000 - val_mae: 1799.3518\n",
            "Epoch 286/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1195.9978 - mse: 2224421.5000 - mae: 1195.9978 - val_loss: 1800.7934 - val_mse: 6169268.5000 - val_mae: 1800.7935\n",
            "Epoch 287/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1163.2278 - mse: 2013640.1250 - mae: 1163.2279 - val_loss: 1799.4300 - val_mse: 6157256.5000 - val_mae: 1799.4301\n",
            "Epoch 288/500\n",
            "30/30 [==============================] - 0s 974us/step - loss: 1266.5620 - mse: 2424685.5000 - mae: 1266.5621 - val_loss: 1800.6235 - val_mse: 6167880.0000 - val_mae: 1800.6234\n",
            "Epoch 289/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1182.0407 - mse: 2154891.2500 - mae: 1182.0406 - val_loss: 1799.4766 - val_mse: 6157782.5000 - val_mae: 1799.4766\n",
            "Epoch 290/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1295.7808 - mse: 2435731.7500 - mae: 1295.7808 - val_loss: 1799.6107 - val_mse: 6159085.0000 - val_mae: 1799.6107\n",
            "Epoch 291/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1213.9504 - mse: 2300266.7500 - mae: 1213.9506 - val_loss: 1799.2364 - val_mse: 6155808.5000 - val_mae: 1799.2363\n",
            "Epoch 292/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1137.6310 - mse: 1970647.2500 - mae: 1137.6310 - val_loss: 1798.8447 - val_mse: 6152368.0000 - val_mae: 1798.8448\n",
            "Epoch 293/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1111.7333 - mse: 1963856.6250 - mae: 1111.7333 - val_loss: 1798.2213 - val_mse: 6146911.5000 - val_mae: 1798.2213\n",
            "Epoch 294/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1071.5904 - mse: 1766332.6250 - mae: 1071.5903 - val_loss: 1797.2725 - val_mse: 6138600.0000 - val_mae: 1797.2726\n",
            "Epoch 295/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1082.8266 - mse: 1937746.8750 - mae: 1082.8265 - val_loss: 1795.7604 - val_mse: 6125465.5000 - val_mae: 1795.7604\n",
            "Epoch 296/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1183.7588 - mse: 2226097.7500 - mae: 1183.7589 - val_loss: 1795.8782 - val_mse: 6126495.0000 - val_mae: 1795.8783\n",
            "Epoch 297/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1225.7054 - mse: 2105834.7500 - mae: 1225.7054 - val_loss: 1795.6755 - val_mse: 6124739.5000 - val_mae: 1795.6755\n",
            "Epoch 298/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1184.7011 - mse: 2322407.0000 - mae: 1184.7010 - val_loss: 1795.0680 - val_mse: 6119464.5000 - val_mae: 1795.0680\n",
            "Epoch 299/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1166.5354 - mse: 2147896.7500 - mae: 1166.5354 - val_loss: 1794.9942 - val_mse: 6118932.5000 - val_mae: 1794.9943\n",
            "Epoch 300/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1182.0451 - mse: 2053424.1250 - mae: 1182.0450 - val_loss: 1795.6518 - val_mse: 6124800.5000 - val_mae: 1795.6519\n",
            "Epoch 301/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1104.0159 - mse: 1947040.6250 - mae: 1104.0159 - val_loss: 1795.0991 - val_mse: 6119979.0000 - val_mae: 1795.0992\n",
            "Epoch 302/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1114.0515 - mse: 2207697.7500 - mae: 1114.0514 - val_loss: 1793.2771 - val_mse: 6104217.0000 - val_mae: 1793.2771\n",
            "Epoch 303/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1036.7945 - mse: 1703493.3750 - mae: 1036.7946 - val_loss: 1791.6834 - val_mse: 6090413.0000 - val_mae: 1791.6833\n",
            "Epoch 304/500\n",
            "30/30 [==============================] - 0s 986us/step - loss: 1159.5117 - mse: 2072821.1250 - mae: 1159.5117 - val_loss: 1790.3231 - val_mse: 6078654.5000 - val_mae: 1790.3230\n",
            "Epoch 305/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1068.1674 - mse: 1793417.6250 - mae: 1068.1674 - val_loss: 1790.0571 - val_mse: 6076381.5000 - val_mae: 1790.0570\n",
            "Epoch 306/500\n",
            "30/30 [==============================] - 0s 991us/step - loss: 1195.2548 - mse: 2170997.5000 - mae: 1195.2548 - val_loss: 1790.1569 - val_mse: 6077257.5000 - val_mae: 1790.1569\n",
            "Epoch 307/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1154.9707 - mse: 2066055.1250 - mae: 1154.9707 - val_loss: 1790.0848 - val_mse: 6076641.0000 - val_mae: 1790.0848\n",
            "Epoch 308/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1262.1577 - mse: 2456178.7500 - mae: 1262.1577 - val_loss: 1790.1466 - val_mse: 6077280.5000 - val_mae: 1790.1466\n",
            "Epoch 309/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1182.8654 - mse: 1995211.5000 - mae: 1182.8654 - val_loss: 1789.0789 - val_mse: 6068084.5000 - val_mae: 1789.0789\n",
            "Epoch 310/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1068.5054 - mse: 1902799.3750 - mae: 1068.5054 - val_loss: 1788.7526 - val_mse: 6065401.5000 - val_mae: 1788.7526\n",
            "Epoch 311/500\n",
            "30/30 [==============================] - 0s 949us/step - loss: 1195.1192 - mse: 2204422.5000 - mae: 1195.1193 - val_loss: 1789.4861 - val_mse: 6071761.5000 - val_mae: 1789.4862\n",
            "Epoch 312/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1145.2580 - mse: 2123212.2500 - mae: 1145.2581 - val_loss: 1789.7963 - val_mse: 6074575.5000 - val_mae: 1789.7964\n",
            "Epoch 313/500\n",
            "30/30 [==============================] - 0s 962us/step - loss: 1139.5786 - mse: 1970148.3750 - mae: 1139.5785 - val_loss: 1788.2214 - val_mse: 6061006.0000 - val_mae: 1788.2213\n",
            "Epoch 314/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1192.1982 - mse: 2141542.7500 - mae: 1192.1981 - val_loss: 1787.1126 - val_mse: 6051497.5000 - val_mae: 1787.1125\n",
            "Epoch 315/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1160.2478 - mse: 2233081.2500 - mae: 1160.2478 - val_loss: 1787.7489 - val_mse: 6057080.0000 - val_mae: 1787.7489\n",
            "Epoch 316/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1090.6211 - mse: 1988881.6250 - mae: 1090.6211 - val_loss: 1787.0888 - val_mse: 6051430.0000 - val_mae: 1787.0887\n",
            "Epoch 317/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1149.2523 - mse: 2060616.0000 - mae: 1149.2523 - val_loss: 1785.7785 - val_mse: 6040205.5000 - val_mae: 1785.7786\n",
            "Epoch 318/500\n",
            "30/30 [==============================] - 0s 871us/step - loss: 1189.5552 - mse: 2159259.0000 - mae: 1189.5552 - val_loss: 1786.4002 - val_mse: 6045670.0000 - val_mae: 1786.4003\n",
            "Epoch 319/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1062.1042 - mse: 1759028.7500 - mae: 1062.1042 - val_loss: 1785.0052 - val_mse: 6033753.5000 - val_mae: 1785.0052\n",
            "Epoch 320/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1125.2723 - mse: 2143759.7500 - mae: 1125.2723 - val_loss: 1785.4155 - val_mse: 6037305.0000 - val_mae: 1785.4155\n",
            "Epoch 321/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1105.9204 - mse: 1932975.8750 - mae: 1105.9203 - val_loss: 1786.2920 - val_mse: 6044952.5000 - val_mae: 1786.2919\n",
            "Epoch 322/500\n",
            "30/30 [==============================] - 0s 894us/step - loss: 1188.2609 - mse: 2267598.2500 - mae: 1188.2610 - val_loss: 1786.4563 - val_mse: 6046414.0000 - val_mae: 1786.4563\n",
            "Epoch 323/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1141.2085 - mse: 2014817.3750 - mae: 1141.2085 - val_loss: 1783.7607 - val_mse: 6023364.5000 - val_mae: 1783.7606\n",
            "Epoch 324/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1132.9303 - mse: 1992900.5000 - mae: 1132.9302 - val_loss: 1785.9733 - val_mse: 6042459.5000 - val_mae: 1785.9731\n",
            "Epoch 325/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1282.5319 - mse: 2307776.5000 - mae: 1282.5317 - val_loss: 1786.7062 - val_mse: 6048803.0000 - val_mae: 1786.7063\n",
            "Epoch 326/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1149.2813 - mse: 2195943.0000 - mae: 1149.2812 - val_loss: 1787.7328 - val_mse: 6057795.5000 - val_mae: 1787.7328\n",
            "Epoch 327/500\n",
            "30/30 [==============================] - 0s 965us/step - loss: 1120.9106 - mse: 1992450.2500 - mae: 1120.9105 - val_loss: 1786.8499 - val_mse: 6050241.0000 - val_mae: 1786.8500\n",
            "Epoch 328/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1133.0560 - mse: 2025528.2500 - mae: 1133.0560 - val_loss: 1785.9431 - val_mse: 6042496.0000 - val_mae: 1785.9431\n",
            "Epoch 329/500\n",
            "30/30 [==============================] - 0s 995us/step - loss: 1061.7829 - mse: 1862742.3750 - mae: 1061.7830 - val_loss: 1784.5648 - val_mse: 6030747.5000 - val_mae: 1784.5648\n",
            "Epoch 330/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1189.9533 - mse: 2193285.0000 - mae: 1189.9532 - val_loss: 1785.2300 - val_mse: 6036627.0000 - val_mae: 1785.2300\n",
            "Epoch 331/500\n",
            "30/30 [==============================] - 0s 910us/step - loss: 1160.4676 - mse: 2185662.7500 - mae: 1160.4675 - val_loss: 1785.8208 - val_mse: 6041761.0000 - val_mae: 1785.8208\n",
            "Epoch 332/500\n",
            "30/30 [==============================] - 0s 928us/step - loss: 1143.0433 - mse: 2062784.2500 - mae: 1143.0432 - val_loss: 1785.0057 - val_mse: 6034935.5000 - val_mae: 1785.0056\n",
            "Epoch 333/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1134.8252 - mse: 2023764.0000 - mae: 1134.8253 - val_loss: 1786.6903 - val_mse: 6049597.0000 - val_mae: 1786.6903\n",
            "Epoch 334/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1071.0671 - mse: 2072308.2500 - mae: 1071.0671 - val_loss: 1785.6208 - val_mse: 6040410.5000 - val_mae: 1785.6208\n",
            "Epoch 335/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1118.5841 - mse: 2167775.0000 - mae: 1118.5841 - val_loss: 1784.7415 - val_mse: 6032914.5000 - val_mae: 1784.7416\n",
            "Epoch 336/500\n",
            "30/30 [==============================] - 0s 900us/step - loss: 1088.7820 - mse: 2034914.6250 - mae: 1088.7820 - val_loss: 1783.6143 - val_mse: 6023305.0000 - val_mae: 1783.6144\n",
            "Epoch 337/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1232.5921 - mse: 2429471.0000 - mae: 1232.5920 - val_loss: 1783.6011 - val_mse: 6023229.5000 - val_mae: 1783.6012\n",
            "Epoch 338/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1020.8656 - mse: 1719463.6250 - mae: 1020.8655 - val_loss: 1784.2875 - val_mse: 6029324.5000 - val_mae: 1784.2875\n",
            "Epoch 339/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1168.9589 - mse: 2101690.7500 - mae: 1168.9589 - val_loss: 1783.0940 - val_mse: 6019206.0000 - val_mae: 1783.0940\n",
            "Epoch 340/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1083.7573 - mse: 1967021.5000 - mae: 1083.7573 - val_loss: 1782.4329 - val_mse: 6013676.5000 - val_mae: 1782.4330\n",
            "Epoch 341/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1196.6333 - mse: 2376978.2500 - mae: 1196.6332 - val_loss: 1783.3242 - val_mse: 6021379.5000 - val_mae: 1783.3242\n",
            "Epoch 342/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1163.7039 - mse: 2192596.5000 - mae: 1163.7039 - val_loss: 1783.9470 - val_mse: 6026835.5000 - val_mae: 1783.9469\n",
            "Epoch 343/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1156.9671 - mse: 2273310.2500 - mae: 1156.9670 - val_loss: 1785.3703 - val_mse: 6039209.0000 - val_mae: 1785.3704\n",
            "Epoch 344/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1180.4324 - mse: 2243173.2500 - mae: 1180.4324 - val_loss: 1784.2877 - val_mse: 6030066.5000 - val_mae: 1784.2877\n",
            "Epoch 345/500\n",
            "30/30 [==============================] - 0s 936us/step - loss: 1227.1108 - mse: 2341489.2500 - mae: 1227.1107 - val_loss: 1785.4317 - val_mse: 6040024.0000 - val_mae: 1785.4316\n",
            "Epoch 346/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1160.6453 - mse: 2166549.7500 - mae: 1160.6453 - val_loss: 1785.2809 - val_mse: 6038920.0000 - val_mae: 1785.2809\n",
            "Epoch 347/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1207.9572 - mse: 2229415.7500 - mae: 1207.9573 - val_loss: 1785.4760 - val_mse: 6040926.5000 - val_mae: 1785.4761\n",
            "Epoch 348/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1107.9337 - mse: 1922077.8750 - mae: 1107.9336 - val_loss: 1785.7094 - val_mse: 6043241.5000 - val_mae: 1785.7094\n",
            "Epoch 349/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1148.2228 - mse: 2057481.6250 - mae: 1148.2228 - val_loss: 1784.9978 - val_mse: 6037535.5000 - val_mae: 1784.9978\n",
            "Epoch 350/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1160.4635 - mse: 1970901.7500 - mae: 1160.4635 - val_loss: 1783.8340 - val_mse: 6027923.0000 - val_mae: 1783.8340\n",
            "Epoch 351/500\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 1192.1012 - mse: 2216169.5000 - mae: 1192.1012 - val_loss: 1784.8463 - val_mse: 6037188.5000 - val_mae: 1784.8462\n",
            "Epoch 352/500\n",
            "30/30 [==============================] - 0s 971us/step - loss: 1149.9452 - mse: 2018991.6250 - mae: 1149.9453 - val_loss: 1784.4221 - val_mse: 6034063.5000 - val_mae: 1784.4221\n",
            "Epoch 353/500\n",
            "30/30 [==============================] - 0s 948us/step - loss: 1154.5847 - mse: 2144207.0000 - mae: 1154.5846 - val_loss: 1785.2146 - val_mse: 6041490.5000 - val_mae: 1785.2146\n",
            "Epoch 354/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1192.7203 - mse: 2303121.0000 - mae: 1192.7203 - val_loss: 1785.7206 - val_mse: 6046729.0000 - val_mae: 1785.7206\n",
            "Epoch 355/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1176.8380 - mse: 2078391.1250 - mae: 1176.8380 - val_loss: 1785.7689 - val_mse: 6048061.5000 - val_mae: 1785.7689\n",
            "Epoch 356/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1144.6700 - mse: 2103302.0000 - mae: 1144.6699 - val_loss: 1784.9617 - val_mse: 6042129.5000 - val_mae: 1784.9617\n",
            "Epoch 357/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1209.7955 - mse: 2287369.0000 - mae: 1209.7955 - val_loss: 1785.3149 - val_mse: 6046638.5000 - val_mae: 1785.3148\n",
            "Epoch 358/500\n",
            "30/30 [==============================] - 0s 921us/step - loss: 1046.9151 - mse: 1865881.6250 - mae: 1046.9152 - val_loss: 1785.6444 - val_mse: 6051198.5000 - val_mae: 1785.6444\n",
            "Epoch 359/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1133.8424 - mse: 1877142.1250 - mae: 1133.8424 - val_loss: 1785.1474 - val_mse: 6049093.5000 - val_mae: 1785.1473\n",
            "Epoch 360/500\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 1239.8867 - mse: 2253235.2500 - mae: 1239.8867 - val_loss: 1787.9057 - val_mse: 6075803.5000 - val_mae: 1787.9058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50 samples, validate on 40 samples\n",
            "Epoch 1/500\n",
            "50/50 [==============================] - 2s 43ms/step - loss: 5100.8959 - mse: 48605656.0000 - mae: 5100.8960 - val_loss: 4477.7876 - val_mse: 33086694.0000 - val_mae: 4477.7876\n",
            "Epoch 2/500\n",
            "50/50 [==============================] - 0s 809us/step - loss: 5100.2202 - mse: 48602352.0000 - mae: 5100.2207 - val_loss: 4475.0573 - val_mse: 33073946.0000 - val_mae: 4475.0576\n",
            "Epoch 3/500\n",
            "50/50 [==============================] - 0s 669us/step - loss: 5093.8772 - mse: 48566240.0000 - mae: 5093.8770 - val_loss: 4463.2372 - val_mse: 33012042.0000 - val_mae: 4463.2373\n",
            "Epoch 4/500\n",
            "50/50 [==============================] - 0s 652us/step - loss: 5081.5674 - mse: 48509280.0000 - mae: 5081.5674 - val_loss: 4448.2789 - val_mse: 32927280.0000 - val_mae: 4448.2788\n",
            "Epoch 5/500\n",
            "50/50 [==============================] - 0s 645us/step - loss: 5063.5371 - mse: 48386468.0000 - mae: 5063.5371 - val_loss: 4429.7110 - val_mse: 32812794.0000 - val_mae: 4429.7109\n",
            "Epoch 6/500\n",
            "50/50 [==============================] - 0s 677us/step - loss: 5046.3930 - mse: 48252404.0000 - mae: 5046.3931 - val_loss: 4410.1352 - val_mse: 32682070.0000 - val_mae: 4410.1353\n",
            "Epoch 7/500\n",
            "50/50 [==============================] - 0s 758us/step - loss: 5027.2870 - mse: 48110152.0000 - mae: 5027.2871 - val_loss: 4389.2852 - val_mse: 32538230.0000 - val_mae: 4389.2852\n",
            "Epoch 8/500\n",
            "50/50 [==============================] - 0s 835us/step - loss: 5007.2469 - mse: 47936672.0000 - mae: 5007.2471 - val_loss: 4364.9636 - val_mse: 32367404.0000 - val_mae: 4364.9639\n",
            "Epoch 9/500\n",
            "50/50 [==============================] - 0s 745us/step - loss: 4979.0831 - mse: 47685008.0000 - mae: 4979.0830 - val_loss: 4337.8355 - val_mse: 32172476.0000 - val_mae: 4337.8359\n",
            "Epoch 10/500\n",
            "50/50 [==============================] - 0s 798us/step - loss: 4967.7459 - mse: 47508272.0000 - mae: 4967.7461 - val_loss: 4311.3190 - val_mse: 31979884.0000 - val_mae: 4311.3188\n",
            "Epoch 11/500\n",
            "50/50 [==============================] - 0s 867us/step - loss: 4928.4243 - mse: 47178784.0000 - mae: 4928.4243 - val_loss: 4280.7435 - val_mse: 31755558.0000 - val_mae: 4280.7432\n",
            "Epoch 12/500\n",
            "50/50 [==============================] - 0s 611us/step - loss: 4919.0586 - mse: 47013364.0000 - mae: 4919.0586 - val_loss: 4250.4913 - val_mse: 31529088.0000 - val_mae: 4250.4912\n",
            "Epoch 13/500\n",
            "50/50 [==============================] - 0s 691us/step - loss: 4897.2632 - mse: 46567408.0000 - mae: 4897.2632 - val_loss: 4222.0608 - val_mse: 31310944.0000 - val_mae: 4222.0610\n",
            "Epoch 14/500\n",
            "50/50 [==============================] - 0s 800us/step - loss: 4860.2455 - mse: 46283448.0000 - mae: 4860.2456 - val_loss: 4188.1663 - val_mse: 31052896.0000 - val_mae: 4188.1665\n",
            "Epoch 15/500\n",
            "50/50 [==============================] - 0s 688us/step - loss: 4847.8319 - mse: 46035624.0000 - mae: 4847.8320 - val_loss: 4151.4436 - val_mse: 30775892.0000 - val_mae: 4151.4438\n",
            "Epoch 16/500\n",
            "50/50 [==============================] - 0s 837us/step - loss: 4789.7166 - mse: 45598664.0000 - mae: 4789.7163 - val_loss: 4108.8414 - val_mse: 30460496.0000 - val_mae: 4108.8413\n",
            "Epoch 17/500\n",
            "50/50 [==============================] - 0s 813us/step - loss: 4792.8504 - mse: 45523352.0000 - mae: 4792.8501 - val_loss: 4070.0676 - val_mse: 30170368.0000 - val_mae: 4070.0676\n",
            "Epoch 18/500\n",
            "50/50 [==============================] - 0s 771us/step - loss: 4783.7147 - mse: 45104664.0000 - mae: 4783.7148 - val_loss: 4036.9515 - val_mse: 29908176.0000 - val_mae: 4036.9517\n",
            "Epoch 19/500\n",
            "50/50 [==============================] - 0s 719us/step - loss: 4773.7523 - mse: 45114972.0000 - mae: 4773.7520 - val_loss: 4005.0182 - val_mse: 29650388.0000 - val_mae: 4005.0183\n",
            "Epoch 20/500\n",
            "50/50 [==============================] - 0s 819us/step - loss: 4707.2188 - mse: 44174792.0000 - mae: 4707.2192 - val_loss: 3966.8796 - val_mse: 29333968.0000 - val_mae: 3966.8796\n",
            "Epoch 21/500\n",
            "50/50 [==============================] - 0s 887us/step - loss: 4684.3976 - mse: 44026352.0000 - mae: 4684.3975 - val_loss: 3932.1649 - val_mse: 29042160.0000 - val_mae: 3932.1648\n",
            "Epoch 22/500\n",
            "50/50 [==============================] - 0s 790us/step - loss: 4673.3690 - mse: 43633752.0000 - mae: 4673.3691 - val_loss: 3895.8319 - val_mse: 28716154.0000 - val_mae: 3895.8320\n",
            "Epoch 23/500\n",
            "50/50 [==============================] - 0s 666us/step - loss: 4639.7461 - mse: 43568432.0000 - mae: 4639.7461 - val_loss: 3858.2045 - val_mse: 28360768.0000 - val_mae: 3858.2046\n",
            "Epoch 24/500\n",
            "50/50 [==============================] - 0s 684us/step - loss: 4616.9225 - mse: 42914440.0000 - mae: 4616.9224 - val_loss: 3822.0846 - val_mse: 28003132.0000 - val_mae: 3822.0845\n",
            "Epoch 25/500\n",
            "50/50 [==============================] - 0s 657us/step - loss: 4553.1399 - mse: 41989656.0000 - mae: 4553.1401 - val_loss: 3786.5879 - val_mse: 27629420.0000 - val_mae: 3786.5879\n",
            "Epoch 26/500\n",
            "50/50 [==============================] - 0s 736us/step - loss: 4562.6895 - mse: 42031300.0000 - mae: 4562.6895 - val_loss: 3755.9706 - val_mse: 27270714.0000 - val_mae: 3755.9707\n",
            "Epoch 27/500\n",
            "50/50 [==============================] - 0s 704us/step - loss: 4520.1647 - mse: 41645352.0000 - mae: 4520.1646 - val_loss: 3729.5383 - val_mse: 26937028.0000 - val_mae: 3729.5383\n",
            "Epoch 28/500\n",
            "50/50 [==============================] - 0s 793us/step - loss: 4488.2404 - mse: 41340160.0000 - mae: 4488.2407 - val_loss: 3704.5295 - val_mse: 26585002.0000 - val_mae: 3704.5298\n",
            "Epoch 29/500\n",
            "50/50 [==============================] - 0s 628us/step - loss: 4491.6911 - mse: 40918420.0000 - mae: 4491.6914 - val_loss: 3678.5492 - val_mse: 26223906.0000 - val_mae: 3678.5493\n",
            "Epoch 30/500\n",
            "50/50 [==============================] - 0s 659us/step - loss: 4491.6810 - mse: 40768452.0000 - mae: 4491.6812 - val_loss: 3657.6500 - val_mse: 25935888.0000 - val_mae: 3657.6499\n",
            "Epoch 31/500\n",
            "50/50 [==============================] - 0s 722us/step - loss: 4438.3068 - mse: 39960884.0000 - mae: 4438.3066 - val_loss: 3630.2169 - val_mse: 25564708.0000 - val_mae: 3630.2168\n",
            "Epoch 32/500\n",
            "50/50 [==============================] - 0s 562us/step - loss: 4430.3673 - mse: 39498372.0000 - mae: 4430.3677 - val_loss: 3606.6488 - val_mse: 25248252.0000 - val_mae: 3606.6489\n",
            "Epoch 33/500\n",
            "50/50 [==============================] - 0s 658us/step - loss: 4482.0257 - mse: 39953828.0000 - mae: 4482.0259 - val_loss: 3591.2119 - val_mse: 25040828.0000 - val_mae: 3591.2117\n",
            "Epoch 34/500\n",
            "50/50 [==============================] - 0s 795us/step - loss: 4352.1366 - mse: 38796056.0000 - mae: 4352.1362 - val_loss: 3567.2981 - val_mse: 24727894.0000 - val_mae: 3567.2981\n",
            "Epoch 35/500\n",
            "50/50 [==============================] - 0s 705us/step - loss: 4379.1176 - mse: 39277096.0000 - mae: 4379.1177 - val_loss: 3547.3248 - val_mse: 24470192.0000 - val_mae: 3547.3250\n",
            "Epoch 36/500\n",
            "50/50 [==============================] - 0s 708us/step - loss: 4333.4237 - mse: 38413244.0000 - mae: 4333.4233 - val_loss: 3527.4385 - val_mse: 24214534.0000 - val_mae: 3527.4387\n",
            "Epoch 37/500\n",
            "50/50 [==============================] - 0s 683us/step - loss: 4328.2148 - mse: 37820320.0000 - mae: 4328.2148 - val_loss: 3501.1347 - val_mse: 23885228.0000 - val_mae: 3501.1343\n",
            "Epoch 38/500\n",
            "50/50 [==============================] - 0s 704us/step - loss: 4313.6659 - mse: 37011256.0000 - mae: 4313.6660 - val_loss: 3480.6604 - val_mse: 23628980.0000 - val_mae: 3480.6602\n",
            "Epoch 39/500\n",
            "50/50 [==============================] - 0s 956us/step - loss: 4385.1877 - mse: 37646756.0000 - mae: 4385.1880 - val_loss: 3468.2258 - val_mse: 23470368.0000 - val_mae: 3468.2258\n",
            "Epoch 40/500\n",
            "50/50 [==============================] - 0s 780us/step - loss: 4430.2805 - mse: 38666896.0000 - mae: 4430.2808 - val_loss: 3456.1338 - val_mse: 23318876.0000 - val_mae: 3456.1340\n",
            "Epoch 41/500\n",
            "50/50 [==============================] - 0s 847us/step - loss: 4318.8727 - mse: 36411092.0000 - mae: 4318.8730 - val_loss: 3443.0210 - val_mse: 23155220.0000 - val_mae: 3443.0210\n",
            "Epoch 42/500\n",
            "50/50 [==============================] - 0s 788us/step - loss: 4390.4664 - mse: 37858692.0000 - mae: 4390.4668 - val_loss: 3431.3665 - val_mse: 23011690.0000 - val_mae: 3431.3665\n",
            "Epoch 43/500\n",
            "50/50 [==============================] - 0s 751us/step - loss: 4312.0067 - mse: 36325336.0000 - mae: 4312.0068 - val_loss: 3418.9429 - val_mse: 22859172.0000 - val_mae: 3418.9429\n",
            "Epoch 44/500\n",
            "50/50 [==============================] - 0s 776us/step - loss: 4269.0074 - mse: 36096368.0000 - mae: 4269.0073 - val_loss: 3401.0861 - val_mse: 22610732.0000 - val_mae: 3401.0859\n",
            "Epoch 45/500\n",
            "50/50 [==============================] - 0s 733us/step - loss: 4264.7365 - mse: 36483040.0000 - mae: 4264.7363 - val_loss: 3389.9525 - val_mse: 22446522.0000 - val_mae: 3389.9526\n",
            "Epoch 46/500\n",
            "50/50 [==============================] - 0s 834us/step - loss: 4245.1526 - mse: 36556148.0000 - mae: 4245.1523 - val_loss: 3378.3619 - val_mse: 22265802.0000 - val_mae: 3378.3618\n",
            "Epoch 47/500\n",
            "50/50 [==============================] - 0s 707us/step - loss: 4168.0457 - mse: 35129996.0000 - mae: 4168.0459 - val_loss: 3362.6323 - val_mse: 22008222.0000 - val_mae: 3362.6321\n",
            "Epoch 48/500\n",
            "50/50 [==============================] - 0s 733us/step - loss: 4241.0886 - mse: 35749072.0000 - mae: 4241.0889 - val_loss: 3353.6759 - val_mse: 21862756.0000 - val_mae: 3353.6763\n",
            "Epoch 49/500\n",
            "50/50 [==============================] - 0s 755us/step - loss: 4242.8371 - mse: 35426420.0000 - mae: 4242.8374 - val_loss: 3344.5481 - val_mse: 21695196.0000 - val_mae: 3344.5481\n",
            "Epoch 50/500\n",
            "50/50 [==============================] - 0s 715us/step - loss: 4340.7577 - mse: 36700988.0000 - mae: 4340.7573 - val_loss: 3338.2161 - val_mse: 21578366.0000 - val_mae: 3338.2163\n",
            "Epoch 51/500\n",
            "50/50 [==============================] - 0s 746us/step - loss: 4233.8964 - mse: 35556228.0000 - mae: 4233.8965 - val_loss: 3329.4625 - val_mse: 21417542.0000 - val_mae: 3329.4624\n",
            "Epoch 52/500\n",
            "50/50 [==============================] - 0s 737us/step - loss: 4273.8409 - mse: 35581976.0000 - mae: 4273.8408 - val_loss: 3321.5549 - val_mse: 21273928.0000 - val_mae: 3321.5552\n",
            "Epoch 53/500\n",
            "50/50 [==============================] - 0s 860us/step - loss: 4219.2820 - mse: 33788468.0000 - mae: 4219.2817 - val_loss: 3312.8489 - val_mse: 21111900.0000 - val_mae: 3312.8489\n",
            "Epoch 54/500\n",
            "50/50 [==============================] - 0s 958us/step - loss: 4061.1284 - mse: 33329164.0000 - mae: 4061.1284 - val_loss: 3301.8308 - val_mse: 20875502.0000 - val_mae: 3301.8306\n",
            "Epoch 55/500\n",
            "50/50 [==============================] - 0s 721us/step - loss: 4184.7026 - mse: 34259040.0000 - mae: 4184.7026 - val_loss: 3292.9512 - val_mse: 20688246.0000 - val_mae: 3292.9512\n",
            "Epoch 56/500\n",
            "50/50 [==============================] - 0s 738us/step - loss: 4202.7309 - mse: 34383740.0000 - mae: 4202.7310 - val_loss: 3287.1356 - val_mse: 20539636.0000 - val_mae: 3287.1355\n",
            "Epoch 57/500\n",
            "50/50 [==============================] - 0s 719us/step - loss: 4178.3844 - mse: 34709900.0000 - mae: 4178.3843 - val_loss: 3280.2955 - val_mse: 20363528.0000 - val_mae: 3280.2954\n",
            "Epoch 58/500\n",
            "50/50 [==============================] - 0s 679us/step - loss: 4120.3438 - mse: 32920246.0000 - mae: 4120.3438 - val_loss: 3275.4811 - val_mse: 20250358.0000 - val_mae: 3275.4812\n",
            "Epoch 59/500\n",
            "50/50 [==============================] - 0s 929us/step - loss: 4248.9754 - mse: 34128188.0000 - mae: 4248.9756 - val_loss: 3269.8376 - val_mse: 20121066.0000 - val_mae: 3269.8374\n",
            "Epoch 60/500\n",
            "50/50 [==============================] - 0s 816us/step - loss: 4265.0350 - mse: 34184548.0000 - mae: 4265.0347 - val_loss: 3263.4027 - val_mse: 20007228.0000 - val_mae: 3263.4026\n",
            "Epoch 61/500\n",
            "50/50 [==============================] - 0s 730us/step - loss: 4192.8076 - mse: 33090852.0000 - mae: 4192.8071 - val_loss: 3218.7396 - val_mse: 19765300.0000 - val_mae: 3218.7397\n",
            "Epoch 62/500\n",
            "50/50 [==============================] - 0s 719us/step - loss: 4046.7129 - mse: 33326302.0000 - mae: 4046.7124 - val_loss: 3069.1667 - val_mse: 19177340.0000 - val_mae: 3069.1665\n",
            "Epoch 63/500\n",
            "50/50 [==============================] - 0s 762us/step - loss: 4067.7737 - mse: 32944112.0000 - mae: 4067.7737 - val_loss: 3070.3105 - val_mse: 18982604.0000 - val_mae: 3070.3105\n",
            "Epoch 64/500\n",
            "50/50 [==============================] - 0s 786us/step - loss: 3981.9325 - mse: 33303276.0000 - mae: 3981.9326 - val_loss: 3003.7048 - val_mse: 18557746.0000 - val_mae: 3003.7048\n",
            "Epoch 65/500\n",
            "50/50 [==============================] - 0s 813us/step - loss: 3749.1512 - mse: 30194286.0000 - mae: 3749.1514 - val_loss: 2986.2994 - val_mse: 18162872.0000 - val_mae: 2986.2993\n",
            "Epoch 66/500\n",
            "50/50 [==============================] - 0s 823us/step - loss: 3844.7276 - mse: 31489648.0000 - mae: 3844.7278 - val_loss: 2887.5032 - val_mse: 17610810.0000 - val_mae: 2887.5032\n",
            "Epoch 67/500\n",
            "50/50 [==============================] - 0s 730us/step - loss: 3800.8608 - mse: 31493358.0000 - mae: 3800.8608 - val_loss: 3200.3643 - val_mse: 17925272.0000 - val_mae: 3200.3645\n",
            "Epoch 68/500\n",
            "50/50 [==============================] - 0s 683us/step - loss: 3861.2940 - mse: 31090956.0000 - mae: 3861.2939 - val_loss: 3108.6514 - val_mse: 17396168.0000 - val_mae: 3108.6514\n",
            "Epoch 69/500\n",
            "50/50 [==============================] - 0s 674us/step - loss: 3655.5996 - mse: 28913210.0000 - mae: 3655.5996 - val_loss: 2741.4274 - val_mse: 16375368.0000 - val_mae: 2741.4275\n",
            "Epoch 70/500\n",
            "50/50 [==============================] - 0s 802us/step - loss: 3556.2973 - mse: 28848372.0000 - mae: 3556.2971 - val_loss: 2787.3216 - val_mse: 16086374.0000 - val_mae: 2787.3215\n",
            "Epoch 71/500\n",
            "50/50 [==============================] - 0s 800us/step - loss: 3407.8132 - mse: 26056200.0000 - mae: 3407.8132 - val_loss: 2656.0186 - val_mse: 15476413.0000 - val_mae: 2656.0186\n",
            "Epoch 72/500\n",
            "50/50 [==============================] - 0s 767us/step - loss: 3587.3442 - mse: 27413056.0000 - mae: 3587.3440 - val_loss: 2638.3813 - val_mse: 15077626.0000 - val_mae: 2638.3813\n",
            "Epoch 73/500\n",
            "50/50 [==============================] - 0s 829us/step - loss: 3505.2449 - mse: 26994130.0000 - mae: 3505.2451 - val_loss: 2927.1661 - val_mse: 15286114.0000 - val_mae: 2927.1663\n",
            "Epoch 74/500\n",
            "50/50 [==============================] - 0s 915us/step - loss: 3413.4613 - mse: 26206680.0000 - mae: 3413.4617 - val_loss: 2890.5900 - val_mse: 14880136.0000 - val_mae: 2890.5901\n",
            "Epoch 75/500\n",
            "50/50 [==============================] - 0s 716us/step - loss: 3365.2860 - mse: 26488028.0000 - mae: 3365.2859 - val_loss: 2615.2493 - val_mse: 13927509.0000 - val_mae: 2615.2493\n",
            "Epoch 76/500\n",
            "50/50 [==============================] - 0s 682us/step - loss: 3339.4146 - mse: 25487496.0000 - mae: 3339.4148 - val_loss: 2479.0389 - val_mse: 13357382.0000 - val_mae: 2479.0388\n",
            "Epoch 77/500\n",
            "50/50 [==============================] - 0s 723us/step - loss: 3262.7516 - mse: 23829726.0000 - mae: 3262.7520 - val_loss: 2607.5767 - val_mse: 13283653.0000 - val_mae: 2607.5767\n",
            "Epoch 78/500\n",
            "50/50 [==============================] - 0s 723us/step - loss: 3152.0682 - mse: 23135632.0000 - mae: 3152.0681 - val_loss: 2413.9291 - val_mse: 12628459.0000 - val_mae: 2413.9292\n",
            "Epoch 79/500\n",
            "50/50 [==============================] - 0s 842us/step - loss: 3174.1742 - mse: 22955508.0000 - mae: 3174.1741 - val_loss: 2540.5366 - val_mse: 12517738.0000 - val_mae: 2540.5366\n",
            "Epoch 80/500\n",
            "50/50 [==============================] - 0s 871us/step - loss: 3096.3414 - mse: 24116052.0000 - mae: 3096.3413 - val_loss: 2511.1487 - val_mse: 12153152.0000 - val_mae: 2511.1489\n",
            "Epoch 81/500\n",
            "50/50 [==============================] - 0s 761us/step - loss: 3110.0043 - mse: 23239782.0000 - mae: 3110.0044 - val_loss: 2406.7814 - val_mse: 11635408.0000 - val_mae: 2406.7815\n",
            "Epoch 82/500\n",
            "50/50 [==============================] - 0s 710us/step - loss: 3122.6524 - mse: 22620724.0000 - mae: 3122.6526 - val_loss: 2398.9479 - val_mse: 11354954.0000 - val_mae: 2398.9480\n",
            "Epoch 83/500\n",
            "50/50 [==============================] - 0s 744us/step - loss: 2996.9989 - mse: 21486712.0000 - mae: 2996.9990 - val_loss: 2507.6318 - val_mse: 11476809.0000 - val_mae: 2507.6318\n",
            "Epoch 84/500\n",
            "50/50 [==============================] - 0s 962us/step - loss: 3037.8732 - mse: 20639628.0000 - mae: 3037.8730 - val_loss: 2320.7464 - val_mse: 10694297.0000 - val_mae: 2320.7466\n",
            "Epoch 85/500\n",
            "50/50 [==============================] - 0s 748us/step - loss: 2865.5529 - mse: 19771986.0000 - mae: 2865.5527 - val_loss: 2265.0035 - val_mse: 10336499.0000 - val_mae: 2265.0034\n",
            "Epoch 86/500\n",
            "50/50 [==============================] - 0s 675us/step - loss: 2755.5079 - mse: 18334904.0000 - mae: 2755.5081 - val_loss: 2172.7224 - val_mse: 9926658.0000 - val_mae: 2172.7224\n",
            "Epoch 87/500\n",
            "50/50 [==============================] - 0s 704us/step - loss: 2789.7005 - mse: 18495696.0000 - mae: 2789.7007 - val_loss: 2130.1794 - val_mse: 9658093.0000 - val_mae: 2130.1792\n",
            "Epoch 88/500\n",
            "50/50 [==============================] - 0s 844us/step - loss: 2638.0590 - mse: 18077360.0000 - mae: 2638.0591 - val_loss: 2205.9458 - val_mse: 9624454.0000 - val_mae: 2205.9458\n",
            "Epoch 89/500\n",
            "50/50 [==============================] - 0s 820us/step - loss: 2890.3447 - mse: 20116504.0000 - mae: 2890.3447 - val_loss: 1954.6716 - val_mse: 8932144.0000 - val_mae: 1954.6716\n",
            "Epoch 90/500\n",
            "50/50 [==============================] - 0s 914us/step - loss: 2756.3673 - mse: 18899956.0000 - mae: 2756.3674 - val_loss: 1959.7959 - val_mse: 8685397.0000 - val_mae: 1959.7957\n",
            "Epoch 91/500\n",
            "50/50 [==============================] - 0s 730us/step - loss: 2716.0750 - mse: 19991224.0000 - mae: 2716.0750 - val_loss: 1966.9389 - val_mse: 8524468.0000 - val_mae: 1966.9391\n",
            "Epoch 92/500\n",
            "50/50 [==============================] - 0s 802us/step - loss: 2831.4314 - mse: 20128820.0000 - mae: 2831.4312 - val_loss: 2052.6554 - val_mse: 8537248.0000 - val_mae: 2052.6555\n",
            "Epoch 93/500\n",
            "50/50 [==============================] - 0s 742us/step - loss: 2626.2027 - mse: 18514128.0000 - mae: 2626.2029 - val_loss: 2049.0624 - val_mse: 8347321.5000 - val_mae: 2049.0625\n",
            "Epoch 94/500\n",
            "50/50 [==============================] - 0s 711us/step - loss: 2544.2589 - mse: 16601740.0000 - mae: 2544.2588 - val_loss: 1977.5856 - val_mse: 7983212.0000 - val_mae: 1977.5856\n",
            "Epoch 95/500\n",
            "50/50 [==============================] - 0s 684us/step - loss: 2413.4111 - mse: 14529184.0000 - mae: 2413.4109 - val_loss: 2016.9481 - val_mse: 7879393.0000 - val_mae: 2016.9480\n",
            "Epoch 96/500\n",
            "50/50 [==============================] - 0s 718us/step - loss: 2667.2581 - mse: 17904814.0000 - mae: 2667.2581 - val_loss: 1998.3290 - val_mse: 7702689.0000 - val_mae: 1998.3289\n",
            "Epoch 97/500\n",
            "50/50 [==============================] - 0s 700us/step - loss: 2446.5232 - mse: 15075569.0000 - mae: 2446.5232 - val_loss: 1971.2393 - val_mse: 7414718.5000 - val_mae: 1971.2393\n",
            "Epoch 98/500\n",
            "50/50 [==============================] - 0s 802us/step - loss: 2613.4604 - mse: 15728879.0000 - mae: 2613.4604 - val_loss: 1894.1053 - val_mse: 7062545.0000 - val_mae: 1894.1052\n",
            "Epoch 99/500\n",
            "50/50 [==============================] - 0s 734us/step - loss: 2612.7576 - mse: 16803550.0000 - mae: 2612.7576 - val_loss: 1812.5858 - val_mse: 6728929.5000 - val_mae: 1812.5857\n",
            "Epoch 100/500\n",
            "50/50 [==============================] - 0s 992us/step - loss: 2292.5677 - mse: 13483063.0000 - mae: 2292.5674 - val_loss: 1749.1155 - val_mse: 6417100.0000 - val_mae: 1749.1155\n",
            "Epoch 101/500\n",
            "50/50 [==============================] - 0s 728us/step - loss: 2317.7253 - mse: 15044483.0000 - mae: 2317.7253 - val_loss: 1825.5735 - val_mse: 6469857.0000 - val_mae: 1825.5735\n",
            "Epoch 102/500\n",
            "50/50 [==============================] - 0s 648us/step - loss: 2369.5866 - mse: 13835032.0000 - mae: 2369.5867 - val_loss: 1823.3278 - val_mse: 6312983.5000 - val_mae: 1823.3278\n",
            "Epoch 103/500\n",
            "50/50 [==============================] - 0s 625us/step - loss: 2385.5781 - mse: 14264146.0000 - mae: 2385.5779 - val_loss: 1779.0039 - val_mse: 6054285.0000 - val_mae: 1779.0039\n",
            "Epoch 104/500\n",
            "50/50 [==============================] - 0s 834us/step - loss: 2242.7825 - mse: 13471181.0000 - mae: 2242.7825 - val_loss: 1633.8612 - val_mse: 5683502.0000 - val_mae: 1633.8611\n",
            "Epoch 105/500\n",
            "50/50 [==============================] - 0s 750us/step - loss: 2312.0120 - mse: 12191665.0000 - mae: 2312.0120 - val_loss: 1775.1940 - val_mse: 5845919.0000 - val_mae: 1775.1941\n",
            "Epoch 106/500\n",
            "50/50 [==============================] - 0s 749us/step - loss: 2327.3081 - mse: 13169747.0000 - mae: 2327.3081 - val_loss: 1809.9054 - val_mse: 5833659.0000 - val_mae: 1809.9053\n",
            "Epoch 107/500\n",
            "50/50 [==============================] - 0s 801us/step - loss: 2418.0506 - mse: 14557274.0000 - mae: 2418.0505 - val_loss: 1721.9179 - val_mse: 5548513.0000 - val_mae: 1721.9177\n",
            "Epoch 108/500\n",
            "50/50 [==============================] - 0s 858us/step - loss: 2478.1987 - mse: 14381271.0000 - mae: 2478.1985 - val_loss: 1804.8360 - val_mse: 5672901.5000 - val_mae: 1804.8359\n",
            "Epoch 109/500\n",
            "50/50 [==============================] - 0s 994us/step - loss: 2219.6512 - mse: 11661315.0000 - mae: 2219.6511 - val_loss: 1768.6789 - val_mse: 5476487.5000 - val_mae: 1768.6790\n",
            "Epoch 110/500\n",
            "50/50 [==============================] - 0s 804us/step - loss: 2335.7131 - mse: 14277219.0000 - mae: 2335.7129 - val_loss: 1692.3230 - val_mse: 5180054.0000 - val_mae: 1692.3230\n",
            "Epoch 111/500\n",
            "50/50 [==============================] - 0s 751us/step - loss: 2371.8061 - mse: 13661888.0000 - mae: 2371.8062 - val_loss: 1782.8549 - val_mse: 5264602.5000 - val_mae: 1782.8549\n",
            "Epoch 112/500\n",
            "50/50 [==============================] - 0s 743us/step - loss: 2495.0644 - mse: 14915244.0000 - mae: 2495.0642 - val_loss: 1769.4586 - val_mse: 5121880.0000 - val_mae: 1769.4586\n",
            "Epoch 113/500\n",
            "50/50 [==============================] - 0s 785us/step - loss: 2446.4682 - mse: 14312154.0000 - mae: 2446.4680 - val_loss: 1741.9871 - val_mse: 4961669.0000 - val_mae: 1741.9871\n",
            "Epoch 114/500\n",
            "50/50 [==============================] - 0s 977us/step - loss: 2168.5352 - mse: 11168525.0000 - mae: 2168.5352 - val_loss: 1729.6317 - val_mse: 4811595.5000 - val_mae: 1729.6316\n",
            "Epoch 115/500\n",
            "50/50 [==============================] - 0s 718us/step - loss: 2049.4038 - mse: 10488972.0000 - mae: 2049.4038 - val_loss: 1749.2179 - val_mse: 4827063.5000 - val_mae: 1749.2180\n",
            "Epoch 116/500\n",
            "50/50 [==============================] - 0s 709us/step - loss: 2141.3678 - mse: 11377435.0000 - mae: 2141.3679 - val_loss: 1662.5232 - val_mse: 4541251.0000 - val_mae: 1662.5231\n",
            "Epoch 117/500\n",
            "50/50 [==============================] - 0s 757us/step - loss: 1965.2999 - mse: 10069286.0000 - mae: 1965.3000 - val_loss: 1723.1215 - val_mse: 4658502.0000 - val_mae: 1723.1215\n",
            "Epoch 118/500\n",
            "50/50 [==============================] - 0s 774us/step - loss: 1855.2371 - mse: 8567055.0000 - mae: 1855.2371 - val_loss: 1627.7499 - val_mse: 4401711.0000 - val_mae: 1627.7500\n",
            "Epoch 119/500\n",
            "50/50 [==============================] - 0s 653us/step - loss: 2059.0384 - mse: 10125284.0000 - mae: 2059.0381 - val_loss: 1581.4650 - val_mse: 4269485.0000 - val_mae: 1581.4651\n",
            "Epoch 120/500\n",
            "50/50 [==============================] - 0s 721us/step - loss: 2109.9007 - mse: 10586316.0000 - mae: 2109.9006 - val_loss: 1648.0891 - val_mse: 4332432.0000 - val_mae: 1648.0891\n",
            "Epoch 121/500\n",
            "50/50 [==============================] - 0s 896us/step - loss: 2239.8433 - mse: 12256172.0000 - mae: 2239.8433 - val_loss: 1686.0885 - val_mse: 4357017.0000 - val_mae: 1686.0885\n",
            "Epoch 122/500\n",
            "50/50 [==============================] - 0s 762us/step - loss: 2237.4844 - mse: 12437388.0000 - mae: 2237.4846 - val_loss: 1549.5694 - val_mse: 4070214.7500 - val_mae: 1549.5693\n",
            "Epoch 123/500\n",
            "50/50 [==============================] - 0s 730us/step - loss: 2269.2710 - mse: 12408726.0000 - mae: 2269.2710 - val_loss: 1562.7625 - val_mse: 4049519.2500 - val_mae: 1562.7625\n",
            "Epoch 124/500\n",
            "50/50 [==============================] - 0s 655us/step - loss: 2025.2504 - mse: 9213100.0000 - mae: 2025.2505 - val_loss: 1724.7471 - val_mse: 4279065.0000 - val_mae: 1724.7473\n",
            "Epoch 125/500\n",
            "50/50 [==============================] - 0s 695us/step - loss: 1999.8315 - mse: 9721962.0000 - mae: 1999.8314 - val_loss: 1649.5373 - val_mse: 4057686.0000 - val_mae: 1649.5374\n",
            "Epoch 126/500\n",
            "50/50 [==============================] - 0s 752us/step - loss: 2179.0482 - mse: 11783242.0000 - mae: 2179.0483 - val_loss: 1657.8974 - val_mse: 4019312.0000 - val_mae: 1657.8972\n",
            "Epoch 127/500\n",
            "50/50 [==============================] - 0s 699us/step - loss: 1981.6977 - mse: 11289234.0000 - mae: 1981.6976 - val_loss: 1611.6077 - val_mse: 3865490.0000 - val_mae: 1611.6077\n",
            "Epoch 128/500\n",
            "50/50 [==============================] - 0s 690us/step - loss: 2130.8961 - mse: 11690685.0000 - mae: 2130.8960 - val_loss: 1664.2824 - val_mse: 3941523.2500 - val_mae: 1664.2825\n",
            "Epoch 129/500\n",
            "50/50 [==============================] - 0s 661us/step - loss: 1825.2796 - mse: 7873744.5000 - mae: 1825.2795 - val_loss: 1597.5722 - val_mse: 3756172.7500 - val_mae: 1597.5721\n",
            "Epoch 130/500\n",
            "50/50 [==============================] - 0s 708us/step - loss: 2159.7074 - mse: 11098067.0000 - mae: 2159.7075 - val_loss: 1536.3200 - val_mse: 3643166.5000 - val_mae: 1536.3199\n",
            "Epoch 131/500\n",
            "50/50 [==============================] - 0s 714us/step - loss: 1816.9943 - mse: 8433119.0000 - mae: 1816.9944 - val_loss: 1614.2944 - val_mse: 3735784.5000 - val_mae: 1614.2943\n",
            "Epoch 132/500\n",
            "50/50 [==============================] - 0s 669us/step - loss: 2030.1049 - mse: 9244131.0000 - mae: 2030.1050 - val_loss: 1659.8968 - val_mse: 3786030.5000 - val_mae: 1659.8969\n",
            "Epoch 133/500\n",
            "50/50 [==============================] - 0s 717us/step - loss: 1937.2423 - mse: 8524346.0000 - mae: 1937.2423 - val_loss: 1567.8310 - val_mse: 3598355.2500 - val_mae: 1567.8311\n",
            "Epoch 134/500\n",
            "50/50 [==============================] - 0s 818us/step - loss: 2136.8600 - mse: 10545726.0000 - mae: 2136.8601 - val_loss: 1705.8887 - val_mse: 3824217.2500 - val_mae: 1705.8887\n",
            "Epoch 135/500\n",
            "50/50 [==============================] - 0s 876us/step - loss: 2017.3387 - mse: 9421116.0000 - mae: 2017.3387 - val_loss: 1610.2275 - val_mse: 3614272.0000 - val_mae: 1610.2275\n",
            "Epoch 136/500\n",
            "50/50 [==============================] - 0s 749us/step - loss: 1819.2247 - mse: 8233447.5000 - mae: 1819.2247 - val_loss: 1594.1360 - val_mse: 3550723.2500 - val_mae: 1594.1360\n",
            "Epoch 137/500\n",
            "50/50 [==============================] - 0s 716us/step - loss: 2012.8000 - mse: 10013076.0000 - mae: 2012.8000 - val_loss: 1610.0415 - val_mse: 3554920.5000 - val_mae: 1610.0415\n",
            "Epoch 138/500\n",
            "50/50 [==============================] - 0s 715us/step - loss: 1785.2594 - mse: 7825740.0000 - mae: 1785.2594 - val_loss: 1639.0788 - val_mse: 3563953.5000 - val_mae: 1639.0789\n",
            "Epoch 139/500\n",
            "50/50 [==============================] - 0s 701us/step - loss: 1727.8725 - mse: 7141599.5000 - mae: 1727.8726 - val_loss: 1565.9126 - val_mse: 3413437.5000 - val_mae: 1565.9126\n",
            "Epoch 140/500\n",
            "50/50 [==============================] - 0s 931us/step - loss: 1837.1274 - mse: 7601364.5000 - mae: 1837.1274 - val_loss: 1544.3748 - val_mse: 3351713.2500 - val_mae: 1544.3748\n",
            "Epoch 141/500\n",
            "50/50 [==============================] - 0s 811us/step - loss: 2155.6408 - mse: 11114476.0000 - mae: 2155.6409 - val_loss: 1553.7934 - val_mse: 3341914.2500 - val_mae: 1553.7933\n",
            "Epoch 142/500\n",
            "50/50 [==============================] - 0s 786us/step - loss: 1919.0661 - mse: 8669201.0000 - mae: 1919.0663 - val_loss: 1617.3662 - val_mse: 3425047.2500 - val_mae: 1617.3662\n",
            "Epoch 143/500\n",
            "50/50 [==============================] - 0s 817us/step - loss: 1690.8829 - mse: 6999208.5000 - mae: 1690.8828 - val_loss: 1541.0845 - val_mse: 3259677.2500 - val_mae: 1541.0845\n",
            "Epoch 144/500\n",
            "50/50 [==============================] - 0s 674us/step - loss: 1865.9167 - mse: 8520214.0000 - mae: 1865.9167 - val_loss: 1489.5063 - val_mse: 3150223.2500 - val_mae: 1489.5063\n",
            "Epoch 145/500\n",
            "50/50 [==============================] - 0s 689us/step - loss: 1793.0107 - mse: 9072199.0000 - mae: 1793.0107 - val_loss: 1465.4841 - val_mse: 3088494.5000 - val_mae: 1465.4841\n",
            "Epoch 146/500\n",
            "50/50 [==============================] - 0s 751us/step - loss: 1785.8627 - mse: 7529341.5000 - mae: 1785.8628 - val_loss: 1481.2596 - val_mse: 3068738.5000 - val_mae: 1481.2595\n",
            "Epoch 147/500\n",
            "50/50 [==============================] - 0s 869us/step - loss: 1843.4339 - mse: 7369762.0000 - mae: 1843.4340 - val_loss: 1388.2207 - val_mse: 2920378.2500 - val_mae: 1388.2207\n",
            "Epoch 148/500\n",
            "50/50 [==============================] - 0s 823us/step - loss: 1837.0765 - mse: 7618847.5000 - mae: 1837.0765 - val_loss: 1496.0324 - val_mse: 3039622.0000 - val_mae: 1496.0325\n",
            "Epoch 149/500\n",
            "50/50 [==============================] - 0s 755us/step - loss: 1748.7240 - mse: 7380987.0000 - mae: 1748.7241 - val_loss: 1426.7942 - val_mse: 2898964.5000 - val_mae: 1426.7942\n",
            "Epoch 150/500\n",
            "50/50 [==============================] - 0s 747us/step - loss: 1651.4031 - mse: 7214951.0000 - mae: 1651.4031 - val_loss: 1392.6421 - val_mse: 2756986.7500 - val_mae: 1392.6422\n",
            "Epoch 151/500\n",
            "50/50 [==============================] - 0s 753us/step - loss: 1750.0910 - mse: 7273312.5000 - mae: 1750.0909 - val_loss: 1341.1122 - val_mse: 2612968.5000 - val_mae: 1341.1122\n",
            "Epoch 152/500\n",
            "50/50 [==============================] - 0s 817us/step - loss: 1664.9719 - mse: 6918848.5000 - mae: 1664.9719 - val_loss: 1418.9930 - val_mse: 2697293.0000 - val_mae: 1418.9929\n",
            "Epoch 153/500\n",
            "50/50 [==============================] - 0s 666us/step - loss: 1635.9969 - mse: 6959461.0000 - mae: 1635.9968 - val_loss: 1350.2494 - val_mse: 2634071.5000 - val_mae: 1350.2493\n",
            "Epoch 154/500\n",
            "50/50 [==============================] - 0s 731us/step - loss: 1803.7119 - mse: 7364363.5000 - mae: 1803.7119 - val_loss: 1280.8022 - val_mse: 2509664.5000 - val_mae: 1280.8021\n",
            "Epoch 155/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1780.2107 - mse: 8044683.5000 - mae: 1780.2108 - val_loss: 1265.7871 - val_mse: 2462788.7500 - val_mae: 1265.7871\n",
            "Epoch 156/500\n",
            "50/50 [==============================] - 0s 811us/step - loss: 1806.1531 - mse: 7736032.5000 - mae: 1806.1531 - val_loss: 1304.3159 - val_mse: 2434664.7500 - val_mae: 1304.3159\n",
            "Epoch 157/500\n",
            "50/50 [==============================] - 0s 755us/step - loss: 1632.7881 - mse: 7759408.5000 - mae: 1632.7881 - val_loss: 1351.7580 - val_mse: 2457517.7500 - val_mae: 1351.7581\n",
            "Epoch 158/500\n",
            "50/50 [==============================] - 0s 705us/step - loss: 1742.5763 - mse: 6419708.0000 - mae: 1742.5763 - val_loss: 1318.9225 - val_mse: 2375616.2500 - val_mae: 1318.9225\n",
            "Epoch 159/500\n",
            "50/50 [==============================] - 0s 892us/step - loss: 1793.8207 - mse: 6540395.5000 - mae: 1793.8208 - val_loss: 1282.4056 - val_mse: 2246851.5000 - val_mae: 1282.4055\n",
            "Epoch 160/500\n",
            "50/50 [==============================] - 0s 683us/step - loss: 1613.0736 - mse: 6066358.0000 - mae: 1613.0735 - val_loss: 1265.8344 - val_mse: 2140009.2500 - val_mae: 1265.8344\n",
            "Epoch 161/500\n",
            "50/50 [==============================] - 0s 699us/step - loss: 1445.2882 - mse: 4098426.0000 - mae: 1445.2881 - val_loss: 1216.4905 - val_mse: 2104519.2500 - val_mae: 1216.4905\n",
            "Epoch 162/500\n",
            "50/50 [==============================] - 0s 701us/step - loss: 1686.2613 - mse: 8195436.0000 - mae: 1686.2612 - val_loss: 1227.6202 - val_mse: 2089737.6250 - val_mae: 1227.6202\n",
            "Epoch 163/500\n",
            "50/50 [==============================] - 0s 714us/step - loss: 1506.5660 - mse: 5395626.0000 - mae: 1506.5659 - val_loss: 1123.1194 - val_mse: 1873343.6250 - val_mae: 1123.1194\n",
            "Epoch 164/500\n",
            "50/50 [==============================] - 0s 763us/step - loss: 1791.7406 - mse: 7365853.5000 - mae: 1791.7406 - val_loss: 1105.7263 - val_mse: 1760429.0000 - val_mae: 1105.7263\n",
            "Epoch 165/500\n",
            "50/50 [==============================] - 0s 697us/step - loss: 1691.1174 - mse: 6710929.5000 - mae: 1691.1173 - val_loss: 1075.9979 - val_mse: 1678646.3750 - val_mae: 1075.9979\n",
            "Epoch 166/500\n",
            "50/50 [==============================] - 0s 716us/step - loss: 1571.7032 - mse: 5799059.0000 - mae: 1571.7032 - val_loss: 1073.6542 - val_mse: 1703702.0000 - val_mae: 1073.6542\n",
            "Epoch 167/500\n",
            "50/50 [==============================] - 0s 735us/step - loss: 1586.6861 - mse: 5857785.5000 - mae: 1586.6860 - val_loss: 1121.8909 - val_mse: 1823844.7500 - val_mae: 1121.8909\n",
            "Epoch 168/500\n",
            "50/50 [==============================] - 0s 788us/step - loss: 1737.7342 - mse: 7074423.0000 - mae: 1737.7340 - val_loss: 1098.9188 - val_mse: 1831865.2500 - val_mae: 1098.9187\n",
            "Epoch 169/500\n",
            "50/50 [==============================] - 0s 824us/step - loss: 1725.9403 - mse: 6580274.5000 - mae: 1725.9403 - val_loss: 1054.8520 - val_mse: 1674182.2500 - val_mae: 1054.8521\n",
            "Epoch 170/500\n",
            "50/50 [==============================] - 0s 715us/step - loss: 1728.5797 - mse: 6375701.0000 - mae: 1728.5796 - val_loss: 1030.0054 - val_mse: 1613826.2500 - val_mae: 1030.0054\n",
            "Epoch 171/500\n",
            "50/50 [==============================] - 0s 998us/step - loss: 1571.9847 - mse: 4708751.0000 - mae: 1571.9847 - val_loss: 1012.0480 - val_mse: 1499087.1250 - val_mae: 1012.0480\n",
            "Epoch 172/500\n",
            "50/50 [==============================] - 0s 784us/step - loss: 1497.2063 - mse: 4647436.0000 - mae: 1497.2063 - val_loss: 1002.5399 - val_mse: 1493257.2500 - val_mae: 1002.5399\n",
            "Epoch 173/500\n",
            "50/50 [==============================] - 0s 790us/step - loss: 1719.2634 - mse: 6833636.0000 - mae: 1719.2634 - val_loss: 1008.4267 - val_mse: 1547015.1250 - val_mae: 1008.4266\n",
            "Epoch 174/500\n",
            "50/50 [==============================] - 0s 717us/step - loss: 1560.8604 - mse: 5574346.0000 - mae: 1560.8605 - val_loss: 933.6444 - val_mse: 1302982.2500 - val_mae: 933.6444\n",
            "Epoch 175/500\n",
            "50/50 [==============================] - 0s 806us/step - loss: 1620.4556 - mse: 6132830.5000 - mae: 1620.4556 - val_loss: 936.4737 - val_mse: 1300162.5000 - val_mae: 936.4738\n",
            "Epoch 176/500\n",
            "50/50 [==============================] - 0s 731us/step - loss: 1516.2838 - mse: 4803428.0000 - mae: 1516.2839 - val_loss: 914.5184 - val_mse: 1260165.5000 - val_mae: 914.5184\n",
            "Epoch 177/500\n",
            "50/50 [==============================] - 0s 799us/step - loss: 1551.3143 - mse: 6215918.0000 - mae: 1551.3143 - val_loss: 813.9333 - val_mse: 1108947.6250 - val_mae: 813.9333\n",
            "Epoch 178/500\n",
            "50/50 [==============================] - 0s 842us/step - loss: 1695.3330 - mse: 6764735.5000 - mae: 1695.3328 - val_loss: 872.7216 - val_mse: 1187359.2500 - val_mae: 872.7216\n",
            "Epoch 179/500\n",
            "50/50 [==============================] - 0s 819us/step - loss: 1689.7578 - mse: 7977016.5000 - mae: 1689.7578 - val_loss: 864.3086 - val_mse: 1158590.2500 - val_mae: 864.3086\n",
            "Epoch 180/500\n",
            "50/50 [==============================] - 0s 886us/step - loss: 1831.1989 - mse: 9116371.0000 - mae: 1831.1989 - val_loss: 832.5954 - val_mse: 1068405.2500 - val_mae: 832.5954\n",
            "Epoch 181/500\n",
            "50/50 [==============================] - 0s 772us/step - loss: 1725.2523 - mse: 7553940.5000 - mae: 1725.2523 - val_loss: 796.3167 - val_mse: 976340.3750 - val_mae: 796.3167\n",
            "Epoch 182/500\n",
            "50/50 [==============================] - 0s 780us/step - loss: 1544.1417 - mse: 5122749.0000 - mae: 1544.1417 - val_loss: 844.8232 - val_mse: 1136721.8750 - val_mae: 844.8232\n",
            "Epoch 183/500\n",
            "50/50 [==============================] - 0s 888us/step - loss: 1611.3064 - mse: 6638767.5000 - mae: 1611.3064 - val_loss: 825.1236 - val_mse: 1080464.8750 - val_mae: 825.1237\n",
            "Epoch 184/500\n",
            "50/50 [==============================] - 0s 751us/step - loss: 1745.9378 - mse: 6880110.5000 - mae: 1745.9379 - val_loss: 811.1010 - val_mse: 1058117.2500 - val_mae: 811.1010\n",
            "Epoch 185/500\n",
            "50/50 [==============================] - 0s 917us/step - loss: 1628.8618 - mse: 7119508.0000 - mae: 1628.8617 - val_loss: 758.5112 - val_mse: 996081.3750 - val_mae: 758.5112\n",
            "Epoch 186/500\n",
            "50/50 [==============================] - 0s 652us/step - loss: 1478.1740 - mse: 4972518.5000 - mae: 1478.1741 - val_loss: 866.3829 - val_mse: 1141115.2500 - val_mae: 866.3829\n",
            "Epoch 187/500\n",
            "50/50 [==============================] - 0s 639us/step - loss: 1567.4437 - mse: 6643732.5000 - mae: 1567.4437 - val_loss: 909.3915 - val_mse: 1189531.7500 - val_mae: 909.3915\n",
            "Epoch 188/500\n",
            "50/50 [==============================] - 0s 701us/step - loss: 1497.1091 - mse: 4845070.5000 - mae: 1497.1090 - val_loss: 911.3053 - val_mse: 1192254.3750 - val_mae: 911.3053\n",
            "Epoch 189/500\n",
            "50/50 [==============================] - 0s 739us/step - loss: 1729.4633 - mse: 6750391.0000 - mae: 1729.4633 - val_loss: 905.2433 - val_mse: 1217695.3750 - val_mae: 905.2433\n",
            "Epoch 190/500\n",
            "50/50 [==============================] - 0s 776us/step - loss: 1744.4577 - mse: 5990782.5000 - mae: 1744.4576 - val_loss: 949.5999 - val_mse: 1308520.3750 - val_mae: 949.5999\n",
            "Epoch 191/500\n",
            "50/50 [==============================] - 0s 771us/step - loss: 1412.0919 - mse: 4535167.5000 - mae: 1412.0919 - val_loss: 892.9692 - val_mse: 1220996.7500 - val_mae: 892.9691\n",
            "Epoch 192/500\n",
            "50/50 [==============================] - 0s 807us/step - loss: 1759.3572 - mse: 6191878.5000 - mae: 1759.3572 - val_loss: 888.1296 - val_mse: 1151308.0000 - val_mae: 888.1297\n",
            "Epoch 193/500\n",
            "50/50 [==============================] - 0s 734us/step - loss: 1605.6020 - mse: 6453220.5000 - mae: 1605.6022 - val_loss: 806.6397 - val_mse: 989828.1875 - val_mae: 806.6396\n",
            "Epoch 194/500\n",
            "50/50 [==============================] - 0s 791us/step - loss: 1306.5468 - mse: 3440430.7500 - mae: 1306.5468 - val_loss: 800.3893 - val_mse: 1003592.3750 - val_mae: 800.3893\n",
            "Epoch 195/500\n",
            "50/50 [==============================] - 0s 748us/step - loss: 1311.2670 - mse: 3748187.5000 - mae: 1311.2668 - val_loss: 792.0231 - val_mse: 978872.6250 - val_mae: 792.0231\n",
            "Epoch 196/500\n",
            "50/50 [==============================] - 0s 826us/step - loss: 1680.6847 - mse: 5970775.5000 - mae: 1680.6847 - val_loss: 851.7710 - val_mse: 1150273.2500 - val_mae: 851.7709\n",
            "Epoch 197/500\n",
            "50/50 [==============================] - 0s 733us/step - loss: 1657.0066 - mse: 6662139.0000 - mae: 1657.0066 - val_loss: 957.3514 - val_mse: 1329895.7500 - val_mae: 957.3514\n",
            "Epoch 198/500\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 1972.6630 - mse: 9168470.0000 - mae: 1972.6631 - val_loss: 872.2027 - val_mse: 1205899.7500 - val_mae: 872.2028\n",
            "Epoch 199/500\n",
            "50/50 [==============================] - 0s 726us/step - loss: 1246.2317 - mse: 3723789.0000 - mae: 1246.2317 - val_loss: 909.9939 - val_mse: 1249991.5000 - val_mae: 909.9940\n",
            "Epoch 200/500\n",
            "50/50 [==============================] - 0s 825us/step - loss: 1931.2203 - mse: 9313229.0000 - mae: 1931.2205 - val_loss: 873.1288 - val_mse: 1214013.7500 - val_mae: 873.1288\n",
            "Epoch 201/500\n",
            "50/50 [==============================] - 0s 832us/step - loss: 1720.7983 - mse: 6842565.0000 - mae: 1720.7985 - val_loss: 809.0261 - val_mse: 1020200.8125 - val_mae: 809.0261\n",
            "Epoch 202/500\n",
            "50/50 [==============================] - 0s 806us/step - loss: 1391.6285 - mse: 3713409.0000 - mae: 1391.6284 - val_loss: 862.9100 - val_mse: 1065191.7500 - val_mae: 862.9100\n",
            "Epoch 203/500\n",
            "50/50 [==============================] - 0s 758us/step - loss: 1365.5208 - mse: 4111116.7500 - mae: 1365.5210 - val_loss: 883.3244 - val_mse: 1088752.2500 - val_mae: 883.3244\n",
            "Epoch 204/500\n",
            "50/50 [==============================] - 0s 794us/step - loss: 1563.9581 - mse: 6679547.5000 - mae: 1563.9581 - val_loss: 763.6879 - val_mse: 902531.8125 - val_mae: 763.6879\n",
            "Epoch 205/500\n",
            "50/50 [==============================] - 0s 822us/step - loss: 1403.7944 - mse: 4698838.0000 - mae: 1403.7944 - val_loss: 774.0782 - val_mse: 991477.5000 - val_mae: 774.0782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60 samples, validate on 60 samples\n",
            "Epoch 1/500\n",
            "60/60 [==============================] - 2s 29ms/step - loss: 8366.7961 - mse: 142962704.0000 - mae: 8366.7959 - val_loss: 12068.7987 - val_mse: 298674304.0000 - val_mae: 12068.7979\n",
            "Epoch 2/500\n",
            "60/60 [==============================] - 0s 671us/step - loss: 8366.5840 - mse: 142960784.0000 - mae: 8366.5840 - val_loss: 12068.1345 - val_mse: 298667680.0000 - val_mae: 12068.1357\n",
            "Epoch 3/500\n",
            "60/60 [==============================] - 0s 599us/step - loss: 8364.8382 - mse: 142949600.0000 - mae: 8364.8379 - val_loss: 12064.1945 - val_mse: 298630560.0000 - val_mae: 12064.1943\n",
            "Epoch 4/500\n",
            "60/60 [==============================] - 0s 649us/step - loss: 8359.4850 - mse: 142911504.0000 - mae: 8359.4854 - val_loss: 12054.8656 - val_mse: 298539936.0000 - val_mae: 12054.8643\n",
            "Epoch 5/500\n",
            "60/60 [==============================] - 0s 613us/step - loss: 8351.2972 - mse: 142855136.0000 - mae: 8351.2979 - val_loss: 12044.8492 - val_mse: 298431616.0000 - val_mae: 12044.8486\n",
            "Epoch 6/500\n",
            "60/60 [==============================] - 0s 626us/step - loss: 8338.4250 - mse: 142757440.0000 - mae: 8338.4248 - val_loss: 12032.3451 - val_mse: 298272672.0000 - val_mae: 12032.3457\n",
            "Epoch 7/500\n",
            "60/60 [==============================] - 0s 647us/step - loss: 8328.6532 - mse: 142642704.0000 - mae: 8328.6533 - val_loss: 12018.3797 - val_mse: 298061088.0000 - val_mae: 12018.3789\n",
            "Epoch 8/500\n",
            "60/60 [==============================] - 0s 688us/step - loss: 8311.4880 - mse: 142460496.0000 - mae: 8311.4883 - val_loss: 12002.3926 - val_mse: 297796960.0000 - val_mae: 12002.3916\n",
            "Epoch 9/500\n",
            "60/60 [==============================] - 0s 698us/step - loss: 8296.5837 - mse: 142288400.0000 - mae: 8296.5840 - val_loss: 11983.7089 - val_mse: 297476384.0000 - val_mae: 11983.7080\n",
            "Epoch 10/500\n",
            "60/60 [==============================] - 0s 656us/step - loss: 8276.8164 - mse: 142008448.0000 - mae: 8276.8164 - val_loss: 11964.7738 - val_mse: 297133440.0000 - val_mae: 11964.7725\n",
            "Epoch 11/500\n",
            "60/60 [==============================] - 0s 775us/step - loss: 8257.2862 - mse: 141704896.0000 - mae: 8257.2861 - val_loss: 11943.6214 - val_mse: 296747872.0000 - val_mae: 11943.6211\n",
            "Epoch 12/500\n",
            "60/60 [==============================] - 0s 660us/step - loss: 8228.2201 - mse: 141357888.0000 - mae: 8228.2197 - val_loss: 11919.1351 - val_mse: 296293312.0000 - val_mae: 11919.1348\n",
            "Epoch 13/500\n",
            "60/60 [==============================] - 0s 638us/step - loss: 8214.2970 - mse: 141069456.0000 - mae: 8214.2969 - val_loss: 11895.0298 - val_mse: 295824672.0000 - val_mae: 11895.0293\n",
            "Epoch 14/500\n",
            "60/60 [==============================] - 0s 622us/step - loss: 8189.4027 - mse: 140566704.0000 - mae: 8189.4028 - val_loss: 11868.7290 - val_mse: 295303520.0000 - val_mae: 11868.7295\n",
            "Epoch 15/500\n",
            "60/60 [==============================] - 0s 610us/step - loss: 8162.6629 - mse: 140162208.0000 - mae: 8162.6631 - val_loss: 11839.7519 - val_mse: 294723232.0000 - val_mae: 11839.7520\n",
            "Epoch 16/500\n",
            "60/60 [==============================] - 0s 630us/step - loss: 8155.4351 - mse: 139939568.0000 - mae: 8155.4355 - val_loss: 11813.4464 - val_mse: 294177568.0000 - val_mae: 11813.4463\n",
            "Epoch 17/500\n",
            "60/60 [==============================] - 0s 616us/step - loss: 8137.2483 - mse: 139671952.0000 - mae: 8137.2480 - val_loss: 11785.1326 - val_mse: 293591168.0000 - val_mae: 11785.1318\n",
            "Epoch 18/500\n",
            "60/60 [==============================] - 0s 652us/step - loss: 8103.6516 - mse: 138969872.0000 - mae: 8103.6514 - val_loss: 11751.7000 - val_mse: 292847616.0000 - val_mae: 11751.7002\n",
            "Epoch 19/500\n",
            "60/60 [==============================] - 0s 716us/step - loss: 8092.0914 - mse: 138524560.0000 - mae: 8092.0913 - val_loss: 11715.3391 - val_mse: 291860640.0000 - val_mae: 11715.3398\n",
            "Epoch 20/500\n",
            "60/60 [==============================] - 0s 663us/step - loss: 8058.2813 - mse: 137880560.0000 - mae: 8058.2817 - val_loss: 11670.2787 - val_mse: 290613312.0000 - val_mae: 11670.2793\n",
            "Epoch 21/500\n",
            "60/60 [==============================] - 0s 727us/step - loss: 8007.1610 - mse: 136658288.0000 - mae: 8007.1611 - val_loss: 11627.0207 - val_mse: 289449440.0000 - val_mae: 11627.0205\n",
            "Epoch 22/500\n",
            "60/60 [==============================] - 0s 660us/step - loss: 7988.1997 - mse: 136111248.0000 - mae: 7988.1997 - val_loss: 11586.2616 - val_mse: 288438848.0000 - val_mae: 11586.2627\n",
            "Epoch 23/500\n",
            "60/60 [==============================] - 0s 650us/step - loss: 7990.3482 - mse: 135564368.0000 - mae: 7990.3481 - val_loss: 11548.1018 - val_mse: 287425600.0000 - val_mae: 11548.1025\n",
            "Epoch 24/500\n",
            "60/60 [==============================] - 0s 708us/step - loss: 7921.8674 - mse: 134435808.0000 - mae: 7921.8672 - val_loss: 11500.8977 - val_mse: 286174336.0000 - val_mae: 11500.8975\n",
            "Epoch 25/500\n",
            "60/60 [==============================] - 0s 643us/step - loss: 7878.1978 - mse: 133701136.0000 - mae: 7878.1973 - val_loss: 11456.8031 - val_mse: 285033312.0000 - val_mae: 11456.8027\n",
            "Epoch 26/500\n",
            "60/60 [==============================] - 0s 618us/step - loss: 7872.2473 - mse: 133320240.0000 - mae: 7872.2471 - val_loss: 11415.3955 - val_mse: 283932800.0000 - val_mae: 11415.3955\n",
            "Epoch 27/500\n",
            "60/60 [==============================] - 0s 614us/step - loss: 7825.7690 - mse: 132317168.0000 - mae: 7825.7686 - val_loss: 11376.4166 - val_mse: 282853216.0000 - val_mae: 11376.4170\n",
            "Epoch 28/500\n",
            "60/60 [==============================] - 0s 567us/step - loss: 7796.0825 - mse: 130976048.0000 - mae: 7796.0825 - val_loss: 11331.9073 - val_mse: 281480992.0000 - val_mae: 11331.9072\n",
            "Epoch 29/500\n",
            "60/60 [==============================] - 0s 629us/step - loss: 7718.3381 - mse: 129665584.0000 - mae: 7718.3379 - val_loss: 11289.5754 - val_mse: 280149408.0000 - val_mae: 11289.5752\n",
            "Epoch 30/500\n",
            "60/60 [==============================] - 0s 627us/step - loss: 7738.6196 - mse: 129575600.0000 - mae: 7738.6191 - val_loss: 11248.7732 - val_mse: 278807776.0000 - val_mae: 11248.7725\n",
            "Epoch 31/500\n",
            "60/60 [==============================] - 0s 710us/step - loss: 7755.6038 - mse: 130901264.0000 - mae: 7755.6040 - val_loss: 11214.9034 - val_mse: 277675520.0000 - val_mae: 11214.9033\n",
            "Epoch 32/500\n",
            "60/60 [==============================] - 0s 713us/step - loss: 7657.8635 - mse: 127290648.0000 - mae: 7657.8628 - val_loss: 11180.0934 - val_mse: 276507168.0000 - val_mae: 11180.0938\n",
            "Epoch 33/500\n",
            "60/60 [==============================] - 0s 859us/step - loss: 7615.0885 - mse: 127265976.0000 - mae: 7615.0884 - val_loss: 11139.7326 - val_mse: 275155776.0000 - val_mae: 11139.7324\n",
            "Epoch 34/500\n",
            "60/60 [==============================] - 0s 685us/step - loss: 7639.1964 - mse: 126994512.0000 - mae: 7639.1958 - val_loss: 11105.8599 - val_mse: 273967296.0000 - val_mae: 11105.8604\n",
            "Epoch 35/500\n",
            "60/60 [==============================] - 0s 658us/step - loss: 7533.9304 - mse: 125077848.0000 - mae: 7533.9302 - val_loss: 11068.6721 - val_mse: 272730272.0000 - val_mae: 11068.6729\n",
            "Epoch 36/500\n",
            "60/60 [==============================] - 0s 650us/step - loss: 7527.0627 - mse: 125588096.0000 - mae: 7527.0625 - val_loss: 11037.0923 - val_mse: 271680768.0000 - val_mae: 11037.0928\n",
            "Epoch 37/500\n",
            "60/60 [==============================] - 0s 684us/step - loss: 7492.9584 - mse: 124337016.0000 - mae: 7492.9580 - val_loss: 11000.0756 - val_mse: 270419488.0000 - val_mae: 11000.0752\n",
            "Epoch 38/500\n",
            "60/60 [==============================] - 0s 580us/step - loss: 7466.4264 - mse: 123572600.0000 - mae: 7466.4268 - val_loss: 10968.0980 - val_mse: 269358912.0000 - val_mae: 10968.0977\n",
            "Epoch 39/500\n",
            "60/60 [==============================] - 0s 671us/step - loss: 7553.2857 - mse: 122398232.0000 - mae: 7553.2861 - val_loss: 10946.6903 - val_mse: 268465952.0000 - val_mae: 10946.6895\n",
            "Epoch 40/500\n",
            "60/60 [==============================] - 0s 646us/step - loss: 7429.9605 - mse: 120683336.0000 - mae: 7429.9604 - val_loss: 10901.4281 - val_mse: 267135632.0000 - val_mae: 10901.4268\n",
            "Epoch 41/500\n",
            "60/60 [==============================] - 0s 611us/step - loss: 7511.3368 - mse: 123492376.0000 - mae: 7511.3369 - val_loss: 10878.4308 - val_mse: 266267712.0000 - val_mae: 10878.4316\n",
            "Epoch 42/500\n",
            "60/60 [==============================] - 0s 706us/step - loss: 7402.6490 - mse: 121103736.0000 - mae: 7402.6489 - val_loss: 10831.2664 - val_mse: 264887664.0000 - val_mae: 10831.2666\n",
            "Epoch 43/500\n",
            "60/60 [==============================] - 0s 719us/step - loss: 7357.4176 - mse: 119602664.0000 - mae: 7357.4175 - val_loss: 10796.7326 - val_mse: 263738864.0000 - val_mae: 10796.7324\n",
            "Epoch 44/500\n",
            "60/60 [==============================] - 0s 638us/step - loss: 7263.2493 - mse: 120249848.0000 - mae: 7263.2485 - val_loss: 10753.4693 - val_mse: 262380128.0000 - val_mae: 10753.4688\n",
            "Epoch 45/500\n",
            "60/60 [==============================] - 0s 655us/step - loss: 7399.7705 - mse: 120881784.0000 - mae: 7399.7710 - val_loss: 10725.4225 - val_mse: 261383440.0000 - val_mae: 10725.4219\n",
            "Epoch 46/500\n",
            "60/60 [==============================] - 0s 641us/step - loss: 7232.4451 - mse: 117194456.0000 - mae: 7232.4453 - val_loss: 10671.8005 - val_mse: 259835216.0000 - val_mae: 10671.7998\n",
            "Epoch 47/500\n",
            "60/60 [==============================] - 0s 660us/step - loss: 7221.0120 - mse: 116169256.0000 - mae: 7221.0127 - val_loss: 10635.6224 - val_mse: 258588384.0000 - val_mae: 10635.6230\n",
            "Epoch 48/500\n",
            "60/60 [==============================] - 0s 669us/step - loss: 7106.5085 - mse: 114098400.0000 - mae: 7106.5083 - val_loss: 10582.2675 - val_mse: 257009840.0000 - val_mae: 10582.2676\n",
            "Epoch 49/500\n",
            "60/60 [==============================] - 0s 750us/step - loss: 7134.2512 - mse: 113096264.0000 - mae: 7134.2515 - val_loss: 10535.9781 - val_mse: 255549520.0000 - val_mae: 10535.9785\n",
            "Epoch 50/500\n",
            "60/60 [==============================] - 0s 665us/step - loss: 7018.8048 - mse: 110726608.0000 - mae: 7018.8047 - val_loss: 10497.8120 - val_mse: 254150944.0000 - val_mae: 10497.8125\n",
            "Epoch 51/500\n",
            "60/60 [==============================] - 0s 674us/step - loss: 7051.0704 - mse: 112611328.0000 - mae: 7051.0703 - val_loss: 10438.9303 - val_mse: 252507616.0000 - val_mae: 10438.9307\n",
            "Epoch 52/500\n",
            "60/60 [==============================] - 0s 721us/step - loss: 6992.7178 - mse: 111082016.0000 - mae: 6992.7183 - val_loss: 10406.7964 - val_mse: 251177584.0000 - val_mae: 10406.7959\n",
            "Epoch 53/500\n",
            "60/60 [==============================] - 0s 743us/step - loss: 7017.6368 - mse: 110327944.0000 - mae: 7017.6362 - val_loss: 10341.5711 - val_mse: 249503536.0000 - val_mae: 10341.5713\n",
            "Epoch 54/500\n",
            "60/60 [==============================] - 0s 697us/step - loss: 6937.7610 - mse: 108906904.0000 - mae: 6937.7603 - val_loss: 10289.6342 - val_mse: 247966288.0000 - val_mae: 10289.6338\n",
            "Epoch 55/500\n",
            "60/60 [==============================] - 0s 600us/step - loss: 6909.1007 - mse: 109139760.0000 - mae: 6909.1006 - val_loss: 10237.0243 - val_mse: 246384848.0000 - val_mae: 10237.0244\n",
            "Epoch 56/500\n",
            "60/60 [==============================] - 0s 850us/step - loss: 6845.6702 - mse: 108083968.0000 - mae: 6845.6709 - val_loss: 10180.9731 - val_mse: 244752384.0000 - val_mae: 10180.9727\n",
            "Epoch 57/500\n",
            "60/60 [==============================] - 0s 692us/step - loss: 6805.3731 - mse: 106352384.0000 - mae: 6805.3740 - val_loss: 10134.1631 - val_mse: 243098352.0000 - val_mae: 10134.1631\n",
            "Epoch 58/500\n",
            "60/60 [==============================] - 0s 848us/step - loss: 6793.5849 - mse: 105488808.0000 - mae: 6793.5850 - val_loss: 10146.7120 - val_mse: 241544480.0000 - val_mae: 10146.7129\n",
            "Epoch 59/500\n",
            "60/60 [==============================] - 0s 714us/step - loss: 6759.3924 - mse: 104572336.0000 - mae: 6759.3921 - val_loss: 10023.0170 - val_mse: 239880512.0000 - val_mae: 10023.0166\n",
            "Epoch 60/500\n",
            "60/60 [==============================] - 0s 797us/step - loss: 6859.6608 - mse: 106374616.0000 - mae: 6859.6611 - val_loss: 10015.9148 - val_mse: 238420304.0000 - val_mae: 10015.9150\n",
            "Epoch 61/500\n",
            "60/60 [==============================] - 0s 850us/step - loss: 6597.3990 - mse: 101307056.0000 - mae: 6597.3989 - val_loss: 10036.3347 - val_mse: 237252016.0000 - val_mae: 10036.3350\n",
            "Epoch 62/500\n",
            "60/60 [==============================] - 0s 725us/step - loss: 6626.2112 - mse: 102685272.0000 - mae: 6626.2114 - val_loss: 9866.2363 - val_mse: 235214256.0000 - val_mae: 9866.2363\n",
            "Epoch 63/500\n",
            "60/60 [==============================] - 0s 664us/step - loss: 6466.0800 - mse: 100517848.0000 - mae: 6466.0796 - val_loss: 9892.9689 - val_mse: 233438000.0000 - val_mae: 9892.9688\n",
            "Epoch 64/500\n",
            "60/60 [==============================] - 0s 794us/step - loss: 6427.8158 - mse: 99850416.0000 - mae: 6427.8154 - val_loss: 9732.6273 - val_mse: 231543440.0000 - val_mae: 9732.6270\n",
            "Epoch 65/500\n",
            "60/60 [==============================] - 0s 743us/step - loss: 6452.8240 - mse: 98383608.0000 - mae: 6452.8237 - val_loss: 9666.9758 - val_mse: 229803680.0000 - val_mae: 9666.9756\n",
            "Epoch 66/500\n",
            "60/60 [==============================] - 0s 618us/step - loss: 6437.5959 - mse: 100338360.0000 - mae: 6437.5957 - val_loss: 9637.6045 - val_mse: 228230112.0000 - val_mae: 9637.6045\n",
            "Epoch 67/500\n",
            "60/60 [==============================] - 0s 653us/step - loss: 6336.7673 - mse: 96753752.0000 - mae: 6336.7676 - val_loss: 9579.3689 - val_mse: 226290448.0000 - val_mae: 9579.3691\n",
            "Epoch 68/500\n",
            "60/60 [==============================] - 0s 654us/step - loss: 6064.1160 - mse: 95139184.0000 - mae: 6064.1162 - val_loss: 9469.8259 - val_mse: 224301040.0000 - val_mae: 9469.8262\n",
            "Epoch 69/500\n",
            "60/60 [==============================] - 0s 674us/step - loss: 6409.2337 - mse: 95616432.0000 - mae: 6409.2334 - val_loss: 9407.5650 - val_mse: 222649744.0000 - val_mae: 9407.5645\n",
            "Epoch 70/500\n",
            "60/60 [==============================] - 0s 665us/step - loss: 6053.0375 - mse: 93051976.0000 - mae: 6053.0376 - val_loss: 9375.2273 - val_mse: 220802304.0000 - val_mae: 9375.2275\n",
            "Epoch 71/500\n",
            "60/60 [==============================] - 0s 642us/step - loss: 6010.4596 - mse: 90814920.0000 - mae: 6010.4595 - val_loss: 9273.7279 - val_mse: 218898368.0000 - val_mae: 9273.7275\n",
            "Epoch 72/500\n",
            "60/60 [==============================] - 0s 651us/step - loss: 6017.0476 - mse: 93376648.0000 - mae: 6017.0474 - val_loss: 9355.4531 - val_mse: 217336576.0000 - val_mae: 9355.4531\n",
            "Epoch 73/500\n",
            "60/60 [==============================] - 0s 744us/step - loss: 5820.8634 - mse: 88302640.0000 - mae: 5820.8638 - val_loss: 9137.4497 - val_mse: 215258544.0000 - val_mae: 9137.4502\n",
            "Epoch 74/500\n",
            "60/60 [==============================] - 0s 869us/step - loss: 5925.9899 - mse: 87744176.0000 - mae: 5925.9902 - val_loss: 9068.7836 - val_mse: 213409120.0000 - val_mae: 9068.7832\n",
            "Epoch 75/500\n",
            "60/60 [==============================] - 0s 597us/step - loss: 5884.8576 - mse: 88019624.0000 - mae: 5884.8574 - val_loss: 8996.8588 - val_mse: 211549488.0000 - val_mae: 8996.8584\n",
            "Epoch 76/500\n",
            "60/60 [==============================] - 0s 625us/step - loss: 5812.9724 - mse: 87691768.0000 - mae: 5812.9722 - val_loss: 8930.4485 - val_mse: 209674704.0000 - val_mae: 8930.4482\n",
            "Epoch 77/500\n",
            "60/60 [==============================] - 0s 671us/step - loss: 5782.2857 - mse: 86159464.0000 - mae: 5782.2861 - val_loss: 8913.2768 - val_mse: 207888896.0000 - val_mae: 8913.2773\n",
            "Epoch 78/500\n",
            "60/60 [==============================] - 0s 772us/step - loss: 5599.7442 - mse: 84344984.0000 - mae: 5599.7441 - val_loss: 8881.3734 - val_mse: 206140928.0000 - val_mae: 8881.3730\n",
            "Epoch 79/500\n",
            "60/60 [==============================] - 0s 723us/step - loss: 5485.3085 - mse: 81410696.0000 - mae: 5485.3091 - val_loss: 8986.9747 - val_mse: 204602080.0000 - val_mae: 8986.9746\n",
            "Epoch 80/500\n",
            "60/60 [==============================] - 0s 690us/step - loss: 5419.4723 - mse: 77273992.0000 - mae: 5419.4717 - val_loss: 8648.3912 - val_mse: 202076512.0000 - val_mae: 8648.3916\n",
            "Epoch 81/500\n",
            "60/60 [==============================] - 0s 661us/step - loss: 5375.0401 - mse: 77984856.0000 - mae: 5375.0400 - val_loss: 8727.8717 - val_mse: 200387872.0000 - val_mae: 8727.8721\n",
            "Epoch 82/500\n",
            "60/60 [==============================] - 0s 658us/step - loss: 5550.4076 - mse: 81591184.0000 - mae: 5550.4077 - val_loss: 8549.3601 - val_mse: 198341296.0000 - val_mae: 8549.3604\n",
            "Epoch 83/500\n",
            "60/60 [==============================] - 0s 685us/step - loss: 5561.0550 - mse: 82134304.0000 - mae: 5561.0552 - val_loss: 8412.0071 - val_mse: 196414384.0000 - val_mae: 8412.0068\n",
            "Epoch 84/500\n",
            "60/60 [==============================] - 0s 674us/step - loss: 5256.5298 - mse: 76151864.0000 - mae: 5256.5303 - val_loss: 8576.0034 - val_mse: 194777936.0000 - val_mae: 8576.0039\n",
            "Epoch 85/500\n",
            "60/60 [==============================] - 0s 968us/step - loss: 5107.9849 - mse: 70736584.0000 - mae: 5107.9849 - val_loss: 8374.7195 - val_mse: 192682896.0000 - val_mae: 8374.7197\n",
            "Epoch 86/500\n",
            "60/60 [==============================] - 0s 807us/step - loss: 5033.8087 - mse: 69245032.0000 - mae: 5033.8091 - val_loss: 8280.0200 - val_mse: 190945792.0000 - val_mae: 8280.0205\n",
            "Epoch 87/500\n",
            "60/60 [==============================] - 0s 717us/step - loss: 5073.5264 - mse: 69560904.0000 - mae: 5073.5264 - val_loss: 8541.9506 - val_mse: 189833104.0000 - val_mae: 8541.9502\n",
            "Epoch 88/500\n",
            "60/60 [==============================] - 0s 643us/step - loss: 5113.2158 - mse: 71837624.0000 - mae: 5113.2158 - val_loss: 8131.3036 - val_mse: 187807888.0000 - val_mae: 8131.3037\n",
            "Epoch 89/500\n",
            "60/60 [==============================] - 0s 663us/step - loss: 5009.9780 - mse: 73669200.0000 - mae: 5009.9780 - val_loss: 8082.6049 - val_mse: 186231056.0000 - val_mae: 8082.6050\n",
            "Epoch 90/500\n",
            "60/60 [==============================] - 0s 667us/step - loss: 5071.3131 - mse: 72953288.0000 - mae: 5071.3130 - val_loss: 8036.6662 - val_mse: 184792928.0000 - val_mae: 8036.6660\n",
            "Epoch 91/500\n",
            "60/60 [==============================] - 0s 682us/step - loss: 5105.9096 - mse: 73581008.0000 - mae: 5105.9092 - val_loss: 8078.3966 - val_mse: 183481632.0000 - val_mae: 8078.3970\n",
            "Epoch 92/500\n",
            "60/60 [==============================] - 0s 649us/step - loss: 4820.4241 - mse: 66901100.0000 - mae: 4820.4243 - val_loss: 8262.8728 - val_mse: 182116720.0000 - val_mae: 8262.8730\n",
            "Epoch 93/500\n",
            "60/60 [==============================] - 0s 716us/step - loss: 5279.9751 - mse: 74604816.0000 - mae: 5279.9756 - val_loss: 7918.5823 - val_mse: 180120928.0000 - val_mae: 7918.5825\n",
            "Epoch 94/500\n",
            "60/60 [==============================] - 0s 656us/step - loss: 4900.1244 - mse: 67542784.0000 - mae: 4900.1240 - val_loss: 7847.5224 - val_mse: 178124352.0000 - val_mae: 7847.5220\n",
            "Epoch 95/500\n",
            "60/60 [==============================] - 0s 675us/step - loss: 4923.9497 - mse: 72047328.0000 - mae: 4923.9502 - val_loss: 7851.5573 - val_mse: 176899728.0000 - val_mae: 7851.5571\n",
            "Epoch 96/500\n",
            "60/60 [==============================] - 0s 737us/step - loss: 4838.4120 - mse: 66269980.0000 - mae: 4838.4116 - val_loss: 7816.7991 - val_mse: 175278544.0000 - val_mae: 7816.7988\n",
            "Epoch 97/500\n",
            "60/60 [==============================] - 0s 846us/step - loss: 4591.1507 - mse: 62132916.0000 - mae: 4591.1509 - val_loss: 7840.4116 - val_mse: 173780016.0000 - val_mae: 7840.4116\n",
            "Epoch 98/500\n",
            "60/60 [==============================] - 0s 735us/step - loss: 4610.2906 - mse: 58173436.0000 - mae: 4610.2905 - val_loss: 7843.6227 - val_mse: 172274880.0000 - val_mae: 7843.6230\n",
            "Epoch 99/500\n",
            "60/60 [==============================] - 0s 725us/step - loss: 4590.7833 - mse: 61427376.0000 - mae: 4590.7832 - val_loss: 7661.9583 - val_mse: 170753456.0000 - val_mae: 7661.9585\n",
            "Epoch 100/500\n",
            "60/60 [==============================] - 0s 744us/step - loss: 4841.1259 - mse: 67278608.0000 - mae: 4841.1260 - val_loss: 7639.3927 - val_mse: 169278128.0000 - val_mae: 7639.3926\n",
            "Epoch 101/500\n",
            "60/60 [==============================] - 0s 875us/step - loss: 4550.3388 - mse: 62712312.0000 - mae: 4550.3384 - val_loss: 7584.0462 - val_mse: 167904032.0000 - val_mae: 7584.0464\n",
            "Epoch 102/500\n",
            "60/60 [==============================] - 0s 858us/step - loss: 4401.1146 - mse: 58474564.0000 - mae: 4401.1147 - val_loss: 7545.7629 - val_mse: 166473504.0000 - val_mae: 7545.7632\n",
            "Epoch 103/500\n",
            "60/60 [==============================] - 0s 772us/step - loss: 4483.3470 - mse: 58133684.0000 - mae: 4483.3467 - val_loss: 7514.9586 - val_mse: 165218304.0000 - val_mae: 7514.9590\n",
            "Epoch 104/500\n",
            "60/60 [==============================] - 0s 793us/step - loss: 4678.5705 - mse: 66049836.0000 - mae: 4678.5703 - val_loss: 7484.4122 - val_mse: 164324544.0000 - val_mae: 7484.4126\n",
            "Epoch 105/500\n",
            "60/60 [==============================] - 0s 793us/step - loss: 4883.7277 - mse: 64285624.0000 - mae: 4883.7280 - val_loss: 7465.3268 - val_mse: 163392480.0000 - val_mae: 7465.3271\n",
            "Epoch 106/500\n",
            "60/60 [==============================] - 0s 775us/step - loss: 4360.0631 - mse: 55306220.0000 - mae: 4360.0630 - val_loss: 7425.6200 - val_mse: 162102704.0000 - val_mae: 7425.6196\n",
            "Epoch 107/500\n",
            "60/60 [==============================] - 0s 713us/step - loss: 4483.9056 - mse: 54933444.0000 - mae: 4483.9058 - val_loss: 7415.2989 - val_mse: 161272544.0000 - val_mae: 7415.2988\n",
            "Epoch 108/500\n",
            "60/60 [==============================] - 0s 796us/step - loss: 4447.4826 - mse: 59252688.0000 - mae: 4447.4824 - val_loss: 7472.4373 - val_mse: 160277952.0000 - val_mae: 7472.4375\n",
            "Epoch 109/500\n",
            "60/60 [==============================] - 0s 748us/step - loss: 4404.8686 - mse: 58630760.0000 - mae: 4404.8687 - val_loss: 7354.7985 - val_mse: 159104000.0000 - val_mae: 7354.7983\n",
            "Epoch 110/500\n",
            "60/60 [==============================] - 0s 827us/step - loss: 4268.8644 - mse: 56292132.0000 - mae: 4268.8643 - val_loss: 7403.3429 - val_mse: 158178592.0000 - val_mae: 7403.3433\n",
            "Epoch 111/500\n",
            "60/60 [==============================] - 0s 716us/step - loss: 4182.0857 - mse: 52120904.0000 - mae: 4182.0854 - val_loss: 7356.4498 - val_mse: 156933440.0000 - val_mae: 7356.4502\n",
            "Epoch 112/500\n",
            "60/60 [==============================] - 0s 733us/step - loss: 4397.4465 - mse: 54942408.0000 - mae: 4397.4468 - val_loss: 7239.2574 - val_mse: 155558608.0000 - val_mae: 7239.2573\n",
            "Epoch 113/500\n",
            "60/60 [==============================] - 0s 736us/step - loss: 4136.4858 - mse: 51711012.0000 - mae: 4136.4858 - val_loss: 7214.7925 - val_mse: 154410688.0000 - val_mae: 7214.7925\n",
            "Epoch 114/500\n",
            "60/60 [==============================] - 0s 645us/step - loss: 4370.8497 - mse: 53652196.0000 - mae: 4370.8501 - val_loss: 7181.1287 - val_mse: 153411584.0000 - val_mae: 7181.1284\n",
            "Epoch 115/500\n",
            "60/60 [==============================] - 0s 700us/step - loss: 4286.5646 - mse: 51440712.0000 - mae: 4286.5645 - val_loss: 7306.7059 - val_mse: 153029696.0000 - val_mae: 7306.7061\n",
            "Epoch 116/500\n",
            "60/60 [==============================] - 0s 710us/step - loss: 4077.4399 - mse: 51610036.0000 - mae: 4077.4397 - val_loss: 7123.9979 - val_mse: 151452112.0000 - val_mae: 7123.9980\n",
            "Epoch 117/500\n",
            "60/60 [==============================] - 0s 732us/step - loss: 3974.0159 - mse: 47323652.0000 - mae: 3974.0161 - val_loss: 7084.9520 - val_mse: 150541488.0000 - val_mae: 7084.9521\n",
            "Epoch 118/500\n",
            "60/60 [==============================] - 0s 692us/step - loss: 4314.5970 - mse: 54096320.0000 - mae: 4314.5972 - val_loss: 7060.5783 - val_mse: 149795664.0000 - val_mae: 7060.5781\n",
            "Epoch 119/500\n",
            "60/60 [==============================] - 0s 704us/step - loss: 4314.5436 - mse: 57842904.0000 - mae: 4314.5435 - val_loss: 7028.1649 - val_mse: 149106640.0000 - val_mae: 7028.1646\n",
            "Epoch 120/500\n",
            "60/60 [==============================] - 0s 863us/step - loss: 4007.5801 - mse: 48905708.0000 - mae: 4007.5796 - val_loss: 7084.3988 - val_mse: 148351424.0000 - val_mae: 7084.3989\n",
            "Epoch 121/500\n",
            "60/60 [==============================] - 0s 628us/step - loss: 3990.5152 - mse: 46294276.0000 - mae: 3990.5154 - val_loss: 6948.4228 - val_mse: 147168272.0000 - val_mae: 6948.4229\n",
            "Epoch 122/500\n",
            "60/60 [==============================] - 0s 787us/step - loss: 4248.1862 - mse: 52685436.0000 - mae: 4248.1865 - val_loss: 6951.7998 - val_mse: 146420944.0000 - val_mae: 6951.7998\n",
            "Epoch 123/500\n",
            "60/60 [==============================] - 0s 766us/step - loss: 4064.9263 - mse: 46877008.0000 - mae: 4064.9263 - val_loss: 6932.1364 - val_mse: 145634960.0000 - val_mae: 6932.1362\n",
            "Epoch 124/500\n",
            "60/60 [==============================] - 0s 670us/step - loss: 4042.1796 - mse: 48574104.0000 - mae: 4042.1794 - val_loss: 6897.0280 - val_mse: 144779792.0000 - val_mae: 6897.0283\n",
            "Epoch 125/500\n",
            "60/60 [==============================] - 0s 669us/step - loss: 4098.7028 - mse: 50967664.0000 - mae: 4098.7031 - val_loss: 6874.1500 - val_mse: 144110320.0000 - val_mae: 6874.1499\n",
            "Epoch 126/500\n",
            "60/60 [==============================] - 0s 726us/step - loss: 3943.8269 - mse: 48501736.0000 - mae: 3943.8271 - val_loss: 6968.5588 - val_mse: 143698208.0000 - val_mae: 6968.5591\n",
            "Epoch 127/500\n",
            "60/60 [==============================] - 0s 678us/step - loss: 3984.8822 - mse: 49070684.0000 - mae: 3984.8821 - val_loss: 6806.0059 - val_mse: 142779984.0000 - val_mae: 6806.0059\n",
            "Epoch 128/500\n",
            "60/60 [==============================] - 0s 703us/step - loss: 3868.3123 - mse: 44204764.0000 - mae: 3868.3125 - val_loss: 6792.4094 - val_mse: 141902336.0000 - val_mae: 6792.4092\n",
            "Epoch 129/500\n",
            "60/60 [==============================] - 0s 682us/step - loss: 3934.4435 - mse: 43596952.0000 - mae: 3934.4431 - val_loss: 6729.7070 - val_mse: 140823360.0000 - val_mae: 6729.7065\n",
            "Epoch 130/500\n",
            "60/60 [==============================] - 0s 678us/step - loss: 3977.8655 - mse: 46686388.0000 - mae: 3977.8657 - val_loss: 6805.0945 - val_mse: 140328752.0000 - val_mae: 6805.0947\n",
            "Epoch 131/500\n",
            "60/60 [==============================] - 0s 678us/step - loss: 3949.0767 - mse: 48258484.0000 - mae: 3949.0769 - val_loss: 6701.0547 - val_mse: 139450032.0000 - val_mae: 6701.0552\n",
            "Epoch 132/500\n",
            "60/60 [==============================] - 0s 731us/step - loss: 3864.1170 - mse: 43491876.0000 - mae: 3864.1174 - val_loss: 6658.6031 - val_mse: 138544928.0000 - val_mae: 6658.6030\n",
            "Epoch 133/500\n",
            "60/60 [==============================] - 0s 676us/step - loss: 4141.1586 - mse: 47811156.0000 - mae: 4141.1582 - val_loss: 6630.2267 - val_mse: 137963184.0000 - val_mae: 6630.2271\n",
            "Epoch 134/500\n",
            "60/60 [==============================] - 0s 679us/step - loss: 4017.7148 - mse: 44982136.0000 - mae: 4017.7146 - val_loss: 6645.6719 - val_mse: 137357792.0000 - val_mae: 6645.6719\n",
            "Epoch 135/500\n",
            "60/60 [==============================] - 0s 682us/step - loss: 4034.8243 - mse: 43614204.0000 - mae: 4034.8245 - val_loss: 6618.3579 - val_mse: 136636928.0000 - val_mae: 6618.3579\n",
            "Epoch 136/500\n",
            "60/60 [==============================] - 0s 686us/step - loss: 4269.2221 - mse: 53677788.0000 - mae: 4269.2217 - val_loss: 6546.8329 - val_mse: 135948224.0000 - val_mae: 6546.8330\n",
            "Epoch 137/500\n",
            "60/60 [==============================] - 0s 680us/step - loss: 3988.3350 - mse: 44508116.0000 - mae: 3988.3352 - val_loss: 6611.8611 - val_mse: 135316256.0000 - val_mae: 6611.8613\n",
            "Epoch 138/500\n",
            "60/60 [==============================] - 0s 618us/step - loss: 3661.1883 - mse: 35703832.0000 - mae: 3661.1880 - val_loss: 6507.3760 - val_mse: 134063928.0000 - val_mae: 6507.3760\n",
            "Epoch 139/500\n",
            "60/60 [==============================] - 0s 672us/step - loss: 4353.2253 - mse: 50966536.0000 - mae: 4353.2251 - val_loss: 6517.1146 - val_mse: 133522568.0000 - val_mae: 6517.1147\n",
            "Epoch 140/500\n",
            "60/60 [==============================] - 0s 717us/step - loss: 3936.6526 - mse: 42141348.0000 - mae: 3936.6526 - val_loss: 6486.1212 - val_mse: 132975208.0000 - val_mae: 6486.1206\n",
            "Epoch 141/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3904.5861 - mse: 39149580.0000 - mae: 3904.5862 - val_loss: 6467.9310 - val_mse: 132261248.0000 - val_mae: 6467.9312\n",
            "Epoch 142/500\n",
            "60/60 [==============================] - 0s 724us/step - loss: 3762.6471 - mse: 40104592.0000 - mae: 3762.6472 - val_loss: 6462.6947 - val_mse: 131491544.0000 - val_mae: 6462.6948\n",
            "Epoch 143/500\n",
            "60/60 [==============================] - 0s 800us/step - loss: 3908.1605 - mse: 39524820.0000 - mae: 3908.1604 - val_loss: 6452.9078 - val_mse: 131019912.0000 - val_mae: 6452.9077\n",
            "Epoch 144/500\n",
            "60/60 [==============================] - 0s 735us/step - loss: 3876.8695 - mse: 44863788.0000 - mae: 3876.8696 - val_loss: 6384.2434 - val_mse: 130384616.0000 - val_mae: 6384.2437\n",
            "Epoch 145/500\n",
            "60/60 [==============================] - 0s 775us/step - loss: 3796.5686 - mse: 49062200.0000 - mae: 3796.5688 - val_loss: 6429.4602 - val_mse: 130037296.0000 - val_mae: 6429.4604\n",
            "Epoch 146/500\n",
            "60/60 [==============================] - 0s 685us/step - loss: 4008.9569 - mse: 42882524.0000 - mae: 4008.9568 - val_loss: 6384.2391 - val_mse: 129342616.0000 - val_mae: 6384.2393\n",
            "Epoch 147/500\n",
            "60/60 [==============================] - 0s 717us/step - loss: 3864.0308 - mse: 46016588.0000 - mae: 3864.0308 - val_loss: 6550.6789 - val_mse: 129078400.0000 - val_mae: 6550.6792\n",
            "Epoch 148/500\n",
            "60/60 [==============================] - 0s 676us/step - loss: 3621.9993 - mse: 38331788.0000 - mae: 3621.9993 - val_loss: 6352.7080 - val_mse: 127478088.0000 - val_mae: 6352.7085\n",
            "Epoch 149/500\n",
            "60/60 [==============================] - 0s 706us/step - loss: 3790.2058 - mse: 43456472.0000 - mae: 3790.2058 - val_loss: 6330.1393 - val_mse: 127042080.0000 - val_mae: 6330.1396\n",
            "Epoch 150/500\n",
            "60/60 [==============================] - 0s 729us/step - loss: 3793.9982 - mse: 36481100.0000 - mae: 3793.9983 - val_loss: 6308.0061 - val_mse: 126289120.0000 - val_mae: 6308.0063\n",
            "Epoch 151/500\n",
            "60/60 [==============================] - 0s 713us/step - loss: 3482.8317 - mse: 34581596.0000 - mae: 3482.8318 - val_loss: 6281.3249 - val_mse: 125536768.0000 - val_mae: 6281.3252\n",
            "Epoch 152/500\n",
            "60/60 [==============================] - 0s 681us/step - loss: 3618.0645 - mse: 38245148.0000 - mae: 3618.0642 - val_loss: 6301.9485 - val_mse: 125119440.0000 - val_mae: 6301.9487\n",
            "Epoch 153/500\n",
            "60/60 [==============================] - 0s 728us/step - loss: 4070.9709 - mse: 45071940.0000 - mae: 4070.9709 - val_loss: 6292.4377 - val_mse: 124723848.0000 - val_mae: 6292.4375\n",
            "Epoch 154/500\n",
            "60/60 [==============================] - 0s 741us/step - loss: 3642.6749 - mse: 37598052.0000 - mae: 3642.6750 - val_loss: 6266.8987 - val_mse: 124252568.0000 - val_mae: 6266.8989\n",
            "Epoch 155/500\n",
            "60/60 [==============================] - 0s 756us/step - loss: 3772.3959 - mse: 38490220.0000 - mae: 3772.3958 - val_loss: 6297.0819 - val_mse: 123898480.0000 - val_mae: 6297.0815\n",
            "Epoch 156/500\n",
            "60/60 [==============================] - 0s 716us/step - loss: 3661.6453 - mse: 39603036.0000 - mae: 3661.6453 - val_loss: 6247.9493 - val_mse: 123043968.0000 - val_mae: 6247.9497\n",
            "Epoch 157/500\n",
            "60/60 [==============================] - 0s 696us/step - loss: 3643.9043 - mse: 38446264.0000 - mae: 3643.9048 - val_loss: 6300.3870 - val_mse: 122838744.0000 - val_mae: 6300.3872\n",
            "Epoch 158/500\n",
            "60/60 [==============================] - 0s 733us/step - loss: 3772.1823 - mse: 41619292.0000 - mae: 3772.1824 - val_loss: 6341.3385 - val_mse: 122586336.0000 - val_mae: 6341.3384\n",
            "Epoch 159/500\n",
            "60/60 [==============================] - 0s 814us/step - loss: 3712.8343 - mse: 38891632.0000 - mae: 3712.8342 - val_loss: 6212.1797 - val_mse: 121677168.0000 - val_mae: 6212.1797\n",
            "Epoch 160/500\n",
            "60/60 [==============================] - 0s 710us/step - loss: 3557.8155 - mse: 37001224.0000 - mae: 3557.8157 - val_loss: 6369.7919 - val_mse: 121715112.0000 - val_mae: 6369.7915\n",
            "Epoch 161/500\n",
            "60/60 [==============================] - 0s 647us/step - loss: 4082.5858 - mse: 46224264.0000 - mae: 4082.5857 - val_loss: 6189.9163 - val_mse: 120745312.0000 - val_mae: 6189.9165\n",
            "Epoch 162/500\n",
            "60/60 [==============================] - 0s 653us/step - loss: 3832.1423 - mse: 40751324.0000 - mae: 3832.1421 - val_loss: 6240.4664 - val_mse: 120651936.0000 - val_mae: 6240.4668\n",
            "Epoch 163/500\n",
            "60/60 [==============================] - 0s 601us/step - loss: 3346.2304 - mse: 32743790.0000 - mae: 3346.2305 - val_loss: 6191.6061 - val_mse: 119769320.0000 - val_mae: 6191.6064\n",
            "Epoch 164/500\n",
            "60/60 [==============================] - 0s 667us/step - loss: 4025.8642 - mse: 46602908.0000 - mae: 4025.8645 - val_loss: 6128.3536 - val_mse: 118984208.0000 - val_mae: 6128.3535\n",
            "Epoch 165/500\n",
            "60/60 [==============================] - 0s 754us/step - loss: 4095.9753 - mse: 44214604.0000 - mae: 4095.9751 - val_loss: 6208.1072 - val_mse: 119122408.0000 - val_mae: 6208.1074\n",
            "Epoch 166/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3220.8240 - mse: 30739844.0000 - mae: 3220.8240 - val_loss: 6161.1222 - val_mse: 118618016.0000 - val_mae: 6161.1226\n",
            "Epoch 167/500\n",
            "60/60 [==============================] - 0s 686us/step - loss: 3995.9650 - mse: 42278600.0000 - mae: 3995.9646 - val_loss: 6329.3399 - val_mse: 119039576.0000 - val_mae: 6329.3394\n",
            "Epoch 168/500\n",
            "60/60 [==============================] - 0s 742us/step - loss: 3757.2940 - mse: 35917048.0000 - mae: 3757.2939 - val_loss: 6132.0913 - val_mse: 117999160.0000 - val_mae: 6132.0918\n",
            "Epoch 169/500\n",
            "60/60 [==============================] - 0s 703us/step - loss: 3890.0668 - mse: 43912916.0000 - mae: 3890.0667 - val_loss: 6154.5941 - val_mse: 117658200.0000 - val_mae: 6154.5942\n",
            "Epoch 170/500\n",
            "60/60 [==============================] - 0s 777us/step - loss: 3820.4131 - mse: 39639236.0000 - mae: 3820.4131 - val_loss: 6111.1252 - val_mse: 117235896.0000 - val_mae: 6111.1250\n",
            "Epoch 171/500\n",
            "60/60 [==============================] - 0s 654us/step - loss: 4019.0866 - mse: 42605048.0000 - mae: 4019.0864 - val_loss: 6167.3873 - val_mse: 117338544.0000 - val_mae: 6167.3877\n",
            "Epoch 172/500\n",
            "60/60 [==============================] - 0s 666us/step - loss: 3639.8350 - mse: 33016446.0000 - mae: 3639.8350 - val_loss: 6176.9432 - val_mse: 116973312.0000 - val_mae: 6176.9434\n",
            "Epoch 173/500\n",
            "60/60 [==============================] - 0s 771us/step - loss: 4127.4058 - mse: 45445684.0000 - mae: 4127.4058 - val_loss: 6094.0481 - val_mse: 116241032.0000 - val_mae: 6094.0479\n",
            "Epoch 174/500\n",
            "60/60 [==============================] - 0s 800us/step - loss: 3984.5800 - mse: 43014660.0000 - mae: 3984.5803 - val_loss: 6075.9404 - val_mse: 115989176.0000 - val_mae: 6075.9404\n",
            "Epoch 175/500\n",
            "60/60 [==============================] - 0s 902us/step - loss: 3910.9686 - mse: 40563532.0000 - mae: 3910.9688 - val_loss: 6124.8070 - val_mse: 115634552.0000 - val_mae: 6124.8066\n",
            "Epoch 176/500\n",
            "60/60 [==============================] - 0s 824us/step - loss: 3241.2754 - mse: 29327062.0000 - mae: 3241.2751 - val_loss: 6078.3552 - val_mse: 115176752.0000 - val_mae: 6078.3550\n",
            "Epoch 177/500\n",
            "60/60 [==============================] - 0s 722us/step - loss: 4145.1323 - mse: 38746676.0000 - mae: 4145.1323 - val_loss: 6132.0834 - val_mse: 115469024.0000 - val_mae: 6132.0835\n",
            "Epoch 178/500\n",
            "60/60 [==============================] - 0s 659us/step - loss: 3701.8507 - mse: 34564524.0000 - mae: 3701.8506 - val_loss: 6151.6493 - val_mse: 115648936.0000 - val_mae: 6151.6494\n",
            "Epoch 179/500\n",
            "60/60 [==============================] - 0s 743us/step - loss: 3558.7189 - mse: 33734716.0000 - mae: 3558.7188 - val_loss: 6113.2064 - val_mse: 115270112.0000 - val_mae: 6113.2061\n",
            "Epoch 180/500\n",
            "60/60 [==============================] - 0s 751us/step - loss: 3972.0118 - mse: 41611972.0000 - mae: 3972.0120 - val_loss: 6080.1960 - val_mse: 115233976.0000 - val_mae: 6080.1958\n",
            "Epoch 181/500\n",
            "60/60 [==============================] - 0s 772us/step - loss: 4009.4515 - mse: 44559268.0000 - mae: 4009.4517 - val_loss: 6113.2645 - val_mse: 115270552.0000 - val_mae: 6113.2646\n",
            "Epoch 182/500\n",
            "60/60 [==============================] - 0s 804us/step - loss: 3775.0533 - mse: 33452206.0000 - mae: 3775.0532 - val_loss: 6074.5863 - val_mse: 114884432.0000 - val_mae: 6074.5864\n",
            "Epoch 183/500\n",
            "60/60 [==============================] - 0s 863us/step - loss: 3540.0452 - mse: 32861336.0000 - mae: 3540.0452 - val_loss: 6072.4237 - val_mse: 114182872.0000 - val_mae: 6072.4233\n",
            "Epoch 184/500\n",
            "60/60 [==============================] - 0s 821us/step - loss: 3918.9124 - mse: 41979544.0000 - mae: 3918.9124 - val_loss: 6101.5653 - val_mse: 114162856.0000 - val_mae: 6101.5654\n",
            "Epoch 185/500\n",
            "60/60 [==============================] - 0s 724us/step - loss: 3993.7048 - mse: 39892520.0000 - mae: 3993.7053 - val_loss: 6035.1404 - val_mse: 114177128.0000 - val_mae: 6035.1406\n",
            "Epoch 186/500\n",
            "60/60 [==============================] - 0s 651us/step - loss: 3632.7640 - mse: 37780332.0000 - mae: 3632.7642 - val_loss: 6055.6426 - val_mse: 114040744.0000 - val_mae: 6055.6426\n",
            "Epoch 187/500\n",
            "60/60 [==============================] - 0s 940us/step - loss: 3800.5443 - mse: 42366600.0000 - mae: 3800.5442 - val_loss: 6072.4993 - val_mse: 114170864.0000 - val_mae: 6072.4995\n",
            "Epoch 188/500\n",
            "60/60 [==============================] - 0s 768us/step - loss: 3612.8766 - mse: 36601128.0000 - mae: 3612.8765 - val_loss: 6058.4244 - val_mse: 113898384.0000 - val_mae: 6058.4243\n",
            "Epoch 189/500\n",
            "60/60 [==============================] - 0s 920us/step - loss: 3645.4476 - mse: 32080218.0000 - mae: 3645.4475 - val_loss: 6066.8967 - val_mse: 113650520.0000 - val_mae: 6066.8970\n",
            "Epoch 190/500\n",
            "60/60 [==============================] - 0s 740us/step - loss: 3372.7417 - mse: 30187950.0000 - mae: 3372.7417 - val_loss: 6022.7751 - val_mse: 113132648.0000 - val_mae: 6022.7749\n",
            "Epoch 191/500\n",
            "60/60 [==============================] - 0s 709us/step - loss: 3331.9661 - mse: 31437990.0000 - mae: 3331.9661 - val_loss: 5972.8403 - val_mse: 112763760.0000 - val_mae: 5972.8403\n",
            "Epoch 192/500\n",
            "60/60 [==============================] - 0s 615us/step - loss: 4000.2151 - mse: 39483472.0000 - mae: 4000.2151 - val_loss: 6103.4361 - val_mse: 112987576.0000 - val_mae: 6103.4360\n",
            "Epoch 193/500\n",
            "60/60 [==============================] - 0s 772us/step - loss: 3707.6227 - mse: 38636672.0000 - mae: 3707.6228 - val_loss: 6052.4309 - val_mse: 112751304.0000 - val_mae: 6052.4307\n",
            "Epoch 194/500\n",
            "60/60 [==============================] - 0s 823us/step - loss: 3177.7968 - mse: 28749590.0000 - mae: 3177.7969 - val_loss: 6033.6414 - val_mse: 112425008.0000 - val_mae: 6033.6416\n",
            "Epoch 195/500\n",
            "60/60 [==============================] - 0s 848us/step - loss: 3481.1838 - mse: 35586764.0000 - mae: 3481.1838 - val_loss: 5972.4782 - val_mse: 112155192.0000 - val_mae: 5972.4780\n",
            "Epoch 196/500\n",
            "60/60 [==============================] - 0s 699us/step - loss: 3878.5329 - mse: 39582668.0000 - mae: 3878.5330 - val_loss: 5952.9038 - val_mse: 112167192.0000 - val_mae: 5952.9038\n",
            "Epoch 197/500\n",
            "60/60 [==============================] - 0s 781us/step - loss: 3733.1635 - mse: 38779896.0000 - mae: 3733.1636 - val_loss: 5940.1466 - val_mse: 111992776.0000 - val_mae: 5940.1470\n",
            "Epoch 198/500\n",
            "60/60 [==============================] - 0s 760us/step - loss: 3592.9919 - mse: 35765376.0000 - mae: 3592.9917 - val_loss: 5945.0665 - val_mse: 111533304.0000 - val_mae: 5945.0669\n",
            "Epoch 199/500\n",
            "60/60 [==============================] - 0s 735us/step - loss: 3467.6003 - mse: 34810724.0000 - mae: 3467.6001 - val_loss: 5912.5178 - val_mse: 111367960.0000 - val_mae: 5912.5176\n",
            "Epoch 200/500\n",
            "60/60 [==============================] - 0s 724us/step - loss: 3653.2610 - mse: 39762844.0000 - mae: 3653.2612 - val_loss: 5910.0106 - val_mse: 111464072.0000 - val_mae: 5910.0103\n",
            "Epoch 201/500\n",
            "60/60 [==============================] - 0s 821us/step - loss: 3510.5067 - mse: 33688136.0000 - mae: 3510.5068 - val_loss: 5873.4335 - val_mse: 111229688.0000 - val_mae: 5873.4331\n",
            "Epoch 202/500\n",
            "60/60 [==============================] - 0s 819us/step - loss: 4081.7645 - mse: 44876156.0000 - mae: 4081.7646 - val_loss: 5926.9132 - val_mse: 111278840.0000 - val_mae: 5926.9131\n",
            "Epoch 203/500\n",
            "60/60 [==============================] - 0s 798us/step - loss: 3887.0560 - mse: 42922148.0000 - mae: 3887.0559 - val_loss: 5940.2260 - val_mse: 111390128.0000 - val_mae: 5940.2261\n",
            "Epoch 204/500\n",
            "60/60 [==============================] - 0s 664us/step - loss: 3707.8142 - mse: 37485492.0000 - mae: 3707.8147 - val_loss: 5868.2504 - val_mse: 110690984.0000 - val_mae: 5868.2505\n",
            "Epoch 205/500\n",
            "60/60 [==============================] - 0s 709us/step - loss: 3594.6514 - mse: 33859040.0000 - mae: 3594.6516 - val_loss: 5917.1498 - val_mse: 110531928.0000 - val_mae: 5917.1499\n",
            "Epoch 206/500\n",
            "60/60 [==============================] - 0s 691us/step - loss: 3328.2925 - mse: 29887138.0000 - mae: 3328.2925 - val_loss: 5967.5691 - val_mse: 110269144.0000 - val_mae: 5967.5688\n",
            "Epoch 207/500\n",
            "60/60 [==============================] - 0s 693us/step - loss: 3563.6836 - mse: 34483352.0000 - mae: 3563.6836 - val_loss: 5826.4238 - val_mse: 109642384.0000 - val_mae: 5826.4238\n",
            "Epoch 208/500\n",
            "60/60 [==============================] - 0s 822us/step - loss: 3600.6786 - mse: 37531684.0000 - mae: 3600.6787 - val_loss: 5880.2875 - val_mse: 109192944.0000 - val_mae: 5880.2876\n",
            "Epoch 209/500\n",
            "60/60 [==============================] - 0s 931us/step - loss: 3931.7156 - mse: 41070848.0000 - mae: 3931.7156 - val_loss: 5814.3386 - val_mse: 109305672.0000 - val_mae: 5814.3384\n",
            "Epoch 210/500\n",
            "60/60 [==============================] - 0s 667us/step - loss: 3654.4181 - mse: 37046108.0000 - mae: 3654.4180 - val_loss: 5867.8765 - val_mse: 109350248.0000 - val_mae: 5867.8765\n",
            "Epoch 211/500\n",
            "60/60 [==============================] - 0s 712us/step - loss: 3473.4967 - mse: 37325360.0000 - mae: 3473.4968 - val_loss: 5885.6863 - val_mse: 109394936.0000 - val_mae: 5885.6865\n",
            "Epoch 212/500\n",
            "60/60 [==============================] - 0s 762us/step - loss: 3297.3659 - mse: 26451430.0000 - mae: 3297.3657 - val_loss: 5798.1371 - val_mse: 109161128.0000 - val_mae: 5798.1377\n",
            "Epoch 213/500\n",
            "60/60 [==============================] - 0s 796us/step - loss: 3683.8595 - mse: 34689936.0000 - mae: 3683.8594 - val_loss: 5795.3787 - val_mse: 109163248.0000 - val_mae: 5795.3784\n",
            "Epoch 214/500\n",
            "60/60 [==============================] - 0s 745us/step - loss: 3397.8924 - mse: 30620460.0000 - mae: 3397.8921 - val_loss: 5774.7836 - val_mse: 109255528.0000 - val_mae: 5774.7832\n",
            "Epoch 215/500\n",
            "60/60 [==============================] - 0s 880us/step - loss: 3388.1332 - mse: 34676728.0000 - mae: 3388.1331 - val_loss: 5821.3220 - val_mse: 109370920.0000 - val_mae: 5821.3218\n",
            "Epoch 216/500\n",
            "60/60 [==============================] - 0s 808us/step - loss: 3291.8367 - mse: 34556172.0000 - mae: 3291.8367 - val_loss: 5771.6371 - val_mse: 108959144.0000 - val_mae: 5771.6372\n",
            "Epoch 217/500\n",
            "60/60 [==============================] - 0s 683us/step - loss: 3827.6796 - mse: 39804416.0000 - mae: 3827.6794 - val_loss: 5744.1667 - val_mse: 109044112.0000 - val_mae: 5744.1665\n",
            "Epoch 218/500\n",
            "60/60 [==============================] - 0s 773us/step - loss: 3491.4984 - mse: 33358390.0000 - mae: 3491.4985 - val_loss: 5700.9980 - val_mse: 108617568.0000 - val_mae: 5700.9980\n",
            "Epoch 219/500\n",
            "60/60 [==============================] - 0s 725us/step - loss: 3699.0182 - mse: 37730916.0000 - mae: 3699.0183 - val_loss: 5648.1216 - val_mse: 108424360.0000 - val_mae: 5648.1221\n",
            "Epoch 220/500\n",
            "60/60 [==============================] - 0s 727us/step - loss: 3340.2091 - mse: 29490338.0000 - mae: 3340.2092 - val_loss: 5649.3354 - val_mse: 108342176.0000 - val_mae: 5649.3354\n",
            "Epoch 221/500\n",
            "60/60 [==============================] - 0s 664us/step - loss: 3339.1846 - mse: 32849600.0000 - mae: 3339.1848 - val_loss: 5617.0168 - val_mse: 107733200.0000 - val_mae: 5617.0166\n",
            "Epoch 222/500\n",
            "60/60 [==============================] - 0s 700us/step - loss: 3531.8026 - mse: 33142186.0000 - mae: 3531.8022 - val_loss: 5553.2014 - val_mse: 107504040.0000 - val_mae: 5553.2017\n",
            "Epoch 223/500\n",
            "60/60 [==============================] - 0s 730us/step - loss: 3685.3003 - mse: 36367552.0000 - mae: 3685.3005 - val_loss: 5484.1679 - val_mse: 107234648.0000 - val_mae: 5484.1675\n",
            "Epoch 224/500\n",
            "60/60 [==============================] - 0s 703us/step - loss: 3280.0121 - mse: 33847540.0000 - mae: 3280.0122 - val_loss: 5458.4921 - val_mse: 106782264.0000 - val_mae: 5458.4917\n",
            "Epoch 225/500\n",
            "60/60 [==============================] - 0s 768us/step - loss: 3323.2312 - mse: 33737644.0000 - mae: 3323.2312 - val_loss: 5415.2432 - val_mse: 106130816.0000 - val_mae: 5415.2432\n",
            "Epoch 226/500\n",
            "60/60 [==============================] - 0s 669us/step - loss: 3065.6254 - mse: 26498764.0000 - mae: 3065.6255 - val_loss: 5337.9434 - val_mse: 105643176.0000 - val_mae: 5337.9438\n",
            "Epoch 227/500\n",
            "60/60 [==============================] - 0s 860us/step - loss: 3223.4835 - mse: 33636596.0000 - mae: 3223.4836 - val_loss: 5321.2867 - val_mse: 105100680.0000 - val_mae: 5321.2866\n",
            "Epoch 228/500\n",
            "60/60 [==============================] - 0s 779us/step - loss: 3028.7977 - mse: 27753656.0000 - mae: 3028.7976 - val_loss: 5289.1656 - val_mse: 104427888.0000 - val_mae: 5289.1655\n",
            "Epoch 229/500\n",
            "60/60 [==============================] - 0s 886us/step - loss: 3097.4764 - mse: 30323174.0000 - mae: 3097.4766 - val_loss: 5262.1387 - val_mse: 104158408.0000 - val_mae: 5262.1387\n",
            "Epoch 230/500\n",
            "60/60 [==============================] - 0s 833us/step - loss: 3013.7604 - mse: 27954894.0000 - mae: 3013.7607 - val_loss: 5256.7830 - val_mse: 103611536.0000 - val_mae: 5256.7832\n",
            "Epoch 231/500\n",
            "60/60 [==============================] - 0s 763us/step - loss: 3415.7205 - mse: 33312126.0000 - mae: 3415.7205 - val_loss: 5240.7643 - val_mse: 103402864.0000 - val_mae: 5240.7646\n",
            "Epoch 232/500\n",
            "60/60 [==============================] - 0s 857us/step - loss: 3233.6333 - mse: 29862042.0000 - mae: 3233.6333 - val_loss: 5340.8571 - val_mse: 103187848.0000 - val_mae: 5340.8574\n",
            "Epoch 233/500\n",
            "60/60 [==============================] - 0s 766us/step - loss: 3577.9895 - mse: 35452056.0000 - mae: 3577.9895 - val_loss: 5439.2705 - val_mse: 102940432.0000 - val_mae: 5439.2710\n",
            "Epoch 234/500\n",
            "60/60 [==============================] - 0s 700us/step - loss: 2991.5609 - mse: 28304980.0000 - mae: 2991.5608 - val_loss: 5433.7266 - val_mse: 102447728.0000 - val_mae: 5433.7271\n",
            "Epoch 235/500\n",
            "60/60 [==============================] - 0s 713us/step - loss: 3089.0165 - mse: 28211936.0000 - mae: 3089.0164 - val_loss: 5317.7143 - val_mse: 101626368.0000 - val_mae: 5317.7144\n",
            "Epoch 236/500\n",
            "60/60 [==============================] - 0s 830us/step - loss: 3059.3407 - mse: 27000750.0000 - mae: 3059.3408 - val_loss: 5342.6817 - val_mse: 101057920.0000 - val_mae: 5342.6816\n",
            "Epoch 237/500\n",
            "60/60 [==============================] - 0s 864us/step - loss: 2918.1312 - mse: 25484360.0000 - mae: 2918.1313 - val_loss: 5416.1872 - val_mse: 100729752.0000 - val_mae: 5416.1875\n",
            "Epoch 238/500\n",
            "60/60 [==============================] - 0s 866us/step - loss: 3059.1912 - mse: 28694766.0000 - mae: 3059.1912 - val_loss: 5258.5080 - val_mse: 100079088.0000 - val_mae: 5258.5083\n",
            "Epoch 239/500\n",
            "60/60 [==============================] - 0s 719us/step - loss: 3091.5148 - mse: 29242816.0000 - mae: 3091.5149 - val_loss: 5236.9847 - val_mse: 99662480.0000 - val_mae: 5236.9849\n",
            "Epoch 240/500\n",
            "60/60 [==============================] - 0s 687us/step - loss: 3124.0839 - mse: 27516618.0000 - mae: 3124.0842 - val_loss: 5236.8986 - val_mse: 99289136.0000 - val_mae: 5236.8989\n",
            "Epoch 241/500\n",
            "60/60 [==============================] - 0s 720us/step - loss: 2979.3228 - mse: 27772446.0000 - mae: 2979.3230 - val_loss: 5316.1065 - val_mse: 99138056.0000 - val_mae: 5316.1064\n",
            "Epoch 242/500\n",
            "60/60 [==============================] - 0s 759us/step - loss: 2859.6051 - mse: 22107388.0000 - mae: 2859.6052 - val_loss: 5169.1582 - val_mse: 98088368.0000 - val_mae: 5169.1582\n",
            "Epoch 243/500\n",
            "60/60 [==============================] - 0s 852us/step - loss: 3345.6571 - mse: 34229896.0000 - mae: 3345.6570 - val_loss: 5179.2104 - val_mse: 97980904.0000 - val_mae: 5179.2104\n",
            "Epoch 244/500\n",
            "60/60 [==============================] - 0s 787us/step - loss: 2835.7296 - mse: 24253948.0000 - mae: 2835.7297 - val_loss: 5221.4056 - val_mse: 97745624.0000 - val_mae: 5221.4058\n",
            "Epoch 245/500\n",
            "60/60 [==============================] - 0s 830us/step - loss: 2821.1585 - mse: 22378328.0000 - mae: 2821.1584 - val_loss: 5186.2713 - val_mse: 97385224.0000 - val_mae: 5186.2710\n",
            "Epoch 246/500\n",
            "60/60 [==============================] - 0s 681us/step - loss: 3288.0770 - mse: 29100288.0000 - mae: 3288.0771 - val_loss: 5316.5818 - val_mse: 97284048.0000 - val_mae: 5316.5825\n",
            "Epoch 247/500\n",
            "60/60 [==============================] - 0s 742us/step - loss: 2824.8100 - mse: 24668670.0000 - mae: 2824.8101 - val_loss: 5124.8761 - val_mse: 96302720.0000 - val_mae: 5124.8760\n",
            "Epoch 248/500\n",
            "60/60 [==============================] - 0s 755us/step - loss: 3321.0491 - mse: 33062424.0000 - mae: 3321.0493 - val_loss: 5153.2123 - val_mse: 95852896.0000 - val_mae: 5153.2124\n",
            "Epoch 249/500\n",
            "60/60 [==============================] - 0s 927us/step - loss: 2777.3017 - mse: 22311006.0000 - mae: 2777.3015 - val_loss: 5042.0821 - val_mse: 95119264.0000 - val_mae: 5042.0825\n",
            "Epoch 250/500\n",
            "60/60 [==============================] - 0s 986us/step - loss: 3059.9170 - mse: 27451398.0000 - mae: 3059.9170 - val_loss: 5029.2969 - val_mse: 94611840.0000 - val_mae: 5029.2969\n",
            "Epoch 251/500\n",
            "60/60 [==============================] - 0s 714us/step - loss: 3416.0502 - mse: 35503764.0000 - mae: 3416.0500 - val_loss: 5084.0871 - val_mse: 94472296.0000 - val_mae: 5084.0874\n",
            "Epoch 252/500\n",
            "60/60 [==============================] - 0s 629us/step - loss: 3333.4058 - mse: 31153954.0000 - mae: 3333.4058 - val_loss: 4967.1351 - val_mse: 93878040.0000 - val_mae: 4967.1353\n",
            "Epoch 253/500\n",
            "60/60 [==============================] - 0s 798us/step - loss: 2859.5335 - mse: 23413734.0000 - mae: 2859.5334 - val_loss: 5011.2228 - val_mse: 93399720.0000 - val_mae: 5011.2231\n",
            "Epoch 254/500\n",
            "60/60 [==============================] - 0s 773us/step - loss: 2816.4897 - mse: 23784234.0000 - mae: 2816.4895 - val_loss: 4992.1433 - val_mse: 92808296.0000 - val_mae: 4992.1431\n",
            "Epoch 255/500\n",
            "60/60 [==============================] - 0s 685us/step - loss: 2663.4920 - mse: 18029334.0000 - mae: 2663.4922 - val_loss: 4953.6951 - val_mse: 92211104.0000 - val_mae: 4953.6948\n",
            "Epoch 256/500\n",
            "60/60 [==============================] - 0s 810us/step - loss: 3056.7726 - mse: 27451874.0000 - mae: 3056.7727 - val_loss: 5007.6640 - val_mse: 91983344.0000 - val_mae: 5007.6636\n",
            "Epoch 257/500\n",
            "60/60 [==============================] - 0s 855us/step - loss: 3148.1234 - mse: 29176904.0000 - mae: 3148.1235 - val_loss: 4977.7847 - val_mse: 91803008.0000 - val_mae: 4977.7847\n",
            "Epoch 258/500\n",
            "60/60 [==============================] - 0s 803us/step - loss: 3420.1093 - mse: 34277104.0000 - mae: 3420.1094 - val_loss: 4903.8773 - val_mse: 91210832.0000 - val_mae: 4903.8770\n",
            "Epoch 259/500\n",
            "60/60 [==============================] - 0s 864us/step - loss: 2661.8736 - mse: 20623558.0000 - mae: 2661.8738 - val_loss: 4969.3050 - val_mse: 90807144.0000 - val_mae: 4969.3052\n",
            "Epoch 260/500\n",
            "60/60 [==============================] - 0s 928us/step - loss: 2622.5771 - mse: 20517282.0000 - mae: 2622.5771 - val_loss: 4876.9260 - val_mse: 90418864.0000 - val_mae: 4876.9263\n",
            "Epoch 261/500\n",
            "60/60 [==============================] - 0s 704us/step - loss: 2885.3870 - mse: 24302772.0000 - mae: 2885.3870 - val_loss: 4883.6937 - val_mse: 90014792.0000 - val_mae: 4883.6938\n",
            "Epoch 262/500\n",
            "60/60 [==============================] - 0s 787us/step - loss: 2883.6373 - mse: 24132690.0000 - mae: 2883.6375 - val_loss: 4892.2082 - val_mse: 89379840.0000 - val_mae: 4892.2085\n",
            "Epoch 263/500\n",
            "60/60 [==============================] - 0s 718us/step - loss: 3143.9672 - mse: 30533846.0000 - mae: 3143.9673 - val_loss: 4929.2829 - val_mse: 89254568.0000 - val_mae: 4929.2827\n",
            "Epoch 264/500\n",
            "60/60 [==============================] - 0s 927us/step - loss: 2986.6844 - mse: 26630718.0000 - mae: 2986.6843 - val_loss: 4798.3680 - val_mse: 88643888.0000 - val_mae: 4798.3677\n",
            "Epoch 265/500\n",
            "60/60 [==============================] - 0s 831us/step - loss: 2767.1689 - mse: 19963034.0000 - mae: 2767.1687 - val_loss: 4825.6548 - val_mse: 88404696.0000 - val_mae: 4825.6553\n",
            "Epoch 266/500\n",
            "60/60 [==============================] - 0s 873us/step - loss: 2725.3834 - mse: 24558824.0000 - mae: 2725.3833 - val_loss: 4801.1870 - val_mse: 87925552.0000 - val_mae: 4801.1870\n",
            "Epoch 267/500\n",
            "60/60 [==============================] - 0s 721us/step - loss: 2825.5508 - mse: 23048542.0000 - mae: 2825.5508 - val_loss: 4802.6688 - val_mse: 87844432.0000 - val_mae: 4802.6689\n",
            "Epoch 268/500\n",
            "60/60 [==============================] - 0s 741us/step - loss: 2833.5742 - mse: 20684760.0000 - mae: 2833.5742 - val_loss: 4816.7216 - val_mse: 87247088.0000 - val_mae: 4816.7217\n",
            "Epoch 269/500\n",
            "60/60 [==============================] - 0s 878us/step - loss: 2630.2960 - mse: 22512290.0000 - mae: 2630.2961 - val_loss: 4794.6055 - val_mse: 86817480.0000 - val_mae: 4794.6060\n",
            "Epoch 270/500\n",
            "60/60 [==============================] - 0s 954us/step - loss: 3045.8938 - mse: 27273042.0000 - mae: 3045.8938 - val_loss: 4752.2937 - val_mse: 86553576.0000 - val_mae: 4752.2939\n",
            "Epoch 271/500\n",
            "60/60 [==============================] - 0s 723us/step - loss: 3279.1960 - mse: 33152980.0000 - mae: 3279.1958 - val_loss: 4869.8335 - val_mse: 86653720.0000 - val_mae: 4869.8335\n",
            "Epoch 272/500\n",
            "60/60 [==============================] - 0s 813us/step - loss: 2841.3022 - mse: 21527262.0000 - mae: 2841.3020 - val_loss: 4772.5581 - val_mse: 86023816.0000 - val_mae: 4772.5581\n",
            "Epoch 273/500\n",
            "60/60 [==============================] - 0s 770us/step - loss: 2748.1551 - mse: 21175826.0000 - mae: 2748.1553 - val_loss: 4789.7823 - val_mse: 85958232.0000 - val_mae: 4789.7822\n",
            "Epoch 274/500\n",
            "60/60 [==============================] - 0s 719us/step - loss: 2894.5678 - mse: 26289920.0000 - mae: 2894.5676 - val_loss: 4754.8156 - val_mse: 85469648.0000 - val_mae: 4754.8154\n",
            "Epoch 275/500\n",
            "60/60 [==============================] - 0s 778us/step - loss: 2506.7137 - mse: 19473334.0000 - mae: 2506.7136 - val_loss: 4692.8588 - val_mse: 84841680.0000 - val_mae: 4692.8589\n",
            "Epoch 276/500\n",
            "60/60 [==============================] - 0s 732us/step - loss: 2916.2856 - mse: 26016442.0000 - mae: 2916.2856 - val_loss: 4690.8765 - val_mse: 84827032.0000 - val_mae: 4690.8765\n",
            "Epoch 277/500\n",
            "60/60 [==============================] - 0s 905us/step - loss: 2591.0378 - mse: 18296988.0000 - mae: 2591.0378 - val_loss: 4738.7047 - val_mse: 84645280.0000 - val_mae: 4738.7046\n",
            "Epoch 278/500\n",
            "60/60 [==============================] - 0s 838us/step - loss: 3103.6171 - mse: 30033228.0000 - mae: 3103.6172 - val_loss: 4682.2922 - val_mse: 84366368.0000 - val_mae: 4682.2920\n",
            "Epoch 279/500\n",
            "60/60 [==============================] - 0s 807us/step - loss: 2975.3605 - mse: 24430972.0000 - mae: 2975.3604 - val_loss: 4748.2265 - val_mse: 84097456.0000 - val_mae: 4748.2266\n",
            "Epoch 280/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2855.4543 - mse: 21718784.0000 - mae: 2855.4541 - val_loss: 4649.9804 - val_mse: 83748088.0000 - val_mae: 4649.9800\n",
            "Epoch 281/500\n",
            "60/60 [==============================] - 0s 868us/step - loss: 2923.8086 - mse: 24060804.0000 - mae: 2923.8088 - val_loss: 4690.1990 - val_mse: 83794816.0000 - val_mae: 4690.1987\n",
            "Epoch 282/500\n",
            "60/60 [==============================] - 0s 711us/step - loss: 3223.2159 - mse: 31335762.0000 - mae: 3223.2158 - val_loss: 4691.5437 - val_mse: 84033688.0000 - val_mae: 4691.5439\n",
            "Epoch 283/500\n",
            "60/60 [==============================] - 0s 851us/step - loss: 2889.5594 - mse: 24162936.0000 - mae: 2889.5593 - val_loss: 4739.5118 - val_mse: 84177168.0000 - val_mae: 4739.5122\n",
            "Epoch 284/500\n",
            "60/60 [==============================] - 0s 748us/step - loss: 2866.0086 - mse: 23539882.0000 - mae: 2866.0083 - val_loss: 4724.0036 - val_mse: 83821600.0000 - val_mae: 4724.0034\n",
            "Epoch 285/500\n",
            "60/60 [==============================] - 0s 807us/step - loss: 3183.0560 - mse: 31896462.0000 - mae: 3183.0562 - val_loss: 4666.3674 - val_mse: 83448968.0000 - val_mae: 4666.3672\n",
            "Epoch 286/500\n",
            "60/60 [==============================] - 0s 764us/step - loss: 2420.0222 - mse: 19877238.0000 - mae: 2420.0222 - val_loss: 4675.7581 - val_mse: 83188616.0000 - val_mae: 4675.7583\n",
            "Epoch 287/500\n",
            "60/60 [==============================] - 0s 731us/step - loss: 2576.9585 - mse: 18974362.0000 - mae: 2576.9583 - val_loss: 4639.8317 - val_mse: 82652544.0000 - val_mae: 4639.8315\n",
            "Epoch 288/500\n",
            "60/60 [==============================] - 0s 749us/step - loss: 2874.6974 - mse: 24738938.0000 - mae: 2874.6975 - val_loss: 4649.6481 - val_mse: 82506976.0000 - val_mae: 4649.6479\n",
            "Epoch 289/500\n",
            "60/60 [==============================] - 0s 998us/step - loss: 3166.8203 - mse: 27458104.0000 - mae: 3166.8203 - val_loss: 4642.0076 - val_mse: 82281048.0000 - val_mae: 4642.0073\n",
            "Epoch 290/500\n",
            "60/60 [==============================] - 0s 806us/step - loss: 2801.9677 - mse: 23982310.0000 - mae: 2801.9678 - val_loss: 4644.4674 - val_mse: 81998408.0000 - val_mae: 4644.4673\n",
            "Epoch 291/500\n",
            "60/60 [==============================] - 0s 897us/step - loss: 3273.8291 - mse: 27005974.0000 - mae: 3273.8291 - val_loss: 4604.0604 - val_mse: 81938816.0000 - val_mae: 4604.0605\n",
            "Epoch 292/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2934.3011 - mse: 24370022.0000 - mae: 2934.3010 - val_loss: 4608.8256 - val_mse: 81860536.0000 - val_mae: 4608.8257\n",
            "Epoch 293/500\n",
            "60/60 [==============================] - 0s 966us/step - loss: 2896.1460 - mse: 22223084.0000 - mae: 2896.1458 - val_loss: 4624.6234 - val_mse: 81517288.0000 - val_mae: 4624.6235\n",
            "Epoch 294/500\n",
            "60/60 [==============================] - 0s 836us/step - loss: 2648.8376 - mse: 18894758.0000 - mae: 2648.8374 - val_loss: 4637.6603 - val_mse: 81230280.0000 - val_mae: 4637.6606\n",
            "Epoch 295/500\n",
            "60/60 [==============================] - 0s 768us/step - loss: 2956.9078 - mse: 25553428.0000 - mae: 2956.9077 - val_loss: 4574.0528 - val_mse: 81360184.0000 - val_mae: 4574.0527\n",
            "Epoch 296/500\n",
            "60/60 [==============================] - 0s 769us/step - loss: 2608.0245 - mse: 19633880.0000 - mae: 2608.0247 - val_loss: 4543.0518 - val_mse: 80793544.0000 - val_mae: 4543.0522\n",
            "Epoch 297/500\n",
            "60/60 [==============================] - 0s 818us/step - loss: 2898.6976 - mse: 21977692.0000 - mae: 2898.6975 - val_loss: 4648.8039 - val_mse: 80645584.0000 - val_mae: 4648.8042\n",
            "Epoch 298/500\n",
            "60/60 [==============================] - 0s 860us/step - loss: 3261.3009 - mse: 30805546.0000 - mae: 3261.3008 - val_loss: 4546.2549 - val_mse: 80622344.0000 - val_mae: 4546.2549\n",
            "Epoch 299/500\n",
            "60/60 [==============================] - 0s 877us/step - loss: 2631.9657 - mse: 18875592.0000 - mae: 2631.9656 - val_loss: 4538.8757 - val_mse: 80381624.0000 - val_mae: 4538.8755\n",
            "Epoch 300/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2452.9331 - mse: 14747913.0000 - mae: 2452.9333 - val_loss: 4569.1526 - val_mse: 80090912.0000 - val_mae: 4569.1528\n",
            "Epoch 301/500\n",
            "60/60 [==============================] - 0s 835us/step - loss: 2634.0056 - mse: 22523448.0000 - mae: 2634.0056 - val_loss: 4555.8122 - val_mse: 80253392.0000 - val_mae: 4555.8125\n",
            "Epoch 302/500\n",
            "60/60 [==============================] - 0s 901us/step - loss: 2736.0001 - mse: 23225460.0000 - mae: 2736.0000 - val_loss: 4543.0673 - val_mse: 79841592.0000 - val_mae: 4543.0674\n",
            "Epoch 303/500\n",
            "60/60 [==============================] - 0s 799us/step - loss: 2780.5276 - mse: 24582452.0000 - mae: 2780.5276 - val_loss: 4746.9491 - val_mse: 80088840.0000 - val_mae: 4746.9487\n",
            "Epoch 304/500\n",
            "60/60 [==============================] - 0s 754us/step - loss: 2677.4407 - mse: 19657852.0000 - mae: 2677.4407 - val_loss: 4507.0850 - val_mse: 79328088.0000 - val_mae: 4507.0854\n",
            "Epoch 305/500\n",
            "60/60 [==============================] - 0s 777us/step - loss: 3274.2583 - mse: 26917892.0000 - mae: 3274.2583 - val_loss: 4545.1110 - val_mse: 79606944.0000 - val_mae: 4545.1108\n",
            "Epoch 306/500\n",
            "60/60 [==============================] - 0s 908us/step - loss: 2729.5730 - mse: 23877624.0000 - mae: 2729.5732 - val_loss: 4571.8338 - val_mse: 79503512.0000 - val_mae: 4571.8340\n",
            "Epoch 307/500\n",
            "60/60 [==============================] - 0s 810us/step - loss: 2897.4649 - mse: 22533688.0000 - mae: 2897.4648 - val_loss: 4617.8241 - val_mse: 79361016.0000 - val_mae: 4617.8237\n",
            "Epoch 308/500\n",
            "60/60 [==============================] - 0s 781us/step - loss: 3071.1267 - mse: 26249330.0000 - mae: 3071.1265 - val_loss: 4664.4269 - val_mse: 79034608.0000 - val_mae: 4664.4272\n",
            "Epoch 309/500\n",
            "60/60 [==============================] - 0s 781us/step - loss: 2529.5508 - mse: 17474510.0000 - mae: 2529.5508 - val_loss: 4457.3949 - val_mse: 78420696.0000 - val_mae: 4457.3950\n",
            "Epoch 310/500\n",
            "60/60 [==============================] - 0s 962us/step - loss: 2645.8478 - mse: 20767580.0000 - mae: 2645.8479 - val_loss: 4508.2746 - val_mse: 78439720.0000 - val_mae: 4508.2744\n",
            "Epoch 311/500\n",
            "60/60 [==============================] - 0s 785us/step - loss: 2989.2229 - mse: 24362916.0000 - mae: 2989.2229 - val_loss: 4478.6241 - val_mse: 78206920.0000 - val_mae: 4478.6240\n",
            "Epoch 312/500\n",
            "60/60 [==============================] - 0s 770us/step - loss: 2956.5083 - mse: 20884924.0000 - mae: 2956.5081 - val_loss: 4511.2646 - val_mse: 78110176.0000 - val_mae: 4511.2646\n",
            "Epoch 313/500\n",
            "60/60 [==============================] - 0s 954us/step - loss: 2863.6142 - mse: 22801244.0000 - mae: 2863.6140 - val_loss: 4445.2855 - val_mse: 77708464.0000 - val_mae: 4445.2856\n",
            "Epoch 314/500\n",
            "60/60 [==============================] - 0s 723us/step - loss: 3107.7560 - mse: 27885924.0000 - mae: 3107.7556 - val_loss: 4515.5472 - val_mse: 77726064.0000 - val_mae: 4515.5474\n",
            "Epoch 315/500\n",
            "60/60 [==============================] - 0s 754us/step - loss: 2533.0067 - mse: 16706165.0000 - mae: 2533.0068 - val_loss: 4522.8881 - val_mse: 77538312.0000 - val_mae: 4522.8882\n",
            "Epoch 316/500\n",
            "60/60 [==============================] - 0s 783us/step - loss: 2791.4318 - mse: 22007860.0000 - mae: 2791.4319 - val_loss: 4452.8059 - val_mse: 77295280.0000 - val_mae: 4452.8062\n",
            "Epoch 317/500\n",
            "60/60 [==============================] - 0s 761us/step - loss: 2989.8411 - mse: 19240934.0000 - mae: 2989.8411 - val_loss: 4412.2730 - val_mse: 77075576.0000 - val_mae: 4412.2729\n",
            "Epoch 318/500\n",
            "60/60 [==============================] - 0s 962us/step - loss: 2219.1569 - mse: 14275482.0000 - mae: 2219.1567 - val_loss: 4406.1822 - val_mse: 76684488.0000 - val_mae: 4406.1821\n",
            "Epoch 319/500\n",
            "60/60 [==============================] - 0s 804us/step - loss: 3068.3608 - mse: 25691288.0000 - mae: 3068.3606 - val_loss: 4597.8064 - val_mse: 76945352.0000 - val_mae: 4597.8062\n",
            "Epoch 320/500\n",
            "60/60 [==============================] - 0s 715us/step - loss: 2703.9423 - mse: 20611202.0000 - mae: 2703.9424 - val_loss: 4424.8836 - val_mse: 76661096.0000 - val_mae: 4424.8833\n",
            "Epoch 321/500\n",
            "60/60 [==============================] - 0s 815us/step - loss: 2627.2306 - mse: 20798248.0000 - mae: 2627.2307 - val_loss: 4472.0393 - val_mse: 76623464.0000 - val_mae: 4472.0396\n",
            "Epoch 322/500\n",
            "60/60 [==============================] - 0s 774us/step - loss: 2892.0389 - mse: 22389990.0000 - mae: 2892.0391 - val_loss: 4483.5779 - val_mse: 76667096.0000 - val_mae: 4483.5781\n",
            "Epoch 323/500\n",
            "60/60 [==============================] - 0s 812us/step - loss: 2613.2698 - mse: 19580418.0000 - mae: 2613.2698 - val_loss: 4437.3351 - val_mse: 76558768.0000 - val_mae: 4437.3350\n",
            "Epoch 324/500\n",
            "60/60 [==============================] - 0s 711us/step - loss: 2857.7143 - mse: 21973834.0000 - mae: 2857.7144 - val_loss: 4476.3712 - val_mse: 76315096.0000 - val_mae: 4476.3706\n",
            "Epoch 325/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2727.4364 - mse: 20752588.0000 - mae: 2727.4365 - val_loss: 4513.0366 - val_mse: 76084264.0000 - val_mae: 4513.0366\n",
            "Epoch 326/500\n",
            "60/60 [==============================] - 0s 716us/step - loss: 3031.4561 - mse: 25702532.0000 - mae: 3031.4563 - val_loss: 4388.7965 - val_mse: 75688312.0000 - val_mae: 4388.7964\n",
            "Epoch 327/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2779.8032 - mse: 22302836.0000 - mae: 2779.8032 - val_loss: 4366.8515 - val_mse: 75503744.0000 - val_mae: 4366.8516\n",
            "Epoch 328/500\n",
            "60/60 [==============================] - 0s 860us/step - loss: 3139.2504 - mse: 26947354.0000 - mae: 3139.2505 - val_loss: 4397.5329 - val_mse: 75592184.0000 - val_mae: 4397.5327\n",
            "Epoch 329/500\n",
            "60/60 [==============================] - 0s 722us/step - loss: 3253.3925 - mse: 27798718.0000 - mae: 3253.3928 - val_loss: 4423.9899 - val_mse: 75443664.0000 - val_mae: 4423.9902\n",
            "Epoch 330/500\n",
            "60/60 [==============================] - 0s 743us/step - loss: 3013.1522 - mse: 26526276.0000 - mae: 3013.1521 - val_loss: 4383.1987 - val_mse: 75429944.0000 - val_mae: 4383.1982\n",
            "Epoch 331/500\n",
            "60/60 [==============================] - 0s 857us/step - loss: 2964.9217 - mse: 24990196.0000 - mae: 2964.9219 - val_loss: 4403.4163 - val_mse: 75223648.0000 - val_mae: 4403.4165\n",
            "Epoch 332/500\n",
            "60/60 [==============================] - 0s 793us/step - loss: 2879.6555 - mse: 20253310.0000 - mae: 2879.6555 - val_loss: 4385.9188 - val_mse: 75258088.0000 - val_mae: 4385.9189\n",
            "Epoch 333/500\n",
            "60/60 [==============================] - 0s 851us/step - loss: 2416.3312 - mse: 19006622.0000 - mae: 2416.3313 - val_loss: 4359.6539 - val_mse: 75000376.0000 - val_mae: 4359.6543\n",
            "Epoch 334/500\n",
            "60/60 [==============================] - 0s 735us/step - loss: 2657.3398 - mse: 20735948.0000 - mae: 2657.3396 - val_loss: 4330.8066 - val_mse: 74670936.0000 - val_mae: 4330.8066\n",
            "Epoch 335/500\n",
            "60/60 [==============================] - 0s 738us/step - loss: 2779.7209 - mse: 17652964.0000 - mae: 2779.7209 - val_loss: 4333.3645 - val_mse: 74455312.0000 - val_mae: 4333.3647\n",
            "Epoch 336/500\n",
            "60/60 [==============================] - 0s 985us/step - loss: 3277.5007 - mse: 29696828.0000 - mae: 3277.5005 - val_loss: 4391.0672 - val_mse: 74383144.0000 - val_mae: 4391.0674\n",
            "Epoch 337/500\n",
            "60/60 [==============================] - 0s 858us/step - loss: 2832.5078 - mse: 19996594.0000 - mae: 2832.5076 - val_loss: 4410.7449 - val_mse: 74176096.0000 - val_mae: 4410.7446\n",
            "Epoch 338/500\n",
            "60/60 [==============================] - 0s 885us/step - loss: 2776.6949 - mse: 23667008.0000 - mae: 2776.6951 - val_loss: 4373.3031 - val_mse: 73953776.0000 - val_mae: 4373.3032\n",
            "Epoch 339/500\n",
            "60/60 [==============================] - 0s 844us/step - loss: 2918.4593 - mse: 25155634.0000 - mae: 2918.4595 - val_loss: 4591.5348 - val_mse: 74323336.0000 - val_mae: 4591.5347\n",
            "Epoch 340/500\n",
            "60/60 [==============================] - 0s 959us/step - loss: 2562.7280 - mse: 16565515.0000 - mae: 2562.7280 - val_loss: 4346.9061 - val_mse: 73835232.0000 - val_mae: 4346.9062\n",
            "Epoch 341/500\n",
            "60/60 [==============================] - 0s 785us/step - loss: 3043.8553 - mse: 25740960.0000 - mae: 3043.8555 - val_loss: 4330.5274 - val_mse: 73732464.0000 - val_mae: 4330.5278\n",
            "Epoch 342/500\n",
            "60/60 [==============================] - 0s 968us/step - loss: 2621.9388 - mse: 19039954.0000 - mae: 2621.9387 - val_loss: 4358.2031 - val_mse: 73773448.0000 - val_mae: 4358.2031\n",
            "Epoch 343/500\n",
            "60/60 [==============================] - 0s 801us/step - loss: 2336.4051 - mse: 14805419.0000 - mae: 2336.4053 - val_loss: 4319.4468 - val_mse: 73533048.0000 - val_mae: 4319.4468\n",
            "Epoch 344/500\n",
            "60/60 [==============================] - 0s 784us/step - loss: 2490.2065 - mse: 19490504.0000 - mae: 2490.2063 - val_loss: 4267.0156 - val_mse: 72975896.0000 - val_mae: 4267.0156\n",
            "Epoch 345/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2849.2779 - mse: 23702562.0000 - mae: 2849.2781 - val_loss: 4293.4565 - val_mse: 73091800.0000 - val_mae: 4293.4565\n",
            "Epoch 346/500\n",
            "60/60 [==============================] - 0s 983us/step - loss: 2954.6028 - mse: 23152474.0000 - mae: 2954.6028 - val_loss: 4286.7130 - val_mse: 73069512.0000 - val_mae: 4286.7129\n",
            "Epoch 347/500\n",
            "60/60 [==============================] - 0s 835us/step - loss: 2828.2026 - mse: 24240564.0000 - mae: 2828.2026 - val_loss: 4292.1412 - val_mse: 73083920.0000 - val_mae: 4292.1411\n",
            "Epoch 348/500\n",
            "60/60 [==============================] - 0s 782us/step - loss: 2582.4253 - mse: 16514010.0000 - mae: 2582.4255 - val_loss: 4324.8808 - val_mse: 72655656.0000 - val_mae: 4324.8809\n",
            "Epoch 349/500\n",
            "60/60 [==============================] - 0s 848us/step - loss: 2745.1925 - mse: 21782702.0000 - mae: 2745.1926 - val_loss: 4240.4878 - val_mse: 72270080.0000 - val_mae: 4240.4878\n",
            "Epoch 350/500\n",
            "60/60 [==============================] - 0s 795us/step - loss: 2928.1518 - mse: 23930938.0000 - mae: 2928.1519 - val_loss: 4261.9933 - val_mse: 72392992.0000 - val_mae: 4261.9932\n",
            "Epoch 351/500\n",
            "60/60 [==============================] - 0s 817us/step - loss: 2648.3973 - mse: 20213128.0000 - mae: 2648.3972 - val_loss: 4242.0015 - val_mse: 72267056.0000 - val_mae: 4242.0015\n",
            "Epoch 352/500\n",
            "60/60 [==============================] - 0s 866us/step - loss: 2589.3167 - mse: 18456478.0000 - mae: 2589.3167 - val_loss: 4244.2690 - val_mse: 72391032.0000 - val_mae: 4244.2690\n",
            "Epoch 353/500\n",
            "60/60 [==============================] - 0s 933us/step - loss: 2997.3749 - mse: 26759714.0000 - mae: 2997.3750 - val_loss: 4226.4470 - val_mse: 72051928.0000 - val_mae: 4226.4468\n",
            "Epoch 354/500\n",
            "60/60 [==============================] - 0s 845us/step - loss: 2982.3165 - mse: 24782902.0000 - mae: 2982.3164 - val_loss: 4233.9301 - val_mse: 72075992.0000 - val_mae: 4233.9302\n",
            "Epoch 355/500\n",
            "60/60 [==============================] - 0s 899us/step - loss: 3048.0628 - mse: 24863922.0000 - mae: 3048.0627 - val_loss: 4302.4950 - val_mse: 72305840.0000 - val_mae: 4302.4951\n",
            "Epoch 356/500\n",
            "60/60 [==============================] - 0s 764us/step - loss: 2702.4332 - mse: 17593580.0000 - mae: 2702.4331 - val_loss: 4272.0831 - val_mse: 72250704.0000 - val_mae: 4272.0830\n",
            "Epoch 357/500\n",
            "60/60 [==============================] - 0s 696us/step - loss: 2646.3251 - mse: 19861582.0000 - mae: 2646.3250 - val_loss: 4259.5370 - val_mse: 72127256.0000 - val_mae: 4259.5371\n",
            "Epoch 358/500\n",
            "60/60 [==============================] - 0s 803us/step - loss: 2678.6475 - mse: 20289884.0000 - mae: 2678.6475 - val_loss: 4515.6784 - val_mse: 72463112.0000 - val_mae: 4515.6787\n",
            "Epoch 359/500\n",
            "60/60 [==============================] - 0s 739us/step - loss: 2620.9805 - mse: 20032392.0000 - mae: 2620.9807 - val_loss: 4300.4557 - val_mse: 71908992.0000 - val_mae: 4300.4556\n",
            "Epoch 360/500\n",
            "60/60 [==============================] - 0s 778us/step - loss: 2859.8270 - mse: 22137492.0000 - mae: 2859.8271 - val_loss: 4503.2852 - val_mse: 72288232.0000 - val_mae: 4503.2856\n",
            "Epoch 361/500\n",
            "60/60 [==============================] - 0s 744us/step - loss: 3135.8994 - mse: 28691208.0000 - mae: 3135.8992 - val_loss: 4236.3043 - val_mse: 71810072.0000 - val_mae: 4236.3042\n",
            "Epoch 362/500\n",
            "60/60 [==============================] - 0s 991us/step - loss: 2741.7615 - mse: 18084826.0000 - mae: 2741.7615 - val_loss: 4311.5285 - val_mse: 71764080.0000 - val_mae: 4311.5283\n",
            "Epoch 363/500\n",
            "60/60 [==============================] - 0s 840us/step - loss: 3086.4199 - mse: 26568452.0000 - mae: 3086.4197 - val_loss: 4326.1181 - val_mse: 71782608.0000 - val_mae: 4326.1182\n",
            "Epoch 364/500\n",
            "60/60 [==============================] - 0s 876us/step - loss: 2722.0280 - mse: 19996846.0000 - mae: 2722.0278 - val_loss: 4271.7517 - val_mse: 71617656.0000 - val_mae: 4271.7520\n",
            "Epoch 365/500\n",
            "60/60 [==============================] - 0s 902us/step - loss: 2678.1616 - mse: 20800958.0000 - mae: 2678.1616 - val_loss: 4361.9790 - val_mse: 71800408.0000 - val_mae: 4361.9790\n",
            "Epoch 366/500\n",
            "60/60 [==============================] - 0s 791us/step - loss: 2729.6901 - mse: 16993842.0000 - mae: 2729.6902 - val_loss: 4234.4955 - val_mse: 71557456.0000 - val_mae: 4234.4951\n",
            "Epoch 367/500\n",
            "60/60 [==============================] - 0s 707us/step - loss: 2720.0994 - mse: 22165276.0000 - mae: 2720.0994 - val_loss: 4243.1431 - val_mse: 71354752.0000 - val_mae: 4243.1431\n",
            "Epoch 368/500\n",
            "60/60 [==============================] - 0s 718us/step - loss: 2611.4206 - mse: 17633112.0000 - mae: 2611.4204 - val_loss: 4268.7092 - val_mse: 71263896.0000 - val_mae: 4268.7095\n",
            "Epoch 369/500\n",
            "60/60 [==============================] - 0s 894us/step - loss: 2943.3679 - mse: 22091500.0000 - mae: 2943.3679 - val_loss: 4304.6756 - val_mse: 71511336.0000 - val_mae: 4304.6753\n",
            "Epoch 370/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2274.7135 - mse: 13209170.0000 - mae: 2274.7136 - val_loss: 4228.2027 - val_mse: 71344888.0000 - val_mae: 4228.2026\n",
            "Epoch 371/500\n",
            "60/60 [==============================] - 0s 784us/step - loss: 3193.8077 - mse: 27483758.0000 - mae: 3193.8079 - val_loss: 4245.5459 - val_mse: 71131824.0000 - val_mae: 4245.5459\n",
            "Epoch 372/500\n",
            "60/60 [==============================] - 0s 780us/step - loss: 2456.0443 - mse: 16183598.0000 - mae: 2456.0442 - val_loss: 4277.5202 - val_mse: 71014800.0000 - val_mae: 4277.5205\n",
            "Epoch 373/500\n",
            "60/60 [==============================] - 0s 746us/step - loss: 2724.3390 - mse: 19902238.0000 - mae: 2724.3389 - val_loss: 4183.0656 - val_mse: 70759848.0000 - val_mae: 4183.0654\n",
            "Epoch 374/500\n",
            "60/60 [==============================] - 0s 828us/step - loss: 2621.0109 - mse: 20509206.0000 - mae: 2621.0110 - val_loss: 4183.4948 - val_mse: 70627024.0000 - val_mae: 4183.4946\n",
            "Epoch 375/500\n",
            "60/60 [==============================] - 0s 762us/step - loss: 2859.4952 - mse: 25147488.0000 - mae: 2859.4951 - val_loss: 4194.9076 - val_mse: 70796248.0000 - val_mae: 4194.9077\n",
            "Epoch 376/500\n",
            "60/60 [==============================] - 0s 805us/step - loss: 2730.9421 - mse: 23640818.0000 - mae: 2730.9421 - val_loss: 4190.3131 - val_mse: 70648432.0000 - val_mae: 4190.3130\n",
            "Epoch 377/500\n",
            "60/60 [==============================] - 0s 746us/step - loss: 2712.7575 - mse: 20657010.0000 - mae: 2712.7573 - val_loss: 4283.3077 - val_mse: 70622952.0000 - val_mae: 4283.3076\n",
            "Epoch 378/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3110.6743 - mse: 29395392.0000 - mae: 3110.6743 - val_loss: 4172.9429 - val_mse: 70438152.0000 - val_mae: 4172.9429\n",
            "Epoch 379/500\n",
            "60/60 [==============================] - 0s 702us/step - loss: 2534.5854 - mse: 16625625.0000 - mae: 2534.5857 - val_loss: 4176.8865 - val_mse: 70370352.0000 - val_mae: 4176.8867\n",
            "Epoch 380/500\n",
            "60/60 [==============================] - 0s 742us/step - loss: 2581.9404 - mse: 17169498.0000 - mae: 2581.9404 - val_loss: 4285.4888 - val_mse: 70551752.0000 - val_mae: 4285.4893\n",
            "Epoch 381/500\n",
            "60/60 [==============================] - 0s 835us/step - loss: 2755.0855 - mse: 22089122.0000 - mae: 2755.0854 - val_loss: 4230.3360 - val_mse: 70357560.0000 - val_mae: 4230.3359\n",
            "Epoch 382/500\n",
            "60/60 [==============================] - 0s 825us/step - loss: 2842.9102 - mse: 22431862.0000 - mae: 2842.9102 - val_loss: 4206.1561 - val_mse: 70213256.0000 - val_mae: 4206.1558\n",
            "Epoch 383/500\n",
            "60/60 [==============================] - 0s 824us/step - loss: 2478.3908 - mse: 20634686.0000 - mae: 2478.3909 - val_loss: 4172.6427 - val_mse: 70170688.0000 - val_mae: 4172.6426\n",
            "Epoch 384/500\n",
            "60/60 [==============================] - 0s 844us/step - loss: 2623.1008 - mse: 18394314.0000 - mae: 2623.1011 - val_loss: 4342.6017 - val_mse: 70056376.0000 - val_mae: 4342.6021\n",
            "Epoch 385/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2536.3653 - mse: 16954006.0000 - mae: 2536.3655 - val_loss: 4466.1289 - val_mse: 69902152.0000 - val_mae: 4466.1294\n",
            "Epoch 386/500\n",
            "60/60 [==============================] - 0s 845us/step - loss: 2441.5808 - mse: 15826367.0000 - mae: 2441.5811 - val_loss: 4218.7716 - val_mse: 69428280.0000 - val_mae: 4218.7715\n",
            "Epoch 387/500\n",
            "60/60 [==============================] - 0s 768us/step - loss: 2831.6034 - mse: 23454850.0000 - mae: 2831.6038 - val_loss: 4209.3586 - val_mse: 69412576.0000 - val_mae: 4209.3589\n",
            "Epoch 388/500\n",
            "60/60 [==============================] - 0s 872us/step - loss: 2746.6086 - mse: 21129122.0000 - mae: 2746.6086 - val_loss: 4073.8014 - val_mse: 69108776.0000 - val_mae: 4073.8013\n",
            "Epoch 389/500\n",
            "60/60 [==============================] - 0s 847us/step - loss: 2963.2041 - mse: 24331340.0000 - mae: 2963.2041 - val_loss: 4188.5953 - val_mse: 69308456.0000 - val_mae: 4188.5952\n",
            "Epoch 390/500\n",
            "60/60 [==============================] - 0s 910us/step - loss: 2730.0969 - mse: 21430510.0000 - mae: 2730.0969 - val_loss: 4103.6013 - val_mse: 68962096.0000 - val_mae: 4103.6011\n",
            "Epoch 391/500\n",
            "60/60 [==============================] - 0s 714us/step - loss: 2994.8703 - mse: 22973042.0000 - mae: 2994.8704 - val_loss: 4192.2337 - val_mse: 69051520.0000 - val_mae: 4192.2334\n",
            "Epoch 392/500\n",
            "60/60 [==============================] - 0s 812us/step - loss: 2409.4931 - mse: 15703949.0000 - mae: 2409.4929 - val_loss: 4082.4380 - val_mse: 68724880.0000 - val_mae: 4082.4380\n",
            "Epoch 393/500\n",
            "60/60 [==============================] - 0s 804us/step - loss: 2625.9656 - mse: 23809514.0000 - mae: 2625.9656 - val_loss: 4086.2717 - val_mse: 68500768.0000 - val_mae: 4086.2717\n",
            "Epoch 394/500\n",
            "60/60 [==============================] - 0s 771us/step - loss: 2867.4196 - mse: 22342796.0000 - mae: 2867.4194 - val_loss: 4141.6295 - val_mse: 68529504.0000 - val_mae: 4141.6294\n",
            "Epoch 395/500\n",
            "60/60 [==============================] - 0s 974us/step - loss: 2464.6414 - mse: 14525293.0000 - mae: 2464.6416 - val_loss: 4067.9896 - val_mse: 68241800.0000 - val_mae: 4067.9895\n",
            "Epoch 396/500\n",
            "60/60 [==============================] - 0s 933us/step - loss: 2627.0270 - mse: 17505802.0000 - mae: 2627.0271 - val_loss: 4064.0922 - val_mse: 68259240.0000 - val_mae: 4064.0923\n",
            "Epoch 397/500\n",
            "60/60 [==============================] - 0s 910us/step - loss: 2878.7846 - mse: 27479114.0000 - mae: 2878.7844 - val_loss: 4069.5026 - val_mse: 68216936.0000 - val_mae: 4069.5027\n",
            "Epoch 398/500\n",
            "60/60 [==============================] - 0s 817us/step - loss: 2347.5286 - mse: 16996198.0000 - mae: 2347.5286 - val_loss: 4107.1500 - val_mse: 68036472.0000 - val_mae: 4107.1499\n",
            "Epoch 399/500\n",
            "60/60 [==============================] - 0s 726us/step - loss: 2860.3742 - mse: 25082136.0000 - mae: 2860.3743 - val_loss: 4058.4357 - val_mse: 67994464.0000 - val_mae: 4058.4358\n",
            "Epoch 400/500\n",
            "60/60 [==============================] - 0s 715us/step - loss: 2513.2965 - mse: 17465796.0000 - mae: 2513.2966 - val_loss: 4157.3209 - val_mse: 68202992.0000 - val_mae: 4157.3208\n",
            "Epoch 401/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3688.3641 - mse: 35109444.0000 - mae: 3688.3640 - val_loss: 4149.5273 - val_mse: 68328616.0000 - val_mae: 4149.5278\n",
            "Epoch 402/500\n",
            "60/60 [==============================] - 0s 845us/step - loss: 2583.8522 - mse: 15966793.0000 - mae: 2583.8523 - val_loss: 4061.9633 - val_mse: 68129744.0000 - val_mae: 4061.9631\n",
            "Epoch 403/500\n",
            "60/60 [==============================] - 0s 751us/step - loss: 2840.6331 - mse: 19810632.0000 - mae: 2840.6333 - val_loss: 4128.8747 - val_mse: 68146112.0000 - val_mae: 4128.8745\n",
            "Epoch 404/500\n",
            "60/60 [==============================] - 0s 876us/step - loss: 2157.0804 - mse: 12428882.0000 - mae: 2157.0803 - val_loss: 4093.4276 - val_mse: 67961296.0000 - val_mae: 4093.4272\n",
            "Epoch 405/500\n",
            "60/60 [==============================] - 0s 805us/step - loss: 2230.0211 - mse: 13561077.0000 - mae: 2230.0210 - val_loss: 4104.4277 - val_mse: 67875792.0000 - val_mae: 4104.4277\n",
            "Epoch 406/500\n",
            "60/60 [==============================] - 0s 744us/step - loss: 2517.6572 - mse: 16233834.0000 - mae: 2517.6572 - val_loss: 4151.2995 - val_mse: 67657536.0000 - val_mae: 4151.2993\n",
            "Epoch 407/500\n",
            "60/60 [==============================] - 0s 837us/step - loss: 2294.2858 - mse: 15488162.0000 - mae: 2294.2859 - val_loss: 4039.4581 - val_mse: 67559680.0000 - val_mae: 4039.4580\n",
            "Epoch 408/500\n",
            "60/60 [==============================] - 0s 899us/step - loss: 2577.8611 - mse: 17941580.0000 - mae: 2577.8611 - val_loss: 4109.3129 - val_mse: 67735208.0000 - val_mae: 4109.3130\n",
            "Epoch 409/500\n",
            "60/60 [==============================] - 0s 870us/step - loss: 2749.2048 - mse: 18358438.0000 - mae: 2749.2046 - val_loss: 4095.1104 - val_mse: 67415984.0000 - val_mae: 4095.1104\n",
            "Epoch 410/500\n",
            "60/60 [==============================] - 0s 698us/step - loss: 2532.2125 - mse: 15666767.0000 - mae: 2532.2124 - val_loss: 4186.8417 - val_mse: 67509816.0000 - val_mae: 4186.8418\n",
            "Epoch 411/500\n",
            "60/60 [==============================] - 0s 776us/step - loss: 2911.4094 - mse: 20416984.0000 - mae: 2911.4094 - val_loss: 4143.0682 - val_mse: 67401896.0000 - val_mae: 4143.0684\n",
            "Epoch 412/500\n",
            "60/60 [==============================] - 0s 801us/step - loss: 2368.0080 - mse: 13235610.0000 - mae: 2368.0078 - val_loss: 4205.7712 - val_mse: 67363040.0000 - val_mae: 4205.7710\n",
            "Epoch 413/500\n",
            "60/60 [==============================] - 0s 806us/step - loss: 2495.9372 - mse: 15444579.0000 - mae: 2495.9370 - val_loss: 4035.6432 - val_mse: 66912152.0000 - val_mae: 4035.6433\n",
            "Epoch 414/500\n",
            "60/60 [==============================] - 0s 939us/step - loss: 2523.6151 - mse: 18040836.0000 - mae: 2523.6150 - val_loss: 4039.0205 - val_mse: 66917320.0000 - val_mae: 4039.0205\n",
            "Epoch 415/500\n",
            "60/60 [==============================] - 0s 828us/step - loss: 3027.3340 - mse: 31068068.0000 - mae: 3027.3337 - val_loss: 4004.7443 - val_mse: 66819516.0000 - val_mae: 4004.7444\n",
            "Epoch 416/500\n",
            "60/60 [==============================] - 0s 862us/step - loss: 3110.6157 - mse: 26843726.0000 - mae: 3110.6160 - val_loss: 4091.5264 - val_mse: 66930228.0000 - val_mae: 4091.5266\n",
            "Epoch 417/500\n",
            "60/60 [==============================] - 0s 843us/step - loss: 3231.8233 - mse: 27427340.0000 - mae: 3231.8232 - val_loss: 4099.1195 - val_mse: 66880176.0000 - val_mae: 4099.1196\n",
            "Epoch 418/500\n",
            "60/60 [==============================] - 0s 839us/step - loss: 2827.1385 - mse: 21300274.0000 - mae: 2827.1384 - val_loss: 4179.3407 - val_mse: 67084500.0000 - val_mae: 4179.3408\n",
            "Epoch 419/500\n",
            "60/60 [==============================] - 0s 991us/step - loss: 2680.9183 - mse: 20013978.0000 - mae: 2680.9182 - val_loss: 4094.8415 - val_mse: 66859440.0000 - val_mae: 4094.8416\n",
            "Epoch 420/500\n",
            "60/60 [==============================] - 0s 716us/step - loss: 2538.4854 - mse: 17424794.0000 - mae: 2538.4854 - val_loss: 4035.5386 - val_mse: 66663412.0000 - val_mae: 4035.5388\n",
            "Epoch 421/500\n",
            "60/60 [==============================] - 0s 804us/step - loss: 2438.1609 - mse: 15550343.0000 - mae: 2438.1609 - val_loss: 4016.3746 - val_mse: 66550152.0000 - val_mae: 4016.3748\n",
            "Epoch 422/500\n",
            "60/60 [==============================] - 0s 812us/step - loss: 2703.6907 - mse: 17808142.0000 - mae: 2703.6904 - val_loss: 4023.2403 - val_mse: 66149212.0000 - val_mae: 4023.2405\n",
            "Epoch 423/500\n",
            "60/60 [==============================] - 0s 944us/step - loss: 2966.9746 - mse: 24128184.0000 - mae: 2966.9744 - val_loss: 3972.8581 - val_mse: 65952728.0000 - val_mae: 3972.8582\n",
            "Epoch 424/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2617.4840 - mse: 19799082.0000 - mae: 2617.4839 - val_loss: 4032.7848 - val_mse: 65936332.0000 - val_mae: 4032.7849\n",
            "Epoch 425/500\n",
            "60/60 [==============================] - 0s 735us/step - loss: 2644.8218 - mse: 17660344.0000 - mae: 2644.8218 - val_loss: 3989.1588 - val_mse: 65879400.0000 - val_mae: 3989.1589\n",
            "Epoch 426/500\n",
            "60/60 [==============================] - 0s 794us/step - loss: 2525.5072 - mse: 22757956.0000 - mae: 2525.5073 - val_loss: 4087.2491 - val_mse: 65889208.0000 - val_mae: 4087.2490\n",
            "Epoch 427/500\n",
            "60/60 [==============================] - 0s 711us/step - loss: 2819.7843 - mse: 21475034.0000 - mae: 2819.7844 - val_loss: 3985.5922 - val_mse: 65645440.0000 - val_mae: 3985.5923\n",
            "Epoch 428/500\n",
            "60/60 [==============================] - 0s 872us/step - loss: 2733.3150 - mse: 19079506.0000 - mae: 2733.3152 - val_loss: 3972.2768 - val_mse: 65634440.0000 - val_mae: 3972.2766\n",
            "Epoch 429/500\n",
            "60/60 [==============================] - 0s 859us/step - loss: 2785.0751 - mse: 20364276.0000 - mae: 2785.0752 - val_loss: 3964.1810 - val_mse: 65728660.0000 - val_mae: 3964.1809\n",
            "Epoch 430/500\n",
            "60/60 [==============================] - 0s 794us/step - loss: 2393.3903 - mse: 16532286.0000 - mae: 2393.3901 - val_loss: 4111.2005 - val_mse: 65792876.0000 - val_mae: 4111.2007\n",
            "Epoch 431/500\n",
            "60/60 [==============================] - 0s 912us/step - loss: 2707.7357 - mse: 19889838.0000 - mae: 2707.7356 - val_loss: 3961.7250 - val_mse: 65405628.0000 - val_mae: 3961.7251\n",
            "Epoch 432/500\n",
            "60/60 [==============================] - 0s 825us/step - loss: 1995.4373 - mse: 9103011.0000 - mae: 1995.4374 - val_loss: 3934.0499 - val_mse: 65122184.0000 - val_mae: 3934.0500\n",
            "Epoch 433/500\n",
            "60/60 [==============================] - 0s 857us/step - loss: 3016.6928 - mse: 24915264.0000 - mae: 3016.6926 - val_loss: 3960.8622 - val_mse: 65319268.0000 - val_mae: 3960.8621\n",
            "Epoch 434/500\n",
            "60/60 [==============================] - 0s 761us/step - loss: 2698.2558 - mse: 18381598.0000 - mae: 2698.2556 - val_loss: 3960.0906 - val_mse: 65239420.0000 - val_mae: 3960.0906\n",
            "Epoch 435/500\n",
            "60/60 [==============================] - 0s 746us/step - loss: 2486.4602 - mse: 16516433.0000 - mae: 2486.4602 - val_loss: 3990.7256 - val_mse: 65519236.0000 - val_mae: 3990.7256\n",
            "Epoch 436/500\n",
            "60/60 [==============================] - 0s 868us/step - loss: 2747.0087 - mse: 20352042.0000 - mae: 2747.0088 - val_loss: 3963.8259 - val_mse: 65365656.0000 - val_mae: 3963.8257\n",
            "Epoch 437/500\n",
            "60/60 [==============================] - 0s 822us/step - loss: 2937.0124 - mse: 22271790.0000 - mae: 2937.0122 - val_loss: 3960.6438 - val_mse: 65250776.0000 - val_mae: 3960.6438\n",
            "Epoch 438/500\n",
            "60/60 [==============================] - 0s 898us/step - loss: 2537.5359 - mse: 17339692.0000 - mae: 2537.5359 - val_loss: 4119.7730 - val_mse: 65351684.0000 - val_mae: 4119.7729\n",
            "Epoch 439/500\n",
            "60/60 [==============================] - 0s 842us/step - loss: 2637.9605 - mse: 20055694.0000 - mae: 2637.9604 - val_loss: 3964.8922 - val_mse: 64964852.0000 - val_mae: 3964.8921\n",
            "Epoch 440/500\n",
            "60/60 [==============================] - 0s 911us/step - loss: 2512.9738 - mse: 20571026.0000 - mae: 2512.9739 - val_loss: 3981.2428 - val_mse: 64947892.0000 - val_mae: 3981.2427\n",
            "Epoch 441/500\n",
            "60/60 [==============================] - 0s 816us/step - loss: 2830.3235 - mse: 18417924.0000 - mae: 2830.3235 - val_loss: 3933.0713 - val_mse: 64821140.0000 - val_mae: 3933.0713\n",
            "Epoch 442/500\n",
            "60/60 [==============================] - 0s 925us/step - loss: 2749.6934 - mse: 20269540.0000 - mae: 2749.6936 - val_loss: 4118.2328 - val_mse: 65215024.0000 - val_mae: 4118.2329\n",
            "Epoch 443/500\n",
            "60/60 [==============================] - 0s 785us/step - loss: 2772.6359 - mse: 20846240.0000 - mae: 2772.6362 - val_loss: 4051.4277 - val_mse: 65197504.0000 - val_mae: 4051.4275\n",
            "Epoch 444/500\n",
            "60/60 [==============================] - 0s 727us/step - loss: 2761.1658 - mse: 18275636.0000 - mae: 2761.1655 - val_loss: 4146.8534 - val_mse: 65007316.0000 - val_mae: 4146.8535\n",
            "Epoch 445/500\n",
            "60/60 [==============================] - 0s 935us/step - loss: 2616.2146 - mse: 18051422.0000 - mae: 2616.2146 - val_loss: 3965.3886 - val_mse: 64950776.0000 - val_mae: 3965.3884\n",
            "Epoch 446/500\n",
            "60/60 [==============================] - 0s 833us/step - loss: 2831.7880 - mse: 23240076.0000 - mae: 2831.7881 - val_loss: 3930.4156 - val_mse: 64713604.0000 - val_mae: 3930.4155\n",
            "Epoch 447/500\n",
            "60/60 [==============================] - 0s 782us/step - loss: 2507.9155 - mse: 14942897.0000 - mae: 2507.9155 - val_loss: 3964.6811 - val_mse: 64437628.0000 - val_mae: 3964.6812\n",
            "Epoch 448/500\n",
            "60/60 [==============================] - 0s 872us/step - loss: 2657.0991 - mse: 20792692.0000 - mae: 2657.0989 - val_loss: 3976.7865 - val_mse: 64260224.0000 - val_mae: 3976.7864\n",
            "Epoch 449/500\n",
            "60/60 [==============================] - 0s 785us/step - loss: 2439.2590 - mse: 15560569.0000 - mae: 2439.2588 - val_loss: 4027.2347 - val_mse: 64194372.0000 - val_mae: 4027.2349\n",
            "Epoch 450/500\n",
            "60/60 [==============================] - 0s 729us/step - loss: 3017.2006 - mse: 22023130.0000 - mae: 3017.2004 - val_loss: 4138.1898 - val_mse: 64350524.0000 - val_mae: 4138.1895\n",
            "Epoch 451/500\n",
            "60/60 [==============================] - 0s 825us/step - loss: 2611.5545 - mse: 18902356.0000 - mae: 2611.5547 - val_loss: 3934.4636 - val_mse: 63890492.0000 - val_mae: 3934.4639\n",
            "Epoch 452/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2885.5178 - mse: 22813364.0000 - mae: 2885.5181 - val_loss: 3944.4698 - val_mse: 63844424.0000 - val_mae: 3944.4697\n",
            "Epoch 453/500\n",
            "60/60 [==============================] - 0s 811us/step - loss: 3034.0796 - mse: 25973270.0000 - mae: 3034.0796 - val_loss: 3997.8276 - val_mse: 63869228.0000 - val_mae: 3997.8276\n",
            "Epoch 454/500\n",
            "60/60 [==============================] - 0s 954us/step - loss: 2486.1854 - mse: 17304668.0000 - mae: 2486.1851 - val_loss: 4031.9708 - val_mse: 64054748.0000 - val_mae: 4031.9709\n",
            "Epoch 455/500\n",
            "60/60 [==============================] - 0s 859us/step - loss: 2806.2896 - mse: 23196054.0000 - mae: 2806.2898 - val_loss: 3885.5353 - val_mse: 63801460.0000 - val_mae: 3885.5354\n",
            "Epoch 456/500\n",
            "60/60 [==============================] - 0s 827us/step - loss: 2990.4264 - mse: 24736618.0000 - mae: 2990.4263 - val_loss: 3976.6298 - val_mse: 63826448.0000 - val_mae: 3976.6296\n",
            "Epoch 457/500\n",
            "60/60 [==============================] - 0s 816us/step - loss: 2669.5702 - mse: 18996810.0000 - mae: 2669.5703 - val_loss: 3959.4409 - val_mse: 63836940.0000 - val_mae: 3959.4412\n",
            "Epoch 458/500\n",
            "60/60 [==============================] - 0s 809us/step - loss: 2583.9867 - mse: 18479074.0000 - mae: 2583.9868 - val_loss: 3887.9412 - val_mse: 63635048.0000 - val_mae: 3887.9412\n",
            "Epoch 459/500\n",
            "60/60 [==============================] - 0s 983us/step - loss: 2797.7244 - mse: 21955932.0000 - mae: 2797.7244 - val_loss: 3929.7715 - val_mse: 63424632.0000 - val_mae: 3929.7717\n",
            "Epoch 460/500\n",
            "60/60 [==============================] - 0s 920us/step - loss: 2080.4381 - mse: 12403312.0000 - mae: 2080.4382 - val_loss: 3961.3468 - val_mse: 63631668.0000 - val_mae: 3961.3469\n",
            "Epoch 461/500\n",
            "60/60 [==============================] - 0s 999us/step - loss: 2646.0669 - mse: 18384462.0000 - mae: 2646.0667 - val_loss: 3902.5208 - val_mse: 63711632.0000 - val_mae: 3902.5208\n",
            "Epoch 462/500\n",
            "60/60 [==============================] - 0s 807us/step - loss: 2711.1011 - mse: 19941222.0000 - mae: 2711.1013 - val_loss: 3887.7135 - val_mse: 63601600.0000 - val_mae: 3887.7136\n",
            "Epoch 463/500\n",
            "60/60 [==============================] - 0s 884us/step - loss: 3256.1067 - mse: 30919748.0000 - mae: 3256.1062 - val_loss: 4077.7697 - val_mse: 64070440.0000 - val_mae: 4077.7695\n",
            "Epoch 464/500\n",
            "60/60 [==============================] - 0s 811us/step - loss: 2414.8647 - mse: 14906170.0000 - mae: 2414.8645 - val_loss: 3904.6492 - val_mse: 63781560.0000 - val_mae: 3904.6489\n",
            "Epoch 465/500\n",
            "60/60 [==============================] - 0s 749us/step - loss: 2564.8982 - mse: 17606220.0000 - mae: 2564.8979 - val_loss: 3952.9549 - val_mse: 63756952.0000 - val_mae: 3952.9546\n",
            "Epoch 466/500\n",
            "60/60 [==============================] - 0s 791us/step - loss: 2974.2077 - mse: 21313370.0000 - mae: 2974.2078 - val_loss: 4024.9152 - val_mse: 63952068.0000 - val_mae: 4024.9150\n",
            "Epoch 467/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 2436.2361 - mse: 21104392.0000 - mae: 2436.2361 - val_loss: 3974.3094 - val_mse: 63724776.0000 - val_mae: 3974.3093\n",
            "Epoch 468/500\n",
            "60/60 [==============================] - 0s 807us/step - loss: 2751.5600 - mse: 18924348.0000 - mae: 2751.5598 - val_loss: 3987.9862 - val_mse: 63774884.0000 - val_mae: 3987.9866\n",
            "Epoch 469/500\n",
            "60/60 [==============================] - 0s 752us/step - loss: 2924.4922 - mse: 22479274.0000 - mae: 2924.4922 - val_loss: 3977.3200 - val_mse: 63826596.0000 - val_mae: 3977.3198\n",
            "Epoch 470/500\n",
            "60/60 [==============================] - 0s 800us/step - loss: 2724.6067 - mse: 18491558.0000 - mae: 2724.6067 - val_loss: 4007.1148 - val_mse: 63890568.0000 - val_mae: 4007.1145\n",
            "Epoch 471/500\n",
            "60/60 [==============================] - 0s 841us/step - loss: 2636.7432 - mse: 21166216.0000 - mae: 2636.7432 - val_loss: 3928.6870 - val_mse: 63458952.0000 - val_mae: 3928.6870\n",
            "Epoch 472/500\n",
            "60/60 [==============================] - 0s 745us/step - loss: 2925.4327 - mse: 21340288.0000 - mae: 2925.4329 - val_loss: 3885.7597 - val_mse: 63251612.0000 - val_mae: 3885.7595\n",
            "Epoch 473/500\n",
            "60/60 [==============================] - 0s 1ms/step - loss: 3023.4816 - mse: 23842182.0000 - mae: 3023.4817 - val_loss: 3946.8562 - val_mse: 63463528.0000 - val_mae: 3946.8562\n",
            "Epoch 474/500\n",
            "60/60 [==============================] - 0s 839us/step - loss: 2688.6939 - mse: 23123626.0000 - mae: 2688.6938 - val_loss: 3939.6161 - val_mse: 63340168.0000 - val_mae: 3939.6162\n",
            "Epoch 475/500\n",
            "60/60 [==============================] - 0s 842us/step - loss: 2959.4875 - mse: 22359838.0000 - mae: 2959.4878 - val_loss: 3996.5919 - val_mse: 63621828.0000 - val_mae: 3996.5916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 80 samples, validate on 70 samples\n",
            "Epoch 1/500\n",
            "80/80 [==============================] - 2s 27ms/step - loss: 20378.7129 - mse: 916129408.0000 - mae: 20378.7129 - val_loss: 18686.1482 - val_mse: 743990720.0000 - val_mae: 18686.1465\n",
            "Epoch 2/500\n",
            "80/80 [==============================] - 0s 599us/step - loss: 20374.7318 - mse: 916041728.0000 - mae: 20374.7324 - val_loss: 18674.6115 - val_mse: 743715136.0000 - val_mae: 18674.6113\n",
            "Epoch 3/500\n",
            "80/80 [==============================] - 0s 542us/step - loss: 20355.7858 - mse: 915429568.0000 - mae: 20355.7852 - val_loss: 18643.1271 - val_mse: 742691968.0000 - val_mae: 18643.1270\n",
            "Epoch 4/500\n",
            "80/80 [==============================] - 0s 535us/step - loss: 20320.3308 - mse: 914239168.0000 - mae: 20320.3301 - val_loss: 18595.5639 - val_mse: 741037056.0000 - val_mae: 18595.5645\n",
            "Epoch 5/500\n",
            "80/80 [==============================] - 0s 516us/step - loss: 20270.1161 - mse: 912216896.0000 - mae: 20270.1172 - val_loss: 18538.7237 - val_mse: 738959488.0000 - val_mae: 18538.7227\n",
            "Epoch 6/500\n",
            "80/80 [==============================] - 0s 547us/step - loss: 20221.6902 - mse: 910139776.0000 - mae: 20221.6895 - val_loss: 18475.2727 - val_mse: 736630272.0000 - val_mae: 18475.2715\n",
            "Epoch 7/500\n",
            "80/80 [==============================] - 0s 650us/step - loss: 20161.5864 - mse: 907065728.0000 - mae: 20161.5859 - val_loss: 18404.1146 - val_mse: 734006976.0000 - val_mae: 18404.1152\n",
            "Epoch 8/500\n",
            "80/80 [==============================] - 0s 672us/step - loss: 20092.1804 - mse: 903933312.0000 - mae: 20092.1797 - val_loss: 18326.0541 - val_mse: 731134848.0000 - val_mae: 18326.0527\n",
            "Epoch 9/500\n",
            "80/80 [==============================] - 0s 642us/step - loss: 20001.9839 - mse: 899585664.0000 - mae: 20001.9844 - val_loss: 18236.5957 - val_mse: 727859648.0000 - val_mae: 18236.5957\n",
            "Epoch 10/500\n",
            "80/80 [==============================] - 0s 571us/step - loss: 19977.0912 - mse: 897275072.0000 - mae: 19977.0898 - val_loss: 18153.4750 - val_mse: 724707520.0000 - val_mae: 18153.4746\n",
            "Epoch 11/500\n",
            "80/80 [==============================] - 0s 586us/step - loss: 19888.2653 - mse: 894440448.0000 - mae: 19888.2656 - val_loss: 18060.2644 - val_mse: 721075264.0000 - val_mae: 18060.2637\n",
            "Epoch 12/500\n",
            "80/80 [==============================] - 0s 701us/step - loss: 19808.1443 - mse: 888156992.0000 - mae: 19808.1445 - val_loss: 17968.1035 - val_mse: 717233152.0000 - val_mae: 17968.1035\n",
            "Epoch 13/500\n",
            "80/80 [==============================] - 0s 594us/step - loss: 19756.1479 - mse: 886990464.0000 - mae: 19756.1465 - val_loss: 17879.9760 - val_mse: 713317376.0000 - val_mae: 17879.9746\n",
            "Epoch 14/500\n",
            "80/80 [==============================] - 0s 638us/step - loss: 19618.4770 - mse: 879856832.0000 - mae: 19618.4766 - val_loss: 17787.9823 - val_mse: 708861504.0000 - val_mae: 17787.9824\n",
            "Epoch 15/500\n",
            "80/80 [==============================] - 0s 554us/step - loss: 19574.8242 - mse: 876293312.0000 - mae: 19574.8242 - val_loss: 17702.3044 - val_mse: 704625408.0000 - val_mae: 17702.3027\n",
            "Epoch 16/500\n",
            "80/80 [==============================] - 0s 603us/step - loss: 19504.2693 - mse: 872147968.0000 - mae: 19504.2695 - val_loss: 17626.2725 - val_mse: 700884160.0000 - val_mae: 17626.2715\n",
            "Epoch 17/500\n",
            "80/80 [==============================] - 0s 584us/step - loss: 19437.3192 - mse: 867980288.0000 - mae: 19437.3184 - val_loss: 17548.9237 - val_mse: 697099968.0000 - val_mae: 17548.9238\n",
            "Epoch 18/500\n",
            "80/80 [==============================] - 0s 622us/step - loss: 19278.0249 - mse: 856772224.0000 - mae: 19278.0254 - val_loss: 17464.9143 - val_mse: 693013504.0000 - val_mae: 17464.9141\n",
            "Epoch 19/500\n",
            "80/80 [==============================] - 0s 635us/step - loss: 19277.6116 - mse: 859719680.0000 - mae: 19277.6094 - val_loss: 17379.4039 - val_mse: 688889536.0000 - val_mae: 17379.4043\n",
            "Epoch 20/500\n",
            "80/80 [==============================] - 0s 618us/step - loss: 19274.2236 - mse: 856513920.0000 - mae: 19274.2227 - val_loss: 17305.7961 - val_mse: 685052928.0000 - val_mae: 17305.7969\n",
            "Epoch 21/500\n",
            "80/80 [==============================] - 0s 562us/step - loss: 19252.3774 - mse: 854293760.0000 - mae: 19252.3770 - val_loss: 17239.4482 - val_mse: 681399168.0000 - val_mae: 17239.4473\n",
            "Epoch 22/500\n",
            "80/80 [==============================] - 0s 671us/step - loss: 19060.2700 - mse: 843237504.0000 - mae: 19060.2695 - val_loss: 17163.5589 - val_mse: 677055616.0000 - val_mae: 17163.5586\n",
            "Epoch 23/500\n",
            "80/80 [==============================] - 0s 676us/step - loss: 19099.4164 - mse: 845286976.0000 - mae: 19099.4160 - val_loss: 17094.3922 - val_mse: 672861504.0000 - val_mae: 17094.3926\n",
            "Epoch 24/500\n",
            "80/80 [==============================] - 0s 597us/step - loss: 19055.0654 - mse: 834787968.0000 - mae: 19055.0645 - val_loss: 17029.5452 - val_mse: 668889408.0000 - val_mae: 17029.5449\n",
            "Epoch 25/500\n",
            "80/80 [==============================] - 0s 576us/step - loss: 18962.6623 - mse: 830214848.0000 - mae: 18962.6621 - val_loss: 16965.6081 - val_mse: 664865856.0000 - val_mae: 16965.6074\n",
            "Epoch 26/500\n",
            "80/80 [==============================] - 0s 612us/step - loss: 18917.6334 - mse: 832229952.0000 - mae: 18917.6328 - val_loss: 16905.3822 - val_mse: 660842816.0000 - val_mae: 16905.3828\n",
            "Epoch 27/500\n",
            "80/80 [==============================] - 0s 642us/step - loss: 18769.0684 - mse: 823430848.0000 - mae: 18769.0684 - val_loss: 16836.1232 - val_mse: 656154432.0000 - val_mae: 16836.1230\n",
            "Epoch 28/500\n",
            "80/80 [==============================] - 0s 738us/step - loss: 18895.6932 - mse: 825114496.0000 - mae: 18895.6934 - val_loss: 16781.3934 - val_mse: 652299584.0000 - val_mae: 16781.3926\n",
            "Epoch 29/500\n",
            "80/80 [==============================] - 0s 610us/step - loss: 18684.4410 - mse: 810989440.0000 - mae: 18684.4414 - val_loss: 16720.2250 - val_mse: 647813952.0000 - val_mae: 16720.2246\n",
            "Epoch 30/500\n",
            "80/80 [==============================] - 0s 597us/step - loss: 18583.6361 - mse: 808981120.0000 - mae: 18583.6367 - val_loss: 16663.5813 - val_mse: 643629376.0000 - val_mae: 16663.5820\n",
            "Epoch 31/500\n",
            "80/80 [==============================] - 0s 785us/step - loss: 18510.8016 - mse: 802203648.0000 - mae: 18510.8008 - val_loss: 16601.2429 - val_mse: 639064000.0000 - val_mae: 16601.2422\n",
            "Epoch 32/500\n",
            "80/80 [==============================] - 0s 606us/step - loss: 18444.0314 - mse: 792352320.0000 - mae: 18444.0332 - val_loss: 16537.5950 - val_mse: 634446144.0000 - val_mae: 16537.5957\n",
            "Epoch 33/500\n",
            "80/80 [==============================] - 0s 651us/step - loss: 18332.3461 - mse: 789608448.0000 - mae: 18332.3477 - val_loss: 16465.8673 - val_mse: 629296064.0000 - val_mae: 16465.8672\n",
            "Epoch 34/500\n",
            "80/80 [==============================] - 0s 644us/step - loss: 18391.3042 - mse: 789796544.0000 - mae: 18391.3047 - val_loss: 16407.6939 - val_mse: 625158848.0000 - val_mae: 16407.6953\n",
            "Epoch 35/500\n",
            "80/80 [==============================] - 0s 659us/step - loss: 18227.2563 - mse: 777156928.0000 - mae: 18227.2559 - val_loss: 16356.0790 - val_mse: 621517632.0000 - val_mae: 16356.0781\n",
            "Epoch 36/500\n",
            "80/80 [==============================] - 0s 586us/step - loss: 18331.5529 - mse: 777010880.0000 - mae: 18331.5527 - val_loss: 16305.2784 - val_mse: 617961920.0000 - val_mae: 16305.2783\n",
            "Epoch 37/500\n",
            "80/80 [==============================] - 0s 572us/step - loss: 18544.2900 - mse: 794400896.0000 - mae: 18544.2910 - val_loss: 16264.4783 - val_mse: 615127424.0000 - val_mae: 16264.4785\n",
            "Epoch 38/500\n",
            "80/80 [==============================] - 0s 531us/step - loss: 18404.5703 - mse: 784859008.0000 - mae: 18404.5703 - val_loss: 16226.1921 - val_mse: 612486336.0000 - val_mae: 16226.1924\n",
            "Epoch 39/500\n",
            "80/80 [==============================] - 0s 625us/step - loss: 18217.2855 - mse: 774803840.0000 - mae: 18217.2852 - val_loss: 16165.7040 - val_mse: 608358656.0000 - val_mae: 16165.7031\n",
            "Epoch 40/500\n",
            "80/80 [==============================] - 0s 607us/step - loss: 18267.5969 - mse: 774240128.0000 - mae: 18267.5977 - val_loss: 16120.2182 - val_mse: 605422400.0000 - val_mae: 16120.2178\n",
            "Epoch 41/500\n",
            "80/80 [==============================] - 0s 608us/step - loss: 18153.4427 - mse: 764525888.0000 - mae: 18153.4434 - val_loss: 15735.4991 - val_mse: 600794240.0000 - val_mae: 15735.5000\n",
            "Epoch 42/500\n",
            "80/80 [==============================] - 0s 625us/step - loss: 17744.2881 - mse: 760391936.0000 - mae: 17744.2871 - val_loss: 15461.2328 - val_mse: 594218432.0000 - val_mae: 15461.2324\n",
            "Epoch 43/500\n",
            "80/80 [==============================] - 0s 654us/step - loss: 17487.3591 - mse: 746243328.0000 - mae: 17487.3613 - val_loss: 15317.1489 - val_mse: 587191552.0000 - val_mae: 15317.1484\n",
            "Epoch 44/500\n",
            "80/80 [==============================] - 0s 753us/step - loss: 17225.7288 - mse: 728841600.0000 - mae: 17225.7285 - val_loss: 15643.2226 - val_mse: 581964800.0000 - val_mae: 15643.2217\n",
            "Epoch 45/500\n",
            "80/80 [==============================] - 0s 686us/step - loss: 17193.3359 - mse: 739925696.0000 - mae: 17193.3379 - val_loss: 15269.5596 - val_mse: 573270656.0000 - val_mae: 15269.5605\n",
            "Epoch 46/500\n",
            "80/80 [==============================] - 0s 681us/step - loss: 16857.9281 - mse: 727393024.0000 - mae: 16857.9277 - val_loss: 14728.6827 - val_mse: 564146048.0000 - val_mae: 14728.6826\n",
            "Epoch 47/500\n",
            "80/80 [==============================] - 0s 758us/step - loss: 16574.0588 - mse: 700231040.0000 - mae: 16574.0586 - val_loss: 15151.0975 - val_mse: 558485376.0000 - val_mae: 15151.0967\n",
            "Epoch 48/500\n",
            "80/80 [==============================] - 0s 752us/step - loss: 16624.6599 - mse: 697613824.0000 - mae: 16624.6602 - val_loss: 14787.7877 - val_mse: 549584064.0000 - val_mae: 14787.7871\n",
            "Epoch 49/500\n",
            "80/80 [==============================] - 0s 673us/step - loss: 16523.0527 - mse: 714006848.0000 - mae: 16523.0508 - val_loss: 14542.5722 - val_mse: 542142208.0000 - val_mae: 14542.5723\n",
            "Epoch 50/500\n",
            "80/80 [==============================] - 0s 704us/step - loss: 16042.8699 - mse: 679305536.0000 - mae: 16042.8691 - val_loss: 14311.4366 - val_mse: 534704704.0000 - val_mae: 14311.4365\n",
            "Epoch 51/500\n",
            "80/80 [==============================] - 0s 596us/step - loss: 16063.3118 - mse: 676744320.0000 - mae: 16063.3105 - val_loss: 14338.1456 - val_mse: 528002240.0000 - val_mae: 14338.1455\n",
            "Epoch 52/500\n",
            "80/80 [==============================] - 0s 658us/step - loss: 15936.4412 - mse: 665020032.0000 - mae: 15936.4404 - val_loss: 14015.2757 - val_mse: 519516672.0000 - val_mae: 14015.2754\n",
            "Epoch 53/500\n",
            "80/80 [==============================] - 0s 576us/step - loss: 15640.3031 - mse: 653688512.0000 - mae: 15640.3027 - val_loss: 13924.7425 - val_mse: 512645472.0000 - val_mae: 13924.7432\n",
            "Epoch 54/500\n",
            "80/80 [==============================] - 0s 583us/step - loss: 15575.5172 - mse: 657209344.0000 - mae: 15575.5156 - val_loss: 13619.1087 - val_mse: 505039712.0000 - val_mae: 13619.1094\n",
            "Epoch 55/500\n",
            "80/80 [==============================] - 0s 642us/step - loss: 15477.6450 - mse: 658227264.0000 - mae: 15477.6455 - val_loss: 13541.9322 - val_mse: 498138976.0000 - val_mae: 13541.9326\n",
            "Epoch 56/500\n",
            "80/80 [==============================] - 0s 656us/step - loss: 15442.2299 - mse: 653469632.0000 - mae: 15442.2295 - val_loss: 13335.3882 - val_mse: 491145216.0000 - val_mae: 13335.3887\n",
            "Epoch 57/500\n",
            "80/80 [==============================] - 0s 633us/step - loss: 15182.9202 - mse: 628247360.0000 - mae: 15182.9189 - val_loss: 13577.8616 - val_mse: 486415200.0000 - val_mae: 13577.8613\n",
            "Epoch 58/500\n",
            "80/80 [==============================] - 0s 620us/step - loss: 15352.0963 - mse: 628620672.0000 - mae: 15352.0967 - val_loss: 13167.4365 - val_mse: 477740992.0000 - val_mae: 13167.4355\n",
            "Epoch 59/500\n",
            "80/80 [==============================] - 0s 646us/step - loss: 15149.6699 - mse: 632909760.0000 - mae: 15149.6689 - val_loss: 12956.5352 - val_mse: 470955744.0000 - val_mae: 12956.5361\n",
            "Epoch 60/500\n",
            "80/80 [==============================] - 0s 578us/step - loss: 14749.1322 - mse: 598179136.0000 - mae: 14749.1309 - val_loss: 12707.7052 - val_mse: 463536448.0000 - val_mae: 12707.7051\n",
            "Epoch 61/500\n",
            "80/80 [==============================] - 0s 525us/step - loss: 14807.1330 - mse: 599887552.0000 - mae: 14807.1328 - val_loss: 12643.9074 - val_mse: 457475040.0000 - val_mae: 12643.9072\n",
            "Epoch 62/500\n",
            "80/80 [==============================] - 0s 607us/step - loss: 14566.9645 - mse: 609453696.0000 - mae: 14566.9639 - val_loss: 12536.3078 - val_mse: 451226976.0000 - val_mae: 12536.3076\n",
            "Epoch 63/500\n",
            "80/80 [==============================] - 0s 648us/step - loss: 14382.5365 - mse: 589189952.0000 - mae: 14382.5371 - val_loss: 12378.5955 - val_mse: 445627232.0000 - val_mae: 12378.5957\n",
            "Epoch 64/500\n",
            "80/80 [==============================] - 0s 538us/step - loss: 14188.0588 - mse: 576543616.0000 - mae: 14188.0596 - val_loss: 12354.2303 - val_mse: 439601664.0000 - val_mae: 12354.2305\n",
            "Epoch 65/500\n",
            "80/80 [==============================] - 0s 592us/step - loss: 14327.1635 - mse: 582017152.0000 - mae: 14327.1641 - val_loss: 12255.6412 - val_mse: 434300576.0000 - val_mae: 12255.6406\n",
            "Epoch 66/500\n",
            "80/80 [==============================] - 0s 669us/step - loss: 14024.7393 - mse: 560989568.0000 - mae: 14024.7393 - val_loss: 12146.2864 - val_mse: 428949984.0000 - val_mae: 12146.2861\n",
            "Epoch 67/500\n",
            "80/80 [==============================] - 0s 653us/step - loss: 14042.6634 - mse: 560101952.0000 - mae: 14042.6621 - val_loss: 12019.9365 - val_mse: 423375904.0000 - val_mae: 12019.9365\n",
            "Epoch 68/500\n",
            "80/80 [==============================] - 0s 606us/step - loss: 14093.5836 - mse: 553611200.0000 - mae: 14093.5830 - val_loss: 11971.3538 - val_mse: 418169344.0000 - val_mae: 11971.3535\n",
            "Epoch 69/500\n",
            "80/80 [==============================] - 0s 646us/step - loss: 13823.3756 - mse: 549568256.0000 - mae: 13823.3770 - val_loss: 11953.6736 - val_mse: 412741440.0000 - val_mae: 11953.6729\n",
            "Epoch 70/500\n",
            "80/80 [==============================] - 0s 668us/step - loss: 13419.1754 - mse: 525630272.0000 - mae: 13419.1748 - val_loss: 11816.7483 - val_mse: 406860672.0000 - val_mae: 11816.7480\n",
            "Epoch 71/500\n",
            "80/80 [==============================] - 0s 691us/step - loss: 13889.3473 - mse: 565539008.0000 - mae: 13889.3467 - val_loss: 11760.9519 - val_mse: 401482080.0000 - val_mae: 11760.9521\n",
            "Epoch 72/500\n",
            "80/80 [==============================] - 0s 614us/step - loss: 13703.9682 - mse: 541229568.0000 - mae: 13703.9668 - val_loss: 11636.8499 - val_mse: 395235808.0000 - val_mae: 11636.8496\n",
            "Epoch 73/500\n",
            "80/80 [==============================] - 0s 645us/step - loss: 13829.3433 - mse: 538699456.0000 - mae: 13829.3438 - val_loss: 11580.6144 - val_mse: 391433792.0000 - val_mae: 11580.6143\n",
            "Epoch 74/500\n",
            "80/80 [==============================] - 0s 581us/step - loss: 13635.8212 - mse: 532920224.0000 - mae: 13635.8203 - val_loss: 11582.9577 - val_mse: 388278016.0000 - val_mae: 11582.9570\n",
            "Epoch 75/500\n",
            "80/80 [==============================] - 0s 579us/step - loss: 13154.8726 - mse: 509956448.0000 - mae: 13154.8721 - val_loss: 11449.0882 - val_mse: 382506400.0000 - val_mae: 11449.0889\n",
            "Epoch 76/500\n",
            "80/80 [==============================] - 0s 554us/step - loss: 13352.1542 - mse: 497808448.0000 - mae: 13352.1543 - val_loss: 11406.0607 - val_mse: 377365344.0000 - val_mae: 11406.0605\n",
            "Epoch 77/500\n",
            "80/80 [==============================] - 0s 628us/step - loss: 13644.7227 - mse: 527536992.0000 - mae: 13644.7217 - val_loss: 11360.4840 - val_mse: 373548576.0000 - val_mae: 11360.4844\n",
            "Epoch 78/500\n",
            "80/80 [==============================] - 0s 631us/step - loss: 13243.9118 - mse: 499197088.0000 - mae: 13243.9111 - val_loss: 11279.5965 - val_mse: 368107680.0000 - val_mae: 11279.5967\n",
            "Epoch 79/500\n",
            "80/80 [==============================] - 0s 632us/step - loss: 13026.8477 - mse: 489345856.0000 - mae: 13026.8477 - val_loss: 11118.4720 - val_mse: 362921088.0000 - val_mae: 11118.4727\n",
            "Epoch 80/500\n",
            "80/80 [==============================] - 0s 593us/step - loss: 12674.5954 - mse: 462845120.0000 - mae: 12674.5957 - val_loss: 11078.9709 - val_mse: 358074880.0000 - val_mae: 11078.9707\n",
            "Epoch 81/500\n",
            "80/80 [==============================] - 0s 640us/step - loss: 12755.0338 - mse: 471742048.0000 - mae: 12755.0342 - val_loss: 11047.3003 - val_mse: 353583072.0000 - val_mae: 11047.2998\n",
            "Epoch 82/500\n",
            "80/80 [==============================] - 0s 589us/step - loss: 13026.3773 - mse: 483375296.0000 - mae: 13026.3770 - val_loss: 10727.9560 - val_mse: 346804480.0000 - val_mae: 10727.9551\n",
            "Epoch 83/500\n",
            "80/80 [==============================] - 0s 671us/step - loss: 12917.3241 - mse: 492364448.0000 - mae: 12917.3242 - val_loss: 10718.7738 - val_mse: 342603840.0000 - val_mae: 10718.7734\n",
            "Epoch 84/500\n",
            "80/80 [==============================] - 0s 653us/step - loss: 12409.1470 - mse: 451278944.0000 - mae: 12409.1465 - val_loss: 10612.4823 - val_mse: 336952928.0000 - val_mae: 10612.4824\n",
            "Epoch 85/500\n",
            "80/80 [==============================] - 0s 619us/step - loss: 12278.1688 - mse: 460533568.0000 - mae: 12278.1699 - val_loss: 10483.1229 - val_mse: 331497824.0000 - val_mae: 10483.1230\n",
            "Epoch 86/500\n",
            "80/80 [==============================] - 0s 733us/step - loss: 12413.7154 - mse: 461355200.0000 - mae: 12413.7158 - val_loss: 10447.0714 - val_mse: 326069152.0000 - val_mae: 10447.0713\n",
            "Epoch 87/500\n",
            "80/80 [==============================] - 0s 617us/step - loss: 12142.2129 - mse: 425473536.0000 - mae: 12142.2129 - val_loss: 10269.5120 - val_mse: 319989632.0000 - val_mae: 10269.5127\n",
            "Epoch 88/500\n",
            "80/80 [==============================] - 0s 687us/step - loss: 12202.0230 - mse: 439177472.0000 - mae: 12202.0234 - val_loss: 9979.4811 - val_mse: 315350272.0000 - val_mae: 9979.4814\n",
            "Epoch 89/500\n",
            "80/80 [==============================] - 0s 736us/step - loss: 11750.4023 - mse: 424555968.0000 - mae: 11750.4023 - val_loss: 9920.5611 - val_mse: 308864832.0000 - val_mae: 9920.5605\n",
            "Epoch 90/500\n",
            "80/80 [==============================] - 0s 759us/step - loss: 11600.1253 - mse: 413291840.0000 - mae: 11600.1250 - val_loss: 9783.6596 - val_mse: 303769344.0000 - val_mae: 9783.6592\n",
            "Epoch 91/500\n",
            "80/80 [==============================] - 0s 647us/step - loss: 11584.7465 - mse: 406151840.0000 - mae: 11584.7451 - val_loss: 9580.8078 - val_mse: 297517120.0000 - val_mae: 9580.8076\n",
            "Epoch 92/500\n",
            "80/80 [==============================] - 0s 637us/step - loss: 11470.4790 - mse: 406068416.0000 - mae: 11470.4795 - val_loss: 9535.3565 - val_mse: 291846944.0000 - val_mae: 9535.3574\n",
            "Epoch 93/500\n",
            "80/80 [==============================] - 0s 556us/step - loss: 11374.4552 - mse: 399556544.0000 - mae: 11374.4551 - val_loss: 9328.3336 - val_mse: 285910272.0000 - val_mae: 9328.3340\n",
            "Epoch 94/500\n",
            "80/80 [==============================] - 0s 572us/step - loss: 11342.4052 - mse: 394320064.0000 - mae: 11342.4043 - val_loss: 8963.5270 - val_mse: 279199392.0000 - val_mae: 8963.5264\n",
            "Epoch 95/500\n",
            "80/80 [==============================] - 0s 633us/step - loss: 11182.0591 - mse: 393966560.0000 - mae: 11182.0596 - val_loss: 8965.2428 - val_mse: 274562144.0000 - val_mae: 8965.2432\n",
            "Epoch 96/500\n",
            "80/80 [==============================] - 0s 655us/step - loss: 11342.3190 - mse: 389221728.0000 - mae: 11342.3193 - val_loss: 8953.8677 - val_mse: 269789024.0000 - val_mae: 8953.8682\n",
            "Epoch 97/500\n",
            "80/80 [==============================] - 0s 696us/step - loss: 10844.3170 - mse: 372504064.0000 - mae: 10844.3174 - val_loss: 8839.9877 - val_mse: 263599808.0000 - val_mae: 8839.9873\n",
            "Epoch 98/500\n",
            "80/80 [==============================] - 0s 582us/step - loss: 11055.4626 - mse: 378738592.0000 - mae: 11055.4629 - val_loss: 8744.1511 - val_mse: 259046512.0000 - val_mae: 8744.1514\n",
            "Epoch 99/500\n",
            "80/80 [==============================] - 0s 647us/step - loss: 10390.0607 - mse: 328528960.0000 - mae: 10390.0605 - val_loss: 8482.9791 - val_mse: 253589744.0000 - val_mae: 8482.9795\n",
            "Epoch 100/500\n",
            "80/80 [==============================] - 0s 655us/step - loss: 10363.5493 - mse: 339480512.0000 - mae: 10363.5498 - val_loss: 8683.8792 - val_mse: 249445024.0000 - val_mae: 8683.8789\n",
            "Epoch 101/500\n",
            "80/80 [==============================] - 0s 645us/step - loss: 10537.1262 - mse: 344787904.0000 - mae: 10537.1260 - val_loss: 8576.0592 - val_mse: 245276880.0000 - val_mae: 8576.0586\n",
            "Epoch 102/500\n",
            "80/80 [==============================] - 0s 600us/step - loss: 10725.6999 - mse: 361402176.0000 - mae: 10725.6992 - val_loss: 8554.1767 - val_mse: 240280352.0000 - val_mae: 8554.1768\n",
            "Epoch 103/500\n",
            "80/80 [==============================] - 0s 656us/step - loss: 10163.9335 - mse: 332244992.0000 - mae: 10163.9336 - val_loss: 8513.2764 - val_mse: 235339696.0000 - val_mae: 8513.2764\n",
            "Epoch 104/500\n",
            "80/80 [==============================] - 0s 629us/step - loss: 10166.6006 - mse: 344824320.0000 - mae: 10166.5996 - val_loss: 8544.8668 - val_mse: 230391776.0000 - val_mae: 8544.8672\n",
            "Epoch 105/500\n",
            "80/80 [==============================] - 0s 786us/step - loss: 10269.1384 - mse: 332293952.0000 - mae: 10269.1387 - val_loss: 8201.3656 - val_mse: 224339312.0000 - val_mae: 8201.3652\n",
            "Epoch 106/500\n",
            "80/80 [==============================] - 0s 544us/step - loss: 9901.2912 - mse: 286321120.0000 - mae: 9901.2910 - val_loss: 8327.5406 - val_mse: 221363552.0000 - val_mae: 8327.5400\n",
            "Epoch 107/500\n",
            "80/80 [==============================] - 0s 649us/step - loss: 9880.6517 - mse: 296494432.0000 - mae: 9880.6514 - val_loss: 8254.8076 - val_mse: 217246608.0000 - val_mae: 8254.8076\n",
            "Epoch 108/500\n",
            "80/80 [==============================] - 0s 709us/step - loss: 9517.9064 - mse: 298351968.0000 - mae: 9517.9072 - val_loss: 8137.9816 - val_mse: 214315712.0000 - val_mae: 8137.9819\n",
            "Epoch 109/500\n",
            "80/80 [==============================] - 0s 644us/step - loss: 10079.1874 - mse: 322796928.0000 - mae: 10079.1875 - val_loss: 8046.7557 - val_mse: 210207520.0000 - val_mae: 8046.7554\n",
            "Epoch 110/500\n",
            "80/80 [==============================] - 0s 664us/step - loss: 10217.9922 - mse: 331437632.0000 - mae: 10217.9932 - val_loss: 7947.8494 - val_mse: 206271200.0000 - val_mae: 7947.8491\n",
            "Epoch 111/500\n",
            "80/80 [==============================] - 0s 661us/step - loss: 10241.8083 - mse: 341868096.0000 - mae: 10241.8086 - val_loss: 7937.2187 - val_mse: 203899616.0000 - val_mae: 7937.2188\n",
            "Epoch 112/500\n",
            "80/80 [==============================] - 0s 596us/step - loss: 10121.1552 - mse: 330485152.0000 - mae: 10121.1543 - val_loss: 7815.9259 - val_mse: 200201904.0000 - val_mae: 7815.9258\n",
            "Epoch 113/500\n",
            "80/80 [==============================] - 0s 667us/step - loss: 9535.8194 - mse: 288606016.0000 - mae: 9535.8203 - val_loss: 7871.0099 - val_mse: 197042208.0000 - val_mae: 7871.0098\n",
            "Epoch 114/500\n",
            "80/80 [==============================] - 0s 813us/step - loss: 9807.3478 - mse: 307229856.0000 - mae: 9807.3486 - val_loss: 7773.7911 - val_mse: 193680272.0000 - val_mae: 7773.7910\n",
            "Epoch 115/500\n",
            "80/80 [==============================] - 0s 647us/step - loss: 9731.2219 - mse: 275511520.0000 - mae: 9731.2207 - val_loss: 7660.8712 - val_mse: 190189280.0000 - val_mae: 7660.8716\n",
            "Epoch 116/500\n",
            "80/80 [==============================] - 0s 626us/step - loss: 9116.7441 - mse: 250822576.0000 - mae: 9116.7441 - val_loss: 7741.1948 - val_mse: 186748240.0000 - val_mae: 7741.1948\n",
            "Epoch 117/500\n",
            "80/80 [==============================] - 0s 602us/step - loss: 9518.0357 - mse: 277142336.0000 - mae: 9518.0352 - val_loss: 7739.9875 - val_mse: 183148048.0000 - val_mae: 7739.9873\n",
            "Epoch 118/500\n",
            "80/80 [==============================] - 0s 599us/step - loss: 9193.4545 - mse: 265926352.0000 - mae: 9193.4541 - val_loss: 7671.7293 - val_mse: 180041648.0000 - val_mae: 7671.7295\n",
            "Epoch 119/500\n",
            "80/80 [==============================] - 0s 639us/step - loss: 9804.0853 - mse: 279839328.0000 - mae: 9804.0859 - val_loss: 7717.7064 - val_mse: 178167776.0000 - val_mae: 7717.7061\n",
            "Epoch 120/500\n",
            "80/80 [==============================] - 0s 638us/step - loss: 9633.6353 - mse: 278429632.0000 - mae: 9633.6348 - val_loss: 7476.4109 - val_mse: 174588976.0000 - val_mae: 7476.4111\n",
            "Epoch 121/500\n",
            "80/80 [==============================] - 0s 606us/step - loss: 8913.6097 - mse: 236470752.0000 - mae: 8913.6104 - val_loss: 7625.2039 - val_mse: 171740096.0000 - val_mae: 7625.2036\n",
            "Epoch 122/500\n",
            "80/80 [==============================] - 0s 658us/step - loss: 8868.0984 - mse: 246707664.0000 - mae: 8868.0986 - val_loss: 7549.1965 - val_mse: 169161648.0000 - val_mae: 7549.1963\n",
            "Epoch 123/500\n",
            "80/80 [==============================] - 0s 741us/step - loss: 8962.9942 - mse: 238440400.0000 - mae: 8962.9941 - val_loss: 7486.4491 - val_mse: 166564384.0000 - val_mae: 7486.4492\n",
            "Epoch 124/500\n",
            "80/80 [==============================] - 0s 669us/step - loss: 8544.8436 - mse: 226073120.0000 - mae: 8544.8438 - val_loss: 7442.1778 - val_mse: 164435088.0000 - val_mae: 7442.1777\n",
            "Epoch 125/500\n",
            "80/80 [==============================] - 0s 695us/step - loss: 8740.0855 - mse: 227761376.0000 - mae: 8740.0859 - val_loss: 7410.9537 - val_mse: 161278208.0000 - val_mae: 7410.9536\n",
            "Epoch 126/500\n",
            "80/80 [==============================] - 0s 705us/step - loss: 8178.4671 - mse: 212664320.0000 - mae: 8178.4663 - val_loss: 7135.2066 - val_mse: 158508816.0000 - val_mae: 7135.2065\n",
            "Epoch 127/500\n",
            "80/80 [==============================] - 0s 662us/step - loss: 8303.5992 - mse: 207047360.0000 - mae: 8303.5996 - val_loss: 7125.3867 - val_mse: 156406608.0000 - val_mae: 7125.3867\n",
            "Epoch 128/500\n",
            "80/80 [==============================] - 0s 636us/step - loss: 8300.5972 - mse: 213108816.0000 - mae: 8300.5977 - val_loss: 7264.8093 - val_mse: 154783680.0000 - val_mae: 7264.8096\n",
            "Epoch 129/500\n",
            "80/80 [==============================] - 0s 619us/step - loss: 7676.1622 - mse: 197330800.0000 - mae: 7676.1626 - val_loss: 7153.2260 - val_mse: 152112848.0000 - val_mae: 7153.2261\n",
            "Epoch 130/500\n",
            "80/80 [==============================] - 0s 651us/step - loss: 8502.2026 - mse: 221927008.0000 - mae: 8502.2021 - val_loss: 7196.6593 - val_mse: 150472800.0000 - val_mae: 7196.6587\n",
            "Epoch 131/500\n",
            "80/80 [==============================] - 0s 684us/step - loss: 8746.8098 - mse: 223184944.0000 - mae: 8746.8096 - val_loss: 7441.8911 - val_mse: 148524928.0000 - val_mae: 7441.8911\n",
            "Epoch 132/500\n",
            "80/80 [==============================] - 0s 655us/step - loss: 8314.0269 - mse: 197898208.0000 - mae: 8314.0273 - val_loss: 7164.9450 - val_mse: 145718320.0000 - val_mae: 7164.9453\n",
            "Epoch 133/500\n",
            "80/80 [==============================] - 0s 620us/step - loss: 7790.8676 - mse: 190421920.0000 - mae: 7790.8672 - val_loss: 7030.6374 - val_mse: 143137936.0000 - val_mae: 7030.6377\n",
            "Epoch 134/500\n",
            "80/80 [==============================] - 0s 665us/step - loss: 9004.5280 - mse: 232655056.0000 - mae: 9004.5283 - val_loss: 7048.8223 - val_mse: 141610432.0000 - val_mae: 7048.8223\n",
            "Epoch 135/500\n",
            "80/80 [==============================] - 0s 655us/step - loss: 8465.0234 - mse: 218389936.0000 - mae: 8465.0225 - val_loss: 7105.6997 - val_mse: 139788048.0000 - val_mae: 7105.7002\n",
            "Epoch 136/500\n",
            "80/80 [==============================] - 0s 605us/step - loss: 9129.9940 - mse: 252015136.0000 - mae: 9129.9941 - val_loss: 7030.4850 - val_mse: 137920400.0000 - val_mae: 7030.4849\n",
            "Epoch 137/500\n",
            "80/80 [==============================] - 0s 674us/step - loss: 8262.0833 - mse: 206850208.0000 - mae: 8262.0840 - val_loss: 6964.9604 - val_mse: 136608128.0000 - val_mae: 6964.9604\n",
            "Epoch 138/500\n",
            "80/80 [==============================] - 0s 581us/step - loss: 8155.7873 - mse: 196613152.0000 - mae: 8155.7876 - val_loss: 6874.0423 - val_mse: 135356784.0000 - val_mae: 6874.0425\n",
            "Epoch 139/500\n",
            "80/80 [==============================] - 0s 653us/step - loss: 8300.7657 - mse: 198836224.0000 - mae: 8300.7656 - val_loss: 6878.3697 - val_mse: 133444960.0000 - val_mae: 6878.3696\n",
            "Epoch 140/500\n",
            "80/80 [==============================] - 0s 623us/step - loss: 8658.0772 - mse: 236700672.0000 - mae: 8658.0771 - val_loss: 6839.7941 - val_mse: 132085728.0000 - val_mae: 6839.7944\n",
            "Epoch 141/500\n",
            "80/80 [==============================] - 0s 636us/step - loss: 8274.3069 - mse: 190347296.0000 - mae: 8274.3066 - val_loss: 6706.2710 - val_mse: 130495736.0000 - val_mae: 6706.2710\n",
            "Epoch 142/500\n",
            "80/80 [==============================] - 0s 703us/step - loss: 8096.7084 - mse: 214426544.0000 - mae: 8096.7080 - val_loss: 6872.9656 - val_mse: 129906104.0000 - val_mae: 6872.9658\n",
            "Epoch 143/500\n",
            "80/80 [==============================] - 0s 713us/step - loss: 9029.5831 - mse: 251890384.0000 - mae: 9029.5840 - val_loss: 6739.3173 - val_mse: 128269696.0000 - val_mae: 6739.3169\n",
            "Epoch 144/500\n",
            "80/80 [==============================] - 0s 717us/step - loss: 8501.5764 - mse: 216141088.0000 - mae: 8501.5762 - val_loss: 6716.9972 - val_mse: 127329088.0000 - val_mae: 6716.9971\n",
            "Epoch 145/500\n",
            "80/80 [==============================] - 0s 602us/step - loss: 7681.0265 - mse: 175739392.0000 - mae: 7681.0264 - val_loss: 6802.2963 - val_mse: 126914824.0000 - val_mae: 6802.2964\n",
            "Epoch 146/500\n",
            "80/80 [==============================] - 0s 606us/step - loss: 8085.3911 - mse: 197801760.0000 - mae: 8085.3906 - val_loss: 6710.8123 - val_mse: 125024664.0000 - val_mae: 6710.8125\n",
            "Epoch 147/500\n",
            "80/80 [==============================] - 0s 716us/step - loss: 7784.2751 - mse: 192814064.0000 - mae: 7784.2749 - val_loss: 6646.8340 - val_mse: 122884944.0000 - val_mae: 6646.8340\n",
            "Epoch 148/500\n",
            "80/80 [==============================] - 0s 656us/step - loss: 7560.8626 - mse: 172597536.0000 - mae: 7560.8623 - val_loss: 6821.6149 - val_mse: 122236592.0000 - val_mae: 6821.6152\n",
            "Epoch 149/500\n",
            "80/80 [==============================] - 0s 599us/step - loss: 7854.6353 - mse: 172396096.0000 - mae: 7854.6353 - val_loss: 6722.1612 - val_mse: 121473184.0000 - val_mae: 6722.1611\n",
            "Epoch 150/500\n",
            "80/80 [==============================] - 0s 639us/step - loss: 7680.5254 - mse: 171749792.0000 - mae: 7680.5249 - val_loss: 6740.2746 - val_mse: 121158088.0000 - val_mae: 6740.2744\n",
            "Epoch 151/500\n",
            "80/80 [==============================] - 0s 593us/step - loss: 9132.6711 - mse: 214891296.0000 - mae: 9132.6709 - val_loss: 6680.6544 - val_mse: 120451912.0000 - val_mae: 6680.6543\n",
            "Epoch 152/500\n",
            "80/80 [==============================] - 0s 643us/step - loss: 7698.6767 - mse: 177576672.0000 - mae: 7698.6758 - val_loss: 6486.9484 - val_mse: 118586896.0000 - val_mae: 6486.9482\n",
            "Epoch 153/500\n",
            "80/80 [==============================] - 0s 561us/step - loss: 7781.2921 - mse: 172144496.0000 - mae: 7781.2920 - val_loss: 6666.7094 - val_mse: 118798592.0000 - val_mae: 6666.7095\n",
            "Epoch 154/500\n",
            "80/80 [==============================] - 0s 682us/step - loss: 7864.9760 - mse: 175699744.0000 - mae: 7864.9756 - val_loss: 6727.6538 - val_mse: 118124864.0000 - val_mae: 6727.6538\n",
            "Epoch 155/500\n",
            "80/80 [==============================] - 0s 661us/step - loss: 7520.3558 - mse: 159474336.0000 - mae: 7520.3555 - val_loss: 6641.9351 - val_mse: 116572712.0000 - val_mae: 6641.9351\n",
            "Epoch 156/500\n",
            "80/80 [==============================] - 0s 642us/step - loss: 7297.2643 - mse: 151340144.0000 - mae: 7297.2642 - val_loss: 6636.2182 - val_mse: 115908960.0000 - val_mae: 6636.2178\n",
            "Epoch 157/500\n",
            "80/80 [==============================] - 0s 603us/step - loss: 7627.5124 - mse: 169549536.0000 - mae: 7627.5127 - val_loss: 6591.5191 - val_mse: 115019072.0000 - val_mae: 6591.5190\n",
            "Epoch 158/500\n",
            "80/80 [==============================] - 0s 583us/step - loss: 7317.7398 - mse: 156611168.0000 - mae: 7317.7397 - val_loss: 6783.1471 - val_mse: 113969096.0000 - val_mae: 6783.1470\n",
            "Epoch 159/500\n",
            "80/80 [==============================] - 0s 646us/step - loss: 7973.2709 - mse: 175028064.0000 - mae: 7973.2705 - val_loss: 6599.6924 - val_mse: 113310088.0000 - val_mae: 6599.6924\n",
            "Epoch 160/500\n",
            "80/80 [==============================] - 0s 759us/step - loss: 7856.5747 - mse: 167073408.0000 - mae: 7856.5742 - val_loss: 6513.7357 - val_mse: 112701568.0000 - val_mae: 6513.7358\n",
            "Epoch 161/500\n",
            "80/80 [==============================] - 0s 848us/step - loss: 8300.2828 - mse: 179999376.0000 - mae: 8300.2832 - val_loss: 6558.3231 - val_mse: 112644048.0000 - val_mae: 6558.3232\n",
            "Epoch 162/500\n",
            "80/80 [==============================] - 0s 681us/step - loss: 7008.7850 - mse: 123631760.0000 - mae: 7008.7842 - val_loss: 6587.1390 - val_mse: 112313344.0000 - val_mae: 6587.1392\n",
            "Epoch 163/500\n",
            "80/80 [==============================] - 0s 769us/step - loss: 7350.3618 - mse: 165911840.0000 - mae: 7350.3618 - val_loss: 6696.0282 - val_mse: 112498016.0000 - val_mae: 6696.0288\n",
            "Epoch 164/500\n",
            "80/80 [==============================] - 0s 675us/step - loss: 7759.7990 - mse: 159217344.0000 - mae: 7759.7983 - val_loss: 6700.1421 - val_mse: 112097656.0000 - val_mae: 6700.1421\n",
            "Epoch 165/500\n",
            "80/80 [==============================] - 0s 652us/step - loss: 7928.9995 - mse: 159064400.0000 - mae: 7928.9990 - val_loss: 6777.1041 - val_mse: 112069048.0000 - val_mae: 6777.1040\n",
            "Epoch 166/500\n",
            "80/80 [==============================] - 0s 653us/step - loss: 6999.7911 - mse: 135128352.0000 - mae: 6999.7915 - val_loss: 6620.0640 - val_mse: 110884176.0000 - val_mae: 6620.0640\n",
            "Epoch 167/500\n",
            "80/80 [==============================] - 0s 586us/step - loss: 8571.0595 - mse: 216443232.0000 - mae: 8571.0596 - val_loss: 6594.0893 - val_mse: 110609760.0000 - val_mae: 6594.0894\n",
            "Epoch 168/500\n",
            "80/80 [==============================] - 0s 707us/step - loss: 6709.4186 - mse: 134154200.0000 - mae: 6709.4189 - val_loss: 6586.5404 - val_mse: 110421072.0000 - val_mae: 6586.5400\n",
            "Epoch 169/500\n",
            "80/80 [==============================] - 0s 672us/step - loss: 6365.9479 - mse: 127057384.0000 - mae: 6365.9482 - val_loss: 6552.9858 - val_mse: 109845984.0000 - val_mae: 6552.9858\n",
            "Epoch 170/500\n",
            "80/80 [==============================] - 0s 682us/step - loss: 7515.2889 - mse: 163004256.0000 - mae: 7515.2891 - val_loss: 6539.6651 - val_mse: 109397944.0000 - val_mae: 6539.6650\n",
            "Epoch 171/500\n",
            "80/80 [==============================] - 0s 688us/step - loss: 8126.7038 - mse: 171064688.0000 - mae: 8126.7041 - val_loss: 6499.4320 - val_mse: 108807496.0000 - val_mae: 6499.4321\n",
            "Epoch 172/500\n",
            "80/80 [==============================] - 0s 599us/step - loss: 7171.3998 - mse: 135649504.0000 - mae: 7171.3999 - val_loss: 6577.9784 - val_mse: 109384168.0000 - val_mae: 6577.9785\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 90 samples, validate on 90 samples\n",
            "Epoch 1/500\n",
            "90/90 [==============================] - 2s 21ms/step - loss: 27657.2554 - mse: 1635663104.0000 - mae: 27657.2559 - val_loss: 32679.8760 - val_mse: 2082572800.0000 - val_mae: 32679.8770\n",
            "Epoch 2/500\n",
            "90/90 [==============================] - 0s 734us/step - loss: 27654.7624 - mse: 1635605760.0000 - mae: 27654.7637 - val_loss: 32671.7702 - val_mse: 2082310400.0000 - val_mae: 32671.7715\n",
            "Epoch 3/500\n",
            "90/90 [==============================] - 0s 643us/step - loss: 27638.8979 - mse: 1634974464.0000 - mae: 27638.8965 - val_loss: 32646.3189 - val_mse: 2080991872.0000 - val_mae: 32646.3203\n",
            "Epoch 4/500\n",
            "90/90 [==============================] - 0s 632us/step - loss: 27611.8299 - mse: 1633804416.0000 - mae: 27611.8301 - val_loss: 32611.6390 - val_mse: 2078943488.0000 - val_mae: 32611.6387\n",
            "Epoch 5/500\n",
            "90/90 [==============================] - 0s 699us/step - loss: 27567.7331 - mse: 1631396992.0000 - mae: 27567.7324 - val_loss: 32565.9644 - val_mse: 2076105856.0000 - val_mae: 32565.9648\n",
            "Epoch 6/500\n",
            "90/90 [==============================] - 0s 763us/step - loss: 27526.1676 - mse: 1629452032.0000 - mae: 27526.1699 - val_loss: 32516.9412 - val_mse: 2073018752.0000 - val_mae: 32516.9414\n",
            "Epoch 7/500\n",
            "90/90 [==============================] - 0s 860us/step - loss: 27483.4462 - mse: 1626657792.0000 - mae: 27483.4473 - val_loss: 32464.8919 - val_mse: 2069706112.0000 - val_mae: 32464.8926\n",
            "Epoch 8/500\n",
            "90/90 [==============================] - 0s 600us/step - loss: 27424.8938 - mse: 1622849280.0000 - mae: 27424.8945 - val_loss: 32400.1168 - val_mse: 2065560704.0000 - val_mae: 32400.1172\n",
            "Epoch 9/500\n",
            "90/90 [==============================] - 0s 590us/step - loss: 27383.7638 - mse: 1620679296.0000 - mae: 27383.7676 - val_loss: 32334.5727 - val_mse: 2061349632.0000 - val_mae: 32334.5723\n",
            "Epoch 10/500\n",
            "90/90 [==============================] - 0s 700us/step - loss: 27315.9933 - mse: 1615690112.0000 - mae: 27315.9941 - val_loss: 32260.9811 - val_mse: 2056621568.0000 - val_mae: 32260.9805\n",
            "Epoch 11/500\n",
            "90/90 [==============================] - 0s 675us/step - loss: 27263.6061 - mse: 1610817024.0000 - mae: 27263.6055 - val_loss: 32183.2441 - val_mse: 2051562752.0000 - val_mae: 32183.2441\n",
            "Epoch 12/500\n",
            "90/90 [==============================] - 0s 683us/step - loss: 27204.5141 - mse: 1608242560.0000 - mae: 27204.5117 - val_loss: 32101.6196 - val_mse: 2046098176.0000 - val_mae: 32101.6191\n",
            "Epoch 13/500\n",
            "90/90 [==============================] - 0s 711us/step - loss: 27091.0064 - mse: 1601534336.0000 - mae: 27091.0078 - val_loss: 32009.5324 - val_mse: 2039678720.0000 - val_mae: 32009.5332\n",
            "Epoch 14/500\n",
            "90/90 [==============================] - 0s 729us/step - loss: 27033.8774 - mse: 1596692864.0000 - mae: 27033.8750 - val_loss: 31925.8719 - val_mse: 2033586432.0000 - val_mae: 31925.8730\n",
            "Epoch 15/500\n",
            "90/90 [==============================] - 0s 732us/step - loss: 27001.1111 - mse: 1594537472.0000 - mae: 27001.1113 - val_loss: 31841.0644 - val_mse: 2027091072.0000 - val_mae: 31841.0645\n",
            "Epoch 16/500\n",
            "90/90 [==============================] - 0s 661us/step - loss: 26927.2992 - mse: 1586696832.0000 - mae: 26927.3008 - val_loss: 31764.2593 - val_mse: 2020837120.0000 - val_mae: 31764.2617\n",
            "Epoch 17/500\n",
            "90/90 [==============================] - 0s 729us/step - loss: 26819.4894 - mse: 1583992192.0000 - mae: 26819.4883 - val_loss: 31676.0924 - val_mse: 2013666944.0000 - val_mae: 31676.0938\n",
            "Epoch 18/500\n",
            "90/90 [==============================] - 0s 742us/step - loss: 26724.6692 - mse: 1573618432.0000 - mae: 26724.6660 - val_loss: 31585.7265 - val_mse: 2006332928.0000 - val_mae: 31585.7285\n",
            "Epoch 19/500\n",
            "90/90 [==============================] - 0s 729us/step - loss: 26633.4500 - mse: 1567526400.0000 - mae: 26633.4492 - val_loss: 31492.6766 - val_mse: 1998807168.0000 - val_mae: 31492.6777\n",
            "Epoch 20/500\n",
            "90/90 [==============================] - 0s 763us/step - loss: 26639.8112 - mse: 1563036928.0000 - mae: 26639.8105 - val_loss: 31412.9154 - val_mse: 1992363520.0000 - val_mae: 31412.9141\n",
            "Epoch 21/500\n",
            "90/90 [==============================] - 0s 741us/step - loss: 26526.7752 - mse: 1557532800.0000 - mae: 26526.7754 - val_loss: 31321.7113 - val_mse: 1985034112.0000 - val_mae: 31321.7109\n",
            "Epoch 22/500\n",
            "90/90 [==============================] - 0s 832us/step - loss: 26496.4009 - mse: 1553664256.0000 - mae: 26496.4004 - val_loss: 31235.1347 - val_mse: 1977638016.0000 - val_mae: 31235.1328\n",
            "Epoch 23/500\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 26323.4722 - mse: 1540514688.0000 - mae: 26323.4727 - val_loss: 31145.4265 - val_mse: 1969666688.0000 - val_mae: 31145.4258\n",
            "Epoch 24/500\n",
            "90/90 [==============================] - 0s 699us/step - loss: 26294.8516 - mse: 1541258368.0000 - mae: 26294.8496 - val_loss: 31064.2891 - val_mse: 1962249728.0000 - val_mae: 31064.2891\n",
            "Epoch 25/500\n",
            "90/90 [==============================] - 0s 673us/step - loss: 26239.3342 - mse: 1530635136.0000 - mae: 26239.3359 - val_loss: 30981.0771 - val_mse: 1954340736.0000 - val_mae: 30981.0781\n",
            "Epoch 26/500\n",
            "90/90 [==============================] - 0s 686us/step - loss: 26121.3934 - mse: 1523256192.0000 - mae: 26121.3945 - val_loss: 30894.3301 - val_mse: 1946095872.0000 - val_mae: 30894.3301\n",
            "Epoch 27/500\n",
            "90/90 [==============================] - 0s 714us/step - loss: 26096.2693 - mse: 1515686144.0000 - mae: 26096.2695 - val_loss: 30809.1689 - val_mse: 1937659776.0000 - val_mae: 30809.1699\n",
            "Epoch 28/500\n",
            "90/90 [==============================] - 0s 744us/step - loss: 26038.0358 - mse: 1512376320.0000 - mae: 26038.0352 - val_loss: 30724.7671 - val_mse: 1929134848.0000 - val_mae: 30724.7676\n",
            "Epoch 29/500\n",
            "90/90 [==============================] - 0s 726us/step - loss: 25958.9545 - mse: 1503569152.0000 - mae: 25958.9551 - val_loss: 30639.6701 - val_mse: 1920321280.0000 - val_mae: 30639.6699\n",
            "Epoch 30/500\n",
            "90/90 [==============================] - 0s 619us/step - loss: 25977.1469 - mse: 1505304448.0000 - mae: 25977.1445 - val_loss: 30567.6959 - val_mse: 1912707456.0000 - val_mae: 30567.6953\n",
            "Epoch 31/500\n",
            "90/90 [==============================] - 0s 775us/step - loss: 25729.8639 - mse: 1488795520.0000 - mae: 25729.8613 - val_loss: 30491.3404 - val_mse: 1904488704.0000 - val_mae: 30491.3398\n",
            "Epoch 32/500\n",
            "90/90 [==============================] - 0s 676us/step - loss: 25707.4400 - mse: 1475152128.0000 - mae: 25707.4395 - val_loss: 30407.8749 - val_mse: 1895548288.0000 - val_mae: 30407.8750\n",
            "Epoch 33/500\n",
            "90/90 [==============================] - 0s 724us/step - loss: 25602.7810 - mse: 1469117184.0000 - mae: 25602.7812 - val_loss: 30322.0694 - val_mse: 1886405888.0000 - val_mae: 30322.0703\n",
            "Epoch 34/500\n",
            "90/90 [==============================] - 0s 647us/step - loss: 25602.4208 - mse: 1464031488.0000 - mae: 25602.4219 - val_loss: 30253.2647 - val_mse: 1879104256.0000 - val_mae: 30253.2637\n",
            "Epoch 35/500\n",
            "90/90 [==============================] - 0s 665us/step - loss: 25574.4723 - mse: 1464572672.0000 - mae: 25574.4727 - val_loss: 30169.5245 - val_mse: 1870265472.0000 - val_mae: 30169.5254\n",
            "Epoch 36/500\n",
            "90/90 [==============================] - 0s 772us/step - loss: 25452.6708 - mse: 1454617600.0000 - mae: 25452.6719 - val_loss: 30104.3912 - val_mse: 1863416576.0000 - val_mae: 30104.3926\n",
            "Epoch 37/500\n",
            "90/90 [==============================] - 0s 807us/step - loss: 25422.4690 - mse: 1448884480.0000 - mae: 25422.4668 - val_loss: 30022.5178 - val_mse: 1854851200.0000 - val_mae: 30022.5195\n",
            "Epoch 38/500\n",
            "90/90 [==============================] - 0s 680us/step - loss: 25462.3424 - mse: 1453072896.0000 - mae: 25462.3438 - val_loss: 29959.3579 - val_mse: 1848269568.0000 - val_mae: 29959.3574\n",
            "Epoch 39/500\n",
            "90/90 [==============================] - 0s 694us/step - loss: 25128.3398 - mse: 1425182208.0000 - mae: 25128.3418 - val_loss: 29881.3180 - val_mse: 1840175872.0000 - val_mae: 29881.3164\n",
            "Epoch 40/500\n",
            "90/90 [==============================] - 0s 718us/step - loss: 25142.2002 - mse: 1425067264.0000 - mae: 25142.1992 - val_loss: 29802.1844 - val_mse: 1832010496.0000 - val_mae: 29802.1836\n",
            "Epoch 41/500\n",
            "90/90 [==============================] - 0s 824us/step - loss: 25268.9850 - mse: 1424554368.0000 - mae: 25268.9824 - val_loss: 29732.2817 - val_mse: 1824595840.0000 - val_mae: 29732.2832\n",
            "Epoch 42/500\n",
            "90/90 [==============================] - 0s 689us/step - loss: 25130.9026 - mse: 1415615744.0000 - mae: 25130.9023 - val_loss: 29664.4485 - val_mse: 1817371136.0000 - val_mae: 29664.4492\n",
            "Epoch 43/500\n",
            "90/90 [==============================] - 0s 651us/step - loss: 25158.4807 - mse: 1420552448.0000 - mae: 25158.4824 - val_loss: 29594.5327 - val_mse: 1809648512.0000 - val_mae: 29594.5332\n",
            "Epoch 44/500\n",
            "90/90 [==============================] - 0s 769us/step - loss: 25161.6480 - mse: 1420547968.0000 - mae: 25161.6504 - val_loss: 29531.9602 - val_mse: 1802467584.0000 - val_mae: 29531.9609\n",
            "Epoch 45/500\n",
            "90/90 [==============================] - 0s 731us/step - loss: 24905.6828 - mse: 1398687872.0000 - mae: 24905.6836 - val_loss: 29459.4168 - val_mse: 1794109184.0000 - val_mae: 29459.4160\n",
            "Epoch 46/500\n",
            "90/90 [==============================] - 0s 798us/step - loss: 25026.2503 - mse: 1392868864.0000 - mae: 25026.2500 - val_loss: 29394.2849 - val_mse: 1786338560.0000 - val_mae: 29394.2832\n",
            "Epoch 47/500\n",
            "90/90 [==============================] - 0s 651us/step - loss: 24929.0830 - mse: 1391244416.0000 - mae: 24929.0840 - val_loss: 29335.0162 - val_mse: 1779301888.0000 - val_mae: 29335.0176\n",
            "Epoch 48/500\n",
            "90/90 [==============================] - 0s 633us/step - loss: 24932.9978 - mse: 1378132096.0000 - mae: 24932.9980 - val_loss: 29276.3815 - val_mse: 1772117504.0000 - val_mae: 29276.3828\n",
            "Epoch 49/500\n",
            "90/90 [==============================] - 0s 717us/step - loss: 24636.7153 - mse: 1361111680.0000 - mae: 24636.7168 - val_loss: 29202.2845 - val_mse: 1764443904.0000 - val_mae: 29202.2832\n",
            "Epoch 50/500\n",
            "90/90 [==============================] - 0s 684us/step - loss: 24751.9902 - mse: 1380489472.0000 - mae: 24751.9922 - val_loss: 29033.9854 - val_mse: 1759384576.0000 - val_mae: 29033.9863\n",
            "Epoch 51/500\n",
            "90/90 [==============================] - 0s 667us/step - loss: 24631.2403 - mse: 1381292288.0000 - mae: 24631.2422 - val_loss: 28865.0119 - val_mse: 1745944576.0000 - val_mae: 28865.0117\n",
            "Epoch 52/500\n",
            "90/90 [==============================] - 0s 907us/step - loss: 24069.3951 - mse: 1338068608.0000 - mae: 24069.3945 - val_loss: 28725.3130 - val_mse: 1733897984.0000 - val_mae: 28725.3145\n",
            "Epoch 53/500\n",
            "90/90 [==============================] - 0s 666us/step - loss: 24091.5228 - mse: 1343324032.0000 - mae: 24091.5215 - val_loss: 28592.8202 - val_mse: 1723522560.0000 - val_mae: 28592.8203\n",
            "Epoch 54/500\n",
            "90/90 [==============================] - 0s 713us/step - loss: 23813.3201 - mse: 1325001984.0000 - mae: 23813.3223 - val_loss: 28446.3914 - val_mse: 1707817600.0000 - val_mae: 28446.3887\n",
            "Epoch 55/500\n",
            "90/90 [==============================] - 0s 587us/step - loss: 23870.4135 - mse: 1340313088.0000 - mae: 23870.4141 - val_loss: 28270.8749 - val_mse: 1694842368.0000 - val_mae: 28270.8750\n",
            "Epoch 56/500\n",
            "90/90 [==============================] - 0s 745us/step - loss: 23454.6654 - mse: 1296070656.0000 - mae: 23454.6660 - val_loss: 28088.9101 - val_mse: 1682250624.0000 - val_mae: 28088.9102\n",
            "Epoch 57/500\n",
            "90/90 [==============================] - 0s 653us/step - loss: 23368.0508 - mse: 1284948096.0000 - mae: 23368.0508 - val_loss: 28020.3510 - val_mse: 1666510976.0000 - val_mae: 28020.3496\n",
            "Epoch 58/500\n",
            "90/90 [==============================] - 0s 650us/step - loss: 23322.0699 - mse: 1289062656.0000 - mae: 23322.0664 - val_loss: 27749.3701 - val_mse: 1653104768.0000 - val_mae: 27749.3730\n",
            "Epoch 59/500\n",
            "90/90 [==============================] - 0s 724us/step - loss: 23347.9834 - mse: 1288757504.0000 - mae: 23347.9824 - val_loss: 27554.3294 - val_mse: 1639357184.0000 - val_mae: 27554.3281\n",
            "Epoch 60/500\n",
            "90/90 [==============================] - 0s 721us/step - loss: 22991.8101 - mse: 1255722496.0000 - mae: 22991.8105 - val_loss: 27486.0508 - val_mse: 1626521728.0000 - val_mae: 27486.0508\n",
            "Epoch 61/500\n",
            "90/90 [==============================] - 0s 758us/step - loss: 22342.0462 - mse: 1208326912.0000 - mae: 22342.0488 - val_loss: 27313.0645 - val_mse: 1610297984.0000 - val_mae: 27313.0645\n",
            "Epoch 62/500\n",
            "90/90 [==============================] - 0s 779us/step - loss: 22593.6622 - mse: 1227737344.0000 - mae: 22593.6641 - val_loss: 26985.4966 - val_mse: 1592785792.0000 - val_mae: 26985.4980\n",
            "Epoch 63/500\n",
            "90/90 [==============================] - 0s 781us/step - loss: 22252.8385 - mse: 1219793536.0000 - mae: 22252.8379 - val_loss: 26806.5571 - val_mse: 1577145088.0000 - val_mae: 26806.5547\n",
            "Epoch 64/500\n",
            "90/90 [==============================] - 0s 707us/step - loss: 22092.2171 - mse: 1216130560.0000 - mae: 22092.2168 - val_loss: 26574.7668 - val_mse: 1561123712.0000 - val_mae: 26574.7676\n",
            "Epoch 65/500\n",
            "90/90 [==============================] - 0s 732us/step - loss: 21974.1877 - mse: 1196602368.0000 - mae: 21974.1895 - val_loss: 26400.7169 - val_mse: 1545477376.0000 - val_mae: 26400.7168\n",
            "Epoch 66/500\n",
            "90/90 [==============================] - 0s 713us/step - loss: 21825.2478 - mse: 1181356544.0000 - mae: 21825.2480 - val_loss: 26240.8400 - val_mse: 1529270912.0000 - val_mae: 26240.8398\n",
            "Epoch 67/500\n",
            "90/90 [==============================] - 0s 915us/step - loss: 21610.3507 - mse: 1152468992.0000 - mae: 21610.3496 - val_loss: 26192.6450 - val_mse: 1513544320.0000 - val_mae: 26192.6445\n",
            "Epoch 68/500\n",
            "90/90 [==============================] - 0s 677us/step - loss: 21611.8064 - mse: 1180254976.0000 - mae: 21611.8047 - val_loss: 25835.2956 - val_mse: 1496937600.0000 - val_mae: 25835.2969\n",
            "Epoch 69/500\n",
            "90/90 [==============================] - 0s 590us/step - loss: 21363.6763 - mse: 1162882944.0000 - mae: 21363.6758 - val_loss: 25595.2988 - val_mse: 1482436224.0000 - val_mae: 25595.3008\n",
            "Epoch 70/500\n",
            "90/90 [==============================] - 0s 659us/step - loss: 21155.4218 - mse: 1147523840.0000 - mae: 21155.4219 - val_loss: 25846.0808 - val_mse: 1470860288.0000 - val_mae: 25846.0801\n",
            "Epoch 71/500\n",
            "90/90 [==============================] - 0s 662us/step - loss: 21152.9253 - mse: 1119925376.0000 - mae: 21152.9258 - val_loss: 25206.7794 - val_mse: 1452129792.0000 - val_mae: 25206.7773\n",
            "Epoch 72/500\n",
            "90/90 [==============================] - 0s 705us/step - loss: 20829.5486 - mse: 1100560768.0000 - mae: 20829.5469 - val_loss: 25323.2341 - val_mse: 1438488576.0000 - val_mae: 25323.2324\n",
            "Epoch 73/500\n",
            "90/90 [==============================] - 0s 659us/step - loss: 20411.5767 - mse: 1078096640.0000 - mae: 20411.5781 - val_loss: 25388.9499 - val_mse: 1423963904.0000 - val_mae: 25388.9492\n",
            "Epoch 74/500\n",
            "90/90 [==============================] - 0s 723us/step - loss: 20648.7888 - mse: 1081840384.0000 - mae: 20648.7871 - val_loss: 24595.6732 - val_mse: 1407624832.0000 - val_mae: 24595.6719\n",
            "Epoch 75/500\n",
            "90/90 [==============================] - 0s 709us/step - loss: 20215.4987 - mse: 1063265536.0000 - mae: 20215.4980 - val_loss: 24626.6757 - val_mse: 1394604032.0000 - val_mae: 24626.6777\n",
            "Epoch 76/500\n",
            "90/90 [==============================] - 0s 748us/step - loss: 20359.5254 - mse: 1092753920.0000 - mae: 20359.5254 - val_loss: 24281.0482 - val_mse: 1379241856.0000 - val_mae: 24281.0508\n",
            "Epoch 77/500\n",
            "90/90 [==============================] - 0s 970us/step - loss: 20096.8190 - mse: 1066318208.0000 - mae: 20096.8203 - val_loss: 24249.3014 - val_mse: 1367125120.0000 - val_mae: 24249.3008\n",
            "Epoch 78/500\n",
            "90/90 [==============================] - 0s 637us/step - loss: 20433.4559 - mse: 1080116992.0000 - mae: 20433.4551 - val_loss: 24074.3676 - val_mse: 1353112320.0000 - val_mae: 24074.3672\n",
            "Epoch 79/500\n",
            "90/90 [==============================] - 0s 650us/step - loss: 20033.2396 - mse: 1042258816.0000 - mae: 20033.2383 - val_loss: 23932.8531 - val_mse: 1340944512.0000 - val_mae: 23932.8535\n",
            "Epoch 80/500\n",
            "90/90 [==============================] - 0s 716us/step - loss: 19417.5022 - mse: 985829824.0000 - mae: 19417.5020 - val_loss: 23925.1049 - val_mse: 1326613760.0000 - val_mae: 23925.1055\n",
            "Epoch 81/500\n",
            "90/90 [==============================] - 0s 677us/step - loss: 19566.9754 - mse: 1002669376.0000 - mae: 19566.9766 - val_loss: 23814.6873 - val_mse: 1312914688.0000 - val_mae: 23814.6895\n",
            "Epoch 82/500\n",
            "90/90 [==============================] - 0s 814us/step - loss: 19326.0460 - mse: 978305920.0000 - mae: 19326.0449 - val_loss: 23348.0948 - val_mse: 1296509056.0000 - val_mae: 23348.0938\n",
            "Epoch 83/500\n",
            "90/90 [==============================] - 0s 676us/step - loss: 19170.9323 - mse: 971799936.0000 - mae: 19170.9336 - val_loss: 23269.4029 - val_mse: 1284643968.0000 - val_mae: 23269.4023\n",
            "Epoch 84/500\n",
            "90/90 [==============================] - 0s 712us/step - loss: 18926.5837 - mse: 984524032.0000 - mae: 18926.5840 - val_loss: 23210.5439 - val_mse: 1271360128.0000 - val_mae: 23210.5449\n",
            "Epoch 85/500\n",
            "90/90 [==============================] - 0s 693us/step - loss: 19000.2252 - mse: 956226688.0000 - mae: 19000.2246 - val_loss: 22900.0073 - val_mse: 1259470720.0000 - val_mae: 22900.0078\n",
            "Epoch 86/500\n",
            "90/90 [==============================] - 0s 670us/step - loss: 19171.4160 - mse: 961628416.0000 - mae: 19171.4160 - val_loss: 23503.9224 - val_mse: 1248966528.0000 - val_mae: 23503.9219\n",
            "Epoch 87/500\n",
            "90/90 [==============================] - 0s 665us/step - loss: 18787.0181 - mse: 944708096.0000 - mae: 18787.0176 - val_loss: 22754.1779 - val_mse: 1232167808.0000 - val_mae: 22754.1777\n",
            "Epoch 88/500\n",
            "90/90 [==============================] - 0s 688us/step - loss: 18904.7715 - mse: 951629440.0000 - mae: 18904.7715 - val_loss: 22511.3691 - val_mse: 1219311360.0000 - val_mae: 22511.3691\n",
            "Epoch 89/500\n",
            "90/90 [==============================] - 0s 653us/step - loss: 18561.6808 - mse: 897349248.0000 - mae: 18561.6816 - val_loss: 22465.5886 - val_mse: 1206435968.0000 - val_mae: 22465.5898\n",
            "Epoch 90/500\n",
            "90/90 [==============================] - 0s 707us/step - loss: 18613.8411 - mse: 926138432.0000 - mae: 18613.8418 - val_loss: 22236.2009 - val_mse: 1194389504.0000 - val_mae: 22236.2012\n",
            "Epoch 91/500\n",
            "90/90 [==============================] - 0s 728us/step - loss: 18823.5615 - mse: 928940288.0000 - mae: 18823.5605 - val_loss: 22133.2069 - val_mse: 1184057600.0000 - val_mae: 22133.2070\n",
            "Epoch 92/500\n",
            "90/90 [==============================] - 0s 689us/step - loss: 18687.2806 - mse: 933286528.0000 - mae: 18687.2812 - val_loss: 22360.1110 - val_mse: 1174059392.0000 - val_mae: 22360.1113\n",
            "Epoch 93/500\n",
            "90/90 [==============================] - 0s 671us/step - loss: 18525.9549 - mse: 922915840.0000 - mae: 18525.9551 - val_loss: 21832.4074 - val_mse: 1161432448.0000 - val_mae: 21832.4082\n",
            "Epoch 94/500\n",
            "90/90 [==============================] - 0s 715us/step - loss: 18412.7807 - mse: 923595904.0000 - mae: 18412.7812 - val_loss: 22224.3026 - val_mse: 1150395904.0000 - val_mae: 22224.3027\n",
            "Epoch 95/500\n",
            "90/90 [==============================] - 0s 740us/step - loss: 17274.7238 - mse: 824535104.0000 - mae: 17274.7227 - val_loss: 21437.3027 - val_mse: 1133157760.0000 - val_mae: 21437.3027\n",
            "Epoch 96/500\n",
            "90/90 [==============================] - 0s 871us/step - loss: 17942.4794 - mse: 880046656.0000 - mae: 17942.4785 - val_loss: 21308.7660 - val_mse: 1121526272.0000 - val_mae: 21308.7656\n",
            "Epoch 97/500\n",
            "90/90 [==============================] - 0s 763us/step - loss: 17826.1776 - mse: 841923328.0000 - mae: 17826.1777 - val_loss: 21288.7946 - val_mse: 1108659456.0000 - val_mae: 21288.7949\n",
            "Epoch 98/500\n",
            "90/90 [==============================] - 0s 669us/step - loss: 18074.9238 - mse: 875666112.0000 - mae: 18074.9238 - val_loss: 21267.4666 - val_mse: 1098530944.0000 - val_mae: 21267.4668\n",
            "Epoch 99/500\n",
            "90/90 [==============================] - 0s 718us/step - loss: 17951.4988 - mse: 851937600.0000 - mae: 17951.5000 - val_loss: 21067.1601 - val_mse: 1085775360.0000 - val_mae: 21067.1602\n",
            "Epoch 100/500\n",
            "90/90 [==============================] - 0s 732us/step - loss: 17728.0016 - mse: 855020160.0000 - mae: 17728.0020 - val_loss: 20746.5072 - val_mse: 1076134400.0000 - val_mae: 20746.5078\n",
            "Epoch 101/500\n",
            "90/90 [==============================] - 0s 719us/step - loss: 17684.7807 - mse: 844720832.0000 - mae: 17684.7812 - val_loss: 20628.3022 - val_mse: 1063991360.0000 - val_mae: 20628.3008\n",
            "Epoch 102/500\n",
            "90/90 [==============================] - 0s 793us/step - loss: 17807.7308 - mse: 890540928.0000 - mae: 17807.7285 - val_loss: 20522.6466 - val_mse: 1054956480.0000 - val_mae: 20522.6465\n",
            "Epoch 103/500\n",
            "90/90 [==============================] - 0s 764us/step - loss: 16756.5724 - mse: 794842880.0000 - mae: 16756.5723 - val_loss: 20379.7030 - val_mse: 1044083392.0000 - val_mae: 20379.7031\n",
            "Epoch 104/500\n",
            "90/90 [==============================] - 0s 736us/step - loss: 17012.2096 - mse: 776472960.0000 - mae: 17012.2109 - val_loss: 20179.3586 - val_mse: 1031832832.0000 - val_mae: 20179.3594\n",
            "Epoch 105/500\n",
            "90/90 [==============================] - 0s 766us/step - loss: 16379.6860 - mse: 731193728.0000 - mae: 16379.6865 - val_loss: 20042.3640 - val_mse: 1021119168.0000 - val_mae: 20042.3633\n",
            "Epoch 106/500\n",
            "90/90 [==============================] - 0s 712us/step - loss: 17174.9694 - mse: 802469184.0000 - mae: 17174.9688 - val_loss: 19993.5924 - val_mse: 1009249664.0000 - val_mae: 19993.5918\n",
            "Epoch 107/500\n",
            "90/90 [==============================] - 0s 718us/step - loss: 16683.6017 - mse: 745662720.0000 - mae: 16683.5996 - val_loss: 19762.1205 - val_mse: 996560256.0000 - val_mae: 19762.1191\n",
            "Epoch 108/500\n",
            "90/90 [==============================] - 0s 689us/step - loss: 16291.2569 - mse: 721950208.0000 - mae: 16291.2568 - val_loss: 19833.9271 - val_mse: 983231168.0000 - val_mae: 19833.9277\n",
            "Epoch 109/500\n",
            "90/90 [==============================] - 0s 800us/step - loss: 16485.0696 - mse: 743758976.0000 - mae: 16485.0703 - val_loss: 19396.7449 - val_mse: 970062848.0000 - val_mae: 19396.7441\n",
            "Epoch 110/500\n",
            "90/90 [==============================] - 0s 676us/step - loss: 15443.0670 - mse: 659366912.0000 - mae: 15443.0664 - val_loss: 19271.0751 - val_mse: 954899456.0000 - val_mae: 19271.0742\n",
            "Epoch 111/500\n",
            "90/90 [==============================] - 0s 618us/step - loss: 16703.4397 - mse: 758438272.0000 - mae: 16703.4395 - val_loss: 19329.9884 - val_mse: 946657024.0000 - val_mae: 19329.9883\n",
            "Epoch 112/500\n",
            "90/90 [==============================] - 0s 787us/step - loss: 16512.6677 - mse: 709917376.0000 - mae: 16512.6680 - val_loss: 19057.8354 - val_mse: 936709696.0000 - val_mae: 19057.8359\n",
            "Epoch 113/500\n",
            "90/90 [==============================] - 0s 759us/step - loss: 14799.1061 - mse: 614727168.0000 - mae: 14799.1055 - val_loss: 18904.8449 - val_mse: 927087296.0000 - val_mae: 18904.8438\n",
            "Epoch 114/500\n",
            "90/90 [==============================] - 0s 688us/step - loss: 15571.3262 - mse: 675408128.0000 - mae: 15571.3252 - val_loss: 18717.1207 - val_mse: 913530880.0000 - val_mae: 18717.1211\n",
            "Epoch 115/500\n",
            "90/90 [==============================] - 0s 645us/step - loss: 15562.2304 - mse: 677092224.0000 - mae: 15562.2305 - val_loss: 18681.9728 - val_mse: 903534272.0000 - val_mae: 18681.9727\n",
            "Epoch 116/500\n",
            "90/90 [==============================] - 0s 690us/step - loss: 15899.4344 - mse: 693539968.0000 - mae: 15899.4346 - val_loss: 18459.6551 - val_mse: 892401920.0000 - val_mae: 18459.6562\n",
            "Epoch 117/500\n",
            "90/90 [==============================] - 0s 743us/step - loss: 15248.7031 - mse: 643009984.0000 - mae: 15248.7031 - val_loss: 18379.1310 - val_mse: 884914432.0000 - val_mae: 18379.1328\n",
            "Epoch 118/500\n",
            "90/90 [==============================] - 0s 721us/step - loss: 15176.8623 - mse: 635801344.0000 - mae: 15176.8613 - val_loss: 18295.3419 - val_mse: 873838848.0000 - val_mae: 18295.3438\n",
            "Epoch 119/500\n",
            "90/90 [==============================] - 0s 764us/step - loss: 15561.5888 - mse: 671456320.0000 - mae: 15561.5898 - val_loss: 18093.7840 - val_mse: 860548864.0000 - val_mae: 18093.7832\n",
            "Epoch 120/500\n",
            "90/90 [==============================] - 0s 738us/step - loss: 15242.7608 - mse: 673161280.0000 - mae: 15242.7607 - val_loss: 18217.6607 - val_mse: 852362560.0000 - val_mae: 18217.6602\n",
            "Epoch 121/500\n",
            "90/90 [==============================] - 0s 719us/step - loss: 14958.2438 - mse: 633478400.0000 - mae: 14958.2432 - val_loss: 17896.7918 - val_mse: 838752512.0000 - val_mae: 17896.7910\n",
            "Epoch 122/500\n",
            "90/90 [==============================] - 0s 735us/step - loss: 15415.3317 - mse: 669248832.0000 - mae: 15415.3320 - val_loss: 17675.1432 - val_mse: 824652032.0000 - val_mae: 17675.1426\n",
            "Epoch 123/500\n",
            "90/90 [==============================] - 0s 743us/step - loss: 14563.5803 - mse: 598932288.0000 - mae: 14563.5791 - val_loss: 17545.6840 - val_mse: 811549760.0000 - val_mae: 17545.6836\n",
            "Epoch 124/500\n",
            "90/90 [==============================] - 0s 691us/step - loss: 14888.4037 - mse: 594675392.0000 - mae: 14888.4043 - val_loss: 17634.8723 - val_mse: 803158720.0000 - val_mae: 17634.8730\n",
            "Epoch 125/500\n",
            "90/90 [==============================] - 0s 700us/step - loss: 14757.8266 - mse: 605249728.0000 - mae: 14757.8262 - val_loss: 17318.6396 - val_mse: 790939328.0000 - val_mae: 17318.6387\n",
            "Epoch 126/500\n",
            "90/90 [==============================] - 0s 745us/step - loss: 14835.2675 - mse: 619337792.0000 - mae: 14835.2666 - val_loss: 17325.4167 - val_mse: 782641408.0000 - val_mae: 17325.4160\n",
            "Epoch 127/500\n",
            "90/90 [==============================] - 0s 663us/step - loss: 14096.5994 - mse: 548143296.0000 - mae: 14096.5977 - val_loss: 17195.9263 - val_mse: 769997376.0000 - val_mae: 17195.9258\n",
            "Epoch 128/500\n",
            "90/90 [==============================] - 0s 675us/step - loss: 14223.0417 - mse: 589157056.0000 - mae: 14223.0420 - val_loss: 17128.2517 - val_mse: 760733568.0000 - val_mae: 17128.2520\n",
            "Epoch 129/500\n",
            "90/90 [==============================] - 0s 698us/step - loss: 13645.8685 - mse: 572908352.0000 - mae: 13645.8691 - val_loss: 16841.6436 - val_mse: 748053312.0000 - val_mae: 16841.6445\n",
            "Epoch 130/500\n",
            "90/90 [==============================] - 0s 765us/step - loss: 14425.6268 - mse: 567259968.0000 - mae: 14425.6260 - val_loss: 16798.2935 - val_mse: 739698176.0000 - val_mae: 16798.2949\n",
            "Epoch 131/500\n",
            "90/90 [==============================] - 0s 803us/step - loss: 14469.5101 - mse: 561323648.0000 - mae: 14469.5098 - val_loss: 16701.4309 - val_mse: 731825344.0000 - val_mae: 16701.4297\n",
            "Epoch 132/500\n",
            "90/90 [==============================] - 0s 649us/step - loss: 14370.6572 - mse: 572032960.0000 - mae: 14370.6572 - val_loss: 16711.0950 - val_mse: 723668672.0000 - val_mae: 16711.0938\n",
            "Epoch 133/500\n",
            "90/90 [==============================] - 0s 699us/step - loss: 14147.3920 - mse: 545982144.0000 - mae: 14147.3926 - val_loss: 16700.5299 - val_mse: 716437376.0000 - val_mae: 16700.5312\n",
            "Epoch 134/500\n",
            "90/90 [==============================] - 0s 622us/step - loss: 14656.9908 - mse: 583387712.0000 - mae: 14656.9902 - val_loss: 16577.1736 - val_mse: 707825664.0000 - val_mae: 16577.1738\n",
            "Epoch 135/500\n",
            "90/90 [==============================] - 0s 660us/step - loss: 13676.7854 - mse: 523579840.0000 - mae: 13676.7852 - val_loss: 16312.8663 - val_mse: 695281280.0000 - val_mae: 16312.8662\n",
            "Epoch 136/500\n",
            "90/90 [==============================] - 0s 704us/step - loss: 13605.5704 - mse: 556234176.0000 - mae: 13605.5693 - val_loss: 16284.2312 - val_mse: 688901888.0000 - val_mae: 16284.2305\n",
            "Epoch 137/500\n",
            "90/90 [==============================] - 0s 719us/step - loss: 13179.7276 - mse: 458255424.0000 - mae: 13179.7275 - val_loss: 16057.6329 - val_mse: 678115072.0000 - val_mae: 16057.6338\n",
            "Epoch 138/500\n",
            "90/90 [==============================] - 0s 632us/step - loss: 13601.0563 - mse: 538231488.0000 - mae: 13601.0566 - val_loss: 16072.0451 - val_mse: 670899584.0000 - val_mae: 16072.0449\n",
            "Epoch 139/500\n",
            "90/90 [==============================] - 0s 623us/step - loss: 13932.0845 - mse: 541477760.0000 - mae: 13932.0830 - val_loss: 15987.2841 - val_mse: 662614592.0000 - val_mae: 15987.2832\n",
            "Epoch 140/500\n",
            "90/90 [==============================] - 0s 604us/step - loss: 13434.8102 - mse: 536551296.0000 - mae: 13434.8115 - val_loss: 16111.1492 - val_mse: 654742656.0000 - val_mae: 16111.1504\n",
            "Epoch 141/500\n",
            "90/90 [==============================] - 0s 608us/step - loss: 12924.7127 - mse: 470171296.0000 - mae: 12924.7129 - val_loss: 15802.7284 - val_mse: 641954240.0000 - val_mae: 15802.7275\n",
            "Epoch 142/500\n",
            "90/90 [==============================] - 0s 646us/step - loss: 13545.1455 - mse: 500051840.0000 - mae: 13545.1455 - val_loss: 15636.4039 - val_mse: 632588544.0000 - val_mae: 15636.4023\n",
            "Epoch 143/500\n",
            "90/90 [==============================] - 0s 669us/step - loss: 13149.7422 - mse: 477960064.0000 - mae: 13149.7412 - val_loss: 15414.8427 - val_mse: 621437376.0000 - val_mae: 15414.8418\n",
            "Epoch 144/500\n",
            "90/90 [==============================] - 0s 673us/step - loss: 13289.5030 - mse: 491830208.0000 - mae: 13289.5039 - val_loss: 15376.8170 - val_mse: 612831168.0000 - val_mae: 15376.8164\n",
            "Epoch 145/500\n",
            "90/90 [==============================] - 0s 592us/step - loss: 12943.0707 - mse: 481178112.0000 - mae: 12943.0713 - val_loss: 15160.3280 - val_mse: 602434752.0000 - val_mae: 15160.3281\n",
            "Epoch 146/500\n",
            "90/90 [==============================] - 0s 612us/step - loss: 13038.8750 - mse: 471495488.0000 - mae: 13038.8750 - val_loss: 15305.3022 - val_mse: 596840576.0000 - val_mae: 15305.3027\n",
            "Epoch 147/500\n",
            "90/90 [==============================] - 0s 620us/step - loss: 12198.8777 - mse: 423385664.0000 - mae: 12198.8779 - val_loss: 15073.9521 - val_mse: 586258944.0000 - val_mae: 15073.9531\n",
            "Epoch 148/500\n",
            "90/90 [==============================] - 0s 666us/step - loss: 13568.6305 - mse: 523299040.0000 - mae: 13568.6309 - val_loss: 14942.0790 - val_mse: 577879040.0000 - val_mae: 14942.0791\n",
            "Epoch 149/500\n",
            "90/90 [==============================] - 0s 636us/step - loss: 12410.3685 - mse: 423383488.0000 - mae: 12410.3691 - val_loss: 14942.4396 - val_mse: 570350976.0000 - val_mae: 14942.4385\n",
            "Epoch 150/500\n",
            "90/90 [==============================] - 0s 612us/step - loss: 11714.0233 - mse: 385264032.0000 - mae: 11714.0234 - val_loss: 14739.6091 - val_mse: 558164992.0000 - val_mae: 14739.6084\n",
            "Epoch 151/500\n",
            "90/90 [==============================] - 0s 681us/step - loss: 11917.6837 - mse: 409839072.0000 - mae: 11917.6836 - val_loss: 14536.8245 - val_mse: 548453824.0000 - val_mae: 14536.8252\n",
            "Epoch 152/500\n",
            "90/90 [==============================] - 0s 629us/step - loss: 11113.9040 - mse: 384139584.0000 - mae: 11113.9043 - val_loss: 14453.5960 - val_mse: 539509760.0000 - val_mae: 14453.5957\n",
            "Epoch 153/500\n",
            "90/90 [==============================] - 0s 731us/step - loss: 12081.7076 - mse: 434923008.0000 - mae: 12081.7080 - val_loss: 14444.5893 - val_mse: 534961728.0000 - val_mae: 14444.5889\n",
            "Epoch 154/500\n",
            "90/90 [==============================] - 0s 714us/step - loss: 12255.3072 - mse: 457159392.0000 - mae: 12255.3086 - val_loss: 14274.7309 - val_mse: 525735648.0000 - val_mae: 14274.7305\n",
            "Epoch 155/500\n",
            "90/90 [==============================] - 0s 688us/step - loss: 11028.7862 - mse: 359414624.0000 - mae: 11028.7852 - val_loss: 14061.3992 - val_mse: 514903968.0000 - val_mae: 14061.4004\n",
            "Epoch 156/500\n",
            "90/90 [==============================] - 0s 666us/step - loss: 12277.4939 - mse: 430589504.0000 - mae: 12277.4941 - val_loss: 13896.2880 - val_mse: 504555936.0000 - val_mae: 13896.2891\n",
            "Epoch 157/500\n",
            "90/90 [==============================] - 0s 675us/step - loss: 11599.7391 - mse: 374562208.0000 - mae: 11599.7393 - val_loss: 13765.4849 - val_mse: 495462944.0000 - val_mae: 13765.4844\n",
            "Epoch 158/500\n",
            "90/90 [==============================] - 0s 623us/step - loss: 11575.6892 - mse: 393256448.0000 - mae: 11575.6904 - val_loss: 13612.9372 - val_mse: 487939648.0000 - val_mae: 13612.9375\n",
            "Epoch 159/500\n",
            "90/90 [==============================] - 0s 641us/step - loss: 11000.6829 - mse: 360774432.0000 - mae: 11000.6816 - val_loss: 13599.7258 - val_mse: 480821792.0000 - val_mae: 13599.7246\n",
            "Epoch 160/500\n",
            "90/90 [==============================] - 0s 786us/step - loss: 11622.8205 - mse: 365813856.0000 - mae: 11622.8213 - val_loss: 13662.4374 - val_mse: 474408000.0000 - val_mae: 13662.4375\n",
            "Epoch 161/500\n",
            "90/90 [==============================] - 0s 809us/step - loss: 10219.7144 - mse: 330734464.0000 - mae: 10219.7139 - val_loss: 13487.9089 - val_mse: 466195808.0000 - val_mae: 13487.9082\n",
            "Epoch 162/500\n",
            "90/90 [==============================] - 0s 738us/step - loss: 10807.4031 - mse: 333999552.0000 - mae: 10807.4023 - val_loss: 13741.9777 - val_mse: 463549632.0000 - val_mae: 13741.9775\n",
            "Epoch 163/500\n",
            "90/90 [==============================] - 0s 695us/step - loss: 11302.0278 - mse: 384940704.0000 - mae: 11302.0273 - val_loss: 13217.0442 - val_mse: 450967552.0000 - val_mae: 13217.0449\n",
            "Epoch 164/500\n",
            "90/90 [==============================] - 0s 662us/step - loss: 11404.3725 - mse: 382491680.0000 - mae: 11404.3711 - val_loss: 13230.9507 - val_mse: 445113792.0000 - val_mae: 13230.9512\n",
            "Epoch 165/500\n",
            "90/90 [==============================] - 0s 868us/step - loss: 10008.3382 - mse: 297278336.0000 - mae: 10008.3379 - val_loss: 13215.6698 - val_mse: 438388224.0000 - val_mae: 13215.6699\n",
            "Epoch 166/500\n",
            "90/90 [==============================] - 0s 745us/step - loss: 11353.6374 - mse: 344253504.0000 - mae: 11353.6377 - val_loss: 12975.3873 - val_mse: 429841920.0000 - val_mae: 12975.3877\n",
            "Epoch 167/500\n",
            "90/90 [==============================] - 0s 713us/step - loss: 11819.2505 - mse: 410005472.0000 - mae: 11819.2500 - val_loss: 12839.3491 - val_mse: 423111968.0000 - val_mae: 12839.3486\n",
            "Epoch 168/500\n",
            "90/90 [==============================] - 0s 645us/step - loss: 10488.8596 - mse: 329745792.0000 - mae: 10488.8594 - val_loss: 12761.5032 - val_mse: 415692832.0000 - val_mae: 12761.5029\n",
            "Epoch 169/500\n",
            "90/90 [==============================] - 0s 686us/step - loss: 10377.4484 - mse: 327696832.0000 - mae: 10377.4482 - val_loss: 12633.4627 - val_mse: 408832576.0000 - val_mae: 12633.4629\n",
            "Epoch 170/500\n",
            "90/90 [==============================] - 0s 636us/step - loss: 9758.7201 - mse: 290552640.0000 - mae: 9758.7197 - val_loss: 12908.9032 - val_mse: 406434528.0000 - val_mae: 12908.9023\n",
            "Epoch 171/500\n",
            "90/90 [==============================] - 0s 734us/step - loss: 9817.1197 - mse: 305378752.0000 - mae: 9817.1191 - val_loss: 12538.6841 - val_mse: 395959040.0000 - val_mae: 12538.6836\n",
            "Epoch 172/500\n",
            "90/90 [==============================] - 0s 669us/step - loss: 9740.8107 - mse: 307211360.0000 - mae: 9740.8115 - val_loss: 12369.2292 - val_mse: 386856096.0000 - val_mae: 12369.2295\n",
            "Epoch 173/500\n",
            "90/90 [==============================] - 0s 645us/step - loss: 10549.6060 - mse: 316037728.0000 - mae: 10549.6064 - val_loss: 12247.6459 - val_mse: 381520704.0000 - val_mae: 12247.6455\n",
            "Epoch 174/500\n",
            "90/90 [==============================] - 0s 720us/step - loss: 10213.0489 - mse: 302351264.0000 - mae: 10213.0498 - val_loss: 12158.2276 - val_mse: 375453984.0000 - val_mae: 12158.2275\n",
            "Epoch 175/500\n",
            "90/90 [==============================] - 0s 709us/step - loss: 10677.4002 - mse: 345728416.0000 - mae: 10677.3994 - val_loss: 12244.0483 - val_mse: 372543168.0000 - val_mae: 12244.0488\n",
            "Epoch 176/500\n",
            "90/90 [==============================] - 0s 659us/step - loss: 8735.3208 - mse: 243417584.0000 - mae: 8735.3213 - val_loss: 12051.7732 - val_mse: 364256640.0000 - val_mae: 12051.7725\n",
            "Epoch 177/500\n",
            "90/90 [==============================] - 0s 778us/step - loss: 9976.5012 - mse: 288652640.0000 - mae: 9976.5010 - val_loss: 11842.6145 - val_mse: 356234240.0000 - val_mae: 11842.6152\n",
            "Epoch 178/500\n",
            "90/90 [==============================] - 0s 656us/step - loss: 9610.1082 - mse: 244018032.0000 - mae: 9610.1084 - val_loss: 11734.8822 - val_mse: 350371872.0000 - val_mae: 11734.8838\n",
            "Epoch 179/500\n",
            "90/90 [==============================] - 0s 721us/step - loss: 9201.1514 - mse: 251231344.0000 - mae: 9201.1514 - val_loss: 11763.8038 - val_mse: 346826752.0000 - val_mae: 11763.8037\n",
            "Epoch 180/500\n",
            "90/90 [==============================] - 0s 821us/step - loss: 9551.1009 - mse: 278329920.0000 - mae: 9551.1006 - val_loss: 11597.4115 - val_mse: 338554400.0000 - val_mae: 11597.4111\n",
            "Epoch 181/500\n",
            "90/90 [==============================] - 0s 677us/step - loss: 10156.9033 - mse: 305554560.0000 - mae: 10156.9043 - val_loss: 11580.7087 - val_mse: 336040352.0000 - val_mae: 11580.7090\n",
            "Epoch 182/500\n",
            "90/90 [==============================] - 0s 671us/step - loss: 8330.8263 - mse: 221954960.0000 - mae: 8330.8262 - val_loss: 11357.2867 - val_mse: 325839296.0000 - val_mae: 11357.2871\n",
            "Epoch 183/500\n",
            "90/90 [==============================] - 0s 642us/step - loss: 9156.8431 - mse: 264123280.0000 - mae: 9156.8438 - val_loss: 11267.8920 - val_mse: 321390784.0000 - val_mae: 11267.8926\n",
            "Epoch 184/500\n",
            "90/90 [==============================] - 0s 652us/step - loss: 9213.2351 - mse: 240405872.0000 - mae: 9213.2344 - val_loss: 11655.6528 - val_mse: 323729984.0000 - val_mae: 11655.6523\n",
            "Epoch 185/500\n",
            "90/90 [==============================] - 0s 649us/step - loss: 7541.5958 - mse: 169020544.0000 - mae: 7541.5952 - val_loss: 11616.1960 - val_mse: 320658560.0000 - val_mae: 11616.1963\n",
            "Epoch 186/500\n",
            "90/90 [==============================] - 0s 795us/step - loss: 9298.0933 - mse: 258921616.0000 - mae: 9298.0928 - val_loss: 11588.4727 - val_mse: 315445696.0000 - val_mae: 11588.4727\n",
            "Epoch 187/500\n",
            "90/90 [==============================] - 0s 720us/step - loss: 10058.1893 - mse: 331462912.0000 - mae: 10058.1895 - val_loss: 11257.5725 - val_mse: 307086464.0000 - val_mae: 11257.5723\n",
            "Epoch 188/500\n",
            "90/90 [==============================] - 0s 694us/step - loss: 9068.6516 - mse: 256584336.0000 - mae: 9068.6514 - val_loss: 11191.7782 - val_mse: 301549472.0000 - val_mae: 11191.7783\n",
            "Epoch 189/500\n",
            "90/90 [==============================] - 0s 627us/step - loss: 9779.9935 - mse: 331983840.0000 - mae: 9779.9941 - val_loss: 11181.8375 - val_mse: 300923008.0000 - val_mae: 11181.8379\n",
            "Epoch 190/500\n",
            "90/90 [==============================] - 0s 717us/step - loss: 8829.5168 - mse: 234811936.0000 - mae: 8829.5166 - val_loss: 11139.7457 - val_mse: 298957024.0000 - val_mae: 11139.7451\n",
            "Epoch 191/500\n",
            "90/90 [==============================] - 0s 729us/step - loss: 9530.4563 - mse: 273240608.0000 - mae: 9530.4570 - val_loss: 11124.3585 - val_mse: 294682624.0000 - val_mae: 11124.3584\n",
            "Epoch 192/500\n",
            "90/90 [==============================] - 0s 791us/step - loss: 9578.9428 - mse: 295029792.0000 - mae: 9578.9424 - val_loss: 11041.6412 - val_mse: 291168128.0000 - val_mae: 11041.6416\n",
            "Epoch 193/500\n",
            "90/90 [==============================] - 0s 667us/step - loss: 8604.1481 - mse: 240639424.0000 - mae: 8604.1475 - val_loss: 11034.5291 - val_mse: 288936576.0000 - val_mae: 11034.5293\n",
            "Epoch 194/500\n",
            "90/90 [==============================] - 0s 691us/step - loss: 8580.0537 - mse: 199268624.0000 - mae: 8580.0537 - val_loss: 10922.6158 - val_mse: 284498272.0000 - val_mae: 10922.6162\n",
            "Epoch 195/500\n",
            "90/90 [==============================] - 0s 658us/step - loss: 7955.2894 - mse: 200600096.0000 - mae: 7955.2891 - val_loss: 10841.7802 - val_mse: 280840800.0000 - val_mae: 10841.7803\n",
            "Epoch 196/500\n",
            "90/90 [==============================] - 0s 728us/step - loss: 8518.5911 - mse: 213495072.0000 - mae: 8518.5918 - val_loss: 10739.3469 - val_mse: 275819072.0000 - val_mae: 10739.3477\n",
            "Epoch 197/500\n",
            "90/90 [==============================] - 0s 632us/step - loss: 8841.9600 - mse: 246964240.0000 - mae: 8841.9600 - val_loss: 10842.2756 - val_mse: 277767392.0000 - val_mae: 10842.2754\n",
            "Epoch 198/500\n",
            "90/90 [==============================] - 0s 679us/step - loss: 7620.9390 - mse: 169474352.0000 - mae: 7620.9390 - val_loss: 10728.0675 - val_mse: 271749024.0000 - val_mae: 10728.0684\n",
            "Epoch 199/500\n",
            "90/90 [==============================] - 0s 828us/step - loss: 8158.1705 - mse: 195348160.0000 - mae: 8158.1709 - val_loss: 10632.4095 - val_mse: 269883712.0000 - val_mae: 10632.4102\n",
            "Epoch 200/500\n",
            "90/90 [==============================] - 0s 728us/step - loss: 8539.1725 - mse: 208547792.0000 - mae: 8539.1719 - val_loss: 10583.6320 - val_mse: 265282704.0000 - val_mae: 10583.6318\n",
            "Epoch 201/500\n",
            "90/90 [==============================] - 0s 623us/step - loss: 7432.3666 - mse: 185565760.0000 - mae: 7432.3672 - val_loss: 10770.6318 - val_mse: 264222672.0000 - val_mae: 10770.6318\n",
            "Epoch 202/500\n",
            "90/90 [==============================] - 0s 697us/step - loss: 7356.4179 - mse: 153033520.0000 - mae: 7356.4175 - val_loss: 10390.2141 - val_mse: 258304864.0000 - val_mae: 10390.2139\n",
            "Epoch 203/500\n",
            "90/90 [==============================] - 0s 711us/step - loss: 7783.8680 - mse: 180535760.0000 - mae: 7783.8682 - val_loss: 10426.6330 - val_mse: 252708576.0000 - val_mae: 10426.6338\n",
            "Epoch 204/500\n",
            "90/90 [==============================] - 0s 676us/step - loss: 8674.0162 - mse: 239306896.0000 - mae: 8674.0156 - val_loss: 10147.5393 - val_mse: 245210320.0000 - val_mae: 10147.5391\n",
            "Epoch 205/500\n",
            "90/90 [==============================] - 0s 665us/step - loss: 8041.2339 - mse: 199729600.0000 - mae: 8041.2349 - val_loss: 9955.1588 - val_mse: 240863552.0000 - val_mae: 9955.1592\n",
            "Epoch 206/500\n",
            "90/90 [==============================] - 0s 678us/step - loss: 8652.1223 - mse: 212656864.0000 - mae: 8652.1230 - val_loss: 9971.5766 - val_mse: 238374688.0000 - val_mae: 9971.5762\n",
            "Epoch 207/500\n",
            "90/90 [==============================] - 0s 703us/step - loss: 8121.0051 - mse: 218890992.0000 - mae: 8121.0049 - val_loss: 9883.7518 - val_mse: 237752032.0000 - val_mae: 9883.7510\n",
            "Epoch 208/500\n",
            "90/90 [==============================] - 0s 626us/step - loss: 8486.2765 - mse: 205937296.0000 - mae: 8486.2764 - val_loss: 10170.8688 - val_mse: 236544768.0000 - val_mae: 10170.8691\n",
            "Epoch 209/500\n",
            "90/90 [==============================] - 0s 615us/step - loss: 7939.8193 - mse: 193901728.0000 - mae: 7939.8188 - val_loss: 9940.2109 - val_mse: 237626528.0000 - val_mae: 9940.2109\n",
            "Epoch 210/500\n",
            "90/90 [==============================] - 0s 645us/step - loss: 8190.9442 - mse: 239134032.0000 - mae: 8190.9443 - val_loss: 10169.1877 - val_mse: 233855568.0000 - val_mae: 10169.1875\n",
            "Epoch 211/500\n",
            "90/90 [==============================] - 0s 668us/step - loss: 8085.2328 - mse: 183752064.0000 - mae: 8085.2334 - val_loss: 10002.0625 - val_mse: 237802224.0000 - val_mae: 10002.0625\n",
            "Epoch 212/500\n",
            "90/90 [==============================] - 0s 684us/step - loss: 6323.3908 - mse: 131978960.0000 - mae: 6323.3911 - val_loss: 9824.1792 - val_mse: 232055520.0000 - val_mae: 9824.1787\n",
            "Epoch 213/500\n",
            "90/90 [==============================] - 0s 647us/step - loss: 8671.1934 - mse: 242299648.0000 - mae: 8671.1943 - val_loss: 9831.6216 - val_mse: 233064032.0000 - val_mae: 9831.6211\n",
            "Epoch 214/500\n",
            "90/90 [==============================] - 0s 647us/step - loss: 8613.8110 - mse: 224484496.0000 - mae: 8613.8115 - val_loss: 9717.3137 - val_mse: 222546832.0000 - val_mae: 9717.3135\n",
            "Epoch 215/500\n",
            "90/90 [==============================] - 0s 741us/step - loss: 8500.1760 - mse: 222073760.0000 - mae: 8500.1758 - val_loss: 9670.8944 - val_mse: 224193312.0000 - val_mae: 9670.8945\n",
            "Epoch 216/500\n",
            "90/90 [==============================] - 0s 720us/step - loss: 7276.8439 - mse: 153632976.0000 - mae: 7276.8442 - val_loss: 9470.6072 - val_mse: 219866928.0000 - val_mae: 9470.6074\n",
            "Epoch 217/500\n",
            "90/90 [==============================] - 0s 688us/step - loss: 7546.9931 - mse: 177826784.0000 - mae: 7546.9922 - val_loss: 9310.9543 - val_mse: 212094272.0000 - val_mae: 9310.9541\n",
            "Epoch 218/500\n",
            "90/90 [==============================] - 0s 668us/step - loss: 8636.0319 - mse: 198854512.0000 - mae: 8636.0322 - val_loss: 9233.6471 - val_mse: 212737712.0000 - val_mae: 9233.6475\n",
            "Epoch 219/500\n",
            "90/90 [==============================] - 0s 628us/step - loss: 6945.8157 - mse: 147884640.0000 - mae: 6945.8154 - val_loss: 9257.5645 - val_mse: 212339376.0000 - val_mae: 9257.5645\n",
            "Epoch 220/500\n",
            "90/90 [==============================] - 0s 615us/step - loss: 6949.1513 - mse: 154198464.0000 - mae: 6949.1509 - val_loss: 9253.0879 - val_mse: 210848224.0000 - val_mae: 9253.0879\n",
            "Epoch 221/500\n",
            "90/90 [==============================] - 0s 631us/step - loss: 7993.7821 - mse: 196010864.0000 - mae: 7993.7827 - val_loss: 9163.7416 - val_mse: 206613824.0000 - val_mae: 9163.7412\n",
            "Epoch 222/500\n",
            "90/90 [==============================] - 0s 765us/step - loss: 6815.9970 - mse: 150821600.0000 - mae: 6815.9971 - val_loss: 9138.7088 - val_mse: 207100864.0000 - val_mae: 9138.7090\n",
            "Epoch 223/500\n",
            "90/90 [==============================] - 0s 645us/step - loss: 8206.8194 - mse: 198594144.0000 - mae: 8206.8184 - val_loss: 9174.0597 - val_mse: 206573136.0000 - val_mae: 9174.0596\n",
            "Epoch 224/500\n",
            "90/90 [==============================] - 0s 670us/step - loss: 7175.7520 - mse: 155390720.0000 - mae: 7175.7520 - val_loss: 9179.4850 - val_mse: 209601312.0000 - val_mae: 9179.4854\n",
            "Epoch 225/500\n",
            "90/90 [==============================] - 0s 669us/step - loss: 8657.7282 - mse: 229735216.0000 - mae: 8657.7275 - val_loss: 9206.2790 - val_mse: 208648464.0000 - val_mae: 9206.2793\n",
            "Epoch 226/500\n",
            "90/90 [==============================] - 0s 726us/step - loss: 8397.2466 - mse: 199175984.0000 - mae: 8397.2461 - val_loss: 9195.3258 - val_mse: 206083728.0000 - val_mae: 9195.3252\n",
            "Epoch 227/500\n",
            "90/90 [==============================] - 0s 695us/step - loss: 8225.7105 - mse: 187799568.0000 - mae: 8225.7100 - val_loss: 9154.4247 - val_mse: 199627664.0000 - val_mae: 9154.4238\n",
            "Epoch 228/500\n",
            "90/90 [==============================] - 0s 884us/step - loss: 8279.4451 - mse: 193371520.0000 - mae: 8279.4463 - val_loss: 9139.1547 - val_mse: 202968912.0000 - val_mae: 9139.1553\n",
            "Epoch 229/500\n",
            "90/90 [==============================] - 0s 675us/step - loss: 7147.9143 - mse: 183809728.0000 - mae: 7147.9146 - val_loss: 9079.2127 - val_mse: 204677216.0000 - val_mae: 9079.2129\n",
            "Epoch 230/500\n",
            "90/90 [==============================] - 0s 616us/step - loss: 6973.3065 - mse: 143525664.0000 - mae: 6973.3062 - val_loss: 8971.9879 - val_mse: 200069344.0000 - val_mae: 8971.9883\n",
            "Epoch 231/500\n",
            "90/90 [==============================] - 0s 724us/step - loss: 7241.0506 - mse: 148843744.0000 - mae: 7241.0508 - val_loss: 9101.1284 - val_mse: 203718112.0000 - val_mae: 9101.1289\n",
            "Epoch 232/500\n",
            "90/90 [==============================] - 0s 672us/step - loss: 8018.4133 - mse: 214003776.0000 - mae: 8018.4141 - val_loss: 9141.3637 - val_mse: 201603808.0000 - val_mae: 9141.3643\n",
            "Epoch 233/500\n",
            "90/90 [==============================] - 0s 638us/step - loss: 7485.1862 - mse: 192347728.0000 - mae: 7485.1870 - val_loss: 8939.8184 - val_mse: 198068640.0000 - val_mae: 8939.8184\n",
            "Epoch 234/500\n",
            "90/90 [==============================] - 0s 648us/step - loss: 7371.2904 - mse: 145571664.0000 - mae: 7371.2900 - val_loss: 8919.6110 - val_mse: 198804928.0000 - val_mae: 8919.6113\n",
            "Epoch 235/500\n",
            "90/90 [==============================] - 0s 646us/step - loss: 6705.5708 - mse: 134393008.0000 - mae: 6705.5708 - val_loss: 9093.0604 - val_mse: 201296576.0000 - val_mae: 9093.0605\n",
            "Epoch 236/500\n",
            "90/90 [==============================] - 0s 692us/step - loss: 7224.4211 - mse: 163128720.0000 - mae: 7224.4214 - val_loss: 8970.5057 - val_mse: 197929376.0000 - val_mae: 8970.5059\n",
            "Epoch 237/500\n",
            "90/90 [==============================] - 0s 668us/step - loss: 6430.5003 - mse: 117660400.0000 - mae: 6430.5005 - val_loss: 8973.4917 - val_mse: 196811360.0000 - val_mae: 8973.4912\n",
            "Epoch 238/500\n",
            "90/90 [==============================] - 0s 736us/step - loss: 7076.4920 - mse: 140035760.0000 - mae: 7076.4917 - val_loss: 8876.1634 - val_mse: 196210304.0000 - val_mae: 8876.1641\n",
            "Epoch 239/500\n",
            "90/90 [==============================] - 0s 648us/step - loss: 7897.3972 - mse: 164629296.0000 - mae: 7897.3975 - val_loss: 8849.7585 - val_mse: 192340736.0000 - val_mae: 8849.7588\n",
            "Epoch 240/500\n",
            "90/90 [==============================] - 0s 706us/step - loss: 7941.4206 - mse: 192276048.0000 - mae: 7941.4209 - val_loss: 8768.3339 - val_mse: 189654672.0000 - val_mae: 8768.3350\n",
            "Epoch 241/500\n",
            "90/90 [==============================] - 0s 611us/step - loss: 6685.7927 - mse: 127038440.0000 - mae: 6685.7925 - val_loss: 9167.2002 - val_mse: 188248592.0000 - val_mae: 9167.2002\n",
            "Epoch 242/500\n",
            "90/90 [==============================] - 0s 769us/step - loss: 7112.9494 - mse: 142879696.0000 - mae: 7112.9492 - val_loss: 8704.7725 - val_mse: 189927600.0000 - val_mae: 8704.7725\n",
            "Epoch 243/500\n",
            "90/90 [==============================] - 0s 662us/step - loss: 7223.1392 - mse: 135063984.0000 - mae: 7223.1396 - val_loss: 8788.2226 - val_mse: 194519728.0000 - val_mae: 8788.2227\n",
            "Epoch 244/500\n",
            "90/90 [==============================] - 0s 637us/step - loss: 8410.4160 - mse: 233653632.0000 - mae: 8410.4160 - val_loss: 8790.7603 - val_mse: 191987664.0000 - val_mae: 8790.7607\n",
            "Epoch 245/500\n",
            "90/90 [==============================] - 0s 622us/step - loss: 8349.4772 - mse: 223469392.0000 - mae: 8349.4775 - val_loss: 8734.9592 - val_mse: 187597840.0000 - val_mae: 8734.9590\n",
            "Epoch 246/500\n",
            "90/90 [==============================] - 0s 650us/step - loss: 6669.8934 - mse: 147755232.0000 - mae: 6669.8931 - val_loss: 8776.4024 - val_mse: 185495184.0000 - val_mae: 8776.4023\n",
            "Epoch 247/500\n",
            "90/90 [==============================] - 0s 700us/step - loss: 7295.7461 - mse: 176695056.0000 - mae: 7295.7466 - val_loss: 8668.4659 - val_mse: 186127024.0000 - val_mae: 8668.4658\n",
            "Epoch 248/500\n",
            "90/90 [==============================] - 0s 639us/step - loss: 7691.9557 - mse: 182258976.0000 - mae: 7691.9556 - val_loss: 8667.4828 - val_mse: 185753328.0000 - val_mae: 8667.4824\n",
            "Epoch 249/500\n",
            "90/90 [==============================] - 0s 610us/step - loss: 7637.8855 - mse: 161697264.0000 - mae: 7637.8853 - val_loss: 8563.9204 - val_mse: 184325344.0000 - val_mae: 8563.9209\n",
            "Epoch 250/500\n",
            "90/90 [==============================] - 0s 662us/step - loss: 8279.1525 - mse: 221307536.0000 - mae: 8279.1523 - val_loss: 8595.2491 - val_mse: 183130304.0000 - val_mae: 8595.2490\n",
            "Epoch 251/500\n",
            "90/90 [==============================] - 0s 654us/step - loss: 7125.5527 - mse: 167603568.0000 - mae: 7125.5522 - val_loss: 8522.1393 - val_mse: 180339312.0000 - val_mae: 8522.1396\n",
            "Epoch 252/500\n",
            "90/90 [==============================] - 0s 686us/step - loss: 6779.2470 - mse: 134477728.0000 - mae: 6779.2466 - val_loss: 8391.2808 - val_mse: 177000032.0000 - val_mae: 8391.2803\n",
            "Epoch 253/500\n",
            "90/90 [==============================] - 0s 694us/step - loss: 7247.5023 - mse: 148679424.0000 - mae: 7247.5020 - val_loss: 8612.3975 - val_mse: 174682688.0000 - val_mae: 8612.3975\n",
            "Epoch 254/500\n",
            "90/90 [==============================] - 0s 726us/step - loss: 7402.8169 - mse: 193280608.0000 - mae: 7402.8169 - val_loss: 8474.2645 - val_mse: 178078096.0000 - val_mae: 8474.2637\n",
            "Epoch 255/500\n",
            "90/90 [==============================] - 0s 826us/step - loss: 8379.8337 - mse: 211214496.0000 - mae: 8379.8330 - val_loss: 8464.3097 - val_mse: 176523760.0000 - val_mae: 8464.3096\n",
            "Epoch 256/500\n",
            "90/90 [==============================] - 0s 639us/step - loss: 8411.6795 - mse: 217014800.0000 - mae: 8411.6797 - val_loss: 8469.3400 - val_mse: 176364048.0000 - val_mae: 8469.3398\n",
            "Epoch 257/500\n",
            "90/90 [==============================] - 0s 660us/step - loss: 7127.3123 - mse: 151957328.0000 - mae: 7127.3125 - val_loss: 8371.7192 - val_mse: 174492832.0000 - val_mae: 8371.7197\n",
            "Epoch 258/500\n",
            "90/90 [==============================] - 0s 652us/step - loss: 7724.7401 - mse: 174098336.0000 - mae: 7724.7402 - val_loss: 8558.4194 - val_mse: 174928832.0000 - val_mae: 8558.4199\n",
            "Epoch 259/500\n",
            "90/90 [==============================] - 0s 638us/step - loss: 6712.7420 - mse: 140371824.0000 - mae: 6712.7417 - val_loss: 8479.6224 - val_mse: 177248624.0000 - val_mae: 8479.6221\n",
            "Epoch 260/500\n",
            "90/90 [==============================] - 0s 695us/step - loss: 8557.9804 - mse: 222922208.0000 - mae: 8557.9805 - val_loss: 8605.0343 - val_mse: 175145008.0000 - val_mae: 8605.0352\n",
            "Epoch 261/500\n",
            "90/90 [==============================] - 0s 666us/step - loss: 7814.4839 - mse: 185983440.0000 - mae: 7814.4839 - val_loss: 8486.0313 - val_mse: 176717552.0000 - val_mae: 8486.0322\n",
            "Epoch 262/500\n",
            "90/90 [==============================] - 0s 789us/step - loss: 8089.0482 - mse: 195591920.0000 - mae: 8089.0488 - val_loss: 8440.3193 - val_mse: 176665472.0000 - val_mae: 8440.3193\n",
            "Epoch 263/500\n",
            "90/90 [==============================] - 0s 670us/step - loss: 7106.0900 - mse: 174615520.0000 - mae: 7106.0894 - val_loss: 8551.7291 - val_mse: 173505904.0000 - val_mae: 8551.7295\n",
            "Epoch 264/500\n",
            "90/90 [==============================] - 0s 648us/step - loss: 7308.8778 - mse: 158765728.0000 - mae: 7308.8779 - val_loss: 8316.8679 - val_mse: 172636848.0000 - val_mae: 8316.8682\n",
            "Epoch 265/500\n",
            "90/90 [==============================] - 0s 675us/step - loss: 7032.9448 - mse: 134048952.0000 - mae: 7032.9453 - val_loss: 8530.9844 - val_mse: 172360992.0000 - val_mae: 8530.9844\n",
            "Epoch 266/500\n",
            "90/90 [==============================] - 0s 662us/step - loss: 6983.0329 - mse: 169689632.0000 - mae: 6983.0327 - val_loss: 8423.1346 - val_mse: 169482704.0000 - val_mae: 8423.1348\n",
            "Epoch 267/500\n",
            "90/90 [==============================] - 0s 816us/step - loss: 6584.1758 - mse: 116428024.0000 - mae: 6584.1763 - val_loss: 8356.8595 - val_mse: 173420912.0000 - val_mae: 8356.8594\n",
            "Epoch 268/500\n",
            "90/90 [==============================] - 0s 712us/step - loss: 7252.7161 - mse: 155646992.0000 - mae: 7252.7158 - val_loss: 8385.1924 - val_mse: 171429456.0000 - val_mae: 8385.1934\n",
            "Epoch 269/500\n",
            "90/90 [==============================] - 0s 747us/step - loss: 7518.5797 - mse: 171933920.0000 - mae: 7518.5801 - val_loss: 8459.3915 - val_mse: 174874064.0000 - val_mae: 8459.3916\n",
            "Epoch 270/500\n",
            "90/90 [==============================] - 0s 657us/step - loss: 7248.0565 - mse: 143062816.0000 - mae: 7248.0562 - val_loss: 8353.1533 - val_mse: 168376608.0000 - val_mae: 8353.1533\n",
            "Epoch 271/500\n",
            "90/90 [==============================] - 0s 687us/step - loss: 8024.0166 - mse: 202323472.0000 - mae: 8024.0176 - val_loss: 8340.0447 - val_mse: 171531664.0000 - val_mae: 8340.0449\n",
            "Epoch 272/500\n",
            "90/90 [==============================] - 0s 703us/step - loss: 8314.8737 - mse: 189762944.0000 - mae: 8314.8740 - val_loss: 8355.5149 - val_mse: 169961184.0000 - val_mae: 8355.5156\n",
            "Epoch 273/500\n",
            "90/90 [==============================] - 0s 711us/step - loss: 7071.7801 - mse: 140616544.0000 - mae: 7071.7798 - val_loss: 8300.4993 - val_mse: 170219584.0000 - val_mae: 8300.4990\n",
            "Epoch 274/500\n",
            "90/90 [==============================] - 0s 701us/step - loss: 7504.5142 - mse: 172426496.0000 - mae: 7504.5137 - val_loss: 8196.1509 - val_mse: 166748416.0000 - val_mae: 8196.1514\n",
            "Epoch 275/500\n",
            "90/90 [==============================] - 0s 669us/step - loss: 8624.4085 - mse: 242582544.0000 - mae: 8624.4082 - val_loss: 8095.1807 - val_mse: 164421152.0000 - val_mae: 8095.1807\n",
            "Epoch 276/500\n",
            "90/90 [==============================] - 0s 623us/step - loss: 7608.0856 - mse: 172569424.0000 - mae: 7608.0854 - val_loss: 8158.9505 - val_mse: 163160704.0000 - val_mae: 8158.9507\n",
            "Epoch 277/500\n",
            "90/90 [==============================] - 0s 657us/step - loss: 7042.3399 - mse: 134879136.0000 - mae: 7042.3394 - val_loss: 8348.9437 - val_mse: 164807936.0000 - val_mae: 8348.9434\n",
            "Epoch 278/500\n",
            "90/90 [==============================] - 0s 678us/step - loss: 7071.3582 - mse: 143303616.0000 - mae: 7071.3584 - val_loss: 8328.5122 - val_mse: 166830368.0000 - val_mae: 8328.5127\n",
            "Epoch 279/500\n",
            "90/90 [==============================] - 0s 826us/step - loss: 6870.5194 - mse: 134433408.0000 - mae: 6870.5195 - val_loss: 8359.6797 - val_mse: 168677824.0000 - val_mae: 8359.6787\n",
            "Epoch 280/500\n",
            "90/90 [==============================] - 0s 651us/step - loss: 7044.9195 - mse: 177630912.0000 - mae: 7044.9194 - val_loss: 8464.3416 - val_mse: 167824016.0000 - val_mae: 8464.3418\n",
            "Epoch 281/500\n",
            "90/90 [==============================] - 0s 593us/step - loss: 7148.9056 - mse: 147899424.0000 - mae: 7148.9048 - val_loss: 8247.4832 - val_mse: 164067968.0000 - val_mae: 8247.4834\n",
            "Epoch 282/500\n",
            "90/90 [==============================] - 0s 571us/step - loss: 6520.7423 - mse: 125206016.0000 - mae: 6520.7422 - val_loss: 8183.7079 - val_mse: 166199040.0000 - val_mae: 8183.7085\n",
            "Epoch 283/500\n",
            "90/90 [==============================] - 0s 673us/step - loss: 7655.4956 - mse: 166868352.0000 - mae: 7655.4951 - val_loss: 8238.6195 - val_mse: 166153856.0000 - val_mae: 8238.6191\n",
            "Epoch 284/500\n",
            "90/90 [==============================] - 0s 658us/step - loss: 8276.5664 - mse: 198041056.0000 - mae: 8276.5664 - val_loss: 8376.7818 - val_mse: 168765424.0000 - val_mae: 8376.7822\n",
            "Epoch 285/500\n",
            "90/90 [==============================] - 0s 811us/step - loss: 8067.5492 - mse: 181248416.0000 - mae: 8067.5498 - val_loss: 8405.6141 - val_mse: 173382128.0000 - val_mae: 8405.6143\n",
            "Epoch 286/500\n",
            "90/90 [==============================] - 0s 812us/step - loss: 7701.9871 - mse: 204537312.0000 - mae: 7701.9873 - val_loss: 8425.6162 - val_mse: 169630224.0000 - val_mae: 8425.6162\n",
            "Epoch 287/500\n",
            "90/90 [==============================] - 0s 661us/step - loss: 7225.6121 - mse: 196052928.0000 - mae: 7225.6123 - val_loss: 8411.0859 - val_mse: 169006224.0000 - val_mae: 8411.0859\n",
            "Epoch 288/500\n",
            "90/90 [==============================] - 0s 694us/step - loss: 6770.7194 - mse: 124580856.0000 - mae: 6770.7192 - val_loss: 8297.9613 - val_mse: 166075168.0000 - val_mae: 8297.9609\n",
            "Epoch 289/500\n",
            "90/90 [==============================] - 0s 661us/step - loss: 7549.0656 - mse: 168483584.0000 - mae: 7549.0654 - val_loss: 8434.2391 - val_mse: 174203408.0000 - val_mae: 8434.2393\n",
            "Epoch 290/500\n",
            "90/90 [==============================] - 0s 632us/step - loss: 7358.2196 - mse: 191065104.0000 - mae: 7358.2192 - val_loss: 8376.3856 - val_mse: 167180368.0000 - val_mae: 8376.3857\n",
            "Epoch 291/500\n",
            "90/90 [==============================] - 0s 804us/step - loss: 8007.1899 - mse: 186779280.0000 - mae: 8007.1904 - val_loss: 8330.4599 - val_mse: 169546928.0000 - val_mae: 8330.4600\n",
            "Epoch 292/500\n",
            "90/90 [==============================] - 0s 663us/step - loss: 7355.3375 - mse: 166647280.0000 - mae: 7355.3374 - val_loss: 8419.0367 - val_mse: 169268512.0000 - val_mae: 8419.0361\n",
            "Epoch 293/500\n",
            "90/90 [==============================] - 0s 676us/step - loss: 6506.9506 - mse: 128399256.0000 - mae: 6506.9507 - val_loss: 8408.4807 - val_mse: 170405088.0000 - val_mae: 8408.4805\n",
            "Epoch 294/500\n",
            "90/90 [==============================] - 0s 725us/step - loss: 6883.5626 - mse: 158475664.0000 - mae: 6883.5625 - val_loss: 8407.1667 - val_mse: 169495728.0000 - val_mae: 8407.1670\n",
            "Epoch 295/500\n",
            "90/90 [==============================] - 0s 799us/step - loss: 7377.7395 - mse: 171745072.0000 - mae: 7377.7388 - val_loss: 8475.7300 - val_mse: 173196272.0000 - val_mae: 8475.7295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im2-1jnV8T_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b652cc3-30ce-4bff-b45b-6b147e9644f4"
      },
      "source": [
        "# Save the running time for each country\n",
        "for i in range(len(countries)):\n",
        "  save_runtime(results_runtime_static[i], path=exp1_runtime_path, country = countries[i], static_learner=True)\n",
        "\n",
        "display_runtime_per_country(results_runtime_static, countries)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_____________Running Time for United_States_of_America________________\n",
            "   PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge   LSTM\n",
            "0            30         0.133             0.037      0.002         0.002          0.028 18.409\n",
            "1            60         0.124             0.061      0.014         0.003          0.004 14.257\n",
            "2            90         0.136             0.099      0.013         0.004          0.004 26.862\n",
            "3           120         0.166             0.130      0.018         0.005          0.004 17.998\n",
            "4           150         0.172             0.165      0.023         0.006          0.004 21.070\n",
            "5           180         0.182             0.196      0.028         0.006          0.005 20.264\n",
            "\n",
            "\n",
            "_____________Running Time for India________________\n",
            "   PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge   LSTM\n",
            "0            30         0.127             0.034      0.002         0.002          0.003  8.524\n",
            "1            60         0.120             0.053      0.005         0.003          0.004 15.679\n",
            "2            90         0.137             0.080      0.012         0.005          0.004 12.994\n",
            "3           120         0.144             0.115      0.017         0.004          0.005 27.301\n",
            "4           150         0.156             0.136      0.021         0.005          0.004 13.858\n",
            "5           180         0.174             0.167      0.027         0.005          0.006 23.349\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NlVmdHK37Bk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9cfe442-603e-4ddb-a158-7159c6800d58"
      },
      "source": [
        "countrywise_error_scores_static = calc_save_err_metric_countrywise(countries, error_metrics, results_static, max_of_pretrain_per_country, max_cases_per_country, path=exp1_path, static_learner=True, transpose=True)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________United_States_of_America____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting   LinearSVR  DecisionTree  BayesianRidge      LSTM\n",
            "0                      MAE        30.000      4093.543          3867.215 1006480.785      4560.880      99068.637  2468.920\n",
            "1                      MAE        60.000      2042.897          1590.215 2886240.906      1845.367       1676.672  9810.243\n",
            "2                      MAE        90.000     28342.025         25233.845  153275.689     25486.720      26921.287 32518.019\n",
            "3                      MAE       120.000      8530.576          6842.980   71920.223      6927.423      10656.728 38917.236\n",
            "4                      MAE       150.000      5149.964          5649.100   54311.707      8471.413       4885.341 10335.895\n",
            "5                      MAE       180.000      7393.098          7508.353   62839.832      7452.797      31479.673 20919.696\n",
            "mean                   NaN       105.000      9258.684          8448.618  705844.857      9124.100      29114.723 19161.668\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_States_of_America____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge  LSTM\n",
            "0                     MAPE        30.000         0.174             0.165     42.852         0.193          4.188 0.092\n",
            "1                     MAPE        60.000         0.086             0.065    120.951         0.075          0.068 0.404\n",
            "2                     MAPE        90.000         0.507             0.444      3.307         0.453          0.479 0.592\n",
            "3                     MAPE       120.000         0.156             0.133      1.354         0.127          0.186 0.672\n",
            "4                     MAPE       150.000         0.134             0.142      1.367         0.211          0.122 0.256\n",
            "5                     MAPE       180.000         0.141             0.143      1.287         0.142          0.651 0.387\n",
            "mean                   NaN       105.000         0.200             0.182     28.520         0.200          0.949 0.400\n",
            "\n",
            "\n",
            "\n",
            "_________________________________United_States_of_America____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting   LinearSVR  DecisionTree  BayesianRidge      LSTM\n",
            "0                     RMSE        30.000      4744.386          4609.011 1271395.761      5284.493     113821.065  3210.672\n",
            "1                     RMSE        60.000      2504.816          2314.059 3558130.736      2600.921       2376.807 13547.039\n",
            "2                     RMSE        90.000     30471.877         27734.113  209418.840     27890.611      29160.952 34231.188\n",
            "3                     RMSE       120.000      9865.211          8583.636   81623.132      8256.524      11244.922 42295.057\n",
            "4                     RMSE       150.000      5666.527          6106.212   63430.163      9177.109       6597.735 12341.651\n",
            "5                     RMSE       180.000      9571.022          9741.810   74241.247      9703.350      32886.465 31958.968\n",
            "mean                   NaN       105.000     10470.640          9848.140  876373.313     10485.501      32681.324 22930.762\n",
            "\n",
            "\n",
            "\n",
            "_________________________________India____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge      LSTM\n",
            "0                      MAE        30.000      2219.274          2119.419  49018.158      2133.470      30825.818  2911.881\n",
            "1                      MAE        60.000      4698.939          4000.698  36792.906      4106.457       3116.148  9104.813\n",
            "2                      MAE        90.000     13901.470         10974.902  18090.459     10935.703       5446.482 18690.791\n",
            "3                      MAE       120.000     20994.592         13996.480  20186.811     13912.277       8424.858 44969.276\n",
            "4                      MAE       150.000     24361.284         18253.243 139414.138     18587.637      57737.370 57682.976\n",
            "5                      MAE       180.000     13069.371         16135.610 103960.188     16010.930     132002.377 33008.206\n",
            "mean                   NaN       105.000     13207.488         10913.392  61243.777     10947.746      39592.175 27727.990\n",
            "\n",
            "\n",
            "\n",
            "_________________________________India____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge  LSTM\n",
            "0                     MAPE        30.000         0.553             0.520     11.097         0.523          6.853 0.790\n",
            "1                     MAPE        60.000         0.429             0.358      3.388         0.368          0.267 0.883\n",
            "2                     MAPE        90.000         0.486             0.366      0.702         0.364          0.163 0.699\n",
            "3                     MAPE       120.000         0.360             0.236      0.351         0.234          0.151 0.793\n",
            "4                     MAPE       150.000         0.285             0.211      1.681         0.215          0.670 0.709\n",
            "5                     MAPE       180.000         0.200             0.243      1.424         0.241          1.807 0.410\n",
            "mean                   NaN       105.000         0.385             0.322      3.108         0.324          1.652 0.714\n",
            "\n",
            "\n",
            "\n",
            "_________________________________India____________________________________________\n",
            "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge      LSTM\n",
            "0                     RMSE        30.000      2630.341          2545.014  65093.625      2558.767      41864.652  3242.004\n",
            "1                     RMSE        60.000      5293.839          4674.987  41330.425      4775.740       4158.752  9480.839\n",
            "2                     RMSE        90.000     15952.494         13441.233  21063.042     13428.463       8319.709 19803.972\n",
            "3                     RMSE       120.000     21886.902         15248.913  23216.691     15225.473       9315.392 45117.118\n",
            "4                     RMSE       150.000     25867.027         20216.601 140951.480     20521.481      65572.720 58063.515\n",
            "5                     RMSE       180.000     16011.392         18700.994 113242.863     18517.183     154810.268 44109.837\n",
            "mean                   NaN       105.000     14606.999         12471.290  67483.021     12504.518      47340.249 29969.547\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU9yFzgj_IHX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "e5762e59-c96b-4f14-ff86-eb53db8d7081"
      },
      "source": [
        "summary_table_countrywise_static = get_summary_table_countrywise(countrywise_error_scores_static, ['MAPE'], static_learner=True)\n",
        "\n",
        "# Saving the transposed matrix\n",
        "save_summary_table(summary_table_countrywise_static, exp1_summary_path, country=True, static_learner=True, alternate_batch=False, transpose=True)\n",
        "\n",
        "summary_table_countrywise_static"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RandomForest</th>\n",
              "      <th>GradientBoosting</th>\n",
              "      <th>LinearSVR</th>\n",
              "      <th>DecisionTree</th>\n",
              "      <th>BayesianRidge</th>\n",
              "      <th>LSTM</th>\n",
              "      <th>EvaluationMeasurement</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Country(MAPE)</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>United_States_of_America</th>\n",
              "      <td>0.200</td>\n",
              "      <td>0.182</td>\n",
              "      <td>28.520</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.949</td>\n",
              "      <td>0.400</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>India</th>\n",
              "      <td>0.385</td>\n",
              "      <td>0.322</td>\n",
              "      <td>3.108</td>\n",
              "      <td>0.324</td>\n",
              "      <td>1.652</td>\n",
              "      <td>0.714</td>\n",
              "      <td>MAPE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          RandomForest  ...  EvaluationMeasurement\n",
              "Country(MAPE)                           ...                       \n",
              "United_States_of_America         0.200  ...                   MAPE\n",
              "India                            0.385  ...                   MAPE\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTMQFGlMfCkT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "58e72658-7ed8-4897-b231-5acf6ed572c3"
      },
      "source": [
        "sum_static_countrywise_mean = get_sum_table_combined_mean(countrywise_error_scores_static,results_runtime_static,static_learner=True)\n",
        "save_combined_summary_table(sum_static_countrywise_mean, exp1_summary_path, static_learner=True, transpose=True) \n",
        "sum_static_countrywise_mean"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RandomForest</th>\n",
              "      <th>GradientBoosting</th>\n",
              "      <th>LinearSVR</th>\n",
              "      <th>DecisionTree</th>\n",
              "      <th>BayesianRidge</th>\n",
              "      <th>LSTM</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Metric</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MAE</th>\n",
              "      <td>11233.086</td>\n",
              "      <td>9681.005</td>\n",
              "      <td>383544.317</td>\n",
              "      <td>10035.923</td>\n",
              "      <td>34353.449</td>\n",
              "      <td>23444.829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MAPE</th>\n",
              "      <td>0.293</td>\n",
              "      <td>0.252</td>\n",
              "      <td>15.814</td>\n",
              "      <td>0.262</td>\n",
              "      <td>1.300</td>\n",
              "      <td>0.557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RMSE</th>\n",
              "      <td>12538.820</td>\n",
              "      <td>11159.715</td>\n",
              "      <td>471928.167</td>\n",
              "      <td>11495.010</td>\n",
              "      <td>40010.787</td>\n",
              "      <td>26450.155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time(sec)</th>\n",
              "      <td>0.148</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.006</td>\n",
              "      <td>18.380</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           RandomForest  GradientBoosting  ...  BayesianRidge      LSTM\n",
              "Metric                                     ...                         \n",
              "MAE           11233.086          9681.005  ...      34353.449 23444.829\n",
              "MAPE              0.293             0.252  ...          1.300     0.557\n",
              "RMSE          12538.820         11159.715  ...      40010.787 26450.155\n",
              "Time(sec)         0.148             0.106  ...          0.006    18.380\n",
              "\n",
              "[4 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIMr3U1QAc63"
      },
      "source": [
        "## Significance tests for Experiment 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlZxtco1AbsI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "4c5a290b-cb03-4e95-f123-2779a4ae46f7"
      },
      "source": [
        "## EXP1\n",
        "# Significance results for Experiment 1\n",
        "err_metric_for_significance = 'MAPE'\n",
        "significance_thresh = 0.01\n",
        "plot_pop = False\n",
        "\n",
        "# Concatenating a population of all results (as in boxplot) for experiment 1\n",
        "concated_df = pd.concat([summary_table_countrywise_incremental.transpose(), summary_table_countrywise_static.transpose()]).transpose().drop(columns=['EvaluationMeasurement'], axis=1)\n",
        "concated_df\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HT_Reg</th>\n",
              "      <th>HAT_Reg</th>\n",
              "      <th>ARF_Reg</th>\n",
              "      <th>PA_Reg</th>\n",
              "      <th>RandomForest</th>\n",
              "      <th>GradientBoosting</th>\n",
              "      <th>LinearSVR</th>\n",
              "      <th>DecisionTree</th>\n",
              "      <th>BayesianRidge</th>\n",
              "      <th>LSTM</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Country(MAPE)</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>United_States_of_America</th>\n",
              "      <td>0.231</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.510</td>\n",
              "      <td>20.214</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.182</td>\n",
              "      <td>28.520</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.949</td>\n",
              "      <td>0.400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>India</th>\n",
              "      <td>0.255</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.938</td>\n",
              "      <td>1.266</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.322</td>\n",
              "      <td>3.108</td>\n",
              "      <td>0.324</td>\n",
              "      <td>1.652</td>\n",
              "      <td>0.714</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         HT_Reg HAT_Reg  ... BayesianRidge  LSTM\n",
              "Country(MAPE)                            ...                    \n",
              "United_States_of_America  0.231   0.223  ...         0.949 0.400\n",
              "India                     0.255   0.273  ...         1.652 0.714\n",
              "\n",
              "[2 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyz23f1PMrpY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "71dc4a03-eb91-4e4b-95bd-e41da1606897"
      },
      "source": [
        "# Selecting the best algorithm for statistical comparisons\n",
        "# We want to know if the best is statistically significantly better compared to the rest.\n",
        "best_algo = concated_df.mean().sort_values(ascending=True).index[0]\n",
        "best_algo"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'HT_Reg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFR6AjtxMv-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96dcca12-7f14-4bf1-8702-fc1f124f3355"
      },
      "source": [
        "print('AVG results across countries')\n",
        "concated_df.mean()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AVG results across countries\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HT_Reg              0.243\n",
              "HAT_Reg             0.248\n",
              "ARF_Reg             0.724\n",
              "PA_Reg             10.740\n",
              "RandomForest        0.293\n",
              "GradientBoosting    0.252\n",
              "LinearSVR          15.814\n",
              "DecisionTree        0.262\n",
              "BayesianRidge       1.300\n",
              "LSTM                0.557\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLYuRNC8M0yF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83161cd5-7515-4fa8-b2ab-b7fd84127fda"
      },
      "source": [
        "print('STDEV across countries')\n",
        "concated_df.std()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STDEV across countries\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HT_Reg              0.017\n",
              "HAT_Reg             0.035\n",
              "ARF_Reg             0.302\n",
              "PA_Reg             13.398\n",
              "RandomForest        0.131\n",
              "GradientBoosting    0.099\n",
              "LinearSVR          17.969\n",
              "DecisionTree        0.088\n",
              "BayesianRidge       0.497\n",
              "LSTM                0.222\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpiiYDApAbeX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "a09aa641-9faf-4be6-81cd-cade3d7f25ae"
      },
      "source": [
        "# Iterate through all the other algorithms to see if the difference in results is significant\n",
        "competitors = list(concated_df.columns)\n",
        "competitors.remove(best_algo)\n",
        "#TODO: \n",
        "for significance_thresh in [0.01,0.05]:\n",
        "  print(f'Running significane at: {significance_thresh}')\n",
        "  for competitor in competitors:\n",
        "    # print(competitor)\n",
        "    pval, significant = check_significance_nonparam(concated_df[best_algo], concated_df[competitor], significance_at=significance_thresh, plot_flag = plot_pop)\n",
        "    print(f'Comparison of {best_algo} to {competitor} pvalue: {pval}   /  significant?: {significant}')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running significane at: 0.01\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-5b1141912f28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mcompetitor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompetitors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# print(competitor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignificant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_significance_nonparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcated_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_algo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcated_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompetitor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignificance_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignificance_thresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_pop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Comparison of {best_algo} to {competitor} pvalue: {pval}   /  significant?: {significant}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'check_significance_nonparam' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3abKxWJwqNU6"
      },
      "source": [
        "# Experiment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM8GB6LTRPxY"
      },
      "source": [
        "### Creating Combined dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7vb_yHqvwPX"
      },
      "source": [
        "# Reading file \n",
        "#url = 'https://drive.google.com/file/d/1e7NsptfEFLG2gGLykYlrzjNbDJLiRbGm/view?usp=sharing'\n",
        "url = 'https://drive.google.com/file/d/1VH-nkePskK3gT6U5qkoOP-0hFT4beszC/view?usp=sharing'\n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "df = pd.read_csv(path)\n",
        "num_selected_countries = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxDuX7-ansDh"
      },
      "source": [
        "\n",
        "# Pre-processing dataset\n",
        "df = preprocess_dataset(df)\n",
        "\n",
        "# Grouping records by country\n",
        "df_grouped = df.groupby('country')\n",
        "\n",
        "# Taking only those countries which have sufficient data records\n",
        "#valid_countries = get_countries_with_valid_size(df_grouped)\n",
        "\n",
        "# Sorting countries by number of cases\n",
        "df_countries_sortedbycases = get_countries_sortedby_cases(valid_countries, df_grouped)\n",
        "\n",
        "# Taking only top selected countries\n",
        "top_selected_countries = df_countries_sortedbycases.iloc[0:num_selected_countries].index\n",
        "\n",
        "# Calculating targets and lags for the above countries\n",
        "result = get_dataset_with_target(top_selected_countries,df_grouped)\n",
        "\n",
        "# Calculating targets and lags for the above countries\n",
        "result = get_dataset_with_target(top_selected_countries,df_grouped)\n",
        "\n",
        "# Getting max of each subset in pretrain size\n",
        "max_of_pretrain_days = calc_max_of_pretrain_days(pretrain_days,result)\n",
        "\n",
        "# Mean of top selected countries\n",
        "max_selected_countries = result['cases'].max()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtxpRv3FvxdD"
      },
      "source": [
        "### Incremental Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcRN9CoVFigI"
      },
      "source": [
        "def scikit_multiflow(df, pretrain_days):\n",
        "\n",
        "  model, model_names = instantiate_regressors()\n",
        "\n",
        "  len_countries = len(df['country'].unique())\n",
        "\n",
        "  frames , running_time_frames = [], []\n",
        "\n",
        "  # Setup the evaluator\n",
        "  for day in pretrain_days:\n",
        "\n",
        "      df_subset = create_subset(df,day)\n",
        "      \n",
        "      # Creating a stream from dataframe\n",
        "      stream = DataStream(np.array(df_subset.iloc[:,4:-1]), y=np.array(df_subset.iloc[:,-1])) #TODO: Drop columns with name \n",
        "\n",
        "      pretrain_size = day * len_countries\n",
        "      max_samples = (day + 30) * len_countries #Testing on set one month ahead only\n",
        "\n",
        "      evaluator = EvaluatePrequential(show_plot=False,\n",
        "                                    pretrain_size=pretrain_size,\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\n",
        "                                    max_samples=max_samples)\n",
        "      # Run evaluation\n",
        "      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
        "\n",
        "      # Dictionary to store each iteration error scores\n",
        "      mdl_evaluation_scores = {}\n",
        "\n",
        "      # Adding Evaluation Measurements and pretraining days\n",
        "      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\n",
        "      mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "\n",
        "      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\n",
        "\n",
        "      # Errors of each model on a specific pre-train days\n",
        "      frames.append(mdl_evaluation_df)\n",
        "\n",
        "      # Run time for each algorithm\n",
        "      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
        "\n",
        "   # Final Run Time DataFrame\n",
        "  running_time_df = pd.concat(running_time_frames,ignore_index=True)\n",
        "\n",
        "  # Final Evaluation Score Dataframe\n",
        "  evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
        "  return evaluation_scores_df, running_time_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32jMOvemFnHG"
      },
      "source": [
        "result_skmlflow, running_time_combined_incremental = scikit_multiflow(result, pretrain_days)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xOniEJ11niO"
      },
      "source": [
        "df_skmlflow = calc_save_err_metric_combined(error_metrics, result_skmlflow, max_of_pretrain_days, max_selected_countries, path=exp2_path, static_learner=False, alternate_batch=False, transpose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq1ffodM5C4f"
      },
      "source": [
        "save_runtime(running_time_combined_incremental, path=exp2_runtime_path, static_learner=False)\n",
        "running_time_combined_incremental"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrGTNx3kTNDU"
      },
      "source": [
        "summary_table_incremental = get_summary_table(df_skmlflow, running_time_combined_incremental, error_metrics, static_learner=False)\n",
        "save_summary_table(summary_table_incremental, exp2_summary_path,static_learner=False,alternate_batch=False, transpose=True)\n",
        "summary_table_incremental"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m4-HvnEwA-l"
      },
      "source": [
        "### Static Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYtaCPtjjpv5"
      },
      "source": [
        "def scikit_learn(df, training_days):\n",
        "  len_countries = len(df['country'].unique())\n",
        "  frames = []\n",
        "  model_predictions = {\n",
        "      'RandomForest':[],\n",
        "      'GradientBoosting':[],\n",
        "      'LinearSVR':[],\n",
        "      'DecisionTree':[],\n",
        "      'BayesianRidge':[],\n",
        "      'LSTM': []\n",
        "      #'MLPRegressor': [],\n",
        "      #'LinearRegression': []\n",
        "    }\n",
        "\n",
        "  # LSTM (TODO: feel free to put the whole LSTM definition in a funtion)\n",
        "  # params (others like epoch and batch size are also hardcoded in train_test_lstm())\n",
        "  layers = [20, 10, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\n",
        "  activations = ['tanh', 'tanh', 'relu']\n",
        "  patience = 20\n",
        "  total_execution_time = []\n",
        "\n",
        "  for day in training_days:\n",
        "\n",
        "    df_subset = create_subset(df,day)\n",
        "    \n",
        "    train_end_day = day * len_countries\n",
        "    test_end_day = (day+30) * len_countries\n",
        "\n",
        "    train = df_subset.iloc[:train_end_day, :] \n",
        "    test = df_subset.iloc[train_end_day:test_end_day, :]  # Testing on set one month ahead only, hence day+30.\n",
        "    test_df, val_df = get_validation_set(test, batch_size=10)  # Getting validation set from test set\n",
        "\n",
        "    # training and test sets for all models except LSTM\n",
        "    X_train, y_train = train.iloc[:,4:-1], train.iloc[:,-1]\n",
        "    X_test, y_test = test.iloc[:,4:-1], test.iloc[:,-1]\n",
        "\n",
        "    # Validation and test set for LSTM\n",
        "    X_test_lstm, y_test_lstm = test_df.iloc[:, 4:-1], test_df.iloc[:, -1]\n",
        "    X_val, y_val = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\n",
        "\n",
        "    X_train_lstm, X_val, X_test_lstm = reshape_dataframe(X_train, X_val, X_test_lstm)  # reshaping the vector for lstm\n",
        "    cur_exec_time = [day]\n",
        "\n",
        "    rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "    model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train,y_train,X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    gb_reg = GradientBoostingRegressor(random_state=0)\n",
        "    model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    lsv_reg = LinearSVR(random_state=0, tol=1e-5)\n",
        "    model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    dt_reg = DecisionTreeRegressor(random_state=0)\n",
        "    model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    br_reg = BayesianRidge()\n",
        "    model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    '''\n",
        "    mlp_reg = MLPRegressor(random_state=1, max_iter=500)\n",
        "    model_predictions['MLPRegressor'], exec_time = train_test_model(mlp_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "\n",
        "    lin_reg = LinearRegression()\n",
        "    model_predictions['LinearRegression'], exec_time = train_test_model(lin_reg, X_train, y_train, X_test)\n",
        "    cur_exec_time.append(exec_time)\n",
        "    '''\n",
        "\n",
        "    lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\n",
        "    model_predictions['LSTM'], exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train, X_val, y_val, X_test_lstm, patience)         \n",
        "    cur_exec_time.append(exec_time)\n",
        "\n",
        "    mdl_evaluation_df = get_scores(y_test,y_test_lstm, model_predictions, day)\n",
        "    total_execution_time.append(cur_exec_time)\n",
        "    frames.append(mdl_evaluation_df)\n",
        "\n",
        "  evaluation_score_df = pd.concat(frames, ignore_index=True)\n",
        "  running_time_df = get_running_time_per_model_static_learner(model_predictions,total_execution_time)\n",
        "  return evaluation_score_df, running_time_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inEIGOoYn0ng"
      },
      "source": [
        "result_sklearn, running_time_static = scikit_learn(result, pretrain_days)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZSPkuft7YMp"
      },
      "source": [
        "df_sklearn = calc_save_err_metric_combined(error_metrics, result_sklearn, max_of_pretrain_days, max_selected_countries, path=exp2_path, static_learner=True, alternate_batch=False, transpose=True)\n",
        "display_scores(df_sklearn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U44wYr8WAqq8"
      },
      "source": [
        "save_runtime(running_time_static, path=exp2_runtime_path, static_learner=True)\n",
        "running_time_static"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7NiYw19VchA"
      },
      "source": [
        "summary_table_static = get_summary_table(df_sklearn, running_time_static, error_metrics, static_learner=True)\n",
        "save_summary_table(summary_table_static, exp2_summary_path,static_learner=True,alternate_batch=False, transpose=True)\n",
        "summary_table_static"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAdncNlrKXD8"
      },
      "source": [
        "## Incremental Learner: Alternate Batches\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DpEHhhFiDfw"
      },
      "source": [
        "def scikit_multiflow_alternate_batch(df, pretrain_days):\n",
        "\n",
        "  model, model_names = instantiate_regressors()\n",
        "\n",
        "  len_countries = len(df['country'].unique())\n",
        "\n",
        "  frames , running_time_frames = [], []\n",
        "\n",
        "  # Setup the evaluator\n",
        "  for day in pretrain_days:\n",
        "\n",
        "      df_subset = create_alternate_batch_subset(df,day,batch_size=10)\n",
        "\n",
        "      # Creating a stream from dataframe\n",
        "      stream = DataStream(np.array(df_subset.iloc[:,4:-1]), y=np.array(df_subset.iloc[:,-1])) #TODO: Drop columns with name \n",
        "\n",
        "      pretrain_size = (day//2) * len_countries\n",
        "      max_samples = (day//2 + 20) * len_countries #Testing on set one month ahead only\n",
        "\n",
        "      evaluator = EvaluatePrequential(show_plot=False,\n",
        "                                    pretrain_size=pretrain_size,\n",
        "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\n",
        "                                    max_samples=max_samples)\n",
        "      # Run evaluation\n",
        "      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
        "\n",
        "      # Dictionary to store each iteration error scores\n",
        "      mdl_evaluation_scores = {}\n",
        "\n",
        "      # Adding Evaluation Measurements and pretraining days\n",
        "      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\n",
        "      mdl_evaluation_scores['PretrainDays'] = [day//2] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
        "\n",
        "      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\n",
        "\n",
        "      # Errors of each model on a specific pre-train days\n",
        "      frames.append(mdl_evaluation_df)\n",
        "\n",
        "      # Run time for each algorithm\n",
        "      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
        "\n",
        "  # Final Run Time DataFrame\n",
        "  running_time_df = pd.concat(running_time_frames,ignore_index=True)\n",
        "\n",
        "  # Final Evaluation Score Dataframe\n",
        "  evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
        "  return evaluation_scores_df, running_time_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv6KT2eyheiH"
      },
      "source": [
        "result_skmlflow_alternate_batch, running_time_incremental_alternate_batch = scikit_multiflow_alternate_batch(result, pretrain_days)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfluozdk7zod"
      },
      "source": [
        "df_alternate_batch = calc_save_err_metric_combined(error_metrics, result_skmlflow_alternate_batch, max_of_pretrain_days, max_selected_countries, path=exp2_path, static_learner=False, alternate_batch=True, transpose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL_NOS6va8bo"
      },
      "source": [
        "display_scores(df_alternate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNBEQbIeA-7H"
      },
      "source": [
        "save_runtime(running_time_incremental_alternate_batch, path=exp2_runtime_path, static_learner=False, alternate_batch=True)\n",
        "running_time_incremental_alternate_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RuBt5n7Vqzf"
      },
      "source": [
        "summary_table_incremental_alternate = get_summary_table(df_alternate_batch, running_time_incremental_alternate_batch, error_metrics, static_learner=False)\n",
        "save_summary_table(summary_table_incremental_alternate, exp2_summary_path,static_learner=False,alternate_batch=True, transpose=True)\n",
        "summary_table_incremental_alternate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQYy8fGQ_EJ5"
      },
      "source": [
        "## Significance tests for Experiment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yH-IbaPBk2D"
      },
      "source": [
        "## EXP2\n",
        "# Significance results for Experiment 2\n",
        "err_metric_for_significance = 'MAPE'\n",
        "significance_thresh = 0.05\n",
        "plot_pop = False\n",
        "\n",
        "# Concatenating a population of all results (as in boxplot) for experiment 2\n",
        "# This is done for runs per batch for experiment 2. But for experiment 1 it's done for runs per country (their final averages, like the result sent to the boxplots).\n",
        "static = df_sklearn[df_sklearn['EvaluationMeasurement'] == err_metric_for_significance].drop(columns=['EvaluationMeasurement'], axis=1).transpose()\n",
        "incremental = df_alternate_batch[df_alternate_batch['EvaluationMeasurement'] == err_metric_for_significance].drop(columns=['EvaluationMeasurement', 'PretrainDays'], axis=1).transpose()\n",
        "concated_df = pd.concat([static, incremental]).transpose()\n",
        "concated_df.set_index('PretrainDays', inplace=True, drop=True)\n",
        "concated_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbQZ10ONBwJZ"
      },
      "source": [
        "# Selecting the best algorithm for statistical comparisons\n",
        "# We want to know if the best is statistically significantly better compared to the rest.\n",
        "best_algo = concated_df.mean().sort_values(ascending=True).index[0]\n",
        "best_algo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i0TvT9BNHwS"
      },
      "source": [
        "print('AVG results across countries')\n",
        "concated_df.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v6za-agNIrF"
      },
      "source": [
        "print('STEDEV across countries')\n",
        "concated_df.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG_TSWRXBwtM"
      },
      "source": [
        "# Iterate through all the other algorithms to see if the difference in results is significant\n",
        "competitors = list(concated_df.columns)\n",
        "competitors.remove(best_algo)\n",
        "for significance_thresh in [0.01,0.05]:\n",
        "  print(f'Running significane at: {significance_thresh}')\n",
        "  for competitor in competitors:\n",
        "    # print(competitor)\n",
        "    pval, significant = check_significance_nonparam(concated_df[best_algo], concated_df[competitor], significance_at=significance_thresh, plot_flag = plot_pop)\n",
        "    print(f'Comparison of {best_algo} to {competitor} pvalue: {pval}   /  significant?: {significant}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPwMeUDwn5Tk"
      },
      "source": [
        "# Download Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fVUOfpj78ES"
      },
      "source": [
        "\n",
        "!zip -r /content/Result.zip /content/Result\n",
        "from google.colab import files\n",
        "files.download(\"/content/Result.zip\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-m0P0s0hKNA"
      },
      "source": [
        "!zip -r /content/csv_files.zip /content/csv_files\n",
        "from google.colab import files\n",
        "files.download(\"/content/csv_files.zip\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDICI28viZYg"
      },
      "source": [
        "# Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DR7Z02IGVCk"
      },
      "source": [
        "## Bar Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkebu9hm0GMy"
      },
      "source": [
        "\"\"\"\n",
        "def get_filenames(pattern):\n",
        "  filenames=[]\n",
        "  for name in glob.glob(f\"{exp1_path}/*{pattern}*.csv\"):\n",
        "    filenames.append(name)\n",
        "  return filenames\n",
        "\n",
        "def get_countryname(filename):\n",
        "  country_name = filename.split('/')[-1].split('.')[0].split('_')[:-2]\n",
        "  return '_'.join(country_name)\n",
        "\n",
        "def get_error_stat_name(pattern):\n",
        "  return pattern.split('_')\n",
        "\n",
        "def plot_graph(df, statistics, metric):\n",
        "  sns.set_theme(style=\"darkgrid\")\n",
        "  df.plot(kind='bar',figsize=(12,10))\n",
        "  plt.title(f'{statistics}')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yscale('log')\n",
        "  plt.ylabel(metric)\n",
        "  ax = plt.gca()\n",
        "  ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))  # Changing scientific notation to plain text\n",
        "  plt.show()\n",
        "\n",
        "def  get_list_of_error_metric_per_country(filenames):\n",
        "  metric_dataframes_list = []\n",
        "  for filename in filenames:\n",
        "    country_name = get_countryname(filename)\n",
        "    df = pd.read_csv(filename)\n",
        "    df = df.set_index('Unnamed: 0')\n",
        "    df['country'] = country_name\n",
        "    metric_dataframes_list.append(df)\n",
        "  return metric_dataframes_list\n",
        "\n",
        "\n",
        "def get_mean_error_dataframes(metric_dataframes_list, learner_type):\n",
        "  mean_error_dataframes=[]\n",
        "  start_row = 'mean'\n",
        "  if learner_type == 'static':\n",
        "    start_col = 'RandomForest'\n",
        "  elif learner_type=='incremental':\n",
        "    start_col='HT_Reg'\n",
        "\n",
        "  for i in range(len(metric_dataframes_list)):\n",
        "    row = pd.DataFrame([metric_dataframes_list[i].loc[start_row][start_col:]])\n",
        "    mean_error_dataframes.append(row) # Passing values as a list for grouped bar chart\n",
        "    \n",
        "  result_df = pd.concat(mean_error_dataframes, ignore_index=True)\n",
        "  result_df.set_index('country', inplace=True)\n",
        "\n",
        "  return result_df\n",
        "\n",
        "filename_patterns = ['MAE_static','MAPE_static','MAE_incremental','MAPE_incremental']\n",
        "error_metric_mapper = {\n",
        "    'MAE': 'Mean Absolute Error(MAE)', \n",
        "    'MAPE':'Mean Absolute Percentage Error(MAPE)',\n",
        "    'RMSE':'Root Mean Square Error(RMSE)'}\n",
        "\n",
        "for pattern in filename_patterns:\n",
        "  filenames = get_filenames(pattern)\n",
        "  stat,learner_type = get_error_stat_name(pattern)\n",
        "  metric_dataframes_list = get_list_of_error_metric_per_country(filenames)\n",
        "  mean_error_dataframes = get_mean_error_dataframes(metric_dataframes_list,learner_type)\n",
        "  plot_graph(mean_error_dataframes,pattern,error_metric_mapper[stat])\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otlR4XEz2Kmf"
      },
      "source": [
        "\"\"\"!zip -r /content/summary_1.zip /content/Result/exp1/summary\n",
        "!zip -r /content/summary_2.zip /content/Result/exp2/summary\n",
        "from google.colab import files\n",
        "files.download(\"/content/summary_1.zip\")\n",
        "files.download(\"/content/summary_2.zip\")\"\"\"\n",
        "\n",
        "\"\"\"!zip -r /content/Result.zip /content/Result\n",
        "from google.colab import files\n",
        "files.download(\"/content/Result.zip\")\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmQXDjmrXAiS"
      },
      "source": [
        "!zip -r /content/csv_files.zip /content/csv_files\n",
        "from google.colab import files\n",
        "files.download(\"/content/csv_files.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQRkdX0oGaec"
      },
      "source": [
        "## Box Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oL2TxDdHRwM"
      },
      "source": [
        "def preprocess_data(df, metric_type, learner_type, col_mapper):\n",
        "  # Only pretrain days records are required not the mean row\n",
        "  df.drop(['mean'], axis=1, inplace=True)\n",
        "\n",
        "  # Renaming the Algorithm Columns\n",
        "  df.rename(columns={'Unnamed: 0': 'Algorithms'}, inplace=True)  \n",
        "\n",
        "  # Dropping first two rows: \"EvaluationMeasurement\" & \"PretrainDays\"\n",
        "  df.drop([0,1], axis=0, inplace=True)  \n",
        "\n",
        "  # Renaming columns based on mapper\n",
        "  df['Algorithms'].replace(col_mapper, inplace=True)  \n",
        "\n",
        "  # Melting the dataframe based on 'Algorithms'\n",
        "  df_melt = df.melt(id_vars=['Algorithms'])  \n",
        "\n",
        "  # Dropping unwanted varibale column(created bcoz of index)\n",
        "  df_melt.drop('variable', axis=1, inplace=True)  \n",
        "\n",
        "  # Renaming the value column by metric type\n",
        "  df_melt.rename(columns={'value':metric_type}, inplace=True)  \n",
        "\n",
        "  # Converting to float value bcoz by default the values are of type object\n",
        "  df_melt[metric_type] = df_melt[metric_type].astype('float64')  \n",
        "  \n",
        "  df_melt['Learner Type']= learner_type  # Adding the learner type\n",
        "\n",
        "  return df_melt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKugGWSVBPH6"
      },
      "source": [
        "def order_by_median(df,reverse = False):\n",
        "    grouped_df = df.groupby('Algorithms')\n",
        "    algo_medians = {}\n",
        "    for cur_group in grouped_df.groups.keys():\n",
        "        df_cur_grp = grouped_df.get_group(cur_group)\n",
        "        algo_medians[cur_group] = df_cur_grp['MAPE'].median()\n",
        "    sorted_algo_medians = dict(sorted(algo_medians.items(), key=lambda kv: kv[1], reverse=reverse))\n",
        "    return list(sorted_algo_medians.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1676SLhHSe_"
      },
      "source": [
        "def draw_boxplot(df):\n",
        "  plt.figure(figsize=(10,6),dpi=90)\n",
        "  #plt.title(title)\n",
        "  ordered_algo_list = order_by_median(df, reverse= False)\n",
        "  ax = sns.boxplot(x=\"Algorithms\", y=metric_type, hue='Learner Type', data=df, order=ordered_algo_list) #, hue_order=['Incremental','Static'])\n",
        "  ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
        "  ax.set(yscale = 'log')\n",
        "  ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKWFUip8HZd3"
      },
      "source": [
        "def read_preprocess_plot_graph(filenames, col_mapper, metric_type='MAPE'):\n",
        "  metric_type=metric_type\n",
        "  frames = []\n",
        "  for filename in filenames:\n",
        "    if 'static' in filename:\n",
        "      learner_type = 'Static'\n",
        "    else:\n",
        "      learner_type = 'Incremental'\n",
        "\n",
        "    df = pd.read_csv(filename)\n",
        "    df_melt = preprocess_data(df, metric_type, learner_type, col_mapper)\n",
        "    frames.append(df_melt)\n",
        "  \n",
        "  final_df = pd.concat(frames, ignore_index=True)\n",
        "  final_df = final_df.sort_values(by=['MAPE']) \n",
        "  #return final_df\n",
        "  draw_boxplot(final_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeltxQL_JnUH"
      },
      "source": [
        "col_mapper = {'HT_Reg':'Hoeffding Trees',\n",
        "              'HAT_Reg':'Hoeffding Adapt Tr',\n",
        "              'ARF_Reg':'Adaptive RF',\n",
        "              'PA_Reg':'Pass Agg Regr',\n",
        "              'RandomForest':'Random Forest',\n",
        "              'GradientBoosting': 'Gradient Boosting',\n",
        "              'DecisionTree': 'Decision Trees',\n",
        "              'LinearSVR': 'Linear SVR',\n",
        "              'BayesianRidge':'Bayesian Ridge'\n",
        "              }\n",
        "\n",
        "metric_type = 'MAPE'\n",
        "exp1_filenames = glob.glob(f'{exp1_path}/*{metric_type}*.csv')\n",
        "exp2_filenames = []\n",
        "\n",
        "for filename in glob.glob(f'{exp2_path}/*{metric_type}*.csv'):\n",
        "  if 'alternate' in filename or 'static' in filename:\n",
        "    exp2_filenames.append(filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFw4A4RoXGhs"
      },
      "source": [
        "read_preprocess_plot_graph(exp1_filenames, col_mapper, metric_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9MYJNUYjvfZ"
      },
      "source": [
        "read_preprocess_plot_graph(exp2_filenames, col_mapper, metric_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYioskYJAIou"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
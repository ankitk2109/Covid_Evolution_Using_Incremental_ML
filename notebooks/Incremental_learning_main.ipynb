{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riu9BVr_-nl2"
   },
   "source": [
    "# Covid Dataset\n",
    "\n",
    "**Required Dataset features and target**\n",
    "\n",
    "The dataset has 53 columns; 1 to represent the country, 1 to represent the day (it will be an integer), 50 floats to represent the positive cases of the 50 previous days, and 1 column to represent the output that is the average of a full week of cases.\n",
    "\n",
    "![required_features.jpg](https://drive.google.com/uc?id=1smUwSHRwMT8h-M8kjG3ymmxdhQbe1HvY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "ZhjnZH15MMQA",
    "outputId": "14dc488c-e363-4f3d-f24b-a150878efaac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-multiflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/b8/dc05e1232cb261429258da43dc6882b4da8debbb485f968f06426e0bf41a/scikit_multiflow-0.5.3-cp37-cp37m-manylinux2010_x86_64.whl (1.1MB)\n",
      "\r",
      "\u001b[K     |▎                               | 10kB 17.0MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20kB 14.2MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 30kB 9.4MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 40kB 7.6MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 51kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 61kB 6.2MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 71kB 6.2MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 81kB 6.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 92kB 6.0MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 102kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 112kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 122kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 133kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 143kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 153kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 163kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 174kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 184kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 194kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 204kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 215kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 225kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 235kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 245kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 256kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 266kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 276kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 286kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 296kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 307kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 317kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 327kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 337kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 348kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 358kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 368kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 378kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 389kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 399kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 409kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 419kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 430kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 440kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 450kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 460kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 471kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 481kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 491kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 501kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 512kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 522kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 532kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 542kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 552kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 563kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 573kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 583kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 593kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 604kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 614kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 624kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 634kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 645kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 655kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 665kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 675kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 686kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 696kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 706kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 716kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 727kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 737kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 747kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 757kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 768kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 778kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 788kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 798kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 808kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 819kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 829kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 839kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 849kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 860kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 870kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 880kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 890kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 901kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 911kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 921kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▋     | 931kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 942kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 952kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 962kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 972kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 983kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 993kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 1.0MB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 1.0MB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 1.0MB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 1.0MB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 1.0MB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.1MB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 1.1MB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 1.1MB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.1MB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 1.1MB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 1.1MB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.1MB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.1MB 5.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from scikit-multiflow) (1.1.5)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from scikit-multiflow) (0.22.2.post1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-multiflow) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from scikit-multiflow) (1.19.5)\n",
      "Requirement already satisfied: sortedcontainers>=1.5.7 in /usr/local/lib/python3.7/dist-packages (from scikit-multiflow) (2.3.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-multiflow) (3.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->scikit-multiflow) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->scikit-multiflow) (2018.9)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->scikit-multiflow) (1.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.3->scikit-multiflow) (1.15.0)\n",
      "Installing collected packages: scikit-multiflow\n",
      "Successfully installed scikit-multiflow-0.5.3\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1f5GgBqjAsTUFnubHODmfqn6qiNyDBqIw\n",
      "To: /content/src.zip\n",
      "100% 26.1k/26.1k [00:00<00:00, 23.9MB/s]\n",
      "Archive:  /content/src.zip\n",
      "  inflating: /content/src/src/_classification_performance_evaluator.py  \n",
      "  inflating: /content/src/src/base_evaluator.py  \n",
      "  inflating: /content/src/src/constants.py  \n",
      "  inflating: /content/src/src/evaluate_prequential.py  \n",
      "  inflating: /content/src/src/evaluation_data_buffer.py  \n",
      "  inflating: /content/src/src/measure_collection.py  \n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n!zip -r /content/file.zip /content/csv_files\\nfrom google.colab import files\\nfiles.download(\"/content/file.zip\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @Andres: Please run [1]\n",
    "# Installing Incremental learner: Scikit-Multiflow\n",
    "!pip install scikit-multiflow\n",
    "\n",
    "\n",
    "# Overdiding some files from scikit multiflow library\n",
    "!gdown https://drive.google.com/uc?id=1f5GgBqjAsTUFnubHODmfqn6qiNyDBqIw\n",
    "!unzip /content/src.zip -d /content/src\n",
    "!cp -r /content/src/src /content/\n",
    "!rm -r /content/src/src\n",
    "\n",
    "# Creating a seperate directory to store all csv's\n",
    "! mkdir -p /content/csv_files\n",
    "! mkdir -p /content/csv_files/processed_null\n",
    "! mkdir -p /content/csv_files/processed\n",
    "\n",
    "! mkdir -p /content/Result/exp1\n",
    "! mkdir -p /content/Result/exp2\n",
    "! mkdir -p /content/Result/exp3\n",
    "\n",
    "! mkdir -p /content/Result/exp1/runtime\n",
    "! mkdir -p /content/Result/exp2/runtime\n",
    "! mkdir -p /content/Result/exp3/runtime\n",
    "\n",
    "! mkdir -p /content/Result/exp1/summary\n",
    "! mkdir -p /content/Result/exp2/summary\n",
    "! mkdir -p /content/Result/exp3/summary\n",
    "\n",
    "! mkdir -p /content/Plots\n",
    "! mkdir -p /content/Plots/barplot\n",
    "! mkdir -p /content/Plots/boxplots\n",
    "\n",
    "! mkdir -p /content/Result/exp1/united_dataframe\n",
    "! mkdir -p /content/Result/exp1/united_dataframe/incremental\n",
    "\n",
    "! mkdir -p /content/Result/exp1/united_dataframe/static\n",
    "! mkdir -p /content/Result/exp2/united_dataframe\n",
    "\n",
    "! mkdir -p /content/Result/exp2/united_dataframe/incremental\n",
    "! mkdir -p /content/Result/exp2/united_dataframe/static\n",
    "\n",
    "! mkdir -p /content/Result/exp3/united_dataframe\n",
    "! mkdir -p /content/Result/exp3/united_dataframe/incremental_alternate\n",
    "\n",
    "# Download the zip file\n",
    "\"\"\"\n",
    "!zip -r /content/file.zip /content/csv_files\n",
    "from google.colab import files\n",
    "files.download(\"/content/file.zip\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYpSAM37_AFE"
   },
   "outputs": [],
   "source": [
    "# @Andres: Please run [2]\n",
    "# For uploading entire results\n",
    "!gdown https://drive.google.com/uc?id=1vIzS2oMkL_UpbV3bFZakC2H2_lfhMEJj\n",
    "!unzip /content/final_content.zip -d /content/final_content\n",
    "!cp -r /content/final_content/final_content/ /content/\n",
    "!rm -r /content/final_content/final_content/\n",
    "\n",
    "import shutil\n",
    "import glob\n",
    "for cur_file in glob.glob('/content/final_content/*'):\n",
    "  shutil.move(cur_file, '/content/')\n",
    "\n",
    "!rm -r /content/final_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cz3gWpvxYPLo",
    "outputId": "5a758018-de5e-4fcb-a036-23b25320117c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open /content/Result.zip, /content/Result.zip.zip or /content/Result.zip.ZIP.\n",
      "cp: cannot stat '/content/Result/content/Result': No such file or directory\n",
      "rm: cannot remove '/content/Result/content/Result': No such file or directory\n",
      "rm: cannot remove '/content/Result/content': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# For Box plot: Run this only if manually uploaded the results\n",
    "\n",
    "!unzip /content/Result.zip -d /content/Result\n",
    "!cp -r /content/Result/content/Result /content/\n",
    "!rm -r /content/Result/content/Result\n",
    "!rm -r /content/Result/content\n",
    "\n",
    "csv_processed_path = '/content/csv_files/processed'\n",
    "csv_processed_with_null_path = '/content/csv_files/processed_null'\n",
    "\n",
    "exp1_path = '/content/Result/exp1'\n",
    "exp2_path = '/content/Result/exp2'\n",
    "exp3_path = '/content/Result/exp3'\n",
    "\n",
    "exp1_runtime_path = '/content/Result/exp1/runtime'\n",
    "exp2_runtime_path = '/content/Result/exp2/runtime'\n",
    "exp3_runtime_path = '/content/Result/exp3/runtime'\n",
    "\n",
    "exp1_summary_path = '/content/Result/exp1/summary'\n",
    "exp2_summary_path = '/content/Result/exp2/summary'\n",
    "exp3_summary_path = '/content/Result/exp3/summary'\n",
    "\n",
    "bar_plot_path = r'/content/Plots/barplot'\n",
    "box_plot_path = r'/content/Plots/boxplots'\n",
    "\n",
    "exp1_static_united_df_path = '/content/Result/exp1/united_dataframe/static'\n",
    "exp1_inc_united_df_path = '/content/Result/exp1/united_dataframe/incremental'\n",
    "\n",
    "exp2_static_united_df_path = '/content/Result/exp2/united_dataframe/static'\n",
    "exp2_inc_united_df_path = '/content/Result/exp2/united_dataframe/incremental'\n",
    "\n",
    "exp3_inc_alt_united_df_path = '/content/Result/exp3/united_dataframe/incremental_alternate'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CqTR_N3fpMON",
    "outputId": "891cd47d-a8f4-4370-d135-edebf1d070b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
      "\u001b[K     |████████████████████████████████| 378kB 6.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.19.5)\n",
      "Collecting keras-applications>=1.0.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 3.9MB/s \n",
      "\u001b[?25hInstalling collected packages: keras-applications, keras\n",
      "  Found existing installation: Keras 2.4.3\n",
      "    Uninstalling Keras-2.4.3:\n",
      "      Successfully uninstalled Keras-2.4.3\n",
      "Successfully installed keras-2.3.1 keras-applications-1.0.8\n",
      "Collecting tensorflow==2.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/56/0dbdae2a3c527a119bec0d5cf441655fe030ce1daa6fa6b9542f7dbd8664/tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8MB)\n",
      "\u001b[K     |████████████████████████████████| 421.8MB 35kB/s \n",
      "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 46.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.12.4)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9MB 43.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.10.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.32.0)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.36.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.19.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow==2.1.0) (54.1.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.27.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.3)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.7.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.4.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
      "Building wheels for collected packages: gast\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=9d1abaa826d08d8039a3f346aa84ccc4a52b1f4df6c13965d49b5caab42d4221\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "Successfully built gast\n",
      "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, gast, tensorflow\n",
      "  Found existing installation: tensorflow-estimator 2.4.0\n",
      "    Uninstalling tensorflow-estimator-2.4.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
      "  Found existing installation: tensorboard 2.4.1\n",
      "    Uninstalling tensorboard-2.4.1:\n",
      "      Successfully uninstalled tensorboard-2.4.1\n",
      "  Found existing installation: gast 0.3.3\n",
      "    Uninstalling gast-0.3.3:\n",
      "      Successfully uninstalled gast-0.3.3\n",
      "  Found existing installation: tensorflow 2.4.1\n",
      "    Uninstalling tensorflow-2.4.1:\n",
      "      Successfully uninstalled tensorflow-2.4.1\n",
      "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall keras\n",
    "#!pip uninstall tensorflow\n",
    "\n",
    "!pip install keras==2.3.1\n",
    "!pip install tensorflow==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvDISgy5CUOC"
   },
   "outputs": [],
   "source": [
    "# @Andres: Please run [3]\n",
    "# General Imports \n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='darkgrid')\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Imports for incremental learner\n",
    "from skmultiflow.data import DataStream\n",
    "from skmultiflow.trees import HoeffdingTreeRegressor\n",
    "from src.evaluate_prequential import EvaluatePrequential\n",
    "from skmultiflow.meta import AdaptiveRandomForestRegressor\n",
    "from skmultiflow.trees import HoeffdingAdaptiveTreeRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "\n",
    "# Imports for static Learner\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import perf_counter as pc_timer\n",
    "from functools import wraps\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "#from tensorflow.keras import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# For significance tests\n",
    "from scipy.stats import normaltest\n",
    "from scipy import stats \n",
    "# pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "wHwu0MJOCm68",
    "outputId": "4eeb7ba6-4a65-4823-b3a2-9bdde97b18db"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dateRep</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>cases</th>\n",
       "      <th>deaths</th>\n",
       "      <th>countriesAndTerritories</th>\n",
       "      <th>geoId</th>\n",
       "      <th>countryterritoryCode</th>\n",
       "      <th>popData2019</th>\n",
       "      <th>continentExp</th>\n",
       "      <th>Cumulative_number_for_14_days_of_COVID-19_cases_per_100000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30/11/2020</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>38041757.000</td>\n",
       "      <td>Asia</td>\n",
       "      <td>6.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29/11/2020</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>2020</td>\n",
       "      <td>228</td>\n",
       "      <td>11</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>38041757.000</td>\n",
       "      <td>Asia</td>\n",
       "      <td>6.845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28/11/2020</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>2020</td>\n",
       "      <td>214</td>\n",
       "      <td>15</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>38041757.000</td>\n",
       "      <td>Asia</td>\n",
       "      <td>6.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27/11/2020</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>38041757.000</td>\n",
       "      <td>Asia</td>\n",
       "      <td>6.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26/11/2020</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>2020</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>38041757.000</td>\n",
       "      <td>Asia</td>\n",
       "      <td>7.342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dateRep  ...  Cumulative_number_for_14_days_of_COVID-19_cases_per_100000\n",
       "0  30/11/2020  ...                                              6.417         \n",
       "1  29/11/2020  ...                                              6.845         \n",
       "2  28/11/2020  ...                                              6.785         \n",
       "3  27/11/2020  ...                                              6.396         \n",
       "4  26/11/2020  ...                                              7.342         \n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://drive.google.com/file/d/1eYy56fHe1XsWgPkBGVc0i6d6A5EU3nxj/view?usp=sharing'\n",
    "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6NQxu_LuoTV"
   },
   "outputs": [],
   "source": [
    "# Grouping countries together for analysis\n",
    "total_countries = df['countriesAndTerritories'].unique()\n",
    "df_grouped = df.groupby('countriesAndTerritories')\n",
    "pretrain_days = [30,60,90,120,150,180,210,240]  # TODO: Make it full list later\n",
    "valid_countries = []\n",
    "decimal = 3  # Specify the scale of decimal places \n",
    "error_metrics = ['MAE','MAPE', 'RMSE']\n",
    "\n",
    "# Setting path variables for both experiments\n",
    "csv_processed_path = '/content/csv_files/processed'\n",
    "csv_processed_with_null_path = '/content/csv_files/processed_null'\n",
    "\n",
    "exp1_path = '/content/Result/exp1'\n",
    "exp2_path = '/content/Result/exp2'\n",
    "exp3_path = '/content/Result/exp3'\n",
    "\n",
    "exp1_runtime_path = '/content/Result/exp1/runtime'\n",
    "exp2_runtime_path = '/content/Result/exp2/runtime'\n",
    "exp3_runtime_path = '/content/Result/exp3/runtime'\n",
    "\n",
    "exp1_summary_path = '/content/Result/exp1/summary'\n",
    "exp2_summary_path = '/content/Result/exp2/summary'\n",
    "exp3_summary_path = '/content/Result/exp3/summary'\n",
    "\n",
    "bar_plot_path = r'/content/Plots/barplot'\n",
    "box_plot_path = r'/content/Plots/boxplots'\n",
    "\n",
    "exp1_static_united_df_path = '/content/Result/exp1/united_dataframe/static'\n",
    "exp1_inc_united_df_path = '/content/Result/exp1/united_dataframe/incremental'\n",
    "\n",
    "exp2_static_united_df_path = '/content/Result/exp2/united_dataframe/static'\n",
    "exp2_inc_united_df_path = '/content/Result/exp2/united_dataframe/incremental'\n",
    "\n",
    "exp3_inc_alt_united_df_path = '/content/Result/exp3/united_dataframe/incremental_alternate'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oubf_WqnATmM"
   },
   "source": [
    "## Feature Set with Individual Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0vj8668gNrP"
   },
   "outputs": [],
   "source": [
    "# Create lags\n",
    "def create_features_with_lags(df):\n",
    "  for i in range(89, 0, -1):  # Loop in reverse order for creating ordered lags eg: cases_t-10, cases_t-9... cases_t-1. t=current cases\n",
    "    df[f'cases_t-{i}'] = df['cases'].shift(i, axis=0)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G23w6M6HdyEb",
    "outputId": "3d33c79f-d49e-423b-985f-bf691e1952b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Pre-Processing dataset and saving them into csv's.\n",
    "for country in total_countries:\n",
    "  df = df_grouped.get_group(country)\n",
    "\n",
    "  # Selecting required features\n",
    "  df= df[['dateRep','cases','countriesAndTerritories']]\n",
    "\n",
    "  # Rename features\n",
    "  df.rename(columns={'countriesAndTerritories':'country', 'dateRep':'date'}, inplace=True)\n",
    "\n",
    "  # Convert to date, sort and set index\n",
    "  df['date'] = pd.to_datetime(df['date'],format='%d/%m/%Y')\n",
    "  df.sort_values('date', inplace=True)\n",
    "  df.set_index('date', inplace=True)\n",
    "\n",
    "  # Adding feature\n",
    "  df['day_no']= pd.Series([i for i in range(1,len(df)+1)], index=df.index)\n",
    "\n",
    "  # Reordering features\n",
    "  df = df[['day_no', 'country','cases']]\n",
    "\n",
    "  # Adding features through lags\n",
    "  df = create_features_with_lags(df)\n",
    "\n",
    "  # Creating target with last 10 days cases\n",
    "  df['target'] = df.iloc[:,[2]+[i*-1 for i in range(1,10)]].mean(axis=1)\n",
    "\n",
    "  # Dropping mid columns\n",
    "  drop_columns = list(df.loc[:,'cases_t-39':'cases_t-1'].columns)  #list(df.loc[:,'cases_t-38':'cases_t-1'].columns)\n",
    "  df.drop(drop_columns, axis=1, inplace=True)\n",
    "\n",
    "  # Country name\n",
    "  filename = df['country'].unique()[0]\n",
    "\n",
    "  # Saving file\n",
    "  df.to_csv(f'{csv_processed_with_null_path}/{filename}.csv')\n",
    "\n",
    "  # Dropping null records\n",
    "  df.dropna(how='any', axis=0, inplace=True)\n",
    "\n",
    "  # Valid countries that have records more than max of pretrain\n",
    "  if len(df)>max(pretrain_days):\n",
    "    valid_countries.append(country)  \n",
    "    df.to_csv(f'{csv_processed_path}/{filename}.csv')\n",
    "  \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMZCoXFwPsHA"
   },
   "source": [
    "## Total cases of top selected countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClOLDLs6480A"
   },
   "outputs": [],
   "source": [
    "# Top countries to select for experiment 1\n",
    "number_of_countries = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDWlSV8OJUQP"
   },
   "outputs": [],
   "source": [
    "# Replaces underscore from country names\n",
    "def format_names(list_countries):\n",
    "  updated_country_list = []\n",
    "  for country_name in list_countries:\n",
    "    updated_country_list.append(country_name.replace(\"_\",\" \"))\n",
    "  return updated_country_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQfWz6LlAyHf"
   },
   "outputs": [],
   "source": [
    "# A dictionary of all countries\n",
    "dict_countries = Counter(valid_countries)\n",
    "\n",
    "for country in valid_countries:\n",
    "  dict_countries[country] = df_grouped.get_group(country)['cases'].sum()\n",
    "\n",
    "# Select top_countries and order(Ascending/Decending) \n",
    "top_countries = sorted(dict_countries.items(), key=lambda dict_countries: dict_countries[1], reverse=True) [0:number_of_countries]\n",
    "\n",
    "# Creating dataframe of top selected countries\n",
    "df_top_countries = pd.DataFrame.from_dict(dict(top_countries), orient='index', columns=['Total Cases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_U3MIv16lLfX",
    "outputId": "6a1ff981-15b6-4b43-9514-c41d44b5c78b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1sAAAR0CAYAAACHYQxoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxN1/7/8VfmwZCIUm4ioXodNBI1xRBFEjULUoSIuobWEN+LTsrtbUvVUEMrhhqqVAwliSHaaiOCS4m6bpsOaqjhCq0pSAjNcPL7wy/nOk4iEYnQvp+Ph8dD1l577c/eSTzOx1rrs61yc3NzERERERERkRJlXdYBiIiIiIiI/BEp2RIRERERESkFSrZERERERERKgZItERERERGRUqBkS0REREREpBTYlnUAIvLwunnzJj/88ANVqlTBxsamrMMREREReWBycnK4cOEC3t7eODo6FmsMJVsiUqAffviBsLCwsg5DREREpMysWrWKJk2aFOtcJVsiUqAqVaoA4NFiOHZOrmUcjYiIiMgtSye2L/Vr/Pbbb4SFhZk+DxXHHz7ZGj9+PBs2bODw4cMP9LoGg4GePXsybdq0B3rdP7J9+/Yxe/Zsjh49SkZGBlOnTqVXr15lHVapeRh+hvKWDto5uWLn7FZmcYiIiIjczsPD44Fd6362UpRpgYzY2FgMBgOxsbH5Hk9JScFgMDB+/PgSve62bduIjIws0THvx+nTp3njjTfo2LEjvr6+NG3alE6dOvHaa6+xb98+s76RkZFs27btvq+5fPnyAp/7w+jq1auMHj2aGzduMH78eGbMmEHTpk2LdO7OnTsxGAzUq1ePs2fPlnKkIiIiIiK3/OFntiZPnszbb79t1rZt2zY2bNjA6NGjyyiq//n+++8JDw/H1taWHj168OSTT3Lz5k1OnTrFnj17KFeuHM2bNzf1nzdvHj179iQoKOi+rvvJJ5/g7u7+yMwMff/996SlpTFlyhSeffbZezo3JiaG6tWrc/HiRWJjY4mIiCilKEtWcnIy1tYqGCoiIiLyqPrDJ1t2dnZlHcJdzZ8/nxs3brBp0ybq1q1rcfzChQtlENXD5+LFiwC4uLjc03mpqals376dESNGcOjQIWJjYxk1ahRWVlalEeZ9u3nzJra2ttja2uLg4FDW4YiIiIjIfXjk/ts8b2lhZGQkiYmJhISE0KBBA/z9/Zk+fTrZ2dlm/cePH4/BYDB9HR4ezoYNG4Bbe2Ly/ty+pO78+fO8+eabtG3bFm9vb/z9/XnjjTe4dOmSRTxHjx5lyJAhNGzYkGbNmvHSSy/l268gJ0+exNXVNd9EC/5XoCDvvgE2bNhgFnuezz//nOHDh5vi9vPzY+TIkfz8889mYxoMBs6cOcP+/fvNxklJSTH1+f777xk1ahR+fn54e3vToUMHFi5caPF8jx49yv/93//RunVrvL29adWqFeHh4ezYsaNI9//zzz+brtOgQQM6d+7MkiVLyMnJMfUJCAjgtddeA2DgwIEW9303mzZtIjs7m+DgYHr27MmZM2fYu3evRb+kpCTTz8GqVavo0KEDDRo0oFu3biQmJgJw+PBhhgwZQqNGjfDz8+Odd94hKyvLYqyTJ0/yyiuv4O/vj7e3NwEBAUyfPp2MjAyzfnk/m6mpqbz++uu0bNmShg0b8ttvvwEUuIR23759vPDCC6ZnFhgYyIQJE0hNTTX1WbVqFYMHDzZ9X/z9/Xn55ZfNvsciIiIiUroe2ZmtnTt3snr1akJDQwkJCSEhIYFly5bh4uLC8OHDCzxv+PDhGI1GDhw4wIwZM0ztjRo1AuDs2bP07duXrKwsnnvuOTw9PTl16hRr1qwhKSmJmJgYKlSoANzaaxUWFkZmZiZhYWFUr16dxMREhg4dWuT78PT05MSJE3z11Vd3XR7n5ubGjBkzePXVV2nSpAl9+vSx6BMVFYWrqyt9+vShSpUq/Pe//2XdunX069ePDRs2ULNmTQBmzJjB1KlTqVSpktmzcnO7VQBhx44dRERE4OXlxeDBg3FxceHbb79l7ty5HDp0iLlz5wJw+fJlnn/+eQBCQ0P5y1/+wuXLl/nhhx/47rvvaNu27V3v/fYllGFhYTz22GMkJiYyc+ZMfv75Z2bNmgXAhAkT2LVrF59++inDhw/niSeeKPLzjYmJoWnTpnh4eFCtWjUqV65MTEwMLVu2zLf/qlWrSEtLo3fv3tjb27Ny5UoiIiL44IMP+Mc//kHXrl0JCgpiz549rFy5Ejc3N0aOHGk6/4cffuD555+nYsWK9O3bl8cff5yff/6ZlStX8p///IeVK1dazLb+7W9/47HHHmPkyJFkZGTg7Oxc4P2sXbuWt956i8cff5zQ0FDc3d05e/YsiYmJnDt3zvQ9XLZsGQ0bNiQ8PBxXV1eOHDlCdHQ0+/btIy4ujkqVKhX5GYqIiIhI8TyyydaxY8fYsmWLqRJJv3796NatG1FRUXdNtlq1akVcXBwHDhwgODjY4vjkyZPJzs5m48aNVKtWzdTesWNH+vbty/Lly017vd5//32uXr3KihUrTPuqwsLCiIiI4KeffirSfYwYMYKvv/6a0aNHU7NmTRo1akSDBg3w8/Ojdu3apn7Ozs4EBwfz6quvUqNGjXxjX7p0qcUH9R49ehAcHMzy5ct56623AAgODuaDDz7gsccesxjn999/Z+LEifj6+rJixQpsbW/9iISGhlK3bl2mTp1KUlISfn5+HDx4kEuXLjFnzhw6d+5cpPu93ZQpU8jMzGTt2rWmmb0BAwYwZswYtmzZwnPPPUeLFi0ICgoiLS2NTz/9lJYtW+Ln51ek8b/77juOHj3K1KlTAbC1taVr166sXbuWq1ev5rsk8fz583z++eemhLp58+YEBwcTERHB3LlzTQlxv3796NWrF6tXrzZLtiZMmECVKlWIjo6mfPnypvYWLVoQERFBXFycxT65v/71r8ycObPQ+/ntt9945513eOKJJ1i7di0VK1Y0HRszZgxGo9H0dVxcnMXPQmBgIIMGDSI6Opphw4YVej0RERERuT+P3DLCPIGBgWYlH62srPDz8+PChQtcv369WGOmp6ezY8cOAgICsLe3JzU11fTH3d0dT09P9uzZA4DRaGT79u14e3ubFbCwsrK6p5mtp59+mpiYGHr27El6ejqxsbG8/fbbdO7cmbCwME6fPl3ksfI+XOfm5nLt2jVSU1OpVKkStWrVIjk5uUhj7Nmzh4sXL9KrVy/S0tLMnsEzzzxj6gOYEpJ//etfXLt2rchxAly6dIn//Oc/BAQEmC2htLKyYsSIEQDEx8ff05h3io6OxtnZmQ4dOpjaevXqxe+//86WLVvyPadXr16m+wKoW7cu5cuXp2rVqhYzj40aNTL7eTt8+DCHDx+ma9euZGZmmj27xo0b4+zsbHp2txsyZEiR7mfr1q1kZWURERFhlmjlub2YRt7PgtFoJD09ndTUVAwGAxUqVCjyz4KIiIiI3J9HYmYrv2IGNWrUsGhzdb310tUrV65Qrly5e77OiRMnMBqNREdHEx0dnW+fvOteunSJjIyMfJe0Pfnkk/d0XYPBYHqX0pkzZ/jmm29Yv349Bw4cYOTIkcTExGBvb1/oOD/99BMffPAB+/fvt9gfVNR3Efzyyy/ArRmaguQVq2jWrBk9evQgNjaWuLg4vL29admyJZ07dy70GeTtHcqv3xNPPIG1tfU9JZp3ysjI4LPPPqNZs2ZcvHjRFLOTkxNeXl5ER0cTFhZmcV5+z8nFxcVslvP2dvjfz1ves4uMjCzw1QJ5cdwub3lnYU6ePAlAvXr1Cu27d+9eFixYwHfffcfvv/9uduzq1atFup6IiIiI3J8yTbYcHR0BuHHjRr7H89rzq8p2t5eL5ebmFiuevPO6d+9Oz5498+1T2hXi3N3dcXd3Jzg4mP79+3Pw4EGSk5Np0qTJXc87e/YsYWFhlC9fnhEjRvDEE0/g5OSElZUV7777rkXyVZC8Z/Dqq68W+KG+atWqpr9Pnz6dIUOGsGvXLg4cOMDHH3/Mhx9+yIQJExgwYEAR77rkbd26levXr7Njx44Ci3UcOnTI4h4L+rm6l5+3vMIU+clvRsrJyanAsYsjOTmZIUOG4OnpyUsvvYSHhweOjo5YWVkxduzYYv9+iIiIiMi9KdNkK28W4fjx4/kez5spKOk3RBdU9tvT0xMrKyuysrIKLKCQx83NDWdn53xjP3bsWInE6Ovry8GDBzl//nyh/ePj48nIyGDhwoVmyxrh1sxLUWbG4H+zLE5OToU+gzx16tShTp06DB061FRcYtasWYSFhRX4rPO+p/k9q+PHj2M0GvOdvSyqmJgYqlatysSJEy2OZWVl8dprrxEdHc0bb7xR7GvcycvLC7i1nK+oz+5e5H1vDh06RK1atQrst2XLFnJycliyZInZM8zIyCAtLa3E4xIRERGR/JXpnq369etTvXp1PvvsM86dO2d2LDMzk1WrVmFlZUVAQECJXjdvP8uVK1fM2itVqkSbNm2Ij4/n22+/tTgvNzfXVF7bxsaGdu3a8cMPP7Bv3z6zPkuXLi1yLHv27LEopw633reUt7/nzkIZd8adF0/e9W+3bt26fN/VVa5cuXzH8ff3p3LlyixZsiTf4zdv3jTtz7py5YpZUQa4NXPj4eHBjRs3LJav3a5y5co8/fTTJCYmcuTIEVN7bm4uixcvBqB9+/YFnn83J06c4MCBA3To0IGOHTta/OnWrRuNGzdmy5YtZGZmFusa+alfvz516tRh7dq1+S6BzM7OzveZFlXHjh2xs7Nj/vz5+e6Ry/veFzQLt2jRIovvl4iIiIiUnjKd2bK1teWtt94iIiKC7t27m0qtX7x4kS+++IKjR4/ec6nvovD19SUqKoq3336bNm3aYGdnh4+PDzVq1OCtt96if//+DBgwgODgYOrXr4/RaOT06dMkJCTQo0cPUzXCMWPGsGvXLoYPH86AAQOoVq0aiYmJZu87KszUqVO5cuUKAQEB1KlTB0dHR3777Tfi4uI4efIkPXr0MHunVMOGDdm7dy+LFy/mL3/5C1ZWVnTp0oVnnnkGJycnXn31VQYMGEDFihU5ePAgu3btwtPT0+y9VXnPIDo6mvfff5/atWtjbW1Nu3btcHZ2Zvr06YwaNYqOHTsSEhKCl5cXaWlpHD9+nPj4eObNm4efnx8bN25kxYoVBAUF4eXlha2tLd988w27d++mU6dOpmWiBZk4cSLh4eGEhYXRv39/qlSpQmJiIrt376Zr1660aNHiHr6r/xMTEwNw11L6HTp0YP/+/cTHx9OlS5diXedOVlZWzJgxg+eff57u3bsTEhLCk08+yc2bNzl16hTx8fGMGzfOohphUVWrVo0JEyYwadIkunXrRnBwMO7u7pw7d46EhATeffdd6tWrR1BQEMuXL2fYsGH07dsXOzs79uzZw+HDh1XyXUREROQBKvMCGW3btmX16tUsXbqUjRs3cuXKFZycnKhXr16xS4oXpmvXrhw6dIjPPvuMrVu3YjQamTp1KjVq1KB69erExMSwZMkStm/fzubNm3FwcKB69eq0a9eOTp06mcbx9PRk1apVTJ8+naioKOzt7WndujUzZswo8jKy8ePHk5CQwL///W++/PJL0tPTqVChAnXq1GHYsGEWH8zffPNNJk2axIcffmiqgtelSxc8PT1ZsmQJs2fP5sMPP8TGxoZGjRqxcuVKJk+ezJkzZ8zGGTt2LFevXmX16tWkpaWRm5tLQkICzs7OtG7dmujoaBYvXszmzZu5fPkyFStWxNPTk0GDBpmSPz8/Pw4dOsSOHTu4cOEC1tbWeHh48NprrxVpv1aDBg1Yu3Ytc+fOZc2aNWRkZFCjRg1efvllBg8eXKTnd6ecnBw2btyIm5vbXfe5tW/fnnfeeYeYmJgSS7bgVvGKDRs2sGjRIrZv387atWspV64c7u7u9OzZs9gJZJ7+/fvj6enJRx99xMqVK8nMzKRq1aq0aNHCVMSjcePGREZGsmDBAj744AMcHBxo2bIlUVFRZbqPTkREROTPxipXu+VFpAApKSkEBgZSK2A8ds5uZR2OiIiICABxsyzfOVvS8j4HJSQkFLuGRJnPbInIw2/pxPYlXqhGREREpLgys3Kwtyu4WvTD4pF9qbGIiIiIiPw5PQqJFijZEhERERERKRVKtkREREREREqBki0RERERESl1mVk5hXf6g1GBDBEp1NAp8apGKCIiIvflQVQQfNhoZkvkEZGUlITBYCA2NvaubSIiIiLycFCyJVLC8hKgjz76qKxDEREREZEypGWEIo+wpk2bkpycjK2tfpVFREREHjb6hCbyCLO2tsbBwaGswxARERGRfGgZoUgpS0lJwWAwEBkZSWJiIiEhITRo0AB/f3+mT59Odna2xTnbtm2jR48eNGjQgDZt2vD+++/n2y+/PVtGo5GFCxcSFhZGq1at8Pb2pm3btrz55ptcvny5VO9VRERERP5HM1siD8jOnTtZvXo1oaGhhISEkJCQwLJly3BxcWH48OGmfvHx8YwePRp3d3dGjRqFjY0NsbGx7Ny5s0jXycrK4qOPPuLZZ58lMDAQJycnvv/+e2JiYjh48CAxMTHY29uX1m2KiIiIyP+nZEvkATl27BhbtmzBw8MDgH79+tGtWzeioqJMyVZOTg5TpkzBxcWF9evX4+Z2q9x6aGgo3bt3L9J17O3t2b17N46Ojqa2fv368fTTT/OPf/yDbdu20blz5xK+OxERERG5k5YRijwggYGBpkQLwMrKCj8/Py5cuMD169cB+PHHH/n111/p1auXKdECqFChAqGhoUW6jpWVlSnRysnJIS0tjdTUVJo3bw5AcnJySd2SiIiIiNyFZrZEHpAaNWpYtLm6ugJw5coVypUrx+nTpwF44oknLPrWrl27yNf6/PPP+fjjjzl06BBZWVlmx65evXovYYuIiIhIMSnZEnlAbGxsCjyWm5tbYtf56quvGDt2LD4+PkyYMIHq1avj4OBATk4OQ4cOLdFriYiIiEjBlGyJPETyZr+OHz9uceyXX34p0hibNm3CwcGBTz75BCcnp3s+X0RERERKhvZsiTxEnnrqKapVq0ZsbCypqamm9mvXrrF27doijWFjY4OVlRVGo9HUlpuby8KFC0s8XhEREREpmGa2RB4iNjY2vP7664wZM4bevXvTp08fbGxsiImJwdXVlbNnzxY6RocOHfjyyy95/vnn6dGjB9nZ2Wzbto0bN248gDsQERERkTya2RJ5yHTs2JG5c+dSvnx5IiMjWblyJR06dODll18u0vldunRh8uTJZGRkMH36dJYuXUqtWrX46KOPSjlyEREREbmdVa52y4tIAVJSUggMDKRWwHjsnN0KP0FERESkAHGzgss6hHuS9zkoISHB7PU990IzWyIiIiIiIqVAe7ZEpFBLJ7Yv9v/oiIiIiABkZuVgb1fwq3D+iDSzJSIiIiIipe7PlmiBki0REREREZFSoWRLRERERESkFCjZEhERERERM5lZOWUdwh+CCmSISKGGTolX6XcREZE/kUetTPvDSjNbIn9QBoOB8ePHm7UFBAQQHh5eRhGJiIiI/LloZkukmJKSkhg4cKBZm729PVWrVqVZs2YMHTqU2rVrl1F0IiIiIlLWlGyJ3KeuXbvyzDPPAPD7779z+PBh1q9fz5dffklcXBzu7u5lEldycjLW1pq8FhERESkrSrZE7lP9+vUJDjZf1+zl5cWUKVOIj49n0KBBBZ577do1ypcvXypxOTg4lMq4IiIiIlI0SrZESkHVqlUBsLOzAyAlJYXAwEAiIiKoXbs2S5cu5dixY3Tu3Jlp06bxyy+/sHLlSr755hvOnj2L0Wikdu3a9OvXj969e5vGzRunIBEREYwePRq4tWerZ8+eTJs2rRTvVEREREQKomRL5D7duHGD1NRU4NYywiNHjjBnzhwqVarEs88+a9Z327ZtrFy5kn79+hEaGmqa1dq/fz8HDhygbdu2eHh4cOPGDbZu3co//vEPUlNTefHFFwFwc3NjxowZFjFs2LCBvXv3Urly5VK+WxEREREpKiVbIvcpMjKSyMhIs7Ynn3ySVatWUaVKFbP2Y8eOsXnzZovCGcHBwfTr18+sbdCgQTz//PMsXryYwYMHY2dnh7Ozs8WSxcTERJKSkmjfvr3FGCIiIiJSdpRsidynvn370rFjR+DWzNaxY8f4+OOPeeGFF/jkk0/MCmS0adMm3wqFzs7Opr///vvvZGRkkJubS6tWrdi/fz/Hjx/HYDBYnHfo0CHGjRtHvXr1eO+997CysiqFOxQRERGR4lCyJXKfvLy8aNmypenrdu3a0axZM/r06cPMmTOZM2eO6VjNmjXzHeP69evMmzePL774gl9//dXieFpamkXbuXPnePHFF6lYsSIffvghTk5O938zIiIiIlJilGyJlAJfX18qVKjAvn37zNoLSoheeuklduzYQZ8+fWjatCmurq7Y2Niwc+dOli9fjtFoNOufkZHB8OHDSU9PZ82aNaaCHCIiIiLy8FCyJVJKcnJyyMzMLLRfWloaO3bsIDg4mEmTJpkd+/rrry36G41Gxo0bx88//8yCBQuoW7duicUsIiIiIiVHbzwVKQV79uwhIyODp556qtC+eS8ezs3NNWs/f/4869evt+g/depUEhMTee2112jXrl3JBCwiIiIiJU4zWyL36aeffmLTpk0AZGZmcuzYMdatW4ednR1jxowp9Pzy5cvTqlUrNm/ejKOjIw0aNODMmTN8+umneHh4cOXKFVPfnTt38sknn/Dkk09SqVIl03XzGAwGzXSJiIiIPCSUbIncpy1btrBlyxbg1iyVq6srrVq14oUXXsDHx6dIY7z33nvMmjWL7du3s2HDBmrWrMnYsWOxtbXl9ddfN/W7dOkScKuE/KuvvmoxTkREhJItERERkYeEVe6da5dERP6/lJQUAgMDqRUwHjtnt7IOR0RERB6QuFnBhXf6g8v7HJSQkICHh0exxtDMlogUaunE9sX+R0ZEREQePZlZOdjb2ZR1GI88FcgQEREREREzSrRKhpItERERERGRUqBkS0REREREpBQo2RIRERERESkFSrZERERERIohMyunrEOQh5yqEYpIoYZOiVfpdxERkTuoPLoURjNbIo+w2NhYDAYDSUlJZR2KiIiIiNxBM1siBUhKSmLgwIFmbc7OztSsWZPg4GAGDBiAra1+hUREREQkf/qkKFKIrl278swzz5Cbm8vFixfZtGkTU6dO5ZdffmHy5MllGltwcDBdunTBzs6uTOMQEREREUtKtkQKUb9+fYKD/7cmu3///nTq1In169czduxY3NzKbi+TjY0NNjZ66aCIiIjIw0h7tkTukbOzM76+vuTm5vLf//4XgPDwcAICAiz6pqSkYDAYiIyMNLUZjUaWL19Ot27dePrpp2nUqBEdOnRgwoQJZGVlmfodPHiQoUOH0qpVKxo0aEDr1q0ZNmwY3377ralPfnu2rl27xpw5c+jduzd+fn54e3vTvn17Zs6cyY0bN0rjkYiIiIhIPjSzJVIMp0+fBsDFxeWez124cCFz586lXbt2hIaGYmNjQ0pKCtu3byczMxM7OzuOHz/O4MGDeeyxxxg4cCCVK1fm0qVL/Pvf/+bnn3+mYcOGBY5/7tw5oqOjefbZZ+natSu2trbs37+fpUuXcujQIT766KNi37eIiIiIFJ2SLZFC3Lhxg9TUVAAuXLjA2rVr+emnn/Dx8aFWrVr3PN62bduoXbs2H374oVn7yy+/bPr77t27uXHjBrNnz8bHx+eexq9RowY7duww28cVFhbG+++/z8KFC0lOTr7nMUVERETk3mkZoUghIiMjadGiBS1atKB79+6sXr2aZ599lgULFhRrvPLly3Pu3DkOHDhQYJ8KFSoAkJCQwO+//35P49vb25sSrezsbK5evUpqaiotW7YE4LvvvitW3CIiIiJybzSzJVKIvn370rFjR7Kysjhy5AhLly7lt99+w8HBoVjjjRs3jlGjRhEWFkbVqlVp1qwZbdu2pUOHDtjb2wPQpUsXNm/ezIcffsjy5cvx9fXF39+fLl264O7uXug1Vq1axdq1azl27BhGo9Hs2NWrV4sVt4iIiIjcGyVbIoXw8vIyzQq1adOGxo0b079/f958803mzJlz13NzcnIs2p5++mni4+PZvXs3SUlJJCUlsWXLFhYuXMjq1atxdXXF3t6ejz/+mOTkZP71r39x4MAB5s6dy7x585g1axbt27cv8Joff/wx06ZNw9/fn4EDB1K1alXs7Ow4d+4c48ePJzc39/4eiIiIiIgUiZItkXvUqFEjgoOD2bhxI+Hh4TRq1AhXV1d+/PFHi755hTTuVK5cOTp06ECHDh2AWzNRkyZNIjo6mqFDh5r6+fj4mPZX/frrr/To0YP333//rsnWpk2bcHd3Z8mSJVhb/2+l8K5du4p1vyIiIiJSPNqzJVIMI0eOxMbGhrlz5wJQs2ZNrl+/TnJysqlPXon3O+UV27jdU089BfxviV9+fapVq4abm1uhywCtra2xsrIym8HKzs5myZIlhd+YiIiIiJQYzWyJFIOXlxedO3cmLi6OAwcO0KdPHz7++GNGjRrFwIEDsbOz48svv8x3GWHnzp1p2LAhPj4+VK1alQsXLrBu3Trs7Ozo0qULcKs8/J49e2jbti0eHh7k5uaSmJjI8ePHzWa+8tOxY0dmzZrFsGHDaN++PdeuXWPLli3Y2urXXURERORB0qcvkWIaMWIEn332GR988AErV65k/vz5zJ49mw8++ABXV1eCg4MJCQmhU6dOZucNHjyYnTt3snLlStLT06lcuTK+vr68+OKL1K1bF4CgoCAuXLjA1q1buXjxIo6Ojnh5efHOO+/w3HPP3TWuIUOGkJubS3R0NFOmTKFKlSp06tSJkJAQOnfuXGrPQ0RERETMWeVqt7yIFCAlJYXAwEBqBYzHztmtrMMRERF5qMTNCi7rEKQU5X0OSkhIwMPDo1hjaGZLRAq1dGL7Yv8jIyIi8keVmZWDvZ1NWYchDzEVyBARERERKQYlWlIYJVsiIiIiIiKlQIGiYQMAACAASURBVMmWiIiIiIhIKVCyJSIiIiJyjzKzLF/vInInFcgQkUINnRKvaoQiIiK3USVCKQrNbImIiIiIiJQCzWyJ3CEpKYmBAwcWePzTTz+lYcOGDzAiEREREXkUKdkSKUDXrl155plnLNo9PT3LIBoRERERedQo2RIpQP369QkOLtp67JycHDIzM3FycirlqERERETkUaE9WyL3KDY2FoPBwNdff838+fMJCgrCx8eHL774AoDdu3czZswYAgMD8fHxoUmTJgwePJj9+/dbjBUeHk5AQADnzp1j3LhxNG3aFF9fX4YMGcKJEycs+mdmZrJkyRKCg4Px9fWlcePG9OrVi6ioKLN+6enpvPfee7Rv3x5vb2+aN2/OuHHjOH36dOk8FBERERGxoJktkQLcuHGD1NRUszZ7e3vT36dPn052djZ9+vShXLly1KpVC4ANGzZw9epVevToQbVq1Th37hzr169n0KBBfPLJJzRp0sRszIyMDAYMGICvry9jx44lJSWFTz75hJEjR7JlyxZsbG69nT4zM5MhQ4awf/9+/P396d69Ow4ODhw5coSvvvqKAQMGALcSrdDQUM6ePUtISAh//etfuXDhAqtXr6Z3797ExMTg7u5emo9ORERERFCyJVKgyMhIIiMjzdo6d+5M69atAbh58yYbN260WDo4efJknJ2dzdpCQ0Pp0qULixYtski2Ll++zJAhQxg2bJipzc3Njffee4+vv/7adL0VK1awf/9+XnzxRcaNG2c2htFoNP39gw8+4PTp06xbt466deua2nv27Em3bt2IjIxk2rRp9/o4REREROQeKdkSKUDfvn3p2LGjWdtjjz3GDz/8AEC/fv3y3aN1e6J1/fp1MjMzsba2xtfXl++++86iv7W1tUX1w+bNmwNw6tQpU7IVFxeHi4sLo0aNyncMgNzcXOLi4mjatClVq1Y1m5lzcnKiYcOG7N69u0j3LyIiIiL3R8mWSAG8vLxo2bKlRXtespW3bPBO//3vf5kzZw67d+8mLS3N7JiVlZVF/6pVq+Lg4GDW5urqCsCVK1dMbadOnaJevXoWfW+XmprKlStX2L17Ny1atMi3T15iJiIiIiKlS8mWSDE5OjpatF2/fp2wsDBu3LjB888/T506dShXrhzW1tYsWrSIffv2WZyTtycrP7m5ufcUU17/li1bmi1LFBEREZEHT8mWSAnau3cv58+f59133yUkJMTs2Pvvv39fY9esWZPjx4+TmZlpVqjjdm5ublSsWJFr167lOysnIiIiIg+O1hOJlKC8Wao7Z6R2796d736te9GtWzeuXr3KggULLI7lXc/a2ppu3bqRnJzM1q1b8x3n0qVL9xWHiIiIiBSNZrZESlDjxo2pUqUK06dP58yZM1SrVo1Dhw6xadMm6tSpw5EjR4o99sCBA0lMTGThwoV8//33+Pv7Y29vz7Fjxzhx4gTLly8HYOzYsRw8eJAxY8bQqVMnfH19sbOz4+zZs+zatYunnnpK1QhFREREHgAlWyIlqGLFiixdupT33nuPqKgosrOz8fb2ZsmSJURHR99XsmVvb8+yZctYtmwZW7ZsYfbs2Tg4OODl5UWvXr1M/SpUqMCaNWtYtmwZW7duJSEhARsbG6pVq0bjxo3p3bt3SdyqiIiIiBTCKvded+CLyJ9GSkoKgYGB1AoYj52zW1mHIyIi8tCImxVc1iFIKcv7HJSQkICHh0exxtDMlogUaunE9sX+R0ZEROSPKDMrB3u7gisKi4AKZIiIiIiI3DMlWlIUSrZERERERERKgZItERERERGRUqBkS0REREREpBQo2RIRERGRP53MrJyyDkH+BFSNUEQKNXRKvEq/i4jIH4pKt8uDoJktkUdUUlISBoOB2NjYsg5FRERERPKhmS2RYjp9+jSLFy/mm2++4ddff8Xe3p7HHnsMHx8fevbsSfPmzcs6RBEREREpQ0q2RIrh+++/Jzw8HFtbW3r06MGTTz7JzZs3OXXqFHv27KFcuXKlnmw1bdqU5ORkbG31aywiIiLyMNKnNJFimD9/Pjdu3GDTpk3UrVvX4viFCxdKPQZra2scHBxK/ToiIiIiUjzasyVSDCdPnsTV1TXfRAugSpUqpr8bDAbGjx/P119/TZ8+ffD19aVVq1a88847XL9+3ey8c+fOMW3aNIKDg2natCkNGjSgc+fOLF68mJwc86pJ+e3Zur0tJiaGLl264O3tTbt27ViyZEkJPgERERERKYxmtkSKwdPTkxMnTvDVV1/x7LPPFtr/xx9/5Msvv6R3794EBweTlJTEypUrOXr0KB9//DHW1rf+3+Pw4cN89dVXtG/fHk9PT7KysvjXv/7FrFmzSElJYdKkSUWKb+3atVy8eJHnnnuOihUrsnnzZmbOnEm1atXo1q3bfd27iIiIiBSNki2RYhgxYgRff/01o0ePpmbNmjRq1IgGDRrg5+dH7dq1LfofOXKE+fPnExQUBEBYWBjvvPMOK1eu5IsvvqBLly4ANGvWjISEBKysrEznDho0iFdeeYX169cTERFB1apVC43v7NmzfPHFF1SoUAGAkJAQ2rVrR1RUlJItERERkQdEywhFiuHpp58mJiaGnj17kp6eTmxsLG+//TadO3cmLCyM06dPm/WvVauWKdHK88ILLwAQHx9vanN0dDQlWpmZmVy5coXU1FT8/f0xGo388MMPRYovJCTElGgBODk50bBhQ06ePFmc2xURERGRYtDMlkgxGQwGpk2bBsCZM2f45ptvWL9+PQcOHGDkyJHExMRgb28PkO9sV9WqValYsaJZYpadnc3ixYvZtGkTp06dIjc31+yctLS0IsXm4eFh0ebq6sqVK1eKfH8iIiIicn+UbImUAHd3d9zd3QkODqZ///4cPHiQ5ORkmjRpck/jTJs2jZUrV9K5c2eGDx+Om5sbdnZ2/Pjjj8ycOROj0VikcWxsbIpzGyIiIiJSgpRsiZQgKysrfH19OXjwIOfPnze1//LLLxZ9z58/T1paGjVq1DC1bdq0iaZNmzJnzhyzvqdOnSq9oEVERESkVGjPlkgx7Nmzh+zsbIv2mzdvsmfPHsB86eCJEyfYtm2bWd+8Uuy37+Wytra2WDqYkZHB8uXLSyp0EREREXlANLMlUgxTp07lypUrBAQEUKdOHRwdHfntt9+Ii4vj5MmT9OjRA4PBYOpfp04dXnnlFXr37o2XlxdJSUl8+eWXNGvWjM6dO5v6dejQgU8//ZQxY8bQsmVLLl68SExMDK6urmVxmyIiIiJyH5RsiRTD+PHjSUhI4N///jdffvkl6enpVKhQgTp16jBs2DB69epl1v+pp57i9ddfZ86cOaxdu5by5cszYMAAxo4da3rHFsDrr79OuXLl2Lp1KwkJCVSvXp2+ffvSoEEDBg0a9IDvUkRERETuh1XunWuWRKREGQwGevbsaapc+ChJSUkhMDCQWgHjsXN2K+twRERESkzcrOCyDkEecnmfgxISEvKt9FwUmtkSkUItndi+2P/IiIiIPIwys3Kwt1P1XildKpAhIiIiIn86SrTkQVCyJSIiIiIiUgq0jFCklB0+fLisQxARERGRMqCZLRERERH5U8nMyinrEORPQjNbIlKooVPiVY1QRET+MFSJUB4UzWxJocaPH2/2gt4HxWAwMH78+BIfNzIyEoPBQEpKSomPfS+SkpIwGAzExsaWaRwiIiIiUjqUbD3iYmNj7/qBPSUlpVSSlm3bthEZGVmiYxZXXtLy0UcfWRzbv38/jRs3xt/fn59//rkMohMRERGRPyslW1KoyZMnk5ycbNa2bds25s2bV0YRFU1iYiJDhw7FxcWF1atXU7duXQBGjBhBcnIy7u7uZRyhiIiIiPyRKdmSQtnZ2eHg4FDWYdyTuLg4IiIi8PT0ZM2aNXh6epqO2dra4uDggJWVVRlGKCIiIiJ/dEq2/oTylhZGRkaSmJhISEgIDRo0wN/fn+nTp5OdnW3W/849W+Hh4WzYsAG4ta8q78/tSxnPnz/Pm2++Sdu2bfH29sbf35833niDS5cuWcRz9OhRhgwZQsOGDWnWrBkvvfRSvv2KavXq1bzyyivUr1+fqKgoHn/8cbPj+e3Zyms7fvw4s2fP5plnnsHb25vu3buzc+dOi2vcuHGDqVOn4u/vj4+PD3369GHv3r0F7m/btm0bPXr0oEGDBrRp04b333/f4jnnSU1N5e2336ZNmzZ4e3vTpk0b3n77bS5fvmzWL28J6d69e5k3bx7t2rXDx8eH3r178+233wK3llH269ePhg0b4u/vz/z58+/5eYqIiIhI8aga4Z/Yzp07Wb16NaGhoYSEhJCQkMCyZctwcXFh+PDhBZ43fPhwjEYjBw4cYMaMGab2Ro0aAXD27Fn69u1LVlYWzz33HJ6enpw6dYo1a9aQlJRETEwMFSpUAOD06dOEhYWRmZlJWFgY1atXNy3/K45FixYxe/ZsmjdvzoIFCyhXrtw9nT9+/HhsbW0ZPHgwWVlZrFixglGjRrF161Y8PDxM/f7+97+zc+dOgoKCaNmyJSkpKYwaNcqsT574+HhGjx6Nu7s7o0aNwsbGhtjY2HyTuPT0dPr168epU6cICQmhfv36HDp0iDVr1rBv3z7Wr19P+fLlzc6ZOXMmRqORgQMHkpWVxbJlyxg8eDAzZsxg4sSJ9OnTh27duvHFF18wd+5cPDw8CA5WFSYRERGR0qZk60/s2LFjbNmyxZQg9OvXj27duhEVFXXXZKtVq1bExcVx4MCBfD+0T548mezsbDZu3Ei1atVM7R07dqRv374sX76c0aNHA/D+++9z9epVVqxYQfPmzQEICwsjIiKCn3766Z7uZ82aNZw+fZqgoCDmzJmDvb39PZ0PUKlSJT788EPTEkM/Pz969+7Np59+yksvvQTcSlJ37txJ7969eeedd0znNm/enBdeeMFsvJycHKZMmYKLiwvr16/Hze1W+fTQ0FC6d+9ucf2lS5dy8uRJ/vnPfxIWFmZqr1evHpMmTWLp0qWMGTPG7Byj0cinn35qut/atWszcuRI/v73v7N27VoaNGgAwHPPPUdAQACrV69WsiUiIiLyAGgZ4Z9YYGCg2UyMlZUVfn5+XLhwgevXrxdrzPT0dHbs2EFAQAD29vakpqaa/ri7u+Pp6cmePXuAW0nC9u3b8fb2NiVaeXEUZ2brwoULAHh6ehYr0QIYOHCg2V4uHx8fnJ2dOXXqlKlt+/btAPztb38zO7dNmzbUrl3brO3HH3/k119/pVevXqZEC6BChQqEhoZaXD8+Ph43Nzf69u1r1t63b1/c3NzYtm2bxTn9+vUzu98mTZqYYs9LtADs7e1p0KABJ0+eLPD+RURERKTkaGbrTyK/YhA1atSwaHN1dQXgypUr97wED+DEiRMYjUaio6OJjo7Ot0/edS9dukRGRgZPPPGERZ8nn3zynq89bNgwvvnmG5YtW0Zubm6xyt3n90wqVapktl8qJSUFa2trs6IbeWrVqsUvv/xi+vr06dMA+d7jnYlZ3tje3t7Y2pr/atra2lKzZs18Z/vujNnFxQUg3yWNLi4uXLlyxaJdREREREqekq1HnKOjI3CrYEN+8trzqyZoY2NT4Li5ubnFiifvvO7du9OzZ898+5RWZUMnJycWLVrE8OHD+fjjjzEajUyYMOGexrC2Lvpk78NSzbCgmO/2/RURERGR0qdk6xGXN3tx/PjxfI/nzbLkN8txPwpKNDw9PbGysiIrK4uWLVvedQw3NzecnZ3zjf3YsWPFisvR0ZEPP/yQESNGsGLFCnJzc5k4cWKxxiqIu7s7RqORU6dOWcxOnThxwuzrvFmn/O7x9hmw2/ufOHGC7Oxss9mt7OxsTp48me/Mm4iIiIg8nLRn6xFXv359qlevzmeffca5c+fMjmVmZrJq1SqsrKwICAgo0es6OzsDWCxJq1SpEm3atCE+Pt5Ufvx2ubm5pKamArdmXtq1a8cPP/zAvn37zPosXbq02LE5OjqycOFCWrVqxSeffGJWxKIk5D3L5cuXm7Xv3LnTIoF66qmnqFatGrGxsab7Brh27Rpr1661GDsoKIjU1FTWr19v1r5u3TpSU1MJCgoqobsQERERkdKmma1HnK2tLW+99RYRERF0797dVGr94sWLfPHFFxw9epThw4fnu2fofvj6+hIVFWV6H5SdnR0+Pj7UqFGDt956i/79+zNgwACCg4OpX78+RqOR06dPk5CQQI8ePUzVCMeMGcOuXbsYPnw4AwYMoFq1aiQmJpolJsWRl3CNHDmSlStXkpubyxtvvFESt06bNm3w9/dn3bp1XL58mRYtWpCSksK6deswGAwcPnzY1NfGxobXX3+dMWPG0Lt3b/r06YONjQ0xMTG4urpy9uxZs7GHDh3K1q1bmTRpEj/99BP16tXj0KFDREdHU6tWrWKXxBcRERGRB0/J1h9A27ZtWb16NUuXLmXjxo1cuXIFJycn6tWrx5w5c+jcuXOJX7Nr164cOnSIzz77jK1bt2I0Gpk6dSo1atSgevXqxMTEsGTJErZv387mzZtxcHCgevXqtGvXjk6dOpnG8fT0ZNWqVUyfPp2oqCjs7e1p3bo1M2bMKHQZYmEcHBxYsGABI0eOJCoqCqPRyD//+c/7vXWsrKyIjIxkzpw5fPbZZ+zatQuDwcC8efNYs2aNWeVCuFXyfu7cucyfP5/IyEgqV65Mz549adq0KYMHDzbrW6FCBdasWcPcuXPZvn07sbGxVK5cmdDQUEaPHm3xji0REREReXhZ5Ra3EoKIWOjWrRtZWVls3bq1rEMpESkpKQQGBlIrYDx2zm6FnyAiIvIIiJul901K4fI+ByUkJBS7/oH2bIkUw82bNy3aduzYwZEjR2jVqlUZRCQiIiIiDxstIxQphvnz5/PTTz/h5+dHhQoVOHToELGxsbi6ujJs2LCyDq/ELZ3YvsQrWoqIiJSVzKwc7O30ihQpfUq2RIqhSZMmHDx4kI8++ohr167h4uLCs88+y9///neqVatW1uGJiIjIXSjRkgdFyZZIMbRp04Y2bdqUdRgiIiIi8hDTni0REREREZFSoGRLRERERP40MrNyyjoE+RPRMkIRKdTQKfEq/S4iIn8IKvsuD5JmtkQecQEBAYSHh5d1GCIiIiJyByVbIg9AUlISBoOBjz76CIC0tDQiIyNJSkoq48hEREREpLQo2RIpA2lpacybN4/9+/eXdSgiIiIiUkqUbImIiIiIiJQCJVsiD1hSUhKBgYEAzJs3D4PBgMFgICAgwNRn1apVDB48mNatW+Pt7Y2/vz8vv/wyKSkphY7fvXt32rZti9FotDj2xRdfYDAY2LhxY8ndkIiIiIjkS9UIRR6w2rVr8/rrrzN16lTat29P+/btAShXrpypz7Jly2jYsCHh4eG4urpy5MgRoqOj2bdvH3FxcVSqVKnA8fv06cPkyZPZs2cPrVu3NjsWHR1NhQoV6NixY+ncnIiIiIiYKNkSecAee+wxgoKCmDp1KgaDgeBgyxK0cXFxODs7m7UFBgYyaNAgoqOjGTZsWIHjd+/enffee4/o6GizZOvXX3/l66+/pm/fvjg6OpbcDYmIiIhIvrSMUOQhlJdoGY1G0tPTSU1NxWAwUKFCBZKTk+96bsWKFenUqRMJCQlcvnzZ1B4TE4PRaOS5554r1dhFRERE5BYlWyIPob179xIeHk7Dhg1p0qQJLVq0oEWLFqSnp3P16tVCz+/Tpw9ZWVls2rQJgNzcXGJjY6lXrx7e3t6lHb6IiIiIoGWEIg+d5ORkhgwZgqenJy+99BIeHh44OjpiZWXF2LFjyc3NLXSMRo0aUadOHWJiYhg0aBB79+7lzJkzDBky5AHcgYiIiIiAki2RMmFlZVXgsS1btpCTk8OSJUuoUaOGqT0jI4O0tLQiX6N3795MmTKF5ORkoqOjcXBwoFu3bvcVt4iIiIgUnZYRipSBvD1Z+S0JtLGxyfecRYsW5VvOvSDBwcE4ODiwdOlS4uPjefbZZ6lYsWLxAhYRERGRe6aZLZEyUKlSJby8vPjss8+oUaMGjz32GE5OTgQEBBAUFMTy5csZNmwYffv2xc7Ojj179nD48OG7lny/k4uLCx06dGDz5s3ArZkuEREREXlwNLMlUkZmzpyJl5cXc+bMYdy4cbzzzjsANG7cmMjISJydnfnggw+IjIzE0dGRqKgoi3Lwhenbty8AXl5eNGvWrMTvQUREREQKppktkQfAz8+Pw4cPm7X5+Piwdu3afPsHBQURFBRk0b59+/YiteWxt7cHICQk5K77xERERESk5CnZEvkDi4qKws7Ojl69et3XOEsntsfDw6OEohIRESk7mVk52Nvlvz9apKQp2RL5g8nIyCAxMZGjR4+yefNm+vTpQ5UqVco6LBERkYeCEi15kJRsifzBpKamMm7cOJydnenQoQOvvvpqWYckIiIi8qekZEvkD8bDw8Nif5iIiIiIPHiqRigiIiIiIlIKlGyJiIiIyEMtMyunrEMQKRYtIxSRQg2dEo+ds1tZhyEiIn9ScbOCyzoEkWLRzJbIQyg2NhaDwUBSUlJZhyIiIiIixaRkSwRISkrCYDBgMBiYNGlSvn0uXbqEt7c3BoOB8PDwBxyhiIiIiDxqlGyJ3MbBwYEtW7aQmZlpcWzTpk3k5uZia1v6q2+Dg4NJTk6madOmpX4tERERESkdSrZEbtO+fXuuXr3Ktm3bLI7FxsbyzDPPYG9vX+px2NjY4ODggLW1fkVFREREHlX6JCdym/r162MwGIiNjTVrT05O5ujRo4SEhOR73vfff8+oUaPw8/PD29ubDh06sHDhQrKzs019ZsyYgcFgYOPGjWbn/vzzz/j4+BAeHo7RaAQK3rOVmZnJkiVLCA4OxtfXl8aNG9OrVy+ioqLM+qWkpPDKK6/QsmVLvL29CQoKYvbs2dy4caPYz0ZERERE7o2qEYrcISQkhGnTpnHu3Dkef/xxAKKjo6lcuTJt27a16L9jxw4iIiLw8vJi8ODBuLi48O233zJ37lwOHTrE3LlzARg7diwHDhzg7bffpmHDhtSsWZMbN24wduxYnJycmDlz5l1nsjIzMxkyZAj79+/H39+f7t274+DgwJEjR/jqq68YMGAAAGfOnKF3796kp6fTv39/vLy82L9/P4sWLeLgwYMsX778gSyFFBEREfmz0ycukTt0796d9957jw0bNjB8+HBu3rzJ559/Tu/evS2SlN9//52JEyfi6+vLihUrTMdDQ0OpW7cuU6dOJSkpCT8/P+zs7Jg1axY9e/Zk3LhxrF27lsmTJ3P8+HEWLlxoSuwKsmLFCvbv38+LL77IuHHjzI7lzYgBzJ49m9TUVBYvXkybNm0ACAsLY/r06SxbtowNGzbQu3fvknhUIiIiInIXWkYocodKlSoREBDAhg0bAPjqq69IT0/Pdwnhnj17uHjxIr169SItLY3U1FTTn2eeecbUJ0+NGjWYNGkSP/74I88//zwxMTGEh4cTEBBQaFxxcXG4uLgwatQoi2N5M2JGo5Ht27dTv359U6KV58UXX8Ta2jrf/WgiIiIiUvI0syWSj5CQEF544QUOHDhATEwMPj4+PPnkkxb9fvnlFwAmTJhQ4FgXL140+7pz585s376duLg46tSpw6uvvlqkmE6dOkW9evVwcHAosE9qaioZGRn5xurq6kqVKlU4ffp0ka4nIiIiIvdHyZZIPvz9/Xn88ceZP38+SUlJvPXWW/n2y83NBeDVV1+lXr16+fapWrWq2ddpaWkcPHgQgPPnz3Pp0iWqV69ecsGLiIiIyENByZZIPmxsbOjRoweLFi3C0dGRrl275tuvZs2aADg5OdGyZcsijT1x4kR+++033njjDWbMmMErr7zCihUrsLGxuet5NWvW5Pjx42RmZhZYft7NzY1y5cpx7Ngxi2NXr17lwoULBSaFIiIiIlKytGdLpAChoaFERETw9ttvU758+Xz7+Pv7U7lyZZYsWcKVK1csjt+8eZNr166Zvl6zZg1fffUVI0aMYMCAAbz22mt88803LFy4sNB4unXrxtWrV1mwYIHFsbwZNmtra9q1a8dPP/3Erl27zPosXrwYo9FIUFBQodcSERERkfunmS2RAvzlL39h9OjRd+3j7OzM9OnTGTVqFB07diQkJAQvLy/S0tI4fvw48fHxzJs3Dz8/P44cOcK0adNo2rQpI0eOBG5VCdyzZw8LFiygefPmNGnSpMBrDRw4kMTERBYuXMj333+Pv78/9vb2HDt2jBMnTrB8+XIAxo0bx9dff82oUaPo378/np6eHDhwgM8//5ymTZvSs2fPEntGIiIiIlIwJVsi96l169ZER0ezePFiNm/ezOXLl6lYsSKenp4MGjQIg8HAzZs3GTduHI6OjsycOdNsyeC7775LcHAwr7zyChs3bsTFxSXf69jb27Ns2TKWLVvGli1bmD17Ng4ODnh5edGrVy9TP3d3d9atW8fcuXPZvHkz6enpPP7447z44ouMGDFC79gSEREReUCscvPWH4mI3CElJYXAwEBqBYzHztmtrMMREZE/qbhZwWUdgvwJ5X0OSkhIwMPDo1hj6L+4RaRQSye2L/Y/MiIiIvcrMysHe7u7F5ISeRipQIaIiIiIPNSUaMmjSsmWiIiIiIhIKVCyJSIiIiIiUgqUbImIiIjIQykzK6esQxC5LyqQISKFGjolXtUIRUTkgVMVQnnUaWZLRERERESkFGhmSx5pv//+O9HR0Xz55ZccOXKE9PR0nJyc8PLyonnz5vTq1ev/sXfvUVVX+f/Hn4CgICpQOjaCgKbHlIuWiuClxHteMBTFC+iAYoWZZjNROlOjXXRMu+CMeBdFC7xg4tdLSmqRhTY1qZWalwysUROQABWE8/ujn2c6HQxEEIXXYy3X4uzP3vvz/pylrPP2vfc+tGzZ/sL5JwAAIABJREFUsrrDFBEREZFaSMmW3LUyMjKYNGkSJ0+epHPnzowfP57GjRtTUFDAN998w8aNG1mxYgV79+7lD3/4Q3WHKyIiIiK1jJItuStduXKFqKgoMjIyWLhwIX369LHoc/XqVVatWlWp9y0uLqawsBB7e/tKnVdEREREah7t2ZK70vr16zl16hSRkZGlJloAdevWZdKkSRZVrZ9//pl58+bRp08fvLy86NKlC8888wwZGRlm/TZt2oTBYGD//v3885//pHfv3vj4+LB9+3bS09MxGAxs2rSJtWvX0q9fP7y9vRk8eDB79uwB4NixY0RGRvLggw/i5+fHyy+/TFFRkdk9Dh06RExMDP369cPX15cOHToQGhrKrl27LJ4nJiYGg8HAzz//zIsvvoi/vz/e3t6Ehoby5Zdfmvp9/fXXGAwG3njjjVLfl6ioKB588EEKCgrKfqNFREREpMJU2ZK70s6dOwEYPnz4TY37+eefCQ0N5YcffmDYsGG0atWKCxcusG7dOkJCQti4cSPNmjUzGzN37lyuXbvGiBEjqF+/Pp6enhQWFgKwdu1acnNzCQkJwc7OjjVr1jB58mTeeustZs6cyaBBg+jduzcff/wxa9aswcXFhSeffNI0965duzh16hT9+/enWbNm5OTkkJyczOTJk3n99dcZPHiwxTNERkbi4uJCdHQ0OTk5rFy5kqioKFJTU3F0dKRt27a0a9eO5ORkpkyZgo2NjWnsuXPnSEtLY9iwYTg4ONzUeyciIiIiN0fJltyVvv32WxwdHXFzczNrLy4u5tKlS2ZtDg4O1KtXD4C33nqLjIwMkpKSaNOmjanPY489xuDBg4mNjWXOnDlm469cucLmzZvNlg6mp6cDcP78ebZt20aDBg0A6NKlC0FBQUyePJm3336bvn37AjBq1CiCg4NZt26dWbL1xBNPMH36dLP7hYWFMXToUBYtWlRqstW2bVteeukl0+uWLVsydepUtm7dSmhoKAAjR47kb3/7G2lpaTz88MOmvps2baK4uJiQkJDS3lYRERERqURaRih3pby8PBwdHS3aT548ib+/v9mftWvXAmA0GklJSaFTp040adKErKws0x97e3vat29PWlqaxZyjRo264R6t4OBgU6IF0KZNGxwdHWnSpIkp0bruwQcf5MKFC+Tn55vafl1dunz5MtnZ2Vy+fJkuXbpw8uRJ8vLyLO45fvx4s9ddunQB4MyZM6a2QYMG4eDgwIYNG0xtRqORjRs30rp1a3x8fEp9HhERERGpPKpsyV3J0dGx1ETE1dWVlStXAnD06FHmzp1rupaVlUVOTg5paWn4+/uXOq+1teX/P3h6et4wDldXV4u2Ro0a0bRp01LbAXJycqhfvz4AFy9e5M033yQ1NZWLFy9ajMnNzbVIKn9bzXN2djbNe139+vUZNGgQycnJZGVl4eLiQnp6OhkZGbzwwgs3fB4RERERqTxKtuSu1KpVKw4ePEhGRoZZ8uHg4EBAQACA2V4l+KWyAxAQEMDEiRPLfa/rSxBL89t7lNX+6ziMRiMRERGcPHmS8PBwvLy8aNCgATY2NmzcuJGtW7dSUlJS7rmvz3vdiBEjSEpKYvPmzURERLBhwwbs7OwICgq6YWwiIiIiUnmUbMldqV+/fhw8eJANGzYwbdq0co1xcXGhYcOG5OXlmRKy6nTs2DGOHj1KdHQ0U6ZMMbu2fv36W57f29ubtm3bsmHDBoYPH877779P7969cXJyuuW5RURERKRs2rMld6WQkBBatGjB8uXLSz0mHSwrPdbW1gwePJhDhw6xY8eOUseUtpSvqlxfsvjbOI8fP37DZ7pZISEhnDx5ktmzZ3P16lUdjCEiIiJyG6myJXelevXqsWTJEiZNmsTkyZPp3Lkz3bp149577yUvL49Tp06xfft2bGxsuO+++0zjpk2bxueff87UqVMZMGAAvr6+2Nra8sMPP/Dhhx/Srl07i9MIq0rLli1p1aoVy5Yt48qVK3h6enL69GkSExNp3bo1X3311S3fY8iQIcybN48tW7bg6up6w71qIiIiIlL5lGzJXcvNzY1NmzaxceNGduzYwYoVK8jLy8Pe3p7mzZszfPhwhg8fTosWLUxjGjRowDvvvMOKFSvYsWMHqamp2NjY0LRpUx566KHbWvmxsbFh8eLFzJ07l+TkZC5fvkyrVq2YO3cuR48erZRky9HRkQEDBrBx40aCg4OxsrKqhMhFREREpDysjL9dwyQiNcpLL71EUlISH3zwQamnJP6ezMxMevXqhWdgDLYOLlUUoYiISOlS5utQJ6k+1z8HpaamlnoCdXmosiVSg/38889s2bKFHj163HSi9WvLZvSp8C8ZERGRiiosKsbO9sYn/Irc6ZRsidRAx48f5+uvv2bz5s0UFBQwadKk6g5JRETkpinRkrudki2RGmjnzp0sXLiQP/zhD7z44ot06NChukMSERERqXWUbInUQE899RRPPfVUdYchIiIiUqvpe7ZERERERESqgJItEREREalWhUXF1R2CSJXQMkIRKdOEV3bp6HcREakyOuJdaipVtkRERERERKqAki2RapCeno7BYGD58uXVHYqIiIiIVBElWyIiIiIiIlVAyZbIHSwvL6+6QxARERGRClKyJXIHyMzMxGAwEBsby7Zt2wgODsbHx4eXX34ZgJMnT/LSSy8xcOBAOnTogK+vL8HBwaxfv95irtjYWAwGA6dOnWLBggX06NEDLy8vhgwZwr59+273o4mIiIjUWjqNUOQOsnv3btasWcOoUaMIDQ3F0dERgAMHDvDZZ5/xyCOP4OrqyuXLl9mxYwczZ84kKyuLSZMmWcwVExNDnTp1iIiIoKioiPj4eKKjo9mxYweurq63+9FEREREah0lWyJ3kBMnTrBlyxZatmxp1h4UFMSoUaPM2saPH8+4ceNYsmQJERER2Nraml13dnYmLi4OKysrAPz8/AgJCSExMZHp06dX7YOIiIiIiJYRitxJHn74YYtEC8DBwcH089WrV8nOziYnJ4euXbuSl5fHqVOnLMaEh4ebEi0AHx8fHBwcOHPmTNUELyIiIiJmVNkSuYN4eHiU2p6fn8/ChQvZvn07P/74o8X13NxcizY3NzeLNmdnZ7Kzs285ThEREREpm5ItkTuIvb19qe3Tp09n7969jBgxgk6dOuHk5ISNjQ379u1j1apVlJSUWIyxtlbhWkRERKQ6KdkSucPl5uayd+9egoKCmDVrltm1/fv3V1NUIiIiIlIW/de3yB3ueoXKaDSatZ8/f77Uo99FRERE5M6gypbIHc7R0ZGuXbuyZcsW6tWrh7e3N2fPniUxMRFXV1dycnKqO0QRERERKYWSLZG7wLx585g/fz4ffPABycnJeHh4MG3aNOrUqcPzzz9f3eGJiIiISCmsjL9dmyQi8v9lZmbSq1cvPANjsHVwqe5wRESkhkqZH1TdIYhYuP45KDU1FVdX1wrNocqWiJRp2Yw+Ff4lIyIiUpbComLsbG2qOwyRSqcDMkRERESkWinRkppKyZaIiIiIiEgVULIlIiIiIiJSBZRsiYiIiEipCouKqzsEkbuaDsgQkTJNeGWXTiMUEamFdEqgyK1RZUukAgIDAwkLC6vw+E2bNmEwGEhPT6/EqERERETkTqLKltRY6enphIeHm7XZ2dnRpEkTOnfuzIQJE2jZsmU1RSciIiIiNZ2SLanxBg0aRI8ePQC4evUqx44dY/369ezcuZOUlBSaNWt222MKCgpi4MCB2Nra3vZ7i4iIiMjtoWRLary2bdsSFGS+5tzd3Z1XXnmFXbt2MX78+Nsek42NDTY2+k4RERERkZpMe7akVmrSpAmARWVp27ZtjBo1ig4dOuDr60tISAg7duwo97zr1q2jX79+eHl50bdvXxISEkrdn1VaW2xsLAaDgczMTIt5S9sjZjAYiImJ4ZNPPmHkyJH4+vrSo0cPlixZAsClS5d44YUX8Pf3x9fXl0mTJnHu3LlyP4uIiIiI3BpVtqTGu3z5MllZWcAvywiPHz/OG2+8gbOzM3379jX1e+ONN4iLi6N79+48/fTTWFtbs2vXLp5++mn+9re/MWbMmN+9z5IlS5g/fz7t2rVj+vTpXL58meXLl+Ps7Fxlz/b111+zZ88eRowYQVBQENu3b2f+/PnUrVuXzZs306xZMyZPnsz333/PmjVreO6551i1alWVxSMiIiIi/6NkS2q82NhYYmNjzdruv/9+1q5dS+PGjQH46quviIuLY9KkSTzzzDOmfuHh4Tz55JPMnz+foKAgHB0dS71HTk4OCxcupHXr1rzzzjvUrVsXgJCQEPr3719FTwbHjx8nMTERX19fAIYPH05gYCCvvfYaY8eOZebMmWb9V61axalTp2jRokWVxSQiIiIiv9AyQqnxRo4cycqVK1m5ciVxcXE8++yzZGdnExUVxdmzZwFISUnBysqKoUOHkpWVZfYnMDCQ/Px8/vOf/9zwHvv37+fq1auMGjXKlGgBNG7cmMGDB1fZs7Vv396UaMEvpy16e3tjNBotlh127NgRgDNnzlRZPCIiIiLyP6psSY3n7u5OQECA6XXPnj3p3LkzI0aM4PXXX+eNN97g5MmTGI1GBgwYcMN5fvrppxteu77PytPT0+JaaW2Vxc3NzaKtUaNGALi6upq1N2zYEPilCiciIiIiVU/JltRKvr6+NGjQgE8//RQAo9GIlZUVS5cuveEpgffff3+VxmRlZXXDa9euXSu1/fdONLzRNaPReHOBiYiIiEiFKNmSWqu4uJjCwkIAPDw8+Oijj/jjH/9YoS86vv5dXadPn8bf39/s2unTp8s1x/WK1KVLl8yqUlevXuXChQu4u7vfdFwiIiIiUn20Z0tqpY8//piCggLatWsHwJAhQwBYsGABxcXFFv1/bwkhQEBAAHZ2drzzzjtcvXrV1H7hwgVSUlLKFZOHhwfwy/6vX1u1ahUlJSXlmkNERERE7hyqbEmN9/XXX/Pee+8BUFhYyIkTJ0hKSsLW1papU6cC4OPjw1NPPUVsbCxDhw6lX79+/OEPf+D8+fN89dVXfPjhhxw5cuSG93B2dmby5MksWLCAUaNGMWTIEC5fvkxSUhIeHh4cOXLkd5cJwi8Jm6enJ2+//TY5OTm4urry73//my+//LJKj48XERERkaqhZEtqvK1bt7J161YArK2tcXJyomvXrkRFReHj42PqN3nyZLy8vFizZg2rV6+moKCAe+65h1atWjFjxowy7zNp0iQcHR1ZvXo1r7/+On/84x+JjIzEaDRy5MgR6tWr97vjbWxsWLRoES+//DIJCQnY2trStWtXEhISGDVq1K29CSIiIiJy21kZtVtepErNnj2bhIQE0tLSTN/rdbfIzMykV69eeAbGYOvgUt3hiIjIbZYyP6i6QxCpNtc/B6Wmplqc8lxe2rMlUkl+vVfruvPnz7N582Zat2591yVaIiIiInJrtIxQpJKkp6czb948+vTpQ9OmTTl79ixJSUkUFBQwffr06g7vliyb0afC/6MjIiJ3r8KiYuxsb/w1IyLy+5RsiVQSd3d33NzcSEpKIicnh7p16+Ll5cWkSZPMvlRZRETkbqFES+TWKNkSqSTu7u7861//qu4wREREROQOoT1bIiIiIiIiVUDJloiIiIiYFBYVV3cIIjWGlhGKSJkmvLJLR7+LiNQSOu5dpPKosiUiIiIiIlIFlGyJVIP09HQMBgPLly+v7lBEREREpIoo2RIREREREakCSrZE7mB5eXnVHYKIiIiIVJCSLZE7QGZmJgaDgdjYWLZt20ZwcDA+Pj68/PLLAJw8eZKXXnqJgQMH0qFDB3x9fQkODmb9+vWlzvftt98SGRlJ+/bt6dy5M9OnT+fixYsYDAZiYmJu56OJiIiI1Fo6jVDkDrJ7927WrFnDqFGjCA0NxdHREYADBw7w2Wef8cgjj+Dq6srly5fZsWMHM2fOJCsri0mTJpnmyMjIYMyYMRQWFjJmzBjuu+8+9uzZw4QJE6rrsURERERqJSVbIneQEydOsGXLFlq2bGnWHhQUxKhRo8zaxo8fz7hx41iyZAkRERHY2toC8Oabb3Lp0iXi4+Pp0qULAGPGjGHy5Ml8/fXXt+dBRERERETLCEXuJA8//LBFogXg4OBg+vnq1atkZ2eTk5ND165dycvL49SpUwCUlJTwwQcf4OXlZUq0AKysrFTZEhEREbnNVNkSuYN4eHiU2p6fn8/ChQvZvn07P/74o8X13NxcAC5evEhBQQEtWrSw6HP//fdXaqwiIiIi8vuUbIncQezt7Uttnz59Onv37mXEiBF06tQJJycnbGxs2LdvH6tWraKkpOQ2RyoiIiIiZVGyJXKHy83NZe/evQQFBTFr1iyza/v37zd77eLigoODg2lZ4a+dOHGiSuMUEREREXPasyVyh7O2/uWfqdFoNGs/f/68xdHvNjY29OzZkyNHjvDpp5+a2o1GI8uWLav6YEVERETERJUtkTuco6MjXbt2ZcuWLdSrVw9vb2/Onj1LYmIirq6u5OTkmPWfOnUqH374IY8//jhjx46ladOm7Nmzh6ysrGp6AhEREZHaSZUtkbvAvHnzGDZsGB988AGzZs0iNTWVadOmMWbMGIu+zZs3Z+3atTz44IMkJCTw9ttv4+TkpMqWiIiIyG2mypZINfDz8+PYsWOm166urmavf8vFxYVXXnml1GvBwcEWbQaDgRUrVtx6oCIiIiJSYUq2RKRMy2b0wdXVtbrDEBGR26CwqBg7W5vqDkOkRtAyQhERERExUaIlUnmUbImIiIiIiFQBLSMUqUV+b1+YiIiIiFQuVbZERERERESqgJItERERkVqusKi4ukMQqZG0jFBEyjThlV3YOrhUdxgiIlJFUuYHVXcIIjWSKlsid5jY2FgMBgOZmZnl6m8wGIiJianiqERERETkZqmyJbXK5cuXSUxM5P333+fEiRPk5+fTqFEj2rVrx4ABAxgyZAh16uifhYiIiIjcOn2qlFrjzJkzREVF8d133xEQEEBUVBTOzs5cvHiRTz75hOeff54TJ07wl7/8pbpDvSmHDh3C2lpFahEREZE7jZItqRWuXLnCpEmTyMzMJDY2lr59+5pdj4qK4tChQxw+fLiaIqy4unXrVncIIiIiIlIKJVtSK6xfv57Tp08zceJEi0TrOh8fH3x8fIBf9k0tXLjwhvOlpqbi6uoKwM8//0xcXBzvv/8+P/74I46OjgQEBDBt2jTc3NzMxhUWFhIfH8/WrVv57rvvqFOnDu7u7gQHBzN27FiLvgsWLGDz5s1kZWXRokULpk+fzsMPP2zWz2Aw8NhjjzFnzhxT27Zt29iyZQtHjx7lp59+on79+jz00ENMmTKFNm3alP+NExEREZEKU7IltcLOnTsBGDlyZLn69+nTh+bNm5u1FRYWMmfOHIqLi6lfvz7wS6IVGhrKDz/8wLBhw2jVqhUXLlxg3bp1hISEsHHjRpo1a2YaHxkZyYEDB+jWrRtDhgyhbt26HD9+nPfff98i2YqJiaFOnTpERERQVFREfHw80dHR7Nixw5To3UhCQgJOTk6MGDGCxo0b8/3335OUlMSoUaNITk7Gw8OjXO+DiIiIiFScki2pFb799lscHR0tKk030qZNG7MKkNFo5JlnniE/P5/Y2FicnZ0BeOutt8jIyCApKcms/2OPPcbgwYOJjY01VZzi4+M5cOAAkyZN4plnnjG7X0lJiUUMzs7OxMXFYWVlBYCfnx8hISEkJiYyffr0341/2bJlODg4mLUNHTqUoKAgVq1axUsvvVSu90FEREREKk7JltQKeXl53HPPPRUe/+abb7Jt2zaeffZZ+vTpA/ySgKWkpNCpUyeaNGlCVlaWqb+9vT3t27cnLS3N1JaSkkKjRo2Ijo62mL+0Ay7Cw8NNiRb8sszRwcGBM2fOlBnv9UTLaDSSn59PYWEhzs7OeHp6cujQofI/uIiIiIhUmJItqRUcHR3Jz8+v0Njk5GTi4uIYPnw4EydONLVnZWWRk5NDWloa/v7+pY79dRJ15swZHnjggXIfaFFaFc7Z2Zns7Owyx3799de89dZbHDhwgIKCArNrZS1BFBEREZHKoWRLaoVWrVpx8OBBMjIyyr2UECA9PZ2//vWvdOnSxWLpndFoBCAgIMAsCassFT3O/YcffmDMmDE4OjryxBNP0KJFC+zt7bGysuLVV1+1SL5EREREpGoo2ZJaoW/fvhw8eJD169db7Je6kVOnTvHUU0/h6urK22+/ja2trdl1FxcXGjZsSF5eHgEBAWXO5+HhwalTpygsLMTOzq5Cz1Eeu3btoqCggEWLFtGlSxezazk5OVV6bxERERH5H30TqtQKISEheHp6smLFCnbv3l1qnyNHjrB27VoAsrOzmTRpElZWVixZsoRGjRpZ9Le2tmbw4MEcOnSIHTt2lDrnxYsXTT8PHjyYS5cu8a9//cui3/UqWWWwsbEpdc6kpCQuXLhQafcRERERkd+nypbUCvb29ixevJioqCiio6Pp1q0bAQEBODk5kZWVRXp6OmlpaUyYMAGAv//973z//feEhobyxRdf8MUXX5jN16dPHxwcHJg2bRqff/45U6dOZcCAAfj6+mJra8sPP/zAhx9+SLt27UynEYaHh7Nnzx4WLVrE4cOH6datG3Z2dpw4cYLTp0+zatWqSnnWHj16YG9vz1/+8hfGjh1Lw4YN+fzzz/nwww9p3rw5xcXFlXIfEREREfl9Srak1nB3d2fz5s0kJiayc+dO4uLiKCgooFGjRnh5eTFnzhwGDx4M/K8i9e677/Luu+9azJWamoqDgwMNGjTgnXfeYcWKFezYsYPU1FRsbGxo2rQpDz30ECEhIaYxdnZ2rFixghUrVrB161YWLFhA3bp1TV9qXFmaN2/O0qVLWbBgAXFxcdjY2PDggw+yZs0aZs+ezdmzZyvtXiIiIiJyY1bGyly/JCI1SmZmJr169cIzMAZbB5fqDkdERKpIyvyg6g5B5I5z/XNQampqhU9zVmVLRMq0bEYfHRkvIlKDFRYVY2drU91hiNQ4OiBDREREpJZToiVSNZRsiYiIiIiIVAElWyIiIiIiIlVAyZaIiIhILVdYpK8FEakKOiBDRMo04ZVdOo1QRKQG02mEIlVDlS2pEcLCwggMDLzt942JicFgMNz2+96pcYiIiIjI/yjZkkqTnp6OwWDAYDCQlJRUah+DwcCkSZMqNP+mTZtYtWrVLUQoIiIiInL7KNmSKhEbG8uVK1cqdc7k5GRWr15dqXOKiIiIiFQVJVtS6by8vDh//jzx8fHVHUqVMBqN5OfnV3cYIiIiInKHU7IllW7AgAG0a9eOpUuXkp2dXWb/w4cPEx0djZ+fH15eXvTr149FixZx7do1U5/AwEAOHDjA2bNnTUsVDQYD6enpZnOdO3eOZ555hk6dOuHr60tkZCSnT5+2uGdhYSFxcXEMHDgQb29vOnbsyOOPP87XX39t1u/60shNmzaxdu1aHn30Uby9vVmxYsUNn+fkyZO89NJLDBw4kA4dOuDr60twcDDr16+36BsbG4vBYODUqVMsWLCAHj164OXlxZAhQ9i3b59F/6tXrzJ37ly6deuGj48Pw4cPJy0trdQ4vv32W6ZMmUL37t3x8vKia9euhIWFsXfv3hvGLiIiIiKVR6cRSqWzsrLi2Wef5U9/+hNxcXE8//zzN+y7d+9eJk+ejLu7OxERETRq1Ij//Oc/vP3223zzzTe8/fbbALzwwgvMnz+f7Oxss/latmxp+rmgoICxY8fi6+vLtGnTyMzMZPXq1Tz55JNs3boVGxsbAIqKioiMjOSLL74gKCiIMWPGkJeXR1JSEqNGjSIhIQFvb2+zOOPj48nJySEkJITGjRvTtGnTGz7TgQMH+Oyzz3jkkUdwdXXl8uXL7Nixg5kzZ5KVlVXqnrWYmBjq1KlDREQERUVFxMfHEx0dzY4dO3B1dTX1e+aZZ9i9ezc9e/ake/fufP/99zz11FNmfQCys7MZN24cAKGhofzxj38kOzubI0eO8OWXX/LII4/cMH4RERERqRxKtqRKBAQE0LVrV9atW0d4eDjNmjWz6HP16lVmzJiBr68v8fHx1Knzy1/H0NBQ2rRpw2uvvUZ6ejp+fn707t2b+Ph4rl69SlBQ6cfTZmdnExkZycSJE01tLi4uzJs3j/3799O9e3cA1q5dy4EDB1i2bJmpDWD06NEMGjSIf/zjH6xZs8Zs7h9//JHt27dzzz33lPnsQUFBjBo1yqxt/PjxjBs3jiVLlhAREYGtra3ZdWdnZ+Li4rCysgLAz8+PkJAQEhMTmT59OgBpaWns3r2bxx57jDlz5pjGdurUiejoaLP5Pv/8cy5evMgbb7zBo48+WmbMIiIiIlL5tIxQqsyzzz5LUVERb731VqnXP/74Y3766SeCg4PJzc0lKyvL9KdHjx6mPuVlbW1NeHi4WVuXLl0AOHPmjKlty5YttGjRgnbt2pnds7CwkICAAP79739bHO4RFBRUrkQLwMHBwfTz1atXyc7OJicnh65du5KXl8epU6csxoSHh5sSLQAfHx8cHBzM4t69ezcAkZGRZmN79+6Np6enWVuDBg0A+Oijj8jLyytX3CIiIiJSuVTZkirTtm1bBg4cSEpKChEREbRp08bs+smTJ4FflgjeyE8//VTu+zVp0oS6deuatTk5OQGQk5Njdt8rV67g7+9/w7mys7O57777TK89PDzKHUd+fj4LFy5k+/bt/PjjjxbXc3NzLdrc3Nws2pydnc32vGVkZGBtbV1qLC1btjTbm9a5c2eGDh3Kpk2bSElJwcvLi4CAAB599FHuv//+cj+LiIiIiFScki2pUlOnTmXnzp28/vrrLFu2zOya0WgE4C9/+QsPPPBAqeObNGlS7ntd35NVmuv3uv7TCbDoAAAgAElEQVRz69atf3cvmYuLi9lre3v7cscxffp09u7dy4gRI+jUqRNOTk7Y2Niwb98+Vq1aRUlJicUYa+vKLzLPnTuXyMhIPvzwQz777DNWrlxJXFwcL7zwAmPHjq30+4mIiIiIOSVbUqXc3NwYNWoUq1evtjg58HqFxt7enoCAgNsWk7u7O9nZ2XTp0qXSk5zc3Fz27t1LUFAQs2bNMru2f//+W5rbzc2NkpISvvvuO1q1amV27XqV8Ldat25N69atmTBhArm5uYSEhDB//nzGjBljtmxRRERERCqf9mxJlXviiSdwdHRk3rx5Zu3dunXjnnvuYenSpWbL/K67cuWK2X6j+vXrc+nSJbMqVUUMHTqUCxcusHLlylKv38zSxd+6nrz9Nsbz58+XevT7zejVqxcAy5cvN2vfvXu3xfH2OTk5FhW0hg0bmk5HvHr16i3FIiIiIiJlU2VLqpyLiwuRkZEWB2U4ODgwd+5coqOj6d+/P8OGDcPd3Z3c3FxOnTrFrl27WLhwIX5+fgD4+vqyZ88eZs2aRYcOHbCxsaFLly7lPrjiuvDwcPbv388//vEPPv30U7p06YKjoyM//PADn376KXZ2dhanEZaXo6MjXbt2ZcuWLdSrVw9vb2/Onj1LYmIirq6upSaV5dW9e3d69uxJcnIyOTk5dO/enYyMDBITE2ndujXHjx839d28eTPx8fH07t0bd3d36tSpw8GDB0lLS2PAgAHUq1evwnGIiIiISPko2ZLb4k9/+hPr1q3jwoULZu3du3dnw4YNLFmyhC1btpCdnU3Dhg1p3rw548ePx2AwmPqOHz+ejIwMdu7cybvvvktJSQmrV6++6WTL1taWxYsXs27dOt577z1iY2OBX/aHeXt789hjj93Ss86bN4/58+fzwQcfkJycjIeHB9OmTaNOnTq/u0+sPN58803efPNNUlJS2L9/P61btyY2NpatW7eaJVt+fn5888037N27lwsXLmBtbY2rqyvPPfec9muJiIiI3CZWxltdkyUiNVZmZia9evXCMzAGWweXsgeIiMhdKWV+6d9hKVKbXf8clJqaiqura4XmUGVLRMq0bEafCv+SERGRO19hUTF2tjc+1VdEKkYHZIiIiIjUckq0RKqGki0REREREZEqoGRLRERERESkCijZEhERERERqQJKtkRERERqkcKi4uoOQaTW0GmEIlKmCa/s0tHvIiI1hI55F7l9VNkSuYvFxsZiMBjIzMys7lBERERE5DdU2ZIaIT09nfDwcLM2BwcHPDw8CAoKYuzYsdSpo7/uIiIiInL76NOn1CiDBg2iR48eGI1GfvrpJ9577z1ee+01Tp48yezZs6s7vEr3xBNPEBUVhZ2dXXWHIiIiIiK/oWRLapS2bdsSFPS/teijR49mwIABrF+/nmnTpuHiUrP2HdWpU0cVOxEREZE7lPZsSY3m4OCAr68vRqOR77//3tR+9OhRoqOj8fPzw9vbm0cffZSlS5dSXGx+QlNMTAwGg4Hs7GxiYmLw8/OjQ4cOPPnkk1y4cAGAxMREBgwYgLe3N/3792f37t0Wcaxdu5aIiAi6d++Ol5cX3bp149lnny11r5XBYCAmJoYvvviCsWPH0r59e/z8/JgxYwb5+flmfUvbs3Xu3DnmzJlDUFAQnTp1Mj3fkiVLLJ5PRERERKqO/ktcaryMjAwAGjVqBMDhw4cJCwujTp06jBkzhnvvvZc9e/bw+uuvc/ToUebPn28xx4QJE2jatClTpkzh+++/Z82aNUyePJk+ffqQlJTE8OHDsbOzY82aNTz99NPs2LEDNzc30/gVK1bQvn17wsLCcHJy4vjx42zYsIFPP/2UlJQUnJ2dze73zTff8PjjjxMcHMygQYM4cOAAGzZswNrauszlkMeOHeP999+nT58+NG/enKKiIj766CPmz59PZmYms2bNutW3VERERETKQcmW1CiXL18mKysLgAsXLvDuu+/y9ddf4+Pjg6enJwCvvPIKhYWFvPvuu7Rp0waAsWPHMnXqVLZu3crw4cPx9/c3m9fHx4cXX3zRrG3VqlWcO3eOrVu34ujoCECXLl0ICgoiKSmJ6dOnm/qmpKTg4OBgNr5Xr16MHz+eDRs2MHHiRLNrx44dIzExEV9fXwBCQ0PJy8tj06ZNxMTEUL9+/Ru+B507dyY1NRUrKytT2/jx4/nzn//M+vXrmTx5Mk2aNCn7zRQRERGRW6JlhFKjxMbG4u/vj7+/P0OGDGHdunX07duXf/3rXwBcvHiRL774gsDAQFOiBWBlZcUTTzwBwK5duyzmHTdunNnrjh07AhAUFGRKtADatGmDo6MjZ86cMet/PdEqKSnh559/JisrC4PBQIMGDTh06JDF/dq3b29KtK7r0qUL165d4+zZs7/7HtSrV8+UaBUWFpKTk0NWVhbdunWjpKSEI0eO/O54EREREakcqmxJjTJy5Ej69+9PUVERx48fZ9myZfz3v/+lbt26AKa9Tffff7/F2BYtWmBtbW1advhrv14SCNCwYUMAXF1dLfo2atSI7Oxss7ZPPvmEf/3rX3z55ZdcvXrV7NqlS5fKvB+Ak5MTADk5ORbXfu3atWssWbKE9957jzNnzmA0Gs2u5+bm/u54EREREakcSrakRnF3dycgIACAhx9+mIceeojRo0fz4osv8sYbb1R4Xhsbm5tq/7VDhw4RGRlJ8+bNmT59Oq6urqbq07Rp0yySobLmLa3/r82ZM4c1a9bw6KOP8vjjj+Pi4oKtrS1fffUVr7/+OiUlJWXGLCIiIiK3TsmW1GgPPvggQUFBbN68mbCwMNzd3QE4ceKERd9Tp05RUlJSalXpVmzdupXi4mKWLl1qNndBQUGVVJnee+89OnXqZJFc/nZpo4iIiIhULe3ZkhrvySefxMbGhrfffpt77rmHDh06sGfPHo4fP27qYzQaWbJkCQB9+vSp1PvfqEq1ePHiKqkyWVtbW1S/CgoKWLVqVaXfS0RERERuTJUtqfHc3d159NFHSUlJ4bPPPmPGjBmEhYUxZswYRo8eTePGjdmzZw9paWkMGjTI4iTCW9W7d29WrVrFxIkTGTlyJLa2tnz88cccO3bM4sj3ytCvXz8SExOZOnUqAQEB/PTTT2zcuNG050tEREREbg9VtqRWeOKJJ7C2tuatt97C29ubd999l06dOvHOO+8wZ84cfvjhB5599ln+8Y9/VPq9H3roIWJjY3FwcOCtt94iNjaWevXqkZCQYHEcfGV4/vnniYiI4Msvv2T27Nls3ryZkSNH8uyzz1b6vURERETkxqyMZe22F5FaKzMzk169euEZGIOtg0t1hyMiIpUgZX5QdYcgcle4/jkoNTW11BOoy0PLCEWkTMtm9KnwLxkREbmzFBYVY2db9mm6InLrtIxQREREpBZRoiVy+yjZEhERERERqQJKtkRERERERKqAki0RERGRGqqwqLi6QxCp1XRAhoiUacIru3QaoYjIXUgnD4pUL1W25K4QGxuLwWAgMzOzXP0NBgMxMTFmbYGBgYSFhZm1hYWFERgYWGlxliUmJgaDwXDb7iciIiIi1UeVLalW6enphIeHm7U5ODjg6elJUFAQY8eOxcZGpyaJiIiIyN1HyZbcEQYNGkSPHj0wGo2cP3+e5ORkXn31VU6cOMHs2bNver5Dhw5hbV124Xb58uUVCbfCZs+ezd///vfbek8RERERqR5KtuSO0LZtW4KC/reufPTo0QwYMID169fz9NNP3/R8devWLVc/Ozu7m577Vtja2t7W+4mIiIhI9dGeLbkjOTo60qFDB4xGIxkZGab2wsJCFixYQI8ePfDy8mLIkCHs27fPYnxpe7ZKU9qerettGRkZPPHEEzz00EM8+OCDREdHm8UCvyyDNBgMbNq0iTVr1tCvXz+8vb3p168fa9assbhfaXu2rrf9/PPPvPjii/j7++Pt7U1oaChffvmlxRxGo5F169YRHByMr68vHTp0ICwsjE8//dSi7+bNmxk+fDgdO3akffv29OrVi+nTp5OVlVXmeyMiIiIit0aVLbkjGY1Gzpw5A4Czs7OpPSYmhjp16hAREUFRURHx8fFER0ezY8cOXF1dK+3+BQUFhIWF4ePjwzPPPMOZM2dYt24dX375JcnJyTRu3Nisf0JCAhcuXGDkyJE4OjqydetWXn75ZS5dusTkyZPLdc/IyEhcXFyIjo4mJyeHlStXEhUVRWpqKo6OjqZ+f/7zn/m///s/+vXrR3BwMIWFhaSkpBAREUFsbCy9evUCfkm0nnvuOTp27MiUKVOoV68eP/74I/v27ePixYu4uOh0QREREZGqpGRL7giXL182VVvOnz9PQkICR48epX379nh4eJj6OTs7ExcXh5WVFQB+fn6EhISQmJjI9OnTKy2e7OxswsPDmTFjhqmtU6dOTJ48mdjYWGbNmmXW//Tp02zfvp2mTZsCvyyDHD16NIsWLWL48OGm9t/Ttm1bXnrpJdPrli1bMnXqVLZu3UpoaCgAu3btIiUlhVmzZjFy5EhT3/DwcEaMGMErr7xCYGAgVlZW7N69m/r16xMfH0+dOv/7p16RZZkiIiIicvO0jFDuCLGxsfj7++Pv709QUBAbN24kMDCQf/7zn2b9wsPDTYkWgI+PDw4ODqYqWGWKiooye92nTx88PT1JTU216Dt48GCzhMrOzo7x48dz7do1Pvjgg3Ldb/z48Wavu3TpAmD2bFu2bKF+/fr07t2brKws05/c3FwCAwM5e/Ys3333HQANGjTgypUr7N27F6PRWK4YRERERKTyqLIld4SRI0fSv39/rKyssLe3x8PDAycnJ4t+bm5uFm3Ozs5kZ2dXajwNGza0WCoIv1Sbdu/eTUFBAQ4ODmbtv3X//fcDWOzzupHfPtv15ZM5OTmmtpMnT5Kfn09AQMAN57l48SKenp5MmjSJgwcPEh0djZOTE507d6ZHjx4MGDDAbFmiiIiIiFQNJVtyR3B3d//dBOK68hznfre60feJ/boqZTQacXFxYf78+Tecp1WrVgB4eHiwbds2PvnkEz755BMOHDjAzJkzefvtt1m7di3Nmzev3AcQERERETNKtkRKkZuby4ULFyyqWydPnuSee+4xq2pdb/+tEydOAKVX4yrK3d2d7777Dl9fX+rXr19mfzs7Ox5++GEefvhhAPbt20dUVBQrV67kxRdfrLS4RERERMRSzS0TiNyiJUuWmL3etWsXp0+fpnfv3hZ9U1JS+O9//2t6XVhYyKpVq7CxsaFnz56VFtPQoUMpKSlhwYIFpV7/6aefTD+Xdrx727ZtAbh06VKlxSQiIiIipVNlS6QUzs7O7Nq1i/Pnz9O5c2fT0e/33ntvqUe5e3p6EhISQmhoKPXr12fr1q0cPnyYJ598kvvuu6/S4urfvz/BwcEkJCTw1Vdf0bNnT5ydnfnvf//Lf/7zH86cOWM6wCMyMpIGDRrQsWNH7rvvPnJzc0lOTsbKysrsC6RFREREpGoo2RIphYODA/Hx8bz66qvMnz8fo9FI9+7diYmJoUmTJhb9x44dS15eHgkJCfzwww/88Y9/5IUXXmDcuHGVHttrr72Gn58fSUlJLF68mKKiIho3bkzbtm3Njr8fNWoU27dvJzExkUuXLuHk5MQDDzzAzJkzTScdioiIiEjVsTLqTGgRM2FhYZw9e7ZcR7anp6cTHh7Oa6+9RnBw8G2I7vbKzMykV69eeAbGYOugL0EWEbnbpMzXSgaRirr+OSg1NRVXV9cKzaE9WyIiIiIiIlVAywhFpEzLZvSp8P/oiIhI9SksKsbOtvSvFhGRqqfKloiIiEgNpURLpHqpsiXyG2vWrCl3Xz8/P44dO1aF0YiIiIjI3UqVLRERERERkSqgZEtERESkhigsKq7uEETkV7SMUETKNOGVXTr6XUTkLqCj3kXuLKpsiZRDZmYmBoOB2NjYMvvGxMRgMBhuQ1QiIiIicidTZUtqpOtfNvxrDg4OeHp6EhQUxNixY7GxubNOaMrMzCQ5OZnevXvzwAMPVHc4IiIiInKLlGxJjTZo0CB69OiB0Wjk/PnzJCcn8+qrr3LixAlmz55dJfecPXs2f//732963NmzZ1m4cCHNmjVTsiUiIiJSAyjZkhqtbdu2BAX9b/366NGjGTBgAOvXr+fpp5/m3nvvrfR72traVvqcIiIiInL30Z4tqVUcHR3p0KEDRqORM2fOsGjRIsaMGUPXrl3x8vLikUce4cUXXyQ7O7tc83300Ud06NCB0aNHc+nSJaD0PVs//vgjzz//PD179sTLywt/f39CQ0NJTk4GYNOmTaZlj88//zwGgwGDwUBYWBgAJSUl5Y711/vL9uzZw7Bhw/D29qZbt27MnTuXa9eu3dJ7KCIiIiLlo8qW1CrXkyz4JfFavnw5ffv2pVevXtjb23P48GE2btzI559/zsaNG7Gzs7vhXMnJycycOZOePXsyf/586tatW2q/a9eu8ac//Ylz584xevRoPDw8yMvL49ixY3z22Wc89thjdOrUiccff5y4uDhGjhzJQw89BGCqvBUVFd10rPv27WPdunWEhoYybNgwUlNTWbFiBY0aNeLxxx+vjLdTRERERH6Hki2p0S5fvkxWVhYA58+fJyEhgaNHj9K+fXtat25NWloa9erVM/UfNWoUHTp0YObMmezevZtHH3201HkXL17MggULGDVqFH/729+wtr5xkfjEiROcPn2aZ599lokTJ5bax83NjYCAAOLi4mjfvr3Z0kcAOzu7m471xIkTbN26FVdXV1P/wYMHk5CQoGRLRERE5DbQMkKp0WJjY/H398ff35+goCA2btxIYGAg//znP7GysjIlL8XFxeTm5pKVlUWXLl0AOHTokMV8JSUlzJo1iwULFvD000/z0ksv/W6iBdCgQQPglxMSL168WKHnqEisvXr1MiVa1+fw8/PjwoUL5OfnVygOERERESk/VbakRhs5ciT9+/fHysoKe3t7PDw8cHJyMl3ftm0bK1eu5JtvvqGoqMhs7PU9WL8WHx9Pfn4+06ZNK3d1qFmzZjz++OMsWbKEbt268cADD9ClSxf69++Pj49PuZ/lZmN1c3OzaLv+7Dk5OdSvX7/c9xYRERGRm6dkS2o0d3d3AgICSr32/vvvM23aNHx8fHjhhRe47777qFu3LsXFxUyYMAGj0WgxpmvXrhw8eJCkpCQGDhxYakJTmmnTpjF8+HD27t3LZ599xoYNG1i+fDkTJkzgz3/+c5njKxLr732PWGn9RURERKRyKdmSWuu9996jbt26rF69Gnt7e1P7yZMnbzimdevWTJkyhXHjxjF27Fji4+Px8PAo1/3c3NwICwsjLCyMq1evEhkZybJly4iIiOCee+7BysqqUmMVERERkeqlPVtSa9nY2GBlZUVJSYmpzWg0smjRot8d16pVK9asWUNxcTFjx44tM+H5+eefLZb91a1blxYtWgD/WwLo4OBg9royYhURERGR6qPKltRa/fr1Y+fOnYwbN46hQ4dy7do1du/ezeXLl8sc27JlSxISEhg3bhzh4eGsWrWKVq1aldo3PT2dv/71r/Tt2xdPT0/q16/PkSNH2LBhA76+vqak6/7776d+/fqsW7eOevXq0bBhQ1xcXPD397+lWEVERESkeijZklpr4MCB5Ofns2rVKubOnUujRo3o2bMn06dPx8/Pr8zxHh4eZgnXypUradOmjUU/g8FAnz59OHDgACkpKZSUlHDfffcxadIkIiIiTP3q1avHG2+8wZtvvsmrr75KYWEhnTt3xt/f/5ZjFREREZHbz8qonfIicgOZmZn06tULz8AYbB1cqjscEREpQ8r8oLI7iUi5XP8clJqaavZ1OjdDlS0RKdOyGX0q/EtGRERun8KiYuxsb3warYjcXjogQ0RERKSGUKIlcmdRsiUiIiIiIlIFlGyJiIiIiIhUASVbIiIiInepwqLi6g5BRH6HDsgQkTJNeGWXTiMUEbkD6fRBkTubKlsiIiIiIiJVQMmWyF0uLCyMwMDA6g5DRERERH5DywilVrp8+TKJiYm8//77nDhxgvz8fBo1akS7du0YMGAAQ4YMoU4d/fMQERERkYrTp0mpdc6cOUNUVBTfffcdAQEBREVF4ezszMWLF/nkk094/vnnOXHiBH/5y1+qO1QRERERuYsp2ZJa5cqVK0yaNInMzExiY2Pp27ev2fWoqCgOHTrE4cOHqylCEREREakplGxJrbJ+/XpOnz7NxIkTLRKt63x8fPDx8TG9TktLY8OGDRw+fJgLFy5gZ2eHj48Pjz/+OJ07dzYbGxYWxtmzZ3nnnXeYO3cuH330EYWFhXTs2JGZM2fi6elp6puXl8fSpUvZv38/33//Pfn5+dx3333069eP6Oho7O3tzea+dOkS8+bNY9euXVy9ehVvb2+ee+65Up/hZmIWERERkaqhZEtqlZ07dwIwcuTIco9JTk7m0qVLDB06lKZNm3Lu3DnWr1/P+PHjWb16NR07djTrX1BQwNixY/H19WXatGlkZmayevVqnnzySbZu3YqNjQ0A586dY8OGDfTt25dBgwZRp04dDhw4wLJly/jmm29Yvny5ac6ioiIiIyM5fPgwQUFB+Pr6cvToUf70pz/h5OR0yzGLiIiISOVTsiW1yrfffoujoyNubm7lHjN79mwcHBzM2kJDQxk4cCCLFy+2SFyys7OJjIxk4sSJpjYXFxfmzZvH/v376d69OwBubm7s3bsXW1tbU78xY8bw5ptvsmjRIg4dOmSqsG3atInDhw8THR3NlClTTP1btmzJa6+9RrNmzW4pZhERERGpfDr6XWqVvLw86tevf1Njfp205Ofnk52djbW1Nb6+vhw6dMiiv7W1NeHh4WZtXbp0AX45nOM6Ozs7U6J17do1Ll26RFZWFgEBAQB8+eWXpr67d+/GxsaGiIgIs3lHjx6No6PjLccsIiIiIpVPlS2pVRwdHcnPz7+pMd9//z1vvPEGaWlp5Obmml2zsrKy6N+kSRPq1q1r1nZ9qV9OTo5Z+9q1a3n33Xc5ceIEJSUlZtcuXbpk+jkjI4PGjRtbJFZ2dna4ublZxHWzMYuIiIhI5VOyJbVKq1atOHjwIBkZGeVaSpifn8+YMWO4fPky48aNo3Xr1tSvXx9ra2sWL17Mp59+ajHm+p6s0hiNRtPPK1euZM6cOXTr1o3w8HCaNGmCra0t586dIyYmxqzvzahIzCIiIiJS+ZRsSa3St29fDh48yPr163nmmWfK7P/JJ59w/vx5Xn31VYYNG2Z27c0337ylWN577z2aNWvG0qVLsbb+34reDz/80KKvm5sbH3/8MXl5eWbVrcLCQjIyMmjUqNFtiVlEREREyk97tqRWCQkJwdPTkxUrVrB79+5S+xw5coS1a9cC/6tS/bbKlJaWZranqiKsra2xsrIym/vatWssXbrUom+vXr0oLi5mxYoVZu3r1q0jLy/PrK0qYxYRERGR8lNlS2oVe3t7Fi9eTFRUFNHR0XTr1o2AgACcnJzIysoiPT2dtLQ0JkyYAMBDDz1E48aNmTt3LmfPnqVp06Z88803vPfee7Ru3Zrjx49XOJb+/fszf/58Jk6cSJ8+fcjLy2Pr1q3UqWP5zzI4OJikpCT++c9/kpmZSfv27fnmm2/YsWMHzZs3p7i42NS3KmMWERERkfJTsiW1jru7O5s3byYxMZGdO3cSFxdHQUEBjRo1wsvLizlz5jB48GAAGjZsyLJly5g3bx4JCQlcu3YNLy8vli5dyoYNG24pcYmMjMRoNLJhwwZeeeUVGjduzIABAxg2bBiPPvqoWV87OztWrFjBP/7xD1JTU3n//ffx9vY2tZ09e9bUtypjFhEREZHyszJWdBe+iNR4mZmZ9OrVC8/AGGwdXKo7HBER+Y2U+UHVHYJIjXX9c1Bqaiqurq4VmkOVLREp07IZfSr8S0ZERKpOYVExdrY3PgVXRKqXDsgQERERuUsp0RK5synZEhERERERqQJKtkRERERERKqAki0RERGRu0BhUXHZnUTkjqIDMkSkTBNe2aXTCEVEqplOHhS5+6iyJVKLBAYGEhYWVt1hiIiIiNQKSrZEbkF6ejoGg4Hly5dXdygiIiIicodRsiUiIiIiIlIFlGyJ3CHy8vKqOwQRERERqUQ6IEOkkm3evJmEhAS+++47rl27xj333EP79u2ZMWMGLi6/HDIRFhbG2bNniY+PZ968eXz66adcunSJY8eOUVJSwuLFi0lLS+O7777j0qVL3HvvvTz88MNMnToVZ2dni3tu27aNNWvWcPToUUpKSmjdujWRkZH079//dj++iIiIiPx/SrZEKtHmzZt57rnn6NixI1OmTKFevXr8+OOP7Nv3/9i787ga0/4P4J/TrkQiy7QI6dBTEdKCQYulMVosScJYRsjYxjxllmdmjKEZpp6JIUIm62ihslbGWCuNJZJlQgrDkDbVtJ3fH36dx5lzUicnoc/79er1cq77uq/7ex+H1/l2Xdf3/g2PHz8WJ1sA8PTpU0yaNAl9+vTBggULkJeXBwCoqKjApk2bMGzYMDg6OqJFixa4dOkSoqKicO7cOURFRUFNTU08TlBQENavX49BgwZh/vz5UFJSQkJCAubPn48vvvgC3t7er/x9ICIiIiImW0QKlZiYCC0tLWzduhUqKv/75zV//nypvvn5+fD19cXChQsl2tXU1HDy5EloaGiI27y8vGBlZYXPPvsMiYmJcHFxAQBkZGRg/fr1mDVrFhYtWiTuP3nyZMyZMwerV6+Gq6srWrZsqehbJSIiIqI6cM8WkQJpa2ujrKwMx44dg0gkqrP/9OnTpdoEAoE40aqqqkJhYSHy8vJga2sLAEhPTxf3jYuLg0AggJubG/Ly8iR+HBwc8PTpU1y4cEFBd0dERERE8uDMFpECzZo1C2fPnsXcuXOho5Njz+MAACAASURBVKOD/v37491338XIkSOlZpd0dXXRqlUrmeMcOHAAW7ZsQWZmJioqKiSOFRQUiP+clZUFkUiEkSNH1hrTo0ePXuKOiIiIiKihmGwRKZCxsTEOHDiAM2fO4MyZM0hNTcVnn32GH3/8Edu3b4eRkZG4b4sWLWSOceTIESxcuBCWlpZYunQpOnXqBHV1dVRVVWHGjBkSM2YikQgCgQAbN26EsrKyzPFMTEwUe5NEREREVC9MtogUTE1NDYMHD8bgwYMBAL/99hs+/PBDbNmyBf/5z3/qPH/fvn1QV1fHzz//LJGQZWVlSfU1NjbGiRMn8M4776Bbt26KuwkiIiIiemncs0WkQDUVBZ9nZmYGQHL534soKytDIBCgurpa3CYSibBu3TqpvqNHjwYA/PDDD6iqqpI6ziWERERERE2HM1tECjR9+nRoa2ujX79+6NSpEwoLCxETEwOBQABXV9d6jTF8+HAcPnwYU6ZMgZubGyorK5GYmIjS0lKpvpaWlpg3bx5CQkLg5uaG4cOHo0OHDnj48CEyMjJw/PhxXL58WdG3SURERET1wGSLSIG8vLxw8OBB7N69GwUFBdDR0UHPnj3x2WefiasJ1uW9997D06dPER4ejsDAQLRu3RpDhw7F4sWLYWNjI9Xfz88P5ubmiIiIwM8//4ySkhK0bdsW3bt3x6effqroWyQiIiKiehKI6lOfmoiapdzcXDg6OqKLgz9UNXXrPoGIiBpN3Or6rZAgIsWo+R6UlJQEAwODBo3BmS0iqlPYp84N/k+GiIgUo7yiCmqqsivPEtHriQUyiIiIiN4ATLSI3jxMtoiIiIiIiBoBky0iIiIiIqJGwGSLiIiIiIioETDZIiIiInpNlVdIP7CeiN4crEZIRHWasTyBpd+JiJoAy70Tvdk4s0VvtNzcXAiFQoSEhDR1KLUSCoXw9/dv6jBemziIiIiImgvObFGjysnJwYYNG3D27Fncv38fampqaNeuHSwtLeHu7g5bW1uFX7OwsBBbt25F//79YWNjo/DxiYiIiIjqg8kWNZpLly7Bx8cHKioqcHNzg4mJCcrKypCdnY1Tp05BS0vrpZMtfX19pKenQ1n5f88eKSwsxJo1a+Dn58dki4iIiIiaDJMtajRr165FaWkp9u3bhx49ekgd/+uvv176GgKBAOrq6i89jqKVlZVBRUUFKir8J0ZERETUXHHPFjWa27dvQ0dHR2aiBQB6enoAAB8fHzg4OEgci4+Ph1AoxOjRoyXad+zYAaFQiIsXLwKQ3rOVkpICR0dHAMCaNWsgFAohFArF4/v4+Ijb/vnzzxhu376NJUuWYODAgTA3N4eDgwMCAwNRUlIi0c/f3x9CoRB5eXkICAiAvb09evfujT///LPW9+bAgQPw9fXFkCFDYG5uDhsbG8yZMwdXr16V6uvg4AAfHx9kZWXhww8/hJWVFfr27YuPPvpIZsJ648YNTJ8+Hb1790b//v2xePFiPH78uNZYiIiIiKhx8Nfu1GiMjIxw69YtHDlyBMOGDau1n62tLX788UfcuXMHRkZGAIAzZ85ASUkJ169fR15eHnR1n1XCS05ORsuWLWFubi5zrG7duiEgIAArVqyAs7MznJ2dAQBaWloAAF9fX4wdO1binJycHISEhKBt27bitsuXL2PKlClo1aoVPD090aFDB1y9ehURERE4f/48IiIioKqqKjHOBx98gHbt2mHOnDkoKSmBpqZmrfe8bds26OjoYPz48dDT08OdO3fwyy+/wMvLCzExMTA2Npbo/+DBA0yePBlOTk745JNPcPXqVezevRvFxcXYvHmzxL14e3ujvLwc3t7e6NSpE3799VfMmDGj1liIiIiIqHEw2aJGM3v2bJw+fRrz5s2DsbEx+vTpAwsLC9jY2KBbt27ifjXJVnJysjjZSk5OxqhRoxAbG4vk5GS4uLhAJBIhNTUV1tbWEnu0nteuXTs4OTlhxYoVEAqFcHWVLJk7YMAAidcFBQXw9PSEjo4OVq9eLW5funQp9PT0EBkZiZYtW4rb7ezs4Ofnh7i4OHh4eEiM1b17d6xatape701YWJhUMubm5gZXV1eEh4fjyy+/lDiWnZ2NoKAguLi4iNuUlJSwY8cO3Lx5E127dgUABAcHo6CgAFu3bhXvh/P29oafnx+uXLlSr9iIiIiISDG4jJAajZWVFaKiouDu7o6ioiJER0fjq6++gouLC7y9vZGTkwMAsLS0hKamJpKTkwEAd+/eRW5uLkaNGgVTU1Nx+7Vr1/DkyROFVTCsqKjAvHnzkJubi7Vr14oTvWvXruHatWsYNWoUysvLkZeXJ/7p27cvNDU1cerUKanxpk+fXu9r1yRaIpEIxcXFyMvLQ5s2bdClSxekp6dL9W/fvr1EogVA/D5kZ2cDAKqrq3H06FGYm5tLvEcCgYAzW0RERERNgDNb1KiEQiFWrlwJ4FkSdfbsWezZswdpaWmYM2cOoqKioKamhr59+yIlJQXAsyWEKioq6NevH2xsbHD8+HEAECddikq2vvjiC6SkpCAwMBD9+vUTt2dlZQEAQkJCan1+16NHj6Ta/rn070WuXLmC//73v0hNTZXaA2ZgYCDV39DQUKpNR0cHAJCfnw8AePz4MUpKSsSzXM8zMTGpd2xEREREpBhMtuiV0dfXh76+PlxdXTFx4kScO3cO6enp6NevH2xtbXHixAncuHEDycnJsLCwEJeGj4iIwL1795CcnIw2bdpAKBS+dCzr169HdHQ0Zs+eDTc3N5l9pk2bhkGDBsk81qpVK6m2Fi1a1Ova9+7dg7e3N1q2bInZs2eja9euaNGiBQQCAb799lup5AtArcsmgWezY0RERET0+mGyRa+cQCBAr169cO7cOTx8+BDA/2arzpw5g+TkZHERCxsbGygrK+PUqVNIS0uDvb09BAJBneO/yIEDBxAcHAwXFxfMnz9f6njnzp0BPNsTZW9vL/f91SUhIQElJSVYt26d1Cxdfn4+1NTUGjSurq4uNDU1cfPmTaljf/zxR4PGJCIiIqKG454tajSnTp1CZWWlVHtZWZl4z1NNoQwzMzO0bt0au3btwl9//SVOQrS1tWFmZobw8HAUFRXVawlhzX6ogoICqWMXLlyAv78/evXqhZUrV8pMzMzMzGBqaopdu3aJ95U9r7KyUrx0ryFqZqn+OSP1yy+/vNSzx5SVlTF06FBcvnxZvOSy5jphYWENHpeIiIiIGoYzW9RoVqxYgfz8fDg4OMDU1BQaGhr4888/ERcXh9u3b8PNzU28JFBJSQnW1tZITEyEuro6+vTpIx7H1tYWGzduFP+5Lm3atEHnzp2xf/9+GBoaol27dmjRogUcHBwwZ84cVFZWYsSIETh06JDEeVpaWnBycoJAIMB3332HKVOmYPTo0RgzZgxMTExQVlaG7OxsJCQkYNGiRVLVCOvr3XffRYsWLfDJJ59g0qRJaNWqFc6dO4fjx4/DyMgIVVVVDRoXABYsWIDjx4/D19cXkyZNQseOHfHrr78iLy+vwWMSERERUcMw2aJG4+/vj6SkJPz+++84fPgwioqKoK2tDVNTU8ycOVMqWbG1tUViYiKsrKwkltLZ2dlh48aN6NChg8ziD7KsWrUK3377LYKCglBaWgp9fX04ODiIH+5bU7Tjefr6+nBycgIA9OzZEzExMQgNDcXRo0exa9cuaGlpQV9fH+7u7rCzs2vo2wIjIyNs3LgRP/zwA9avXw9lZWX06dMHERERWLZsGe7evftSY2/fvh2BgYHYtm0b1NTUMGjQIHz33XeNsiSSiIiIiGonEHF3PRHVIjc3F46Ojuji4A9VTd2mDoeIqNmJW+1adyciahQ134OSkpJkVouuD85sEVGdwj51bvB/MkRE1HDlFVVQU629Ii0Rvd5YIIOIiIjoNcVEi+jNxmSLiIiIiIioETDZIiIiIiIiagRMtoiIiIiaUHlFwx/5QUSvNxbIIKI6zViewGqERESNhBUHid5enNkieoP5+/uLHwxNRERERK8XJltEzykoKIClpSWEQiH27t37Sq6Zm5uLkJAQZGZmvpLrEREREdGrwWSL6DlxcXEoLy+HgYEBoqKiXsk17969izVr1jQo2Vq2bBnS09MbISoiIiIiellMtoieExkZCRsbG0yZMgVnz55FTk5OU4ckRSQS4enTpwAAVVVVqKurN3FERERERCQLky2i/5eRkYHMzEy4u7tj1KhRUFFRQWRkpESf3NxcCIVChISESJ0fEhICoVCI3Nxccdv9+/cREBCAoUOHwtzcHHZ2dpgwYQJiYmIAANHR0Zg8eTIAICAgAEKhEEKhED4+PgCAlJQUCIVCREdHY/v27XBxcYGFhQU2b94MQPaeraysLHz55Zd47733YGVlhV69esHDwwN79uxR3JtFRERERHViNUKi/xcZGQlNTU0MGzYMmpqaGDJkCPbu3Yv58+dDSUn+30tUVlbigw8+wIMHDzBx4kQYGxujuLgY165dQ1paGtzd3WFtbQ1fX1+sX78enp6e6Nu3LwCgXbt2EmNt3boV+fn5GDduHPT09NCxY8dar5uamoq0tDQMGTIEBgYGKC0txaFDh/DZZ58hLy8Ps2bNkvteiIiIiEh+TLaIAPz999+Ij4/H8OHDoampCQBwc3NDQkICTpw4gcGDB8s95h9//IFbt27h448/xsyZM2X2MTQ0hL29PdavX4/evXvD1VV2+d/79+/j4MGDaNu2bZ3XdXV1hZeXl0Tb1KlTMWXKFGzYsAHTpk2Dqqqq3PdDRERERPLhMkIiAEeOHEFhYSHc3NzEbYMHD4aurm6DC2Voa2sDeLYU8PHjxy8Vn6ura70SLQDiZBF4lkQ+efIE+fn5GDBgAIqLi3Hz5s2XioWIiIiI6oczW0R4toRQV1cXHTt2RHZ2trh9wIABOHToEPLy8qCrK99DffX19eHr64sNGzZg4MCB6NmzJ2xtbTFixAhYWlrKNZaxsXG9+z59+hRr1qzBwYMHcf/+fanjhYWFcl2biIiIiBqGyRY1ezk5OUhJSYFIJMLw4cNl9omNjcXUqVMhEAhqHaeyslKqbeHChRg7diyOHTuGtLQ0REZGYtOmTZgxYwaWLFlS7xhbtGhR776LFy/GsWPHMH78eFhbW0NHRwfKysr47bffEB4ejurq6nqPRUREREQNx2SLmr3o6GiIRCJ888034qV/zwsODkZUVBSmTp2K1q1bA3j28ON/er4K4fMMDQ3h4+MDHx8f/P3335g+fTrCwsIwbdo0tG3b9oUJnLwKCwtx7NgxuLq64uuvv5Y4dvr0aYVdh4iIiIjqxmSLmrXq6mrExMTA1NQU48aNk9nnjz/+QEhICNLT02FpaQk9PT0kJydDJBKJE6WcnBwkJiZKnFdUVAQNDQ2JYhTq6uro2rUrzp49i4KCArRt21a8x0pWAievmqqJIpFIov3hw4cs/U5ERET0ijHZombt5MmTuH//PsaOHVtrn2HDhiEkJASRkZGwtLSEt7c3goODMWPGDDg5OeHhw4fYtWsXunfvjkuXLonPS0lJweeff45hw4ahS5cu0NLSwuXLlxEZGYlevXqha9euAAATExNoaWlhx44d0NDQQKtWraCrqws7Ozu576dly5YYMGAAYmNjoaGhAQsLC9y9exe7d++GgYEB8vPz5X+TiIiIiKhBmGxRs1bz0GJnZ+da+5iamsLY2BgHDhzA0qVLMXPmTBQVFSE2NhapqakwMTHB8uXLkZGRIZFsCYVCODs7IzU1FXFxcaiurkanTp0wa9YsTJs2TdxPQ0MDQUFBCA4Oxrfffovy8nL079+/QckWAHz//fdYvXo1jh49ipiYGBgbG2PhwoVQUVFBQEBAg8YkIiIiIvkJRP9cb0RE9P9yc3Ph6OiILg7+UNWUrxojERHVT9xq2c9YJKKmVfM9KCkpCQYGBg0ag8/ZIiIiIiIiagRcRkhEdQr71LnBv9EhIqIXK6+ogpqqclOHQUSNgDNbRERERE2IiRbR24vJFhERERERUSNgskVERERERNQImGwRERERvULlFVVNHQIRvSIskEFEdZqxPIGl34mIFISl3omaD85sEb1m/P39IRQKmzoMIiIiInpJnNmiZiMnJwcbNmzA2bNncf/+faipqaFdu3awtLSEu7s7bG1tmzpEIiIiInqLMNmiZuHSpUvw8fGBiooK3NzcYGJigrKyMmRnZ+PUqVPQ0tJiskVERERECsVki5qFtWvXorS0FPv27UOPHj2kjv/1119NEBURERERvc24Z4uahdu3b0NHR0dmogUAenp6AAAfHx84ODhIHIuPj4dQKMTo0aMl2nfs2AGhUIiLFy+K20QiEXbs2AEPDw/06tULVlZW8PHxQXJystQ1//77bwQGBmLgwIGwtLTE2LFjcfLkyRfew5IlSzBw4ECYm5vDwcEBgYGBKCkpkehXs+erqKgI//nPf2BnZwcLCwtMmDBBIlYiIiIialxMtqhZMDIyQn5+Po4cOfLCfra2trh79y7u3Lkjbjtz5gyUlJRw/fp15OXliduTk5PRsmVLmJubi9uWLFmCZcuWwcjICEuWLMG8efNQXFyMadOmISkpSeJaixYtwubNm2Fubo5///vf6Nu3L+bNm4eMjAypuC5fvowxY8YgLS0Nnp6e+OKLLzBkyBBERERg2rRpqKiokDpn+vTpePDgAebOnYtZs2bhxo0b+PDDD1FcXFzv942IiIiIGo7LCKlZmD17Nk6fPo158+bB2NgYffr0gYWFBWxsbNCtWzdxP1tbW/z4449ITk6GkZERgGdJ1ahRoxAbG4vk5GS4uLhAJBIhNTUV1tbWUFZWBgAkJCQgLi4OX3/9NTw9PcVjTp48GePHj8fy5cvh4OAAgUCAkydPIjExEe7u7li5cqW4r7W1NebOnSsV/9KlS6Gnp4fIyEi0bNlS3G5nZwc/Pz/ExcXBw8ND4hwzMzN8+eWX4tfdunXDggULEB8fjwkTJrzcG0pEREREdeLMFjULVlZWiIqKgru7O4qKihAdHY2vvvoKLi4u8Pb2Rk5ODgDA0tISmpqa4mV/d+/eRW5uLkaNGgVTU1Nx+7Vr1/DkyROJohqxsbHQ0tKCk5MT8vLyxD+FhYVwcHDA3bt3cfv2bQBAYmIigGezT89zcnJCly5dJNquXbuGa9euYdSoUSgvL5cYu2/fvtDU1MSpU6ek7nnq1KkSr2tizc7ObuC7SERERETy4MwWNRtCoVA8i3T37l2cPXsWe/bsQVpaGubMmYOoqCioqamhb9++SElJAfBsCaGKigr69esHGxsbHD9+HADESdfzyVZWVhaePn0Ke3v7WmN4/PgxunTpgpycHCgpKcHY2FiqT7du3XDr1i2JcQEgJCQEISEhMsd99OiRVJuhoaHE6zZt2gAA8vPza42PiIiIiBSHyRY1S/r6+tDX14erqysmTpyIc+fOIT09Hf369YOtrS1OnDiBGzduIDk5GRYWFuLS8BEREbh37x6Sk5PRpk0biYcPi0Qi6OrqYvXq1bVet3v37g2Oedq0aRg0aJDMY61atZJqq1ne+E8ikajBMRARERFR/THZomZNIBCgV69eOHfuHB4+fAjgf7NVZ86cQXJyMsaOHQsAsLGxgbKyMk6dOoW0tDTY29tDIBCIx+rcuTNu376NXr16QUtL64XXNTQ0RHV1NW7fvi2VgNXMZD0/LgAoKSm9cNaMiIiIiF4v3LNFzcKpU6dQWVkp1V5WVibe71RTKMPMzAytW7fGrl278Ndff4mTL21tbZiZmSE8PBxFRUVSD0F2c3NDdXU1fvjhB5kxPL/Uz9HREQCwadMmiT6JiYkSSwhr4jE1NcWuXbvEe8ueV1lZyaWBRERERK8hzmxRs7BixQrk5+fDwcEBpqam0NDQwJ9//om4uDjcvn0bbm5u4iWBSkpKsLa2RmJiItTV1dGnTx/xOLa2tti4caP4z88bMWIEPDw8sG3bNmRkZGDo0KFo06YN/vzzT1y4cAHZ2dni8u+DBg3C0KFDERMTg/z8fAwaNAg5OTnYvXs3TE1Ncf36dfG4AoEA3333HaZMmYLRo0djzJgxMDExQVlZGbKzs5GQkIBFixZJVSMkIiIioqbFZIuaBX9/fyQlJeH333/H4cOHUVRUBG1tbZiammLmzJlSiYqtrS0SExNhZWUFNTU1cbudnR02btyIDh06oGvXrlLXWbFiBWxsbPDLL78gNDQUFRUV0NPTg5mZGRYvXizRNzg4GMHBwYiLi8Pp06dhamqKkJAQxMfHSyRbANCzZ0/ExMQgNDQUR48exa5du6ClpQV9fX24u7vDzs5Oge8WERERESmCQMTd8kRUi9zcXDg6OqKLgz9UNXWbOhwiordC3GrXpg6BiOqh5ntQUlISDAwMGjQGZ7aIqE5hnzo3+D8ZIiKSVF5RBTVV2RVjiejtopACGRUVFbhy5Qpu3rypiOGIiIiI3lpMtIiaD7mSrQMHDmD+/PkSlc/u3LmDUaNGYcyYMXjvvffg5+cns+obERERERFRcyJXshUVFYWbN29CR0dH3LZy5UpkZ2fDxsYGQqEQSUlJiI6OVnigREREREREbxK5kq2srCxYWFiIXxcXF+P48eMYOXIkwsPDsWfPHnTt2pXJFhEREb21yiuqmjoEInpDyFUgIy8vD3p6euLX58+fR2VlJd577z0AgKqqKuzt7bF//37FRklETWrG8gRWIyQi+n+sJkhE9SXXzJaWlhaKi4vFr8+ePQuBQCDx0Fd1dXU8ffpUcRESERERERG9geRKtjp37ozjx4+jvLwc5eXlOHjwIIRCIXR1//cb73v37qFt27YKD5SouUpJSYFQKOTyXCIiIqI3jFzLCD09PREQEIBhw4ZBRUUFd+/eRUBAgESfjIwMmJiYKDRIooZKSUnB5MmTaz2urKyMK1euvMKIiIiIiKi5kCvZcnd3x61bt7B7924AgLe3N3x8fMTHz507h+zsbIwfP16xURK9pFGjRuHdd9+ValdSUsij5oiIiIiIpMiVbAHAokWLsGjRIpnHzM3NcfbsWbRo0eKlAyNSJDMzM7i6ckNzjaqqKpSXl/PfKhEREVEjUuiv9dXU1KCtrQ0VFblzOKLXwuHDh+Hj44N+/fqhV69eGD58OL755huUl5cDAKKjoyEUCpGSkiJ1ro+PDxwcHCTaTp48iQULFsDR0RGWlpbo168fpk2bhtTUVJnXT0xMhJubGywsLDB48GAEBwfX+pDwvLw8fPXVVxg8eDDMzc0xePBgfPXVV3jy5IlEv5qYT58+jbVr18LJyQmWlpY4ePBgQ94iIiIiIqqnBmVFV69eRXx8PLKyslBaWorw8HAAQG5uLtLT0zFgwAC0bt1akXESvZTS0lLk5eVJtaupqaFly5YAgKCgIKxfvx4mJiaYOnUq9PT0cOfOHRw5cgQfffQR1NTU5L5uTEwMCgoK4Obmho4dO+LBgwfYs2cPpk6dip9//hn9+vUT901ISMC8efOgr6+PuXPnQllZGdHR0fjtt9+kxi0qKoKXlxeys7MxZswYmJmZITMzEzt37kRycjL27Nkjvq8agYGBqKysxPjx46GlpYUuXbrIfT9EREREVH9yJ1v//e9/ERoaiurqagCAQCAQHxOJRFi8eDGWLl0qsZeLqKmFhIQgJCREqn3IkCEIDQ1Feno61q9fDxsbG2zcuBHq6uriPh9//HGDr7ts2TJoampKtE2YMAHvvfceQkNDxclWVVUVli9fjtatW2PPnj3iCp8TJkzA6NGjpcYNCwvD7du38cUXX8Db21vc3rNnT3z99dcICwvDggULJM4pKyvD3r17uXSQiIiI6BWRaxnh/v37sW7dOtjb22Pv3r2YNWuWxHFDQ0OYm5vj6NGjCg2S6GV5enpiy5YtUj8LFy4EAMTGxgIAFi9eLJFoAc9+ofD8LxXk8Xyi9fTpUzx58gRKSkro1asX0tPTxccyMjJw//59eHh4SDxKQVtbGxMmTJAaNyEhAbq6uvD09JS6T11dXSQmJkqd4+XlxUSLiIiI6BWSa2YrIiICnTt3xk8//QQ1NTWZX+i6detW634UoqbSuXNn2Nvb13o8OzsbAoEAPXr0UOh179y5g6CgIJw8eRKFhYUSx55P4HJycgAAXbt2lRqjW7duUm25ubkwNzeX2h+poqICY2NjmeXsuWyQiIiI6NWSK9m6du0aPDw8Xrh3pX379nj06NFLB0b0qtVnButFx/9ZyOLp06fw9vZGaWkppkyZAlNTU2hpaUFJSQmhoaFITk5WSNz1paGh8UqvR0RERNTcyV2NsK4vo48ePZJahkX0ujM2NkZ1dTWuXr36wn41hV8KCgqkjuXm5kq8PnPmDB4+fIiAgADMmzcPw4cPx8CBA2Fvb4/S0lKJvoaGhgCAmzdvSo2blZUl1WZoaIhbt25JJXiVlZW4ffu2eDwiIiIiajpyJVudO3fG+fPnaz1eXV2N33//HSYmJi8dGNGr9P777wMAfvjhB3GZ9+eJRCIAz5IyADh9+rTE8fj4eDx8+FCiTVlZWeLcGidPnsTFixcl2v71r3+hY8eOiI6OlqiaWFxcjF27dknF4+TkhLy8POzZs0ei/ZdffkFeXh6cnJxqvVciIiIiejXkWkY4cuRIBAcHY/PmzZg2bZrU8fXr1+POnTuYPHmywgIkUoQrV65g3759Mo/VPHdq5syZ2LhxIzw8PDBy5Ejo6ekhNzcXhw8fxp49e9CqVSt07doV9vb22L17N0QiEXr27InMzEwkJiaic+fOEjNNffv2hZ6eHgIDA3H37l107NgRmZmZ2LdvH0xNTXH9+nVxX2VlZQQEBGDBggUYN24cxo8fD2VlZURFRUFHRwf37t2TiHnGjBk4dOgQvv76a1y5ckUcR2RkJLp06YIZM2Y0zhtJRERERPUmV7I1ZcoUMiWjbgAAIABJREFUHDp0CN9//z0OHjwoXlIYGBiItLQ0XL58Gb169ZKqkEbU1OLj4xEfHy/z2JEjR6ClpYWPP/4YPXr0wLZt2xAWFgaRSISOHTvi3Xffldjv9N1332HZsmWIi4tDbGws+vbti59//hlffvkl7t69K+7XqlUrhIWF4fvvv8e2bdtQWVkJc3NzbNy4EZGRkRLJFgCMGDECP/74I9auXYuQkBC0bdsW7u7usLa2lvrlhra2Nnbu3Ikff/wRR48eRXR0NNq2bYsJEyZg3rx5Us/YIiIiIqJXTyD65xqnOhQVFWH58uWIi4tDVVWVuF1JSQnvv/8+Pv/8c37RI3pL5ObmwtHREV0c/KGqqVv3CUREzUDcatemDoGIXoGa70FJSUkwMDBo0BhyP9RYW1sbK1euhL+/Py5duoT8/Hxoa2vD0tJS4vlARPT2CPvUucH/yRARvW3KK6qgpqrc1GEQ0RtA7mSrho6ODgYNGqTIWIiIiIhee0y0iKi+5C79TkRERERERHV74cxWQEAABAIBFi1ahHbt2iEgIKBegwoEAnz77bcKCZCIiIiIiOhN9MJkKyYmBgKBADNnzkS7du0QExNTr0GZbBEREdGbjPuyiEgRXphsJSUlAQA6dOgg8ZqImpcZyxNYjZCImhVWHCQiRXjhni19fX3o6+tDRUVF4nV9fugZf39/CIXCV35doVAIf3//V35deURHR0MoFCIlJaWpQ5FLSkoKhEIhoqOjmzoUIiIiInqNyVUgw9HREV999VVjxfLK1XzZr+1Lc25ubqMkLYmJiQgJCVHomIpQUFAAS0tLCIVC7N27t6nDqTehUPjCn7S0tKYOUS6v6+eDiIiIiOQjV+n3vLw8aGtrN1Ysb6Vly5ZJJaiJiYmIiYnBvHnzmigq2eLi4lBeXg4DAwNERUXBzc2tqUOqt549e+KDDz6Qeaxr164KvZa1tTXS09PFM76K9rp+PoiIiIhIPnJ9W+zevTvu3LnTWLG8lVRVVZs6hHqLjIyEjY0NHB0d8e233yInJweGhob1Ore4uBgtW7Zs5Ahr16FDB7i6vpr19UpKSlBXV6+zn0gkQklJCbS0tF5BVERERET0upFrGaGPjw9+/fVXXL16tbHiee3VLC0MCQnBr7/+ijFjxsDCwgIDBw5EYGAgKisrJfr/c8+Wj4+PuKrj80vdnl/K+PDhQ/znP//BkCFDYG5ujoEDB+Lzzz/H48ePpeK5ceMGpk+fjt69e6N///5YvHixzH51ycjIQGZmJtzd3TFq1CioqKggMjJSZt+apZVnzpyBl5cXrKysMHv2bADAgwcPsHLlSri6usLa2hoWFhZwcXHBhg0bUFVVJXO8qqoqhISEYOjQoTA3N8f777+P/fv3y30P9eHg4AAfHx9cvXoVU6dOhZWVFezs7LBy5UpUVlbi77//RmBgIAYNGgQLCwt4e3sjKytLYgxZe7aeb9u+fTtcXFxgYWGBzZs3AwDS09Ph7++P4cOHo1evXrCyssKECROQkJAgMbYiPx/5+fn49ttv4eTkBAsLC9jY2MDDwwNhYWEKfU+JiIiISDa5ZrY6duwIOzs7eHl5YcKECbCwsEC7du0gEAik+lpbWyssyNfRb7/9hh07dmDChAkYM2YMkpKSsHnzZrRu3Rq+vr61nufr64vq6mqkpaXhu+++E7f36dMHAHDv3j14enqioqICY8eOhZGREbKzs7Fz506kpKQgKipKvJQzJycH3t7eKC8vh7e3Nzp16oRff/0VM2bMkPt+IiMjoampiWHDhkFTUxNDhgzB3r17MX/+fCgpSefkly9fxuHDhzF+/Hi4u7uL269du4YjR47A2dkZRkZGqKiowIkTJ7B69Wrk5ubi66+/lhpr1apVKCkpgZeXF4Bne+kWLVqEv//+Gx4eHvWKv7KyEnl5eVLtAoEAbdq0kWj7888/8cEHH8DFxQXDhw/HqVOnsGXLFigrK+OPP/5AWVkZPvzwQzx58gSbN2/GnDlzcPDgQZnvwz9t3boV+fn5GDduHPT09NCxY0cAQEJCAm7evIkRI0ZAX18f+fn5iImJgZ+fH1atWoX3338fgGI/H/Pnz0daWhomTJgAoVCIsrIyZGVlITU1tUGfESIiIiKSj1zJlo+PDwQCAUQiEbZs2SIzyaqRmZn50sG9zv744w/Ex8fDwMAAAODl5YX3338f27Zte2GyNWDAAMTFxSEtLU3msrdly5ahsrISe/fuFX9RB4ARI0bA09MT4eHh4r08wcHBKCgowNatW2FrawsA8Pb2hp+fH65cuVLve/n7778RHx+P4cOHQ1NTEwDg5uaGhIQEnDhxAoMHD5Y658aNG9iyZQvs7e0l2vv374+kpCSJz8bUqVOxZMkS7NmzB35+fmjfvr3EOU+ePEFsbKw4SfDy8sLo0aOxcuVKuLi4QENDo857OHnyJOzs7KTaNTU1cf78eYm2O3fuIDg4GCNHjhRfz8PDA5s2bcLQoUMRHh4ujl9HRwfLly/HqVOnMGjQoDrjuH//Pg4ePIi2bdtKtM+ePRuLFy+WaPPx8YGbmxvWrVsnTrYU9fkoKipCcnIyvLy88Pnnn9cZNxEREREpnlzJ1ty5c1+YYDUnjo6O4kQLeDaDYmNjg23btuHp06cN2qdTVFSEY8eOwcPDA2pqahIzNfr6+jAyMsKpU6cwb948VFdX4+jRozA3NxcnWjVxzJgxA4mJifW+7pEjR1BYWChREGPw4MHQ1dVFVFSUzGSrR48eUokWAInEqLy8HCUlJaiursbAgQMRGxuLy5cvw8HBQeIcLy8vicIr2tramDBhAn744QekpKTIvP4/9erVCwsWLJBqV1aWfiBlhw4dxIlWjT59+iAjI0P8C4Ua/fr1AwBkZ2fXK9lydXWVSrQAiJNYACgtLUVZWRlEIhFsbW2xa9eueu15k+fzoa6uDjU1NaSnpyM3N1fis0pEREREr4ZcyVZzrY4mK8GUVThCR0cHwLO9Mg1Jtm7duoXq6mpERkbWul+q5rqPHz9GSUmJzEp7JiYmcl03MjISurq66NixI7Kzs8XtAwYMwKFDh5CXlwddXckH2hobG8scq7KyEhs2bMC+ffuQnZ0NkUgkcbywsFDqHFn30K1bNwDP9sjVR5s2bWQmf7LISjxat24t81irVq0APPs7rY/a3pfHjx8jODgYSUlJMvfUFRYW1plsyfP5UFNTw9KlS7F8+XI4OjrCxMQEtra2cHJykjkDSERERESKJ1eyde/ePbRq1eqFXwqLi4tRWFiId95556WDa2w1szClpaUyj9e0y6o8J2vGpMY/E4z6qjlv9OjREvugnlefKnjyyMnJQUpKCkQiEYYPHy6zT2xsLKZOnSrR1qJFC5l9V65ciYiICLi4uMDX1xe6urpQVVVFRkYGVq1aherqaoXG3xAv+rurbV9Wff9OZb0vIpEI06ZNQ1ZWFiZPngxzc3Noa2tDWVkZUVFRiI+Pr9f7Iu/nw8vLC46Ojvjtt9+QmpqKw4cPY9u2bXBxcUFQUFC97oeIiIiIGk6uZMvR0RF+fn6YO3durX0iIiLw448/vhF7tmpmMW7evCnzeE0VOkUvwaptKaaRkREEAgEqKirqnKXR1dWFpqamzNj/+OOPescSHR0NkUiEb775RuYz1IKDgxEVFSWVbNVm3759sLa2lvoy//yM2T/JuofGeu+bwrVr13D16lXMnTsXH330kcSxPXv2SPVXxOejRvv27TFu3DiMGzcOVVVV+OSTTxAfH48PPvgAlpaW8t8MEREREdWbXKXfRSJRg2dtXkdmZmbo1KkT9u/fjwcPHkgcKy8vx/bt2yEQCKT2GL2smv07/1ya1qZNGwwePBgJCQm4cOGC1HkikUi8T0dZWRlDhw7F5cuXkZycLNGnvqW9q6urERMTA1NTU4wbNw4jRoyQ+hk1ahSuX7+O9PT0eo2ppKQk9RkpKSlBeHh4refs3LkTRUVF4tdFRUXYtWsXWrVqhf79+9fruq+zmtmyf74v169flyr9Dijm81FaWio1Y6usrCx+DEFBQUED74aIiIiI6kuuma36ePToUa1LzF43Kioq+PLLL+Hn54fRo0eLS2k/evQIBw8exI0bN+Dr6ytzT9HL6NWrF7Zt24avvvoKgwcPhqqqKiwtLWFoaIgvv/wSEydOxKRJk+Dq6gozMzNUV1cjJycHSUlJcHNzE++dW7BgAY4fPw5fX19MmjQJHTt2xK+//iqzBLosJ0+exP379zF27Nha+wwbNgwhISGIjIys10zI8OHDsXv3bixYsAD29vZ49OgRoqKixPvZZGnTpg3GjRsnLvMeHR2Ne/fu4Ztvvqn3Z+nBgwfYt2+fzGNWVlYwMjKq1ziNoVu3bujevTvCwsJQVlaGLl264NatW9i9ezdMTU2RkZEh0V8Rn4/bt29j0qRJcHZ2Rvfu3dGqVSvcvHkTO3fuhIGBgbjwBxERERE1njqTrb1790q8vnr1qlQb8OzBtPfv30dsbCxMTU0VF2EjGzJkCHbs2IGwsDDs3bsX+fn5aNGiBXr27ImgoCC4uLgo/JqjRo1CZmYm9u/fj0OHDqG6uhorVqyAoaEhOnXqhKioKGzcuBFHjx5FbGws1NXV0alTJwwdOlSiip6RkRG2b9+OwMBAbNu2DWpqahg0aBC+++67ei0zqymy4OzsXGsfU1NTGBsb48CBA1i6dGmdZdgDAgKgpaWFQ4cOISkpCZ06dYKnpycsLCxqXYr48ccfIy0tDTt27MCjR4/QpUsXiWdP1UdmZiY++eQTmce++eabJk22lJWVERoaisDAQMTExKC0tBTdu3dHYGAgrl69KpVsKeLz0bFjR4wZMwYpKSlITExEeXk5OnTogHHjxmHmzJlvzC9EiIiIiN5kAlEd6wJ79OhRr3LvNcO0aNECISEhGDhwoGIiJKImk5ubC0dHR3Rx8Ieqpm7dJxARvSXiVks/65CImpea70FJSUkNriNQ58zWihUrADxLppYuXQonJyc4OjpK9VNSUoKOjg6srKzE5bKJ6O0Q9qnzW1GshIiovsorqqCmWnv1WiKi+qgz2Xq+xHRMTAycnJwkHn5LRERE9LZhokVEiiBXgYyIiIjGioOIiIiIiOitIlfpdyIiIiIiIqofuUu/p6amYtOmTUhPT0dhYSGqq6ul+ggEAly5ckUhARIREREREb2J5Eq2jh07hrlz56KqqgrvvPMOunTpAmVlrmkmIiKiNw+LYBBRY5Mr2QoJCYGKigpCQ0NZ2p2oGZmxPIGl34norcPy7kTU2OTas3Xjxg24uLgw0SIiIiIiIqqDXMmWpqYmWrdu3VixEL0xUlJSIBQKsWnTpqYOhYiIiIheU3IlW3Z2drhw4UJjxUJERERERPTWkCvZ+vjjj3Hnzh389NNPEIlEjRUTERERERHRG0+uAhlr1qyBiYkJQkJCEBUVhZ49e0JbW1uqn0AgwLfffquwIIneBNu3b0dSUhJu3LiBJ0+eQEdHB7a2tliwYAEMDAwk+gqFQri7u2P06NEIDg7GtWvX0LJlS4wcORILFy6ElpaWuO+DBw+wZcsWnDlzBvfu3UNZWRkMDQ3h5uaG6dOnS1QEjY6ORkBAAMLDw3HlyhXs3LkTf/75J/T19eHr6wt3d/dX9n4QERERNXdyJVsxMTHiP9+9exd3796V2Y/JFjVHmzdvRu/eveHj4wMdHR1cv34dkZGRSE5ORlxcHNq0aSPRPyMjA4cPH8a4cePg6uqKlJQURERE4MaNG9iyZQuUlJ5NPF+7dg1HjhyBs7MzjIyMUFFRgRMnTmD16tXIzc3F119/LRVLUFAQysrK4OnpCTU1NezcuRP+/v4wMjJC3759X8n7QURERNTcyZVsJSUlNVYcRG+8uLg4aGpqSrQ5Ojpi6tSpiIyMxMyZMyWOXb9+HWvXroWTkxMAwNvbG9988w0iIiJw8OBBvPfeewCA/v37IykpCQKBQHzu1KlTsWTJEuzZswd+fn5o3769xNjl5eWIjIyEmpoaAGDEiBFwdHTE9u3bmWwRERERvSJy7dnS19ev9w9Rc1OTaFVXV6OoqAh5eXkQCoXQ1tZGenq6VP8uXbqIE60aH374IQAgISFB3KahoSFOtMrLy5Gfn4+8vDwMHDgQ1dXVuHz5stTYEydOFCdaANChQwd06dIFt2/ffun7JCIiIqL6kWtmi4hqd+bMGfz000+4ePEi/v77b4ljBQUFUv27desm1da+fXu0atUKOTk54rbKykps2LAB+/btQ3Z2tlRxmsLCQqlxDA0Npdp0dHRqXfpLRERERIonV7J17969evd955135A6G6E2Vnp6O6dOnw8jICIsXL4aBgYF4RmrhwoUvVb1z5cqViIiIgIuLC3x9faGrqwtVVVVkZGRg1apVqK6uljqnZr8XERERETUduZItBwcHiX0jtREIBLhy5UqDgyJ608THx6OqqgobN26UmFUqKSmROfMEAFlZWVJtDx8+RGFhocQY+/btg7W1NYKCgiT6ZmdnKyh6IiIiImoMciVbbm5uMpOtwsJCZGZm4t69e+jfvz/3bFGz83z59eeFhobKnHkCgFu3biExMVFi39bGjRsBQKJNSUlJamaspKQE4eHhLxk1ERERETUmuZKtlStX1nqsuroaP/30E3bt2oXAwMCXDozoTeLk5ITw8HDMnDkTnp6eUFVVxalTp3Dt2jWpku81TE1NsWTJEowbNw6dO3dGSkoKDh8+jP79+8PFxUXcb/jw4di9ezcWLFgAe3t7PHr0CFFRUdDR0XlVt0dEREREDaCwjR1KSkrw8/ODvr4+Vq1apahhiV5LNTNNNTNaffv2RUhICDQ1NfHf//4XISEh0NDQwLZt26TKwdf417/+hbVr1+L8+fMIDAxEWloaJk2ahHXr1knsuQoICMC0adNw8eJFLFu2DHv37oWnpyc+/vjjxr9RIiIiImowhVcjtLKywt69exU9LNFrpbi4GACgra0tbnNycpIq5Q4AR48erXUce3t72Nvbv/BaLVq0wL///W/8+9//ljp27do1idceHh7w8PCQOU5ERMQLr0NEREREiqXwZKugoAClpaWKHpbotXLx4kUAz5YCNgdhnzrDwMCgqcMgIlKo8ooqqKnK3nNLRKQICk22Tp8+jQMHDqB79+6KHJbotREfH4/Lly8jIiICZmZmsLCwaOqQiIiogZhoEVFjkyvZmjx5ssz2qqoq3L9/H/fv3wcAzJ079+UjI3oNffXVVxAIBHB2dkZAQEBTh0NERERErzG5kq3U1FSZ7QKBAK1atcLAgQMxbdo02NnZKSQ4otfN2bNnFTLOP/daEREREdHbR65k6+rVq40VBxEREdErwb1aRPSqKLxABhG9fWYsT4Cqpm5Th0FEpBBxq12bOgQiaiZe6jlbxcXFuH//vrgMNhE1rdzcXAiFQoSEhDR1KERERETNntzJVmVlJTZs2ABnZ2dYW1vDwcEB1tbWcHZ2xoYNG1BZWdkYcRK9MikpKRAKhdi0aZPUsdTUVPTt2xcDBw58Y5bVFhYWIiQkBCkpKU0dChEREVGzItcywvLycsyYMQNnz56FQCBAp06doKenh7/++gt3795FUFAQTpw4gU2bNkFNTa2xYiZqEr/++ivmz5+Pdu3aITw8HEZGRk0dkhR9fX2kp6dDWfl/exEKCwuxZs0a+Pn5wcbGpgmjIyIiImpe5JrZCg8PR2pqKgYPHowDBw7g6NGj2L17N44ePYpDhw5h6NChSEtLQ3h4eCOFS9Q04uLi4OfnByMjI+zcufO1TLSAZ5VB1dXVoaLC7ZhERERETU2uZCsuLg7du3fHTz/9BGNjY4ljRkZGWLNmDUxMTBAXF6fIGIma1I4dO7BkyRKYmZlh27Zt6NChAwDA398fQqFQ5jlCoRD+/v7i1w4ODpg0aZJEn9DQUAiFQsyePVui/fvvv4dQKMSjR48APNsbGRQUhHHjxsHGxgbm5uZwdnbGqlWrUFpaKnHuP/dspaSkwNHREQCwZs0aCIVCCIVCODg4vMQ7QkRERET1Idevv+/cuYNJkyZBSUl2jqakpIR3330X27ZtU0hwRE0tNDQUP/zwA2xtbfHTTz9BS0urQePY2toiNjYWZWVl0NDQAACcOXMGSkpKOHv2LKqqqsRL/5KTk2FiYoJ27doBAB48eIDIyEgMGzYMo0aNgoqKClJTUxEWFobMzEyZe8tqdOvWDQEBAVixYgWcnZ3h7OwMAA2+DyIiIiKqP7mSLVVVVZSUlLywT2lpKZcw0Vth586dyMnJgZOTE4KCgl5qH6KtrS2ioqLw+++/Y8CAASgvL8f58+cxatQoxMbGIiMjA5aWligqKkJmZia8vLzE5xoaGuLYsWNQVVUVt3l7eyM4OBjr1q1Deno6LC0tZV63Xbt2cHJywooVKyAUCuHqynLHRERERK+KXMsIhUIhDh8+jLy8PJnH8/LycPjwYfTo0UMhwRE1pb/++gvAsyWyL1vwxdbWFsCzWSsAOH/+PMrKyjBjxgxoa2vjzJkzAJ5VO6yqqhL3BwA1NTVxolVZWYmCggLk5eXB3t4eAHDx4sWXio2IiIiIGodcyZa3tzfy8vIwduxY7NmzBzk5OSgrK0NOTg6ioqIwfvx45OXlwdvbu7HiJXplZs6cCVtbW2zevBkrV658qbHat2+Prl27ipOt5ORk6OnpQSgUwtraWqJdSUkJ/fv3lzh/+/bteP/992FhYYH+/fvDzs4OPj4+AICCgoKXio2IiIiIGodc6/1cXFxw9epVbNiwAV988YXUcZFIhBkzZsDFxUVhARI1lRYtWiA0NBS+vr7YsmULqqursXTpUvFxgUAg87zanjVna2uL3bt3o6ioCMnJyeIy7La2tvjhhx9QXl6O5ORk9OzZE61btxaft2XLFqxcuRIDBw7E5MmT0b59e6iqquLBgwfw9/eHSCRS4F0TERERkaLIvblq0aJFcHBwQGRkJK5cuYLi4mK0bNkSZmZmGDNmDKysrBojTqImoaGhgfXr12P27NnYunUrRCIRPv30UwAQJ0T5+fnQ0dERn5OTkyNzLFtbW+zYsQPHjh3DpUuX4OHhAQCws7NDWVkZkpKScOPGDXzwwQcS5+3btw/6+vrYuHGjRHGa48eP1+seaksKiYiIiKhxNaiSRe/evdG7d29Fx0L0WtLQ0MC6deswZ84c/PzzzxCJRPjss8/Ejz84ffq0xGzuli1bZI5jY2MDgUCAdevWoaKiQrwvy9TUFG3btsWaNWsgEokk9msBz6p8CgQCiRmsyspKbNy4sV7xa2pqAuByQyIiIqJXrc5kq7y8HBMnToSWlhbCwsIkKqL9s9/MmTNRWlqK7du319qP6E30fMIVEREBkUiEhQsXIigoCF988QVu3rwJHR0dnDhxAk+ePJE5ho6ODnr06IHMzEzo6+vD0NBQfMzGxgYHDhyAqqoq+vXrJ3HeiBEjsHr1asycORPOzs4oLi5GfHx8vat+tmnTBp07d8b+/fthaGiIdu3aoUWLFnzWFhEREVEjq7NARk1Z6mnTpr0wgVJTU8P06dORnp7OhxrTW0ldXR0//fQTBg4ciG3btmH16tXYsGEDTExMEBoaipCQELRv3x5hYWG1jlEza/XP2Ss7OzsAgLm5udQzsKZPn45FixYhJycHy5cvx44dOzBgwAB899139Y591apV6Ny5M4KCgrBo0SJ888039T6XiIiIiBpGIKpjd/2sWbNw69YtHDlypF4DDh8+HJ07d8aGDRsUEiARNZ3c3Fw4Ojqii4M/VDV1mzocIiKFiFvNZw4SUd1qvgclJSXBwMCgQWPUObN15coVqTLUL2JtbY3MzMwGBUNERERERPS2qHPTx5MnT9C2bdt6D9i2bVvk5+e/VFBE9HoJ+9S5wb/RISJ63ZRXVEFNVbmpwyCiZqDOmS0NDQ2UlJTUe8CSkhKoq6u/VFBEREREjYWJFhG9KnUmW506dcLly5frPeDly5fRqVOnlwqKiIiIiIjoTVdnstW/f39cuHABly5dqnOwy5cv4/z587CxsVFIcERERERERG+qOpMtb29vCAQCzJ8/H1lZWbX2y8rKwvz586GsrIyJEycqNEgiIiIiRSivqGrqEIioGamzQEbXrl0xZ84crFmzBm5ubhg+fDhsbW3RsWNHAMCDBw9w5swZHDlyBOXl5fjoo4/QtWvXRg+ciF6dGcsTWPqdiN4KLPtORK9SnckWAPj5+UFFRQVr1qxBfHw89u/fL3FcJBJBRUUFCxcuxKxZsxolUCIiIiIiojdJvZItAPD19cX777+PqKgonDt3Dn/99RcAQE9PD3379oWHhwf09fUbLVCi11FxcTG2bt2KxMRE3L59G9XV1dDX18eQIUMwffp0uR6b8E+FhYXYunUr+vfvz32QRERERG+geidbAKCvr4+PPvqosWIheqPcunUL06dPx7179zBs2DCMHTsWKioquHDhAn7++WdER0cjNDQUvXr1atD4hYWFWLNmDfz8/JhsEREREb2B5Eq2iOiZ0tJS+Pr64uHDh1i/fj2GDBkiPubp6YmJEyfigw8+wOzZsxEXF/dSM1yNpbi4GC1btmzqMIiIiIjeWnVWIyQiaZGRkbh9+zYmT54skWjVsLCwwP+xd+dRVZZ7/8ffgKAMTqSmCSIa4pMImKVIlB0GMdM0FdEUNUHU5JRThdnznNOolGYeLIdQ6TiFAmpgDkidzDKzkRzAQiXI53RMJN0CgsDvjx72r90GBRRR+bzWYhXXfV3f+3vfa8Hi6zXsmTNncubMGeLi4ozt5eXlLFu2jLFjx3Lffffh4eHBgw8+yN/+9jfOnj1r7HfgwAECAgIAWLp0Ke7u7ri7u+Pv72/ss379eiZNmsT999+Ph4cHfn5+zJkzh7y8PLN83N3diY6OZv/+/YwZM4ZevXoxbdrqH4dmAAAgAElEQVS0a/hGREREROTPNLMlUge7du0Cfp/Fqs7w4cOZP38+u3fv5tlnnwWgtLSUVatWMWDAAAICArC1teX777837oVMSkrCxsaGrl27MnfuXObPn09QUBBBQUEA2NvbG+OvXr0ab29vwsLCaNWqFceOHSMxMZHPP/+clJQUWrdubZLPoUOH2LVrF6NGjeLRRx+91q9ERERERP5ExZZIHfzwww/Y29vj4uJSbR9bW1tcXV05duwYFy5cwN7eHhsbG/bt20ezZs2M/Spnmp5//nn27NnDoEGDaNOmDYGBgcyfPx93d3eGDjU/qjglJQU7OzuTtoCAACZOnEhiYiKTJ082y3nNmjX4+vpe5dOLiIiISE1oGaFIHRgMBpo3b37FfpV7oi5cuACAhYWFsdAqKyvj3Llz5Ofn4+PjA0BGRkaNc6gstMrLyzl//jz5+fm4u7vTvHnzKuN0795dhZaIiIjIdaSZLZE6cHBwwGAwXLGfwWDA0tLSZEnfBx98wJo1azh69CilpaUm/X/77bca57B//37efvttvvvuOy5evHjFOJ07d65xbBERERG5eiq2ROrAzc2NgwcPkpOTU+1SwqKiIk6cOMEdd9yBtbU1ALt372bmzJl4enry3HPP0aFDB5o2bUpZWRkRERFUVFTU6P4ZGRmEh4fTqVMnZs+ejZOTE82aNcPCwoKZM2dWGcfW1rbuDywiIiIitaZiS6QOgoODOXjwIJs3b2bOnDlV9tm6dSulpaU88sgjxrZt27bRtGlT/vnPf5oUP9nZ2WbjLSwsqr1/amoqZWVlvPPOOzg7OxvbCwsLOXfuXF0eSURERESuMe3ZEqmDkSNH0rlzZ+Lj49m7d6/Z9cOHD/PGG2/Qtm1bxo4da2y3srLCwsKC8vJyY1tFRQXLli0zi1G5J6uqJYFWVlZV5rVixQqT2CIiIiLScDSzJVIHtra2LFu2jIiICKZMmcKAAQPo06cPTZo0ISMjg23bttGyZUuWLVtGmzZtjOOCg4PZtWsXEyZMYNiwYVy6dIk9e/ZQVFRkdo/WrVvj4uLC9u3bcXZ2pk2bNtja2uLv709gYCDx8fFMnjyZ0NBQrK2t+fTTT8nKyjI78l1EREREGoaKLZE66tKlC++//z7vvvsuaWlp7N27l8LCQuD3PV0bNmygRYsWJmMefvhhLly4QHx8PDExMbRs2ZK//OUvzJ49m759+5rdY+HChbz66qssXryYoqIiOnbsiL+/P7179yY2Npa3336bJUuW0LRpU3x9fVm3bh3jxo27Ls8vIiIiIpdnUVHTHfkickWXLl3iqaeeYs+ePcydO5eJEyc2dEpXJS8vj4CAAFz9o7G2c2zodERErlrKIvPPLRQRqUrl30Hp6ek4OTnVKYZmtkSuoSZNmrB48WKioqKYP38+NjY2PPbYYw2d1lWLmxdU518yIiI3kpLSMmysq973KiJyranYErnGbGxsWLlyZUOnISIiVVChJSLXk04jFBERERERqQcqtkREREREROqBii0RERFpFEpKyxo6BRFpZLRnS0SuKOKVNJ1GKCI3PZ1EKCLXm2a2RERERERE6oGKLZGbXFhYGP7+/g2dhoiIiIj8iZYRSqNy8eJFEhMT2bVrF8eOHeP8+fPY2tri4uKCj48Pw4cPp2vXrg2dpoiIiIjcAlRsSaORm5vLlClTyM7Opk+fPkycOJG2bdtSWFjI0aNHSUpKYvXq1fzrX//i9ttvb+h0a2zVqlUNnYKIiIiIVEHFljQKxcXFREZGkpuby9KlSwkKCjLrc/HiReLj469/cv+nrKyMkpISbG1tazXOxsamnjISERERkauhPVvSKGzevJnjx48THh5eZaEF0LRpU6ZMmWI2q3X+/Hlef/11goKC8PDwwMfHh1mzZpGbm2sWIz8/nxdeeIH+/fvj4eFB//79eeGFFzh79qxJv+TkZNzd3fnss8946623CAwMxNPTkx07dgBw9uxZ5s6dS9++fenVqxfjx4/nyJEjVe7PqqotIyOD6OhogoOD8fLyolevXowePZq0tLRavzsRERERqRvNbEmjsGvXLgBGjhxZq3Hnz59n9OjRnDp1ihEjRuDm5sbp06fZsGEDISEhJCUl0bFjR2PfMWPGkJOTw4gRI7jrrrs4evQoGzdu5PPPP2fz5s04ODiYxI+JieHSpUuMGjUKe3t7XF1dKSkp4fHHH+fo0aMMHz6cnj17kpWVxeOPP07Lli1rlHdaWhrHjx9n4MCBdOzYkYKCArZs2UJUVBQLFy5kyJAhtXoPIiIiIlJ7KrakUfjhhx9wcHDA2dnZpL2srIzffvvNpM3Ozo5mzZoBsGTJEnJzc9m0aRPdu3c39nn00UcZMmQIsbGxLFiwAIC4uDhOnjzJ//zP/zB27Fhj3//6r//ixRdfJC4ujhkzZpjcq7i4mK1bt5osHVy/fj1Hjx5lxowZTJs2zdjerVs3XnzxRWNxdznTpk1j9uzZJm1hYWEMGzaMZcuWqdgSERERuQ60jFAaBYPBYDarBJCdnU2/fv1MvtavXw9ARUUFKSkp3HvvvbRr1478/Hzjl62tLd7e3uzbt88YKy0tDUdHR0JDQ03uERoaiqOjI3v27DG7/5gxY8z2aH300UdYWVkxfvx4k/aQkBCaN29eo+e1s7Mz/n9RURFnz56lqKgIHx8fsrOzMRgMNYojIiIiInWnmS1pFBwcHKosMJycnFizZg0AmZmZxMTEGK/l5+dTUFDAvn376NevX5VxLS3//79X5OXl4eHhQZMmpj9WTZo0oXPnzhw5csRsvKurq1lbXl4e7dq1w97e3qTdxsYGJycnzp07d5kn/d2ZM2d48803SU9P58yZM2bXz507V2XxKSIiIiLXjootaRTc3Nw4ePAgubm5JksJ7ezs8PX1BcDKyspkTEVFBQC+vr5Mnjy5XvKqXK54LVVUVDBp0iSys7MZP348Hh4eNG/eHCsrK5KSkkhNTaW8vPya31dERERETKnYkkYhODiYgwcPkpiYyMyZM2s0xtHRkRYtWmAwGIwF2eU4Oztz4sQJLl26ZDK7denSJU6ePGm2X6w6HTt2ZP/+/Vy4cMFkdqu0tJS8vDxatGhx2fFZWVlkZmYyffp0nnzySZNrmzdvrlEOIiIiInL1tGdLGoWQkBC6dOnCqlWrqj3+vHImq5KlpSVDhgwhIyODnTt3Vjnmj0v0AgMDyc/PNytoNm3aRH5+PoGBgTXK1d/fn7KyMv75z3+axTl//vwVx1cubfzz8xw7dkxHv4uIiIhcR5rZkkahWbNmrFy5kilTphAVFUWfPn3w8/OjTZs2GAwGjh8/zo4dO7CysqJDhw7GcTNnzuTrr79mxowZPPTQQ3h5eWFtbc2pU6fYu3cvPXr0MJ5GGBERwc6dO3nxxRc5cuQI//Vf/8XRo0dJTEzE1dWViIiIGuUaEhLCe++9x5tvvslPP/1kPPp9586duLi4cOnSpcuO79q1K25ubsTFxVFcXIyrqysnTpwgISGBbt26cfjw4bq/SBERERGpMRVb0mg4OzuTnJxMUlISO3fuZPXq1RgMBmxtbenUqRMjR45k5MiRdOnSxTimefPmbNy4kdWrV7Nz507S09OxsrKiffv29O7dm5CQELO+//jHP/jwww9JTk7mtttuY/To0fz1r3+t8YEUNjY2vPvuu7z22mukp6ezY8cOPD09iY+PZ968eRQXF192vJWVFStWrCAmJoYtW7ZQVFSEm5sbMTExZGZmqtgSERERuU4sKv681khEbkhlZWX4+Pjg6enJqlWrrss98/LyCAgIwNU/Gms7x+tyTxGR+pKyaGhDpyAiN5HKv4PS09NxcnKqUwzNbIncgIqLi81OKnzvvfc4d+4c991333XPJ25eUJ1/yYiI3ChKSsuwsba6ckcRkWtExZbIDej555+npKSEXr16YWNjwzfffENqaiouLi6MGjWqodMTEbkpqdASketNxZbIDcjPz4/169ezf/9+CgsLue222wgJCeGpp57ShxGLiIiI3CRUbIncgIYNG8awYcMaOg0RERERuQr6nC0RERG5JZWUljV0CiLSyGlmS0SuKOKVNJ1GKCI3HZ0+KCINTTNbIpcRHR2Nu7t7Q6chIiIiIjchFVtySzlw4ADu7u7X7XOoRERERESqo2JLRERERESkHqjYEhERERERqQcqtqRROnnyJE8//TR+fn54eHjg7+9PTEwMhYWFVfbPz8/nmWeeoW/fvnh7ezNhwgQOHz5s1m/9+vVMmjSJ+++/Hw8PD/z8/JgzZw55eXlmfd3d3YmOjuabb75h3LhxeHt707dvX+bNm8eFCxfM+mdmZjJ9+nT69u1Lz549GTRoEO+88w5lZaanbVXuMzt//jx/+9vf6NevHz179mT06NF89913dXxjIiIiIlJbOo1QGp1Dhw4xYcIEWrRoQWhoKLfffjuZmZmsXbuWb775hrVr12JtbW0yJiIigpYtWxIVFcWvv/7KunXrGDduHAkJCXTr1s3Yb/Xq1Xh7exMWFkarVq04duwYiYmJfP7556SkpNC6dWuTuEePHmXq1KkMHz6cwYMH88UXX5CYmIilpSUvvfSSsd/3339PWFgYTZo0YezYsbRp04aPPvqIhQsXkpmZyaJFi8yeMzw8HEdHR6ZPn05BQQFr1qwhMjKS9PR0fTCyiIiIyHWgYksaneeee462bduSmJhoUnT069ePqKgoUlJSGD58uMmYO+64g9jYWCwsLAAICgpi5MiRxMTEmBzGkZKSgp2dncnYgIAAJk6cSGJiIpMnTza5lpWVRUJCAl5eXgCMHj0ag8FAcnIy0dHR2NvbA/DKK69QUlLCe++9R/fu3QEYN24cM2bMIDU1lZEjR9KvXz+T2HfddRd///vfjd937drV2H/06NF1eXUiIiIiUgtaRiiNSlZWFllZWQwePJiSkhLy8/ONX71798bOzo5PP/3UbFxERISx0ALw8PDgvvvuY//+/SZL/ioLrfLycs6fP09+fj7u7u40b96cjIwMs7je3t7GQquSj48Ply5d4ueffwbgzJkzfPPNN/j7+xsLLQALCwumTZsGQFpamlnsiRMnmsUFyMnJuew7EhEREZFrQzNb0qhkZ2cDEBsbS2xsbJV9fv31V7O2rl27Vtm2b98+Tp06hZubGwD79+/n7bff5rvvvuPixYsm/X/77TezGM7OzmZtrVq1AqCgoADAuN/rzjvvNOvbpUsXLC0tyc3NvWLsyiWMlXFFREREpH6p2JJGqfIQi6q0aNGiTjEzMjIIDw+nU6dOzJ49GycnJ5o1a4aFhQUzZ86koqLCbIyVlVW18arqXxvVxb7auCIiIiJSMyq2pFFxcXEBwNLSEl9f3xqPy87Oxtvb26zNysqKO+64A4DU1FTKysp45513TGaVCgsLOXfuXJ1zdnJyAuDHH380u3b8+HHKy8urnCETERERkYalPVvSqNx1111069aN9957r8qld5cuXapymV1cXJzJjNDhw4f57LPP6Nevn/EQi+pmklasWEF5eXmdc77tttvo1asXH330EceOHTO2V1RUsHLlSuD3AztERERE5MaimS25Je3fv99szxT8vm/ptddeY8KECTzyyCOMGDGCO++8k+LiYnJyckhLS2PWrFlmpxGeOnWK8PBw/P39OX36NOvWraNZs2Y8/fTTxj6BgYHEx8czefJkQkNDsba25tNPPyUrK8vsyPfamjdvHmFhYYwdO5bHHnuMtm3b8tFHH7Fv3z4GDx5sdhKhiIiIiDQ8FVtyS/rkk0/45JNPzNpdXV0ZM2YMW7ZsYcWKFXz44Ye899572Nvb07FjRx599NEqC5e4uDjmz59PbGwsxcXFeHl58cwzz5icDti7d29iY2N5++23WbJkCU2bNsXX19f4mVxXo2fPnrz33nv84x//YOPGjRQWFuLs7MycOXOYNGnSVcUWERERkfphUaHd8iJSjby8PAICAnD1j8bazrGh0xERqZWURUMbOgURuYlV/h2Unp5u3ENfW5rZEpEripsXVOdfMiIiDaWktAwb6+pPfRURqW86IENERERuSSq0RKShqdgSERERERGpByq2RERERERE6oGKLRERERERkXqgYktERERuSSWlZQ2dgog0cjqNUESuKOKVNB39LiI3HR39LiINTTNbIje52NhY3N3dycvLa+hUREREROQPNLMlAhQVFZGQkMDu3bv58ccfuXDhAi1btqRHjx489NBDPPLIIzRp0nA/Lnv27OHo0aP89a9/bbAcRERERKR2NLMljV5OTg7Dhg1j/vz5NG3alMjISF588UUmTpzIpUuXmDt3Lm+88UaD5rhnzx6WLl1a5bVp06aRkZFBx44dr3NWIiIiInI5mtmSRq24uJgpU6aQl5dHbGwsAwYMMLkeGRlJRkYG33///WXjGAwGHBwc6jPVajVp0qRBZ91EREREpGr6C00atc2bN3PixAkmT55sVmhV8vT0xNPT0/i9v78/HTt2ZO7cuSxatIhvv/2Wli1b8uGHHwJw8OBB3n77bTIyMigtLaVr16489thjhISEmMTNyMhgw4YNfPPNN/z73//G0tISd3d3wsPDCQoKMvYLCwvjiy++AMDd3d3YPn/+fIYPH05sbCxLly4lPT0dJycnAH755RfWrFnD/v37OXXqFMXFxTg7OzNs2DDCw8OxsrK6Ni9QRERERKqlYksatV27dgEQGhpaq3GnTp1iwoQJDBw4kAEDBlBYWAjAhx9+SFRUFG3atOHxxx/HwcGB7du38/zzz5OXl8fMmTONMdLS0jh+/DgDBw6kY8eOFBQUsGXLFqKioli4cCFDhgwBYOrUqZSXl/Pll1/y2muvGcfffffd1eaXlZXF7t27CQoKolOnTpSWlvLJJ5+waNEi8vLyePHFF2v1vCIiIiJSexYVFRUVDZ2ESEPp27cvly5d4quvvqrxGH9/f37++Wdefvllk9mqsrIyAgMDOX/+PNu3b+f2228HoKSkhPHjx/Pdd9+xY8cOOnfuDEBhYSF2dnYmsYuKihg2bBhWVlZ88MEHxvbo6Gi2bNlCVlaWWT5VzWwVFxfTtGlTLCwsTPo+/fTTpKam8vHHH9OuXbsrPmteXh4BAQG4+kfr6HcRueno6HcRuRqVfwf98W+s2tIBGdKoGQwG7O3taz2uVatWDB8+3KTt8OHDnDp1ihEjRhgLLQAbGxsiIiIoLy8nPT3d2P7HQquoqIizZ89SVFSEj48P2dnZGAyGOjzR75o1a2YstEpKSigoKCA/Px8/Pz/Ky8s5dOhQnWOLiIiISM1oGaE0ag4ODly4cKHW45ydnc32PVV+ztWdd95p1t/NzQ2A3NxcY9uZM2d48803SU9P58yZM2Zjzp07V+dDNy5dusTKlSvZtm0bOTk5/HkC+9y5c3WKKyIiIiI1p2JLGjU3NzcOHjxIbm4uzs7ONR5na2t7VfetqKhg0qRJZGdnM378eDw8PGjevDlWVlYkJSWRmppKeXl5neMvWLCAtWvXMmjQIKZOnYqjoyPW1tYcPnyYhQsXXlVsEREREakZFVvSqA0YMICDBw+yefNmZs2adVWxKtfy/vjjj2bXKtsqC7qsrCwyMzOZPn06Tz75pEnfzZs3m43/896rK9m2bRv33nsvixcvNmnPycmpVRwRERERqTvt2ZJGLSQkBFdXV1avXs2ePXuq7HPo0CHWr19/xVg9evTgjjvuIDk5mdOnTxvbS0tLWbVqFRYWFgQEBABgafn7j96fl/cdO3aMtLQ0s9iV+7sKCgpq9FyWlpZmsQsLC4mPj6/ReBERERG5eprZkkbN1taWFStWEBkZyfTp0/Hz88PX15dWrVqRn5/PgQMH2LdvHxEREVeMZWVlxX//938TFRXFyJEjGTVqFPb29uzYsYNvv/2WqVOnGk8i7Nq1K25ubsTFxVFcXIyrqysnTpwgISGBbt26cfjwYZPYXl5erFu3jhdeeIH+/ftjbW2Np6dntUsfg4ODSUhIYMaMGfj6+vLrr7+SlJREq1atrvqdiYiIiEjNqNiSRs/FxYWtW7eSkJDArl27WL58OYWFhbRs2RIPDw8WLFhg/MyrK/H39yc+Pp5ly5axatUq44ca//mYeCsrK1asWEFMTAxbtmyhqKgINzc3YmJiyMzMNCu2Bg8ezNGjR9m+fTs7d+6kvLyc+fPnV1tszZ07F3t7e3bu3El6ejodOnQgNDSUnj17MnHixDq/KxERERGpOX3OlohUS5+zJSI3M33OlohcjWvxOVua2RKRK4qbF1TnXzIiIg2lpLQMG2urK3cUEaknOiBDREREbkkqtESkoanYEhERERERqQcqtkREREREROqBii0RERG55ZSUljV0CiIiOiBDRK4s4pU0nUYoIjcVnUQoIjcCzWyJiIiIiIjUA81sidSCwWDg3XffZc+ePZw8eZLy8nI6duxI//79CQ8Pp02bNg2dooiIiIjcIFRsidTQiRMnCA8P59SpUwwYMICRI0fSpEkTvv32W9auXUtycjLLly+nV69eDZ2qiIiIiNwAVGyJ1EBRURFTp07lP//5D8uXL+fBBx80XgsNDeWxxx7j8ccf54knniAlJUUzXCIiIiKiPVsiNZGYmMjJkycZP368SaFVqWfPnsycOZP8/HxWrVplbD9w4ADu7u4kJyezfv16goOD6dmzJ0OGDOGjjz4CICsri/DwcO6++2769u3Lyy+/TGlpqUn8jIwMoqOjCQ4OxsvLi169ejF69GjS0tLMcomOjsbd3Z3z58/zt7/9jX79+tGzZ09Gjx7Nd999d21fjIiIiIhUS8WWSA3s2rUL+H0WqzrDhw/H2tra2PeP1q9fT3x8PCNGjGD27NkUFhYSFRXFnj17mDBhAq6urjz99NPce++9rF27lnfeecdkfFpaGsePH2fgwIHMmzePadOm8dtvvxEVFUVKSkqV+YSHh/PLL78wffp0pkyZwg8//EBkZCQGg+Eq3oSIiIiI1JSWEYrUwA8//IC9vT0uLi7V9rG1tcXV1ZVjx45x4cIF7O3tjdf+85//8MEHH9C8eXMAfHx8GDp0KFFRUfzjH/9gwIABAIwZM4bhw4ezYcMGnnjiCeP4adOmMXv2bJP7hYWFMWzYMJYtW8aQIUPM8rnrrrv4+9//bvy+a9euzJgxg9TUVEaPHl2n9yAiIiIiNaeZLZEaMBgMxkLpchwcHIz9/2j48OEm47t3746DgwPt2rUzFlqV7r77bk6fPs2FCxeMbXZ2dsb/Lyoq4uzZsxQVFeHj40N2dnaVs1UTJ040+d7HxweAnJycKz6HiIiIiFw9zWyJ1ICDg0ONlt9V9qksuio5OTmZ9W3ZsiXt27evsh2goKDAODt25swZ3nzzTdLT0zlz5ozZmHPnzpnd09nZ2eT71q1bG+OKiIiISP1TsSVSA25ubhw8eJCcnJxqlxIWFRVx4sQJOnbsaLKEEMDKyqrKMdW1A1RUVBj/O2nSJLKzsxk/fjweHh40b94cKysrkpKSSE1Npby8vMaxK+OKiIiISP3SMkKRGggKCgJg8+bN1fbZunUrpaWlZssCr1ZWVhaZmZlERkbyzDPPMGjQIO6//358fX2rLLJERERE5MagYkukBkJCQnBxcSE+Pp69e/eaXT98+DBvvPEGjo6OhIeHX9N7W1r+/mP65xmpY8eOVXn0u4iIiIjcGLSMUKQG7OzsWLZsGREREUyZMoUBAwbQp08fmjRpQkZGBtu2bcPe3p633nqLtm3bXtN7d+3aFTc3N+Li4iguLsbV1ZUTJ06QkJBAt27dOHz48DW9n4iIiIhcGyq2RGqoa9euvP/++7z77rukpaWxd+9eysrKuOOOOwgLC2PSpEnXvNCC3/derVixgpiYGLZs2UJRURFubm7ExMSQmZmpYktERETkBmVRod3yIlKNvLw8AgICcPWPxtrOsaHTERGpsZRFQxs6BRG5yVX+HZSenl7lydI1oT1bIiIiIiIi9UDLCEXkiuLmBdX5X3RERBpCSWkZNtbVf7yGiMj1oJktERERueWo0BKRG4GKLRERERERkXqgYktERERERKQeqNgSERGRm15JaVlDpyAiYkYHZIjIFUW8kqaj30Xkhqaj3kXkRqSZLRERERERkXqgmS2ROjhw4ADjx4+v9rqVlRVHjhy5jhld3p49ezh69Ch//etfGzoVERERkUZDxZbIVRg8eDAPPPCAWbul5Y01abxnzx62bNmiYktERETkOlKxJXIV7rrrLoYO1T4BERERETF3Y/3zu8gtqKioiPnz5+Pn54enpyejRo1i//79REdH4+7ubuw3bdo0vLy8MBgMZjEyMjJwd3dn6dKlAOTl5eHu7k5sbCypqakMGTKEnj178uCDDxIbG8ulS5eMY8PCwtiyZQsA7u7uxq/k5OR6fnIRERGRxk0zWyJXoaioiPz8fLN2GxsbHBwcAHjqqaf4+OOPCQwMxNfXl7y8PKZPn46Tk5PJmFGjRvHhhx+SmprK6NGjTa4lJiZiaWnJyJEjTdo//PBDcnNzGTt2LG3atOHDDz9k6dKlnDp1ivnz5wMwdepUysvL+fLLL3nttdeMY+++++5r8g5EREREpGoqtkSuQmxsLLGxsWbtDz74ICtWrODjjz/m448/JiQkhJdfftl43cfHh8jISJMxDzzwAB06dCAxMdGk2CoqKmL79u34+fnRvn17kzGZmZkkJibSo0cPAMaNG0dUVBTJycmEhobi7e3NfffdR0pKCl9++aWWPIqIiIhcRyq2RK5CaGgoAwcONGt3dPz9M6k+/PBDAB5//HGT6/3796dr165kZ2cb26ysrBgxYgRLly4lKyvLuMRw165dGAwGs1ktAF9fX2OhBWBhYUFERAR79uwhLS0Nb2/vq39IEREREakTFVsiV8HFxQVfX99qr+fl5WFpaUmnTlrQUvgAACAASURBVJ3Mrrm6upoUWwAjR45k2bJlJCYmMm/ePOD3JYS33XYb/v7+ZjG6du1q1nbnnXcCkJubW6tnEREREZFrSwdkiFwHFhYWNerXoUMH7r//ft5//31KSko4efIkBw8eZOjQoVhbW9dzliIiIiJyLanYEqlHHTt2pLy8nJycHLNrJ06cqHLMqFGjKCgoYM+ePSQlJQFUuYQQMJsZA/jxxx8BcHZ2NrbVtNgTERERkWtHxZZIPapc+hcfH2/S/vHHH1dZKMHvh2u0a9eOhIQEtmzZwt13313lckGAzz77jMOHDxu/r6ioIC4uDoDAwEBju52dHQAFBQV1fhYRERERqR3t2RK5CkeOHGHbtm1VXgsMDKR///74+fmxadMmzp49S79+/cjLy2PTpk24u7uTlZVlNq7yoIxly5YBMGvWrGrv3717dyZMmMDYsWNp27Yt6enpfPbZZwwdOpRevXoZ+3l5ebFu3TpeeOEF+vfvj7W1NZ6eniazXyIiIiJybanYErkKqamppKamVnlt9+7duLi4EBsby+LFi9m+fTt79+41fjjxxo0bq1xeCBASEsKKFSuwtbWt8rTDSv7+/ri6urJixQpOnDjBbbfdxhNPPMETTzxh0m/w4MEcPXqU7du3s3PnTsrLy5k/f76KLREREZF6pGJLpA769u1b5axUVezs7Jg3b57xdMFKr776Kh06dKhyjLW1NRYWFgwePNi4BLA6gwcPZvDgwZftY2lpybPPPsuzzz5bo5xFRERE5Oqp2BKpZ8XFxTRr1syk7V//+hfHjh1j3LhxVY7ZuHEjZWVljBo16nqkeEVx84JwcnJq6DRERKpVUlqGjbVVQ6chImJCxZZIPXvrrbc4cuQIffv2pXnz5hw9epTk5GRatWrF5MmTTfpu376dU6dOsWrVKvz8/PDw8GigrEVEbi4qtETkRqRiS6Se3XPPPXz99desWrUKg8FAy5YtGTBgAE899RTt27c36Ttr1iyaNm3KPffcw6uvvtpAGYuIiIjItaBiS6Se9e/fn/79+9eob033gTk5OdW4r4iIiIg0DH3OloiIiNxUSkrLGjoFEZEa0cyWiFxRxCtpWNs5NnQaIiIApCwa2tApiIjUiGa2RERERERE6oGKLRERERERkXqgZYQiwMWLF0lMTGTXrl0cO3aM8+fPY2tri4uLCz4+PgwfPpyuXbs2dJoiIiIichNRsSWNXm5uLlOmTCE7O5s+ffowceJE2rZtS2FhIUePHiUpKYnVq1fzr3/9i9tvv72h0xURERGRm4SKLWnUiouLiYyMJDc3l6VLlxIUFGTW5+LFi8THx18xVmlpKeXl5TRt2rQeMhURERGRm432bEmjtnnzZo4fP054eHiVhRZA06ZNmTJlismsVmxsLO7u7vzwww/Mnz+fBx54AE9PT7799lsASkpKWL58OQ8//DA9e/bknnvuYerUqRw5csQsfkVFBRs2bGD48OF4eXnRq1cvwsLC+Pzzz6vMZ9euXYSFhXHPPffg5eVFcHAwL7/8MiUlJXWOKSIiIiLXnma2pFHbtWsXACNHjqzT+Dlz5tCsWTMmTZoEQNu2bSktLSU8PJxvvvmGoUOHMnbsWAwGA5s2bWLMmDGsW7eOnj17GmM8/fTTbN++neDgYIYPH05JSQkpKSlMmjSJ2NhYAgICjH0XL17M8uXLufPOO43LHX/66Sd2797Nk08+iY2NTa1jioiIiEj9ULEljdoPP/yAg4MDzs7OJu1lZWX89ttvJm12dnY0a9bMpK1FixasWbOGJk3+/49SfHw8X3zxBXFxcdx///3G9scee4zBgwfz2muvsXbtWgDS0tJISUnhxRdfJDQ01Nh3/PjxjBo1ildeeQV/f38sLCzIyMhg+fLl9O3bl3feecdkueKcOXOM/1+bmCIiIiJSf7SMUBo1g8GAg4ODWXt2djb9+vUz+Vq/fr1ZvwkTJpgUWgDvv/8+Xbp0oUePHuTn5xu/SkpK8PX15auvvqK4uNjY197ensDAQJO+586dw9/fn59//pmTJ08a+wLMnj3bbF+YhYWFsXiqTUwRERERqT+a2ZJGzcHBAYPBYNbu5OTEmjVrAMjMzCQmJqbK8Z07dzZry87Opri4mH79+lV737Nnz9KhQweys7O5cOECvr6+1fY9c+YMrq6u5OTkYGFhQffu3S/7TLWJKSIiIiL1R8WWNGpubm4cPHiQ3Nxck6WEdnZ2xmLFysqq2vF/XlYIvx9O0a1bN+bOnVvtOEdHR2NfR0dHFi1adNkcK/1xBqs6tY0pIiIiIvVDxZY0asHBwRw8eJDExERmzpx5TWK6uLhw9uxZfHx8sLS8/EpdFxcXTp48iZeXF/b29pft27lzZ/bu3UtmZiaenp7XJKaIiIiI1B/t2ZJGLSQkhC5durBq1SrS0tKq7FNRUVGrmMOGDeP06dPGZYh/9uuvv5r0LS8v54033rhi3yFDhgDwxhtvmBzz/uc8axNTREREROqPZrakUWvWrBkrV65kypQpREVF0adPH/z8/GjTpg0Gg4Hjx4+zY8cOrKys6NChQ41ijh8/ns8++4zXXnuNzz//HB8fHxwcHDh16hSff/45NjY2xtMIBw4cyPDhw1m3bh2HDx/mL3/5C61bt+bf//433377LTk5OaSnpwPg6enJ5MmTeeeddxg+fDgPPfQQbdu2JS8vj127drF582ZatGhRq5giIiIiUn9UbEmj5+zsTHJyMklJSezcuZPVq1djMBiwtbWlU6dOjBw5kpEjR9KlS5caxbO2tmbFihVs2LCBbdu2ERsbC0C7du3o2bMnjz76qEn/+fPn07dvXzZt2sSKFSsoLS2lbdu23HXXXcyePduk75w5c+jevTvr1q0jLi6OiooK2rdvzwMPPGCyf6w2MUVERESkflhU1HaNlIg0Gnl5eQQEBODqH421nWNDpyMiAkDKoqENnYKINAKVfwelp6fj5ORUpxia2RKRK4qbF1TnXzIiItdaSWkZNtbVnxQrInKj0AEZIiIiclNRoSUiNwsVWyIiIiIiIvVAxZaIiIiIiEg9ULElIiIiN5yS0rKGTkFE5KrpgAwRuaKIV9J0GqGIXFc6cVBEbgWa2RK5Dvz9/QkLC6tR3+TkZNzd3Tlw4EA9ZyUiIiIi9UkzWyJ1VFRUREJCArt37+bHH3/kwoULtGzZkh49evDQQw/xyCOP0KSJfsREREREGiv9JShSBzk5OURGRnLy5El8fX2JjIykdevWnDlzhv379zN37lx+/PFHnnnmmVrHHjp0KA8//DDW1tb1kLmIiIiIXC8qtkRqqbi4mClTppCXl0dsbCwDBgwwuR4ZGUlGRgbff/99neJbWVlhZaXPkBERERG52WnPlkgtbd68mRMnTvD444+bFVqVPD09GTt2rFl7dnY2kZGR9OrVi969e/Pkk09y+vRpkz5V7dmqbNu/fz+rVq0iMDAQDw8PgoOD2bJli9l9PvjgA6ZOncqDDz6Ih4cHffv25YknniAzM/Mqn15EREREakozWyK1tGvXLgBCQ0NrNe6XX35h/PjxBAYG8swzz5CZmUlCQgIGg4HVq1fXKMbixYspLi4mNDQUGxsbNm7cSHR0NJ06daJ3797GfuvWraNVq1aMGjWKtm3b8tNPP7Fp0ybGjBnDli1b6Ny5c61yFxEREZHaU7ElUks//PADDg4OODs712pcTk4OixcvZtCgQcY2S0tLNmzYwPHjx+nSpcsVY5SUlJCYmIiNjQ0AAwcOJCAggPXr15sUW3FxcdjZ2ZmMHTZsGEOHDiU+Pp6///3vtcpdRERERGpPywhFaslgMGBvb1/rce3atTMptAB8fHyA3wuxmnjssceMhRbA7bffjqurKydPnjTpV1loVVRUYDAYyM/Pp3Xr1ri6upKRkVHr3EVERESk9jSzJVJLDg4OXLhwodbjqpoJa9WqFQAFBQVXFePnn382aTty5AhLlizhiy++oLCw0OSak5NTTVMWERERkaugYkukltzc3Dh48CC5ubm1Wkp4uRMGKyoqahTD0vLKk9GnTp1i7NixODg4MG3aNLp06YKtrS0WFha8+uqrZsWXiIiIiNQPFVsitTRgwAAOHjzI5s2bmTVrVkOnYyYtLY3CwkKWLVtmXKZYqaCgwGQZooiIiIjUH+3ZEqmlkJAQXF1dWb16NXv27Kmyz6FDh1i/fv11zux3lTNof54t27Rpk9kx8yIiIiJSfzSzJVJLtra2rFixgsjISKZPn46fnx++vr60atWK/Px8Dhw4wL59+4iIiGiQ/B544AFsbW155plnGDduHC1atODrr79m7969dOrUibKysgbJS0RERKSxUbElUgcuLi5s3bqVhIQEdu3axfLlyyksLKRly5Z4eHiwYMEChgwZ0iC5derUiXfeeYc33niD5cuXY2Vlxd13383atWt56aWXzA7TEBEREZH6YVFR0535ItLo5OXlERAQgKt/NNZ2jg2djog0IimLhjZ0CiLSyFX+HZSenl7n05w1syUiVxQ3L0hHxovIdVVSWoaNdfWnuIqI3Ax0QIaIiIjccFRoicitQMWWiIiIiIhIPVCxJSIiIiIiUg9UbImIiIiIiNQDFVsiIiJyQygp1ecAisitRacRisgVRbySpqPfRaTe6bh3EbnVaGZL5Bpwd3cnOjr6ut/X39+fsLCw635fEREREbkyzWxJo3HgwAHGjx9v0mZjY0O7du3o06cPERERdO3atYGyExEREZFbjYotaXQGDx7MAw88AMDFixfJyspi8+bN7Nq1i5SUFDp27NjAGdbczp07GzoFEREREamGii1pdO666y6GDjXdF+Di4sIrr7xCWloaEydOvO45FRcX06RJE5o0qd2PpI2NTT1lJCIiIiJXS3u2RIB27doBYG1tbWxbv349kyZN4v7778fDwwM/Pz/mzJlDXl5etXG++eYbxo0bh7e3N3379mXevHlcuHDBpE90dDTu7u7k5+czd+5cfH198fb25t///net71vVnq3KtuzsbCIjI+nVqxe9e/fmySef5PTp03V+RyIiIiJSO5rZkkanqKiI/Px84PdlhMeOHWPx4sW0bt2aAQMGGPutXr0ab29vwsLCaNWqFceOHSMxMZHPP/+clJQUWrdubRL36NGjTJ06leHDhzN48GC++OILEhMTsbS05KWXXjLL4/HHH6dNmzY88cQTFBYWYmdnV6f7VuWXX35h/PjxBAYG8swzz5CZmUlCQgIGg4HVq1dfzesTERERkRpSsSWNTmxsLLGxsSZtd955J+vXr6dt27bGtpSUFGMBVCkgIICJEyeSmJjI5MmTTa5lZWWRkJCAl5cXAKNHj8ZgMJCcnEx0dDT29vYm/d3c3Fi4cKFZfrW9b1VycnJYvHgxgwYNMrZZWlqyYcMGjh8/TpcuXa4YQ0RERESujpYRSqMTGhrKmjVrWLNmDcuXL2fOnDmcPXuWyMhIfv75Z2O/yoKnvLyc8+fPk5+fj7u7O82bNycjI8Msrre3t7HQquTj48OlS5dM4lYKDw+vMr/a3rcq7dq1Mym0KnOB3wsxEREREal/mtmSRsfFxQVfX1/j93/5y1/o06cPo0aNYuHChSxevBiA/fv38/bbb/Pdd99x8eJFkxi//fabWVxnZ2eztlatWgFQUFBgdq1z585V5lfb+1altrmIiIiIyLWnYksE8PLyonnz5nz++ecAZGRkEB4eTqdOnZg9ezZOTk40a9YMCwsLZs6cSUVFhVkMKyurauNX1d/W1tasrS73rUptcxERERGRa0/Flsj/KSsro6SkBIDU1FTKysp45513TGaJCgsLOXfuXL3l0FD3FREREZFrT3u2RIBPP/2UwsJCevToAVQ/M7RixQrKy8vrLY+Guq+IiIiIXHua2ZJG58iRI2zbtg2AkpISfvzxRzZt2oS1tTUzZswAIDAwkPj4eCZPnkxoaCjW1tZ8+umnZGVl1ejo9bpqqPuKiIiIyLWnYksandTUVFJTU4Hfj0Nv1aoV9913H5GRkXh6egLQu3dvYmNjefvtt1myZAlNmzbF19eXdevWMW7cuHrLraHuKyIiIiLXnkWFdsuLSDXy8vIICAjA1T8aazvHhk5HRG5xKYuGNnQKIiJGlX8Hpaen4+TkVKcYmtkSkSuKmxdU518yIiI1VVJaho119aepiojcbHRAhoiIiNwQVGiJyK1GxZaIiIiIiEg9ULElIiIiIiJSD1RsiYiISIMpKS1r6BREROqNDsgQkSuKeCVNpxGKSL3QCYQicivTzJZILfn7+xMWFtbQaQAQFhaGv79/Q6chIiIiIlVQsSXyB7/99huenp64u7uzdevWhk5HRERERG5iKrZE/iAlJYWSkhKcnJxISkpq6HSuaNWqVezcubOh0xARERGRKqjYEvmDxMRE+vbty4QJEzh48CC5ubkNnZKZsrIyioqKALCxscHGxqaBMxIRERGRqqjYEvk/hw8f5ujRozz66KMMHjyYJk2akJiYWOPxGzZsIDg4GA8PDwYMGMC6detITk7G3d2dAwcOmPQ9f/48r7/+OkFBQXh4eODj48OsWbPMirvK8Z999hlvvfUWgYGBeHp6smPHDqDqPVsZGRlER0cTHByMl5cXvXr1YvTo0aSlpdXxzYiIiIhIXeg0QpH/k5iYiJ2dHQMGDMDOzo4HH3yQrVu38tRTT2Fpefl/l1i5ciWLFi2iR48ezJ49m6KiIlatWkXr1q3N+p4/f57Ro0dz6tQpRowYgZubG6dPn2bDhg2EhISQlJREx44dTcbExMRw6dIlRo0ahb29Pa6urtXmkpaWxvHjxxk4cCAdO3akoKCALVu2EBUVxcKFCxkyZEjdXpCIiIiI1IqKLRHg4sWLpKamEhwcjJ2dHQDDhg0jLS2NTz75hP79+1c7tqCggKVLl9KtWzc2btxI06ZNAQgJCWHgwIFm/ZcsWUJubi6bNm2ie/fuxvZHH32UIUOGEBsby4IFC0zGFBcXs3XrVmxtba/4LNOmTWP27NkmbWFhYQwbNoxly5ap2BIRERG5TrSMUATYvXs3586dY9iwYca2/v374+joeMWDMj777DMuXrzImDFjjIUWQNu2bc0Km4qKClJSUrj33ntp164d+fn5xi9bW1u8vb3Zt2+f2T3GjBlTo0ILMBaLAEVFRZw9e5aioiJ8fHzIzs7GYDDUKI6IiIiIXB3NbInw+xJCR0dH2rdvT05OjrH9vvvuY+fOneTn5+PoWPWH+ubl5QFUubTvz235+fkUFBSwb98++vXrV2W8qpYsXm7Z4J+dOXOGN998k/T0dM6cOWN2/dy5czg4ONQ4noiIiIjUjYotafRyc3M5cOAAFRUVBAcHV9nn/fffZ+LEiVd9r4qKCgB8fX2ZPHlyjcc1a9asxvEnTZpEdnY248ePx8PDg+bNm2NlZUVSUhKpqamUl5fXKXcRERERqR0VW9LoJScnU1FRwcsvv0zz5s3Nrr/55pskJSVVW2xVHmZx4sQJs9mqEydOmHzv6OhIixYtMBgM+Pr6XpsH+IOsrCwyMzOZPn06Tz75pMm1zZs3X/P7iYiIiEj1VGxJo1ZeXs6WLVvo1q0bISEhVfb58ccfiY2NJSMjA09PT7Prvr6+2NjYsHHjRkaMGGHct3X69GlSUlJM+lpaWjJkyBDWr1/Pzp07qzxA48yZM9x22211ep7KJYiVM2iVjh07pqPfRURERK4zFVvSqO3bt4///d//ZeTIkdX2GTBgALGxsSQmJlZZbLVu3ZqoqCjeeOMNxowZwyOPPEJRURGbNm2ic+fOHDp0CAsLC2P/mTNn8vXXXzNjxgweeughvLy8sLa25tSpU+zdu5cePXqYnUZYU127dsXNzY24uDiKi4txdXXlxIkTJCQk0K1bNw4fPlynuCIiIiJSeyq2pFGr/NDioKCgavt069aNzp0788EHH/Dcc89V2WfKlCk4ODjwz3/+k4ULF3LHHXcQHh5ORUUFhw4dMtlz1bx5czZu3Mjq1avZuXMn6enpWFlZ0b59e3r37l3tDFtNWFlZsWLFCmJiYtiyZQtFRUW4ubkRExNDZmamii0RERGR68ii4s/rjUTkmnnppZdYt24d+/bto23btg2dTq3l5eUREBCAq3801nZVn8YoInI1UhYNbegURESqVPl3UHp6Ok5OTnWKoc/ZErkGLl68aNb2n//8h61bt9KtW7ebstASERERkaujZYQi18CBAwd4/fXXCQoKon379vz8889s2rSJwsJCZs+e3dDpXbW4eUF1/hcdEZHLKSktw8baqqHTEBGpFyq2RK4BFxcXnJ2d2bRpEwUFBTRt2hQPDw+mTJlSL0e8i4jcKlRoicitTMWWyDXg4uLC22+/3dBpiIiIiMgNRHu2RERERERE6oGKLREREbnuSkrLGjoFEZF6p2WEInJFEa+k6eh3EbmmdOS7iDQGmtkSuQX5+/sTFhbW0GmIiIiINGqa2ZKb0oEDBxg/frzxe0tLSxwcHLj99tvp0aMHDz/8MPfffz8WFhYNmKWIiIiINGYqtuSmNnjwYB544AEqKiq4cOECJ06cID09na1bt+Lr68uSJUto0aJFQ6cpIiIiIo2Qii25qd11110MHWq67n/u3Lm8/vrrrFmzhlmzZhEXF9dA2V1fZWVllJSUYGtr29CpiIiIiAjasyW3ICsrK6Kjo+nduzeffPIJX375pfHa+fPnef311wkKCsLDwwMfHx9mzZpFbm6uSYzk5GTc3d3Zv38/q1atIjAwEA8PD4KDg9myZYvZPd3d3YmOjmb//v2Ehobi5eXFAw88wMqVKwH47bffeO655+jXrx9eXl5MmTKFX375xSTGL7/8woIFCxg6dCj33nsvPXv2ZNCgQaxcuZKyMtNTuyrz++yzz3jrrbcIDAzE09OTHTt2VPtecnNzCQ4Oxs/Pj8zMzFq/VxERERGpHc1syS1r5MiRfPXVV3z88cfcc889nD9/ntGjR3Pq1ClGjBiBm5sbp0+fZsOGDYSEhJCUlETHjh1NYixevJji4mJCQ0OxsbFh48aNREdH06lTJ3r37m3S98iRI3z00UeMGjWKoUOHsmPHDhYtWkTTpk3ZunUrHTt2JCoqip9++om1a9fy7LPPEh8fbxyflZXF7t27CQoKolOnTpSWlvLJJ5+waNEi8vLyePHFF82eMSYmhkuXLjFq1Cjs7e1xdXWt8l0cPnyYyMhIWrRoQUJCgtlzioiIiMi1p2JLblnu7u4AnDx5EoAlS5aQm5vLpk2b6N69u7Hfo48+ypAhQ4iNjWXBggUmMUpKSkhMTMTGxgaAgQMHEhAQwPr1682KrWPHjpGQkICXlxfwe7Hn7+/P/PnzGTduHM8//7xJ//j4eI4fP06XLl0A6NOnD+np6SaHekycOJGnn36azZs3ExUVRbt27UxiFBcXs3Xr1ssuHfz000+JiorC3d2dZcuW0bp16yu+OxERERG5elpGKLcsBwcHAAwGAxUVFaSkpHDvvffSrl078vPzjV+2trZ4e3uzb98+sxiPPfaYsdACuP3223F1dTUWcH/k7e1tLLQAbGxs6NmzJxUVFWbHsN9zzz0A5OTkGNuaNWtmLLRKSkooKCggPz8fPz8/ysvLOXTokNk9x4wZc9lCa9u2bUyZMgUfHx/i4+NVaImIiIhcR5rZkluWwWAAfi+68vPzKSgoYN++ffTr16/K/paW5v/24OzsbNbWqlUrfv755xr1bdmyJQBOTk4m7ZUnJBYUFBjbLl26xMqVK9m2bRs5OTlUVFSYjDl37pxZ/OqWDQIcOnSIgwcP4ufnx9KlS7Gysqq2r4iIiIhceyq25JaVlZUF/F6QVBYuvr6+TJ48ucYxqirAqnO5Yqa6a38sqBYsWMDatWsZNGgQU6dOxdHREWtraw4fPszChQspLy83G9+sWbNq79m5c2eaNGnCgQMH+OSTT3jwwQdr/CwiIiIicvVUbMktKzExEYD+/fvj6OhIixYtMBgM+Pr6NnBmVdu2bRv33nsvixcvNmn/41LD2nBwcGDZsmVEREQQFRXFm2++SWBg4LVIVURERERqQHu25JZTVlZGTEwMX331Ff3796d3795YWloyZMgQMjIy2LlzZ5Xjzpw5c50zNWVpaWm2dLCwsNDkxMLacnBwYNWqVXh5eTFjxoz/x96dh1Vd5v8ffwKCihtimA3iksspY9HUAdEsQc01cEFNBAnUNJvStKLJfjmNjpGSTlQu4FKpubCkMKUiaY4bbjNhKpVLDmhpiSAoBsL5/VGer6cDIgji8npcl9fFuT/38v6c64LrvL0/9/uwcePGm4xSRERERG6Udrbkjnb48GHWrVsHwMWLFzlx4gQpKSmcOnWKbt26ERkZaeo7efJkDhw4wKRJk+jbty8eHh7Y2tpy+vRptm3bxiOPPGJRjfBWevLJJ1m9ejWTJk3C29ubX375hbi4OBwcHG5q3jp16hAdHc348eN56aWXmD17Nv369aukqEVERESkNEq25I6WlJREUlIS1tbW2Nvb06RJEzp37sz06dPp3r27Wd969erx6aefsmTJEjZs2EBKSgo2NjY0adKEjh07EhAQUE138ZvXXnuNOnXqmGJ74IEHGD58OG5uboSEhNzU3Pb29ixatIiJEycydepUrly5wlNPPVU5gYuIiIhIiayMf3xuSUTkd5mZmfj6+tLSJxxbe8fqDkdE7iKJkX7VHYKIyHVd/RyUkpJiUVn6RmlnS0TKFPN6rwr/kRERKUlBYRF2tvpKChG5u6lAhoiIiNxySrRE5F6gZEtERERERKQKKNkSERERERGpAkq2REREpEoVFBZVdwgiItVCBTJEpExjZiarGqGIVJgqD4rIvUo7WyIiIiIiIlVAyZbILRAUFISPj091h3HbxCEiIiJyL1CyJfK71NRUDAYDixcvru5QREREROQuDpjcnQAAIABJREFUoGRLRERERESkCijZEimnvLy86g5BRERERO4ASrZESpGZmYnBYCAqKorPP/+cwYMH4+7uzowZM0x9du7cSWhoKJ06dcLNzY2BAwfy6aef3tD8aWlphIeH8+STT+Lh4UGHDh0YMWIEycnJFn3Dw8MxGAzk5uby5ptv0qVLF9zc3BgxYgRff/21Rf+cnBymTZuGp6cn7du3JygoiG+++abib4aIiIiIlJtKv4uUYfPmzXzyySc8/fTTjBgxgrp16wKwevVq3nzzTdq3b8/48eOpXbs2O3fuZPr06fzvf//j1Vdfve68ycnJHD9+nD59+uDs7Ex2djYJCQk8//zzzJkzh4EDB1qMCQsLw9HRkYkTJ5Kdnc3SpUsZN24cKSkpprgKCwsJCwvj4MGD+Pn54eHhQXp6Os888wwODg6V/waJiIiISImUbImU4ejRo6xfv55WrVqZ2s6ePcuMGTPo378/kZGRpvbAwEBmzJjBsmXLGDlyJC4uLqXOO2HCBKZMmWLWFhQUhL+/P/Pnzy8x2WrXrh3Tp083vW7VqhWTJk0iKSmJESNGABAfH8/BgweZOHEiL7zwglnfWbNm4ezsXO73QERERETKT48RipTh8ccfN0u0ADZu3EhBQQFDhw4lKyvL7J+Pjw/FxcXs3LnzuvPa29ubfs7Pz+f8+fPk5+fj5eXFsWPHSjwbFhISYvbay8sLgJMnT5raNm/ejI2NDaGhoWZ9R44cadr9EhEREZGqp50tkTK0aNHCou3YsWOAZfJzrV9++eW68547d4558+aRkpLCuXPnLK5fuHDBIjn6405Zw4YNAcjOzja1ZWRk4OTkZDHWzs4OFxcXLly4cN24RERERKRyKNkSKUPt2rUt2oxGIwARERE0bty4xHHXe4TQaDQSGhrKsWPHCA4OxtXVlXr16mFjY0NcXBxJSUkUFxdbjLOxsSl1PhERERG5vSjZEqmAq7tdDRs2xNvbu9zjv/32W9LT0y3OVQGsXbv2pmJzcXFhx44d5OXlme1uFRQUkJGRQYMGDW5qfhERERG5MTqzJVIBffv2xc7OjqioKC5fvmxxPTc3l4KCglLHW1v/9qv3xx2p7777rsTS7+Xh6+tLUVERS5YsMWtfuXKlviNMRERE5BbSzpZIBTRp0oTp06czbdo0+vXrx1NPPYWzszNZWVl89913bN68mX/96180bdq0xPGtWrWiTZs2xMTEcPnyZVq2bMmJEydYvXo1bdu25dChQxWObfDgwaxZs4YPPviAzMxM2rdvz5EjR9iwYQPNmjWjqKiownOLiIiIyI1TsiVSQUOGDKFFixYsWbKE1atXk5ubi4ODAy1btuTFF1/Eycmp1LE2NjYsXLiQiIgIEhISyM/Pp02bNkRERJCenn5TyZadnR1LlizhnXfeISUlhU2bNuHm5mZqO3XqVIXnFhEREZEbZ2XUyXoRKUVmZia+vr609AnH1t6xusMRkTtUYqRfdYcgIlJuVz8HpaSklPq0Ulm0syUiZYp5vVeF/8iIiBQUFmFnW3I1VRGRu5kKZIiIiEiVUqIlIvcqJVsiIiIiIiJVQMmWiIiIiIhIFVCyJSIiIpWioFBfLSEici0VyBCRMo2ZmaxqhCJSJlUdFBExp50tkXIIDw/HYDBUdxgm8fHxGAwGUlNTqzsUEREREfkDJVtyV0hNTcVgMGAwGHjrrbdK7HPu3DlcXV0xGAwEBQXd4ghFRERE5F6jZEvuKjVr1iQpKYmCggKLa+vWrcNoNFKjxt3z9Kyfnx9paWl07ty5ukMRERERkT9QsiV3lV69epGTk8PmzZstrsXHx9O9e3fs7OyqIbKqYWNjQ82aNbG21q+yiIiIyO1Gn9DkrtKuXTsMBgPx8fFm7WlpaXz//fcMGTLEYsz27duZNGkSvr6+uLu706lTJ0JDQ9mzZ88NrXns2DGmT59O//796dChAx4eHgwePJi1a9ea9Vu2bBkGg4EdO3ZYzFFQUICnpyfBwcGmtgMHDjBmzBi6du2Km5sbjz32GGPHjuW///2vqU9JZ7by8vKYO3cuAQEBeHp64urqSq9evZgzZw75+fk3dE8iIiIicvPunuepRH43ZMgQ3n77bc6cOcP9998PQGxsLI0aNeKJJ56w6J+QkEBOTg7+/v40adKEM2fOsHbtWkJCQvj444/p1KnTddfbs2cP+/bt44knnqBp06bk5+ezYcMGpk2bRlZWFs8++yzw2yN/kZGRxMXF0bVrV7M5kpOTyc7OJiAgAIDjx48TGhrKfffdR3BwMI0aNeLcuXPs37+f9PR02rdvX2o8Z86cITY2lt69ezNgwABq1KjBnj17iImJ4ciRIyxevLg8b6eIiIiIVJCSLbnrPPXUU8yePZuEhATGjx/P5cuX+fzzzwkICCjxvNbf//537O3tzdpGjBhB//79WbhwYZnJlp+fH08//bRZW0hICKNHj2bRokWEhoZia2tLw4YN6d27N5s2bSI7OxsHBwdT/9jYWBo0aEDv3r2B33bb8vPzeffdd3F3dy/X/bu4uLB161ZsbW1NbYGBgcybN4/58+eTlpZW7jlFREREpPz0GKHcdRo2bIiPjw8JCQkAbNq0idzc3BIfIQTMEq2LFy9y/vx5rK2t8fDwIC0trcz1rh3/66+/cv78ebKzs+natSt5eXkcP37cdH3YsGEUFBSQmJhoasvMzGTXrl0MHDiQmjVrAlCvXj0AUlJS+PXXX8tx92BnZ2dKtK5cuUJOTg5ZWVl4e3sD8PXXX5drPhERERGpGO1syV1pyJAhjBs3jn379hEXF4e7uzutW7cuse///vc/5s6dy/bt27lw4YLZNSsrqzLXunjxIu+//z5ffPEFP/74o8X1a+f09PSkRYsWxMbGmsrPx8fHYzQaTY8QAvTv35/169ezYMECli1bhoeHB926daN///44OzuXGdOKFStYtWoVR48epbi42OxaTk5OmeNFRERE5OYp2ZK7Urdu3bj//vv54IMPSE1NZfr06SX2u3jxIoGBgeTn5zN69Gjatm1LnTp1sLa2ZuHChezevbvMtaZMmcLWrVsZNmwYnTt3xsHBARsbG7766iuWLVtmkewMGzaMd955h2+++YZ27dqRkJCAq6srDz30kKmPnZ0dS5cuJS0tjX//+9/s27eP9957j/fff5/IyEh69epVajxLly7l7bffplu3bgQHB9O4cWNsbW05c+YM4eHhGI3GG3sTRUREROSmKNmSu5KNjQ3+/v4sXLiQWrVqMWDAgBL77dq1i7Nnz/KPf/zD4jHDefPmlbnOhQsX2Lp1K35+fhZfprxz584SxwwaNIi5c+cSGxuLr68vp0+fZty4cSX2dXd3N52v+vHHH/H392fevHnXTbbWrVuHs7Mz0dHRZiXht23bVub9iIiIiEjlUbIld60RI0Zga2uLi4sLdevWLbGPjY0NgMVuz/bt22/obNPVZOaP48+ePWtR+v0qR0dHevbsSVJSEj/99BO1a9dm4MCBZn2ysrJwdHQ0a2vSpAmOjo5lPgZobW2NlZWVWUxXrlwhOjq6zPsRERERkcqjZEvuWn/605/4y1/+ct0+HTt2xMnJiYiICE6dOkWTJk04cuQI69ato23btnz33XfXHV+3bl26du3K+vXrqVWrFm5ubpw6dYrVq1fTtGlTsrOzSxw3fPhwvvjiC7Zs2cKgQYMsksH58+ezY8cOUzl5o9HIli1bOH78OGPGjLluTH369CEyMpKxY8fSq1cv8vLySEpKKrESo4iIiIhUHX36knta/fr1iYmJYfbs2SxfvpwrV67g6upKdHQ0sbGxZSZbALNnzyYyMpIvv/yShIQEWrRoweTJk6lRowavvfZaiWO8vLxo3rw5J0+eZOjQoRbXe/bsyc8//8yGDRv45ZdfqFWrFs2bN2fGjBkl9r9WWFgYRqOR2NhYZs6ciZOTE3379mXIkCH069fvxt4YEREREblpVkadlhepFv3796eoqIgNGzZUdyilyszMxNfXl5Y+4djaO5Y9QETuaYmRftUdgohIpbn6OSglJYWmTZtWaA7tbIlUg127dnH06FFeffXV6g7lhsS83qvCf2RE5N5RUFiEna1NdYchInLbULIlcgvt2rWLjIwMFi5ciKOjI8OGDavukEREKo0SLRERc0q2RG6hDz/8kP3799OqVSsiIiJKrZIoIiIiInc+JVsit9Ann3xS3SGIiIiIyC1iXXYXERERERERKS8lWyIiIlJuBYVF1R2CiMhtT48RikiZxsxMVul3ETGjMu8iImXTzpbc8Xx8fAgKCqruMEREREREzGhnS25bGRkZLFq0iL179/Ljjz9iZ2fHfffdh7u7O4MGDcLLy6u6QxQRERERKZWSLbktHTx4kKCgIGrUqIG/vz+tW7fm8uXLnDx5kh07dlCnTh1TsrVhw4ZqjlZERERExJKSLbktffDBB+Tn57Nu3Toeeughi+s///yz6Wc7O7tbGdptq7CwkOLiYmrWrFndoYiIiIgIOrMlt6kffvgBBweHEhMtACcnJ9PPJZ3Zutp27Ngxxo0bR4cOHejYsSMvvPCCWaJ2VXp6OqGhobRv3x5PT09effVVsrKyMBgMhIeHm/VdsWIFoaGhPPbYY7i6utKtWzemTp1KZmamxbxXx+/cuZNhw4bh4eFB165dmTFjBhcvXrTon5mZycsvv4y3tzeurq707NmTd999l/z8fLN+UVFRGAwGvv/+e2bNmkX37t1xd3fnv//9LwAFBQUsWLCA/v374+bmRqdOnRg/fjyHDx8u5R0XERERkcqmnS25LTVr1owTJ06wadMmevfuXaE5zpw5Q3BwMD179uSVV14hPT2d1atXk5eXx5IlS0z9fvjhBwIDAykuLiYoKIj777+fr776ijFjxpQ475IlS2jfvj1BQUE4ODjw3XffERsby+7du0lMTKRhw4Zm/Q8dOsTGjRsJCAjAz8+P1NRUPvnkE77//nuWLl2KtfVv/+dx6tQpAgICyM3NZeTIkTRv3pw9e/awcOFCDhw4wLJly6hRw/xXdurUqdSqVYvQ0FDgtyS0sLCQsLAw/vOf/+Dn50dgYCB5eXmsWbOGp59+muXLl+Pm5lah91REREREbpySLbktTZgwgZ07d/KXv/yFFi1a8Oijj+Lm5oanpyetWrW6oTlOnjzJ3Llz6devn6nN2tqalStXcvz4cR588EEA5s6dS15eHitXrqRjx44AjBo1ikmTJnHo0CGLeRMTE7G3tzdr8/X1JSQkhNjYWMaOHWt27bvvvuODDz6gZ8+eAAQGBjJjxgw++eQTvvjiC/r37w/Au+++S1ZWFosWLeLxxx839Y2IiGDJkiUkJCQQEBBgNnf9+vVZunSpWRK2bNky9uzZQ0xMDI899pipfeTIkQwYMIB33nmHTz755IbeQxERERGpOD1GKLelDh06EBcXx6BBg8jNzSU+Pp6//e1v9OvXj8DAQDIyMsqco3HjxmaJFmAqqnHy5EkAioqK2LZtG+7u7qZE66qru0V/dDXRKi4uJjc31/S4Yb169UhLS7Po37JlS1OiddW4ceMASE5ONs315Zdf0q5dO1OiddWzzz6LtbU1mzdvtph79OjRFrtd69ev58EHH+SRRx4hKyvL9K+goABvb2/279/P5cuXS7w3EREREak82tmS25bBYODtt98GfnvEbu/evaxdu5Z9+/bx3HPPERcXd93iGC4uLhZtDg4OAGRnZwOQlZXFpUuXaNmypUXfktoAdu3axYcffsjXX3/Nr7/+anYtJyfHon9JO3GNGzemfv36pqTxahytW7cuMWYnJ6cSE8wWLVpYtB07dozLly/TpUuXEuMHOH/+PA888ECp10VERETk5inZkjuCs7Mzzs7O+Pn5MXLkSA4cOEBaWhqdOnUqdYyNjU2p14xGY4XiSEtLIywsjGbNmjFlyhSaNm1KrVq1sLKyYvLkyRWet6Jq1apl0WY0Gmnbti2vvfZaqeMcHR2rMiwRERERQcmW3GGsrKzw8PDgwIEDnD179qbnc3R0xN7enhMnTlhcK6ktKSmJoqIioqOjzXbOLl26xIULF0pc49ixYxZtZ8+e5cKFC6Y5HB0dqVOnDkePHrXom5OTw88//8zDDz98Q/fUvHlzzp8/j5eXl6n4hoiIiIjcevokJrelHTt2cOXKFYv2y5cvs2PHDqDkx/PKy8bGhscee4y0tDT2799vdu3aioXX9i/JwoULKS4uLvHaiRMnLM5bRUdHA5jOcllbW9OjRw8OHz7Mtm3bzPouWrSI4uJii3NfpfH39+fnn39m6dKlJV7/5ZdfbmgeEREREbk52tmS29KsWbPIzs7Gx8eHtm3bUqtWLX766ScSExP54Ycf8Pf3x2AwVMpakyZNYvv27YwZM4ZRo0bRpEkTtm7dSlZWFvDbbtpVPXv2ZNmyZYwdO5bhw4dja2vLjh07+Pbbby1Kvl/Vtm1bXn75ZQICAmjevDmpqals3LiRP//5z2YFPF566SV27tzJxIkTGTlyJM2aNWPfvn18/vnndO7cmUGDBt3Q/QQHB7Nz507eeecddu/ejZeXF3Xr1uX06dPs3r0bOzs7VSMUERERuQWUbMltKTw8nJSUFPbv38/GjRvJzc2lXr16tG3blrFjxzJ48OBKW+vBBx9kxYoVRERE8PHHH1OzZk2eeOIJ/t//+3/07NmTmjVrmvp27NiRqKgoPvzwQ/75z39Ss2ZNvL29Wb58OaNGjSpx/kceeYTXXnuNuXPnsmrVKurWrcuoUaOYPHmy2WN+zs7OrFmzhvfee4/169eTm5vL/fffz7PPPsuECRMsqg6WxtbWloULF7Jy5UrWrVtHVFQU8FtRDjc3txtO2kRERETk5lgZb/WJfpE7xDfffMOQIUOYMmWKqVR7eRkMBgYNGmSqqninyczMxNfXl5Y+4djaq6iGiPyfxEi/6g5BRKRKXf0clJKSQtOmTSs0h3a2RPjtLNi1lf2MRiMxMTEAeHt7V1dYt42Y13tV+I+MiNydCgqLsLMtveqriIgo2RIBwM/PDy8vL9q2bUt+fj5btmxh37599OvXD1dX1+oOT0TktqNES0SkbEq2RABfX1+2bNnC+vXruXLlCk2bNuXFF19k7Nix1R2aiIiIiNyhlGyJAK+88gqvvPJKpc/77bffVvqcIiIiInJn0PdsiYiI3CUKCouqOwQREbmGdrZEpExjZiarGqHIHUAVAkVEbi/a2RIREREREakCSrbknpOamorBYMBgMLBmzZoS+xgMBp599tlbHJmIiIiI3E2UbMk9LSoqisuXL1d3GCIiIiJyF1KyJfcsV1dXzp49y0cffVTla+Xl5VX5GiIiIiJye1GyJfesvn378sgjjxAdHc358+fL7L9582ZGjBhB+/bt6dChAyNGjGDz5s0W/Xx8fAgKCuLw4cOEhYXRsWNHnnrqKU6dOoXBYOC9994z6x8WFobBYGDZsmVm7QEBAfTt29f0+tixY0yfPp3+/fvToUMHPDw8GDx4MGvXrjUbt2zZMgwGAzt27LCIraCgAE9PT4KDg8u8XxERERG5OUq25J5lZWXF1KlTyc3NZcGCBdftu2LFCiZOnEhOTg7PPfccEyZMICcnh4kTJ7J69WqL/qdPn2b06NH86U9/4pVXXiEoKAhnZ2dcXFzYvXu3qV9BQQH79+/H2trarD0vL49Dhw7h5eVlatuzZw/79u3jiSee4JVXXuHFF1+kRo0aTJs2jYULF5r6+fn5YWdnR1xcnEVcycnJZGdnExAQUK73SkRERETKT6Xf5Z7m7e1N165dWblyJcHBwTg7O1v0ycnJYc6cOTRr1oy1a9dSt25dAEaOHIm/vz9vv/02ffv2pX79+qYxmZmZzJgxwyKp8fLy4rPPPiM/P5/atWvz9ddfk5+fz1NPPUVKSgpXrlyhRo0a7Nmzh6KiIrNky8/Pj6efftpsvpCQEEaPHs2iRYsIDQ3F1taWhg0b0rt3bzZt2kR2djYODg6m/rGxsTRo0IDevXtXyvsnIiIiIqXTzpbc86ZOnUphYSH//Oc/S7y+Y8cOLl26RFBQkCnRAqhbty5BQUFcunSJnTt3mo1xcHBg8ODBFnN5eXlRWFjIvn37ANi9ezeNGjUiODiYixcvcvDgQeC3iolWVlZ4enqaxtrb25t+/vXXXzl//jzZ2dl07dqVvLw8jh8/bro+bNgwCgoKSExMNLVlZmaya9cuBg4cSM2aNcvzFomIiIhIBSjZknteu3bt6N+/P4mJiaSnp1tcz8zMBKBNmzYW1662ZWRkmLW7uLhgY2Nj0f/qTtXVRwZ3796Np6cnjzzyCA0aNDBrf+ihh8x2pS5evEhERARPPPEE7u7ueHl50aVLF+bOnQvAhQsXTH09PT1p0aIFsbGxprb4+HiMRqMeIRQRERG5RZRsiQCTJk3CxsaGOXPmVMp8tWvXLrH9vvvuo3Xr1uzevZv8/Hy+/vprvLy8sLa2pnPnzuzatYvz58/z7bffmj1CCDBlyhSWLl1K9+7dmTNnDjExMSxdupSQkBAAiouLzfoPGzaM9PR0vvnmG4qLi0lISMDV1ZWHHnqoUu5RRERERK5PyZYIv+1EPf300/z73/8mNTXV4hrA999/bzHu6NGjZn1uhJeXF4cPH2bLli0UFhbSpUsXALp06cJ//vMftm3bhtFoNEu2Lly4wNatW/Hz8+Ott95i4MCBPPbYY3h7e2Nra1viOoMGDcLW1pbY2Fh27NjB6dOnGTp06A3HKSIiIiI3R8mWyO8mTJhA3bp1mT17tll7165dsbe3Z/ny5Wbfl5WXl8fy5cuxt7ena9euN7yOl5cXxcXFvP/++/zpT3+iWbNmpvaCggIWLVpEjRo16NSpk2mMtfVvv6pGo9FsrrNnz1qUfr/K0dGRnj17kpSUxIoVK6hduzYDBw684ThFRERE5OaoGqHI7xwdHQkLC7MolFG/fn2mTp3KW2+9xbBhwxg0aBAACQkJnDx5krfeeot69erd8Dp//vOfsba25tixY2ZFNFq3bo2TkxNHjx6lffv2FsU4unbtyvr166lVqxZubm6cOnWK1atX07RpU7Kzs0tca/jw4XzxxRds2bKFQYMGmc0pIiIiIlVLO1si13jmmWdwcnKyaA8MDOT999+nfv36fPDBB3zwwQemn4cPH16uNRo0aMDDDz8MYFZt8NrXfzyvBTB79myGDBnCl19+yVtvvUVKSgqTJ08mMDCw1LW8vLxo3rw5gB4hFBEREbnFrIx/fC5JRO4q/fv3p6ioiA0bNpR7bGZmJr6+vrT0CcfW3rEKohORypQY6VfdIYiI3DWufg5KSUmhadOmFZpDO1sid7Fdu3Zx9OhRhg0bVt2hiIiIiNxzdGZL5C60a9cuMjIyWLhwIY6OjjedbMW83qvC/6MjIrdOQWERdraW3/EnIiLVQ8mWyF3oww8/ZP/+/bRq1YqIiAgVxhC5RyjREhG5vSjZErkLffLJJ9UdgoiIiMg9T2e2REREREREqoCSLRERkbtAQWFRdYcgIiJ/oMcIRaRMY2Ymq/S7yG1OZd9FRG4/2tmSe0p4eDgGg6G6wxARERGRe4CSLalUGRkZvPHGG/Tp0wcPDw86d+5M3759efXVV9m9e/ctiWHz5s1ERUVV2fzx8fEYDIYSvyT4888/x9XVlT59+vDjjz9WWQwiIiIicvvTY4RSaQ4ePEhQUBA1atTA39+f1q1bc/nyZU6ePMmOHTuoU6cOXl5eVR7H5s2bSUhI4C9/+UuVr3Wt1atXM336dB5++GFiYmJwdNRjdyIiIiL3MiVbUmk++OAD8vPzWbduHQ899JDF9Z9//rkaoro1oqOjmTNnDp07d2bBggWV9r1Wly9fpkaNGtSooV9VERERkTuNHiOUSvPDDz/g4OBQYqIF4OTkZNG2du1aBg0ahLu7Ox07diQ0NJR9+/aZ9cnMzMRgMJT4aGBUVBQGg4HMzEwAgoKCSEhIAMBgMJj+xcfHm43Lzc3lzTffpEuXLri5uTFixAi+/vrrCt13ZGQkc+bMoUePHsTExFgkWunp6UycOBFPT0/c3Nzo168f0dHRFBWZVw67ep4sKyuL1157DW9vb9q3b89PP/1kinn27Nn06tULV1dXvLy8eOmll8jIyDCbJy8vj7lz5xIQEICnpyeurq706tWLOXPmkJ+fX6F7FBEREZHy03+XS6Vp1qwZJ06cYNOmTfTu3bvM/rNnzyYmJgZ3d3deeukl8vLyWLNmDaNHj+bDDz/k8ccfL3cM48ePp7i4mH379vHOO++Y2h999FGzfmFhYTg6OjJx4kSys7NZunQp48aNIyUl5YZ3pYqLi3nzzTdZtWoVAwYMICIiwmIH6tpHKwMDA7nvvvvYsmULc+bMIT09ncjISIt5n3nmGe677z6ee+45Ll26hL29Pbm5uYwYMYLTp08zZMgQ2rRpw88//8zKlSsJCAggLi4OZ2dnAM6cOUNsbCy9e/dmwIAB1KhRgz179hATE8ORI0dYvHhxed9WEREREakAJVtSaSZMmMDOnTv5y1/+QosWLXj00Udxc3PD09OTVq1amfU9fvw4ixcv5tFHH+Wjjz7Czs4OgICAAPr378/f/vY3kpOTsbGxKVcMXbt2JTExkX379uHnV3oZ5Hbt2jF9+nTT61atWjFp0iSSkpIYMWLEDa317rvvkpGRQWBgIG+88QZWVlYWfWbOnElBQQGrVq0y7fiNGjXKtNbQoUPp0qWL2Zg2bdowZ84cs7YZM2aQkZHBmjVrzHYOBw0axMCBA4mKiuLtt98GwMXFha1bt2Jra2vqFxgYyLx585g/fz5paWm4u7vf0D2KiIiISMXpMUKpNB06dCAuLo5BgwaRm5tLfHw8f/vb3+jXrx+BgYFmj7ulpKRgNBoZM2aMKdECuP/++xk8eDCnTp3i8OHHwtgeAAAgAElEQVTDVRZrSEiI2eurhTtOnjx5w3NcPYPWsmXLEhOtc+fO8Z///AcfHx+zBMnKyooJEyYAkJycbDEuLCzM7LXRaCQxMZHOnTvTuHFjsrKyTP9q165N+/bt2b59u6m/nZ2dKdG6cuUKOTk5ZGVl4e3tDVDhxyVFREREpHy0syWVymAwmHZYTp06xd69e1m7di379u3jueeeIy4uDjs7O9MZqzZt2ljMcbUtIyMDNze3KonTxcXF7HXDhg0ByM7OvuE5Xn31VT799FNmzJiB0WgkODjY7PrVe2zdurXF2AcffBBra2uL81YALVq0MHudlZVFdnY227dvt9gFu8ra2vz/TVasWMGqVas4evQoxcXFZtdycnLKvDcRERERuXlKtqTKODs74+zsjJ+fHyNHjuTAgQOkpaXRqVOncs1T0q7RVVeuXKlQbKU9nmg0Gm94DkdHRz766CNCQkKYOXMmxcXFFjtmFVG7du0SY/L29mbs2LFljl+6dClvv/023bp1Izg4mMaNG2Nra8uZM2cIDw8v1z2KiIiISMUp2ZIqZ2VlhYeHBwcOHODs2bPA/+0sff/99zRr1sys/9GjR836NGjQACh5R+bq7tEf17tVHB0dWbZsGSEhIcyaNQv4v0cUmzZtCvzf/Vzr+PHjFBcXW+ywlbZG/fr1ycvLMz0KeD3r1q3D2dmZ6Ohosx2vbdu23cgtiYiIiEgl0ZktqTQ7duwocafp8uXL7NixA8BUKMPHxwcrKysWL15MYWGhqe/Zs2eJj4/H2dmZdu3aAVC3bl2cnJzYvXu32a5MRkYGmzdvtljP3t4eKN8jgTfj6g7XQw89xKxZs1iyZAkAjRo1okOHDmzZsoXvvvvO1N9oNLJo0SIAevXqVeb81tbWDBw4kLS0NDZs2FBin3Pnzpn1t7KyMnuvrly5QnR0dIXuT0REREQqRjtbUmlmzZpFdnY2Pj4+tG3bllq1avHTTz+RmJjIDz/8gL+/PwaDAfjtzFJYWBgxMTGMGjWKvn37cvHiRdasWcOlS5eYM2eO2aN+V6vpjRkzhp49e3L27FlWrVpFmzZtOHjwoFkcHh4eLF++nL/97W88/vjj2Nra4u7ufkO7SBXVsGFDli1bxjPPPENERATFxcWMGTOG119/naCgIAIDAxk5ciROTk5s2bKF7du3M2DAgFLPYP3R5MmTOXDgAJMmTaJv3754eHhga2vL6dOn2bZtG4888ojprFyfPn2IjIxk7Nix9OrVi7y8PJKSkvTFyCIiIiK3mD59SaUJDw8nJSWF/fv3s3HjRnJzc6lXrx5t27Zl7NixDB482Kz/yy+/TPPmzVm5ciWRkZHY2tri4eFBZGSkxbmusWPHkpuby/r169mzZw+tW7dm5syZHDp0yCLZGjBgAEeOHOFf//oXGzZsoLi4mFmzZlVpsgXmCdfs2bMpLi5m3LhxrFq1ivfee49PP/2US5cu4eLiwtSpUwkNDb3huevVq8enn37KkiVL2LBhAykpKdjY2NCkSRM6duxIQECAqW9YWBhGo5HY2FhmzpyJk5MTffv2ZciQIfTr168qbl1ERERESmBl1Gl5ESlFZmYmvr6+tPQJx9besbrDEZHrSIws/bsFRUSk/K5+DkpJSTGdxS8v7WyJSJliXu9V4T8yInJrFBQWYWdbvi+CFxGRqqUCGSIiIncBJVoiIrcfJVsiIiIiIiJVQMmWiIiIiIhIFVCyJSIicpsqKCyq7hBEROQmqECGiJRpzMxkVSMUqQaqMCgicmfTzpaIiIiIiEgVULIlcoeIiorCYDCQmZlpaouPj8dgMJCamlqNkYmIiIhISZRsyV0lJycHd3d3DAYDn3322S1ZMzMzk6ioKI4cOXJL1hMRERGRO4OSLbmrJCYmUlBQQNOmTYmLi7sla546dYr333+/WpItPz8/0tLS6Ny58y1fW0RERESuT8mW3FViY2Px9PRk9OjR7N27l4yMjOoOyYLRaOTixYuVMpeNjQ01a9bE2lq/yiIiIiK3G31Ck7vGoUOHOHLkCIMGDWLAgAHUqFGD2NhYsz6ZmZkYDAaioqIsxpd0JurHH3/ktddeo0ePHri6utKlSxdGjBhBQkIC8NuZqeDgYABee+01DAYDBoOBoKAgAFJTUzEYDMTHx7NixQr69euHm5sbS5YsASAtLY3w8HCefPJJPDw86NChAyNGjCA5OfmG7rmkM1t5eXnMnTuXgIAAPD09cXV1pVevXsyZM4f8/PxyvKMiIiIicjNU+l3uGrGxsdjb29O7d2/s7e154okn+Oyzz3jxxRcrtPNz5coVnnnmGc6cOcPIkSNp0aIFeXl5fPvtt+zbt49BgwbRuXNnxo8fz4IFCxg+fDgdO3YE4L777jOb66OPPiI7O5uAgACcnJxo0qQJAMnJyRw/fpw+ffrg7OxMdnY2CQkJPP/888yZM4eBAweWO+4zZ84QGxtL7969TUnnnj17iImJ4ciRIyxevLjcc4qIiIhI+SnZkrvCr7/+SlJSEk8++ST29vYA+Pv7k5yczL///W8ef/zxcs959OhRTpw4wdSpUxk7dmyJfVxcXPD29mbBggW0b98eP7+SvxPnxx9/5IsvvqBRo0Zm7RMmTGDKlClmbUFBQfj7+zN//vwKJVsuLi5s3boVW1tbU1tgYCDz5s1j/vz5pKWl4e7uXu55RURERKR89Bih3BU2bdrEhQsX8Pf3N7U9/vjjODo6VrhQRr169YDfHgU8d+7cTcXn5+dnkWgBpsQQID8/n/Pnz5Ofn4+XlxfHjh0jLy+v3GvZ2dmZEq0rV66Qk5NDVlYW3t7eAHz99dcVvAsRERERKQ/tbMldITY2FkdHR5o0acLJkydN7V27dmXDhg1kZWXh6OhYrjmdnZ0ZP348ixYtolu3bjz88MN4eXnRp0+fcu8MtWjRosT2c+fOMW/ePFJSUkpM6C5cuEDdunXLtRbAihUrWLVqFUePHqW4uNjsWk5OTrnnExEREZHyU7Ild7yMjAxSU1MxGo08+eSTJfZZv349ISEhWFlZlTrPlStXLNomT57M0KFD2bp1K/v27SM2NpbFixczZswYXn755RuOsXbt2hZtRqOR0NBQjh07RnBwMK6urtSrVw8bGxvi4uJISkqySJRuxNKlS3n77bfp1q0bwcHBNG7cGFtbW86cOUN4eDhGo7Hcc4qIiIhI+SnZkjtefHw8RqORGTNmmB79u9a8efOIi4sjJCSEBg0aACXv7lxbhfBaLi4uBAUFERQUxK+//kpYWBgxMTGEhobSqFGj6yZw1/Ptt9+Snp7OxIkTeeGFF8yurV27tkJzAqxbtw5nZ2eio6PNCoNs27atwnOKiIiISPkp2ZI7WnFxMQkJCbRt25aAgIAS+xw9epSoqChTYQgnJyd2796N0Wg0JUoZGRls3rzZbFxubi61atUyKzRRs2ZNHnzwQfbu3UtOTg6NGjUynbsq7+N5VxOhP+40fffddzdc+r20ea2srMzmvXLlCtHR0RWeU0RERETKT8mW3NG2b9/Ojz/+yNChQ0vt07t3b6KiooiNjcXd3d1UmW/MmDH07NmTs2fPsmrVKtq0acPBgwdN41JTU3njjTfo3bs3LVu2pE6dOnzzzTfExsbi4eHBgw8+CEDr1q2pU6cOK1eupFatWtSvXx9HR0e6dOly3dhbtWpFmzZtiImJ4fLly7Rs2ZITJ06wevVq2rZty6FDhyr0nvTp04fIyEjGjh1Lr169yMvLIykpiRo19OsuIiIicivp05fc0a5+aXGvXr1K7dO2bVtatGjB559/zl//+lfGjh1Lbm4u69evZ8+ePbRu3ZqZM2dy6NAhs2TLYDDQq1cv9uzZQ2JiIsXFxTzwwAM8++yzhIaGmvrVqlWLuXPnMm/ePP7xj39QUFDAn//85zKTLRsbGxYuXEhERAQJCQnk5+fTpk0bIiIiSE9Pr3CyFRYWhtFoJDY2lpkzZ+Lk5ETfvn0ZMmQI/fr1q9CcIiIiIlJ+VkadlheRUmRmZuLr60tLn3Bs7ctXzVFEbl5iZMnf3SciIlXv6ueglJQUmjZtWqE5tLMlImWKeb1Xhf/IiEjFFRQWYWdrU91hiIhIBelLjUVERG5TSrRERO5sSrZERERERESqgJItERERERGRKqBkS0REpBoVFBZVdwgiIlJFVCBDRMo0ZmayqhGKVBFVHBQRuXtpZ0ukDKmpqRgMBuLj4ys8R1BQED4+PpUY1Z0dh4iIiMi9QDtbcs9LTU0lODi41OtTpky5hdGIiIiIyN1CyZbI7wYMGED37t0t2rt168bo0aOpUUO/LiIiIiJy4/TpUeR37dq1w89PZydEREREpHLozJZIGUo6s3VtW1xcHP3798fV1ZUePXoQHR19Q/OmpaURHh7Ok08+iYeHBx06dGDEiBEkJydb9A0PD8dgMJCbm8ubb75Jly5dcHNzY8SIEXz99dcW/XNycpg2bRqenp60b9+eoKAgvvnmm4q/CSIiIiJSbtrZEvldfn4+WVlZZm12dnbXHbNq1Sp++eUXhg4dSv369Vm/fj1z5syhSZMmDBw48Lpjk5OTOX78OH369MHZ2Zns7GwSEhJ4/vnnmTNnTonjw8LCcHR0ZOLEiWRnZ7N06VLGjRtHSkoKdevWBaCwsJCwsDAOHjyIn58fHh4epKen88wzz+Dg4FDOd0VEREREKkrJlsjvoqKiiIqKMmvr168fI0aMKHXM6dOn+eKLL6hXrx4AQ4YMoUePHixfvrzMZGvChAkWxTeCgoLw9/dn/vz5JY5v164d06dPN71u1aoVkyZNIikpyRRnfHw8Bw8eZOLEibzwwgtmfWfNmoWzs/N14xIRERGRyqFkS+R3w4cPp0+fPmZt9913H+fPny91zJAhQ0yJFkDt2rVp3749//nPf8pcz97e3vRzfn4+ly9fxmg04uXlxapVq8jLyzPtVl0VEhJi9trLywuAkydPmto2b96MjY0NoaGhZn1HjhxpkUyKiIiISNVRsiXyu+bNm+Pt7W3RnpqaWuqYpk2bWrQ5ODiQnZ1d5nrnzp1j3rx5pKSkcO7cOYvrFy5csEi2XFxczF43bNgQwGy9jIwMnJycLMba2dnh4uLChQsXyoxNRERERG6eki2Rm2BjY1OhcUajkdDQUI4dO0ZwcDCurq7Uq1cPGxsb4uLiSEpKori4+IbXMxqNFYpDRERERKqOki2RavDtt9+Snp5uca4KYO3atTc1t4uLCzt27LB4DLGgoICMjAwaNGhwU/OLiIiIyI1R6XeRamBt/duv3h93pL777rsSS7+Xh6+vL0VFRSxZssSsfeXKleTl5d3U3CIiIiJy47SzJVINWrVqRZs2bYiJieHy5cu0bNmSEydOsHr1atq2bcuhQ4cqPPfgwYNZs2YNH3zwAZmZmbRv354jR46wYcMGmjVrRlFRUSXeiYiIiIiURjtbItXAxsaGhQsX0qNHDxISEpg5cyZ79+4lIiKCHj163NTcdnZ2LFmyhCFDhvDVV1/xzjvv8MMPP7BkyRKaNGlSSXcgIiIiImWxMupkvYiUIjMzE19fX1r6hGNr71jd4YjclRIj/ao7BBERKcHVz0EpKSklVqC+EXqMUETKFPN6rwr/kRGR6ysoLMLOtmKVTUVE5PamxwhFRESqkRItEZG7l5ItERERERGRKqBkS0REREREpAoo2RIREREREakCSrZEROSeUFCo75gTEZFbS9UIRaRMY2Ymq/S73PFUYl1ERG417WyJ3MFSU1MxGAzEx8dXdygiIiIi8gfa2ZJ7VmpqKsHBwaVet7Gx4fDhwzc8X3x8PBcuXCAkJKQSohMRERGRO52SLbnnDRgwgO7du1u0W1uXb+M3ISGBU6dO3dJkq3PnzqSlpVGjhn6VRURERG43+oQm97x27drh53dnnuWwtramZs2a1R2GiIiIiJRAZ7ZEbsBnn33G0KFD6dSpE+3bt8fX15cpU6aQlZUFgI+PD3v27OHUqVMYDAbTv9TUVNMce/fu5ZlnnqFjx464u7szaNAg1q5da7FWUFAQPj4+nDlzhpdeeonOnTvj4eFBWFgYJ06cMOtb0pmt4uJi5s+fT2BgIF27dsXV1ZUnnniCN998k/Pnz1fROyQiIiIif6SdLbnn5efnm5Kma9nZ2VG3bl0+++wzXn31VTp16sQLL7xArVq1+PHHH/nqq684d+4cjo6O/PWvfyUyMpLz58/z2muvmeZo1aoVAF9++SXPP/889913H8888wx169blX//6F9OmTSMzM5PJkyebrX3p0iVGjRqFh4cHkydPJjMzk48//pjnnnuOpKQkbGxsSr2fwsJCFi9eTO/evfH19aV27docPHiQuLg4Dhw4QFxcHHZ2dpX07omIiIhIaZRsyT0vKiqKqKgoi/YnnniChQsXsnnzZurUqcNHH31kdjbqxRdfNP3cs2dPPvroI3799VeLRxKLior4+9//jr29PWvXruX+++8HYOTIkQQHB7No0SIGDRpEixYtTGPOnz9PWFgYY8eONbU5Ojoye/Zsdu7cyWOPPVbq/djZ2bF9+3Zq1aplanv66afp0KED06ZNY/PmzfTr1+/G3yARERERqRAlW3LPGz58OH369LFod3T87Xul6tWrx+XLl9m6dSu+vr5YWVmVa/5Dhw5x+vRpQkJCTIkW/JYUjRkzhokTJ5KSkkJYWJjpmrW1tUWlRC8vLwBOnjx53WTLysrKlGgVFRVx8eJFrly5YhqflpamZEtERETkFlCyJfe85s2b4+3tXer1Z599lr179zJx4kQcHBz485//TPfu3enbty9169Ytc/7MzEwAWrdubXGtTZs2AGRkZJi1N27c2KLwhYODAwDZ2dllrvn555+zdOlSjhw5QmFhodm1nJycMseLiIiIyM1TsiVShhYtWvD555+za9cudu3axZ49e5g2bRrvvfceK1asoFmzZpW+5vXOZBmNxuuO3bRpE5MnT8bd3Z2//vWvPPDAA9SsWZOioiLGjBlT5ngRERERqRxKtkRugJ2dHY8//jiPP/44AF999RXjxo1j6dKlvPnmm9cd27RpUwCOHj1qce1qm4uLS6XFum7dOmrWrMnHH39M7dq1Te3Hjh2rtDVEREREpGwq/S5ShpIqFbZr1w4wfySvTp065OTkWOwcPfLII/zpT38iPj6en3/+2dR+tWqglZUVvr6+lRavjY0NVlZWFBcXm9qMRiPz58+vtDVEREREpGza2ZJ73uHDh1m3bl2J13r27ElYWBj16tWjU6dOPPDAA1y4cIGEhASsrKzMKg96eHiwZcsW3nrrLTp06ICNjQ1eXl40atSIN954g+eff56hQ4cybNgw6tSpwxdffMF///tfxo8fb1aJ8GY9+eSTbNy4kdGjR+Pv78+VK1fYvHkz+fn5lbaGiIiIiJRNyZbc85KSkkhKSirx2qZNm3j66af54osvWL16NTk5OTg4OPDwww8zbdo0U4U/gJCQEDIyMti4cSOrVq2iuLiYjz/+mEaNGuHj48OyZcuYP38+ixcvprCwkFatWjFjxgwCAgIq9X769+/PxYsXWbZsGRERETRo0IAePXowZcoUPD09K3UtERERESmdlVGn5UWkFJmZmfj6+tLSJxxbe8fqDkfkpiRG+pXdSURE5HdXPwelpKSYzuCXl3a2RKRMMa/3qvAfGZHbRUFhEXa2pVf6FBERqWwqkCEiIvcEJVoiInKrKdkSERERERGpAkq2REREREREqoCSLRERuSkFhUXVHYKIiMhtSQUyRKRMY2YmqxqhlEpV/kREREqmnS2RW8BgMBAeHl7dYdw2cYiIiIjcC5RsyT0tNTUVg8HA4sWLqzsUEREREbnLKNkSERERERGpAkq2REREREREqoAKZIiUYOPGjSxfvpwjR45QWFhIkyZNeOyxx3jllVews7MDwGg08umnnxIbG8uxY8ewtrbG1dWViRMn4uXlVeYan3/+OevXryc9PZ1ffvmFOnXq0LFjR1544QUeeughs74+Pj44Ozszffp0IiIi2Lt3L9bW1nTt2pU33ngDJycns/7ff/89b7/9Nvv378fOzo7HHnuMv/71r5X3BomIiIhImZRsifzB3LlzWbBgAa1btyYkJAQnJyf+97//sWnTJl544QVTsvXyyy/zr3/9iyeffJLBgwdTUFBAYmIioaGhREVF4evre911li9fjoODA8OGDTOtsWbNGp5++mkSEhJo0aKFWf8zZ84QHBxMz549eeWVV0hPT2f16tXk5eWxZMkSU7+MjAwCAwMpKCggMDCQBx54gC1btjBmzJhKf69EREREpHRKtkSukZaWxoIFC/D09CQ6OpqaNWuark2dOtX0c3JyMomJibz11lsMHz7c1B4cHMywYcOYOXMmPj4+WFlZlbpWTEwM9vb2Zm3+/v74+fmxbNkypk+fbnbt5MmTzJ07l379+pnarK2tWblyJcePH+fBBx8EYN68eeTk5PDRRx+ZdtgCAwN5/vnnOXz4cPnfFBERERGpEJ3ZErnG+vXrAZgyZYpZogVgZWVlSp7Wr19PnTp16NmzJ1lZWaZ/Fy5cwMfHh1OnTvHDDz9cd62riZbRaCQvL4+srCwaNmxIy5YtSUtLs+jfuHFjs0QLMCVTJ0+eBKC4uJgvv/wSV1dXs0cZraystLMlIiIicotpZ0vkGidPnsTKysrizNQfHTt2jIsXL+Lt7V1qn3PnztGyZctSrx8+fJh//vOf7Nmzh0uXLplda9q0qUV/FxcXizYHBwcAsrOzTWteunTJtMt1rdatW5cai4iIiIhUPiVbIn9w7Q5WaYxGI46OjkRGRpbap02bNqVeO336NIGBgdStW5cJEybw4IMPUrt2baysrPjHP/5hkXwB2NjYXDceEREREbm9KNkSuUaLFi3Ytm0b6enpuLu7l9qvefPm/PDDD3h4eFCnTp1yr5OcnMylS5eYP3++ReXC7OxsUxGO8nJ0dMTe3p7jx49bXDt69GiF5hQRERGRitGZLZFrDBw4EIB3332XgoICi+tXd5D8/f0pLi7m3XffLXGeX3755brrXN2l+uOO1Jo1a/j555/LHfe18/bo0YNvvvmG3bt3m8UdExNT4XlFREREpPy0syVyDXd3d8aOHUt0dDSDBw+mb9++ODk5kZmZycaNG1m7di3169enT58+DB48mOXLl3Po0CF69OhBw4YN+emnn/jvf//LyZMnSUlJKXWd7t27U7t2bV555RVGjRpF/fr1OXDgANu2baNZs2YUFRVV+B4mTZrEtm3bGD9+PKNGjaJJkyZs2bKFrKysCs8pIiIiIuWnZEvkD6ZOncpDDz3E8uXLiYmJwWg00qRJE7p3706tWrVM/WbNmoWnpydr1qxh4cKFFBYW4uTkRLt27ZgyZcp112jWrBnR0dG8++67LFiwABsbGx599FE++eQT/v73v3Pq1KkKx9+sWTNWrFhBREQEy5cvN32p8TvvvHPdgh4iIiIiUrmsjDpZLyKlyMzMxNfXl5Y+4djaO1Z3OHKbSoz0q+4QREREKt3Vz0EpKSklVoq+ETqzJSIiIiIiUgX0GKGIlCnm9V4V/h8dufsVFBZhZ1v6VxOIiIjcq7SzJSIiN0WJloiISMmUbImIiIiIiFQBJVsiIiIiIiJVQMmWiMhNKCis+HeiiYiIyN1NBTJEpExjZiar9HspVPZcRERESqOdLZHbTFRUFAaDgczMzOoORURERERugpItuaUyMjJ444036NOnDx4eHnTu3Jm+ffvy6quvsnv37mqNLTw8HIPBgMFg4ODBgyX2WbZsmalPfHz8LY5QRERERO4keoxQbpmDBw8SFBREjRo18Pf3p3Xr1ly+fJmTJ0+yY8cO6tSpg5eXV3WHSc2aNYmP///t3XlYFEf+P/D3cF8i+BURDRqNziBBzIAKHjEyICqHIhBXo6BxjaKo2ay7WTXJxo3x3MRo0BATb5EEEgYP1IiAFyp4X6gxYkTBcCgCooAc/fvD38w6mUEZZEDh/XqePDLV1VXVXT0VPlR3tRw9e/ZU2xYXFwdjY2NUVFTorP5p06ZhypQpMDIy0lkdRERERKR7DLao0axevRplZWXYvn07HBwc1LYXFBQ0QavUDRkyBLt27cLcuXNVAp7z58/j6tWr8PPzQ0JCQoPXW1paCgsLCxgYGMDAgF9NIiIiopcdbyOkRnPjxg1YWVlpDLQAwMbGRuXz7t27ERYWhsGDB8PJyQlubm6YPn06rly5oravTCZDSEgIMjMzMWXKFEilUri6umLWrFlaB3GBgYEoLi5GUlKSSrpcLkebNm3g4eGhtk9NTQ0iIyMxbtw4DBgwAE5OThg8eDA+/fRT3Lt3TyVvdnY2JBIJIiIisHv3bgQGBsLZ2Rmff/45AM3PbCnSrl+/juXLl2PQoEFwcnLCiBEjcPDgQbX2VFVV4bvvvoOPjw969uwJNzc3hIeH49dff9XqXBARERFR/fHP59RoOnXqhN9//x2JiYnw9vZ+Zv6oqChYWVlh9OjRsLGxwc2bNxEbG4uxY8ciPj4er776qkr+vLw8hIaGwsvLCx9++CGuXLmCmJgYlJaWYv369XVuZ48ePdCjRw/ExcXBx8cHAFBRUYFdu3YhMDBQ46xTZWUl1q1bB29vb3h6esLU1BQXLlxAXFwcTp8+jbi4OLXbApOSkrBlyxaMHTsWY8aMgYWFxTPbNmfOHBgYGGDSpEmorKzEpk2bEB4ejl9++QWvvPKKMt8//vEP7NmzBwMGDMDYsWNx584dbN26FWPGjMHWrVvh6OhY5/NBRERERPXDYIsazbRp03D06FHMnDkTr776KlxcXJSzLq+99ppa/rVr18LMzEwlLSAgACNHjsTGjRsxf/58lW1ZWVn46quvlAESAOjp6SE6OhrXr19H165d69zWoKAgLFq0CLm5uWjfvj0SExNRUlKCoFe8BpcAACAASURBVKAgXL9+XS2/kZERUlNTYWJiokwbO3YspFIpPv74YyQlJam0CwCuXbuGHTt2aDz22lhbW+Pbb7+FSCQCALi5ueHtt99GTEwMZs+eDQA4cuQI9uzZg+HDh+Orr75S5h0+fDgCAwPx+eefIzo6us51EhEREVH98DZCajRSqRRxcXEYNWoU7t+/D7lcjv/85z/w8fHBuHHjcOvWLZX8ikBLEASUlpaisLAQ1tbW6NKlC86fP69Wfrt27dQCGsWCG1lZWVq11d/fHwYGBoiPjwcA5YIZYrFYY36RSKQMtKqrq1FSUoLCwkJl/Zra+9Zbb2kVaAFAaGioMngCAGdnZ5iZmakc3759+wAAYWFhKnkdHBzg4eGBU6dOobCwUKt6iYiIiEh7nNmiRiWRSLBkyRIAQE5ODk6cOIGffvoJJ0+exPTp01Vut7t06RJWrlyJ48eP4+HDhyrlPHnLnIK9vb1ampWVFQCgqKhIq3ZaWVlBJpMhPj4eI0aMQFpaGj755JOn7rN7925s2LABly9fRmVlpcq24uJitfx/vg2yLjQdo7W1tcpzYdnZ2dDT09MYyHXr1g1JSUnIzs5GmzZ8STERERGRLjHYoibTsWNHdOzYESNHjsQ777yD06dP4/z58+jduzdu376NcePGwcLCAtOmTUPXrl1hamoKkUiERYsWqQVfAKCvr19rXYIgaN2+oKAgvPfee/jkk09gaGgIPz+/WvMmJibigw8+gLOzM+bNmwc7OzsYGxujuroakydP1li/qamp1m3S0+NkNBEREdHLgsEWNTmRSIRevXrh9OnTyM/PB/D4VriHDx8iMjJS7d1bRUVFjfIOqoEDB6J9+/Y4cuQI/Pz8YGlpWWve7du3w9jYGJs3b1YJojIzM3Xezj+zt7dHTU0NMjMz1VZ+VLRH08wgERERETUs/pmcGs2RI0dQVVWlll5eXo4jR44AgPLWN8Us1Z9nhGJjYxvtfVx6enr497//jRkzZuC99957al59fX2IRCLU1NQo0wRBQGRkpK6bqcbLywsA8N1336mcv6tXryIlJQWurq68hZCIiIioEXBmixrN4sWLUVRUBJlMBrFYDBMTE+Tm5mLnzp24ceMGAgICIJFIAACDBg2CqakpPvzwQ4wfPx6WlpY4ffo0Dh06hE6dOqG6urpR2uzp6QlPT89n5hs6dCj27t2LCRMmICAgAFVVVUhKSkJZWVkjtFLVgAEDMHz4cOzatQvFxcXw8PBAQUEBoqOjYWxsjI8//rjR20RERETUEjHYokYzZ84cJCcn49SpU9i7dy/u37+PVq1aQSwW47333kNgYKAyb6dOnfD9999j+fLl+Pbbb6Gvrw8XFxds2bIFCxYsQE5OThMeiTpfX188ePAAGzduxNKlS9G6dWt4eHhg9uzZcHNza/T2fPHFF3B0dER8fDyWLFkCMzMz9OnTB++//74yoCUiIiIi3RIJ9Vk5gIhahOzsbHh6eqKLbA4MzXjroSY7vxzZ1E0gIiIiHVD8HpScnFzv5905s0VEz7T2oyFcVKMWjyqrYWRY+0qYRERE1HJxgQwioufAQIuIiIhqw2CLiIiIiIhIBxhsERERERER6QCDLSKiWjyqbJxXDBAREVHzxAUyiOiZJi/c1yJXI+RKg0RERPQ8OLNFRERERESkAwy2qEVLT0+HRCKBXC5v6qYQERERUTPDYIualVmzZkEikeDy5cu15hEEATKZDL1790Z5eflz1VdSUoKIiAikp6c/VzlERERE1Pww2KJmJTg4GAAQFxdXa560tDTk5OTAx8cHb775Js6fP4+RI+v3bE5JSQlWrVqF48eP12t/IiIiImq+GGxRszJw4EDY2dlh586dePTokcY8ilsGg4ODoaenB2NjY+jr88W0RERERNSwGGxRs6Knp4dRo0ahqKgIKSkpattLS0uRmJgIsVgMZ2fnWp/ZEgQB0dHRCAwMRK9evSCVShESEoK0tDRlnvT0dHh6egIAVq1aBYlEAolEAplMBgDIzs6GRCJBREQE9u/fj6CgIPTs2RMDBw7E0qVLUVVVpVLn+fPnMWfOHAwdOlRZ55gxY7Bv3z6145gzZw4kEgnu3buHOXPmwM3NDVKpFNOnT0dBQQEAICYmBsOHD0fPnj0xbNgwJCUlPd/JJSIiIiKtMNiiZicwMBAikUjjohe7du1CeXk5goKCnlrGP//5TyxYsACdOnXCP//5T8ycOROlpaWYNGkSkpOTAQCvvfYa5s6dCwAYMmQIli1bhmXLlmHevHkqZR08eBDz5s3DoEGDMHfuXEgkEqxfvx5r165Vybdv3z5cv34dw4YNw0cffYRp06ahuLgYM2bMwM6dOzW2c/Lkybh//z5mzZqF0aNH48CBA5gxYwbWrl2LdevWYdSoUZg9ezYqKyvx/vvv49atW3U+j0RERET0fPieLWp27O3t4ebmhtTUVOTn56Ndu3bKbXK5HIaGhhgxYkSt++/btw87d+7EZ599hr/85S/K9NDQUIwePRoLFy6ETCZD27Zt4eXlhcWLF0MikdT63Ne1a9eQkJCAV155BQAwduxY+Pv7IyoqCmFhYcp806ZNw+zZs1X2DQkJQUBAACIjI+Hv769WtrOzMz799FOVtI0bNyIvLw8JCQmwsLAAALi7u2PkyJGIjY1Vq4OIiIiIdIMzW9QsBQcHo7q6Gtu2bVOmZWZm4uzZs5DJZGjTpvYX9O7YsQPm5ubw8vJCYWGh8r+SkhLIZDLk5OTgxo0bdW6Lp6enMtACAJFIBDc3NxQUFODBgwfKdDMzM+XPZWVluHfvHsrKyuDu7o7MzEyUlpaqlT1hwgSVz7179wYAjBw5UhloAYCDgwMsLCyQlZVV53YTERER0fPhzBY1S97e3rC0tIRcLseUKVMA/G+FwmfdQpiZmYkHDx6gf//+tea5e/cuunTpUqe22Nvbq6VZWVkBAIqKimBubq4sc8WKFUhOTsbdu3fV9ikpKVEJoDSVbWlpCQAqwZ1C69atce/evTq1mYiIiIieH4MtapaMjY3h5+eH6OhonD59Gr169cKOHTvQvn17vPnmm0/dVxAEtGnTBl9++WWtebp3717ntjxtpUNBEJT/Tpo0CZmZmQgNDYWTkxNatWoFfX19xMXFISEhATU1NXUum6srEhERETU9BlvUbAUHByM6OhpyuRzFxcUoKChAWFgY9PSefvds586dcePGDfTq1Us561QbkUjUIG399ddfceXKFYSHh2PWrFkq23766acGqYOIiIiIGhef2aJm6/XXX0ePHj2we/dubN26FSKRSPnS46cJCAhATU0Nli9frnH7nTt3lD8rnrMqLi5+rrYqAkDFTJfC1atXNS79TkREREQvPs5sUbMWHByMBQsW4PDhw+jbt6/G56f+bNiwYQgMDERUVBQyMjLg4eEBa2tr5Obm4uzZs8jKylIu/25tbY3OnTtj165dsLe3R9u2bWFqaqp811Zdvfbaa+jevTvWrl2L8vJydOnSBb///jtiYmIgFouRkZFRr+MnIiIioqbDYIuaNX9/fyxbtgwVFRXPXBjjSYsXL4abmxtiY2OxZs0aVFZWwsbGBo6OjmpLp3/xxRdYtGgRvvrqK5SVlaFjx45aB1v6+vpYs2YNli5divj4eJSVlaF79+5YunQprly5wmCLiIiI6CUkEv583xIR0f+XnZ0NT09PdJHNgaFZ7cvlN1c7v9T87jQiIiJq/hS/ByUnJ2tc6bkuOLNFRM+09qMh9R5kXmaPKqthZMiVHYmIiKh+uEAGEVEtGGgRERHR82CwRUREREREpAMMtoiIiIiIiHSAwRYRvRAeVVY3dROIiIiIGhQXyCCiZ5q8cJ/OVyPkyn9ERETU3HBmi0hHJBIJ5syZ09TNICIiIqImwpktIjwOjOrqed61QEREREQtB4MtIgDLli1T+Xzq1CnExMTgL3/5C1xdXVW2tWlTt9vpzp8/Dz09Th4TERERtVQMtogAjByp+rxQdXU1YmJi8MYbb6htqytjY+OGaBoRERERvaT4Z3eiOqqpqUFkZCTGjRuHAQMGwMnJCYMHD8ann36Ke/fuqeX/8zNbISEhkMlkKnkSEhIgkUgwYsQIlfTo6GhIJBKcO3dO67qzs7MhkUgQERGB/fv3IygoCD179sTAgQOxdOlSVFVVNdQpISIiIqKn4MwWUR1VVlZi3bp18Pb2hqenJ0xNTXHhwgXExcXh9OnTiIuLg5GRUa37u7u74+uvv8bNmzfRqVMnAMCxY8egp6eHq1evorCwUHmLYlpaGiwsLODk5FTvug8ePIjo6GiMGTMGQUFBSE5Oxvr169G6dWuEhYXp6CwRERERkQKDLaI6MjIyQmpqKkxMTJRpY8eOhVQqxccff4ykpCT4+PjUur8i2EpLS1MGW2lpafDz88OOHTuQlpYGHx8fCIKA48ePo0+fPtDX16933deuXUNCQoJyMY+xY8fC398fUVFRDLaIiIiIGgFvIySqI5FIpAx2qqurUVJSgsLCQri7uwN4vCDG0zg7O8PMzAxpaWkAgJycHGRnZ8PPzw9isViZ/uuvv+LevXvKcutbt6enp8qqiSKRCG5ubigoKMCDBw/qexqIiIiIqI44s0Wkhd27d2PDhg24fPkyKisrVbYVFxc/dV9DQ0O4uroiPT0dwONbCA0MDNC7d2+4ubnh0KFDAKAMup4MtupTt729vVqalZUVAKCoqAjm5uZPbS8RERERPR8GW0R1lJiYiA8++ADOzs6YN28e7OzsYGxsjOrqakyePBmCIDyzDHd3dxw+fBi//fYb0tLS0LNnT5ibm8Pd3R1btmzB7du3kZaWBmtra5V3f9WnbsUtiJrUpa1ERERE9HwYbBHV0fbt22FsbIzNmzfD1NRUmZ6ZmVnnMhSzVceOHUNaWhqCg4MBAG5ubtDX18eRI0dw8uRJ9O/fHyKRqEHrJiIiIqLGxWe2iOpIX18fIpEINTU1yjRBEBAZGVnnMhwdHdG6dWv8+OOPKCgoUAZfrVq1gqOjIzZu3Ij79++r3ULYEHUTERERUePizBZRHQ0dOhR79+7FhAkTEBAQgKqqKiQlJaGsrKzOZejp6aFPnz5ISkqCsbExXFxclNvc3d3x/fffK39u6LqJiIiIqHFxZouojnx9fbFgwQI8fPgQS5cuxdq1a9GlSxesW7dOq3IUgZRUKlV5N1a/fv0AALa2tujatatO6iYiIiKixiMS+KQ8EdUiKysL3t7eeKVfGAxNrXRa19qPhui0fCIiIiJt5ObmYty4cUhMTETnzp3rVQZvIySiWhUUFAAAso99q/O6PFOW6LwOIiIiIm0VFBTUO9jizBYR1aq8vBwXL16EjY3NU5eSJyIiImpuqqurUVBQACcnJ5iYmNSrDAZbREREREREOsAFMoiIiIiIiHSAwRYREREREZEOMNgiIiIiIiLSAQZbREREREREOsBgi4iIiIiISAcYbBEREREREekAgy0iIiIiIiIdYLBFRERERESkAwy2iKhJrVmzBrNmzYKnpyckEglkMlm9ytm2bRsCAgLg7OyM/v3746OPPkJhYaHGvOfOncPEiRMhlUrh4uKCv/71r7h8+bLGvHl5efjwww/h7u4OZ2dnBAYGYs+ePfVqY0ugbX9q0xcNUYY2/fno0SOsXLkSMpkMTk5O8PLywjfffIPKykqt2vey02WfNsT3S5d9qs248qJrCWNtS7oWONY+Xxm6uo41EQmCINQ5NxFRA5NIJLCysoKjoyMyMjJgYWGBlJQUrcrYuHEjFi9ejL59+8LPzw+5ubnYuHEjOnTogJ9++glmZmbKvGfPnkVISAhsbW0xfvx4AEBUVBTu3r2LH3/8ERKJRJm3qKgIQUFBKCwsxMSJE9G+fXskJCTg+PHjWLRoEYKCghrmJDQj2vSnNn1RG1325/Tp05GcnIygoCBIpVKcOXMGcXFxGDVqFJYsWVLfU/TS0VWfNsT3S5d9qs248jJo7mNtS7sWONaq0lV/NsS5g0BE1IRu3ryp/NnX11fw8PDQav+7d+8KvXr1EoKCgoSqqiplenJysiAWi4XIyEiV/EFBQYJUKhVyc3OVabm5uYJUKhXeffddlbxLly4VxGKxkJycrEyrqqoSgoKChL59+wqlpaVatbUl0KY/temLhihDm/48cOCAIBaLhcWLF6uUsXjxYkEsFgunTp2qU/uaA131aUN8v3TVp9qOKy+D5j7WtrRrgWPt/+iyPxvi3PE2QiJqUvb29s+1f3JyMsrKyjB+/Hjo6+sr02UyGezt7bFjxw5lWlZWFi5cuIBhw4bB1tZWmW5ra4thw4bh6NGjKCgoUKYnJCSgU6dOKrdn6OvrY/z48SgqKsLBgwefq+3NUV37U9u+aIgytOnPnTt3AgAmTJigUqfi85PXVXOnqz5tiO+XrvpUm3HlZdHcx9qWdi1wrP0fXfVnQ5w7gM9sEdFL7sKFCwAAqVSqtq1Xr164fv06Hjx48My8b7zxBgRBQEZGBgAgPz8feXl56NWrl8a8T5ZH2tOmLxqiDG3788KFC7C1tYWdnZ1KXjs7O7Rr1459r0Fjf7902afajCstxYs81vJaqF1LGGt11Z8Nce4ABltE9JLLz88HAJW/OinY2tpCEARlHsW/7dq105gXePxQb13KfTIPaU+bvmiIMrTtz/z8fI15Ffmf1baWqLG/X7rsU23GlZbiRR5reS3UriWMtbrqz4Y4dwCDLSJ6yZWVlQEAjIyM1LYZGxsDAMrLy5+ZV5GmyKPY52nlKvKS9rTpi4YoQ9v+LC8v15hXkV9RHv1PY3+/dNmn2owrLcWLPNbyWqhdSxhrddWfDXHuAAZbRPSSMzU1BfB42dc/q6ioAACYmJg8M68iTZFHsc/TylXkJe1p0xcNUYa2/WliYqIxryK/ojz6n8b+fumyT7UZV1qKF3ms5bVQu5Yw1uqqPxvi3AEMtojoJaeY3tc0lZ+XlweRSKTMo/hX0y0fiv0VtwY8q9wn85D2tOmLhihD2/5s165drbeH5OXlPbNtLVFjf7902afajCstxYs81vJaqF1LGGt11Z8Nce4ABltE9JLr2bMnAODMmTNq286dO4cuXbrA3Nz8mXnPnj0LkUiE119/HcDjQdbW1hbnzp3TmPfJ8kh72vRFQ5ShbX/27NkTeXl5+OOPP1Ty/vHHH8jPz4eTk9NT29YSNfb3S5d9qs240lK8yGMtr4XatYSxVlf92RDnDmCwRUQvOU9PT5iYmGDr1q2orq5WpqekpODWrVvw9/dXpnXu3BlOTk745ZdfVP6qlZeXh19++QXu7u6wsbFRpvv6+uLmzZsqL4qsrq5GVFQULC0tMWjQIB0fXfOlbV80RBna9Kefnx8AYNOmTSp1Kj4/eV3RY03x/dJVn2ozrrQUL/pYy2tBs5Yw1uqqPxvi3AGA/vz58+c/MxcRkY5s27YNKSkpOHHiBNLT01FWVoaqqiqcOHECOTk5cHBwUOaNiIhAaGgoOnbsiB49egB4fL+0sbEx5HI5Tpw4gcrKSqSkpGDp0qWwt7fHokWLVB5u7d69O37++WckJiaipqYGZ8+exfz58/Hw4UOsWLECbdu2VeZ9/fXXsWfPHmzfvh2PHj3CjRs3sGzZMpw5cwaffPKJxuVgWzpt+lObvkhPT4enpydycnLg5eVVrzK06c9XX30VGRkZiI+PR25uLgoLCxEdHY3o6GiMGDECEydO1O2JfIHoqk+1/X5JJBLEx8ervEtHV32q7bjyMmhOYy2vhZY71oaEhGDu3LkYNWoULC0ttS5Dl9dxbUSCIAjP7lIiIt0ICQnB8ePHNW7r27cvtmzZovy8ZMkSbNiwAevXr8eAAQNU8srlcmzcuBG///47LCwsMHjwYPzjH//A//3f/6mVe+bMGaxYsQLnz58HALi4uODvf/+7xtsB8vLy8MUXX+DQoUN4+PAhunXrhvfeew8+Pj7Pc9jNljb9CdS9L1JSUjBt2jSEhYXhgw8+qFcZgHb9WVFRgW+++QY7d+5ULi0cGBiIKVOmwNDQsO4n5SWnqz4F6t4fpaWlcHV1hVQqxY8//livMgDt+1SbceVF11zGWl4Lj7XUsTYwMBDXr1/HoUOHlMGWtmUAuruONWGwRUQvjVGjRsHc3BxRUVFN3RRqZIsXL0Z8fDwSExNhZWXV1M2hRpacnIzp06dj06ZNcHd3b+rmNHsv8ljLa0G3XuSxtri4GP369UNYWBhmzZrV1M2pM4OmbgARUV3cvXsXV65cQWxsbFM3hZpAamoqwsLCXrj/+VPjSE1NhYeHB3+5bgQv+ljLa0G3XuSx9ujRo2jTpg0mT57c1E3RCme2iIiIiIiIdICrERIREREREekAgy0iIiIiIiIdYLBFRERERESkAwy2iIiIiIiIdIDBFhERERERkQ4w2CIiIiIAj1+UKpFImroZDerGjRsIDw/HgAEDIJFI0Lt376ZuEhG1IHzPFhERUQNSBCsdOnTAL7/8AmNjY7U8MpkMOTk5yMjIgIEB/1esK9XV1QgPD0dWVhZGjhyJ9u3ba+yP2mRmZiI6Ohrp6en4448/UFFRASsrKzg6OmLIkCEYOXIkjIyMdHgERPSy4whPRESkA7dv38amTZswZcqUpm5Ki5WdnY1r165h9OjRWLBggVb7rlq1CqtXr0ZNTQ2kUilGjRoFMzMz3LlzB8ePH8fHH3+MH374AXK5XEetJ6LmgMEWERFRA2vdujVEIhG+++47BAcHo02bNk3dpBYpPz8fANCuXTut9vv2228REREBOzs7rFy5Er169VLLs3//fqxfv75B2klEzRef2SIiImpgJiYmmDZtGu7fv4/Vq1fXaZ/09HRIJBJERERo3C6TySCTyVTS5HI5JBIJ5HI5jhw5gnfeeQdSqRTu7u6YO3cuSkpKAACXLl3C1KlT0adPH0ilUoSFhSE7O7vWtjx69AhfffUVZDIZnJyc4OXlhVWrVuHRo0ca82dmZmLOnDl466234OTkhP79+2P27Nm4fv26Wt45c+ZAIpHg1q1b2LJlC/z9/eHs7IyQkJA6naeLFy9i5syZ6NevH5ycnODh4YH58+crAysFiUSC8ePHA3g8SyWRSJ56fhWys7OxatUqGBoa4rvvvtMYaAGAh4cH1q1bp5Iml8sxc+ZMeHp6wtnZGS4uLhgzZgy2b9+usYxbt27hk08+wZAhQ+Ds7Iy+ffvC398f//73v3Hv3j21/AkJCQgJCUHv3r3Rs2dPDB8+HN98843Gfjl58iTCwsIwaNAgODk5YcCAARg9ejRWrVr11OMnoobFmS0iIiIdGDduHLZu3YqYmBiEhITg1Vdf1VldKSkpOHDgAAYPHowxY8bgzJkzkMvlyM7OxuzZszFx4kS4uroiODgYV69exf79+5GdnY0dO3ZAT0/9767vv/8+Lly4gGHDhsHAwADJycmIiIjAxYsXERkZCZFIpMx76NAhzJw5E1VVVfDw8ECnTp2Ql5eHxMREHDhwAJs3b8brr7+uVsfChQtx8uRJvPXWW3jrrbegr6//zOPcv38/Zs6cCQAYOnQoOnTogIyMDPzwww9ITk5GdHQ07O3tAQAzZsxATk4O4uPj0bdvX/Tt2xcAlP/WRi6Xo7KyEr6+vhCLxU/N++fntebPn49u3bqhT58+sLGxQVFREQ4ePIgPP/wQv//+O/72t78p8+bn5yM4OBilpaUYNGgQvL29UVFRoeyX8ePHw9raWpl/7ty5kMvlaN++Pby9vWFpaYmzZ89i5cqVOHbsGDZs2KB8/u/QoUOYOnUqLCwsIJPJYGtri6KiIly/fh3R0dGYMWPGM881ETUQgYiIiBqMWCwW3nzzTUEQBGHPnj2CWCwWwsPDVfJ4eHgIYrFYqKysVKalpaUJYrFY+PrrrzWW6+HhIXh4eKikxcXFCWKxWOjRo4eQnp6uTK+urhYmTpwoiMVioU+fPsL27dtV9ps7d64gFouFffv2qaSPHz9eEIvFgre3t1BUVKRMLy8vF0aPHi2IxWIhPj5emV5UVCT07t1b6Nu3r/Dbb7+plPXrr78Kb7zxhhAQEKCS/q9//UsQi8XCwIEDhZs3b2o8Vk1KS0uFvn37Cg4ODsKJEydUtq1Zs0YQi8XCu+++q5L+rHOqSWhoqCAWi4XY2Ng676OQlZWlllZRUSGEhoYKjo6OQm5urjJ98+bNglgsFjZu3Ki2z4MHD4SysjLlZ0U/h4eHq6QLgiB8/fXXauXMmDFDEIvFwuXLl9XKvnv3rtbHRUT1x9sIiYiIdGTYsGGQSqXYt28fTp48qbN6fH19VWZs9PT0MHLkSABA9+7dMWLECJX8AQEBAIArV65oLG/atGlo3bq18rOxsTH+/ve/AwDi4uKU6du2bUNJSQlmzZqFbt26qZQhFovx9ttv49KlS7h27ZpaHZMnT1bOQtVFcnIyioqK4OPjo7Z8+6RJk9CxY0ccOXIEt2/frnOZmhQUFAAAbG1ttd63U6dOamlGRkYYN24cqqqqcOzYMbXtJiYmamlmZmYq6Zs3b4aBgQEWLVqkln/69OmwsrLCzp071crRtPIinx8kaly8jZCIiEiH/vWvf2HMmDFYtmwZYmNjdVKHk5OTWppiUQhNt/ApAonc3FyN5Wm61c7V1RX6+vq4fPmyMu3s2bMAHgdtmp6FunHjBoDHz3T9ORhzdnbWWHdtLl26BABwd3dX22ZgYIA+ffogJycHly5dQocOHbQqu6Hcvn0b33//PY4dO4Y//vgD5eXlKtvz8vKUP8tkMixfvhyfffYZUlNTMXDgQLi4uKBbt24qt2mWlZXhypUrsLa2xqZNmzTWa2RkhMzMTOVnf39/JCYmYvTo0Rg+fDjc3d3h4uKC9u3bN/ARE9GzMNgiIiLSIalUiqFDh2Lv3r3YvXs3fHx8GryOVq1aqaUpnoF62raqqiqNhBnKgAAABfVJREFU5bVt21YtzcDAANbW1rh7964yraioCACeGUQ+fPiwTnU8zf379wEANjY2Grcr0hX56svGxgaZmZkqgVFd3Lp1C8HBwSgpKUHv3r0xcOBAWFhYQF9fX/ns2JMLWXTs2BE///wzIiIicPjwYSQmJgIA7OzsMGnSJISGhgIASkpKIAgCCgsL67y4hbe3N9asWYP169dDLpcjJiYGwOPAe/bs2RgwYIBWx0ZE9cdgi4iISMdmz56NlJQUfPnll/Dy8tKYR7FQRW0BUElJCSwtLXXWxifduXNHbXaoqqoK9+7dg4WFhTJNEcht374dDg4OWtXx5OxNXSjqUtzm92eKdE3BpTZcXV2RlpaGtLQ0vP3223Xeb8OGDSgqKsLixYsRGBiosi0hIQHx8fFq+7z22mtYsWIFqqqqcOXKFRw9ehRRUVFYuHAhTE1N8fbbbyvPt6Ojo8YyajN48GAMHjwYDx8+xLlz53DgwAH88MMPmDp1KrZt26Y200hEusFntoiIiHSsc+fOGDt2LLKzsxEVFaUxjyKQ0nRrX1ZW1nPP2Gjj+PHjammnTp1CdXU1evTooUxTLIt+6tQpnbdJUa+mtlVVVSmfiXN0dHyuegIDA2FoaIi9e/dqfNbsSU/OVGVlZQF4PKv0Z5ra/CQDAwM4OTlhypQpWL58OYDHz6gBgLm5Obp3747ffvtNOZOoDTMzM/Tr1w9z587F1KlTUVlZiUOHDmldDhHVD4MtIiKiRhAeHg5LS0t8++23ePDggdr2rl27wsLCAsnJySq36pWXl+Pzzz9vzKYiMjISxcXFys8VFRXKICAoKEiZHhgYCEtLS6xatQrnz59XK6empgbp6ekN0iYvLy9YWVlh165dymfFFDZt2oTs7Gz079//uZ/XeuWVVzBjxgxUVlZiypQpuHDhgsZ8hw4dwuTJk5WfO3bsCEA9sDp8+DB+/vlntf0vXryoMYC+c+cOANWFMyZOnIjKykrMmzdP+e60JxUXFyMjI0P5+cSJExpnSBXXlaZFOYhIN3gbIRERUSOwsrLC1KlT8d///lfjdkNDQ4SGhuKbb75BQEAAhgwZgqqqKhw9ehTt2rVTLnjRGLp27QpfX1+V92zdvHkTgwcPVq5yCADW1tb4+uuvER4ejtGjR6Nfv37KBR5yc3Nx5swZFBUV1RqwaMPc3BwLFy7E3/72N4wfPx7Dhg1TvmcrNTUVNjY2+Oyzz567HgAICwtDVVUVVq9ejeDgYEilUjg5OcHc3Bx37tzByZMncePGDZWFSd555x3I5XK8//77GDp0KNq1a4fffvsNhw8fxvDhw7F7926VOrZv346YmBi4urrC3t4erVu3xs2bN7F//34YGRlhwoQJyrzBwcHIyMhAdHQ0hgwZgoEDB8LOzg7FxcXIzs7GiRMnEBgYqDz+zz//HHl5eXBxcUHHjh1haGiIjIwMpKWloWPHjvD19W2Q80REz8Zgi4iIqJGEhoYiOjoaOTk5GrfPmjULpqamiI2NRWxsLNq2bQsfHx/MnDmzUX9BXrlyJVavXo2dO3ciPz8ftra2mDlzJqZMmaL2rFW/fv2wY8cOrF+/HqmpqTh58iQMDQ3Rrl07uLu7Y+jQoQ3WLi8vL0RHR2PNmjVITU1FaWkp2rZtizFjxmD69On1Wq69NjNmzMDw4cMRHR2N9PR0yOVyPHr0CFZWVnBwcMDkyZNVAk8HBwds3rwZK1aswMGDB1FVVQUHBwesWrUKrVq1Ugu2/Pz88OjRI5w5cwYZGRkoLy+Hra0tfH198e6776q9UPnTTz/FoEGD8OOPP+Lo0aO4f/8+WrduDTs7O/z1r39VWd5/6tSpSEpKwsWLF3Hs2DGIRCJ06NABYWFhmDBhgsqy/kSkWyJBEISmbgQREREREVFzw2e2iIiIiIiIdIDBFhERERERkQ4w2CIiIiIiItIBBltEREREREQ6wGCLiIiIiIhIBxhsERERERER6QCDLSIiIiIiIh1gsEVERERERKQDDLaIiIiIiIh0gMEWERERERGRDvw/umSHXBPWlqsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of countries to plot on bar graph\n",
    "num_country_plot = 50\n",
    "\n",
    "df_plot_countries = df_top_countries.iloc[0:num_country_plot]\n",
    "\n",
    "# Plotting graph\n",
    "sns.set_theme(style='white')\n",
    "plt.figure(figsize=(12,16))\n",
    "top_countries_list = format_names(df_plot_countries.index)\n",
    "plt.barh(top_countries_list[::-1], df_plot_countries['Total Cases'].values[::-1]) # Reversing the order to have heighest values at the top of bar chart\n",
    "#plt.title(f'Top {len(top_countries)} Countries with Most Cases')\n",
    "plt.xscale('log')\n",
    "ax = plt.axes() # for updating axes values to plain text\n",
    "ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "plt.margins(y=0)\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "plt.xlabel('Number of Cases', fontsize=20)\n",
    "plt.ylabel('Countries',fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{bar_plot_path}/top_selected_country_cases.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "qbgnoDsBEsoQ",
    "outputId": "280768db-8233-461b-90c2-eb8aba1665fb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n# Plotting graph\\nsns.set_theme(style='white')\\nplt.figure(figsize=(12,16), dpi=90)\\n\\n#plt.bar(df_top_countries.index, df_top_countries['Total Cases'].values)\\ntop_countries_list = format_names(df_top_countries.index)\\nplt.barh(top_countries_list, df_top_countries['Total Cases'].values)\\n\\n#plt.axvline(Number_of_countries-0.5, 0,1, ls='--', c='black')\\nplt.axhline(y=(Number_of_countries-0.5), ls='--', c='black')\\n\\nplt.title(f'Top {len(top_countries)} Countries with Most Cases')\\n\\nplt.xscale('log')\\nax = plt.axes() # for updating \\nax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\\n\\n#plt.gcf().axes[0].yaxis.get_major_formatter().set_scientific(False)  # To get labels in plain text \\n#plt.xticks(rotation=90)\\nplt.margins(y=0)\\nplt.xlabel('Number of Cases')\\nplt.ylabel('Countries')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Plotting graph\n",
    "sns.set_theme(style='white')\n",
    "plt.figure(figsize=(12,16), dpi=90)\n",
    "\n",
    "#plt.bar(df_top_countries.index, df_top_countries['Total Cases'].values)\n",
    "top_countries_list = format_names(df_top_countries.index)\n",
    "plt.barh(top_countries_list, df_top_countries['Total Cases'].values)\n",
    "\n",
    "#plt.axvline(Number_of_countries-0.5, 0,1, ls='--', c='black')\n",
    "plt.axhline(y=(Number_of_countries-0.5), ls='--', c='black')\n",
    "\n",
    "plt.title(f'Top {len(top_countries)} Countries with Most Cases')\n",
    "\n",
    "plt.xscale('log')\n",
    "ax = plt.axes() # for updating \n",
    "ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "#plt.gcf().axes[0].yaxis.get_major_formatter().set_scientific(False)  # To get labels in plain text \n",
    "#plt.xticks(rotation=90)\n",
    "plt.margins(y=0)\n",
    "plt.xlabel('Number of Cases')\n",
    "plt.ylabel('Countries')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0TQucg2Pwu1"
   },
   "source": [
    "## Average cases of top selected countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6kOjiJOSRCtG",
    "outputId": "3c880b9f-b481-4e7b-bc64-dd49b9c69150"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Algeria</th>\n",
       "      <td>248.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Australia</th>\n",
       "      <td>83.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austria</th>\n",
       "      <td>827.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bahrain</th>\n",
       "      <td>259.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Belgium</th>\n",
       "      <td>1719.735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brazil</th>\n",
       "      <td>18793.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Canada</th>\n",
       "      <td>1102.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>China</th>\n",
       "      <td>273.256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Croatia</th>\n",
       "      <td>380.216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Czechia</th>\n",
       "      <td>1546.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Denmark</th>\n",
       "      <td>236.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ecuador</th>\n",
       "      <td>580.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Egypt</th>\n",
       "      <td>348.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estonia</th>\n",
       "      <td>36.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finland</th>\n",
       "      <td>74.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>6602.628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Georgia</th>\n",
       "      <td>407.159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>3136.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greece</th>\n",
       "      <td>312.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iceland</th>\n",
       "      <td>16.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>India</th>\n",
       "      <td>28154.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iran</th>\n",
       "      <td>2823.658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iraq</th>\n",
       "      <td>1648.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ireland</th>\n",
       "      <td>216.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Israel</th>\n",
       "      <td>1008.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>4717.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japan</th>\n",
       "      <td>436.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kuwait</th>\n",
       "      <td>427.706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lebanon</th>\n",
       "      <td>382.361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malaysia</th>\n",
       "      <td>192.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mexico</th>\n",
       "      <td>3294.854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netherlands</th>\n",
       "      <td>1542.479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Norway</th>\n",
       "      <td>103.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oman</th>\n",
       "      <td>374.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pakistan</th>\n",
       "      <td>1202.489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philippines</th>\n",
       "      <td>1294.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qatar</th>\n",
       "      <td>417.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romania</th>\n",
       "      <td>1411.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russia</th>\n",
       "      <td>6753.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San_Marino</th>\n",
       "      <td>4.734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Singapore</th>\n",
       "      <td>173.253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South_Korea</th>\n",
       "      <td>101.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spain</th>\n",
       "      <td>4905.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sweden</th>\n",
       "      <td>766.554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Switzerland</th>\n",
       "      <td>943.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taiwan</th>\n",
       "      <td>2.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United_Arab_Emirates</th>\n",
       "      <td>508.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United_Kingdom</th>\n",
       "      <td>4813.473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United_States_of_America</th>\n",
       "      <td>39831.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vietnam</th>\n",
       "      <td>4.045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Average\n",
       "Algeria                    248.402\n",
       "Australia                   83.015\n",
       "Austria                    827.923\n",
       "Bahrain                    259.066\n",
       "Belgium                   1719.735\n",
       "Brazil                   18793.869\n",
       "Canada                    1102.018\n",
       "China                      273.256\n",
       "Croatia                    380.216\n",
       "Czechia                   1546.795\n",
       "Denmark                    236.167\n",
       "Ecuador                    580.414\n",
       "Egypt                      348.015\n",
       "Estonia                     36.411\n",
       "Finland                     74.184\n",
       "France                    6602.628\n",
       "Georgia                    407.159\n",
       "Germany                   3136.515\n",
       "Greece                     312.057\n",
       "Iceland                     16.015\n",
       "India                    28154.301\n",
       "Iran                      2823.658\n",
       "Iraq                      1648.009\n",
       "Ireland                    216.290\n",
       "Israel                    1008.949\n",
       "Italy                     4717.792\n",
       "Japan                      436.786\n",
       "Kuwait                     427.706\n",
       "Lebanon                    382.361\n",
       "Malaysia                   192.493\n",
       "Mexico                    3294.854\n",
       "Netherlands               1542.479\n",
       "Norway                     103.414\n",
       "Oman                       374.194\n",
       "Pakistan                  1202.489\n",
       "Philippines               1294.771\n",
       "Qatar                      417.614\n",
       "Romania                   1411.784\n",
       "Russia                    6753.917\n",
       "San_Marino                   4.734\n",
       "Singapore                  173.253\n",
       "South_Korea                101.789\n",
       "Spain                     4905.318\n",
       "Sweden                     766.554\n",
       "Switzerland                943.503\n",
       "Taiwan                       2.009\n",
       "United_Arab_Emirates       508.342\n",
       "United_Kingdom            4813.473\n",
       "United_States_of_America 39831.312\n",
       "Vietnam                      4.045"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_countries_avg = Counter(total_countries)\n",
    "\n",
    "for country in dict_countries.keys():\n",
    "  dict_countries_avg[country] = df_grouped.get_group(country)['cases'].mean()\n",
    "\n",
    "# Average cases for all countries\n",
    "df_avg_cases_countries = pd.DataFrame.from_dict(dict_countries_avg, orient='index', columns=['Average'])\n",
    "\n",
    "# List of top selected countries\n",
    "top_countries = list(df_top_countries.index)\n",
    "\n",
    "# Average of selected top countries\n",
    "avg_df = df_avg_cases_countries[df_avg_cases_countries.index.isin(top_countries)]\n",
    "avg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkabfg44B2iE"
   },
   "source": [
    "# Common Methods for All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwvQ0-1b7Nk5"
   },
   "outputs": [],
   "source": [
    "countries = top_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jkIXUcMCpwy"
   },
   "source": [
    "## Common Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHYQO1XBALXe"
   },
   "outputs": [],
   "source": [
    "# Return a combined dataframe for a each error statistics(MAE,RMSE,MAPE etc) along with the newly added mean row.\n",
    "def get_metric_with_mean(result: pd.DataFrame, error_metric: str)->pd.DataFrame:\n",
    "  df_grouped = result.groupby('EvaluationMeasurement')\n",
    "  df = df_grouped.get_group(error_metric).reset_index(drop=True)\n",
    "  df = df.append(df.describe().loc['mean'])\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AVGVaKLwhHu"
   },
   "outputs": [],
   "source": [
    "def calc_mean_to_max_error(df, max_of_pretrain_days, max_of_df):\n",
    "  i=-1\n",
    "  for row_num in range(len(df)-1):  # Go before mean row\n",
    "    i += 1\n",
    "    for col_num in df.columns[2:]:\n",
    "      df.loc[row_num,col_num] = df.loc[row_num,col_num]/max_of_pretrain_days[i] \n",
    "  \n",
    "  for col in df.columns[2:]:\n",
    "      df.loc['mean',col] = df.loc['mean',col]/max_of_df\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgvjK5nJkUaG"
   },
   "outputs": [],
   "source": [
    "# Note: Do not change the filenames, since they are later being used for visualizations \n",
    "def save_runtime(df,path,country=None,static_learner=True,alternate_batch=False, transpose=False):\n",
    "  df = df.apply(pd.to_numeric,errors='ignore') # Converting the dataframe to numeric\n",
    "  df = df.round(decimal) # Setting the precision\n",
    "  \n",
    "  # if transpose flag is set to true\n",
    "  if transpose:\n",
    "    df = df.transpose()\n",
    "\n",
    "  if country==None:\n",
    "    if static_learner:\n",
    "      df.to_latex(f'{path}/combined_country_runtime_static.tex')\n",
    "      df.to_csv(f'{path}/combined_country_runtime_static.csv')\n",
    "    else:\n",
    "      if alternate_batch:\n",
    "         df.to_latex(f'{path}/combined_country_runtime_incremental_alternate_batch.tex')\n",
    "         df.to_csv(f'{path}/combined_country_runtime_incremental_alternate_batch.csv')\n",
    "      else:\n",
    "        df.to_latex(f'{path}/combined_country_runtime_incremental.tex')\n",
    "        df.to_csv(f'{path}/combined_country_runtime_incremental.csv')\n",
    "  else:\n",
    "    if static_learner:\n",
    "      df.to_latex(f'{path}/{country}_runtime_static.tex')\n",
    "      df.to_csv(f'{path}/{country}_runtime_static.csv')\n",
    "    else:\n",
    "      df.to_latex(f'{path}/{country}_runtime_incremental.tex')\n",
    "      df.to_csv(f'{path}/{country}_runtime_incremental.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJCVpF7oT7r0"
   },
   "outputs": [],
   "source": [
    "# Note: Do not change the filenames, since they are later being used for visualizations \n",
    "def save_summary_table(df,path,country=False,static_learner=True,alternate_batch=False, transpose=False):\n",
    "\n",
    "  df = df.apply(pd.to_numeric,errors='ignore') # Converting the dataframe to numeric\n",
    "  df = df.round(decimal) # Setting the precision\n",
    "  \n",
    "  # if transpose flag is set to true\n",
    "  if transpose:\n",
    "    df = df.transpose()\n",
    "\n",
    "  if country:\n",
    "    metric = df.loc['EvaluationMeasurement'].unique()[0]\n",
    "    if static_learner:\n",
    "      df.to_latex(f'{path}/top_countries_{metric}_summary_table_static.tex')\n",
    "      df.to_csv(f'{path}/top_countries_{metric}_summary_table_static.csv')\n",
    "    else:\n",
    "      df.to_latex(f'{path}/top_countries_{metric}_summary_table_incremental.tex')\n",
    "      df.to_csv(f'{path}/top_countries_{metric}_summary_table_incremental.csv')\n",
    "    \n",
    "  else:\n",
    "    if static_learner:\n",
    "      df.to_latex(f'{path}/combined_country_summary_table_static.tex')\n",
    "      df.to_csv(f'{path}/combined_country_summary_table_static.csv')\n",
    "    else:\n",
    "      if alternate_batch:\n",
    "         df.to_latex(f'{path}/combined_country_summary_table_incremental_alternate_batch.tex')\n",
    "         df.to_csv(f'{path}/combined_country_summary_table_incremental_alternate_batch.csv')\n",
    "      else:\n",
    "        df.to_latex(f'{path}/combined_country_summary_table_incremental.tex')\n",
    "        df.to_csv(f'{path}/combined_country_summary_table_incremental.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnS1UsFdKRbW"
   },
   "outputs": [],
   "source": [
    "# Note: Do not change the filenames since they are later being used for visualizations\n",
    "def save_metrics(df, path, country=None, static_learner=True, alternate_batch=False, transpose=False): \n",
    "  df = df.apply(pd.to_numeric,errors='ignore') # Converting the dataframe to numeric\n",
    "  df = df.round(decimal) # Setting the precision\n",
    "  \n",
    "  # if transpose flag is set to true\n",
    "  if transpose:\n",
    "    df = df.transpose()\n",
    "\n",
    "  metric_type = df.loc['EvaluationMeasurement'].unique()[0]\n",
    "  if country==None:\n",
    "    if static_learner:\n",
    "      df.to_latex(f'{path}/combined_country_{metric_type}_static.tex')\n",
    "      df.to_csv(f'{path}/combined_country_{metric_type}_static.csv')\n",
    "    else:\n",
    "      if alternate_batch:\n",
    "         df.to_latex(f'{path}/combined_country_{metric_type}_incremental_alternate_batch.tex')\n",
    "         df.to_csv(f'{path}/combined_country_{metric_type}_incremental_alternate_batch.csv')\n",
    "      else:\n",
    "        df.to_latex(f'{path}/combined_country_{metric_type}_incremental.tex')\n",
    "        df.to_csv(f'{path}/combined_country_{metric_type}_incremental.csv')\n",
    "  else:\n",
    "    if static_learner:\n",
    "      df.to_latex(f'{path}/{country}_{metric_type}_static.tex')\n",
    "      df.to_csv(f'{path}/{country}_{metric_type}_static.csv')\n",
    "    else:\n",
    "      df.to_latex(f'{path}/{country}_{metric_type}_incremental.tex')\n",
    "      df.to_csv(f'{path}/{country}_{metric_type}_incremental.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQFR3NKzuSK-"
   },
   "outputs": [],
   "source": [
    "def save_combined_summary_table(df, path, static_learner=False,transpose=False):\n",
    "  df = df.apply(pd.to_numeric,errors='ignore')\n",
    "  df = df.round(decimal)\n",
    "  if transpose:\n",
    "    df = df.transpose()\n",
    "  \n",
    "  if static_learner:\n",
    "    save_path = f'{path}/summary_table_combined_mean_static'\n",
    "  else:\n",
    "    save_path = f'{path}/summary_table_combined_mean_incremental'\n",
    "\n",
    "  df.to_csv(f'{save_path}.csv')\n",
    "  df.to_latex(f'{save_path}.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Umh96O9TSp2Z"
   },
   "outputs": [],
   "source": [
    "def save_united_df(df, path, country=None):\n",
    "    if country:\n",
    "        df.to_csv(f'{path}/{country}.csv')\n",
    "    else:\n",
    "        df.to_csv(f'{path}/united_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUrUegDEPqof"
   },
   "outputs": [],
   "source": [
    "def display_runtime_per_country(results_runtime,countries):\n",
    "  for i in range(len(countries)):\n",
    "    print(f'_____________Running Time for {countries[i]}________________')\n",
    "    print(results_runtime[i].to_string())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI4IiENZtLFG"
   },
   "outputs": [],
   "source": [
    "def calc_save_err_metric_countrywise(countries, error_metrics, results, max_of_pretrain_per_country, max_cases_per_country, path, static_learner, transpose):\n",
    "  countrywise_error_scores={}\n",
    "  for i in range(len(countries)):\n",
    "    country_error_score = []\n",
    "    for error_metric in error_metrics:\n",
    "      \n",
    "      df_error_metric = get_metric_with_mean(results[i], error_metric=error_metric)\n",
    "\n",
    "      #if error_metric != 'MAPE':\n",
    "      #  df_error_metric = calc_mean_to_max_error(df_error_metric, max_of_pretrain_per_country[i], max_cases_per_country[i])\n",
    "\n",
    "      country_error_score.append(df_error_metric)\n",
    "      display_countrywise_scores(countries[i],df_error_metric)\n",
    "\n",
    "      # Transposing the metrics while saving\n",
    "      save_metrics(df_error_metric, path=path, country=countries[i], static_learner=static_learner, transpose=transpose)\n",
    "      \n",
    "    countrywise_error_scores[countries[i]] = pd.concat(country_error_score,ignore_index=True)\n",
    "    \n",
    "  return countrywise_error_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0e4uCM0f65tm"
   },
   "outputs": [],
   "source": [
    "def calc_save_err_metric_combined(error_metrics, results, max_of_pretrain_days, max_selected_countries, path, static_learner, alternate_batch, transpose):\n",
    "  combined_err_metric = []\n",
    "  for error_metric in error_metrics:\n",
    "    df_error_metric = get_metric_with_mean(results, error_metric=error_metric)\n",
    "\n",
    "    #if error_metric != 'MAPE':\n",
    "    #  df_error_metric = calc_mean_to_max_error(df_error_metric, max_of_pretrain_days, max_selected_countries)\n",
    "\n",
    "    # Transposing the metrics while saving\n",
    "    save_metrics(df_error_metric, path=path, static_learner=static_learner, alternate_batch=alternate_batch, transpose=transpose)\n",
    "    \n",
    "    combined_err_metric.append(df_error_metric)\n",
    "  return (pd.concat(combined_err_metric, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7nnCdGu8QVD"
   },
   "outputs": [],
   "source": [
    "def get_summary_table(df_result, df_runtime_result, error_metrics, static_learner=True):\n",
    "  sum_metric=[]\n",
    "  measure_col_name = 'Metric'\n",
    "  \n",
    "  # Setting start row and column for static and incremental learner\n",
    "  for metric in error_metrics:\n",
    "    start_row = 'mean'\n",
    "    if static_learner:\n",
    "      start_col='RandomForest'\n",
    "    else:\n",
    "      start_col='HT_Reg'\n",
    "\n",
    "    df_metric = get_metric_with_mean(df_result, metric)\n",
    "    df_row = pd.DataFrame([df_metric.loc[start_row][start_col:]])\n",
    "    \n",
    "    df_row[measure_col_name] = str(metric)    \n",
    "    sum_metric.append(df_row)\n",
    "\n",
    "  # Adding run time\n",
    "  df_runtime_row = pd.DataFrame([df_runtime_result.describe().loc[start_row][start_col:]])\n",
    "  df_runtime_row[measure_col_name]='Time(sec)'\n",
    "  sum_metric.append(df_runtime_row)\n",
    "\n",
    "  df_summary = pd.concat(sum_metric, ignore_index=True)\n",
    "  df_summary.set_index(measure_col_name, inplace=True)\n",
    "\n",
    "  return df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLrQqgWXAPgi"
   },
   "outputs": [],
   "source": [
    "def get_summary_table_countrywise(df_result_dict, error_metrics, static_learner=True):  #df_runtime_result,\n",
    "  summary_metric=[]\n",
    "  measure_col_name = f'Country({str(error_metrics[0])})'\n",
    "  eval_measure_col = 'EvaluationMeasurement'\n",
    "  start_row = 'mean'\n",
    "  if static_learner:\n",
    "    start_col='RandomForest'\n",
    "  else:\n",
    "    start_col='HT_Reg'\n",
    "\n",
    "  for country in df_result_dict.keys():\n",
    "    df_result = df_result_dict[country]\n",
    "\n",
    "    # Setting start row and column for static and incremental learner\n",
    "    for metric in error_metrics:      \n",
    "      df_metric = get_metric_with_mean(df_result, metric)\n",
    "      df_row = pd.DataFrame([df_metric.loc[start_row][start_col:]])\n",
    "      df_row[eval_measure_col] = metric\n",
    "      df_row[measure_col_name] = country\n",
    "      summary_metric.append(df_row)\n",
    "\n",
    "  df_summary = pd.concat(summary_metric, ignore_index=True)\n",
    "  df_summary.set_index(measure_col_name, inplace=True)\n",
    "\n",
    "  return df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSX0nYK3SW2Q"
   },
   "outputs": [],
   "source": [
    "def get_sum_table_combined_mean(countrywise_error_score_incremental,results_runtime, static_learner=False):\n",
    "  sum_table_combined_mean=[]\n",
    "  measure_col_name = 'Metric'\n",
    "  start_row = 'mean'\n",
    "  if static_learner:\n",
    "    start_col = 'RandomForest'\n",
    "  else:\n",
    "    start_col= 'HT_Reg'\n",
    "\n",
    "  for metric in error_metrics:\n",
    "    df_sum_cur_metric = get_summary_table_countrywise(countrywise_error_score_incremental, [metric], static_learner=static_learner)\n",
    "    df_row = pd.DataFrame([df_sum_cur_metric.describe().loc[start_row]])\n",
    "\n",
    "    df_row[measure_col_name] = metric\n",
    "    sum_table_combined_mean.append(df_row)\n",
    "\n",
    "  # Adding run time\n",
    "  df_runtime = pd.concat(results_runtime, ignore_index=True).describe().loc[start_row][start_col:]\n",
    "  df_runtime_row = pd.DataFrame([df_runtime])\n",
    "  df_runtime_row[measure_col_name]='Time(sec)'\n",
    "  sum_table_combined_mean.append(df_runtime_row)\n",
    "\n",
    "  # Concating results to one dataframe\n",
    "  sum_table_combined_mean = pd.concat(sum_table_combined_mean, ignore_index=True)\n",
    "  sum_table_combined_mean.set_index(measure_col_name, inplace=True)\n",
    "  return sum_table_combined_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AKhvSFwAzUj"
   },
   "outputs": [],
   "source": [
    "def check_significance(target_pop, competitor_pop, significance_at: float):\n",
    "    \"\"\"\n",
    "    Comparing algorithms per batch or per country pairs (exp 2 or 1 respectively), \n",
    "      so for each pair, we compare the significance of the best algo to all of the the other algos.\n",
    "    Ttest performed if the distribution is normal, otherwise we perform a non-parametric test.\n",
    "    \"\"\"\n",
    "    model_pop, population = target_pop, competitor_pop  \n",
    "    \n",
    "    # Normality tests\n",
    "    if len(model_pop) >= 8:  # skew test not valid for smaller populations\n",
    "      value_mdl, p_mdl = normaltest(model_pop.values)\n",
    "      value_pop, p_pop = normaltest(population.values)\n",
    "      if (p_mdl >= 0.05) & (p_pop >= 0.05):\n",
    "          # print('It is likely that both populations are normal. Thus, running T-Test...')\n",
    "          tset, pval = stats.ttest_ind(model_pop, population)\n",
    "          if pval < significance_at:    # alpha value is 0.05 or 5%\n",
    "              significant = 'Significant (Ttest)'\n",
    "          else:\n",
    "              significant = 'Not Significant (Ttest)'\n",
    "      else:\n",
    "          # print('It is unlikely that the result is normal. Thus, running Wilcoxon test...')\n",
    "          if np.sum(np.subtract(list(model_pop), list(competitor_pop))) != 0.0:  # if values are identical the test will crash, but we now it's not significant\n",
    "              tset, pval = stats.wilcoxon(model_pop, population)\n",
    "              if pval < significance_at:    # alpha value is 0.05 or 5%\n",
    "                  significant = 'Significant (Wilcox Test)'\n",
    "              else:\n",
    "                  significant = 'Not Significant (Wilcox Test)'\n",
    "          else:\n",
    "              # print('Warning: results are identical')\n",
    "              tset, pval = stats.ttest_ind(model_pop, population)\n",
    "              significant = 'Not Significant (Wilcox Test)'\n",
    "    else:\n",
    "      print('Population too small.')\n",
    "      if np.sum(np.subtract(list(model_pop), list(competitor_pop))) != 0.0:  # if values are identical the test will crash, but we now it's not significant\n",
    "          tset, pval = stats.wilcoxon(model_pop, population)\n",
    "          if pval < significance_at:    # alpha value is 0.05 or 5%\n",
    "              significant = 'Significant (Wilcox Test)'\n",
    "          else:\n",
    "              significant = 'Not Significant (Wilcox Test)'\n",
    "    return pval, significant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuW-RWNMWGE5"
   },
   "outputs": [],
   "source": [
    "def unit_incremental_df(country_name, evaluator, date, milestone):  # Added Now\n",
    "    frame = {}\n",
    "    frame['date'] = date\n",
    "    if type(country_name) == pd.Series:\n",
    "        frame['Country'] = country_name\n",
    "    else:\n",
    "        frame['Country'] = [country_name] * len(date)\n",
    "    frame['Milestone'] = [milestone] * len(date)\n",
    "    frame['y_true'] = evaluator.mean_eval_measurements[0].y_true_vector\n",
    "    for i in range(len(evaluator.model_names)):\n",
    "        frame[f'pred_{evaluator.model_names[i]}'] = evaluator.mean_eval_measurements[i].y_pred_vector\n",
    "    return pd.DataFrame(frame)\n",
    "\n",
    "\n",
    "\n",
    "def unit_static_df(country_name, date, y_true,  milestone, model_predictions):  # Added Now\n",
    "    frame = {}\n",
    "    frame['date'] = date\n",
    "    if type(country_name) == pd.Series:\n",
    "        frame['Country'] = country_name\n",
    "    else:\n",
    "        frame['Country'] = [country_name] * len(date)\n",
    "\n",
    "    frame['Milestone'] = [milestone] * len(date)\n",
    "    frame['y_true'] = y_true\n",
    "\n",
    "    for algo, y_pred in model_predictions.items():  # Updated Now\n",
    "        if algo == 'LSTM':\n",
    "            frame[f'pred_{algo}'] = y_pred.flatten().tolist()\n",
    "        else:\n",
    "            frame[f'pred_{algo}'] = y_pred\n",
    "\n",
    "    return pd.DataFrame(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZYx2PYrRBMV"
   },
   "source": [
    "## Combining Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g11ZDONMQvHT"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def sortby_date_and_set_index(df):\n",
    "  df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y')\n",
    "  df.sort_values('date', inplace=True)\n",
    "  df.set_index('date', inplace=True)\n",
    "  return df\n",
    "'''\n",
    "\n",
    "def sortby_date_and_set_index(df):  # Updated\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y')\n",
    "    df.sort_values('date', inplace=True)\n",
    "    # df.set_index('date', inplace=True) #TODO:  Not setting date as idx; Might need to remove this line later\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZr-9CCDwIVo"
   },
   "outputs": [],
   "source": [
    "def get_dataset_with_target(countries, df_grouped):\n",
    "    # Empty list to store Dataframes of each country\n",
    "    frames = []\n",
    "\n",
    "    for country in countries:\n",
    "        df = df_grouped.get_group(country)\n",
    "\n",
    "        # Creating feature 'day_no'\n",
    "        df['day_no'] = pd.Series([i for i in range(1, len(df) + 1)], index=df.index)\n",
    "\n",
    "        # Reordering features\n",
    "        # df = df[['day_no', 'country', 'cases']]\n",
    "        df = df[['date', 'day_no', 'country', 'cases']]  # Added: Date column\n",
    "\n",
    "        # Adding features through lags\n",
    "        df = create_features_with_lags(df)\n",
    "\n",
    "        # Creating target with last 10 days cases\n",
    "        idx_cases = list(df.columns).index('cases')  # Added: Earlier hard coded idx\n",
    "        df['target'] = df.iloc[:, [idx_cases] + [i * -1 for i in range(1, 10)]].mean(axis=1)  # Updated: Replacing idx with idx_cases\n",
    "\n",
    "        # Dropping null columns\n",
    "        df.dropna(how='any', axis=0, inplace=True)\n",
    "\n",
    "        # Dropping mid columns\n",
    "        drop_columns = list(df.loc[:, 'cases_t-39':'cases_t-1'].columns)  # Updated: cases_t-38 to t-39 for exact 50 columns of lags\n",
    "        df.drop(drop_columns, axis=1, inplace=True)\n",
    "\n",
    "        frames.append(df)\n",
    "\n",
    "    return (pd.concat(frames, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sit5iHT_0UWl"
   },
   "outputs": [],
   "source": [
    "def reshape_dataframe(*data: np.ndarray):\n",
    "    # This function adds an extra dimension which is necessary in the LSTM\n",
    "    arr = []\n",
    "    for d in data:\n",
    "        arr.append(np.reshape(np.array(d), (d.shape[0], 1, d.shape[1])))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1buCiuYoliN"
   },
   "outputs": [],
   "source": [
    "def get_countries_sortedby_cases(valid_countries, df_grouped):\n",
    "  # A dictionary of all countries\n",
    "  dict_countries = Counter(valid_countries)\n",
    "\n",
    "  for country in dict_countries.keys():\n",
    "    dict_countries[country] = df_grouped.get_group(country)['cases'].sum()\n",
    "\n",
    "  # Sorting countries based on number of cases\n",
    "  countries_sortedby_cases = sorted(dict_countries.items(), key=lambda dict_countries: dict_countries[1], reverse=True)\n",
    "\n",
    "  # Creating dataframe \n",
    "  df_countries_sortedbycases = pd.DataFrame.from_dict(dict(countries_sortedby_cases), orient='index', columns=['Total Cases'])\n",
    "  \n",
    "  return df_countries_sortedbycases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvuRHEVArNJ0"
   },
   "outputs": [],
   "source": [
    "# Getting a list of valid countries\n",
    "def get_countries_with_valid_size(df):\n",
    "  total_countries = list(df_grouped.groups.keys())\n",
    "\n",
    "  # A list for countries with required datasize\n",
    "  valid_countries = []\n",
    "\n",
    "  # List of countries with more than 230 records. Because, max training size = 150, lags removed = 50, prediction = 30.\n",
    "  for country in total_countries:\n",
    "    if len(df_grouped.get_group(country)) >= 230:\n",
    "      valid_countries.append(country)\n",
    "\n",
    "  return valid_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tqHxnYhwsbk"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(df):  \n",
    "  # Selecting required features\n",
    "  df= df[['dateRep','cases','countriesAndTerritories']]\n",
    "\n",
    "  # Rename features\n",
    "  df.rename(columns={'countriesAndTerritories':'country', 'dateRep':'date'}, inplace=True)\n",
    "\n",
    "  # Convert to date, sort and set index\n",
    "  df = sortby_date_and_set_index(df)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6nW5zkHuFLn"
   },
   "outputs": [],
   "source": [
    "# Calculating maximum of dataframe for every pretrain size\n",
    "def calc_max_of_pretrain_days(pretrain_days,df)->list:\n",
    "  max_of_pretrain_days = []\n",
    "  \n",
    "  for day in pretrain_days:\n",
    "    df_subset = create_subset(df,day)\n",
    "    max_of_pretrain_days.append(df_subset['cases'].max())\n",
    "  \n",
    "  return max_of_pretrain_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ze3Ju2mOFltA"
   },
   "outputs": [],
   "source": [
    "def display_scores(results):\n",
    "  #print(f'_________________________________{country}____________________________________________')\n",
    "  df_MAE = get_metric_with_mean(results,'MAE' )\n",
    "  df_RMSE = get_metric_with_mean(results,'RMSE')\n",
    "  df_MAPE = get_metric_with_mean(results,'MAPE')\n",
    "  print('MAE Score')\n",
    "  print(df_MAE.to_string())\n",
    "  print('-----------------------------------------------------------------------------------')\n",
    "  print('RMSE Score')\n",
    "  print(df_RMSE.to_string())\n",
    "  print('-----------------------------------------------------------------------------------')\n",
    "  print('MAPE Score')\n",
    "  print(df_MAPE.to_string())\n",
    "  print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZbb27ZOpQmT"
   },
   "source": [
    "## Alternate Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SKzw1NdWenN"
   },
   "outputs": [],
   "source": [
    "def get_alternate_batch_records_idx(batch_size,total_records): \n",
    "  total_batches = total_records//batch_size\n",
    "  current_batch=1\n",
    "  start_idx = 0\n",
    "  end_idx = batch_size\n",
    "  idx_list = []\n",
    "  \n",
    "  while current_batch <= total_batches:\n",
    "    if current_batch%2!=0:\n",
    "      idx_list.extend([x for x in range(start_idx,end_idx)])\n",
    "      start_idx = idx_list[-1]+(batch_size+1)\n",
    "      end_idx = start_idx + batch_size\n",
    "    current_batch += 1\n",
    "  \n",
    "  '''Added code for odd records'''\n",
    "  if total_batches == 0 and total_records != 0:  # records less the batch size\n",
    "      idx_list.extend([x for x in range(start_idx, total_records)])\n",
    "\n",
    "  elif total_batches % 2 == 0 and total_records % batch_size != 0:  # if few extra records present in odd batches\n",
    "      extra_records = total_records % batch_size\n",
    "      s = idx_list[-1]+(batch_size+1)  # index start\n",
    "      idx_list.extend([x for x in range(s, s+extra_records)])\n",
    "\n",
    "  return idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1Sdy4o7oufO"
   },
   "outputs": [],
   "source": [
    "def create_alternate_batch_subset(df,days,batch_size):\n",
    "  df_grouped = df.groupby('country')\n",
    "  countries = df['country'].unique()\n",
    "  frame1,frame2 = [],[]\n",
    "\n",
    "  for country in countries:\n",
    "    df_cur_country = df_grouped.get_group(country)\n",
    "\n",
    "    df1 = df_cur_country.iloc[0:days//2]\n",
    "    df2 = df_cur_country.iloc[days:days+30]  # Adding 30 for a testing batch that is one month ahead\n",
    "    \n",
    "    # Selecting alternate batches\n",
    "    idx = get_alternate_batch_records_idx(batch_size,total_records=len(df2))\n",
    "    df2 = df2.iloc[idx]\n",
    "\n",
    "    # Appending dataframes\n",
    "    frame1.append(df1)\n",
    "    frame2.append(df2)\n",
    "\n",
    "  r1 = pd.concat(frame1, ignore_index=True)\n",
    "  r2 = pd.concat(frame2, ignore_index=True)\n",
    "  r = r1.append(r2, ignore_index=True)\n",
    "  \n",
    "  return (r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPViJY9uDZ0q"
   },
   "source": [
    "## Incremental Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9DZn8PjUter"
   },
   "outputs": [],
   "source": [
    "def instantiate_regressors():\n",
    "  ht_reg = HoeffdingTreeRegressor()\n",
    "  hat_reg = HoeffdingAdaptiveTreeRegressor()\n",
    "  arf_reg = AdaptiveRandomForestRegressor()\n",
    "  pa_reg = PassiveAggressiveRegressor(max_iter=1, random_state=0, tol=1e-3)\n",
    "\n",
    "  model = [ht_reg, hat_reg, arf_reg, pa_reg]\n",
    "  model_names = ['HT_Reg', 'HAT_Reg', 'ARF_Reg', 'PA_Reg']\n",
    "\n",
    "  return model, model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NwhtdOBXBUy"
   },
   "outputs": [],
   "source": [
    "def get_error_scores_per_model(evaluator, mdl_evaluation_scores)-> pd.DataFrame:\n",
    "  \n",
    "  for i in range(len(evaluator.model_names)):\n",
    "    # Desired error metrics\n",
    "    mse = evaluator.mean_eval_measurements[i].get_mean_square_error()\n",
    "    mae = evaluator.mean_eval_measurements[i].get_average_error()\n",
    "    mape = evaluator.mean_eval_measurements[i].get_mean_absolute_percentage_error()\n",
    "    rmse = sqrt(mse)\n",
    "\n",
    "    # Dictionary of errors per model\n",
    "    mdl_evaluation_scores[str(evaluator.model_names[i])] = [rmse, mae, mape]\n",
    "\n",
    "  return(pd.DataFrame(mdl_evaluation_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSq_iuypN2BE"
   },
   "outputs": [],
   "source": [
    "def get_running_time_per_model_incremental_learner(evaluator,day):\n",
    "    cols = ['PretrainDays']  # Adding pretrain as first column\n",
    "    cols += evaluator.model_names  # Adding remaining columns of different algorithm\n",
    "    running_time = []\n",
    "    running_time.append(day)\n",
    "    for i in range(len(evaluator.model_names)):\n",
    "        running_time.append(evaluator.running_time_measurements[i]._total_time)\n",
    "\n",
    "    return (pd.DataFrame([running_time],columns=cols))  # Passing running_time as a list of list to insert it as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VbOOY4WfUzT"
   },
   "outputs": [],
   "source": [
    "def display_countrywise_scores(country,df_error_metric):\n",
    "  print(f'_________________________________{country}____________________________________________')\n",
    "  print(df_error_metric.to_string())\n",
    "  print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QC8vD-Dl3uNs"
   },
   "outputs": [],
   "source": [
    "# Create a dataframe of all countries with pre-train size = pretrain days and test&train size = pretrain days\n",
    "def create_subset(result,days):\n",
    "  result_grouped = result.groupby('country')\n",
    "  countries = result['country'].unique()\n",
    "  frame1,frame2 = [],[]\n",
    "  for country in countries:\n",
    "    df = result_grouped.get_group(country) \n",
    "    df1 = df.iloc[0:days]\n",
    "    df2 = df.iloc[days:days+30]\n",
    "    frame1.append(df1)\n",
    "    frame2.append(df2)\n",
    "\n",
    "  r1 = pd.concat(frame1, ignore_index=True)\n",
    "  r2 = pd.concat(frame2, ignore_index=True)\n",
    "  r = r1.append(r2, ignore_index=True)\n",
    "  \n",
    "  return (r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGzbZRfVDc-c"
   },
   "source": [
    "## Static Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKVRq7O9onX7"
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(actual, predicted):\n",
    "    \"\"\"\n",
    "    Mean absolute percentage error (MAPE).\n",
    "    :return error\n",
    "    \"\"\"\n",
    "    actual =  np.array(actual) \n",
    "    predicted = np.array(predicted) \n",
    "\n",
    "    mask = actual != 0\n",
    "    return (np.fabs(actual - predicted) / np.fabs(actual))[mask].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olpCsifni4p-"
   },
   "outputs": [],
   "source": [
    "def get_scores(y_true, model_predictions, days):\n",
    "    mdl_evaluation_scores = {}\n",
    "    mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE', 'MAE', 'MAPE']\n",
    "    mdl_evaluation_scores['PretrainDays'] = [days] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
    "\n",
    "    for model in model_predictions:\n",
    "        y_pred = model_predictions[model]\n",
    "        if model == 'LSTM':\n",
    "            rmse = mean_squared_error(y_true[:, np.newaxis], y_pred, squared=False)\n",
    "            mae = mean_absolute_error(y_true[:, np.newaxis], y_pred)\n",
    "            mape = mean_absolute_percentage_error(y_true[:, np.newaxis], y_pred)\n",
    "        else:\n",
    "            rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "        mdl_evaluation_scores[model] = [rmse, mae, mape]\n",
    "    return pd.DataFrame(mdl_evaluation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBKHhy5I0sWy"
   },
   "outputs": [],
   "source": [
    "def get_running_time_per_model_static_learner(model_predictions,total_execution_time):\n",
    "    cols = ['PretrainDays']\n",
    "    cols += model_predictions.keys()\n",
    "    return pd.DataFrame(total_execution_time, columns=cols)\n",
    "\n",
    "\n",
    "def measure(wrapped_func):\n",
    "    @wraps(wrapped_func)\n",
    "    def _time_it(*args, **kwargs):\n",
    "        start = pc_timer()\n",
    "        try:\n",
    "            model_predictions = wrapped_func(*args, **kwargs)\n",
    "        finally:\n",
    "            end_ = pc_timer() - start\n",
    "            return model_predictions, end_\n",
    "    return _time_it\n",
    "\n",
    "\n",
    "@measure\n",
    "def train_test_model(regressor, X_train, y_train, X_test):\n",
    "    regressor.fit(X_train, y_train)\n",
    "    return regressor.predict(X_test)\n",
    "\n",
    "\n",
    "@measure\n",
    "def train_test_lstm(regressor, X_train_lstm, y_train, X_val_lstm, y_val, X_test_lstm, patience, epochs, batch_size_lstm):\n",
    "    regressor.compile(loss='mae', optimizer='adagrad', metrics=['mse', 'mae'])\n",
    "\n",
    "    history = regressor.fit(\n",
    "        X_train_lstm,\n",
    "        y_train,\n",
    "        validation_data=(X_val_lstm, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size_lstm,\n",
    "        callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                                 mode='min',\n",
    "                                 patience=patience)])\n",
    "\n",
    "    return regressor.predict(X_test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bB-GhumBoRRH"
   },
   "outputs": [],
   "source": [
    "def define_lstm_model(x_train_lstm, layers, activations, patience):\n",
    "    # Start defining the model\n",
    "    input_shape = x_train_lstm.shape\n",
    "\n",
    "    # Definining model first with LSTM n layers\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(layers[0], input_shape=input_shape[1:], activation=activations[0], return_sequences=True))\n",
    "\n",
    "    # Adding middle layers\n",
    "    for l in range(1, len(layers)-1):\n",
    "      model.add(LSTM(layers[l], activation=activations[l], return_sequences=True))\n",
    "      model.add(Dropout(0.2))\n",
    "\n",
    "    # Add last Dense and LSTMs layers\n",
    "    if len(layers) > 1:\n",
    "      model.add(Dense(layers[-1], activation=activations[-1]))\n",
    "      model.add(Dropout(0.2))\n",
    "      model.add(LSTM(layers[-1], activation=activations[-1]))\n",
    "\n",
    "    model.add(Dense(1))  # output layer. Since we have only 1 output value\n",
    "    # End defining model\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zjnk6w9_cqG6"
   },
   "outputs": [],
   "source": [
    "def normalize_dataset(*dataframes):\n",
    "    arr = []\n",
    "    for df in dataframes:\n",
    "        arr.append(StandardScaler().fit_transform(df))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GQGxLLNw1z5"
   },
   "outputs": [],
   "source": [
    "def get_validation_set(df_train, batch_size=10):  # Updated Now\n",
    "    '''\n",
    "    lst_idx = -1\n",
    "    total_batches = len(df_train) // batch_size\n",
    "    train_set, val_set = [], []\n",
    "\n",
    "    for cur_batch in range(total_batches):\n",
    "        start = lst_idx + 1\n",
    "        end = start + batch_size\n",
    "        if cur_batch % 2 == 0:\n",
    "            train_set.append(df_train.iloc[start:end])\n",
    "        else:\n",
    "            val_set.append(df_train.iloc[start:end])\n",
    "\n",
    "        lst_idx = end - 1  # adjusting last index because we add 1 in starting\n",
    "    '''\n",
    "    train_set, val_set = [], []\n",
    "    countries = df_train['country'].unique()\n",
    "    for country in countries:\n",
    "        train_set.append(df_train[df_train['country'] == country].iloc[:-batch_size, :])\n",
    "        val_set.append(df_train[df_train['country'] == country].iloc[-batch_size:])\n",
    "    return pd.concat(train_set, ignore_index=True), pd.concat(val_set, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMyZ5jcy_m6j"
   },
   "source": [
    "# Experiment 1\n",
    "Training and testing with five countries individually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDnpy-TycyY9"
   },
   "source": [
    "### Dataset Description\n",
    "\n",
    "* cases(t): Number of cases on current day(Column='cases') \n",
    "\n",
    "* cases(t-n): Number of cases 'n' days before current day 't'\n",
    "\n",
    "* 30 day gap: Training from day number t-89 to t-39(50 days). Then a gap of 30 days and then creating target by averaging t to t-9(10 Days).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeJJB0sdNnIq"
   },
   "outputs": [],
   "source": [
    "# Sample set for understanding dataset\n",
    "sample_df = pd.read_csv(f'{csv_processed_path}/United_States_of_America.csv')\n",
    "sample_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJiGVxO_hsNv"
   },
   "source": [
    "## Incremental Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZrg_v5CLReb"
   },
   "outputs": [],
   "source": [
    "def reset_evaluator(evaluator):  # Added Now\n",
    "  for j in range(evaluator.n_models):\n",
    "      evaluator.mean_eval_measurements[j].reset()\n",
    "      evaluator.current_eval_measurements[j].reset()\n",
    "  return evaluator\n",
    "\n",
    "\n",
    "def update_incremental_metrics(evaluator, y, prediction):  # Added Now\n",
    "  for j in range(evaluator.n_models):\n",
    "    for i in range(len(prediction[0])):\n",
    "      evaluator.mean_eval_measurements[j].add_result(y[i], prediction[j][i])\n",
    "      evaluator.current_eval_measurements[j].add_result(y[i], prediction[j][i])\n",
    "\n",
    "    # Adding result manually causes y_true_vector to have a objects inserted like array([123.45]) in a list.\n",
    "    # For calculating metrics we have to convert them into flat list.\n",
    "    evaluator.mean_eval_measurements[j].y_true_vector = np.array(evaluator.mean_eval_measurements[j].y_true_vector).flatten().tolist()\n",
    "    evaluator.current_eval_measurements[j].y_true_vector = np.array(evaluator.current_eval_measurements[j].y_true_vector).flatten().tolist()\n",
    "  return evaluator\n",
    "\n",
    "\n",
    "def get_error_scores_per_model(evaluator, mdl_evaluation_scores, inc_alt_batches=False) -> pd.DataFrame:\n",
    "  for i in range(len(evaluator.model_names)):\n",
    "    # Desired error metrics\n",
    "    mse = evaluator.mean_eval_measurements[i].get_mean_square_error()\n",
    "    mae = evaluator.mean_eval_measurements[i].get_average_error()\n",
    "    if not inc_alt_batches:\n",
    "      mae = mae[0]  # get_average_error() is returning a List instead of single value.\n",
    "    mape = evaluator.mean_eval_measurements[i].get_mean_absolute_percentage_error()\n",
    "    rmse = sqrt(mse)\n",
    "\n",
    "    # Dictionary of errors per model\n",
    "    mdl_evaluation_scores[str(evaluator.model_names[i])] = [rmse, mae, mape]\n",
    "  return (pd.DataFrame(mdl_evaluation_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQ3htjYaL-1o"
   },
   "outputs": [],
   "source": [
    "def scikit_multiflow(df, pretrain_days, country):  # Added Country in parameter\n",
    "    # Creating a stream from dataframe\n",
    "    stream = DataStream(np.array(df.iloc[:, 4:-1]), y=np.array(df.iloc[:, -1]))  # Selecting features x=[t-89:t-39] and y=[target].\n",
    "\n",
    "    model, model_names = instantiate_regressors()\n",
    "\n",
    "    frames, running_time_frames = [], []\n",
    "\n",
    "    united_dataframe = []  # Added Now\n",
    "\n",
    "    # Setup the evaluator\n",
    "    for day in pretrain_days:\n",
    "        pretrain_days = day\n",
    "        # max_samples = pretrain_days + 30  # Training and then testing on set one month ahead only\n",
    "        max_samples = pretrain_days + 1\n",
    "        testing_samples_size = 30\n",
    "\n",
    "        evaluator = EvaluatePrequential(show_plot=False,\n",
    "                                        pretrain_size=pretrain_days,\n",
    "                                        metrics=['mean_square_error', 'mean_absolute_error',\n",
    "                                                 'mean_absolute_percentage_error'],\n",
    "                                        max_samples=max_samples)\n",
    "\n",
    "        # Run evaluation\n",
    "        evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
    "\n",
    "        X = stream.X[pretrain_days: pretrain_days + testing_samples_size]\n",
    "        y = stream.y[pretrain_days: pretrain_days + testing_samples_size]\n",
    "        target_dates = df.iloc[pretrain_days: pretrain_days +testing_samples_size, 0]  # Added Now\n",
    "\n",
    "        prediction = evaluator.predict(X)\n",
    "\n",
    "        # Since we add one extra sample, reset the evaluator\n",
    "        evaluator = reset_evaluator(evaluator)\n",
    "\n",
    "        evaluator = update_incremental_metrics(evaluator, y, prediction)\n",
    "\n",
    "        united_dataframe.append(unit_incremental_df(country, evaluator, target_dates, pretrain_days))  # Added now\n",
    "\n",
    "        # Dictionary to store each iteration error scores\n",
    "        mdl_evaluation_scores = {}\n",
    "\n",
    "        # Adding Evaluation Measurements and pretraining days\n",
    "        mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE', 'MAE', 'MAPE']\n",
    "        mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
    "        mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\n",
    "\n",
    "        # Errors of each model on a specific pre-train days\n",
    "        frames.append(mdl_evaluation_df)\n",
    "\n",
    "        # Run time for each algorithm\n",
    "        running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
    "\n",
    "    # Final Run Time DataFrame\n",
    "    running_time_df = pd.concat(running_time_frames, ignore_index=True)\n",
    "\n",
    "    # Final Evaluation Score Dataframe\n",
    "    evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    united_dataframe = pd.concat(united_dataframe, ignore_index=True)  # Added Now\n",
    "    return evaluation_scores_df, running_time_df, united_dataframe  # Added united_dataframe in return statement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUw_30g9T4fS"
   },
   "outputs": [],
   "source": [
    "# Training all countries\n",
    "results_incremental = []\n",
    "results_runtime_incremental = []\n",
    "max_of_pretrain_per_country = []\n",
    "max_cases_per_country = []\n",
    "\n",
    "for country in countries:\n",
    "    # Read each country\n",
    "    df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\n",
    "\n",
    "    # Get evaluation scores and running time for country\n",
    "    evaluation_scores_df, running_time_df, united_dataframe = scikit_multiflow(df_country,pretrain_days, country)\n",
    "\n",
    "    save_united_df(united_dataframe, exp1_inc_united_df_path, country=country)\n",
    "\n",
    "    # Appending evaluation scores and runtime for each country\n",
    "    results_incremental.append(evaluation_scores_df)\n",
    "\n",
    "    results_runtime_incremental.append(running_time_df)\n",
    "\n",
    "    # Get max of each pretrain subset and for each country dataset\n",
    "    max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days,df_country))\n",
    "    max_cases_per_country.append(df_country['cases'].max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HQdWByW7GTn"
   },
   "outputs": [],
   "source": [
    "# Save the running time for each country\n",
    "for i in range(len(countries)):\n",
    "  save_runtime(results_runtime_incremental[i], path=exp1_runtime_path, country = countries[i], static_learner=False)\n",
    "\n",
    "# Display countrywise running time complexity\n",
    "display_runtime_per_country(results_runtime_incremental, countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMH2CG_nuloA"
   },
   "outputs": [],
   "source": [
    "countrywise_error_score_incremental = calc_save_err_metric_countrywise(countries, error_metrics, results_incremental, max_of_pretrain_per_country, max_cases_per_country, path=exp1_path, static_learner=False, transpose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6hi5rPZF4Uf"
   },
   "outputs": [],
   "source": [
    "# Get summary table for each country for specified metric\n",
    "summary_table_countrywise_incremental = get_summary_table_countrywise(countrywise_error_score_incremental, ['MAPE'], static_learner=False)\n",
    "\n",
    "# Saving the summary table\n",
    "save_summary_table(summary_table_countrywise_incremental, exp1_summary_path,country=True, static_learner=False,alternate_batch=False,transpose=True)\n",
    "\n",
    "summary_table_countrywise_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jEmM1GgdAqq"
   },
   "outputs": [],
   "source": [
    "sum_inc_countrywise_mean = get_sum_table_combined_mean(countrywise_error_score_incremental,results_runtime_incremental)\n",
    "save_combined_summary_table(sum_inc_countrywise_mean, exp1_summary_path, static_learner=False, transpose=True) \n",
    "sum_inc_countrywise_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEBzePUFh8FO"
   },
   "source": [
    "## Static Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dtk8rnQGTPgy"
   },
   "outputs": [],
   "source": [
    "def scikit_learn(df, training_days, country):  #Added country now\n",
    "    frames = []\n",
    "    model_predictions = {\n",
    "        'RandomForest': [],\n",
    "        'GradientBoosting': [],\n",
    "        'LinearSVR': [],\n",
    "        'DecisionTree': [],\n",
    "        'BayesianRidge': [],\n",
    "        'LSTM': [] \n",
    "      # 'MLPRegressor': [],\n",
    "      # 'LinearRegression': []\n",
    "    }\n",
    "    total_execution_time = []\n",
    "\n",
    "    # params (others like epoch and batch size are also hardcoded in train_test_lstm())\n",
    "    layers = [50, 30, 20, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\n",
    "    activations = ['tanh', 'tanh', 'relu']\n",
    "    epochs = 200  # Previously: 500\n",
    "    patience = 20\n",
    "    batch_size_lstm = 10\n",
    "    united_dataframe = []  # Added Now\n",
    "\n",
    "    for day in training_days:\n",
    "        print(f\"~~~~~~~~~~~~~~~~~~~~*Pretraining Day: {day}~~~~~~~~~~~~~~~~~~~~\")\n",
    "        testing_samples_size = 30  # Added Now\n",
    "        cur_exec_time = [day]  # Keeping running time for each pre-train set\n",
    "        target_dates = df.iloc[day: day + testing_samples_size, 0]  # Added Now\n",
    "        train = df.iloc[:day, :]\n",
    "        test = df.iloc[day:day + testing_samples_size, :]  # Testing on set one month ahead only, hence day+30.\n",
    "\n",
    "        # training and test sets for all models except LSTM\n",
    "        X_train, y_train = train.iloc[:, 4:-1], train.iloc[:, -1]\n",
    "        X_test, y_test = test.iloc[:, 4:-1], test.iloc[:, -1]\n",
    "\n",
    "        # Seperating validation set from train set\n",
    "        train_df, val_df = get_validation_set(train, batch_size=10)\n",
    "\n",
    "        # Splitting test and validation into dependent and independent sets\n",
    "        X_train_batch, y_train_batch = train_df.iloc[:, 4:-1], train_df.iloc[:, -1]  # Consist only odd batches\n",
    "        X_val_batch, y_val_batch = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\n",
    "\n",
    "        # Normalizing dataset\n",
    "        X_train_lstm_norm, X_test_lstm_norm, X_val_lstm_norm = normalize_dataset(X_train_batch, X_test, X_val_batch)\n",
    "\n",
    "        # Reshaping the dataframes\n",
    "        X_train_lstm, X_val_lstm, X_test_lstm = reshape_dataframe(X_train_lstm_norm, X_val_lstm_norm, X_test_lstm_norm)\n",
    "\n",
    "        rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "        model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train, y_train, X_test)\n",
    "        cur_exec_time.append(exec_time)\n",
    "\n",
    "        gb_reg = GradientBoostingRegressor(random_state=0)\n",
    "        model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\n",
    "        cur_exec_time.append(exec_time)\n",
    "\n",
    "        lsv_reg = LinearSVR(random_state=0, tol=1e-5)\n",
    "        model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\n",
    "        cur_exec_time.append(exec_time)\n",
    "\n",
    "        dt_reg = DecisionTreeRegressor(random_state=0)\n",
    "        model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\n",
    "        cur_exec_time.append(exec_time)\n",
    "\n",
    "        br_reg = BayesianRidge()\n",
    "        model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\n",
    "        cur_exec_time.append(exec_time)\n",
    "\n",
    "        lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\n",
    "        model_predictions['LSTM'], exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train_batch, X_val_lstm, y_val_batch, X_test_lstm, patience, epochs, batch_size_lstm)\n",
    "        cur_exec_time.append(exec_time)\n",
    "\n",
    "        united_dataframe.append(unit_static_df(country, target_dates, y_test, day, model_predictions))  # Added now\n",
    "\n",
    "        mdl_evaluation_df = get_scores(y_test, model_predictions, day)\n",
    "        total_execution_time.append(cur_exec_time)\n",
    "        frames.append(mdl_evaluation_df)\n",
    "\n",
    "    evaluation_score_df = pd.concat(frames, ignore_index=True)\n",
    "    united_dataframe = pd.concat(united_dataframe, ignore_index=True)  # Added Now\n",
    "    running_time_df = get_running_time_per_model_static_learner(model_predictions, total_execution_time)\n",
    "    return evaluation_score_df, running_time_df, united_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCrlBMu3TZMm"
   },
   "outputs": [],
   "source": [
    "results_static = []\n",
    "results_runtime_static = []\n",
    "max_of_pretrain_per_country = []\n",
    "max_cases_per_country = []\n",
    "\n",
    "\n",
    "for country in countries:  #Remove list slicing\n",
    "    # Read country wise csv file\n",
    "    df_country = pd.read_csv(f'{csv_processed_path}/{country}.csv')\n",
    "\n",
    "    print(f\"*******************Processing {country}************************\")\n",
    "    # Evaluation scores and running time of each algorithm over different pre-training days\n",
    "    evaluation_scores_df, running_time_df, united_dataframe = scikit_learn(df_country, pretrain_days, country)  # Returning united_dataframe also\n",
    "\n",
    "    save_united_df(united_dataframe, exp1_static_united_df_path, country=country)\n",
    "\n",
    "    # Append result of each pretrain size in results\n",
    "    results_static.append(evaluation_scores_df)\n",
    "\n",
    "    # Appending every country runtime\n",
    "    # results_runtime_static.append(running_time_df)\n",
    "\n",
    "    # Saving runtime for each country\n",
    "    save_runtime(running_time_df, path=exp1_runtime_path, country = country, static_learner=True)\n",
    "\n",
    "    # Calculating max cases per country based on pre-train size\n",
    "    max_of_pretrain_per_country.append(calc_max_of_pretrain_days(pretrain_days, df_country))\n",
    "\n",
    "    # Maximum case of each country\n",
    "    max_cases_per_country.append(df_country['cases'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "im2-1jnV8T_W"
   },
   "outputs": [],
   "source": [
    "# Save the running time for each country\n",
    "# for i in range(len(countries)):\n",
    "#   save_runtime(results_runtime_static[i], path=exp1_runtime_path, country = countries[i], static_learner=True)\n",
    "\n",
    "# display_runtime_per_country(results_runtime_static, countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NlVmdHK37Bk"
   },
   "outputs": [],
   "source": [
    "countrywise_error_scores_static = calc_save_err_metric_countrywise(countries, error_metrics, results_static, max_of_pretrain_per_country, max_cases_per_country, path=exp1_path, static_learner=True, transpose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vU9yFzgj_IHX"
   },
   "outputs": [],
   "source": [
    "summary_table_countrywise_static = get_summary_table_countrywise(countrywise_error_scores_static, ['MAPE'], static_learner=True)\n",
    "\n",
    "# Saving the transposed matrix\n",
    "save_summary_table(summary_table_countrywise_static, exp1_summary_path, country=True, static_learner=True, alternate_batch=False, transpose=True)\n",
    "\n",
    "summary_table_countrywise_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTMQFGlMfCkT"
   },
   "outputs": [],
   "source": [
    "sum_static_countrywise_mean = get_sum_table_combined_mean(countrywise_error_scores_static, results_runtime_static, static_learner=True)\n",
    "save_combined_summary_table(sum_static_countrywise_mean, exp1_summary_path, static_learner=True, transpose=True) \n",
    "sum_static_countrywise_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIMr3U1QAc63"
   },
   "source": [
    "## Significance tests for Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2atqrPW4Pqk",
    "outputId": "ed5dd50a-c69b-4606-e23d-83eb96de31f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      United_States_of_America  India  ... Vietnam   Taiwan\n",
      "HT_Reg                                  10.534  1.488  ...   2.854   17.403\n",
      "HAT_Reg                                  8.893   1.51  ...   3.151   19.294\n",
      "ARF_Reg                                 15.386  1.063  ...   3.796   16.182\n",
      "PA_Reg                                   76.28  7.672  ...   16.51  101.086\n",
      "EvaluationMeasurement                     MAPE   MAPE  ...    MAPE     MAPE\n",
      "\n",
      "[5 rows x 50 columns]\n",
      "                 Algeria Australia  ... United_States_of_America Vietnam\n",
      "RandomForest       0.584     1.917  ...                    0.282   3.477\n",
      "GradientBoosting   0.433     1.123  ...                    0.233   3.731\n",
      "LinearSVR          2.895   298.687  ...                   21.624  15.684\n",
      "DecisionTree       0.427     0.965  ...                    0.245   3.757\n",
      "BayesianRidge      1.229    15.421  ...                    0.837   2.342\n",
      "\n",
      "[5 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "# @Andres: Please run [4]\n",
    "# Only run this cell if manually uploaded the results\n",
    "summary_table_countrywise_incremental = pd.read_csv(r'/content/Result/exp1/summary/top_countries_MAPE_summary_table_incremental.csv', index_col='Unnamed: 0')\n",
    "summary_table_countrywise_static = pd.read_csv(r'/content/Result/exp1/summary/top_countries_MAPE_summary_table_static.csv', index_col='Unnamed: 0')\n",
    "summary_table_countrywise_static.index.name = None\n",
    "print(summary_table_countrywise_incremental.head())\n",
    "print(summary_table_countrywise_static.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "VlZxtco1AbsI",
    "outputId": "1204d2fe-95fa-42a1-a22d-9fdbca35ba32"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-18f9f481bc0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Concatenating a population of all results (as in boxplot) for experiment 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mconcated_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msummary_table_countrywise_incremental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_table_countrywise_static\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EvaluationMeasurement'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mconcated_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4172\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4174\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4175\u001b[0m         )\n\u001b[1;32m   4176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3887\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3888\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3889\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3940\u001b[0m                 \u001b[0mlabels_missing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3941\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabels_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3942\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3944\u001b[0m             \u001b[0mslicer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['EvaluationMeasurement'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# @Andres: Please run [5]\n",
    "## EXP1\n",
    "# Significance results for Experiment 1\n",
    "err_metric_for_significance = 'MAPE'\n",
    "significance_thresh = 0.01\n",
    "\n",
    "# Concatenating a population of all results (as in boxplot) for experiment 1\n",
    "concated_df = pd.concat([summary_table_countrywise_incremental.transpose(), summary_table_countrywise_static.transpose()]).transpose().drop(columns=['EvaluationMeasurement'], axis=1)\n",
    "concated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "hyz23f1PMrpY",
    "outputId": "8b0a45f6-051e-48cd-80b3-33d6fddd053f"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-92f943b4b471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Selecting the best algorithm for statistical comparisons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# We want to know if the best is statistically significantly better compared to the rest.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbest_algo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcated_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mbest_algo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4103\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# @Andres: Please run [6]\n",
    "# Selecting the best algorithm for statistical comparisons\n",
    "# We want to know if the best is statistically significantly better compared to the rest.\n",
    "best_algo = concated_df.mean().sort_values(ascending=True).index[0]\n",
    "best_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFR6AjtxMv-M"
   },
   "outputs": [],
   "source": [
    "# @Andres: Please run [7]\n",
    "print('AVG results across countries')\n",
    "concated_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLYuRNC8M0yF"
   },
   "outputs": [],
   "source": [
    "# @Andres: Please run [8]\n",
    "print('STDEV across countries')\n",
    "concated_df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpiiYDApAbeX"
   },
   "outputs": [],
   "source": [
    "# @Andres: Please run [9]\n",
    "# Iterate through all the other algorithms to see if the difference in results is significant\n",
    "competitors = list(concated_df.columns)\n",
    "competitors.remove(best_algo)\n",
    "\n",
    "for significance_thresh in [0.01,0.05]:\n",
    "  print(f'Running significane at: {significance_thresh}')\n",
    "  for competitor in competitors:\n",
    "    # print(competitor)\n",
    "    pval, significant = check_significance(concated_df[best_algo], concated_df[competitor], significance_at=significance_thresh)\n",
    "    print(f'Comparison of {best_algo} to {competitor} pvalue: {pval}   /  significant?: {significant}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0kt9cV_XWS5"
   },
   "outputs": [],
   "source": [
    "# @Andres: Please run [10]\n",
    "# Iterate through all the other algorithms to see if the difference in results is significant\n",
    "best_algo2 = concated_df.mean().sort_values(ascending=True).index[1]\n",
    "competitors = list(concated_df.columns)\n",
    "competitors.remove(best_algo2)\n",
    "for significance_thresh in [0.01,0.05]:\n",
    "  print(f'Running significane at: {significance_thresh}')\n",
    "  for competitor in competitors:\n",
    "    # print(competitor)\n",
    "    pval, significant = check_significance(concated_df[best_algo2], concated_df[competitor], significance_at=significance_thresh)\n",
    "    print(f'Comparison of {best_algo2} to {competitor} pvalue: {pval}   /  significant?: {significant}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3abKxWJwqNU6"
   },
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM8GB6LTRPxY"
   },
   "source": [
    "### Creating Combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7vb_yHqvwPX"
   },
   "outputs": [],
   "source": [
    "# Reading file \n",
    "url = 'https://drive.google.com/file/d/1eYy56fHe1XsWgPkBGVc0i6d6A5EU3nxj/view?usp=sharing'\n",
    "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
    "df = pd.read_csv(path)\n",
    "num_selected_countries = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxDuX7-ansDh"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Pre-processing dataset\n",
    "df = preprocess_dataset(df)\n",
    "\n",
    "# Grouping records by country\n",
    "df_grouped = df.groupby('country')\n",
    "\n",
    "# Taking only those countries which have sufficient data records\n",
    "#valid_countries = get_countries_with_valid_size(df_grouped)\n",
    "\n",
    "# Sorting countries by number of cases\n",
    "df_countries_sortedbycases = get_countries_sortedby_cases(valid_countries, df_grouped)\n",
    "\n",
    "# Taking only top selected countries\n",
    "top_selected_countries = df_countries_sortedbycases.iloc[0:num_selected_countries].index\n",
    "\n",
    "# Calculating targets and lags for the above countries\n",
    "result = get_dataset_with_target(top_selected_countries,df_grouped)\n",
    "\n",
    "# Getting max of each subset in pretrain size\n",
    "max_of_pretrain_days = calc_max_of_pretrain_days(pretrain_days,result)\n",
    "\n",
    "# Mean of top selected countries\n",
    "max_selected_countries = result['cases'].max()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtxpRv3FvxdD"
   },
   "source": [
    "### Incremental Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "JcRN9CoVFigI",
    "outputId": "50a73637-dad0-437e-ce5a-75eb3b577589"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\ndef scikit_multiflow(df, pretrain_days):\\n\\n  model, model_names = instantiate_regressors()\\n\\n  len_countries = len(df['country'].unique())\\n\\n  frames , running_time_frames = [], []\\n\\n  # Setup the evaluator\\n  for day in pretrain_days:\\n\\n      df_subset = create_subset(df,day)\\n      \\n      # Creating a stream from dataframe\\n      stream = DataStream(np.array(df_subset.iloc[:,4:-1]), y=np.array(df_subset.iloc[:,-1])) #TODO: Drop columns with name \\n\\n      pretrain_size = day * len_countries\\n      max_samples = (day + 30) * len_countries #Testing on set one month ahead only\\n\\n      evaluator = EvaluatePrequential(show_plot=False,\\n                                    pretrain_size=pretrain_size,\\n                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\\n                                    max_samples=max_samples)\\n      # Run evaluation\\n      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\\n\\n      # Dictionary to store each iteration error scores\\n      mdl_evaluation_scores = {}\\n\\n      # Adding Evaluation Measurements and pretraining days\\n      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\\n      mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\\n\\n      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\\n\\n      # Errors of each model on a specific pre-train days\\n      frames.append(mdl_evaluation_df)\\n\\n      # Run time for each algorithm\\n      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\\n\\n   # Final Run Time DataFrame\\n  running_time_df = pd.concat(running_time_frames,ignore_index=True)\\n\\n  # Final Evaluation Score Dataframe\\n  evaluation_scores_df = pd.concat(frames, ignore_index=True)\\n  return evaluation_scores_df, running_time_df\\n\""
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old Script\n",
    "\"\"\"\n",
    "def scikit_multiflow(df, pretrain_days):\n",
    "\n",
    "  model, model_names = instantiate_regressors()\n",
    "\n",
    "  len_countries = len(df['country'].unique())\n",
    "\n",
    "  frames , running_time_frames = [], []\n",
    "\n",
    "  # Setup the evaluator\n",
    "  for day in pretrain_days:\n",
    "\n",
    "      df_subset = create_subset(df,day)\n",
    "      \n",
    "      # Creating a stream from dataframe\n",
    "      stream = DataStream(np.array(df_subset.iloc[:,4:-1]), y=np.array(df_subset.iloc[:,-1])) #TODO: Drop columns with name \n",
    "\n",
    "      pretrain_size = day * len_countries\n",
    "      max_samples = (day + 30) * len_countries #Testing on set one month ahead only\n",
    "\n",
    "      evaluator = EvaluatePrequential(show_plot=False,\n",
    "                                    pretrain_size=pretrain_size,\n",
    "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\n",
    "                                    max_samples=max_samples)\n",
    "      # Run evaluation\n",
    "      evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
    "\n",
    "      # Dictionary to store each iteration error scores\n",
    "      mdl_evaluation_scores = {}\n",
    "\n",
    "      # Adding Evaluation Measurements and pretraining days\n",
    "      mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\n",
    "      mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
    "\n",
    "      mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\n",
    "\n",
    "      # Errors of each model on a specific pre-train days\n",
    "      frames.append(mdl_evaluation_df)\n",
    "\n",
    "      # Run time for each algorithm\n",
    "      running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
    "\n",
    "   # Final Run Time DataFrame\n",
    "  running_time_df = pd.concat(running_time_frames,ignore_index=True)\n",
    "\n",
    "  # Final Evaluation Score Dataframe\n",
    "  evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
    "  return evaluation_scores_df, running_time_df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6RpyYrC-HSB"
   },
   "outputs": [],
   "source": [
    "def scikit_multiflow(df, pretrain_days):  # Updated Now\n",
    "    model, model_names = instantiate_regressors()\n",
    "\n",
    "    len_countries = len(df['country'].unique())\n",
    "\n",
    "    # Selecting only required countries\n",
    "    df = df[df['country'].isin(df['country'].unique()[0:len_countries])]  # Added Now\n",
    "\n",
    "    frames, running_time_frames = [], []\n",
    "\n",
    "    united_dataframe = []  # Added Now\n",
    "\n",
    "    # Setup the evaluator\n",
    "    for day in pretrain_days:\n",
    "        df_subset = create_subset(df, day)\n",
    "\n",
    "        # Creating a stream from dataframe\n",
    "        stream = DataStream(np.array(df_subset.iloc[:, 4:-1]), y=np.array(df_subset.iloc[:, -1]))  # Selecting features x=[t-89:t-39] and y=[target].\n",
    "\n",
    "        pretrain_size = day * len_countries\n",
    "        max_samples = pretrain_size + 1  # One Extra Sample\n",
    "        testing_samples_size = (day + 30) * len_countries\n",
    "\n",
    "        evaluator = EvaluatePrequential(show_plot=False,\n",
    "                                        pretrain_size=pretrain_size,\n",
    "                                        metrics=['mean_square_error', 'mean_absolute_error',\n",
    "                                                 'mean_absolute_percentage_error'],\n",
    "                                        max_samples=max_samples)\n",
    "        # Run evaluation\n",
    "        evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
    "\n",
    "        # Added Now\n",
    "        X = stream.X[pretrain_size: testing_samples_size]  # Updated Now\n",
    "        y = stream.y[pretrain_size: testing_samples_size]  # Updated Now\n",
    "        date_idx = list(df_subset.columns).index('date')  # Added Now\n",
    "        target_dates = df_subset.iloc[pretrain_size: testing_samples_size, date_idx]  # Added Now\n",
    "\n",
    "        prediction = evaluator.predict(X)\n",
    "\n",
    "        # Since we add one extra sample, reset the evaluator\n",
    "        evaluator = reset_evaluator(evaluator)\n",
    "        evaluator = update_incremental_metrics(evaluator, y, prediction)\n",
    "\n",
    "        country_idx = list(df_subset.columns).index('country')  # Added Now\n",
    "        subset_countries_names = df_subset.iloc[pretrain_size:testing_samples_size, country_idx]  # Added Now\n",
    "        united_dataframe.append(unit_incremental_df(subset_countries_names, evaluator, target_dates, day))  # Added now\n",
    "\n",
    "        # Dictionary to store each iteration error scores\n",
    "        mdl_evaluation_scores = {}\n",
    "\n",
    "        # Adding Evaluation Measurements and pretraining days\n",
    "        mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE', 'MAE', 'MAPE']  # ,'MSE']\n",
    "        mdl_evaluation_scores['PretrainDays'] = [day] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
    "        mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores)\n",
    "\n",
    "        # Errors of each model on a specific pre-train days\n",
    "        frames.append(mdl_evaluation_df)\n",
    "\n",
    "        # Run time for each algorithm\n",
    "        running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
    "\n",
    "    # Final Run Time DataFrame\n",
    "    running_time_df = pd.concat(running_time_frames, ignore_index=True)\n",
    "\n",
    "    united_df = pd.concat(united_dataframe, ignore_index=True)\n",
    "\n",
    "    # Final Evaluation Score Dataframe\n",
    "    evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    return evaluation_scores_df, running_time_df, united_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32jMOvemFnHG",
    "outputId": "0e19531d-5631-4a4d-e73a-d249634f6733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 1500 sample(s).\n",
      "Evaluating...\n",
      " #################### [100%] [37.77s]\n",
      "Processed samples: 1501\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 792400596.9906\n",
      "HT_Reg - MAPE          : 0.9833\n",
      "HT_Reg - MAE          : 28149.610956\n",
      "HAT_Reg - MSE          : 792400596.9906\n",
      "HAT_Reg - MAPE          : 0.9833\n",
      "HAT_Reg - MAE          : 28149.610956\n",
      "ARF_Reg - MSE          : 845025604.6210\n",
      "ARF_Reg - MAPE          : 1.0154\n",
      "ARF_Reg - MAE          : 29069.324117\n",
      "PA_Reg - MSE          : 151592944.2449\n",
      "PA_Reg - MAPE          : 0.4301\n",
      "PA_Reg - MAE          : 12312.308648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 3000 sample(s).\n",
      "Evaluating...\n",
      " #################### [100%] [103.24s]\n",
      "Processed samples: 3001\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 2672707896.8412\n",
      "HT_Reg - MAPE          : 2.4251\n",
      "HT_Reg - MAE          : 51698.238818\n",
      "HAT_Reg - MSE          : 2672707896.8412\n",
      "HAT_Reg - MAPE          : 2.4251\n",
      "HAT_Reg - MAE          : 51698.238818\n",
      "ARF_Reg - MSE          : 1504529857.9753\n",
      "ARF_Reg - MAPE          : 1.8195\n",
      "ARF_Reg - MAE          : 38788.269592\n",
      "PA_Reg - MSE          : 357551845658.0593\n",
      "PA_Reg - MAPE          : 28.0499\n",
      "PA_Reg - MAE          : 597956.391101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 4500 sample(s).\n",
      "Evaluating...\n",
      " #################### [100%] [136.14s]\n",
      "Processed samples: 4501\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 3470722143.2694\n",
      "HT_Reg - MAPE          : 1.7847\n",
      "HT_Reg - MAE          : 58912.835132\n",
      "HAT_Reg - MSE          : 936996423.1719\n",
      "HAT_Reg - MAPE          : 0.9273\n",
      "HAT_Reg - MAE          : 30610.397305\n",
      "ARF_Reg - MSE          : 1669711043.8664\n",
      "ARF_Reg - MAPE          : 1.2378\n",
      "ARF_Reg - MAE          : 40862.097889\n",
      "PA_Reg - MSE          : 8641375491.8798\n",
      "PA_Reg - MAPE          : 2.8160\n",
      "PA_Reg - MAE          : 92958.998983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 6000 sample(s).\n",
      "Evaluating...\n",
      " #################### [100%] [176.81s]\n",
      "Processed samples: 6001\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 2783777922.3290\n",
      "HT_Reg - MAPE          : 0.8021\n",
      "HT_Reg - MAE          : 52761.519333\n",
      "HAT_Reg - MSE          : 67041847.4327\n",
      "HAT_Reg - MAPE          : 0.1245\n",
      "HAT_Reg - MAE          : 8187.908612\n",
      "ARF_Reg - MSE          : 1408870350.6166\n",
      "ARF_Reg - MAPE          : 0.5706\n",
      "ARF_Reg - MAE          : 37534.921748\n",
      "PA_Reg - MSE          : 353413080657.7103\n",
      "PA_Reg - MAPE          : 9.0375\n",
      "PA_Reg - MAE          : 594485.559671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 7500 sample(s).\n",
      "Evaluating...\n",
      " #################### [100%] [198.55s]\n",
      "Processed samples: 7501\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 1876645157.8326\n",
      "HT_Reg - MAPE          : 1.0367\n",
      "HT_Reg - MAE          : 43320.262670\n",
      "HAT_Reg - MSE          : 1814856990.2327\n",
      "HAT_Reg - MAPE          : 1.0195\n",
      "HAT_Reg - MAE          : 42601.138368\n",
      "ARF_Reg - MSE          : 1040745046.7178\n",
      "ARF_Reg - MAPE          : 0.7720\n",
      "ARF_Reg - MAE          : 32260.580384\n",
      "PA_Reg - MSE          : 6230281265236.9238\n",
      "PA_Reg - MAPE          : 59.7338\n",
      "PA_Reg - MAE          : 2496053.137503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 9000 sample(s).\n",
      "Evaluating...\n",
      " #################### [100%] [234.33s]\n",
      "Processed samples: 9001\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 701934137.7044\n",
      "HT_Reg - MAPE          : 0.6255\n",
      "HT_Reg - MAE          : 26494.039664\n",
      "HAT_Reg - MSE          : 2971098205.5067\n",
      "HAT_Reg - MAPE          : 1.2868\n",
      "HAT_Reg - MAE          : 54507.781146\n",
      "ARF_Reg - MSE          : 3428729475.3887\n",
      "ARF_Reg - MAPE          : 1.3823\n",
      "ARF_Reg - MAE          : 58555.353943\n",
      "PA_Reg - MSE          : 12388754646649.5938\n",
      "PA_Reg - MAPE          : 83.0923\n",
      "PA_Reg - MAE          : 3519766.277276\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 10500 sample(s).\n",
      "Evaluating...\n",
      " #################### [100%] [271.93s]\n",
      "Processed samples: 10501\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 375305965.1585\n",
      "HT_Reg - MAPE          : 0.2932\n",
      "HT_Reg - MAE          : 19372.815107\n",
      "HAT_Reg - MSE          : 6371909610.3034\n",
      "HAT_Reg - MAPE          : 1.2083\n",
      "HAT_Reg - MAE          : 79824.241996\n",
      "ARF_Reg - MSE          : 4057459905.5702\n",
      "ARF_Reg - MAPE          : 0.9642\n",
      "ARF_Reg - MAE          : 63698.193896\n",
      "PA_Reg - MSE          : 7130763469.2829\n",
      "PA_Reg - MAPE          : 1.2782\n",
      "PA_Reg - MAE          : 84443.848025\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 12000 sample(s).\n",
      "Evaluating...\n",
      " #################### [100%] [376.97s]\n",
      "Processed samples: 12001\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 10821835860.1702\n",
      "HT_Reg - MAPE          : 0.6188\n",
      "HT_Reg - MAE          : 104028.053236\n",
      "HAT_Reg - MSE          : 11155871595.1223\n",
      "HAT_Reg - MAPE          : 0.6282\n",
      "HAT_Reg - MAE          : 105621.359559\n",
      "ARF_Reg - MSE          : 24292069118.7354\n",
      "ARF_Reg - MAPE          : 0.9270\n",
      "ARF_Reg - MAE          : 155859.132292\n",
      "PA_Reg - MSE          : 7083773425.3373\n",
      "PA_Reg - MAPE          : 0.5006\n",
      "PA_Reg - MAE          : 84165.155649\n"
     ]
    }
   ],
   "source": [
    "result_skmlflow, running_time_combined_incremental, united_df = scikit_multiflow(result, pretrain_days)  # Updated Now\n",
    "save_united_df(united_df, exp2_inc_united_df_path)  # Added Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xOniEJ11niO"
   },
   "outputs": [],
   "source": [
    "df_skmlflow = calc_save_err_metric_combined(error_metrics, result_skmlflow, max_of_pretrain_days, max_selected_countries, path=exp2_path, static_learner=False, alternate_batch=False, transpose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "Cq1ffodM5C4f",
    "outputId": "fafe588c-c57f-401c-e403-78ae08aeba17"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PretrainDays</th>\n",
       "      <th>HT_Reg</th>\n",
       "      <th>HAT_Reg</th>\n",
       "      <th>ARF_Reg</th>\n",
       "      <th>PA_Reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.705</td>\n",
       "      <td>1.064</td>\n",
       "      <td>36.035</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>2.139</td>\n",
       "      <td>4.838</td>\n",
       "      <td>96.296</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>4.634</td>\n",
       "      <td>15.004</td>\n",
       "      <td>116.625</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>6.224</td>\n",
       "      <td>17.836</td>\n",
       "      <td>152.780</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150</td>\n",
       "      <td>7.666</td>\n",
       "      <td>18.284</td>\n",
       "      <td>172.613</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180</td>\n",
       "      <td>9.976</td>\n",
       "      <td>24.202</td>\n",
       "      <td>200.179</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>210</td>\n",
       "      <td>12.046</td>\n",
       "      <td>35.800</td>\n",
       "      <td>224.099</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>240</td>\n",
       "      <td>14.732</td>\n",
       "      <td>52.842</td>\n",
       "      <td>309.417</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
       "0            30   0.705    1.064   36.035   0.003\n",
       "1            60   2.139    4.838   96.296   0.003\n",
       "2            90   4.634   15.004  116.625   0.004\n",
       "3           120   6.224   17.836  152.780   0.004\n",
       "4           150   7.666   18.284  172.613   0.005\n",
       "5           180   9.976   24.202  200.179   0.005\n",
       "6           210  12.046   35.800  224.099   0.007\n",
       "7           240  14.732   52.842  309.417   0.007"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_runtime(running_time_combined_incremental, path=exp2_runtime_path, static_learner=False)\n",
    "running_time_combined_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "yrGTNx3kTNDU",
    "outputId": "7e398c04-a347-469f-8953-6403a1115cdc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HT_Reg</th>\n",
       "      <th>HAT_Reg</th>\n",
       "      <th>ARF_Reg</th>\n",
       "      <th>PA_Reg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>5495.883</td>\n",
       "      <td>4802.677</td>\n",
       "      <td>4599.165</td>\n",
       "      <td>40040.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE</th>\n",
       "      <td>30.874</td>\n",
       "      <td>36.171</td>\n",
       "      <td>41.651</td>\n",
       "      <td>136.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE</th>\n",
       "      <td>17380.819</td>\n",
       "      <td>13945.722</td>\n",
       "      <td>14150.650</td>\n",
       "      <td>130604.297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time(sec)</th>\n",
       "      <td>7.265</td>\n",
       "      <td>21.234</td>\n",
       "      <td>163.506</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             HT_Reg   HAT_Reg   ARF_Reg     PA_Reg\n",
       "Metric                                            \n",
       "MAE        5495.883  4802.677  4599.165  40040.947\n",
       "MAPE         30.874    36.171    41.651    136.261\n",
       "RMSE      17380.819 13945.722 14150.650 130604.297\n",
       "Time(sec)     7.265    21.234   163.506      0.005"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_table_incremental = get_summary_table(df_skmlflow, running_time_combined_incremental, error_metrics, static_learner=False)\n",
    "save_summary_table(summary_table_incremental, exp2_summary_path,static_learner=False,alternate_batch=False, transpose=True)\n",
    "summary_table_incremental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m4-HvnEwA-l"
   },
   "source": [
    "### Static Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "uYtaCPtjjpv5",
    "outputId": "01b2bb86-31f0-4e05-9e2d-1028cea3e93f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\ndef scikit_learn(df, training_days):\\n  len_countries = len(df['country'].unique())\\n  frames = []\\n  model_predictions = {\\n      'RandomForest':[],\\n      'GradientBoosting':[],\\n      'LinearSVR':[],\\n      'DecisionTree':[],\\n      'BayesianRidge':[],\\n      'LSTM': []\\n      #'MLPRegressor': [],\\n      #'LinearRegression': []\\n    }\\n\\n  # LSTM (TODO: feel free to put the whole LSTM definition in a funtion)\\n  # params (others like epoch and batch size are also hardcoded in train_test_lstm())\\n  layers = [20, 10, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\\n  activations = ['tanh', 'tanh', 'relu']\\n  patience = 20\\n  total_execution_time = []\\n\\n  for day in training_days:\\n\\n    df_subset = create_subset(df,day)\\n    \\n    train_end_day = day * len_countries\\n    test_end_day = (day+30) * len_countries\\n\\n    train = df_subset.iloc[:train_end_day, :] \\n    test = df_subset.iloc[train_end_day:test_end_day, :]  # Testing on set one month ahead only, hence day+30.\\n    test_df, val_df = get_validation_set(test, batch_size=10)  # Getting validation set from test set\\n\\n    # training and test sets for all models except LSTM\\n    X_train, y_train = train.iloc[:,4:-1], train.iloc[:,-1]\\n    X_test, y_test = test.iloc[:,4:-1], test.iloc[:,-1]\\n\\n    # Validation and test set for LSTM\\n    X_test_lstm, y_test_lstm = test_df.iloc[:, 4:-1], test_df.iloc[:, -1]\\n    X_val, y_val = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\\n\\n    X_train_lstm, X_val, X_test_lstm = reshape_dataframe(X_train, X_val, X_test_lstm)  # reshaping the vector for lstm\\n    cur_exec_time = [day]\\n\\n    rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\\n    model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train,y_train,X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    gb_reg = GradientBoostingRegressor(random_state=0)\\n    model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    lsv_reg = LinearSVR(random_state=0, tol=1e-5)\\n    model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    dt_reg = DecisionTreeRegressor(random_state=0)\\n    model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    br_reg = BayesianRidge()\\n    model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    '''\\n    mlp_reg = MLPRegressor(random_state=1, max_iter=500)\\n    model_predictions['MLPRegressor'], exec_time = train_test_model(mlp_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n\\n\\n    lin_reg = LinearRegression()\\n    model_predictions['LinearRegression'], exec_time = train_test_model(lin_reg, X_train, y_train, X_test)\\n    cur_exec_time.append(exec_time)\\n    '''\\n\\n    lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\\n    model_predictions['LSTM'], exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train, X_val, y_val, X_test_lstm, patience)         \\n    cur_exec_time.append(exec_time)\\n\\n    mdl_evaluation_df = get_scores(y_test,y_test_lstm, model_predictions, day)\\n    total_execution_time.append(cur_exec_time)\\n    frames.append(mdl_evaluation_df)\\n\\n  evaluation_score_df = pd.concat(frames, ignore_index=True)\\n  running_time_df = get_running_time_per_model_static_learner(model_predictions,total_execution_time)\\n  return evaluation_score_df, running_time_df\\n  \""
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def scikit_learn(df, training_days):\n",
    "  len_countries = len(df['country'].unique())\n",
    "  frames = []\n",
    "  model_predictions = {\n",
    "      'RandomForest':[],\n",
    "      'GradientBoosting':[],\n",
    "      'LinearSVR':[],\n",
    "      'DecisionTree':[],\n",
    "      'BayesianRidge':[],\n",
    "      'LSTM': []\n",
    "      #'MLPRegressor': [],\n",
    "      #'LinearRegression': []\n",
    "    }\n",
    "\n",
    "  # LSTM (TODO: feel free to put the whole LSTM definition in a funtion)\n",
    "  # params (others like epoch and batch size are also hardcoded in train_test_lstm())\n",
    "  layers = [20, 10, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\n",
    "  activations = ['tanh', 'tanh', 'relu']\n",
    "  patience = 20\n",
    "  total_execution_time = []\n",
    "\n",
    "  for day in training_days:\n",
    "\n",
    "    df_subset = create_subset(df,day)\n",
    "    \n",
    "    train_end_day = day * len_countries\n",
    "    test_end_day = (day+30) * len_countries\n",
    "\n",
    "    train = df_subset.iloc[:train_end_day, :] \n",
    "    test = df_subset.iloc[train_end_day:test_end_day, :]  # Testing on set one month ahead only, hence day+30.\n",
    "    test_df, val_df = get_validation_set(test, batch_size=10)  # Getting validation set from test set\n",
    "\n",
    "    # training and test sets for all models except LSTM\n",
    "    X_train, y_train = train.iloc[:,4:-1], train.iloc[:,-1]\n",
    "    X_test, y_test = test.iloc[:,4:-1], test.iloc[:,-1]\n",
    "\n",
    "    # Validation and test set for LSTM\n",
    "    X_test_lstm, y_test_lstm = test_df.iloc[:, 4:-1], test_df.iloc[:, -1]\n",
    "    X_val, y_val = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\n",
    "\n",
    "    X_train_lstm, X_val, X_test_lstm = reshape_dataframe(X_train, X_val, X_test_lstm)  # reshaping the vector for lstm\n",
    "    cur_exec_time = [day]\n",
    "\n",
    "    rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "    model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train,y_train,X_test)\n",
    "    cur_exec_time.append(exec_time)\n",
    "\n",
    "\n",
    "    gb_reg = GradientBoostingRegressor(random_state=0)\n",
    "    model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\n",
    "    cur_exec_time.append(exec_time)\n",
    "\n",
    "\n",
    "    lsv_reg = LinearSVR(random_state=0, tol=1e-5)\n",
    "    model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\n",
    "    cur_exec_time.append(exec_time)\n",
    "\n",
    "\n",
    "    dt_reg = DecisionTreeRegressor(random_state=0)\n",
    "    model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\n",
    "    cur_exec_time.append(exec_time)\n",
    "\n",
    "\n",
    "    br_reg = BayesianRidge()\n",
    "    model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\n",
    "    cur_exec_time.append(exec_time)\n",
    "\n",
    "\n",
    "    '''\n",
    "    mlp_reg = MLPRegressor(random_state=1, max_iter=500)\n",
    "    model_predictions['MLPRegressor'], exec_time = train_test_model(mlp_reg, X_train, y_train, X_test)\n",
    "    cur_exec_time.append(exec_time)\n",
    "\n",
    "\n",
    "    lin_reg = LinearRegression()\n",
    "    model_predictions['LinearRegression'], exec_time = train_test_model(lin_reg, X_train, y_train, X_test)\n",
    "    cur_exec_time.append(exec_time)\n",
    "    '''\n",
    "\n",
    "    lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\n",
    "    model_predictions['LSTM'], exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train, X_val, y_val, X_test_lstm, patience)         \n",
    "    cur_exec_time.append(exec_time)\n",
    "\n",
    "    mdl_evaluation_df = get_scores(y_test,y_test_lstm, model_predictions, day)\n",
    "    total_execution_time.append(cur_exec_time)\n",
    "    frames.append(mdl_evaluation_df)\n",
    "\n",
    "  evaluation_score_df = pd.concat(frames, ignore_index=True)\n",
    "  running_time_df = get_running_time_per_model_static_learner(model_predictions,total_execution_time)\n",
    "  return evaluation_score_df, running_time_df\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2oijsU0UJkD"
   },
   "outputs": [],
   "source": [
    "def scikit_learn(df, training_days):\n",
    "\n",
    "    len_countries = len(df['country'].unique())\n",
    "\n",
    "    # Selecting only required countries\n",
    "    df = df[df['country'].isin(df['country'].unique()[0:len_countries])]  # Added Now\n",
    "\n",
    "    frames = []\n",
    "    model_predictions = {\n",
    "        'RandomForest': [],\n",
    "        'GradientBoosting': [],\n",
    "        'LinearSVR': [],\n",
    "        'DecisionTree': [],\n",
    "        'BayesianRidge': [],\n",
    "        'LSTM': []\n",
    "    }\n",
    "    total_execution_time = []\n",
    "\n",
    "    layers = [50, 30, 20, 10]  # the final net will have n_layers +  2 + 1 = n*LSTMs + Dense + LSTM + output\n",
    "    activations = ['tanh', 'tanh', 'relu']\n",
    "    epochs = 500 \n",
    "    patience = 20 * num_selected_countries\n",
    "    batch_size_lstm = 10 * num_selected_countries\n",
    "\n",
    "    united_dataframe = []  # Added Now\n",
    "\n",
    "    for day in training_days:\n",
    "        df_subset = create_subset(df, day)\n",
    "\n",
    "        train_end_day = day * len_countries\n",
    "        test_end_day = (day + 30) * len_countries\n",
    "\n",
    "        date_idx = list(df_subset.columns).index('date')  # Added Now\n",
    "        target_dates = df_subset.iloc[train_end_day: test_end_day, date_idx]  # Added Now\n",
    "\n",
    "        train = df_subset.iloc[:train_end_day, :]\n",
    "        test = df_subset.iloc[train_end_day:test_end_day, :]  # Testing on set one month ahead only, hence day+30.\n",
    "        cur_exec_time = [day]\n",
    "\n",
    "        # training and test sets for all models except LSTM\n",
    "        X_train, y_train = train.iloc[:, 4:-1], train.iloc[:, -1]\n",
    "        X_test, y_test = test.iloc[:, 4:-1], test.iloc[:, -1]\n",
    "\n",
    "        # Seperating validation set from train set\n",
    "        train_df, val_df = get_validation_set(train, batch_size=10)\n",
    "\n",
    "        # Splitting test and validation into dependent and independent sets\n",
    "        X_train_batch, y_train_batch = train_df.iloc[:, 4:-1], train_df.iloc[:, -1]  # Consist only odd batches\n",
    "        X_val_batch, y_val_batch = val_df.iloc[:, 4:-1], val_df.iloc[:, -1]\n",
    "\n",
    "        # Normalizing dataset\n",
    "        X_train_lstm_norm, X_test_lstm_norm, X_val_lstm_norm = normalize_dataset(X_train_batch, X_test, X_val_batch)\n",
    "\n",
    "        # Reshaping the dataframes\n",
    "        X_train_lstm, X_val_lstm, X_test_lstm = reshape_dataframe(X_train_lstm_norm, X_val_lstm_norm, X_test_lstm_norm)\n",
    "\n",
    "        rf_reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "        model_predictions['RandomForest'], exec_time = train_test_model(rf_reg, X_train, y_train, X_test)\n",
    "        cur_exec_time.append(exec_time)\n",
    "\n",
    "        gb_reg = GradientBoostingRegressor(random_state=0)\n",
    "        model_predictions['GradientBoosting'], exec_time = train_test_model(gb_reg, X_train, y_train, X_test)\n",
    "        cur_exec_time.append(exec_time)\n",
    "\n",
    "        lsv_reg = LinearSVR(random_state=0, tol=1e-5)\n",
    "        model_predictions['LinearSVR'], exec_time = train_test_model(lsv_reg, X_train, y_train, X_test)\n",
    "        cur_exec_time.append(exec_time)\n",
    "\n",
    "        dt_reg = DecisionTreeRegressor(random_state=0)\n",
    "        model_predictions['DecisionTree'], exec_time = train_test_model(dt_reg, X_train, y_train, X_test)\n",
    "        cur_exec_time.append(exec_time)\n",
    "\n",
    "        br_reg = BayesianRidge()\n",
    "        model_predictions['BayesianRidge'], exec_time = train_test_model(br_reg, X_train, y_train, X_test)\n",
    "        cur_exec_time.append(exec_time)\n",
    "\n",
    "        lstm_model = define_lstm_model(X_train_lstm, layers, activations, patience)\n",
    "        model_predictions['LSTM'], exec_time = train_test_lstm(lstm_model, X_train_lstm, y_train_batch, X_val_lstm, y_val_batch, X_test_lstm, patience, epochs,batch_size_lstm)\n",
    "        cur_exec_time.append(exec_time)\n",
    "\n",
    "        country_idx = list(df_subset.columns).index('country')  # Added Now\n",
    "        subset_countries_names = df_subset.iloc[train_end_day: test_end_day, country_idx]  # Added Now\n",
    "        united_dataframe.append(unit_static_df(subset_countries_names, target_dates, y_test, day, model_predictions))  # Added now\n",
    "\n",
    "        mdl_evaluation_df = get_scores(y_test, model_predictions, day)\n",
    "        total_execution_time.append(cur_exec_time)\n",
    "        frames.append(mdl_evaluation_df)\n",
    "\n",
    "    evaluation_score_df = pd.concat(frames, ignore_index=True)\n",
    "    united_df = pd.concat(united_dataframe, ignore_index=True)  # Added Now\n",
    "    running_time_df = get_running_time_per_model_static_learner(model_predictions, total_execution_time)\n",
    "    return evaluation_score_df, running_time_df, united_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "inEIGOoYn0ng",
    "outputId": "67865033-3113-4205-a1e3-f5b8f6014a5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 500 samples\n",
      "Epoch 1/500\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1253.2611 - mse: 15535691.0000 - mae: 1253.2611 - val_loss: 1334.5077 - val_mse: 18961846.0000 - val_mae: 1334.5077\n",
      "Epoch 2/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1253.2376 - mse: 15535634.0000 - mae: 1253.2377 - val_loss: 1334.4880 - val_mse: 18961794.0000 - val_mae: 1334.4880\n",
      "Epoch 3/500\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 1253.2184 - mse: 15535584.0000 - mae: 1253.2185 - val_loss: 1334.4664 - val_mse: 18961740.0000 - val_mae: 1334.4664\n",
      "Epoch 4/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1253.1973 - mse: 15535532.0000 - mae: 1253.1973 - val_loss: 1334.4293 - val_mse: 18961662.0000 - val_mae: 1334.4293\n",
      "Epoch 5/500\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 1253.1634 - mse: 15535450.0000 - mae: 1253.1633 - val_loss: 1334.3365 - val_mse: 18961488.0000 - val_mae: 1334.3365\n",
      "Epoch 6/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1253.0817 - mse: 15535269.0000 - mae: 1253.0817 - val_loss: 1334.0594 - val_mse: 18960980.0000 - val_mae: 1334.0594\n",
      "Epoch 7/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1252.8364 - mse: 15534753.0000 - mae: 1252.8364 - val_loss: 1333.3025 - val_mse: 18959572.0000 - val_mae: 1333.3025\n",
      "Epoch 8/500\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 1252.1257 - mse: 15533079.0000 - mae: 1252.1257 - val_loss: 1331.7595 - val_mse: 18956542.0000 - val_mae: 1331.7595\n",
      "Epoch 9/500\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 1250.5485 - mse: 15529441.0000 - mae: 1250.5485 - val_loss: 1329.5519 - val_mse: 18951676.0000 - val_mae: 1329.5519\n",
      "Epoch 10/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1248.0435 - mse: 15523101.0000 - mae: 1248.0435 - val_loss: 1326.9265 - val_mse: 18945384.0000 - val_mae: 1326.9265\n",
      "Epoch 11/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1245.1929 - mse: 15515754.0000 - mae: 1245.1929 - val_loss: 1324.2145 - val_mse: 18938458.0000 - val_mae: 1324.2145\n",
      "Epoch 12/500\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 1241.9026 - mse: 15505167.0000 - mae: 1241.9026 - val_loss: 1321.5706 - val_mse: 18930898.0000 - val_mae: 1321.5706\n",
      "Epoch 13/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1238.8621 - mse: 15499680.0000 - mae: 1238.8621 - val_loss: 1318.9325 - val_mse: 18922936.0000 - val_mae: 1318.9325\n",
      "Epoch 14/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1235.5397 - mse: 15486673.0000 - mae: 1235.5398 - val_loss: 1316.3823 - val_mse: 18914690.0000 - val_mae: 1316.3823\n",
      "Epoch 15/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1232.4169 - mse: 15474608.0000 - mae: 1232.4169 - val_loss: 1313.9207 - val_mse: 18906160.0000 - val_mae: 1313.9207\n",
      "Epoch 16/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1229.2600 - mse: 15467727.0000 - mae: 1229.2600 - val_loss: 1311.5861 - val_mse: 18897158.0000 - val_mae: 1311.5861\n",
      "Epoch 17/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1226.6024 - mse: 15455236.0000 - mae: 1226.6024 - val_loss: 1309.3734 - val_mse: 18888032.0000 - val_mae: 1309.3734\n",
      "Epoch 18/500\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 1222.1423 - mse: 15443730.0000 - mae: 1222.1423 - val_loss: 1307.0510 - val_mse: 18878286.0000 - val_mae: 1307.0510\n",
      "Epoch 19/500\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 1219.4244 - mse: 15426594.0000 - mae: 1219.4246 - val_loss: 1304.7306 - val_mse: 18868374.0000 - val_mae: 1304.7306\n",
      "Epoch 20/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1217.0153 - mse: 15422074.0000 - mae: 1217.0153 - val_loss: 1302.3917 - val_mse: 18858372.0000 - val_mae: 1302.3917\n",
      "Epoch 21/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1212.5217 - mse: 15406304.0000 - mae: 1212.5217 - val_loss: 1299.9945 - val_mse: 18847848.0000 - val_mae: 1299.9945\n",
      "Epoch 22/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1211.5036 - mse: 15400837.0000 - mae: 1211.5037 - val_loss: 1297.7345 - val_mse: 18837686.0000 - val_mae: 1297.7345\n",
      "Epoch 23/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1207.8998 - mse: 15389026.0000 - mae: 1207.8998 - val_loss: 1295.3479 - val_mse: 18826722.0000 - val_mae: 1295.3479\n",
      "Epoch 24/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1204.4943 - mse: 15372734.0000 - mae: 1204.4943 - val_loss: 1292.9185 - val_mse: 18815362.0000 - val_mae: 1292.9185\n",
      "Epoch 25/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1202.9850 - mse: 15355045.0000 - mae: 1202.9850 - val_loss: 1290.7729 - val_mse: 18804942.0000 - val_mae: 1290.7729\n",
      "Epoch 26/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1201.0586 - mse: 15348662.0000 - mae: 1201.0586 - val_loss: 1288.6449 - val_mse: 18793992.0000 - val_mae: 1288.6449\n",
      "Epoch 27/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1198.5980 - mse: 15324799.0000 - mae: 1198.5980 - val_loss: 1286.6469 - val_mse: 18782774.0000 - val_mae: 1286.6469\n",
      "Epoch 28/500\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 1196.2961 - mse: 15323525.0000 - mae: 1196.2961 - val_loss: 1284.8579 - val_mse: 18771080.0000 - val_mae: 1284.8579\n",
      "Epoch 29/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1194.3051 - mse: 15300661.0000 - mae: 1194.3052 - val_loss: 1283.3882 - val_mse: 18759856.0000 - val_mae: 1283.3882\n",
      "Epoch 30/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1195.1162 - mse: 15306138.0000 - mae: 1195.1162 - val_loss: 1282.0569 - val_mse: 18748802.0000 - val_mae: 1282.0569\n",
      "Epoch 31/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1192.0445 - mse: 15289934.0000 - mae: 1192.0446 - val_loss: 1280.8269 - val_mse: 18737138.0000 - val_mae: 1280.8269\n",
      "Epoch 32/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1189.4940 - mse: 15285631.0000 - mae: 1189.4940 - val_loss: 1279.7552 - val_mse: 18725480.0000 - val_mae: 1279.7552\n",
      "Epoch 33/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1190.0748 - mse: 15264626.0000 - mae: 1190.0747 - val_loss: 1278.8824 - val_mse: 18714748.0000 - val_mae: 1278.8824\n",
      "Epoch 34/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1188.3246 - mse: 15261487.0000 - mae: 1188.3245 - val_loss: 1278.0697 - val_mse: 18703594.0000 - val_mae: 1278.0697\n",
      "Epoch 35/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1186.9726 - mse: 15253211.0000 - mae: 1186.9727 - val_loss: 1277.2988 - val_mse: 18692690.0000 - val_mae: 1277.2988\n",
      "Epoch 36/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1185.7361 - mse: 15227744.0000 - mae: 1185.7362 - val_loss: 1276.5851 - val_mse: 18682268.0000 - val_mae: 1276.5851\n",
      "Epoch 37/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1183.3806 - mse: 15216887.0000 - mae: 1183.3805 - val_loss: 1275.8927 - val_mse: 18672082.0000 - val_mae: 1275.8927\n",
      "Epoch 38/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1184.1876 - mse: 15222565.0000 - mae: 1184.1876 - val_loss: 1275.1747 - val_mse: 18661410.0000 - val_mae: 1275.1747\n",
      "Epoch 39/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1182.0759 - mse: 15198302.0000 - mae: 1182.0759 - val_loss: 1274.5095 - val_mse: 18650974.0000 - val_mae: 1274.5095\n",
      "Epoch 40/500\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 1180.9125 - mse: 15192826.0000 - mae: 1180.9125 - val_loss: 1273.9095 - val_mse: 18641372.0000 - val_mae: 1273.9095\n",
      "Epoch 41/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1183.6714 - mse: 15169640.0000 - mae: 1183.6715 - val_loss: 1273.3495 - val_mse: 18632000.0000 - val_mae: 1273.3495\n",
      "Epoch 42/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1178.7812 - mse: 15177763.0000 - mae: 1178.7812 - val_loss: 1272.7921 - val_mse: 18622630.0000 - val_mae: 1272.7921\n",
      "Epoch 43/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1177.9234 - mse: 15161491.0000 - mae: 1177.9235 - val_loss: 1272.3594 - val_mse: 18614834.0000 - val_mae: 1272.3594\n",
      "Epoch 44/500\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 1178.7193 - mse: 15150297.0000 - mae: 1178.7192 - val_loss: 1271.8431 - val_mse: 18605414.0000 - val_mae: 1271.8431\n",
      "Epoch 45/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1179.5572 - mse: 15156134.0000 - mae: 1179.5573 - val_loss: 1271.4258 - val_mse: 18596388.0000 - val_mae: 1271.4258\n",
      "Epoch 46/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1179.9768 - mse: 15152718.0000 - mae: 1179.9769 - val_loss: 1271.1201 - val_mse: 18587900.0000 - val_mae: 1271.1201\n",
      "Epoch 47/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1178.0057 - mse: 15134920.0000 - mae: 1178.0057 - val_loss: 1270.7679 - val_mse: 18579398.0000 - val_mae: 1270.7679\n",
      "Epoch 48/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1176.4216 - mse: 15115088.0000 - mae: 1176.4218 - val_loss: 1270.5306 - val_mse: 18571770.0000 - val_mae: 1270.5306\n",
      "Epoch 49/500\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 1176.9351 - mse: 15107736.0000 - mae: 1176.9351 - val_loss: 1270.3699 - val_mse: 18566394.0000 - val_mae: 1270.3699\n",
      "Epoch 50/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1177.6368 - mse: 15111268.0000 - mae: 1177.6368 - val_loss: 1270.2979 - val_mse: 18560626.0000 - val_mae: 1270.2979\n",
      "Epoch 51/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1174.1362 - mse: 15118875.0000 - mae: 1174.1362 - val_loss: 1270.1843 - val_mse: 18554784.0000 - val_mae: 1270.1843\n",
      "Epoch 52/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1176.8769 - mse: 15092988.0000 - mae: 1176.8770 - val_loss: 1270.1573 - val_mse: 18550012.0000 - val_mae: 1270.1573\n",
      "Epoch 53/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1176.9029 - mse: 15099727.0000 - mae: 1176.9028 - val_loss: 1270.0920 - val_mse: 18543984.0000 - val_mae: 1270.0920\n",
      "Epoch 54/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1175.8368 - mse: 15096975.0000 - mae: 1175.8368 - val_loss: 1270.0358 - val_mse: 18540448.0000 - val_mae: 1270.0358\n",
      "Epoch 55/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1176.8140 - mse: 15089457.0000 - mae: 1176.8140 - val_loss: 1269.9376 - val_mse: 18535926.0000 - val_mae: 1269.9376\n",
      "Epoch 56/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1175.1752 - mse: 15074657.0000 - mae: 1175.1753 - val_loss: 1269.9374 - val_mse: 18533170.0000 - val_mae: 1269.9374\n",
      "Epoch 57/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1173.9095 - mse: 15071310.0000 - mae: 1173.9095 - val_loss: 1269.8683 - val_mse: 18529974.0000 - val_mae: 1269.8683\n",
      "Epoch 58/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1175.9910 - mse: 15072610.0000 - mae: 1175.9910 - val_loss: 1269.9054 - val_mse: 18525676.0000 - val_mae: 1269.9054\n",
      "Epoch 59/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1179.8772 - mse: 15056898.0000 - mae: 1179.8771 - val_loss: 1269.8840 - val_mse: 18524820.0000 - val_mae: 1269.8840\n",
      "Epoch 60/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1173.4170 - mse: 15048542.0000 - mae: 1173.4170 - val_loss: 1269.7914 - val_mse: 18520584.0000 - val_mae: 1269.7914\n",
      "Epoch 61/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1175.0005 - mse: 15076721.0000 - mae: 1175.0005 - val_loss: 1269.7465 - val_mse: 18517484.0000 - val_mae: 1269.7465\n",
      "Epoch 62/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1174.9729 - mse: 15055930.0000 - mae: 1174.9729 - val_loss: 1269.7416 - val_mse: 18516472.0000 - val_mae: 1269.7416\n",
      "Epoch 63/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1174.1085 - mse: 15052041.0000 - mae: 1174.1085 - val_loss: 1269.7206 - val_mse: 18515716.0000 - val_mae: 1269.7206\n",
      "Epoch 64/500\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 1171.6174 - mse: 15053942.0000 - mae: 1171.6174 - val_loss: 1269.6780 - val_mse: 18512706.0000 - val_mae: 1269.6780\n",
      "Epoch 65/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1175.7734 - mse: 15058510.0000 - mae: 1175.7733 - val_loss: 1269.7285 - val_mse: 18513176.0000 - val_mae: 1269.7285\n",
      "Epoch 66/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1172.0769 - mse: 15083353.0000 - mae: 1172.0769 - val_loss: 1269.7661 - val_mse: 18513176.0000 - val_mae: 1269.7661\n",
      "Epoch 67/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1172.9250 - mse: 15044131.0000 - mae: 1172.9250 - val_loss: 1269.7065 - val_mse: 18510134.0000 - val_mae: 1269.7065\n",
      "Epoch 68/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1174.2076 - mse: 15037684.0000 - mae: 1174.2076 - val_loss: 1269.7451 - val_mse: 18509590.0000 - val_mae: 1269.7451\n",
      "Epoch 69/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1175.8778 - mse: 15058189.0000 - mae: 1175.8778 - val_loss: 1269.7443 - val_mse: 18508438.0000 - val_mae: 1269.7443\n",
      "Epoch 70/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1173.5766 - mse: 15036738.0000 - mae: 1173.5767 - val_loss: 1269.6620 - val_mse: 18505472.0000 - val_mae: 1269.6620\n",
      "Epoch 71/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1172.5248 - mse: 15062960.0000 - mae: 1172.5249 - val_loss: 1269.5969 - val_mse: 18502962.0000 - val_mae: 1269.5969\n",
      "Epoch 72/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1173.0038 - mse: 15034321.0000 - mae: 1173.0039 - val_loss: 1269.5575 - val_mse: 18500760.0000 - val_mae: 1269.5575\n",
      "Epoch 73/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1179.6183 - mse: 15068041.0000 - mae: 1179.6183 - val_loss: 1269.4810 - val_mse: 18500374.0000 - val_mae: 1269.4810\n",
      "Epoch 74/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1172.7088 - mse: 15049338.0000 - mae: 1172.7087 - val_loss: 1269.5437 - val_mse: 18499172.0000 - val_mae: 1269.5437\n",
      "Epoch 75/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1176.7589 - mse: 15058592.0000 - mae: 1176.7589 - val_loss: 1269.5122 - val_mse: 18499490.0000 - val_mae: 1269.5122\n",
      "Epoch 76/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1175.8122 - mse: 15071950.0000 - mae: 1175.8121 - val_loss: 1269.4554 - val_mse: 18497964.0000 - val_mae: 1269.4554\n",
      "Epoch 77/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1177.1497 - mse: 15052831.0000 - mae: 1177.1498 - val_loss: 1269.5585 - val_mse: 18500172.0000 - val_mae: 1269.5585\n",
      "Epoch 78/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1174.8281 - mse: 15020062.0000 - mae: 1174.8280 - val_loss: 1269.5479 - val_mse: 18499734.0000 - val_mae: 1269.5479\n",
      "Epoch 79/500\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 1177.0776 - mse: 15045648.0000 - mae: 1177.0775 - val_loss: 1269.5367 - val_mse: 18499412.0000 - val_mae: 1269.5367\n",
      "Epoch 80/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1174.4200 - mse: 15036508.0000 - mae: 1174.4200 - val_loss: 1269.5022 - val_mse: 18498374.0000 - val_mae: 1269.5022\n",
      "Epoch 81/500\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 1174.1276 - mse: 15053598.0000 - mae: 1174.1277 - val_loss: 1269.4976 - val_mse: 18497572.0000 - val_mae: 1269.4976\n",
      "Epoch 82/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1173.4496 - mse: 15024794.0000 - mae: 1173.4495 - val_loss: 1269.3829 - val_mse: 18495014.0000 - val_mae: 1269.3829\n",
      "Epoch 83/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1174.5761 - mse: 15051572.0000 - mae: 1174.5762 - val_loss: 1269.3419 - val_mse: 18495876.0000 - val_mae: 1269.3419\n",
      "Epoch 84/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1175.2697 - mse: 15056763.0000 - mae: 1175.2698 - val_loss: 1269.3878 - val_mse: 18496516.0000 - val_mae: 1269.3878\n",
      "Epoch 85/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1173.2250 - mse: 15042357.0000 - mae: 1173.2250 - val_loss: 1269.4556 - val_mse: 18497770.0000 - val_mae: 1269.4556\n",
      "Epoch 86/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1178.8293 - mse: 15052604.0000 - mae: 1178.8292 - val_loss: 1269.4496 - val_mse: 18497732.0000 - val_mae: 1269.4496\n",
      "Epoch 87/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1175.0342 - mse: 15059474.0000 - mae: 1175.0342 - val_loss: 1269.3059 - val_mse: 18496710.0000 - val_mae: 1269.3059\n",
      "Epoch 88/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1179.4916 - mse: 15065912.0000 - mae: 1179.4915 - val_loss: 1269.2482 - val_mse: 18497942.0000 - val_mae: 1269.2482\n",
      "Epoch 89/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1173.2245 - mse: 15045767.0000 - mae: 1173.2245 - val_loss: 1269.3121 - val_mse: 18497964.0000 - val_mae: 1269.3121\n",
      "Epoch 90/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1172.2600 - mse: 15042895.0000 - mae: 1172.2600 - val_loss: 1269.2433 - val_mse: 18494614.0000 - val_mae: 1269.2433\n",
      "Epoch 91/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1177.0228 - mse: 15047827.0000 - mae: 1177.0227 - val_loss: 1269.1106 - val_mse: 18490764.0000 - val_mae: 1269.1106\n",
      "Epoch 92/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1171.6135 - mse: 15027955.0000 - mae: 1171.6135 - val_loss: 1269.0830 - val_mse: 18491498.0000 - val_mae: 1269.0830\n",
      "Epoch 93/500\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 1174.4115 - mse: 15060628.0000 - mae: 1174.4115 - val_loss: 1269.2197 - val_mse: 18492810.0000 - val_mae: 1269.2197\n",
      "Epoch 94/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1173.2196 - mse: 15028002.0000 - mae: 1173.2195 - val_loss: 1269.1587 - val_mse: 18492182.0000 - val_mae: 1269.1587\n",
      "Epoch 95/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1174.1577 - mse: 15043119.0000 - mae: 1174.1576 - val_loss: 1269.1854 - val_mse: 18491566.0000 - val_mae: 1269.1854\n",
      "Epoch 96/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1171.9332 - mse: 15024546.0000 - mae: 1171.9331 - val_loss: 1269.1582 - val_mse: 18491314.0000 - val_mae: 1269.1582\n",
      "Epoch 97/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1175.4391 - mse: 15040129.0000 - mae: 1175.4390 - val_loss: 1269.1143 - val_mse: 18490878.0000 - val_mae: 1269.1143\n",
      "Epoch 98/500\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 1171.8771 - mse: 15047075.0000 - mae: 1171.8770 - val_loss: 1269.0833 - val_mse: 18490338.0000 - val_mae: 1269.0833\n",
      "Epoch 99/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1174.1829 - mse: 15030649.0000 - mae: 1174.1830 - val_loss: 1269.1421 - val_mse: 18491438.0000 - val_mae: 1269.1421\n",
      "Epoch 100/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1173.2476 - mse: 15025862.0000 - mae: 1173.2477 - val_loss: 1269.1481 - val_mse: 18492842.0000 - val_mae: 1269.1481\n",
      "Epoch 101/500\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 1174.7161 - mse: 15044125.0000 - mae: 1174.7159 - val_loss: 1269.0299 - val_mse: 18491102.0000 - val_mae: 1269.0299\n",
      "Epoch 102/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1173.2520 - mse: 15029166.0000 - mae: 1173.2520 - val_loss: 1269.0057 - val_mse: 18491870.0000 - val_mae: 1269.0057\n",
      "Epoch 103/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1174.7745 - mse: 15049136.0000 - mae: 1174.7745 - val_loss: 1268.9775 - val_mse: 18490774.0000 - val_mae: 1268.9775\n",
      "Epoch 104/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1173.8995 - mse: 15026674.0000 - mae: 1173.8995 - val_loss: 1268.9113 - val_mse: 18487076.0000 - val_mae: 1268.9113\n",
      "Epoch 105/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1176.4934 - mse: 15006020.0000 - mae: 1176.4935 - val_loss: 1268.9014 - val_mse: 18487782.0000 - val_mae: 1268.9014\n",
      "Epoch 106/500\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 1172.2025 - mse: 15034225.0000 - mae: 1172.2025 - val_loss: 1268.7700 - val_mse: 18486292.0000 - val_mae: 1268.7700\n",
      "Epoch 107/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1172.6828 - mse: 15017970.0000 - mae: 1172.6827 - val_loss: 1268.7606 - val_mse: 18485662.0000 - val_mae: 1268.7606\n",
      "Epoch 108/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1173.6475 - mse: 15033350.0000 - mae: 1173.6475 - val_loss: 1268.8875 - val_mse: 18486364.0000 - val_mae: 1268.8875\n",
      "Epoch 109/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1173.4164 - mse: 15019844.0000 - mae: 1173.4165 - val_loss: 1268.9398 - val_mse: 18486732.0000 - val_mae: 1268.9398\n",
      "Epoch 110/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1174.4261 - mse: 15054748.0000 - mae: 1174.4263 - val_loss: 1268.9159 - val_mse: 18486532.0000 - val_mae: 1268.9159\n",
      "Epoch 111/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1173.9291 - mse: 15062604.0000 - mae: 1173.9291 - val_loss: 1268.9500 - val_mse: 18486506.0000 - val_mae: 1268.9500\n",
      "Epoch 112/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1176.1980 - mse: 15018196.0000 - mae: 1176.1980 - val_loss: 1268.9156 - val_mse: 18487844.0000 - val_mae: 1268.9156\n",
      "Epoch 113/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1171.7458 - mse: 15015001.0000 - mae: 1171.7457 - val_loss: 1268.8005 - val_mse: 18485688.0000 - val_mae: 1268.8005\n",
      "Epoch 114/500\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 1173.9305 - mse: 15018967.0000 - mae: 1173.9305 - val_loss: 1268.7991 - val_mse: 18485396.0000 - val_mae: 1268.7991\n",
      "Epoch 115/500\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 1172.9604 - mse: 15017768.0000 - mae: 1172.9603 - val_loss: 1268.8746 - val_mse: 18486276.0000 - val_mae: 1268.8746\n",
      "Epoch 116/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1173.1058 - mse: 15028427.0000 - mae: 1173.1058 - val_loss: 1268.8461 - val_mse: 18486224.0000 - val_mae: 1268.8461\n",
      "Epoch 117/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1174.6161 - mse: 15032144.0000 - mae: 1174.6162 - val_loss: 1268.8175 - val_mse: 18485094.0000 - val_mae: 1268.8175\n",
      "Epoch 118/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1173.8288 - mse: 15015771.0000 - mae: 1173.8287 - val_loss: 1268.7292 - val_mse: 18483264.0000 - val_mae: 1268.7292\n",
      "Epoch 119/500\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 1174.1235 - mse: 15012188.0000 - mae: 1174.1235 - val_loss: 1268.7751 - val_mse: 18484874.0000 - val_mae: 1268.7751\n",
      "Epoch 120/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1176.0062 - mse: 15041458.0000 - mae: 1176.0062 - val_loss: 1268.6536 - val_mse: 18486484.0000 - val_mae: 1268.6536\n",
      "Epoch 121/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1176.8169 - mse: 15030901.0000 - mae: 1176.8169 - val_loss: 1268.6301 - val_mse: 18487276.0000 - val_mae: 1268.6301\n",
      "Epoch 122/500\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 1176.9270 - mse: 15039622.0000 - mae: 1176.9270 - val_loss: 1268.6494 - val_mse: 18488246.0000 - val_mae: 1268.6494\n",
      "Epoch 123/500\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 1176.6049 - mse: 15028074.0000 - mae: 1176.6049 - val_loss: 1268.6354 - val_mse: 18487956.0000 - val_mae: 1268.6354\n",
      "Epoch 124/500\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 1175.2952 - mse: 15055787.0000 - mae: 1175.2953 - val_loss: 1268.5725 - val_mse: 18488536.0000 - val_mae: 1268.5725\n",
      "Epoch 125/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1176.2217 - mse: 15039114.0000 - mae: 1176.2218 - val_loss: 1268.5243 - val_mse: 18488824.0000 - val_mae: 1268.5243\n",
      "Epoch 126/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1174.3515 - mse: 15060226.0000 - mae: 1174.3514 - val_loss: 1268.6554 - val_mse: 18490816.0000 - val_mae: 1268.6554\n",
      "Epoch 127/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1172.5724 - mse: 15035619.0000 - mae: 1172.5725 - val_loss: 1268.6461 - val_mse: 18490130.0000 - val_mae: 1268.6461\n",
      "Epoch 128/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1175.4301 - mse: 15041722.0000 - mae: 1175.4302 - val_loss: 1268.5999 - val_mse: 18489350.0000 - val_mae: 1268.5999\n",
      "Epoch 129/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1172.9975 - mse: 15040892.0000 - mae: 1172.9976 - val_loss: 1268.6086 - val_mse: 18488908.0000 - val_mae: 1268.6086\n",
      "Epoch 130/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1172.1928 - mse: 15029415.0000 - mae: 1172.1927 - val_loss: 1268.6434 - val_mse: 18486364.0000 - val_mae: 1268.6434\n",
      "Epoch 131/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1176.6816 - mse: 15039015.0000 - mae: 1176.6815 - val_loss: 1268.6807 - val_mse: 18486354.0000 - val_mae: 1268.6807\n",
      "Epoch 132/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1173.8078 - mse: 15046732.0000 - mae: 1173.8077 - val_loss: 1268.6232 - val_mse: 18486198.0000 - val_mae: 1268.6232\n",
      "Epoch 133/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1176.3740 - mse: 15059344.0000 - mae: 1176.3740 - val_loss: 1268.5619 - val_mse: 18487106.0000 - val_mae: 1268.5619\n",
      "Epoch 134/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1172.3004 - mse: 15031423.0000 - mae: 1172.3004 - val_loss: 1268.5260 - val_mse: 18485924.0000 - val_mae: 1268.5260\n",
      "Epoch 135/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1174.2659 - mse: 15016501.0000 - mae: 1174.2659 - val_loss: 1268.6053 - val_mse: 18487006.0000 - val_mae: 1268.6053\n",
      "Epoch 136/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1171.5703 - mse: 15014955.0000 - mae: 1171.5702 - val_loss: 1268.5803 - val_mse: 18486840.0000 - val_mae: 1268.5803\n",
      "Epoch 137/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1174.4316 - mse: 15032869.0000 - mae: 1174.4315 - val_loss: 1268.6553 - val_mse: 18486394.0000 - val_mae: 1268.6553\n",
      "Epoch 138/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1171.9911 - mse: 15046815.0000 - mae: 1171.9911 - val_loss: 1268.5751 - val_mse: 18484818.0000 - val_mae: 1268.5751\n",
      "Epoch 139/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1172.5696 - mse: 15014185.0000 - mae: 1172.5696 - val_loss: 1268.5724 - val_mse: 18485896.0000 - val_mae: 1268.5724\n",
      "Epoch 140/500\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 1173.8823 - mse: 15016428.0000 - mae: 1173.8822 - val_loss: 1268.4702 - val_mse: 18484952.0000 - val_mae: 1268.4702\n",
      "Epoch 141/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1173.8094 - mse: 15028713.0000 - mae: 1173.8094 - val_loss: 1268.3754 - val_mse: 18483106.0000 - val_mae: 1268.3754\n",
      "Epoch 142/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1174.4671 - mse: 15032927.0000 - mae: 1174.4672 - val_loss: 1268.5082 - val_mse: 18484122.0000 - val_mae: 1268.5082\n",
      "Epoch 143/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1175.7918 - mse: 15051582.0000 - mae: 1175.7917 - val_loss: 1268.5197 - val_mse: 18484622.0000 - val_mae: 1268.5197\n",
      "Epoch 144/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1175.9560 - mse: 15049282.0000 - mae: 1175.9561 - val_loss: 1268.4791 - val_mse: 18485448.0000 - val_mae: 1268.4791\n",
      "Epoch 145/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1172.0073 - mse: 15011545.0000 - mae: 1172.0072 - val_loss: 1268.3674 - val_mse: 18484552.0000 - val_mae: 1268.3674\n",
      "Epoch 146/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1173.9979 - mse: 15042721.0000 - mae: 1173.9979 - val_loss: 1268.2715 - val_mse: 18484830.0000 - val_mae: 1268.2715\n",
      "Epoch 147/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1173.6756 - mse: 15018809.0000 - mae: 1173.6757 - val_loss: 1268.3193 - val_mse: 18485680.0000 - val_mae: 1268.3193\n",
      "Epoch 148/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1175.1459 - mse: 15055209.0000 - mae: 1175.1459 - val_loss: 1268.3760 - val_mse: 18486396.0000 - val_mae: 1268.3760\n",
      "Epoch 149/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1173.3461 - mse: 15054890.0000 - mae: 1173.3459 - val_loss: 1268.3435 - val_mse: 18486702.0000 - val_mae: 1268.3435\n",
      "Epoch 150/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1174.1644 - mse: 15049314.0000 - mae: 1174.1646 - val_loss: 1268.2744 - val_mse: 18485590.0000 - val_mae: 1268.2744\n",
      "Epoch 151/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1172.6054 - mse: 15035015.0000 - mae: 1172.6053 - val_loss: 1268.3409 - val_mse: 18486052.0000 - val_mae: 1268.3409\n",
      "Epoch 152/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1175.8124 - mse: 15042123.0000 - mae: 1175.8124 - val_loss: 1268.2720 - val_mse: 18487420.0000 - val_mae: 1268.2720\n",
      "Epoch 153/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1173.9260 - mse: 15049298.0000 - mae: 1173.9260 - val_loss: 1268.2783 - val_mse: 18488152.0000 - val_mae: 1268.2783\n",
      "Epoch 154/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1171.2556 - mse: 15050598.0000 - mae: 1171.2556 - val_loss: 1268.2100 - val_mse: 18488922.0000 - val_mae: 1268.2100\n",
      "Epoch 155/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1174.3153 - mse: 15035195.0000 - mae: 1174.3153 - val_loss: 1268.2013 - val_mse: 18488782.0000 - val_mae: 1268.2013\n",
      "Epoch 156/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1173.9562 - mse: 15045310.0000 - mae: 1173.9563 - val_loss: 1268.2826 - val_mse: 18489314.0000 - val_mae: 1268.2826\n",
      "Epoch 157/500\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 1173.9552 - mse: 15043630.0000 - mae: 1173.9552 - val_loss: 1268.3008 - val_mse: 18489434.0000 - val_mae: 1268.3008\n",
      "Epoch 158/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1174.3022 - mse: 15042280.0000 - mae: 1174.3021 - val_loss: 1268.2766 - val_mse: 18489754.0000 - val_mae: 1268.2766\n",
      "Epoch 159/500\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 1174.7076 - mse: 15032137.0000 - mae: 1174.7078 - val_loss: 1268.1654 - val_mse: 18487110.0000 - val_mae: 1268.1654\n",
      "Epoch 160/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1174.5538 - mse: 15043289.0000 - mae: 1174.5537 - val_loss: 1268.0822 - val_mse: 18485528.0000 - val_mae: 1268.0822\n",
      "Epoch 161/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1172.0685 - mse: 15046462.0000 - mae: 1172.0685 - val_loss: 1268.1423 - val_mse: 18484590.0000 - val_mae: 1268.1423\n",
      "Epoch 162/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1176.6926 - mse: 15056000.0000 - mae: 1176.6925 - val_loss: 1268.0775 - val_mse: 18484022.0000 - val_mae: 1268.0775\n",
      "Epoch 163/500\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 1170.7887 - mse: 15004520.0000 - mae: 1170.7887 - val_loss: 1268.0748 - val_mse: 18481672.0000 - val_mae: 1268.0748\n",
      "Epoch 164/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1174.6222 - mse: 15026903.0000 - mae: 1174.6221 - val_loss: 1268.1965 - val_mse: 18485864.0000 - val_mae: 1268.1965\n",
      "Epoch 165/500\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 1178.1117 - mse: 15051238.0000 - mae: 1178.1117 - val_loss: 1268.2284 - val_mse: 18490152.0000 - val_mae: 1268.2284\n",
      "Epoch 166/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1174.7324 - mse: 15055610.0000 - mae: 1174.7324 - val_loss: 1268.1012 - val_mse: 18489060.0000 - val_mae: 1268.1012\n",
      "Epoch 167/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1173.5922 - mse: 15067260.0000 - mae: 1173.5923 - val_loss: 1268.1307 - val_mse: 18490090.0000 - val_mae: 1268.1307\n",
      "Epoch 168/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1173.5186 - mse: 15071597.0000 - mae: 1173.5187 - val_loss: 1268.0715 - val_mse: 18489332.0000 - val_mae: 1268.0715\n",
      "Epoch 169/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1176.5209 - mse: 15056387.0000 - mae: 1176.5210 - val_loss: 1268.0314 - val_mse: 18488004.0000 - val_mae: 1268.0314\n",
      "Epoch 170/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1173.3262 - mse: 15040430.0000 - mae: 1173.3263 - val_loss: 1267.9868 - val_mse: 18485912.0000 - val_mae: 1267.9868\n",
      "Epoch 171/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1173.2526 - mse: 15069354.0000 - mae: 1173.2527 - val_loss: 1268.1305 - val_mse: 18485264.0000 - val_mae: 1268.1305\n",
      "Epoch 172/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1172.7552 - mse: 15037000.0000 - mae: 1172.7552 - val_loss: 1268.0956 - val_mse: 18483554.0000 - val_mae: 1268.0956\n",
      "Epoch 173/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1176.8283 - mse: 15046570.0000 - mae: 1176.8282 - val_loss: 1268.1252 - val_mse: 18486042.0000 - val_mae: 1268.1252\n",
      "Epoch 174/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1176.2465 - mse: 15042546.0000 - mae: 1176.2465 - val_loss: 1268.0552 - val_mse: 18485642.0000 - val_mae: 1268.0552\n",
      "Epoch 175/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1174.4597 - mse: 15049110.0000 - mae: 1174.4596 - val_loss: 1268.1266 - val_mse: 18488058.0000 - val_mae: 1268.1266\n",
      "Epoch 176/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1175.5763 - mse: 15026512.0000 - mae: 1175.5763 - val_loss: 1268.0553 - val_mse: 18489176.0000 - val_mae: 1268.0553\n",
      "Epoch 177/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1174.7962 - mse: 15046633.0000 - mae: 1174.7963 - val_loss: 1268.0687 - val_mse: 18489254.0000 - val_mae: 1268.0687\n",
      "Epoch 178/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1171.6124 - mse: 15051684.0000 - mae: 1171.6124 - val_loss: 1267.9512 - val_mse: 18487500.0000 - val_mae: 1267.9512\n",
      "Epoch 179/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1173.6640 - mse: 15066049.0000 - mae: 1173.6639 - val_loss: 1268.0288 - val_mse: 18486416.0000 - val_mae: 1268.0288\n",
      "Epoch 180/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1176.7122 - mse: 15036289.0000 - mae: 1176.7123 - val_loss: 1268.0579 - val_mse: 18487830.0000 - val_mae: 1268.0579\n",
      "Epoch 181/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1171.8022 - mse: 15032268.0000 - mae: 1171.8022 - val_loss: 1268.0099 - val_mse: 18485302.0000 - val_mae: 1268.0099\n",
      "Epoch 182/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1173.8844 - mse: 15029258.0000 - mae: 1173.8844 - val_loss: 1268.0897 - val_mse: 18486580.0000 - val_mae: 1268.0897\n",
      "Epoch 183/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1171.5587 - mse: 15029248.0000 - mae: 1171.5587 - val_loss: 1267.9929 - val_mse: 18485386.0000 - val_mae: 1267.9929\n",
      "Epoch 184/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1174.5699 - mse: 15048635.0000 - mae: 1174.5699 - val_loss: 1268.0242 - val_mse: 18486976.0000 - val_mae: 1268.0242\n",
      "Epoch 185/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1171.8643 - mse: 15016852.0000 - mae: 1171.8643 - val_loss: 1267.9374 - val_mse: 18482922.0000 - val_mae: 1267.9374\n",
      "Epoch 186/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1171.7981 - mse: 15016272.0000 - mae: 1171.7981 - val_loss: 1267.9520 - val_mse: 18481744.0000 - val_mae: 1267.9520\n",
      "Epoch 187/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1172.5145 - mse: 15025818.0000 - mae: 1172.5145 - val_loss: 1268.0507 - val_mse: 18482788.0000 - val_mae: 1268.0507\n",
      "Epoch 188/500\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 1174.3807 - mse: 15038287.0000 - mae: 1174.3807 - val_loss: 1268.0162 - val_mse: 18483790.0000 - val_mae: 1268.0162\n",
      "Epoch 189/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1176.1416 - mse: 15050107.0000 - mae: 1176.1416 - val_loss: 1267.9769 - val_mse: 18485498.0000 - val_mae: 1267.9769\n",
      "Epoch 190/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1169.7845 - mse: 15019823.0000 - mae: 1169.7845 - val_loss: 1267.9977 - val_mse: 18484236.0000 - val_mae: 1267.9977\n",
      "Epoch 191/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1173.2027 - mse: 15023239.0000 - mae: 1173.2028 - val_loss: 1267.9551 - val_mse: 18483162.0000 - val_mae: 1267.9551\n",
      "Epoch 192/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1174.0564 - mse: 15055880.0000 - mae: 1174.0564 - val_loss: 1267.9463 - val_mse: 18481814.0000 - val_mae: 1267.9463\n",
      "Epoch 193/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1176.0294 - mse: 15041016.0000 - mae: 1176.0295 - val_loss: 1267.9021 - val_mse: 18480390.0000 - val_mae: 1267.9021\n",
      "Epoch 194/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1171.5808 - mse: 15044469.0000 - mae: 1171.5808 - val_loss: 1267.8381 - val_mse: 18480090.0000 - val_mae: 1267.8381\n",
      "Epoch 195/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1172.5632 - mse: 15043383.0000 - mae: 1172.5632 - val_loss: 1267.8434 - val_mse: 18479792.0000 - val_mae: 1267.8434\n",
      "Epoch 196/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1175.6800 - mse: 15063626.0000 - mae: 1175.6801 - val_loss: 1267.9796 - val_mse: 18481020.0000 - val_mae: 1267.9796\n",
      "Epoch 197/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1175.1898 - mse: 15032533.0000 - mae: 1175.1897 - val_loss: 1267.8823 - val_mse: 18479724.0000 - val_mae: 1267.8823\n",
      "Epoch 198/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1173.8449 - mse: 15030929.0000 - mae: 1173.8450 - val_loss: 1267.8351 - val_mse: 18479000.0000 - val_mae: 1267.8351\n",
      "Epoch 199/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1171.9020 - mse: 15039326.0000 - mae: 1171.9020 - val_loss: 1267.7976 - val_mse: 18475890.0000 - val_mae: 1267.7976\n",
      "Epoch 200/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1173.7200 - mse: 15014388.0000 - mae: 1173.7200 - val_loss: 1267.8732 - val_mse: 18477280.0000 - val_mae: 1267.8732\n",
      "Epoch 201/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1170.2581 - mse: 15022838.0000 - mae: 1170.2581 - val_loss: 1267.8535 - val_mse: 18475966.0000 - val_mae: 1267.8535\n",
      "Epoch 202/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1173.1364 - mse: 15031101.0000 - mae: 1173.1365 - val_loss: 1267.8890 - val_mse: 18475636.0000 - val_mae: 1267.8890\n",
      "Epoch 203/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1176.0657 - mse: 15037889.0000 - mae: 1176.0658 - val_loss: 1267.7817 - val_mse: 18475484.0000 - val_mae: 1267.7817\n",
      "Epoch 204/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1174.3445 - mse: 15042664.0000 - mae: 1174.3445 - val_loss: 1267.8031 - val_mse: 18474602.0000 - val_mae: 1267.8031\n",
      "Epoch 205/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1174.6436 - mse: 15032760.0000 - mae: 1174.6436 - val_loss: 1267.7476 - val_mse: 18474082.0000 - val_mae: 1267.7476\n",
      "Epoch 206/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1173.7374 - mse: 15039887.0000 - mae: 1173.7374 - val_loss: 1267.8488 - val_mse: 18476444.0000 - val_mae: 1267.8488\n",
      "Epoch 207/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1174.3748 - mse: 15068729.0000 - mae: 1174.3748 - val_loss: 1267.9017 - val_mse: 18478194.0000 - val_mae: 1267.9017\n",
      "Epoch 208/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1172.8251 - mse: 15044178.0000 - mae: 1172.8250 - val_loss: 1267.8735 - val_mse: 18477180.0000 - val_mae: 1267.8735\n",
      "Epoch 209/500\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 1174.4055 - mse: 15035908.0000 - mae: 1174.4055 - val_loss: 1267.7809 - val_mse: 18477604.0000 - val_mae: 1267.7809\n",
      "Epoch 210/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1174.1570 - mse: 15036414.0000 - mae: 1174.1570 - val_loss: 1267.7416 - val_mse: 18477068.0000 - val_mae: 1267.7416\n",
      "Epoch 211/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1175.0006 - mse: 15033066.0000 - mae: 1175.0006 - val_loss: 1267.7850 - val_mse: 18479312.0000 - val_mae: 1267.7850\n",
      "Epoch 212/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1175.3418 - mse: 15050166.0000 - mae: 1175.3419 - val_loss: 1267.8083 - val_mse: 18479278.0000 - val_mae: 1267.8083\n",
      "Epoch 213/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1172.5607 - mse: 15052769.0000 - mae: 1172.5608 - val_loss: 1267.6914 - val_mse: 18478456.0000 - val_mae: 1267.6914\n",
      "Epoch 214/500\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 1175.6018 - mse: 15047231.0000 - mae: 1175.6018 - val_loss: 1267.7119 - val_mse: 18478870.0000 - val_mae: 1267.7119\n",
      "Epoch 215/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1173.3509 - mse: 15052481.0000 - mae: 1173.3510 - val_loss: 1267.8157 - val_mse: 18480600.0000 - val_mae: 1267.8157\n",
      "Epoch 216/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1175.2061 - mse: 15061057.0000 - mae: 1175.2062 - val_loss: 1267.6571 - val_mse: 18478892.0000 - val_mae: 1267.6571\n",
      "Epoch 217/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1170.7238 - mse: 15004918.0000 - mae: 1170.7238 - val_loss: 1267.7294 - val_mse: 18477758.0000 - val_mae: 1267.7294\n",
      "Epoch 218/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1173.8857 - mse: 15022643.0000 - mae: 1173.8857 - val_loss: 1267.6715 - val_mse: 18476976.0000 - val_mae: 1267.6715\n",
      "Epoch 219/500\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 1174.1972 - mse: 15035316.0000 - mae: 1174.1973 - val_loss: 1267.7144 - val_mse: 18475846.0000 - val_mae: 1267.7144\n",
      "Epoch 220/500\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 1172.9356 - mse: 15030567.0000 - mae: 1172.9357 - val_loss: 1267.6455 - val_mse: 18474920.0000 - val_mae: 1267.6455\n",
      "Epoch 221/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1172.7393 - mse: 15016959.0000 - mae: 1172.7393 - val_loss: 1267.7013 - val_mse: 18476856.0000 - val_mae: 1267.7013\n",
      "Epoch 222/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1174.2580 - mse: 15030944.0000 - mae: 1174.2581 - val_loss: 1267.7572 - val_mse: 18477064.0000 - val_mae: 1267.7572\n",
      "Epoch 223/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1172.6687 - mse: 15017777.0000 - mae: 1172.6687 - val_loss: 1267.7900 - val_mse: 18478064.0000 - val_mae: 1267.7900\n",
      "Epoch 224/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1175.3168 - mse: 15049089.0000 - mae: 1175.3169 - val_loss: 1267.6274 - val_mse: 18477272.0000 - val_mae: 1267.6274\n",
      "Epoch 225/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1171.9319 - mse: 15013581.0000 - mae: 1171.9319 - val_loss: 1267.6472 - val_mse: 18477414.0000 - val_mae: 1267.6472\n",
      "Epoch 226/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1175.3443 - mse: 15046091.0000 - mae: 1175.3442 - val_loss: 1267.6011 - val_mse: 18478880.0000 - val_mae: 1267.6011\n",
      "Epoch 227/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1175.7113 - mse: 15051837.0000 - mae: 1175.7113 - val_loss: 1267.5006 - val_mse: 18478422.0000 - val_mae: 1267.5006\n",
      "Epoch 228/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1172.6375 - mse: 15035081.0000 - mae: 1172.6373 - val_loss: 1267.5659 - val_mse: 18479468.0000 - val_mae: 1267.5659\n",
      "Epoch 229/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1173.0222 - mse: 15036301.0000 - mae: 1173.0222 - val_loss: 1267.5538 - val_mse: 18477576.0000 - val_mae: 1267.5538\n",
      "Epoch 230/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1173.9275 - mse: 15045884.0000 - mae: 1173.9275 - val_loss: 1267.4933 - val_mse: 18477642.0000 - val_mae: 1267.4933\n",
      "Epoch 231/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1174.1431 - mse: 15028161.0000 - mae: 1174.1431 - val_loss: 1267.5089 - val_mse: 18477008.0000 - val_mae: 1267.5089\n",
      "Epoch 232/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1172.3674 - mse: 15055961.0000 - mae: 1172.3676 - val_loss: 1267.5303 - val_mse: 18475786.0000 - val_mae: 1267.5303\n",
      "Epoch 233/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1172.2089 - mse: 15054499.0000 - mae: 1172.2090 - val_loss: 1267.6626 - val_mse: 18476540.0000 - val_mae: 1267.6626\n",
      "Epoch 234/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1177.2886 - mse: 15036744.0000 - mae: 1177.2886 - val_loss: 1267.6445 - val_mse: 18477824.0000 - val_mae: 1267.6445\n",
      "Epoch 235/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1173.1422 - mse: 15058321.0000 - mae: 1173.1422 - val_loss: 1267.6337 - val_mse: 18478084.0000 - val_mae: 1267.6337\n",
      "Epoch 236/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1172.3535 - mse: 15036799.0000 - mae: 1172.3535 - val_loss: 1267.5858 - val_mse: 18478870.0000 - val_mae: 1267.5858\n",
      "Epoch 237/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1172.0316 - mse: 15022864.0000 - mae: 1172.0315 - val_loss: 1267.6018 - val_mse: 18477408.0000 - val_mae: 1267.6018\n",
      "Epoch 238/500\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1171.1097 - mse: 15034612.0000 - mae: 1171.1097 - val_loss: 1267.6602 - val_mse: 18479200.0000 - val_mae: 1267.6602\n",
      "Epoch 239/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1175.1878 - mse: 15056454.0000 - mae: 1175.1877 - val_loss: 1267.5568 - val_mse: 18476806.0000 - val_mae: 1267.5568\n",
      "Epoch 240/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1172.3124 - mse: 15043685.0000 - mae: 1172.3124 - val_loss: 1267.5900 - val_mse: 18477198.0000 - val_mae: 1267.5900\n",
      "Epoch 241/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1176.4205 - mse: 15044454.0000 - mae: 1176.4205 - val_loss: 1267.6434 - val_mse: 18476506.0000 - val_mae: 1267.6434\n",
      "Epoch 242/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1173.9364 - mse: 15023221.0000 - mae: 1173.9364 - val_loss: 1267.5593 - val_mse: 18476284.0000 - val_mae: 1267.5593\n",
      "Epoch 243/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1174.8829 - mse: 15020935.0000 - mae: 1174.8829 - val_loss: 1267.5763 - val_mse: 18476304.0000 - val_mae: 1267.5763\n",
      "Epoch 244/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1172.9418 - mse: 15036987.0000 - mae: 1172.9418 - val_loss: 1267.6207 - val_mse: 18476818.0000 - val_mae: 1267.6207\n",
      "Epoch 245/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1175.5450 - mse: 15033799.0000 - mae: 1175.5450 - val_loss: 1267.4329 - val_mse: 18475772.0000 - val_mae: 1267.4329\n",
      "Epoch 246/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1173.4213 - mse: 15050045.0000 - mae: 1173.4213 - val_loss: 1267.3401 - val_mse: 18473318.0000 - val_mae: 1267.3401\n",
      "Epoch 247/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1174.5467 - mse: 15040500.0000 - mae: 1174.5468 - val_loss: 1267.4153 - val_mse: 18474602.0000 - val_mae: 1267.4153\n",
      "Epoch 248/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1175.4872 - mse: 15055120.0000 - mae: 1175.4873 - val_loss: 1267.3771 - val_mse: 18474620.0000 - val_mae: 1267.3771\n",
      "Epoch 249/500\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 1175.7615 - mse: 15040981.0000 - mae: 1175.7615 - val_loss: 1267.3378 - val_mse: 18473152.0000 - val_mae: 1267.3378\n",
      "Epoch 250/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1172.4030 - mse: 15048770.0000 - mae: 1172.4030 - val_loss: 1267.4318 - val_mse: 18473608.0000 - val_mae: 1267.4318\n",
      "Epoch 251/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1174.6510 - mse: 15027482.0000 - mae: 1174.6510 - val_loss: 1267.5085 - val_mse: 18475110.0000 - val_mae: 1267.5085\n",
      "Epoch 252/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1172.3267 - mse: 15038146.0000 - mae: 1172.3268 - val_loss: 1267.3253 - val_mse: 18472740.0000 - val_mae: 1267.3253\n",
      "Epoch 253/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1176.3397 - mse: 15048368.0000 - mae: 1176.3397 - val_loss: 1267.4692 - val_mse: 18474740.0000 - val_mae: 1267.4692\n",
      "Epoch 254/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1173.4209 - mse: 15030382.0000 - mae: 1173.4209 - val_loss: 1267.4457 - val_mse: 18474378.0000 - val_mae: 1267.4457\n",
      "Epoch 255/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1173.5798 - mse: 15037573.0000 - mae: 1173.5797 - val_loss: 1267.4156 - val_mse: 18474210.0000 - val_mae: 1267.4156\n",
      "Epoch 256/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1173.5792 - mse: 15035533.0000 - mae: 1173.5792 - val_loss: 1267.4459 - val_mse: 18473286.0000 - val_mae: 1267.4459\n",
      "Epoch 257/500\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 1174.3733 - mse: 15049380.0000 - mae: 1174.3733 - val_loss: 1267.2848 - val_mse: 18470356.0000 - val_mae: 1267.2848\n",
      "Epoch 258/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1170.7063 - mse: 15042568.0000 - mae: 1170.7063 - val_loss: 1267.3811 - val_mse: 18470548.0000 - val_mae: 1267.3811\n",
      "Epoch 259/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1174.9609 - mse: 15037161.0000 - mae: 1174.9608 - val_loss: 1267.3510 - val_mse: 18471668.0000 - val_mae: 1267.3510\n",
      "Epoch 260/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1172.3114 - mse: 15017409.0000 - mae: 1172.3115 - val_loss: 1267.3502 - val_mse: 18471524.0000 - val_mae: 1267.3502\n",
      "Epoch 261/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1172.3828 - mse: 15030026.0000 - mae: 1172.3827 - val_loss: 1267.3105 - val_mse: 18470522.0000 - val_mae: 1267.3105\n",
      "Epoch 262/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1172.9032 - mse: 15027898.0000 - mae: 1172.9032 - val_loss: 1267.2346 - val_mse: 18470210.0000 - val_mae: 1267.2346\n",
      "Epoch 263/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1174.4160 - mse: 15034582.0000 - mae: 1174.4160 - val_loss: 1267.2976 - val_mse: 18471672.0000 - val_mae: 1267.2976\n",
      "Epoch 264/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1173.3091 - mse: 15012241.0000 - mae: 1173.3091 - val_loss: 1267.2955 - val_mse: 18470862.0000 - val_mae: 1267.2955\n",
      "Epoch 265/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1172.9510 - mse: 15016132.0000 - mae: 1172.9510 - val_loss: 1267.3435 - val_mse: 18469906.0000 - val_mae: 1267.3435\n",
      "Epoch 266/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1175.0501 - mse: 15045894.0000 - mae: 1175.0500 - val_loss: 1267.3691 - val_mse: 18469316.0000 - val_mae: 1267.3691\n",
      "Epoch 267/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1172.6899 - mse: 15046100.0000 - mae: 1172.6898 - val_loss: 1267.4113 - val_mse: 18469060.0000 - val_mae: 1267.4113\n",
      "Epoch 268/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1174.9971 - mse: 15005394.0000 - mae: 1174.9971 - val_loss: 1267.3174 - val_mse: 18469440.0000 - val_mae: 1267.3174\n",
      "Epoch 269/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1176.4525 - mse: 15043417.0000 - mae: 1176.4525 - val_loss: 1267.3300 - val_mse: 18471412.0000 - val_mae: 1267.3300\n",
      "Epoch 270/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1169.7419 - mse: 15011710.0000 - mae: 1169.7417 - val_loss: 1267.2263 - val_mse: 18470632.0000 - val_mae: 1267.2263\n",
      "Epoch 271/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1172.9677 - mse: 15016761.0000 - mae: 1172.9678 - val_loss: 1267.3352 - val_mse: 18473020.0000 - val_mae: 1267.3352\n",
      "Epoch 272/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1172.0652 - mse: 15051380.0000 - mae: 1172.0653 - val_loss: 1267.2872 - val_mse: 18471880.0000 - val_mae: 1267.2872\n",
      "Epoch 273/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1173.7223 - mse: 15035256.0000 - mae: 1173.7223 - val_loss: 1267.2595 - val_mse: 18471356.0000 - val_mae: 1267.2595\n",
      "Epoch 274/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1174.2565 - mse: 15053670.0000 - mae: 1174.2565 - val_loss: 1267.2230 - val_mse: 18470520.0000 - val_mae: 1267.2230\n",
      "Epoch 275/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1175.3048 - mse: 15042489.0000 - mae: 1175.3048 - val_loss: 1267.2605 - val_mse: 18472280.0000 - val_mae: 1267.2605\n",
      "Epoch 276/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1173.9850 - mse: 15026203.0000 - mae: 1173.9850 - val_loss: 1267.3302 - val_mse: 18473410.0000 - val_mae: 1267.3302\n",
      "Epoch 277/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1173.2070 - mse: 15034112.0000 - mae: 1173.2070 - val_loss: 1267.1709 - val_mse: 18471804.0000 - val_mae: 1267.1709\n",
      "Epoch 278/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1173.4943 - mse: 15027909.0000 - mae: 1173.4943 - val_loss: 1267.1624 - val_mse: 18471628.0000 - val_mae: 1267.1624\n",
      "Epoch 279/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1172.3182 - mse: 15032527.0000 - mae: 1172.3182 - val_loss: 1267.1667 - val_mse: 18471564.0000 - val_mae: 1267.1667\n",
      "Epoch 280/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1174.5664 - mse: 15040029.0000 - mae: 1174.5665 - val_loss: 1267.2167 - val_mse: 18472582.0000 - val_mae: 1267.2167\n",
      "Epoch 281/500\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 1173.1866 - mse: 15036109.0000 - mae: 1173.1866 - val_loss: 1267.1804 - val_mse: 18471872.0000 - val_mae: 1267.1804\n",
      "Epoch 282/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1170.9262 - mse: 15057975.0000 - mae: 1170.9263 - val_loss: 1267.1396 - val_mse: 18470850.0000 - val_mae: 1267.1396\n",
      "Epoch 283/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1171.8920 - mse: 15027198.0000 - mae: 1171.8920 - val_loss: 1267.2231 - val_mse: 18470190.0000 - val_mae: 1267.2231\n",
      "Epoch 284/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1176.7820 - mse: 15024382.0000 - mae: 1176.7820 - val_loss: 1267.1821 - val_mse: 18470190.0000 - val_mae: 1267.1821\n",
      "Epoch 285/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1171.4237 - mse: 15032517.0000 - mae: 1171.4237 - val_loss: 1267.1906 - val_mse: 18469696.0000 - val_mae: 1267.1906\n",
      "Epoch 286/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1173.8377 - mse: 15019865.0000 - mae: 1173.8376 - val_loss: 1267.1206 - val_mse: 18470076.0000 - val_mae: 1267.1206\n",
      "Epoch 287/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1173.6758 - mse: 15008172.0000 - mae: 1173.6758 - val_loss: 1267.1583 - val_mse: 18469758.0000 - val_mae: 1267.1583\n",
      "Epoch 288/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1173.0301 - mse: 15025879.0000 - mae: 1173.0302 - val_loss: 1267.2587 - val_mse: 18470774.0000 - val_mae: 1267.2587\n",
      "Epoch 289/500\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 1171.0717 - mse: 15016773.0000 - mae: 1171.0718 - val_loss: 1267.1552 - val_mse: 18470076.0000 - val_mae: 1267.1552\n",
      "Epoch 290/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1170.0106 - mse: 15030333.0000 - mae: 1170.0106 - val_loss: 1267.1625 - val_mse: 18468676.0000 - val_mae: 1267.1625\n",
      "Epoch 291/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1172.9944 - mse: 15016481.0000 - mae: 1172.9944 - val_loss: 1267.2719 - val_mse: 18469648.0000 - val_mae: 1267.2719\n",
      "Epoch 292/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1174.9373 - mse: 15032893.0000 - mae: 1174.9373 - val_loss: 1267.3163 - val_mse: 18471782.0000 - val_mae: 1267.3163\n",
      "Epoch 293/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1168.1651 - mse: 14987241.0000 - mae: 1168.1652 - val_loss: 1267.2926 - val_mse: 18470096.0000 - val_mae: 1267.2926\n",
      "Epoch 294/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1169.8499 - mse: 15010557.0000 - mae: 1169.8500 - val_loss: 1267.2771 - val_mse: 18467328.0000 - val_mae: 1267.2771\n",
      "Epoch 295/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1174.8344 - mse: 15014437.0000 - mae: 1174.8344 - val_loss: 1267.1042 - val_mse: 18466980.0000 - val_mae: 1267.1042\n",
      "Epoch 296/500\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 1171.4591 - mse: 15044243.0000 - mae: 1171.4591 - val_loss: 1267.0790 - val_mse: 18466458.0000 - val_mae: 1267.0790\n",
      "Epoch 297/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1174.7218 - mse: 15038215.0000 - mae: 1174.7218 - val_loss: 1267.1222 - val_mse: 18468004.0000 - val_mae: 1267.1222\n",
      "Epoch 298/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1174.2650 - mse: 15027027.0000 - mae: 1174.2650 - val_loss: 1267.1550 - val_mse: 18469240.0000 - val_mae: 1267.1550\n",
      "Epoch 299/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1173.8387 - mse: 15045760.0000 - mae: 1173.8387 - val_loss: 1267.2469 - val_mse: 18469082.0000 - val_mae: 1267.2469\n",
      "Epoch 300/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1175.1638 - mse: 15054643.0000 - mae: 1175.1637 - val_loss: 1267.1610 - val_mse: 18468742.0000 - val_mae: 1267.1610\n",
      "Epoch 301/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1175.4327 - mse: 15051272.0000 - mae: 1175.4326 - val_loss: 1267.1389 - val_mse: 18471362.0000 - val_mae: 1267.1389\n",
      "Epoch 302/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1175.6425 - mse: 15023690.0000 - mae: 1175.6425 - val_loss: 1267.2019 - val_mse: 18472404.0000 - val_mae: 1267.2019\n",
      "Epoch 303/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1171.3835 - mse: 15039647.0000 - mae: 1171.3835 - val_loss: 1267.2123 - val_mse: 18470654.0000 - val_mae: 1267.2123\n",
      "Epoch 304/500\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 1175.3848 - mse: 15048892.0000 - mae: 1175.3849 - val_loss: 1267.1093 - val_mse: 18470406.0000 - val_mae: 1267.1093\n",
      "Epoch 305/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1176.7365 - mse: 15049576.0000 - mae: 1176.7365 - val_loss: 1267.1569 - val_mse: 18472046.0000 - val_mae: 1267.1569\n",
      "Epoch 306/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1173.2845 - mse: 15054578.0000 - mae: 1173.2845 - val_loss: 1267.2015 - val_mse: 18471924.0000 - val_mae: 1267.2015\n",
      "Epoch 307/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1174.4433 - mse: 15039684.0000 - mae: 1174.4432 - val_loss: 1267.1105 - val_mse: 18471784.0000 - val_mae: 1267.1105\n",
      "Epoch 308/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1175.2081 - mse: 15030600.0000 - mae: 1175.2080 - val_loss: 1267.1104 - val_mse: 18470348.0000 - val_mae: 1267.1104\n",
      "Epoch 309/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1172.9877 - mse: 15036388.0000 - mae: 1172.9878 - val_loss: 1267.0940 - val_mse: 18470798.0000 - val_mae: 1267.0940\n",
      "Epoch 310/500\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 1176.3395 - mse: 15036189.0000 - mae: 1176.3395 - val_loss: 1267.1566 - val_mse: 18473780.0000 - val_mae: 1267.1566\n",
      "Epoch 311/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1173.3528 - mse: 15041120.0000 - mae: 1173.3528 - val_loss: 1267.1069 - val_mse: 18474140.0000 - val_mae: 1267.1069\n",
      "Epoch 312/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1173.6151 - mse: 15048836.0000 - mae: 1173.6151 - val_loss: 1267.0398 - val_mse: 18473946.0000 - val_mae: 1267.0398\n",
      "Epoch 313/500\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 1172.1051 - mse: 15047997.0000 - mae: 1172.1051 - val_loss: 1267.0040 - val_mse: 18472534.0000 - val_mae: 1267.0040\n",
      "Epoch 314/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1172.4848 - mse: 15016798.0000 - mae: 1172.4847 - val_loss: 1267.1069 - val_mse: 18473670.0000 - val_mae: 1267.1069\n",
      "Epoch 315/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1172.3919 - mse: 15053160.0000 - mae: 1172.3918 - val_loss: 1267.0221 - val_mse: 18472852.0000 - val_mae: 1267.0221\n",
      "Epoch 316/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1173.7454 - mse: 15028455.0000 - mae: 1173.7455 - val_loss: 1267.0972 - val_mse: 18474244.0000 - val_mae: 1267.0972\n",
      "Epoch 317/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1173.4713 - mse: 15035351.0000 - mae: 1173.4712 - val_loss: 1267.1587 - val_mse: 18474144.0000 - val_mae: 1267.1587\n",
      "Epoch 318/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1173.3658 - mse: 15015383.0000 - mae: 1173.3657 - val_loss: 1266.9919 - val_mse: 18474192.0000 - val_mae: 1266.9919\n",
      "Epoch 319/500\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 1176.5026 - mse: 15036346.0000 - mae: 1176.5027 - val_loss: 1267.0640 - val_mse: 18474860.0000 - val_mae: 1267.0640\n",
      "Epoch 320/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1173.8221 - mse: 15061791.0000 - mae: 1173.8220 - val_loss: 1267.0455 - val_mse: 18474138.0000 - val_mae: 1267.0455\n",
      "Epoch 321/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1170.7205 - mse: 15017429.0000 - mae: 1170.7205 - val_loss: 1266.9723 - val_mse: 18472374.0000 - val_mae: 1266.9723\n",
      "Epoch 322/500\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 1171.4924 - mse: 15032486.0000 - mae: 1171.4926 - val_loss: 1267.0126 - val_mse: 18472784.0000 - val_mae: 1267.0126\n",
      "Epoch 323/500\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 1175.1290 - mse: 15046543.0000 - mae: 1175.1290 - val_loss: 1267.0509 - val_mse: 18472474.0000 - val_mae: 1267.0509\n",
      "Epoch 324/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1171.5499 - mse: 15037417.0000 - mae: 1171.5499 - val_loss: 1267.0421 - val_mse: 18472362.0000 - val_mae: 1267.0421\n",
      "Epoch 325/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1174.3540 - mse: 15046589.0000 - mae: 1174.3540 - val_loss: 1266.9952 - val_mse: 18472526.0000 - val_mae: 1266.9952\n",
      "Epoch 326/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1175.8270 - mse: 15038416.0000 - mae: 1175.8270 - val_loss: 1267.0212 - val_mse: 18472266.0000 - val_mae: 1267.0212\n",
      "Epoch 327/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1178.0049 - mse: 15053830.0000 - mae: 1178.0049 - val_loss: 1267.0333 - val_mse: 18475008.0000 - val_mae: 1267.0333\n",
      "Epoch 328/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1174.4263 - mse: 15032330.0000 - mae: 1174.4263 - val_loss: 1267.0573 - val_mse: 18475882.0000 - val_mae: 1267.0573\n",
      "Epoch 329/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1176.2680 - mse: 15047154.0000 - mae: 1176.2679 - val_loss: 1266.9939 - val_mse: 18474796.0000 - val_mae: 1266.9939\n",
      "Epoch 330/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1172.3903 - mse: 15028615.0000 - mae: 1172.3903 - val_loss: 1266.9843 - val_mse: 18474150.0000 - val_mae: 1266.9843\n",
      "Epoch 331/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1173.9098 - mse: 15041417.0000 - mae: 1173.9098 - val_loss: 1266.9821 - val_mse: 18473794.0000 - val_mae: 1266.9821\n",
      "Epoch 332/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1172.2917 - mse: 15041741.0000 - mae: 1172.2917 - val_loss: 1266.9792 - val_mse: 18473394.0000 - val_mae: 1266.9792\n",
      "Epoch 333/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1174.2182 - mse: 15050770.0000 - mae: 1174.2183 - val_loss: 1266.9556 - val_mse: 18472092.0000 - val_mae: 1266.9556\n",
      "Epoch 334/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1174.0626 - mse: 15042697.0000 - mae: 1174.0625 - val_loss: 1266.9233 - val_mse: 18472988.0000 - val_mae: 1266.9233\n",
      "Epoch 335/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1173.6973 - mse: 15046608.0000 - mae: 1173.6974 - val_loss: 1267.0524 - val_mse: 18473102.0000 - val_mae: 1267.0524\n",
      "Epoch 336/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1173.9100 - mse: 15036699.0000 - mae: 1173.9100 - val_loss: 1266.9963 - val_mse: 18472370.0000 - val_mae: 1266.9963\n",
      "Epoch 337/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1174.7971 - mse: 15043461.0000 - mae: 1174.7970 - val_loss: 1267.0052 - val_mse: 18474530.0000 - val_mae: 1267.0052\n",
      "Epoch 338/500\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 1170.6274 - mse: 15031201.0000 - mae: 1170.6274 - val_loss: 1267.0492 - val_mse: 18472788.0000 - val_mae: 1267.0492\n",
      "Epoch 339/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1174.6573 - mse: 15031065.0000 - mae: 1174.6572 - val_loss: 1266.9363 - val_mse: 18472194.0000 - val_mae: 1266.9363\n",
      "Epoch 340/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1172.4059 - mse: 15039240.0000 - mae: 1172.4059 - val_loss: 1266.9978 - val_mse: 18472240.0000 - val_mae: 1266.9978\n",
      "Epoch 341/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1173.9063 - mse: 15036580.0000 - mae: 1173.9062 - val_loss: 1266.9399 - val_mse: 18472418.0000 - val_mae: 1266.9399\n",
      "Epoch 342/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1171.8521 - mse: 15003652.0000 - mae: 1171.8521 - val_loss: 1266.9971 - val_mse: 18470956.0000 - val_mae: 1266.9971\n",
      "Epoch 343/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1173.6225 - mse: 15038079.0000 - mae: 1173.6226 - val_loss: 1266.8386 - val_mse: 18470164.0000 - val_mae: 1266.8386\n",
      "Epoch 344/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1175.1989 - mse: 15031468.0000 - mae: 1175.1989 - val_loss: 1267.0166 - val_mse: 18471492.0000 - val_mae: 1267.0166\n",
      "Epoch 345/500\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 1172.3295 - mse: 15049133.0000 - mae: 1172.3295 - val_loss: 1266.8885 - val_mse: 18469576.0000 - val_mae: 1266.8885\n",
      "Epoch 346/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1174.5471 - mse: 15021695.0000 - mae: 1174.5470 - val_loss: 1267.0212 - val_mse: 18472052.0000 - val_mae: 1267.0212\n",
      "Epoch 347/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1173.5486 - mse: 15036804.0000 - mae: 1173.5486 - val_loss: 1267.0061 - val_mse: 18469792.0000 - val_mae: 1267.0061\n",
      "Epoch 348/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1173.1504 - mse: 15046806.0000 - mae: 1173.1504 - val_loss: 1266.8523 - val_mse: 18469664.0000 - val_mae: 1266.8523\n",
      "Epoch 349/500\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 1174.0806 - mse: 15037921.0000 - mae: 1174.0804 - val_loss: 1266.9778 - val_mse: 18469360.0000 - val_mae: 1266.9778\n",
      "Epoch 350/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1172.8384 - mse: 15045024.0000 - mae: 1172.8384 - val_loss: 1266.9965 - val_mse: 18469848.0000 - val_mae: 1266.9965\n",
      "Epoch 351/500\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 1173.0528 - mse: 15028206.0000 - mae: 1173.0527 - val_loss: 1267.0439 - val_mse: 18471254.0000 - val_mae: 1267.0439\n",
      "Epoch 352/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1170.2079 - mse: 15003333.0000 - mae: 1170.2080 - val_loss: 1266.9467 - val_mse: 18468644.0000 - val_mae: 1266.9467\n",
      "Epoch 353/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1174.5251 - mse: 15057480.0000 - mae: 1174.5253 - val_loss: 1266.8514 - val_mse: 18468684.0000 - val_mae: 1266.8514\n",
      "Epoch 354/500\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 1175.6230 - mse: 15043357.0000 - mae: 1175.6230 - val_loss: 1266.9294 - val_mse: 18470636.0000 - val_mae: 1266.9294\n",
      "Epoch 355/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1170.4807 - mse: 15037086.0000 - mae: 1170.4807 - val_loss: 1266.7966 - val_mse: 18467532.0000 - val_mae: 1266.7966\n",
      "Epoch 356/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1173.6667 - mse: 15041479.0000 - mae: 1173.6667 - val_loss: 1266.8571 - val_mse: 18467706.0000 - val_mae: 1266.8571\n",
      "Epoch 357/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1174.3083 - mse: 15034548.0000 - mae: 1174.3083 - val_loss: 1266.8196 - val_mse: 18467844.0000 - val_mae: 1266.8196\n",
      "Epoch 358/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1171.7446 - mse: 15024078.0000 - mae: 1171.7445 - val_loss: 1266.7783 - val_mse: 18468778.0000 - val_mae: 1266.7783\n",
      "Epoch 359/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1171.5605 - mse: 15018482.0000 - mae: 1171.5605 - val_loss: 1266.8250 - val_mse: 18468664.0000 - val_mae: 1266.8250\n",
      "Epoch 360/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1172.8124 - mse: 15021906.0000 - mae: 1172.8125 - val_loss: 1266.7500 - val_mse: 18467852.0000 - val_mae: 1266.7500\n",
      "Epoch 361/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1174.4119 - mse: 15046748.0000 - mae: 1174.4120 - val_loss: 1266.7935 - val_mse: 18469532.0000 - val_mae: 1266.7935\n",
      "Epoch 362/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1173.4231 - mse: 15035740.0000 - mae: 1173.4231 - val_loss: 1266.7545 - val_mse: 18469652.0000 - val_mae: 1266.7545\n",
      "Epoch 363/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1175.1111 - mse: 15057093.0000 - mae: 1175.1110 - val_loss: 1266.7704 - val_mse: 18470450.0000 - val_mae: 1266.7704\n",
      "Epoch 364/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1170.9088 - mse: 15022680.0000 - mae: 1170.9087 - val_loss: 1266.7241 - val_mse: 18467452.0000 - val_mae: 1266.7241\n",
      "Epoch 365/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1172.8401 - mse: 15046387.0000 - mae: 1172.8401 - val_loss: 1266.7399 - val_mse: 18468290.0000 - val_mae: 1266.7399\n",
      "Epoch 366/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1174.0075 - mse: 15042120.0000 - mae: 1174.0074 - val_loss: 1266.7445 - val_mse: 18469378.0000 - val_mae: 1266.7445\n",
      "Epoch 367/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1170.4027 - mse: 15016849.0000 - mae: 1170.4027 - val_loss: 1266.7831 - val_mse: 18469402.0000 - val_mae: 1266.7831\n",
      "Epoch 368/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1172.7202 - mse: 15027306.0000 - mae: 1172.7202 - val_loss: 1266.7423 - val_mse: 18467804.0000 - val_mae: 1266.7423\n",
      "Epoch 369/500\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 1173.1298 - mse: 15026199.0000 - mae: 1173.1298 - val_loss: 1266.8413 - val_mse: 18470878.0000 - val_mae: 1266.8413\n",
      "Epoch 370/500\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 1173.2278 - mse: 15041206.0000 - mae: 1173.2278 - val_loss: 1266.8146 - val_mse: 18471380.0000 - val_mae: 1266.8146\n",
      "Epoch 371/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1174.9235 - mse: 15025299.0000 - mae: 1174.9235 - val_loss: 1266.9408 - val_mse: 18472096.0000 - val_mae: 1266.9408\n",
      "Epoch 372/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1172.8365 - mse: 15052952.0000 - mae: 1172.8365 - val_loss: 1266.7777 - val_mse: 18470322.0000 - val_mae: 1266.7777\n",
      "Epoch 373/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1171.1024 - mse: 15052919.0000 - mae: 1171.1025 - val_loss: 1266.7936 - val_mse: 18469374.0000 - val_mae: 1266.7936\n",
      "Epoch 374/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1169.9304 - mse: 15026492.0000 - mae: 1169.9305 - val_loss: 1266.7605 - val_mse: 18466822.0000 - val_mae: 1266.7605\n",
      "Epoch 375/500\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 1172.6102 - mse: 15049417.0000 - mae: 1172.6102 - val_loss: 1266.7316 - val_mse: 18466274.0000 - val_mae: 1266.7316\n",
      "Epoch 376/500\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 1168.8863 - mse: 15022211.0000 - mae: 1168.8862 - val_loss: 1266.6394 - val_mse: 18464646.0000 - val_mae: 1266.6394\n",
      "Epoch 377/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1174.2661 - mse: 15045539.0000 - mae: 1174.2661 - val_loss: 1266.7144 - val_mse: 18466118.0000 - val_mae: 1266.7144\n",
      "Epoch 378/500\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 1172.5475 - mse: 15013065.0000 - mae: 1172.5475 - val_loss: 1266.7581 - val_mse: 18465880.0000 - val_mae: 1266.7581\n",
      "Epoch 379/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1172.5637 - mse: 15024555.0000 - mae: 1172.5637 - val_loss: 1266.7444 - val_mse: 18463544.0000 - val_mae: 1266.7444\n",
      "Epoch 380/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1172.5707 - mse: 15000576.0000 - mae: 1172.5708 - val_loss: 1266.7913 - val_mse: 18465000.0000 - val_mae: 1266.7913\n",
      "Epoch 381/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1170.5058 - mse: 15029560.0000 - mae: 1170.5057 - val_loss: 1266.8030 - val_mse: 18464638.0000 - val_mae: 1266.8030\n",
      "Epoch 382/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1174.4346 - mse: 15045280.0000 - mae: 1174.4344 - val_loss: 1266.7424 - val_mse: 18465722.0000 - val_mae: 1266.7424\n",
      "Epoch 383/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1173.2485 - mse: 15026418.0000 - mae: 1173.2485 - val_loss: 1266.7655 - val_mse: 18465792.0000 - val_mae: 1266.7655\n",
      "Epoch 384/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1173.0627 - mse: 15016601.0000 - mae: 1173.0627 - val_loss: 1266.7402 - val_mse: 18464762.0000 - val_mae: 1266.7402\n",
      "Epoch 385/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1171.9291 - mse: 15034227.0000 - mae: 1171.9290 - val_loss: 1266.7966 - val_mse: 18465538.0000 - val_mae: 1266.7966\n",
      "Epoch 386/500\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 1173.1320 - mse: 15059608.0000 - mae: 1173.1320 - val_loss: 1266.8030 - val_mse: 18466054.0000 - val_mae: 1266.8030\n",
      "Epoch 387/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1173.5357 - mse: 15028490.0000 - mae: 1173.5358 - val_loss: 1266.7300 - val_mse: 18466428.0000 - val_mae: 1266.7300\n",
      "Epoch 388/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1172.7346 - mse: 15023241.0000 - mae: 1172.7346 - val_loss: 1266.6929 - val_mse: 18465776.0000 - val_mae: 1266.6929\n",
      "Epoch 389/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1173.9963 - mse: 15038769.0000 - mae: 1173.9963 - val_loss: 1266.7678 - val_mse: 18466426.0000 - val_mae: 1266.7678\n",
      "Epoch 390/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1173.9780 - mse: 15039363.0000 - mae: 1173.9780 - val_loss: 1266.6996 - val_mse: 18467250.0000 - val_mae: 1266.6996\n",
      "Epoch 391/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1171.8843 - mse: 15016632.0000 - mae: 1171.8843 - val_loss: 1266.6741 - val_mse: 18466274.0000 - val_mae: 1266.6741\n",
      "Epoch 392/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1174.0916 - mse: 15014056.0000 - mae: 1174.0917 - val_loss: 1266.6643 - val_mse: 18466140.0000 - val_mae: 1266.6643\n",
      "Epoch 393/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1170.8976 - mse: 15039821.0000 - mae: 1170.8976 - val_loss: 1266.6371 - val_mse: 18466722.0000 - val_mae: 1266.6371\n",
      "Epoch 394/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1175.0568 - mse: 15041939.0000 - mae: 1175.0569 - val_loss: 1266.5925 - val_mse: 18468178.0000 - val_mae: 1266.5925\n",
      "Epoch 395/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1173.2436 - mse: 15040025.0000 - mae: 1173.2437 - val_loss: 1266.7499 - val_mse: 18468436.0000 - val_mae: 1266.7499\n",
      "Epoch 396/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1172.7949 - mse: 15037278.0000 - mae: 1172.7949 - val_loss: 1266.8031 - val_mse: 18469004.0000 - val_mae: 1266.8031\n",
      "Epoch 397/500\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 1172.1134 - mse: 15028816.0000 - mae: 1172.1135 - val_loss: 1266.6716 - val_mse: 18468416.0000 - val_mae: 1266.6716\n",
      "Epoch 398/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1174.2680 - mse: 15030145.0000 - mae: 1174.2679 - val_loss: 1266.6033 - val_mse: 18467634.0000 - val_mae: 1266.6033\n",
      "Epoch 399/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1173.1335 - mse: 15030456.0000 - mae: 1173.1335 - val_loss: 1266.7744 - val_mse: 18469270.0000 - val_mae: 1266.7744\n",
      "Epoch 400/500\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 1173.5842 - mse: 15036189.0000 - mae: 1173.5841 - val_loss: 1266.6752 - val_mse: 18467028.0000 - val_mae: 1266.6752\n",
      "Epoch 401/500\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 1172.1758 - mse: 15025231.0000 - mae: 1172.1758 - val_loss: 1266.6394 - val_mse: 18468034.0000 - val_mae: 1266.6394\n",
      "Epoch 402/500\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 1173.7450 - mse: 15041631.0000 - mae: 1173.7450 - val_loss: 1266.6615 - val_mse: 18470124.0000 - val_mae: 1266.6615\n",
      "Epoch 403/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1175.5067 - mse: 15033356.0000 - mae: 1175.5067 - val_loss: 1266.6265 - val_mse: 18469598.0000 - val_mae: 1266.6265\n",
      "Epoch 404/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1171.9769 - mse: 15029404.0000 - mae: 1171.9769 - val_loss: 1266.6519 - val_mse: 18469806.0000 - val_mae: 1266.6519\n",
      "Epoch 405/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1173.1409 - mse: 15032016.0000 - mae: 1173.1409 - val_loss: 1266.6265 - val_mse: 18469216.0000 - val_mae: 1266.6265\n",
      "Epoch 406/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1172.8463 - mse: 15038747.0000 - mae: 1172.8462 - val_loss: 1266.7538 - val_mse: 18471052.0000 - val_mae: 1266.7538\n",
      "Epoch 407/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1173.0162 - mse: 15020548.0000 - mae: 1173.0161 - val_loss: 1266.7050 - val_mse: 18469934.0000 - val_mae: 1266.7050\n",
      "Epoch 408/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1175.8873 - mse: 15057648.0000 - mae: 1175.8872 - val_loss: 1266.6429 - val_mse: 18469990.0000 - val_mae: 1266.6429\n",
      "Epoch 409/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1171.6049 - mse: 15061695.0000 - mae: 1171.6049 - val_loss: 1266.6897 - val_mse: 18470574.0000 - val_mae: 1266.6897\n",
      "Epoch 410/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1174.7775 - mse: 15025443.0000 - mae: 1174.7775 - val_loss: 1266.7050 - val_mse: 18471762.0000 - val_mae: 1266.7050\n",
      "Epoch 411/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1173.3102 - mse: 15028808.0000 - mae: 1173.3103 - val_loss: 1266.6443 - val_mse: 18470512.0000 - val_mae: 1266.6443\n",
      "Epoch 412/500\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 1174.8729 - mse: 15069202.0000 - mae: 1174.8729 - val_loss: 1266.6294 - val_mse: 18470902.0000 - val_mae: 1266.6294\n",
      "Epoch 413/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1173.8499 - mse: 15018922.0000 - mae: 1173.8500 - val_loss: 1266.7656 - val_mse: 18471004.0000 - val_mae: 1266.7656\n",
      "Epoch 414/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1172.9684 - mse: 15042648.0000 - mae: 1172.9684 - val_loss: 1266.6836 - val_mse: 18471690.0000 - val_mae: 1266.6836\n",
      "Epoch 415/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1174.1882 - mse: 15030927.0000 - mae: 1174.1882 - val_loss: 1266.5879 - val_mse: 18470172.0000 - val_mae: 1266.5879\n",
      "Epoch 416/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1173.8641 - mse: 15035451.0000 - mae: 1173.8641 - val_loss: 1266.6655 - val_mse: 18470198.0000 - val_mae: 1266.6655\n",
      "Epoch 417/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1172.9875 - mse: 15045443.0000 - mae: 1172.9875 - val_loss: 1266.6244 - val_mse: 18470348.0000 - val_mae: 1266.6244\n",
      "Epoch 418/500\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 1174.6152 - mse: 15038380.0000 - mae: 1174.6152 - val_loss: 1266.5371 - val_mse: 18468758.0000 - val_mae: 1266.5371\n",
      "Epoch 419/500\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 1169.6619 - mse: 15013249.0000 - mae: 1169.6620 - val_loss: 1266.5432 - val_mse: 18467196.0000 - val_mae: 1266.5432\n",
      "Epoch 420/500\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 1170.5456 - mse: 15009700.0000 - mae: 1170.5455 - val_loss: 1266.6121 - val_mse: 18466516.0000 - val_mae: 1266.6121\n",
      "Epoch 421/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1172.5751 - mse: 15036682.0000 - mae: 1172.5752 - val_loss: 1266.7028 - val_mse: 18467480.0000 - val_mae: 1266.7028\n",
      "Epoch 422/500\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 1175.9464 - mse: 15050791.0000 - mae: 1175.9464 - val_loss: 1266.7311 - val_mse: 18468870.0000 - val_mae: 1266.7311\n",
      "Epoch 423/500\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 1173.7574 - mse: 15036941.0000 - mae: 1173.7574 - val_loss: 1266.7191 - val_mse: 18467808.0000 - val_mae: 1266.7191\n",
      "Epoch 424/500\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 1169.4401 - mse: 15019688.0000 - mae: 1169.4402 - val_loss: 1266.6219 - val_mse: 18466224.0000 - val_mae: 1266.6219\n",
      "Epoch 425/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1176.7087 - mse: 15036049.0000 - mae: 1176.7087 - val_loss: 1266.6003 - val_mse: 18467772.0000 - val_mae: 1266.6003\n",
      "Epoch 426/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1172.5819 - mse: 15029862.0000 - mae: 1172.5819 - val_loss: 1266.6777 - val_mse: 18468080.0000 - val_mae: 1266.6777\n",
      "Epoch 427/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1172.9327 - mse: 15001478.0000 - mae: 1172.9327 - val_loss: 1266.6880 - val_mse: 18466364.0000 - val_mae: 1266.6880\n",
      "Epoch 428/500\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 1174.8446 - mse: 15033190.0000 - mae: 1174.8446 - val_loss: 1266.6630 - val_mse: 18466590.0000 - val_mae: 1266.6630\n",
      "Epoch 429/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1174.6191 - mse: 15037766.0000 - mae: 1174.6191 - val_loss: 1266.6558 - val_mse: 18467266.0000 - val_mae: 1266.6558\n",
      "Epoch 430/500\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 1172.1658 - mse: 15041486.0000 - mae: 1172.1658 - val_loss: 1266.7122 - val_mse: 18467446.0000 - val_mae: 1266.7122\n",
      "Epoch 431/500\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 1173.0063 - mse: 15037225.0000 - mae: 1173.0062 - val_loss: 1266.6940 - val_mse: 18466900.0000 - val_mae: 1266.6940\n",
      "Epoch 432/500\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 1173.0706 - mse: 15033377.0000 - mae: 1173.0707 - val_loss: 1266.6882 - val_mse: 18466838.0000 - val_mae: 1266.6882\n",
      "Epoch 433/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1173.5460 - mse: 15021492.0000 - mae: 1173.5460 - val_loss: 1266.7076 - val_mse: 18465840.0000 - val_mae: 1266.7076\n",
      "Epoch 434/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1172.8884 - mse: 15025056.0000 - mae: 1172.8885 - val_loss: 1266.7843 - val_mse: 18465554.0000 - val_mae: 1266.7843\n",
      "Epoch 435/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1175.8225 - mse: 15035458.0000 - mae: 1175.8225 - val_loss: 1266.7134 - val_mse: 18465074.0000 - val_mae: 1266.7134\n",
      "Epoch 436/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1172.4930 - mse: 15015667.0000 - mae: 1172.4930 - val_loss: 1266.7249 - val_mse: 18465084.0000 - val_mae: 1266.7249\n",
      "Epoch 437/500\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 1173.2242 - mse: 15031656.0000 - mae: 1173.2242 - val_loss: 1266.6459 - val_mse: 18465654.0000 - val_mae: 1266.6459\n",
      "Epoch 438/500\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 1174.1638 - mse: 15025455.0000 - mae: 1174.1637 - val_loss: 1266.6035 - val_mse: 18464244.0000 - val_mae: 1266.6035\n",
      "Epoch 439/500\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 1174.3568 - mse: 15012020.0000 - mae: 1174.3567 - val_loss: 1266.6167 - val_mse: 18465434.0000 - val_mae: 1266.6167\n",
      "Epoch 440/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1171.3381 - mse: 15012536.0000 - mae: 1171.3381 - val_loss: 1266.6660 - val_mse: 18465004.0000 - val_mae: 1266.6660\n",
      "Epoch 441/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1173.1797 - mse: 15028478.0000 - mae: 1173.1798 - val_loss: 1266.6813 - val_mse: 18464256.0000 - val_mae: 1266.6813\n",
      "Epoch 442/500\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 1174.1143 - mse: 15036969.0000 - mae: 1174.1143 - val_loss: 1266.6571 - val_mse: 18465354.0000 - val_mae: 1266.6571\n",
      "Epoch 443/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1172.7026 - mse: 15017583.0000 - mae: 1172.7026 - val_loss: 1266.7250 - val_mse: 18465672.0000 - val_mae: 1266.7250\n",
      "Epoch 444/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1171.9958 - mse: 15035488.0000 - mae: 1171.9958 - val_loss: 1266.6124 - val_mse: 18464584.0000 - val_mae: 1266.6124\n",
      "Epoch 445/500\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 1171.9701 - mse: 15018988.0000 - mae: 1171.9700 - val_loss: 1266.5918 - val_mse: 18464084.0000 - val_mae: 1266.5918\n",
      "Epoch 446/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1172.5134 - mse: 15017980.0000 - mae: 1172.5135 - val_loss: 1266.5927 - val_mse: 18465460.0000 - val_mae: 1266.5927\n",
      "Epoch 447/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1173.6422 - mse: 15034839.0000 - mae: 1173.6422 - val_loss: 1266.5990 - val_mse: 18465114.0000 - val_mae: 1266.5990\n",
      "Epoch 448/500\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 1171.0099 - mse: 15011465.0000 - mae: 1171.0100 - val_loss: 1266.5818 - val_mse: 18463760.0000 - val_mae: 1266.5818\n",
      "Epoch 449/500\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 1173.0319 - mse: 15042916.0000 - mae: 1173.0320 - val_loss: 1266.5935 - val_mse: 18465090.0000 - val_mae: 1266.5935\n",
      "Epoch 450/500\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 1171.5588 - mse: 15019391.0000 - mae: 1171.5587 - val_loss: 1266.6349 - val_mse: 18463854.0000 - val_mae: 1266.6349\n",
      "Epoch 451/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1173.1785 - mse: 15033496.0000 - mae: 1173.1785 - val_loss: 1266.5891 - val_mse: 18463814.0000 - val_mae: 1266.5891\n",
      "Epoch 452/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1173.8253 - mse: 15028521.0000 - mae: 1173.8252 - val_loss: 1266.5557 - val_mse: 18464194.0000 - val_mae: 1266.5557\n",
      "Epoch 453/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1174.0520 - mse: 15041372.0000 - mae: 1174.0520 - val_loss: 1266.4860 - val_mse: 18464360.0000 - val_mae: 1266.4860\n",
      "Epoch 454/500\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 1171.8877 - mse: 15037530.0000 - mae: 1171.8877 - val_loss: 1266.5148 - val_mse: 18463392.0000 - val_mae: 1266.5148\n",
      "Epoch 455/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1171.6859 - mse: 15044263.0000 - mae: 1171.6859 - val_loss: 1266.4362 - val_mse: 18461898.0000 - val_mae: 1266.4362\n",
      "Epoch 456/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1174.2542 - mse: 15015953.0000 - mae: 1174.2543 - val_loss: 1266.4540 - val_mse: 18463024.0000 - val_mae: 1266.4540\n",
      "Epoch 457/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1174.3063 - mse: 15046130.0000 - mae: 1174.3063 - val_loss: 1266.4293 - val_mse: 18463192.0000 - val_mae: 1266.4293\n",
      "Epoch 458/500\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 1172.6996 - mse: 15033361.0000 - mae: 1172.6997 - val_loss: 1266.5020 - val_mse: 18464376.0000 - val_mae: 1266.5020\n",
      "Epoch 459/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1171.5491 - mse: 15028232.0000 - mae: 1171.5492 - val_loss: 1266.5216 - val_mse: 18462278.0000 - val_mae: 1266.5216\n",
      "Epoch 460/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1171.8970 - mse: 15013259.0000 - mae: 1171.8970 - val_loss: 1266.5609 - val_mse: 18464548.0000 - val_mae: 1266.5609\n",
      "Epoch 461/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1173.8877 - mse: 15027552.0000 - mae: 1173.8877 - val_loss: 1266.5046 - val_mse: 18464244.0000 - val_mae: 1266.5046\n",
      "Epoch 462/500\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 1175.2531 - mse: 15039931.0000 - mae: 1175.2532 - val_loss: 1266.5815 - val_mse: 18465980.0000 - val_mae: 1266.5815\n",
      "Epoch 463/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1171.3383 - mse: 15040356.0000 - mae: 1171.3383 - val_loss: 1266.5455 - val_mse: 18466786.0000 - val_mae: 1266.5455\n",
      "Epoch 464/500\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 1174.9518 - mse: 15020041.0000 - mae: 1174.9519 - val_loss: 1266.5764 - val_mse: 18467690.0000 - val_mae: 1266.5764\n",
      "Epoch 465/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1172.0590 - mse: 15004746.0000 - mae: 1172.0590 - val_loss: 1266.6127 - val_mse: 18466816.0000 - val_mae: 1266.6127\n",
      "Epoch 466/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1171.9865 - mse: 15036642.0000 - mae: 1171.9865 - val_loss: 1266.5901 - val_mse: 18466998.0000 - val_mae: 1266.5901\n",
      "Epoch 467/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1173.3016 - mse: 15027145.0000 - mae: 1173.3016 - val_loss: 1266.6036 - val_mse: 18466474.0000 - val_mae: 1266.6036\n",
      "Epoch 468/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1172.7162 - mse: 15025150.0000 - mae: 1172.7163 - val_loss: 1266.5886 - val_mse: 18465804.0000 - val_mae: 1266.5886\n",
      "Epoch 469/500\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 1171.4734 - mse: 15046286.0000 - mae: 1171.4734 - val_loss: 1266.5454 - val_mse: 18464336.0000 - val_mae: 1266.5454\n",
      "Epoch 470/500\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 1172.3120 - mse: 15046445.0000 - mae: 1172.3120 - val_loss: 1266.5785 - val_mse: 18463678.0000 - val_mae: 1266.5785\n",
      "Epoch 471/500\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 1172.3932 - mse: 15023436.0000 - mae: 1172.3931 - val_loss: 1266.6348 - val_mse: 18464738.0000 - val_mae: 1266.6348\n",
      "Epoch 472/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1171.2432 - mse: 15034057.0000 - mae: 1171.2432 - val_loss: 1266.6672 - val_mse: 18465058.0000 - val_mae: 1266.6672\n",
      "Epoch 473/500\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 1175.0804 - mse: 15051366.0000 - mae: 1175.0802 - val_loss: 1266.6532 - val_mse: 18465412.0000 - val_mae: 1266.6532\n",
      "Epoch 474/500\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 1175.4492 - mse: 15047859.0000 - mae: 1175.4491 - val_loss: 1266.5713 - val_mse: 18465630.0000 - val_mae: 1266.5713\n",
      "Epoch 475/500\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 1172.4005 - mse: 15023823.0000 - mae: 1172.4005 - val_loss: 1266.5524 - val_mse: 18463638.0000 - val_mae: 1266.5524\n",
      "Epoch 476/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1171.9381 - mse: 15017445.0000 - mae: 1171.9380 - val_loss: 1266.6128 - val_mse: 18464180.0000 - val_mae: 1266.6128\n",
      "Epoch 477/500\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 1175.7938 - mse: 15069192.0000 - mae: 1175.7937 - val_loss: 1266.6028 - val_mse: 18465352.0000 - val_mae: 1266.6028\n",
      "Epoch 478/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1173.2789 - mse: 15010996.0000 - mae: 1173.2791 - val_loss: 1266.6035 - val_mse: 18464774.0000 - val_mae: 1266.6035\n",
      "Epoch 479/500\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 1174.0181 - mse: 15036236.0000 - mae: 1174.0179 - val_loss: 1266.6122 - val_mse: 18465458.0000 - val_mae: 1266.6122\n",
      "Epoch 480/500\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 1174.7642 - mse: 15051204.0000 - mae: 1174.7643 - val_loss: 1266.5718 - val_mse: 18465872.0000 - val_mae: 1266.5718\n",
      "Epoch 481/500\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 1173.8111 - mse: 15036121.0000 - mae: 1173.8112 - val_loss: 1266.6544 - val_mse: 18465914.0000 - val_mae: 1266.6544\n",
      "Epoch 482/500\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 1173.4788 - mse: 15042652.0000 - mae: 1173.4788 - val_loss: 1266.5562 - val_mse: 18465928.0000 - val_mae: 1266.5562\n",
      "Epoch 483/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1172.7696 - mse: 15048762.0000 - mae: 1172.7695 - val_loss: 1266.5403 - val_mse: 18464462.0000 - val_mae: 1266.5403\n",
      "Epoch 484/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1171.9535 - mse: 15049216.0000 - mae: 1171.9535 - val_loss: 1266.4757 - val_mse: 18462246.0000 - val_mae: 1266.4757\n",
      "Epoch 485/500\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 1173.3640 - mse: 15016529.0000 - mae: 1173.3640 - val_loss: 1266.4708 - val_mse: 18464846.0000 - val_mae: 1266.4708\n",
      "Epoch 486/500\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1173.3599 - mse: 15040758.0000 - mae: 1173.3597 - val_loss: 1266.4968 - val_mse: 18465122.0000 - val_mae: 1266.4968\n",
      "Epoch 487/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1170.9274 - mse: 15011657.0000 - mae: 1170.9274 - val_loss: 1266.4958 - val_mse: 18463256.0000 - val_mae: 1266.4958\n",
      "Epoch 488/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1171.3018 - mse: 15020883.0000 - mae: 1171.3018 - val_loss: 1266.5619 - val_mse: 18463962.0000 - val_mae: 1266.5619\n",
      "Epoch 489/500\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 1170.8758 - mse: 15021603.0000 - mae: 1170.8757 - val_loss: 1266.5446 - val_mse: 18463580.0000 - val_mae: 1266.5446\n",
      "Epoch 490/500\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 1175.0710 - mse: 15007471.0000 - mae: 1175.0710 - val_loss: 1266.5090 - val_mse: 18464406.0000 - val_mae: 1266.5090\n",
      "Epoch 491/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1174.7182 - mse: 15041386.0000 - mae: 1174.7183 - val_loss: 1266.5010 - val_mse: 18465174.0000 - val_mae: 1266.5010\n",
      "Epoch 492/500\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 1173.9380 - mse: 15026151.0000 - mae: 1173.9380 - val_loss: 1266.4750 - val_mse: 18464118.0000 - val_mae: 1266.4750\n",
      "Epoch 493/500\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 1169.9133 - mse: 15018104.0000 - mae: 1169.9132 - val_loss: 1266.4979 - val_mse: 18461730.0000 - val_mae: 1266.4979\n",
      "Epoch 494/500\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 1172.2517 - mse: 15026495.0000 - mae: 1172.2517 - val_loss: 1266.5399 - val_mse: 18463088.0000 - val_mae: 1266.5399\n",
      "Epoch 495/500\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 1173.8413 - mse: 15009476.0000 - mae: 1173.8413 - val_loss: 1266.6061 - val_mse: 18464860.0000 - val_mae: 1266.6061\n",
      "Epoch 496/500\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 1175.1723 - mse: 15031833.0000 - mae: 1175.1722 - val_loss: 1266.5034 - val_mse: 18466238.0000 - val_mae: 1266.5034\n",
      "Epoch 497/500\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 1172.5117 - mse: 15027661.0000 - mae: 1172.5116 - val_loss: 1266.4794 - val_mse: 18465728.0000 - val_mae: 1266.4794\n",
      "Epoch 498/500\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 1171.0264 - mse: 15022750.0000 - mae: 1171.0265 - val_loss: 1266.4285 - val_mse: 18464426.0000 - val_mae: 1266.4285\n",
      "Epoch 499/500\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 1173.5640 - mse: 15048036.0000 - mae: 1173.5640 - val_loss: 1266.4517 - val_mse: 18465230.0000 - val_mae: 1266.4517\n",
      "Epoch 500/500\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 1173.2091 - mse: 15030381.0000 - mae: 1173.2091 - val_loss: 1266.5184 - val_mse: 18465302.0000 - val_mae: 1266.5184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 500 samples\n",
      "Epoch 1/500\n",
      "2500/2500 [==============================] - 2s 837us/step - loss: 1308.6143 - mse: 17030106.0000 - mae: 1308.6143 - val_loss: 1448.6058 - val_mse: 17109320.0000 - val_mae: 1448.6058\n",
      "Epoch 2/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1308.5641 - mse: 17029972.0000 - mae: 1308.5640 - val_loss: 1448.5353 - val_mse: 17109108.0000 - val_mae: 1448.5353\n",
      "Epoch 3/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1308.4520 - mse: 17029678.0000 - mae: 1308.4520 - val_loss: 1447.9685 - val_mse: 17107722.0000 - val_mae: 1447.9685\n",
      "Epoch 4/500\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 1307.2401 - mse: 17026596.0000 - mae: 1307.2402 - val_loss: 1444.7220 - val_mse: 17098736.0000 - val_mae: 1444.7220\n",
      "Epoch 5/500\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 1302.8181 - mse: 17016120.0000 - mae: 1302.8180 - val_loss: 1440.5367 - val_mse: 17084012.0000 - val_mae: 1440.5367\n",
      "Epoch 6/500\n",
      "2500/2500 [==============================] - 0s 31us/step - loss: 1297.0378 - mse: 16997678.0000 - mae: 1297.0377 - val_loss: 1436.6211 - val_mse: 17065938.0000 - val_mae: 1436.6211\n",
      "Epoch 7/500\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 1290.1066 - mse: 16976826.0000 - mae: 1290.1064 - val_loss: 1432.3903 - val_mse: 17043400.0000 - val_mae: 1432.3903\n",
      "Epoch 8/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1283.0875 - mse: 16949488.0000 - mae: 1283.0874 - val_loss: 1429.3591 - val_mse: 17019882.0000 - val_mae: 1429.3591\n",
      "Epoch 9/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1276.3884 - mse: 16923678.0000 - mae: 1276.3884 - val_loss: 1427.3027 - val_mse: 16995310.0000 - val_mae: 1427.3027\n",
      "Epoch 10/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1270.4467 - mse: 16894424.0000 - mae: 1270.4467 - val_loss: 1426.1249 - val_mse: 16968332.0000 - val_mae: 1426.1249\n",
      "Epoch 11/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1265.6119 - mse: 16860274.0000 - mae: 1265.6121 - val_loss: 1425.1655 - val_mse: 16938600.0000 - val_mae: 1425.1655\n",
      "Epoch 12/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1260.8153 - mse: 16822760.0000 - mae: 1260.8152 - val_loss: 1424.3506 - val_mse: 16910066.0000 - val_mae: 1424.3506\n",
      "Epoch 13/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1256.9583 - mse: 16788726.0000 - mae: 1256.9583 - val_loss: 1423.6372 - val_mse: 16881382.0000 - val_mae: 1423.6372\n",
      "Epoch 14/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1253.6696 - mse: 16756744.0000 - mae: 1253.6696 - val_loss: 1423.1539 - val_mse: 16856422.0000 - val_mae: 1423.1539\n",
      "Epoch 15/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1251.2268 - mse: 16723159.0000 - mae: 1251.2268 - val_loss: 1422.6843 - val_mse: 16830776.0000 - val_mae: 1422.6843\n",
      "Epoch 16/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1250.5454 - mse: 16699646.0000 - mae: 1250.5454 - val_loss: 1422.0973 - val_mse: 16801542.0000 - val_mae: 1422.0973\n",
      "Epoch 17/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1246.3422 - mse: 16654010.0000 - mae: 1246.3422 - val_loss: 1421.5293 - val_mse: 16767461.0000 - val_mae: 1421.5293\n",
      "Epoch 18/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1244.4821 - mse: 16619068.0000 - mae: 1244.4821 - val_loss: 1421.3398 - val_mse: 16731676.0000 - val_mae: 1421.3398\n",
      "Epoch 19/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1243.9118 - mse: 16602022.0000 - mae: 1243.9117 - val_loss: 1421.2572 - val_mse: 16699165.0000 - val_mae: 1421.2572\n",
      "Epoch 20/500\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 1242.5779 - mse: 16561224.0000 - mae: 1242.5779 - val_loss: 1420.9412 - val_mse: 16676224.0000 - val_mae: 1420.9412\n",
      "Epoch 21/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1241.7081 - mse: 16568271.0000 - mae: 1241.7081 - val_loss: 1420.8627 - val_mse: 16656116.0000 - val_mae: 1420.8627\n",
      "Epoch 22/500\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 1241.7057 - mse: 16552097.0000 - mae: 1241.7057 - val_loss: 1420.6284 - val_mse: 16638420.0000 - val_mae: 1420.6284\n",
      "Epoch 23/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1242.3303 - mse: 16529682.0000 - mae: 1242.3302 - val_loss: 1420.2765 - val_mse: 16621847.0000 - val_mae: 1420.2765\n",
      "Epoch 24/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1239.7276 - mse: 16520266.0000 - mae: 1239.7275 - val_loss: 1420.1565 - val_mse: 16603745.0000 - val_mae: 1420.1565\n",
      "Epoch 25/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1239.0541 - mse: 16511990.0000 - mae: 1239.0541 - val_loss: 1419.7991 - val_mse: 16592870.0000 - val_mae: 1419.7991\n",
      "Epoch 26/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1242.1699 - mse: 16515796.0000 - mae: 1242.1699 - val_loss: 1419.3805 - val_mse: 16590338.0000 - val_mae: 1419.3805\n",
      "Epoch 27/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1238.5788 - mse: 16516153.0000 - mae: 1238.5789 - val_loss: 1419.2770 - val_mse: 16579722.0000 - val_mae: 1419.2770\n",
      "Epoch 28/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1240.6810 - mse: 16509813.0000 - mae: 1240.6810 - val_loss: 1419.2122 - val_mse: 16569989.0000 - val_mae: 1419.2122\n",
      "Epoch 29/500\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 1238.4717 - mse: 16503059.0000 - mae: 1238.4716 - val_loss: 1419.1759 - val_mse: 16561917.0000 - val_mae: 1419.1759\n",
      "Epoch 30/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1239.8448 - mse: 16512465.0000 - mae: 1239.8448 - val_loss: 1418.8588 - val_mse: 16558522.0000 - val_mae: 1418.8588\n",
      "Epoch 31/500\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 1239.8947 - mse: 16510224.0000 - mae: 1239.8947 - val_loss: 1418.5839 - val_mse: 16556057.0000 - val_mae: 1418.5839\n",
      "Epoch 32/500\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 1235.9594 - mse: 16478472.0000 - mae: 1235.9594 - val_loss: 1418.5353 - val_mse: 16548581.0000 - val_mae: 1418.5353\n",
      "Epoch 33/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1238.4172 - mse: 16491743.0000 - mae: 1238.4172 - val_loss: 1418.3700 - val_mse: 16544753.0000 - val_mae: 1418.3700\n",
      "Epoch 34/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1238.6360 - mse: 16496720.0000 - mae: 1238.6360 - val_loss: 1418.0498 - val_mse: 16543542.0000 - val_mae: 1418.0498\n",
      "Epoch 35/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1238.5491 - mse: 16497186.0000 - mae: 1238.5492 - val_loss: 1417.8615 - val_mse: 16539488.0000 - val_mae: 1417.8615\n",
      "Epoch 36/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1237.1190 - mse: 16488094.0000 - mae: 1237.1190 - val_loss: 1417.9399 - val_mse: 16528846.0000 - val_mae: 1417.9399\n",
      "Epoch 37/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1237.5150 - mse: 16490083.0000 - mae: 1237.5150 - val_loss: 1417.7626 - val_mse: 16524987.0000 - val_mae: 1417.7626\n",
      "Epoch 38/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1237.2984 - mse: 16491518.0000 - mae: 1237.2983 - val_loss: 1417.8990 - val_mse: 16512303.0000 - val_mae: 1417.8990\n",
      "Epoch 39/500\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 1235.9258 - mse: 16498429.0000 - mae: 1235.9258 - val_loss: 1417.9581 - val_mse: 16498826.0000 - val_mae: 1417.9581\n",
      "Epoch 40/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1237.4795 - mse: 16487535.0000 - mae: 1237.4796 - val_loss: 1417.4980 - val_mse: 16492500.0000 - val_mae: 1417.4980\n",
      "Epoch 41/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1235.8022 - mse: 16487457.0000 - mae: 1235.8022 - val_loss: 1417.0123 - val_mse: 16480253.0000 - val_mae: 1417.0123\n",
      "Epoch 42/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1234.2726 - mse: 16474649.0000 - mae: 1234.2726 - val_loss: 1416.8201 - val_mse: 16459918.0000 - val_mae: 1416.8201\n",
      "Epoch 43/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1233.8223 - mse: 16474631.0000 - mae: 1233.8224 - val_loss: 1416.0193 - val_mse: 16440835.0000 - val_mae: 1416.0193\n",
      "Epoch 44/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1233.2764 - mse: 16487706.0000 - mae: 1233.2764 - val_loss: 1414.7772 - val_mse: 16416766.0000 - val_mae: 1414.7772\n",
      "Epoch 45/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1230.2615 - mse: 16477657.0000 - mae: 1230.2615 - val_loss: 1415.0520 - val_mse: 16382051.0000 - val_mae: 1415.0520\n",
      "Epoch 46/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1230.0977 - mse: 16488630.0000 - mae: 1230.0977 - val_loss: 1417.6691 - val_mse: 16340325.0000 - val_mae: 1417.6691\n",
      "Epoch 47/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1230.0920 - mse: 16450237.0000 - mae: 1230.0920 - val_loss: 1419.0009 - val_mse: 16303193.0000 - val_mae: 1419.0009\n",
      "Epoch 48/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1226.5143 - mse: 16441922.0000 - mae: 1226.5142 - val_loss: 1419.8604 - val_mse: 16265616.0000 - val_mae: 1419.8604\n",
      "Epoch 49/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1225.7112 - mse: 16437676.0000 - mae: 1225.7112 - val_loss: 1420.1447 - val_mse: 16228045.0000 - val_mae: 1420.1447\n",
      "Epoch 50/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1223.0263 - mse: 16413937.0000 - mae: 1223.0264 - val_loss: 1420.4163 - val_mse: 16186744.0000 - val_mae: 1420.4163\n",
      "Epoch 51/500\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 1223.4981 - mse: 16420017.0000 - mae: 1223.4982 - val_loss: 1425.7086 - val_mse: 16136497.0000 - val_mae: 1425.7086\n",
      "Epoch 52/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1220.0055 - mse: 16360089.0000 - mae: 1220.0056 - val_loss: 1425.1489 - val_mse: 16095746.0000 - val_mae: 1425.1489\n",
      "Epoch 53/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1218.3972 - mse: 16351222.0000 - mae: 1218.3972 - val_loss: 1426.9269 - val_mse: 16049380.0000 - val_mae: 1426.9269\n",
      "Epoch 54/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1214.4684 - mse: 16322219.0000 - mae: 1214.4684 - val_loss: 1431.2902 - val_mse: 15995450.0000 - val_mae: 1431.2902\n",
      "Epoch 55/500\n",
      "2500/2500 [==============================] - 0s 32us/step - loss: 1216.1046 - mse: 16320792.0000 - mae: 1216.1046 - val_loss: 1433.5686 - val_mse: 15948749.0000 - val_mae: 1433.5686\n",
      "Epoch 56/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1211.2662 - mse: 16293038.0000 - mae: 1211.2662 - val_loss: 1433.7745 - val_mse: 15897700.0000 - val_mae: 1433.7745\n",
      "Epoch 57/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1211.7343 - mse: 16277170.0000 - mae: 1211.7343 - val_loss: 1440.1385 - val_mse: 15849420.0000 - val_mae: 1440.1385\n",
      "Epoch 58/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1209.8876 - mse: 16210808.0000 - mae: 1209.8875 - val_loss: 1431.0197 - val_mse: 15799245.0000 - val_mae: 1431.0197\n",
      "Epoch 59/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1208.6302 - mse: 16237570.0000 - mae: 1208.6302 - val_loss: 1442.5474 - val_mse: 15751651.0000 - val_mae: 1442.5474\n",
      "Epoch 60/500\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 1207.1896 - mse: 16214753.0000 - mae: 1207.1896 - val_loss: 1441.7491 - val_mse: 15702241.0000 - val_mae: 1441.7491\n",
      "Epoch 61/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1203.9106 - mse: 16177740.0000 - mae: 1203.9106 - val_loss: 1454.0049 - val_mse: 15651719.0000 - val_mae: 1454.0049\n",
      "Epoch 62/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1202.1884 - mse: 16139810.0000 - mae: 1202.1884 - val_loss: 1456.3370 - val_mse: 15596537.0000 - val_mae: 1456.3370\n",
      "Epoch 63/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1200.1542 - mse: 16136869.0000 - mae: 1200.1542 - val_loss: 1461.8279 - val_mse: 15545853.0000 - val_mae: 1461.8279\n",
      "Epoch 64/500\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 1197.0109 - mse: 16081433.0000 - mae: 1197.0107 - val_loss: 1453.7633 - val_mse: 15488791.0000 - val_mae: 1453.7633\n",
      "Epoch 65/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1196.9399 - mse: 16070554.0000 - mae: 1196.9399 - val_loss: 1467.8462 - val_mse: 15441085.0000 - val_mae: 1467.8462\n",
      "Epoch 66/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1195.5312 - mse: 16017565.0000 - mae: 1195.5312 - val_loss: 1461.1937 - val_mse: 15384382.0000 - val_mae: 1461.1937\n",
      "Epoch 67/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1193.4338 - mse: 16016099.0000 - mae: 1193.4338 - val_loss: 1467.4841 - val_mse: 15330248.0000 - val_mae: 1467.4841\n",
      "Epoch 68/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1190.0698 - mse: 15973684.0000 - mae: 1190.0698 - val_loss: 1469.3466 - val_mse: 15276835.0000 - val_mae: 1469.3466\n",
      "Epoch 69/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1189.0819 - mse: 15999151.0000 - mae: 1189.0818 - val_loss: 1488.2571 - val_mse: 15238215.0000 - val_mae: 1488.2571\n",
      "Epoch 70/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1185.3439 - mse: 15949534.0000 - mae: 1185.3439 - val_loss: 1494.6030 - val_mse: 15190865.0000 - val_mae: 1494.6030\n",
      "Epoch 71/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1186.5023 - mse: 15927391.0000 - mae: 1186.5023 - val_loss: 1498.7659 - val_mse: 15143836.0000 - val_mae: 1498.7659\n",
      "Epoch 72/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1185.1013 - mse: 15887252.0000 - mae: 1185.1014 - val_loss: 1478.2075 - val_mse: 15076949.0000 - val_mae: 1478.2075\n",
      "Epoch 73/500\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 1180.9109 - mse: 15885196.0000 - mae: 1180.9109 - val_loss: 1534.2552 - val_mse: 15071582.0000 - val_mae: 1534.2552\n",
      "Epoch 74/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1183.7206 - mse: 15878005.0000 - mae: 1183.7206 - val_loss: 1522.4745 - val_mse: 15015094.0000 - val_mae: 1522.4745\n",
      "Epoch 75/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1176.7977 - mse: 15800423.0000 - mae: 1176.7976 - val_loss: 1521.1987 - val_mse: 14963929.0000 - val_mae: 1521.1987\n",
      "Epoch 76/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1181.7780 - mse: 15816424.0000 - mae: 1181.7780 - val_loss: 1535.8805 - val_mse: 14936456.0000 - val_mae: 1535.8805\n",
      "Epoch 77/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1176.8998 - mse: 15780354.0000 - mae: 1176.8998 - val_loss: 1526.2852 - val_mse: 14877491.0000 - val_mae: 1526.2852\n",
      "Epoch 78/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1178.0572 - mse: 15809892.0000 - mae: 1178.0573 - val_loss: 1533.6683 - val_mse: 14847524.0000 - val_mae: 1533.6683\n",
      "Epoch 79/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1173.4461 - mse: 15717748.0000 - mae: 1173.4460 - val_loss: 1525.9590 - val_mse: 14802590.0000 - val_mae: 1525.9590\n",
      "Epoch 80/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1175.8506 - mse: 15805699.0000 - mae: 1175.8506 - val_loss: 1554.5463 - val_mse: 14798327.0000 - val_mae: 1554.5463\n",
      "Epoch 81/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1172.3160 - mse: 15673768.0000 - mae: 1172.3160 - val_loss: 1539.4742 - val_mse: 14747677.0000 - val_mae: 1539.4742\n",
      "Epoch 82/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1171.5173 - mse: 15750753.0000 - mae: 1171.5172 - val_loss: 1552.4274 - val_mse: 14722224.0000 - val_mae: 1552.4274\n",
      "Epoch 83/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1172.5187 - mse: 15737281.0000 - mae: 1172.5187 - val_loss: 1564.1991 - val_mse: 14700977.0000 - val_mae: 1564.1991\n",
      "Epoch 84/500\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 1172.9419 - mse: 15685226.0000 - mae: 1172.9420 - val_loss: 1577.5607 - val_mse: 14693368.0000 - val_mae: 1577.5607\n",
      "Epoch 85/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1172.2341 - mse: 15693160.0000 - mae: 1172.2341 - val_loss: 1554.8802 - val_mse: 14640412.0000 - val_mae: 1554.8802\n",
      "Epoch 86/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1168.4851 - mse: 15699791.0000 - mae: 1168.4851 - val_loss: 1601.9840 - val_mse: 14662021.0000 - val_mae: 1601.9840\n",
      "Epoch 87/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1169.8431 - mse: 15623989.0000 - mae: 1169.8431 - val_loss: 1580.4573 - val_mse: 14614500.0000 - val_mae: 1580.4573\n",
      "Epoch 88/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1170.5739 - mse: 15609797.0000 - mae: 1170.5739 - val_loss: 1561.1721 - val_mse: 14578300.0000 - val_mae: 1561.1721\n",
      "Epoch 89/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1163.4034 - mse: 15593675.0000 - mae: 1163.4034 - val_loss: 1559.9473 - val_mse: 14543819.0000 - val_mae: 1559.9473\n",
      "Epoch 90/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1163.6982 - mse: 15587767.0000 - mae: 1163.6982 - val_loss: 1553.9941 - val_mse: 14508336.0000 - val_mae: 1553.9941\n",
      "Epoch 91/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1163.8883 - mse: 15571109.0000 - mae: 1163.8883 - val_loss: 1590.5686 - val_mse: 14511775.0000 - val_mae: 1590.5686\n",
      "Epoch 92/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1160.3457 - mse: 15547299.0000 - mae: 1160.3457 - val_loss: 1585.9603 - val_mse: 14471961.0000 - val_mae: 1585.9603\n",
      "Epoch 93/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1164.6864 - mse: 15559398.0000 - mae: 1164.6864 - val_loss: 1546.8595 - val_mse: 14394160.0000 - val_mae: 1546.8595\n",
      "Epoch 94/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1163.5549 - mse: 15596477.0000 - mae: 1163.5551 - val_loss: 1576.1506 - val_mse: 14415817.0000 - val_mae: 1576.1506\n",
      "Epoch 95/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1162.4582 - mse: 15547344.0000 - mae: 1162.4583 - val_loss: 1601.6613 - val_mse: 14434277.0000 - val_mae: 1601.6613\n",
      "Epoch 96/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1161.2040 - mse: 15459933.0000 - mae: 1161.2040 - val_loss: 1555.3381 - val_mse: 14329209.0000 - val_mae: 1555.3381\n",
      "Epoch 97/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1159.6025 - mse: 15446991.0000 - mae: 1159.6024 - val_loss: 1596.7723 - val_mse: 14369048.0000 - val_mae: 1596.7723\n",
      "Epoch 98/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1158.8538 - mse: 15428932.0000 - mae: 1158.8538 - val_loss: 1543.8940 - val_mse: 14264224.0000 - val_mae: 1543.8940\n",
      "Epoch 99/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1153.5901 - mse: 15452080.0000 - mae: 1153.5901 - val_loss: 1541.4249 - val_mse: 14234756.0000 - val_mae: 1541.4249\n",
      "Epoch 100/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1158.2771 - mse: 15470238.0000 - mae: 1158.2770 - val_loss: 1594.4283 - val_mse: 14281805.0000 - val_mae: 1594.4283\n",
      "Epoch 101/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1155.7300 - mse: 15441537.0000 - mae: 1155.7300 - val_loss: 1588.2419 - val_mse: 14228458.0000 - val_mae: 1588.2419\n",
      "Epoch 102/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1160.1907 - mse: 15440495.0000 - mae: 1160.1908 - val_loss: 1580.1462 - val_mse: 14202934.0000 - val_mae: 1580.1462\n",
      "Epoch 103/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1159.6661 - mse: 15437167.0000 - mae: 1159.6663 - val_loss: 1600.4723 - val_mse: 14229777.0000 - val_mae: 1600.4723\n",
      "Epoch 104/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1156.0738 - mse: 15428193.0000 - mae: 1156.0739 - val_loss: 1571.8335 - val_mse: 14140546.0000 - val_mae: 1571.8335\n",
      "Epoch 105/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1159.0985 - mse: 15355409.0000 - mae: 1159.0984 - val_loss: 1591.5277 - val_mse: 14160988.0000 - val_mae: 1591.5277\n",
      "Epoch 106/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1157.4244 - mse: 15426778.0000 - mae: 1157.4244 - val_loss: 1572.6238 - val_mse: 14112776.0000 - val_mae: 1572.6238\n",
      "Epoch 107/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1157.0952 - mse: 15455614.0000 - mae: 1157.0952 - val_loss: 1619.1700 - val_mse: 14181624.0000 - val_mae: 1619.1700\n",
      "Epoch 108/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1155.9494 - mse: 15362841.0000 - mae: 1155.9493 - val_loss: 1540.0913 - val_mse: 14040175.0000 - val_mae: 1540.0913\n",
      "Epoch 109/500\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 1155.8221 - mse: 15315621.0000 - mae: 1155.8221 - val_loss: 1574.8496 - val_mse: 14063382.0000 - val_mae: 1574.8496\n",
      "Epoch 110/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1153.8799 - mse: 15368420.0000 - mae: 1153.8798 - val_loss: 1611.5736 - val_mse: 14092599.0000 - val_mae: 1611.5736\n",
      "Epoch 111/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1154.7904 - mse: 15346506.0000 - mae: 1154.7904 - val_loss: 1582.2933 - val_mse: 14043801.0000 - val_mae: 1582.2933\n",
      "Epoch 112/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1148.0333 - mse: 15332745.0000 - mae: 1148.0333 - val_loss: 1630.0211 - val_mse: 14092421.0000 - val_mae: 1630.0211\n",
      "Epoch 113/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1153.7237 - mse: 15354511.0000 - mae: 1153.7238 - val_loss: 1568.8279 - val_mse: 13987540.0000 - val_mae: 1568.8279\n",
      "Epoch 114/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1151.7371 - mse: 15335567.0000 - mae: 1151.7371 - val_loss: 1559.5833 - val_mse: 13941354.0000 - val_mae: 1559.5833\n",
      "Epoch 115/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1156.0841 - mse: 15374343.0000 - mae: 1156.0842 - val_loss: 1588.0220 - val_mse: 13989175.0000 - val_mae: 1588.0220\n",
      "Epoch 116/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1154.1417 - mse: 15313836.0000 - mae: 1154.1417 - val_loss: 1556.1639 - val_mse: 13917452.0000 - val_mae: 1556.1639\n",
      "Epoch 117/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1156.7255 - mse: 15335975.0000 - mae: 1156.7255 - val_loss: 1583.0530 - val_mse: 13950916.0000 - val_mae: 1583.0530\n",
      "Epoch 118/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1152.1419 - mse: 15319782.0000 - mae: 1152.1418 - val_loss: 1568.4868 - val_mse: 13917450.0000 - val_mae: 1568.4868\n",
      "Epoch 119/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1151.8374 - mse: 15292123.0000 - mae: 1151.8374 - val_loss: 1644.2635 - val_mse: 14015010.0000 - val_mae: 1644.2635\n",
      "Epoch 120/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1152.5001 - mse: 15227719.0000 - mae: 1152.5001 - val_loss: 1590.2966 - val_mse: 13901928.0000 - val_mae: 1590.2966\n",
      "Epoch 121/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1145.2444 - mse: 15226782.0000 - mae: 1145.2444 - val_loss: 1541.2227 - val_mse: 13827806.0000 - val_mae: 1541.2227\n",
      "Epoch 122/500\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 1150.1843 - mse: 15270547.0000 - mae: 1150.1844 - val_loss: 1524.9235 - val_mse: 13797771.0000 - val_mae: 1524.9235\n",
      "Epoch 123/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1150.2909 - mse: 15318118.0000 - mae: 1150.2909 - val_loss: 1575.2980 - val_mse: 13840975.0000 - val_mae: 1575.2980\n",
      "Epoch 124/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1148.0146 - mse: 15254949.0000 - mae: 1148.0146 - val_loss: 1523.5500 - val_mse: 13773898.0000 - val_mae: 1523.5500\n",
      "Epoch 125/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1150.5760 - mse: 15333075.0000 - mae: 1150.5760 - val_loss: 1540.2841 - val_mse: 13792758.0000 - val_mae: 1540.2841\n",
      "Epoch 126/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1148.2442 - mse: 15293945.0000 - mae: 1148.2441 - val_loss: 1569.4413 - val_mse: 13807990.0000 - val_mae: 1569.4413\n",
      "Epoch 127/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1150.6388 - mse: 15239242.0000 - mae: 1150.6388 - val_loss: 1565.8367 - val_mse: 13807982.0000 - val_mae: 1565.8367\n",
      "Epoch 128/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1149.8009 - mse: 15278046.0000 - mae: 1149.8010 - val_loss: 1526.2585 - val_mse: 13744292.0000 - val_mae: 1526.2585\n",
      "Epoch 129/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1148.4577 - mse: 15203533.0000 - mae: 1148.4576 - val_loss: 1529.3678 - val_mse: 13740303.0000 - val_mae: 1529.3678\n",
      "Epoch 130/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1147.7581 - mse: 15238555.0000 - mae: 1147.7581 - val_loss: 1519.7338 - val_mse: 13704612.0000 - val_mae: 1519.7338\n",
      "Epoch 131/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1153.6585 - mse: 15374954.0000 - mae: 1153.6584 - val_loss: 1495.1821 - val_mse: 13677025.0000 - val_mae: 1495.1821\n",
      "Epoch 132/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1147.1509 - mse: 15267335.0000 - mae: 1147.1508 - val_loss: 1563.4523 - val_mse: 13770852.0000 - val_mae: 1563.4523\n",
      "Epoch 133/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1148.7694 - mse: 15166964.0000 - mae: 1148.7694 - val_loss: 1534.4569 - val_mse: 13719765.0000 - val_mae: 1534.4569\n",
      "Epoch 134/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1146.8145 - mse: 15209028.0000 - mae: 1146.8145 - val_loss: 1553.0332 - val_mse: 13728254.0000 - val_mae: 1553.0332\n",
      "Epoch 135/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1145.6226 - mse: 15209615.0000 - mae: 1145.6226 - val_loss: 1531.9489 - val_mse: 13718241.0000 - val_mae: 1531.9489\n",
      "Epoch 136/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1143.1570 - mse: 15198156.0000 - mae: 1143.1570 - val_loss: 1526.4669 - val_mse: 13679970.0000 - val_mae: 1526.4669\n",
      "Epoch 137/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1145.0099 - mse: 15183726.0000 - mae: 1145.0100 - val_loss: 1504.9708 - val_mse: 13650657.0000 - val_mae: 1504.9708\n",
      "Epoch 138/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1148.8475 - mse: 15210837.0000 - mae: 1148.8477 - val_loss: 1502.9464 - val_mse: 13639946.0000 - val_mae: 1502.9464\n",
      "Epoch 139/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1147.2400 - mse: 15263472.0000 - mae: 1147.2400 - val_loss: 1502.9465 - val_mse: 13640095.0000 - val_mae: 1502.9465\n",
      "Epoch 140/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1149.8631 - mse: 15195105.0000 - mae: 1149.8632 - val_loss: 1529.9341 - val_mse: 13671542.0000 - val_mae: 1529.9341\n",
      "Epoch 141/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1142.5696 - mse: 15095605.0000 - mae: 1142.5696 - val_loss: 1512.0773 - val_mse: 13644290.0000 - val_mae: 1512.0773\n",
      "Epoch 142/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1148.2575 - mse: 15170853.0000 - mae: 1148.2574 - val_loss: 1492.9990 - val_mse: 13641795.0000 - val_mae: 1492.9990\n",
      "Epoch 143/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1140.7796 - mse: 15167701.0000 - mae: 1140.7795 - val_loss: 1540.3384 - val_mse: 13665966.0000 - val_mae: 1540.3384\n",
      "Epoch 144/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1141.9173 - mse: 15183579.0000 - mae: 1141.9174 - val_loss: 1550.9126 - val_mse: 13658960.0000 - val_mae: 1550.9126\n",
      "Epoch 145/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1149.2508 - mse: 15144375.0000 - mae: 1149.2509 - val_loss: 1465.9907 - val_mse: 13590325.0000 - val_mae: 1465.9907\n",
      "Epoch 146/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1144.7144 - mse: 15192449.0000 - mae: 1144.7144 - val_loss: 1540.6469 - val_mse: 13660624.0000 - val_mae: 1540.6469\n",
      "Epoch 147/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1142.3867 - mse: 15133847.0000 - mae: 1142.3867 - val_loss: 1499.9340 - val_mse: 13615997.0000 - val_mae: 1499.9340\n",
      "Epoch 148/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1146.0833 - mse: 15133008.0000 - mae: 1146.0834 - val_loss: 1523.2007 - val_mse: 13629565.0000 - val_mae: 1523.2007\n",
      "Epoch 149/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1145.2265 - mse: 15176260.0000 - mae: 1145.2264 - val_loss: 1483.7581 - val_mse: 13568604.0000 - val_mae: 1483.7581\n",
      "Epoch 150/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1142.5880 - mse: 15114353.0000 - mae: 1142.5880 - val_loss: 1446.7988 - val_mse: 13532514.0000 - val_mae: 1446.7988\n",
      "Epoch 151/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1149.7927 - mse: 15226521.0000 - mae: 1149.7928 - val_loss: 1471.4371 - val_mse: 13581943.0000 - val_mae: 1471.4371\n",
      "Epoch 152/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1142.5522 - mse: 15099442.0000 - mae: 1142.5522 - val_loss: 1503.9823 - val_mse: 13619466.0000 - val_mae: 1503.9823\n",
      "Epoch 153/500\n",
      "2500/2500 [==============================] - 0s 33us/step - loss: 1140.6365 - mse: 15089661.0000 - mae: 1140.6365 - val_loss: 1489.9767 - val_mse: 13609827.0000 - val_mae: 1489.9767\n",
      "Epoch 154/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1143.4603 - mse: 15104603.0000 - mae: 1143.4604 - val_loss: 1450.1821 - val_mse: 13534424.0000 - val_mae: 1450.1821\n",
      "Epoch 155/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1141.1458 - mse: 15072882.0000 - mae: 1141.1458 - val_loss: 1461.6384 - val_mse: 13526268.0000 - val_mae: 1461.6384\n",
      "Epoch 156/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1143.4739 - mse: 15151676.0000 - mae: 1143.4739 - val_loss: 1405.3960 - val_mse: 13488612.0000 - val_mae: 1405.3960\n",
      "Epoch 157/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1141.2925 - mse: 15095294.0000 - mae: 1141.2924 - val_loss: 1506.6974 - val_mse: 13586230.0000 - val_mae: 1506.6974\n",
      "Epoch 158/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1141.7250 - mse: 15086941.0000 - mae: 1141.7250 - val_loss: 1473.7642 - val_mse: 13549942.0000 - val_mae: 1473.7642\n",
      "Epoch 159/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1138.1974 - mse: 15039527.0000 - mae: 1138.1974 - val_loss: 1413.3796 - val_mse: 13480376.0000 - val_mae: 1413.3796\n",
      "Epoch 160/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1148.0010 - mse: 15220013.0000 - mae: 1148.0010 - val_loss: 1471.0399 - val_mse: 13526515.0000 - val_mae: 1471.0399\n",
      "Epoch 161/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1137.5834 - mse: 15029481.0000 - mae: 1137.5834 - val_loss: 1458.8262 - val_mse: 13485141.0000 - val_mae: 1458.8262\n",
      "Epoch 162/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1141.8962 - mse: 14971145.0000 - mae: 1141.8962 - val_loss: 1503.0250 - val_mse: 13546415.0000 - val_mae: 1503.0250\n",
      "Epoch 163/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1142.6824 - mse: 14938715.0000 - mae: 1142.6824 - val_loss: 1407.6909 - val_mse: 13473525.0000 - val_mae: 1407.6909\n",
      "Epoch 164/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1141.5091 - mse: 15044721.0000 - mae: 1141.5092 - val_loss: 1450.8123 - val_mse: 13494601.0000 - val_mae: 1450.8123\n",
      "Epoch 165/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1137.4645 - mse: 14973115.0000 - mae: 1137.4644 - val_loss: 1429.8323 - val_mse: 13463187.0000 - val_mae: 1429.8323\n",
      "Epoch 166/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1143.7753 - mse: 15110316.0000 - mae: 1143.7754 - val_loss: 1408.2714 - val_mse: 13448677.0000 - val_mae: 1408.2714\n",
      "Epoch 167/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1136.7461 - mse: 14916575.0000 - mae: 1136.7460 - val_loss: 1419.8217 - val_mse: 13460792.0000 - val_mae: 1419.8217\n",
      "Epoch 168/500\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 1139.9192 - mse: 15054832.0000 - mae: 1139.9192 - val_loss: 1446.6804 - val_mse: 13482362.0000 - val_mae: 1446.6804\n",
      "Epoch 169/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1138.0685 - mse: 15048419.0000 - mae: 1138.0685 - val_loss: 1424.4550 - val_mse: 13444615.0000 - val_mae: 1424.4550\n",
      "Epoch 170/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1141.4057 - mse: 14987349.0000 - mae: 1141.4056 - val_loss: 1398.0387 - val_mse: 13436968.0000 - val_mae: 1398.0387\n",
      "Epoch 171/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1139.6926 - mse: 15037304.0000 - mae: 1139.6927 - val_loss: 1454.4755 - val_mse: 13465126.0000 - val_mae: 1454.4755\n",
      "Epoch 172/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1133.9084 - mse: 14927302.0000 - mae: 1133.9084 - val_loss: 1446.9175 - val_mse: 13450804.0000 - val_mae: 1446.9175\n",
      "Epoch 173/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1136.6901 - mse: 14889286.0000 - mae: 1136.6901 - val_loss: 1403.4653 - val_mse: 13408099.0000 - val_mae: 1403.4653\n",
      "Epoch 174/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1136.9529 - mse: 14947038.0000 - mae: 1136.9528 - val_loss: 1390.9082 - val_mse: 13402424.0000 - val_mae: 1390.9082\n",
      "Epoch 175/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1139.5979 - mse: 14991131.0000 - mae: 1139.5978 - val_loss: 1395.8871 - val_mse: 13431979.0000 - val_mae: 1395.8871\n",
      "Epoch 176/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1136.1190 - mse: 14916832.0000 - mae: 1136.1190 - val_loss: 1372.8162 - val_mse: 13405841.0000 - val_mae: 1372.8162\n",
      "Epoch 177/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1135.6123 - mse: 14906484.0000 - mae: 1135.6122 - val_loss: 1413.6976 - val_mse: 13428866.0000 - val_mae: 1413.6976\n",
      "Epoch 178/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1134.1090 - mse: 14896438.0000 - mae: 1134.1091 - val_loss: 1433.7446 - val_mse: 13439931.0000 - val_mae: 1433.7446\n",
      "Epoch 179/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1140.8227 - mse: 14952063.0000 - mae: 1140.8228 - val_loss: 1435.4344 - val_mse: 13443544.0000 - val_mae: 1435.4344\n",
      "Epoch 180/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1136.5204 - mse: 14908244.0000 - mae: 1136.5204 - val_loss: 1402.3702 - val_mse: 13427126.0000 - val_mae: 1402.3702\n",
      "Epoch 181/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1133.8990 - mse: 14905795.0000 - mae: 1133.8990 - val_loss: 1407.6605 - val_mse: 13411096.0000 - val_mae: 1407.6605\n",
      "Epoch 182/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1138.6830 - mse: 14946170.0000 - mae: 1138.6827 - val_loss: 1386.4314 - val_mse: 13423800.0000 - val_mae: 1386.4314\n",
      "Epoch 183/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1140.3675 - mse: 15011268.0000 - mae: 1140.3676 - val_loss: 1416.4985 - val_mse: 13476097.0000 - val_mae: 1416.4985\n",
      "Epoch 184/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1139.3251 - mse: 14899526.0000 - mae: 1139.3251 - val_loss: 1372.4797 - val_mse: 13437011.0000 - val_mae: 1372.4797\n",
      "Epoch 185/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1131.7007 - mse: 14998956.0000 - mae: 1131.7006 - val_loss: 1391.6774 - val_mse: 13434667.0000 - val_mae: 1391.6774\n",
      "Epoch 186/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1137.8964 - mse: 14747504.0000 - mae: 1137.8965 - val_loss: 1328.3190 - val_mse: 13435859.0000 - val_mae: 1328.3190\n",
      "Epoch 187/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1139.1386 - mse: 14969337.0000 - mae: 1139.1385 - val_loss: 1370.8756 - val_mse: 13442090.0000 - val_mae: 1370.8756\n",
      "Epoch 188/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1134.8198 - mse: 14870928.0000 - mae: 1134.8198 - val_loss: 1367.1925 - val_mse: 13440072.0000 - val_mae: 1367.1925\n",
      "Epoch 189/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1138.6356 - mse: 14950688.0000 - mae: 1138.6356 - val_loss: 1320.0273 - val_mse: 13450296.0000 - val_mae: 1320.0273\n",
      "Epoch 190/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1137.1393 - mse: 15015340.0000 - mae: 1137.1392 - val_loss: 1321.1652 - val_mse: 13503790.0000 - val_mae: 1321.1652\n",
      "Epoch 191/500\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 1136.8361 - mse: 14948193.0000 - mae: 1136.8361 - val_loss: 1386.1606 - val_mse: 13456755.0000 - val_mae: 1386.1606\n",
      "Epoch 192/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1136.4777 - mse: 14949822.0000 - mae: 1136.4777 - val_loss: 1359.9725 - val_mse: 13452142.0000 - val_mae: 1359.9725\n",
      "Epoch 193/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1132.2935 - mse: 14927774.0000 - mae: 1132.2936 - val_loss: 1345.1011 - val_mse: 13449752.0000 - val_mae: 1345.1011\n",
      "Epoch 194/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1133.8950 - mse: 14849848.0000 - mae: 1133.8950 - val_loss: 1344.8883 - val_mse: 13440689.0000 - val_mae: 1344.8883\n",
      "Epoch 195/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1140.9153 - mse: 14924172.0000 - mae: 1140.9153 - val_loss: 1337.6530 - val_mse: 13460281.0000 - val_mae: 1337.6530\n",
      "Epoch 196/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1137.9224 - mse: 14894670.0000 - mae: 1137.9224 - val_loss: 1345.2629 - val_mse: 13449204.0000 - val_mae: 1345.2629\n",
      "Epoch 197/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1136.3607 - mse: 14838699.0000 - mae: 1136.3606 - val_loss: 1354.8314 - val_mse: 13468959.0000 - val_mae: 1354.8314\n",
      "Epoch 198/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1134.6836 - mse: 14848064.0000 - mae: 1134.6836 - val_loss: 1352.3265 - val_mse: 13476887.0000 - val_mae: 1352.3265\n",
      "Epoch 199/500\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 1134.5352 - mse: 14900814.0000 - mae: 1134.5352 - val_loss: 1357.1678 - val_mse: 13505596.0000 - val_mae: 1357.1678\n",
      "Epoch 200/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1140.4518 - mse: 14957730.0000 - mae: 1140.4517 - val_loss: 1361.5081 - val_mse: 13518817.0000 - val_mae: 1361.5081\n",
      "Epoch 201/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1135.7827 - mse: 14860398.0000 - mae: 1135.7827 - val_loss: 1341.3910 - val_mse: 13503457.0000 - val_mae: 1341.3910\n",
      "Epoch 202/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1136.3042 - mse: 14841830.0000 - mae: 1136.3042 - val_loss: 1340.5393 - val_mse: 13511625.0000 - val_mae: 1340.5393\n",
      "Epoch 203/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1134.8653 - mse: 14924163.0000 - mae: 1134.8654 - val_loss: 1344.9247 - val_mse: 13480221.0000 - val_mae: 1344.9247\n",
      "Epoch 204/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1139.3180 - mse: 14896148.0000 - mae: 1139.3179 - val_loss: 1329.2264 - val_mse: 13548310.0000 - val_mae: 1329.2264\n",
      "Epoch 205/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1135.0231 - mse: 14885134.0000 - mae: 1135.0231 - val_loss: 1336.3451 - val_mse: 13531102.0000 - val_mae: 1336.3451\n",
      "Epoch 206/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1130.6265 - mse: 14852486.0000 - mae: 1130.6265 - val_loss: 1322.0936 - val_mse: 13514639.0000 - val_mae: 1322.0936\n",
      "Epoch 207/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1131.7221 - mse: 14776294.0000 - mae: 1131.7220 - val_loss: 1330.8134 - val_mse: 13517384.0000 - val_mae: 1330.8134\n",
      "Epoch 208/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1132.1427 - mse: 14874051.0000 - mae: 1132.1427 - val_loss: 1325.0374 - val_mse: 13549165.0000 - val_mae: 1325.0374\n",
      "Epoch 209/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1131.4005 - mse: 14830985.0000 - mae: 1131.4005 - val_loss: 1350.8030 - val_mse: 13526349.0000 - val_mae: 1350.8030\n",
      "Epoch 210/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1138.3249 - mse: 14885598.0000 - mae: 1138.3250 - val_loss: 1320.4081 - val_mse: 13568250.0000 - val_mae: 1320.4081\n",
      "Epoch 211/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1136.7391 - mse: 14928752.0000 - mae: 1136.7391 - val_loss: 1318.7819 - val_mse: 13589913.0000 - val_mae: 1318.7819\n",
      "Epoch 212/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1131.0935 - mse: 14887094.0000 - mae: 1131.0935 - val_loss: 1317.9855 - val_mse: 13547772.0000 - val_mae: 1317.9855\n",
      "Epoch 213/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1131.9302 - mse: 14789296.0000 - mae: 1131.9302 - val_loss: 1325.8083 - val_mse: 13565066.0000 - val_mae: 1325.8083\n",
      "Epoch 214/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1132.7927 - mse: 14790581.0000 - mae: 1132.7927 - val_loss: 1326.8623 - val_mse: 13610323.0000 - val_mae: 1326.8623\n",
      "Epoch 215/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1136.6865 - mse: 14823131.0000 - mae: 1136.6865 - val_loss: 1333.5377 - val_mse: 13580708.0000 - val_mae: 1333.5377\n",
      "Epoch 216/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1135.9878 - mse: 14833957.0000 - mae: 1135.9878 - val_loss: 1311.4701 - val_mse: 13582352.0000 - val_mae: 1311.4701\n",
      "Epoch 217/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1132.9230 - mse: 14775001.0000 - mae: 1132.9230 - val_loss: 1306.1023 - val_mse: 13601432.0000 - val_mae: 1306.1023\n",
      "Epoch 218/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1137.4094 - mse: 14801260.0000 - mae: 1137.4094 - val_loss: 1307.9996 - val_mse: 13621658.0000 - val_mae: 1307.9996\n",
      "Epoch 219/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1131.0604 - mse: 14819872.0000 - mae: 1131.0604 - val_loss: 1323.7640 - val_mse: 13586944.0000 - val_mae: 1323.7640\n",
      "Epoch 220/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1131.6886 - mse: 14725017.0000 - mae: 1131.6885 - val_loss: 1291.0977 - val_mse: 13610755.0000 - val_mae: 1291.0977\n",
      "Epoch 221/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1128.7360 - mse: 14763524.0000 - mae: 1128.7360 - val_loss: 1315.2079 - val_mse: 13546984.0000 - val_mae: 1315.2079\n",
      "Epoch 222/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1130.3631 - mse: 14793361.0000 - mae: 1130.3632 - val_loss: 1298.6332 - val_mse: 13562113.0000 - val_mae: 1298.6332\n",
      "Epoch 223/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1132.6388 - mse: 14831102.0000 - mae: 1132.6388 - val_loss: 1296.7421 - val_mse: 13558768.0000 - val_mae: 1296.7421\n",
      "Epoch 224/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1132.0564 - mse: 14731731.0000 - mae: 1132.0564 - val_loss: 1333.0055 - val_mse: 13545764.0000 - val_mae: 1333.0055\n",
      "Epoch 225/500\n",
      "2500/2500 [==============================] - 0s 36us/step - loss: 1128.1758 - mse: 14749454.0000 - mae: 1128.1757 - val_loss: 1340.0312 - val_mse: 13547004.0000 - val_mae: 1340.0312\n",
      "Epoch 226/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1131.5661 - mse: 14765815.0000 - mae: 1131.5660 - val_loss: 1314.7061 - val_mse: 13589925.0000 - val_mae: 1314.7061\n",
      "Epoch 227/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1136.3433 - mse: 14856510.0000 - mae: 1136.3431 - val_loss: 1295.1804 - val_mse: 13646566.0000 - val_mae: 1295.1804\n",
      "Epoch 228/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1135.1284 - mse: 14875401.0000 - mae: 1135.1284 - val_loss: 1295.0560 - val_mse: 13632660.0000 - val_mae: 1295.0560\n",
      "Epoch 229/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1132.2024 - mse: 14682482.0000 - mae: 1132.2024 - val_loss: 1299.5531 - val_mse: 13590544.0000 - val_mae: 1299.5531\n",
      "Epoch 230/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1136.3113 - mse: 14828100.0000 - mae: 1136.3112 - val_loss: 1306.1361 - val_mse: 13620142.0000 - val_mae: 1306.1361\n",
      "Epoch 231/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1134.7265 - mse: 14923992.0000 - mae: 1134.7264 - val_loss: 1316.8451 - val_mse: 13571188.0000 - val_mae: 1316.8451\n",
      "Epoch 232/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1131.7408 - mse: 14761576.0000 - mae: 1131.7408 - val_loss: 1322.3430 - val_mse: 13578417.0000 - val_mae: 1322.3430\n",
      "Epoch 233/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1131.2251 - mse: 14697100.0000 - mae: 1131.2251 - val_loss: 1305.8210 - val_mse: 13661900.0000 - val_mae: 1305.8210\n",
      "Epoch 234/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1132.7469 - mse: 14817804.0000 - mae: 1132.7469 - val_loss: 1299.6207 - val_mse: 13670480.0000 - val_mae: 1299.6207\n",
      "Epoch 235/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1124.0414 - mse: 14685718.0000 - mae: 1124.0414 - val_loss: 1300.1299 - val_mse: 13655515.0000 - val_mae: 1300.1299\n",
      "Epoch 236/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1129.9980 - mse: 14756428.0000 - mae: 1129.9980 - val_loss: 1305.0012 - val_mse: 13638119.0000 - val_mae: 1305.0012\n",
      "Epoch 237/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1128.7947 - mse: 14750808.0000 - mae: 1128.7947 - val_loss: 1303.6039 - val_mse: 13617747.0000 - val_mae: 1303.6039\n",
      "Epoch 238/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1129.9448 - mse: 14765570.0000 - mae: 1129.9448 - val_loss: 1291.1742 - val_mse: 13628618.0000 - val_mae: 1291.1742\n",
      "Epoch 239/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1127.6070 - mse: 14637210.0000 - mae: 1127.6071 - val_loss: 1301.1669 - val_mse: 13612469.0000 - val_mae: 1301.1669\n",
      "Epoch 240/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1132.5787 - mse: 14826930.0000 - mae: 1132.5789 - val_loss: 1292.0179 - val_mse: 13665006.0000 - val_mae: 1292.0179\n",
      "Epoch 241/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1133.9169 - mse: 14835676.0000 - mae: 1133.9169 - val_loss: 1291.5883 - val_mse: 13681505.0000 - val_mae: 1291.5883\n",
      "Epoch 242/500\n",
      "2500/2500 [==============================] - 0s 34us/step - loss: 1128.8046 - mse: 14736166.0000 - mae: 1128.8046 - val_loss: 1286.0968 - val_mse: 13699387.0000 - val_mae: 1286.0968\n",
      "Epoch 243/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1134.1410 - mse: 14842520.0000 - mae: 1134.1410 - val_loss: 1292.5341 - val_mse: 13643445.0000 - val_mae: 1292.5341\n",
      "Epoch 244/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1133.7205 - mse: 14824664.0000 - mae: 1133.7205 - val_loss: 1304.0698 - val_mse: 13659119.0000 - val_mae: 1304.0698\n",
      "Epoch 245/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1133.2074 - mse: 14747366.0000 - mae: 1133.2074 - val_loss: 1296.7941 - val_mse: 13676129.0000 - val_mae: 1296.7941\n",
      "Epoch 246/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1131.2633 - mse: 14724134.0000 - mae: 1131.2633 - val_loss: 1292.0974 - val_mse: 13715381.0000 - val_mae: 1292.0974\n",
      "Epoch 247/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1131.6963 - mse: 14767240.0000 - mae: 1131.6964 - val_loss: 1302.3114 - val_mse: 13658235.0000 - val_mae: 1302.3114\n",
      "Epoch 248/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1130.7341 - mse: 14744669.0000 - mae: 1130.7341 - val_loss: 1296.7294 - val_mse: 13681915.0000 - val_mae: 1296.7294\n",
      "Epoch 249/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1125.7362 - mse: 14706524.0000 - mae: 1125.7361 - val_loss: 1289.4886 - val_mse: 13707059.0000 - val_mae: 1289.4886\n",
      "Epoch 250/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1133.2446 - mse: 14719453.0000 - mae: 1133.2446 - val_loss: 1291.6146 - val_mse: 13671515.0000 - val_mae: 1291.6146\n",
      "Epoch 251/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1131.0202 - mse: 14665053.0000 - mae: 1131.0201 - val_loss: 1287.0266 - val_mse: 13761151.0000 - val_mae: 1287.0266\n",
      "Epoch 252/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1133.4716 - mse: 14737683.0000 - mae: 1133.4716 - val_loss: 1287.0709 - val_mse: 13778149.0000 - val_mae: 1287.0709\n",
      "Epoch 253/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1126.3075 - mse: 14668618.0000 - mae: 1126.3075 - val_loss: 1292.6287 - val_mse: 13728316.0000 - val_mae: 1292.6287\n",
      "Epoch 254/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1126.6877 - mse: 14654649.0000 - mae: 1126.6877 - val_loss: 1293.8850 - val_mse: 13703443.0000 - val_mae: 1293.8850\n",
      "Epoch 255/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1126.7089 - mse: 14677724.0000 - mae: 1126.7089 - val_loss: 1297.7540 - val_mse: 13670481.0000 - val_mae: 1297.7540\n",
      "Epoch 256/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1125.1519 - mse: 14574287.0000 - mae: 1125.1519 - val_loss: 1286.4720 - val_mse: 13745887.0000 - val_mae: 1286.4720\n",
      "Epoch 257/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1128.0200 - mse: 14695521.0000 - mae: 1128.0201 - val_loss: 1292.4310 - val_mse: 13677245.0000 - val_mae: 1292.4310\n",
      "Epoch 258/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1125.2249 - mse: 14663557.0000 - mae: 1125.2250 - val_loss: 1291.3243 - val_mse: 13688657.0000 - val_mae: 1291.3243\n",
      "Epoch 259/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1129.4067 - mse: 14632292.0000 - mae: 1129.4067 - val_loss: 1286.8799 - val_mse: 13704928.0000 - val_mae: 1286.8799\n",
      "Epoch 260/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1127.9281 - mse: 14547730.0000 - mae: 1127.9280 - val_loss: 1287.7863 - val_mse: 13720036.0000 - val_mae: 1287.7863\n",
      "Epoch 261/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1130.6439 - mse: 14689606.0000 - mae: 1130.6439 - val_loss: 1286.3419 - val_mse: 13713436.0000 - val_mae: 1286.3419\n",
      "Epoch 262/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1129.3022 - mse: 14707995.0000 - mae: 1129.3021 - val_loss: 1288.2496 - val_mse: 13714040.0000 - val_mae: 1288.2496\n",
      "Epoch 263/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1127.9038 - mse: 14617107.0000 - mae: 1127.9038 - val_loss: 1286.4392 - val_mse: 13698755.0000 - val_mae: 1286.4392\n",
      "Epoch 264/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1131.4314 - mse: 14730065.0000 - mae: 1131.4314 - val_loss: 1289.3750 - val_mse: 13727280.0000 - val_mae: 1289.3750\n",
      "Epoch 265/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1133.9978 - mse: 14704089.0000 - mae: 1133.9978 - val_loss: 1288.8265 - val_mse: 13740157.0000 - val_mae: 1288.8265\n",
      "Epoch 266/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1127.2947 - mse: 14643069.0000 - mae: 1127.2948 - val_loss: 1287.8481 - val_mse: 13795640.0000 - val_mae: 1287.8481\n",
      "Epoch 267/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1130.2743 - mse: 14712989.0000 - mae: 1130.2743 - val_loss: 1290.9043 - val_mse: 13780199.0000 - val_mae: 1290.9043\n",
      "Epoch 268/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1128.2513 - mse: 14743144.0000 - mae: 1128.2513 - val_loss: 1291.6240 - val_mse: 13781802.0000 - val_mae: 1291.6240\n",
      "Epoch 269/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1127.8534 - mse: 14533188.0000 - mae: 1127.8534 - val_loss: 1288.7495 - val_mse: 13781875.0000 - val_mae: 1288.7495\n",
      "Epoch 270/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1127.2659 - mse: 14632118.0000 - mae: 1127.2659 - val_loss: 1295.0381 - val_mse: 13797182.0000 - val_mae: 1295.0381\n",
      "Epoch 271/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1130.5904 - mse: 14680423.0000 - mae: 1130.5905 - val_loss: 1289.9258 - val_mse: 13832554.0000 - val_mae: 1289.9258\n",
      "Epoch 272/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1128.7017 - mse: 14582743.0000 - mae: 1128.7017 - val_loss: 1290.0228 - val_mse: 13804261.0000 - val_mae: 1290.0228\n",
      "Epoch 273/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1127.9740 - mse: 14609968.0000 - mae: 1127.9740 - val_loss: 1290.5728 - val_mse: 13777833.0000 - val_mae: 1290.5728\n",
      "Epoch 274/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1129.9015 - mse: 14726283.0000 - mae: 1129.9014 - val_loss: 1292.0159 - val_mse: 13814822.0000 - val_mae: 1292.0159\n",
      "Epoch 275/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1127.6073 - mse: 14599800.0000 - mae: 1127.6073 - val_loss: 1290.4255 - val_mse: 13806896.0000 - val_mae: 1290.4255\n",
      "Epoch 276/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1127.6682 - mse: 14701861.0000 - mae: 1127.6682 - val_loss: 1291.2776 - val_mse: 13736541.0000 - val_mae: 1291.2776\n",
      "Epoch 277/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1127.1510 - mse: 14660882.0000 - mae: 1127.1510 - val_loss: 1291.4260 - val_mse: 13749187.0000 - val_mae: 1291.4260\n",
      "Epoch 278/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1134.0544 - mse: 14672575.0000 - mae: 1134.0544 - val_loss: 1295.1481 - val_mse: 13826501.0000 - val_mae: 1295.1481\n",
      "Epoch 279/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1124.7823 - mse: 14506983.0000 - mae: 1124.7823 - val_loss: 1292.3191 - val_mse: 13798647.0000 - val_mae: 1292.3191\n",
      "Epoch 280/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1128.7917 - mse: 14656786.0000 - mae: 1128.7917 - val_loss: 1292.9927 - val_mse: 13832554.0000 - val_mae: 1292.9927\n",
      "Epoch 281/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1125.3199 - mse: 14583292.0000 - mae: 1125.3198 - val_loss: 1291.9498 - val_mse: 13860350.0000 - val_mae: 1291.9498\n",
      "Epoch 282/500\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 1127.5077 - mse: 14591990.0000 - mae: 1127.5077 - val_loss: 1294.9585 - val_mse: 13808667.0000 - val_mae: 1294.9585\n",
      "Epoch 283/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1130.0267 - mse: 14641358.0000 - mae: 1130.0266 - val_loss: 1294.7588 - val_mse: 13875593.0000 - val_mae: 1294.7588\n",
      "Epoch 284/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1131.9265 - mse: 14785015.0000 - mae: 1131.9265 - val_loss: 1296.2410 - val_mse: 13920538.0000 - val_mae: 1296.2410\n",
      "Epoch 285/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1129.6498 - mse: 14682749.0000 - mae: 1129.6498 - val_loss: 1300.2280 - val_mse: 13884707.0000 - val_mae: 1300.2280\n",
      "Epoch 286/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1126.2756 - mse: 14471548.0000 - mae: 1126.2756 - val_loss: 1295.3887 - val_mse: 13954452.0000 - val_mae: 1295.3887\n",
      "Epoch 287/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1125.0888 - mse: 14482104.0000 - mae: 1125.0887 - val_loss: 1295.2600 - val_mse: 14005838.0000 - val_mae: 1295.2600\n",
      "Epoch 288/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1124.4362 - mse: 14587467.0000 - mae: 1124.4362 - val_loss: 1294.6661 - val_mse: 13898371.0000 - val_mae: 1294.6661\n",
      "Epoch 289/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1123.7702 - mse: 14623277.0000 - mae: 1123.7701 - val_loss: 1294.8015 - val_mse: 13868491.0000 - val_mae: 1294.8015\n",
      "Epoch 290/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1130.2852 - mse: 14616950.0000 - mae: 1130.2852 - val_loss: 1293.5295 - val_mse: 13896012.0000 - val_mae: 1293.5295\n",
      "Epoch 291/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1126.2324 - mse: 14512991.0000 - mae: 1126.2324 - val_loss: 1294.3750 - val_mse: 13835310.0000 - val_mae: 1294.3750\n",
      "Epoch 292/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1122.3690 - mse: 14538909.0000 - mae: 1122.3690 - val_loss: 1294.9336 - val_mse: 13859178.0000 - val_mae: 1294.9336\n",
      "Epoch 293/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1129.4665 - mse: 14595868.0000 - mae: 1129.4666 - val_loss: 1295.5129 - val_mse: 13936997.0000 - val_mae: 1295.5129\n",
      "Epoch 294/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1124.3831 - mse: 14468329.0000 - mae: 1124.3831 - val_loss: 1294.5422 - val_mse: 13875167.0000 - val_mae: 1294.5422\n",
      "Epoch 295/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1126.3685 - mse: 14649568.0000 - mae: 1126.3687 - val_loss: 1296.4933 - val_mse: 13910687.0000 - val_mae: 1296.4933\n",
      "Epoch 296/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1128.1419 - mse: 14589675.0000 - mae: 1128.1420 - val_loss: 1296.7600 - val_mse: 13934866.0000 - val_mae: 1296.7600\n",
      "Epoch 297/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1128.0291 - mse: 14590371.0000 - mae: 1128.0291 - val_loss: 1298.5038 - val_mse: 13917230.0000 - val_mae: 1298.5038\n",
      "Epoch 298/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1128.7042 - mse: 14619946.0000 - mae: 1128.7042 - val_loss: 1298.1335 - val_mse: 13905689.0000 - val_mae: 1298.1335\n",
      "Epoch 299/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1129.6509 - mse: 14631866.0000 - mae: 1129.6509 - val_loss: 1297.3372 - val_mse: 13936616.0000 - val_mae: 1297.3372\n",
      "Epoch 300/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1124.4918 - mse: 14584514.0000 - mae: 1124.4918 - val_loss: 1298.7275 - val_mse: 13989838.0000 - val_mae: 1298.7275\n",
      "Epoch 301/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1127.4522 - mse: 14636732.0000 - mae: 1127.4521 - val_loss: 1300.1888 - val_mse: 13906189.0000 - val_mae: 1300.1888\n",
      "Epoch 302/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1124.0664 - mse: 14466725.0000 - mae: 1124.0665 - val_loss: 1300.1791 - val_mse: 14042371.0000 - val_mae: 1300.1791\n",
      "Epoch 303/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1125.8020 - mse: 14576891.0000 - mae: 1125.8020 - val_loss: 1300.4473 - val_mse: 13953119.0000 - val_mae: 1300.4473\n",
      "Epoch 304/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1126.6738 - mse: 14586561.0000 - mae: 1126.6738 - val_loss: 1299.9487 - val_mse: 13990420.0000 - val_mae: 1299.9487\n",
      "Epoch 305/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1126.7217 - mse: 14549187.0000 - mae: 1126.7216 - val_loss: 1301.0546 - val_mse: 13943116.0000 - val_mae: 1301.0546\n",
      "Epoch 306/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1126.3970 - mse: 14579460.0000 - mae: 1126.3970 - val_loss: 1299.8247 - val_mse: 13982581.0000 - val_mae: 1299.8247\n",
      "Epoch 307/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1128.0807 - mse: 14485529.0000 - mae: 1128.0807 - val_loss: 1299.1133 - val_mse: 13985022.0000 - val_mae: 1299.1133\n",
      "Epoch 308/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1127.1559 - mse: 14607840.0000 - mae: 1127.1559 - val_loss: 1298.8907 - val_mse: 13919841.0000 - val_mae: 1298.8907\n",
      "Epoch 309/500\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 1127.9577 - mse: 14561003.0000 - mae: 1127.9576 - val_loss: 1298.6053 - val_mse: 13895361.0000 - val_mae: 1298.6053\n",
      "Epoch 310/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1128.2816 - mse: 14537830.0000 - mae: 1128.2816 - val_loss: 1299.8954 - val_mse: 13983415.0000 - val_mae: 1299.8954\n",
      "Epoch 311/500\n",
      "2500/2500 [==============================] - 0s 39us/step - loss: 1122.5861 - mse: 14565817.0000 - mae: 1122.5861 - val_loss: 1300.2946 - val_mse: 13910559.0000 - val_mae: 1300.2946\n",
      "Epoch 312/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1123.8581 - mse: 14437948.0000 - mae: 1123.8582 - val_loss: 1300.8510 - val_mse: 13954951.0000 - val_mae: 1300.8510\n",
      "Epoch 313/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1126.3904 - mse: 14567627.0000 - mae: 1126.3904 - val_loss: 1301.6270 - val_mse: 14012036.0000 - val_mae: 1301.6270\n",
      "Epoch 314/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1122.7260 - mse: 14456753.0000 - mae: 1122.7260 - val_loss: 1301.4115 - val_mse: 13996239.0000 - val_mae: 1301.4115\n",
      "Epoch 315/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1124.6717 - mse: 14514378.0000 - mae: 1124.6716 - val_loss: 1301.7769 - val_mse: 13996662.0000 - val_mae: 1301.7769\n",
      "Epoch 316/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1123.1003 - mse: 14562145.0000 - mae: 1123.1003 - val_loss: 1300.8879 - val_mse: 13981968.0000 - val_mae: 1300.8879\n",
      "Epoch 317/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1127.9273 - mse: 14607345.0000 - mae: 1127.9272 - val_loss: 1302.0851 - val_mse: 13996853.0000 - val_mae: 1302.0851\n",
      "Epoch 318/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1123.5716 - mse: 14556217.0000 - mae: 1123.5717 - val_loss: 1301.5044 - val_mse: 13963112.0000 - val_mae: 1301.5044\n",
      "Epoch 319/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1126.8489 - mse: 14448375.0000 - mae: 1126.8488 - val_loss: 1301.4849 - val_mse: 13999105.0000 - val_mae: 1301.4849\n",
      "Epoch 320/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1122.3490 - mse: 14610763.0000 - mae: 1122.3490 - val_loss: 1301.8004 - val_mse: 13838206.0000 - val_mae: 1301.8004\n",
      "Epoch 321/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1122.4359 - mse: 14402273.0000 - mae: 1122.4360 - val_loss: 1299.3187 - val_mse: 13907319.0000 - val_mae: 1299.3187\n",
      "Epoch 322/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1120.5892 - mse: 14497908.0000 - mae: 1120.5892 - val_loss: 1301.6748 - val_mse: 13995121.0000 - val_mae: 1301.6748\n",
      "Epoch 323/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1124.1246 - mse: 14480404.0000 - mae: 1124.1245 - val_loss: 1302.4174 - val_mse: 13999566.0000 - val_mae: 1302.4174\n",
      "Epoch 324/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1126.1565 - mse: 14437427.0000 - mae: 1126.1564 - val_loss: 1301.8403 - val_mse: 13988490.0000 - val_mae: 1301.8403\n",
      "Epoch 325/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1127.8253 - mse: 14589288.0000 - mae: 1127.8254 - val_loss: 1301.5020 - val_mse: 13959888.0000 - val_mae: 1301.5020\n",
      "Epoch 326/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1123.6617 - mse: 14569098.0000 - mae: 1123.6617 - val_loss: 1301.3453 - val_mse: 13952291.0000 - val_mae: 1301.3453\n",
      "Epoch 327/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1127.0957 - mse: 14460717.0000 - mae: 1127.0957 - val_loss: 1303.3097 - val_mse: 14052689.0000 - val_mae: 1303.3097\n",
      "Epoch 328/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1121.2127 - mse: 14407002.0000 - mae: 1121.2126 - val_loss: 1302.4620 - val_mse: 13981425.0000 - val_mae: 1302.4620\n",
      "Epoch 329/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1124.0626 - mse: 14550596.0000 - mae: 1124.0626 - val_loss: 1302.8239 - val_mse: 13993750.0000 - val_mae: 1302.8239\n",
      "Epoch 330/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1120.9319 - mse: 14457876.0000 - mae: 1120.9320 - val_loss: 1302.1061 - val_mse: 13956255.0000 - val_mae: 1302.1061\n",
      "Epoch 331/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1125.4412 - mse: 14484899.0000 - mae: 1125.4412 - val_loss: 1302.3599 - val_mse: 13977572.0000 - val_mae: 1302.3599\n",
      "Epoch 332/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1123.7487 - mse: 14527414.0000 - mae: 1123.7487 - val_loss: 1302.3856 - val_mse: 13989039.0000 - val_mae: 1302.3856\n",
      "Epoch 333/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1127.3958 - mse: 14476232.0000 - mae: 1127.3958 - val_loss: 1302.6708 - val_mse: 13933834.0000 - val_mae: 1302.6708\n",
      "Epoch 334/500\n",
      "2500/2500 [==============================] - 0s 37us/step - loss: 1120.1878 - mse: 14400065.0000 - mae: 1120.1877 - val_loss: 1303.6576 - val_mse: 13994756.0000 - val_mae: 1303.6576\n",
      "Epoch 335/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1121.1414 - mse: 14336229.0000 - mae: 1121.1414 - val_loss: 1303.1949 - val_mse: 14002925.0000 - val_mae: 1303.1949\n",
      "Epoch 336/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1124.3386 - mse: 14501218.0000 - mae: 1124.3386 - val_loss: 1304.4650 - val_mse: 14057295.0000 - val_mae: 1304.4650\n",
      "Epoch 337/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1127.1100 - mse: 14605294.0000 - mae: 1127.1101 - val_loss: 1304.0037 - val_mse: 13968965.0000 - val_mae: 1304.0037\n",
      "Epoch 338/500\n",
      "2500/2500 [==============================] - 0s 51us/step - loss: 1126.6437 - mse: 14475521.0000 - mae: 1126.6437 - val_loss: 1304.0223 - val_mse: 14016971.0000 - val_mae: 1304.0223\n",
      "Epoch 339/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1118.9703 - mse: 14407279.0000 - mae: 1118.9705 - val_loss: 1304.0023 - val_mse: 13979816.0000 - val_mae: 1304.0023\n",
      "Epoch 340/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1119.9329 - mse: 14343817.0000 - mae: 1119.9329 - val_loss: 1304.5193 - val_mse: 14023851.0000 - val_mae: 1304.5193\n",
      "Epoch 341/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1119.5064 - mse: 14402499.0000 - mae: 1119.5063 - val_loss: 1303.3190 - val_mse: 13923154.0000 - val_mae: 1303.3190\n",
      "Epoch 342/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1121.9781 - mse: 14453219.0000 - mae: 1121.9781 - val_loss: 1304.9597 - val_mse: 14019980.0000 - val_mae: 1304.9597\n",
      "Epoch 343/500\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 1129.7714 - mse: 14573462.0000 - mae: 1129.7714 - val_loss: 1304.6392 - val_mse: 13966178.0000 - val_mae: 1304.6392\n",
      "Epoch 344/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1120.3481 - mse: 14339424.0000 - mae: 1120.3481 - val_loss: 1305.6182 - val_mse: 14026995.0000 - val_mae: 1305.6182\n",
      "Epoch 345/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1127.8743 - mse: 14479055.0000 - mae: 1127.8741 - val_loss: 1305.8801 - val_mse: 14035711.0000 - val_mae: 1305.8801\n",
      "Epoch 346/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1124.0803 - mse: 14359241.0000 - mae: 1124.0803 - val_loss: 1304.5929 - val_mse: 13915642.0000 - val_mae: 1304.5929\n",
      "Epoch 347/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1125.4458 - mse: 14488032.0000 - mae: 1125.4458 - val_loss: 1306.6631 - val_mse: 14019104.0000 - val_mae: 1306.6631\n",
      "Epoch 348/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1127.8558 - mse: 14456549.0000 - mae: 1127.8557 - val_loss: 1306.2395 - val_mse: 14009809.0000 - val_mae: 1306.2395\n",
      "Epoch 349/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1122.7608 - mse: 14380384.0000 - mae: 1122.7607 - val_loss: 1306.0938 - val_mse: 14003442.0000 - val_mae: 1306.0938\n",
      "Epoch 350/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1119.8010 - mse: 14392318.0000 - mae: 1119.8010 - val_loss: 1306.3230 - val_mse: 14009049.0000 - val_mae: 1306.3230\n",
      "Epoch 351/500\n",
      "2500/2500 [==============================] - 0s 52us/step - loss: 1122.3032 - mse: 14378249.0000 - mae: 1122.3033 - val_loss: 1305.6437 - val_mse: 13947463.0000 - val_mae: 1305.6437\n",
      "Epoch 352/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1124.2350 - mse: 14464998.0000 - mae: 1124.2350 - val_loss: 1306.7285 - val_mse: 13971418.0000 - val_mae: 1306.7285\n",
      "Epoch 353/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1123.4164 - mse: 14516476.0000 - mae: 1123.4164 - val_loss: 1307.2676 - val_mse: 13973645.0000 - val_mae: 1307.2676\n",
      "Epoch 354/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1127.3680 - mse: 14540659.0000 - mae: 1127.3680 - val_loss: 1307.4475 - val_mse: 13996683.0000 - val_mae: 1307.4475\n",
      "Epoch 355/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1122.7419 - mse: 14492543.0000 - mae: 1122.7419 - val_loss: 1308.1731 - val_mse: 14042653.0000 - val_mae: 1308.1731\n",
      "Epoch 356/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1120.8870 - mse: 14347780.0000 - mae: 1120.8871 - val_loss: 1308.6251 - val_mse: 14025592.0000 - val_mae: 1308.6251\n",
      "Epoch 357/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1124.8892 - mse: 14390154.0000 - mae: 1124.8892 - val_loss: 1308.6313 - val_mse: 14023083.0000 - val_mae: 1308.6313\n",
      "Epoch 358/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1123.9562 - mse: 14451697.0000 - mae: 1123.9563 - val_loss: 1308.8071 - val_mse: 14057814.0000 - val_mae: 1308.8071\n",
      "Epoch 359/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1125.5676 - mse: 14502017.0000 - mae: 1125.5676 - val_loss: 1309.1053 - val_mse: 14092624.0000 - val_mae: 1309.1053\n",
      "Epoch 360/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1126.8136 - mse: 14523243.0000 - mae: 1126.8136 - val_loss: 1309.7917 - val_mse: 14114045.0000 - val_mae: 1309.7917\n",
      "Epoch 361/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1129.0263 - mse: 14563793.0000 - mae: 1129.0264 - val_loss: 1309.8239 - val_mse: 14077757.0000 - val_mae: 1309.8239\n",
      "Epoch 362/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1128.9179 - mse: 14508019.0000 - mae: 1128.9178 - val_loss: 1311.3527 - val_mse: 14105467.0000 - val_mae: 1311.3527\n",
      "Epoch 363/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1122.1492 - mse: 14327549.0000 - mae: 1122.1492 - val_loss: 1311.3855 - val_mse: 14102788.0000 - val_mae: 1311.3855\n",
      "Epoch 364/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1119.1329 - mse: 14360327.0000 - mae: 1119.1331 - val_loss: 1311.4615 - val_mse: 14099634.0000 - val_mae: 1311.4615\n",
      "Epoch 365/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1124.7699 - mse: 14532629.0000 - mae: 1124.7700 - val_loss: 1312.4999 - val_mse: 14144670.0000 - val_mae: 1312.4999\n",
      "Epoch 366/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1116.9698 - mse: 14356033.0000 - mae: 1116.9698 - val_loss: 1310.5775 - val_mse: 13992260.0000 - val_mae: 1310.5775\n",
      "Epoch 367/500\n",
      "2500/2500 [==============================] - 0s 40us/step - loss: 1125.3401 - mse: 14410655.0000 - mae: 1125.3401 - val_loss: 1311.2737 - val_mse: 14050340.0000 - val_mae: 1311.2737\n",
      "Epoch 368/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1116.6750 - mse: 14332228.0000 - mae: 1116.6750 - val_loss: 1311.5724 - val_mse: 14060163.0000 - val_mae: 1311.5724\n",
      "Epoch 369/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1121.7004 - mse: 14385086.0000 - mae: 1121.7004 - val_loss: 1311.5604 - val_mse: 14083185.0000 - val_mae: 1311.5604\n",
      "Epoch 370/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1120.9523 - mse: 14313552.0000 - mae: 1120.9523 - val_loss: 1311.4799 - val_mse: 14094529.0000 - val_mae: 1311.4799\n",
      "Epoch 371/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1117.9829 - mse: 14224037.0000 - mae: 1117.9829 - val_loss: 1310.2527 - val_mse: 13992248.0000 - val_mae: 1310.2527\n",
      "Epoch 372/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1125.2314 - mse: 14392644.0000 - mae: 1125.2314 - val_loss: 1309.9514 - val_mse: 14005016.0000 - val_mae: 1309.9514\n",
      "Epoch 373/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1122.5363 - mse: 14405891.0000 - mae: 1122.5363 - val_loss: 1311.6337 - val_mse: 14082893.0000 - val_mae: 1311.6337\n",
      "Epoch 374/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1124.4545 - mse: 14531569.0000 - mae: 1124.4545 - val_loss: 1312.5337 - val_mse: 14093873.0000 - val_mae: 1312.5337\n",
      "Epoch 375/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1118.5775 - mse: 14349753.0000 - mae: 1118.5776 - val_loss: 1311.9178 - val_mse: 14080783.0000 - val_mae: 1311.9178\n",
      "Epoch 376/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1117.3621 - mse: 14313290.0000 - mae: 1117.3621 - val_loss: 1314.1554 - val_mse: 14152001.0000 - val_mae: 1314.1554\n",
      "Epoch 377/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1125.2840 - mse: 14528723.0000 - mae: 1125.2841 - val_loss: 1314.0464 - val_mse: 14122178.0000 - val_mae: 1314.0464\n",
      "Epoch 378/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1122.7656 - mse: 14520987.0000 - mae: 1122.7657 - val_loss: 1313.9410 - val_mse: 14082256.0000 - val_mae: 1313.9410\n",
      "Epoch 379/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1128.0730 - mse: 14547922.0000 - mae: 1128.0731 - val_loss: 1314.9071 - val_mse: 14120904.0000 - val_mae: 1314.9071\n",
      "Epoch 380/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1122.8704 - mse: 14380311.0000 - mae: 1122.8704 - val_loss: 1315.0869 - val_mse: 14097662.0000 - val_mae: 1315.0869\n",
      "Epoch 381/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1125.5810 - mse: 14494826.0000 - mae: 1125.5811 - val_loss: 1314.3613 - val_mse: 14040371.0000 - val_mae: 1314.3613\n",
      "Epoch 382/500\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 1121.9558 - mse: 14508525.0000 - mae: 1121.9558 - val_loss: 1314.9064 - val_mse: 14074013.0000 - val_mae: 1314.9064\n",
      "Epoch 383/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1123.7265 - mse: 14385363.0000 - mae: 1123.7264 - val_loss: 1315.6400 - val_mse: 14102767.0000 - val_mae: 1315.6400\n",
      "Epoch 384/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1125.9032 - mse: 14535082.0000 - mae: 1125.9033 - val_loss: 1317.9625 - val_mse: 14243929.0000 - val_mae: 1317.9625\n",
      "Epoch 385/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1125.1416 - mse: 14520739.0000 - mae: 1125.1416 - val_loss: 1315.5048 - val_mse: 14151050.0000 - val_mae: 1315.5048\n",
      "Epoch 386/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1125.0028 - mse: 14494248.0000 - mae: 1125.0029 - val_loss: 1314.1921 - val_mse: 14076869.0000 - val_mae: 1314.1921\n",
      "Epoch 387/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1124.0802 - mse: 14488025.0000 - mae: 1124.0802 - val_loss: 1313.7869 - val_mse: 14084747.0000 - val_mae: 1313.7869\n",
      "Epoch 388/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1124.0850 - mse: 14489163.0000 - mae: 1124.0850 - val_loss: 1313.9250 - val_mse: 14064069.0000 - val_mae: 1313.9250\n",
      "Epoch 389/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1117.0244 - mse: 14256713.0000 - mae: 1117.0244 - val_loss: 1316.0103 - val_mse: 14151760.0000 - val_mae: 1316.0103\n",
      "Epoch 390/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1121.7956 - mse: 14430698.0000 - mae: 1121.7957 - val_loss: 1314.3143 - val_mse: 14107774.0000 - val_mae: 1314.3143\n",
      "Epoch 391/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1115.5300 - mse: 14396064.0000 - mae: 1115.5300 - val_loss: 1312.9849 - val_mse: 14055083.0000 - val_mae: 1312.9849\n",
      "Epoch 392/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1119.7785 - mse: 14167107.0000 - mae: 1119.7784 - val_loss: 1313.8872 - val_mse: 14102542.0000 - val_mae: 1313.8872\n",
      "Epoch 393/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1122.9590 - mse: 14412153.0000 - mae: 1122.9589 - val_loss: 1312.9207 - val_mse: 14031721.0000 - val_mae: 1312.9207\n",
      "Epoch 394/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1120.2150 - mse: 14394576.0000 - mae: 1120.2150 - val_loss: 1311.4403 - val_mse: 13996478.0000 - val_mae: 1311.4403\n",
      "Epoch 395/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1121.6057 - mse: 14296049.0000 - mae: 1121.6056 - val_loss: 1312.3574 - val_mse: 14036365.0000 - val_mae: 1312.3574\n",
      "Epoch 396/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1116.2394 - mse: 14409481.0000 - mae: 1116.2394 - val_loss: 1312.7584 - val_mse: 14024966.0000 - val_mae: 1312.7584\n",
      "Epoch 397/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1129.3492 - mse: 14370952.0000 - mae: 1129.3491 - val_loss: 1314.6721 - val_mse: 14097020.0000 - val_mae: 1314.6721\n",
      "Epoch 398/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1117.8047 - mse: 14208726.0000 - mae: 1117.8048 - val_loss: 1313.7120 - val_mse: 14021115.0000 - val_mae: 1313.7120\n",
      "Epoch 399/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1119.2431 - mse: 14250277.0000 - mae: 1119.2432 - val_loss: 1313.5574 - val_mse: 14043810.0000 - val_mae: 1313.5574\n",
      "Epoch 400/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1120.0585 - mse: 14280188.0000 - mae: 1120.0585 - val_loss: 1313.9875 - val_mse: 14035169.0000 - val_mae: 1313.9875\n",
      "Epoch 401/500\n",
      "2500/2500 [==============================] - 0s 35us/step - loss: 1123.6052 - mse: 14381099.0000 - mae: 1123.6052 - val_loss: 1314.5608 - val_mse: 14041657.0000 - val_mae: 1314.5608\n",
      "Epoch 402/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1123.0747 - mse: 14408403.0000 - mae: 1123.0746 - val_loss: 1315.7535 - val_mse: 14093489.0000 - val_mae: 1315.7535\n",
      "Epoch 403/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1121.5838 - mse: 14515898.0000 - mae: 1121.5837 - val_loss: 1315.9514 - val_mse: 14094137.0000 - val_mae: 1315.9514\n",
      "Epoch 404/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1124.1520 - mse: 14326927.0000 - mae: 1124.1520 - val_loss: 1317.3915 - val_mse: 14198640.0000 - val_mae: 1317.3915\n",
      "Epoch 405/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1120.7818 - mse: 14454658.0000 - mae: 1120.7819 - val_loss: 1315.9423 - val_mse: 14078079.0000 - val_mae: 1315.9423\n",
      "Epoch 406/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1121.4924 - mse: 14246083.0000 - mae: 1121.4924 - val_loss: 1316.4739 - val_mse: 14109663.0000 - val_mae: 1316.4739\n",
      "Epoch 407/500\n",
      "2500/2500 [==============================] - 0s 38us/step - loss: 1120.5801 - mse: 14315781.0000 - mae: 1120.5801 - val_loss: 1316.0787 - val_mse: 14069096.0000 - val_mae: 1316.0787\n",
      "Epoch 408/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1128.4068 - mse: 14360900.0000 - mae: 1128.4069 - val_loss: 1317.4446 - val_mse: 14116003.0000 - val_mae: 1317.4446\n",
      "Epoch 409/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1118.0183 - mse: 14413778.0000 - mae: 1118.0183 - val_loss: 1317.9806 - val_mse: 14136463.0000 - val_mae: 1317.9806\n",
      "Epoch 410/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1119.5810 - mse: 14200744.0000 - mae: 1119.5811 - val_loss: 1318.0984 - val_mse: 14112530.0000 - val_mae: 1318.0984\n",
      "Epoch 411/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1117.8632 - mse: 14353456.0000 - mae: 1117.8632 - val_loss: 1317.7372 - val_mse: 14051867.0000 - val_mae: 1317.7372\n",
      "Epoch 412/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1117.9527 - mse: 14165624.0000 - mae: 1117.9528 - val_loss: 1319.0616 - val_mse: 14141001.0000 - val_mae: 1319.0616\n",
      "Epoch 413/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1117.3218 - mse: 14296007.0000 - mae: 1117.3219 - val_loss: 1318.2225 - val_mse: 14101493.0000 - val_mae: 1318.2225\n",
      "Epoch 414/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1124.0910 - mse: 14358834.0000 - mae: 1124.0909 - val_loss: 1317.9918 - val_mse: 14126847.0000 - val_mae: 1317.9918\n",
      "Epoch 415/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1116.1831 - mse: 14161196.0000 - mae: 1116.1831 - val_loss: 1317.8461 - val_mse: 14119079.0000 - val_mae: 1317.8461\n",
      "Epoch 416/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1123.7972 - mse: 14432656.0000 - mae: 1123.7972 - val_loss: 1317.1949 - val_mse: 14088553.0000 - val_mae: 1317.1949\n",
      "Epoch 417/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1116.1131 - mse: 14219587.0000 - mae: 1116.1130 - val_loss: 1318.2933 - val_mse: 14112290.0000 - val_mae: 1318.2933\n",
      "Epoch 418/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1119.7514 - mse: 14373347.0000 - mae: 1119.7513 - val_loss: 1318.9341 - val_mse: 14086690.0000 - val_mae: 1318.9341\n",
      "Epoch 419/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1119.0927 - mse: 14214370.0000 - mae: 1119.0927 - val_loss: 1318.0780 - val_mse: 14101037.0000 - val_mae: 1318.0780\n",
      "Epoch 420/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1123.7957 - mse: 14263193.0000 - mae: 1123.7957 - val_loss: 1318.6703 - val_mse: 14099511.0000 - val_mae: 1318.6703\n",
      "Epoch 421/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1121.5346 - mse: 14347973.0000 - mae: 1121.5345 - val_loss: 1318.9060 - val_mse: 14117466.0000 - val_mae: 1318.9060\n",
      "Epoch 422/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1121.2454 - mse: 14295373.0000 - mae: 1121.2454 - val_loss: 1319.4000 - val_mse: 14122557.0000 - val_mae: 1319.4000\n",
      "Epoch 423/500\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 1127.6249 - mse: 14477774.0000 - mae: 1127.6248 - val_loss: 1320.3783 - val_mse: 14120801.0000 - val_mae: 1320.3783\n",
      "Epoch 424/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1111.7910 - mse: 14106214.0000 - mae: 1111.7909 - val_loss: 1319.0822 - val_mse: 14082355.0000 - val_mae: 1319.0822\n",
      "Epoch 425/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1115.9421 - mse: 14301644.0000 - mae: 1115.9421 - val_loss: 1319.0731 - val_mse: 14094645.0000 - val_mae: 1319.0731\n",
      "Epoch 426/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1120.1129 - mse: 14328798.0000 - mae: 1120.1130 - val_loss: 1319.6460 - val_mse: 14149728.0000 - val_mae: 1319.6460\n",
      "Epoch 427/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1120.4511 - mse: 14359770.0000 - mae: 1120.4510 - val_loss: 1318.9911 - val_mse: 14111976.0000 - val_mae: 1318.9911\n",
      "Epoch 428/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1121.4072 - mse: 14329754.0000 - mae: 1121.4071 - val_loss: 1319.9546 - val_mse: 14135246.0000 - val_mae: 1319.9546\n",
      "Epoch 429/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1119.5742 - mse: 14241864.0000 - mae: 1119.5743 - val_loss: 1319.4569 - val_mse: 14079964.0000 - val_mae: 1319.4569\n",
      "Epoch 430/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1121.7803 - mse: 14229001.0000 - mae: 1121.7803 - val_loss: 1318.8879 - val_mse: 14050428.0000 - val_mae: 1318.8879\n",
      "Epoch 431/500\n",
      "2500/2500 [==============================] - 0s 53us/step - loss: 1115.6955 - mse: 14287467.0000 - mae: 1115.6956 - val_loss: 1319.2424 - val_mse: 14087674.0000 - val_mae: 1319.2424\n",
      "Epoch 432/500\n",
      "2500/2500 [==============================] - 0s 51us/step - loss: 1122.0411 - mse: 14251432.0000 - mae: 1122.0411 - val_loss: 1320.8181 - val_mse: 14121047.0000 - val_mae: 1320.8181\n",
      "Epoch 433/500\n",
      "2500/2500 [==============================] - 0s 52us/step - loss: 1120.7134 - mse: 14297290.0000 - mae: 1120.7134 - val_loss: 1322.2721 - val_mse: 14166401.0000 - val_mae: 1322.2721\n",
      "Epoch 434/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1118.0665 - mse: 14310502.0000 - mae: 1118.0665 - val_loss: 1321.1688 - val_mse: 14093395.0000 - val_mae: 1321.1688\n",
      "Epoch 435/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1119.2447 - mse: 14228564.0000 - mae: 1119.2446 - val_loss: 1321.0685 - val_mse: 14099466.0000 - val_mae: 1321.0685\n",
      "Epoch 436/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1119.0132 - mse: 14197149.0000 - mae: 1119.0132 - val_loss: 1320.3329 - val_mse: 14079109.0000 - val_mae: 1320.3329\n",
      "Epoch 437/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1121.5553 - mse: 14305501.0000 - mae: 1121.5552 - val_loss: 1322.2888 - val_mse: 14137542.0000 - val_mae: 1322.2888\n",
      "Epoch 438/500\n",
      "2500/2500 [==============================] - 0s 51us/step - loss: 1117.0961 - mse: 14345196.0000 - mae: 1117.0959 - val_loss: 1321.9944 - val_mse: 14098222.0000 - val_mae: 1321.9944\n",
      "Epoch 439/500\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 1123.0624 - mse: 14348245.0000 - mae: 1123.0624 - val_loss: 1322.0327 - val_mse: 14133107.0000 - val_mae: 1322.0327\n",
      "Epoch 440/500\n",
      "2500/2500 [==============================] - 0s 43us/step - loss: 1118.6549 - mse: 14276762.0000 - mae: 1118.6550 - val_loss: 1321.8785 - val_mse: 14129956.0000 - val_mae: 1321.8785\n",
      "Epoch 441/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1114.3271 - mse: 14271785.0000 - mae: 1114.3270 - val_loss: 1321.2850 - val_mse: 14089565.0000 - val_mae: 1321.2850\n",
      "Epoch 442/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1116.3028 - mse: 14144707.0000 - mae: 1116.3029 - val_loss: 1322.0541 - val_mse: 14128538.0000 - val_mae: 1322.0541\n",
      "Epoch 443/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1114.5916 - mse: 14241635.0000 - mae: 1114.5916 - val_loss: 1322.2919 - val_mse: 14113896.0000 - val_mae: 1322.2919\n",
      "Epoch 444/500\n",
      "2500/2500 [==============================] - 0s 41us/step - loss: 1116.4875 - mse: 14319808.0000 - mae: 1116.4875 - val_loss: 1321.6398 - val_mse: 14104446.0000 - val_mae: 1321.6398\n",
      "Epoch 445/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1118.4365 - mse: 14235718.0000 - mae: 1118.4365 - val_loss: 1321.5670 - val_mse: 14081746.0000 - val_mae: 1321.5670\n",
      "Epoch 446/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1119.3384 - mse: 14288629.0000 - mae: 1119.3384 - val_loss: 1322.5994 - val_mse: 14122197.0000 - val_mae: 1322.5994\n",
      "Epoch 447/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1122.9869 - mse: 14222431.0000 - mae: 1122.9869 - val_loss: 1321.5699 - val_mse: 14088608.0000 - val_mae: 1321.5699\n",
      "Epoch 448/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1119.5811 - mse: 14288467.0000 - mae: 1119.5811 - val_loss: 1322.2290 - val_mse: 14131216.0000 - val_mae: 1322.2290\n",
      "Epoch 449/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1116.2861 - mse: 14119397.0000 - mae: 1116.2860 - val_loss: 1321.7092 - val_mse: 14131702.0000 - val_mae: 1321.7092\n",
      "Epoch 450/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1113.9046 - mse: 14108123.0000 - mae: 1113.9045 - val_loss: 1322.6621 - val_mse: 14132070.0000 - val_mae: 1322.6621\n",
      "Epoch 451/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1117.1135 - mse: 14219005.0000 - mae: 1117.1136 - val_loss: 1322.2365 - val_mse: 14135005.0000 - val_mae: 1322.2365\n",
      "Epoch 452/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1113.3389 - mse: 14152357.0000 - mae: 1113.3389 - val_loss: 1321.2484 - val_mse: 14104987.0000 - val_mae: 1321.2484\n",
      "Epoch 453/500\n",
      "2500/2500 [==============================] - 0s 42us/step - loss: 1119.1291 - mse: 14223671.0000 - mae: 1119.1292 - val_loss: 1321.2371 - val_mse: 14076114.0000 - val_mae: 1321.2371\n",
      "Epoch 454/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1110.2006 - mse: 14022561.0000 - mae: 1110.2007 - val_loss: 1322.3158 - val_mse: 14126291.0000 - val_mae: 1322.3158\n",
      "Epoch 455/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1123.0861 - mse: 14386720.0000 - mae: 1123.0861 - val_loss: 1323.3263 - val_mse: 14156213.0000 - val_mae: 1323.3263\n",
      "Epoch 456/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1122.7661 - mse: 14338990.0000 - mae: 1122.7661 - val_loss: 1322.7142 - val_mse: 14118076.0000 - val_mae: 1322.7142\n",
      "Epoch 457/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1123.3698 - mse: 14132106.0000 - mae: 1123.3698 - val_loss: 1323.3899 - val_mse: 14125529.0000 - val_mae: 1323.3899\n",
      "Epoch 458/500\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 1115.6252 - mse: 14210110.0000 - mae: 1115.6252 - val_loss: 1323.5339 - val_mse: 14104865.0000 - val_mae: 1323.5339\n",
      "Epoch 459/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1122.0875 - mse: 14195601.0000 - mae: 1122.0875 - val_loss: 1324.5548 - val_mse: 14182779.0000 - val_mae: 1324.5548\n",
      "Epoch 460/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1110.0187 - mse: 14107025.0000 - mae: 1110.0188 - val_loss: 1324.4426 - val_mse: 14160686.0000 - val_mae: 1324.4426\n",
      "Epoch 461/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1118.8126 - mse: 14330171.0000 - mae: 1118.8125 - val_loss: 1324.3286 - val_mse: 14159511.0000 - val_mae: 1324.3286\n",
      "Epoch 462/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1123.7927 - mse: 14285396.0000 - mae: 1123.7927 - val_loss: 1323.7476 - val_mse: 14110521.0000 - val_mae: 1323.7476\n",
      "Epoch 463/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1119.3011 - mse: 14168847.0000 - mae: 1119.3011 - val_loss: 1324.4940 - val_mse: 14180177.0000 - val_mae: 1324.4940\n",
      "Epoch 464/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1114.7210 - mse: 14234878.0000 - mae: 1114.7209 - val_loss: 1323.8716 - val_mse: 14152988.0000 - val_mae: 1323.8716\n",
      "Epoch 465/500\n",
      "2500/2500 [==============================] - 0s 52us/step - loss: 1115.4273 - mse: 14190608.0000 - mae: 1115.4272 - val_loss: 1323.6128 - val_mse: 14137238.0000 - val_mae: 1323.6128\n",
      "Epoch 466/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1120.9093 - mse: 14124712.0000 - mae: 1120.9093 - val_loss: 1325.3779 - val_mse: 14199191.0000 - val_mae: 1325.3779\n",
      "Epoch 467/500\n",
      "2500/2500 [==============================] - 0s 51us/step - loss: 1115.8838 - mse: 14233125.0000 - mae: 1115.8838 - val_loss: 1325.1180 - val_mse: 14206591.0000 - val_mae: 1325.1180\n",
      "Epoch 468/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1119.9667 - mse: 14235101.0000 - mae: 1119.9667 - val_loss: 1324.0079 - val_mse: 14137222.0000 - val_mae: 1324.0079\n",
      "Epoch 469/500\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 1115.1804 - mse: 14151714.0000 - mae: 1115.1804 - val_loss: 1323.4424 - val_mse: 14097509.0000 - val_mae: 1323.4424\n",
      "Epoch 470/500\n",
      "2500/2500 [==============================] - 0s 45us/step - loss: 1115.6433 - mse: 14124302.0000 - mae: 1115.6433 - val_loss: 1323.7003 - val_mse: 14129023.0000 - val_mae: 1323.7003\n",
      "Epoch 471/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1110.2759 - mse: 14080377.0000 - mae: 1110.2758 - val_loss: 1323.5073 - val_mse: 14117500.0000 - val_mae: 1323.5073\n",
      "Epoch 472/500\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 1121.2936 - mse: 14210331.0000 - mae: 1121.2937 - val_loss: 1324.3445 - val_mse: 14149389.0000 - val_mae: 1324.3445\n",
      "Epoch 473/500\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 1116.0198 - mse: 14298907.0000 - mae: 1116.0199 - val_loss: 1323.4100 - val_mse: 14119244.0000 - val_mae: 1323.4100\n",
      "Epoch 474/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1117.9125 - mse: 14178176.0000 - mae: 1117.9125 - val_loss: 1322.5541 - val_mse: 14084431.0000 - val_mae: 1322.5541\n",
      "Epoch 475/500\n",
      "2500/2500 [==============================] - 0s 52us/step - loss: 1105.1212 - mse: 13981857.0000 - mae: 1105.1212 - val_loss: 1321.5645 - val_mse: 14014586.0000 - val_mae: 1321.5645\n",
      "Epoch 476/500\n",
      "2500/2500 [==============================] - 0s 46us/step - loss: 1114.6330 - mse: 14088860.0000 - mae: 1114.6331 - val_loss: 1323.8623 - val_mse: 14146684.0000 - val_mae: 1323.8623\n",
      "Epoch 477/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1119.2254 - mse: 14230992.0000 - mae: 1119.2255 - val_loss: 1322.6320 - val_mse: 14058515.0000 - val_mae: 1322.6320\n",
      "Epoch 478/500\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 1114.4436 - mse: 14103400.0000 - mae: 1114.4436 - val_loss: 1322.0383 - val_mse: 14036015.0000 - val_mae: 1322.0383\n",
      "Epoch 479/500\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 1117.4741 - mse: 14182343.0000 - mae: 1117.4741 - val_loss: 1322.2600 - val_mse: 14077471.0000 - val_mae: 1322.2600\n",
      "Epoch 480/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1115.6993 - mse: 14140251.0000 - mae: 1115.6993 - val_loss: 1321.5623 - val_mse: 13998255.0000 - val_mae: 1321.5623\n",
      "Epoch 481/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1115.2976 - mse: 14071484.0000 - mae: 1115.2976 - val_loss: 1322.9227 - val_mse: 14038715.0000 - val_mae: 1322.9227\n",
      "Epoch 482/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1113.8917 - mse: 14322919.0000 - mae: 1113.8918 - val_loss: 1323.9517 - val_mse: 14057581.0000 - val_mae: 1323.9517\n",
      "Epoch 483/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1117.6989 - mse: 14151500.0000 - mae: 1117.6989 - val_loss: 1323.4934 - val_mse: 14046205.0000 - val_mae: 1323.4934\n",
      "Epoch 484/500\n",
      "2500/2500 [==============================] - 0s 51us/step - loss: 1117.6940 - mse: 14111813.0000 - mae: 1117.6940 - val_loss: 1324.8258 - val_mse: 14100176.0000 - val_mae: 1324.8258\n",
      "Epoch 485/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1116.3849 - mse: 14208459.0000 - mae: 1116.3848 - val_loss: 1324.8591 - val_mse: 14114954.0000 - val_mae: 1324.8591\n",
      "Epoch 486/500\n",
      "2500/2500 [==============================] - 0s 44us/step - loss: 1116.9687 - mse: 14186503.0000 - mae: 1116.9688 - val_loss: 1324.8397 - val_mse: 14137475.0000 - val_mae: 1324.8397\n",
      "Epoch 487/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1116.7015 - mse: 14119043.0000 - mae: 1116.7014 - val_loss: 1326.5040 - val_mse: 14164717.0000 - val_mae: 1326.5040\n",
      "Epoch 488/500\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 1116.3091 - mse: 14118709.0000 - mae: 1116.3090 - val_loss: 1325.5420 - val_mse: 14102660.0000 - val_mae: 1325.5420\n",
      "Epoch 489/500\n",
      "2500/2500 [==============================] - 0s 52us/step - loss: 1121.7578 - mse: 14225228.0000 - mae: 1121.7578 - val_loss: 1325.2740 - val_mse: 14096495.0000 - val_mae: 1325.2740\n",
      "Epoch 490/500\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 1115.6984 - mse: 14102828.0000 - mae: 1115.6984 - val_loss: 1324.3511 - val_mse: 14068777.0000 - val_mae: 1324.3511\n",
      "Epoch 491/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1116.9793 - mse: 14162448.0000 - mae: 1116.9792 - val_loss: 1323.3883 - val_mse: 14069166.0000 - val_mae: 1323.3883\n",
      "Epoch 492/500\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 1119.2584 - mse: 14138979.0000 - mae: 1119.2585 - val_loss: 1324.7843 - val_mse: 14055721.0000 - val_mae: 1324.7843\n",
      "Epoch 493/500\n",
      "2500/2500 [==============================] - 0s 47us/step - loss: 1116.3016 - mse: 14150792.0000 - mae: 1116.3016 - val_loss: 1325.2385 - val_mse: 14116284.0000 - val_mae: 1325.2385\n",
      "Epoch 494/500\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 1116.9223 - mse: 14226188.0000 - mae: 1116.9222 - val_loss: 1326.0869 - val_mse: 14099479.0000 - val_mae: 1326.0869\n",
      "Epoch 495/500\n",
      "2500/2500 [==============================] - 0s 55us/step - loss: 1118.5636 - mse: 14122569.0000 - mae: 1118.5635 - val_loss: 1325.4575 - val_mse: 14101864.0000 - val_mae: 1325.4575\n",
      "Epoch 496/500\n",
      "2500/2500 [==============================] - 0s 50us/step - loss: 1112.9189 - mse: 14089290.0000 - mae: 1112.9189 - val_loss: 1324.1268 - val_mse: 14045032.0000 - val_mae: 1324.1268\n",
      "Epoch 497/500\n",
      "2500/2500 [==============================] - 0s 53us/step - loss: 1115.7382 - mse: 14068176.0000 - mae: 1115.7382 - val_loss: 1323.9523 - val_mse: 14043190.0000 - val_mae: 1323.9523\n",
      "Epoch 498/500\n",
      "2500/2500 [==============================] - 0s 53us/step - loss: 1116.6589 - mse: 14096014.0000 - mae: 1116.6589 - val_loss: 1323.0792 - val_mse: 14011495.0000 - val_mae: 1323.0792\n",
      "Epoch 499/500\n",
      "2500/2500 [==============================] - 0s 49us/step - loss: 1116.4015 - mse: 14045880.0000 - mae: 1116.4016 - val_loss: 1322.7659 - val_mse: 14024110.0000 - val_mae: 1322.7659\n",
      "Epoch 500/500\n",
      "2500/2500 [==============================] - 0s 48us/step - loss: 1111.5231 - mse: 14016620.0000 - mae: 1111.5232 - val_loss: 1322.6388 - val_mse: 14023906.0000 - val_mae: 1322.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 500 samples\n",
      "Epoch 1/500\n",
      "4000/4000 [==============================] - 2s 596us/step - loss: 1434.1342 - mse: 18903024.0000 - mae: 1434.1342 - val_loss: 2028.7013 - val_mse: 36130132.0000 - val_mae: 2028.7013\n",
      "Epoch 2/500\n",
      "4000/4000 [==============================] - 0s 35us/step - loss: 1431.2109 - mse: 18896082.0000 - mae: 1431.2108 - val_loss: 2021.3264 - val_mse: 36119236.0000 - val_mae: 2021.3264\n",
      "Epoch 3/500\n",
      "4000/4000 [==============================] - 0s 35us/step - loss: 1421.4256 - mse: 18865722.0000 - mae: 1421.4257 - val_loss: 2017.1393 - val_mse: 36101536.0000 - val_mae: 2017.1393\n",
      "Epoch 4/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1413.2375 - mse: 18832098.0000 - mae: 1413.2375 - val_loss: 2016.1107 - val_mse: 36079256.0000 - val_mae: 2016.1107\n",
      "Epoch 5/500\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 1406.4111 - mse: 18796934.0000 - mae: 1406.4111 - val_loss: 2016.4238 - val_mse: 36054316.0000 - val_mae: 2016.4238\n",
      "Epoch 6/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1402.1845 - mse: 18757318.0000 - mae: 1402.1843 - val_loss: 2016.7496 - val_mse: 36025300.0000 - val_mae: 2016.7496\n",
      "Epoch 7/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1398.7622 - mse: 18722102.0000 - mae: 1398.7622 - val_loss: 2017.1107 - val_mse: 35992300.0000 - val_mae: 2017.1107\n",
      "Epoch 8/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1394.5859 - mse: 18682182.0000 - mae: 1394.5861 - val_loss: 2017.6761 - val_mse: 35950608.0000 - val_mae: 2017.6761\n",
      "Epoch 9/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1392.4818 - mse: 18630978.0000 - mae: 1392.4819 - val_loss: 2017.7227 - val_mse: 35887244.0000 - val_mae: 2017.7227\n",
      "Epoch 10/500\n",
      "4000/4000 [==============================] - 0s 37us/step - loss: 1389.2045 - mse: 18587252.0000 - mae: 1389.2045 - val_loss: 2016.9412 - val_mse: 35796252.0000 - val_mae: 2016.9412\n",
      "Epoch 11/500\n",
      "4000/4000 [==============================] - 0s 37us/step - loss: 1388.0567 - mse: 18546814.0000 - mae: 1388.0566 - val_loss: 2015.8201 - val_mse: 35709880.0000 - val_mae: 2015.8201\n",
      "Epoch 12/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1386.2845 - mse: 18507428.0000 - mae: 1386.2845 - val_loss: 2014.8458 - val_mse: 35651924.0000 - val_mae: 2014.8458\n",
      "Epoch 13/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1382.7411 - mse: 18481228.0000 - mae: 1382.7411 - val_loss: 2014.5730 - val_mse: 35599488.0000 - val_mae: 2014.5730\n",
      "Epoch 14/500\n",
      "4000/4000 [==============================] - 0s 37us/step - loss: 1380.3517 - mse: 18437626.0000 - mae: 1380.3519 - val_loss: 2014.5967 - val_mse: 35554716.0000 - val_mae: 2014.5967\n",
      "Epoch 15/500\n",
      "4000/4000 [==============================] - 0s 35us/step - loss: 1378.3591 - mse: 18411506.0000 - mae: 1378.3590 - val_loss: 2013.1946 - val_mse: 35515028.0000 - val_mae: 2013.1946\n",
      "Epoch 16/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1373.0995 - mse: 18399632.0000 - mae: 1373.0996 - val_loss: 2010.4023 - val_mse: 35421900.0000 - val_mae: 2010.4023\n",
      "Epoch 17/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1366.5814 - mse: 18332932.0000 - mae: 1366.5814 - val_loss: 2005.1715 - val_mse: 35284408.0000 - val_mae: 2005.1715\n",
      "Epoch 18/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1360.7775 - mse: 18267154.0000 - mae: 1360.7775 - val_loss: 1997.0270 - val_mse: 35098528.0000 - val_mae: 1997.0270\n",
      "Epoch 19/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1353.8226 - mse: 18205340.0000 - mae: 1353.8228 - val_loss: 1998.8324 - val_mse: 34884244.0000 - val_mae: 1998.8324\n",
      "Epoch 20/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1347.8091 - mse: 18109726.0000 - mae: 1347.8091 - val_loss: 1985.1384 - val_mse: 34684828.0000 - val_mae: 1985.1384\n",
      "Epoch 21/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1341.1890 - mse: 18010742.0000 - mae: 1341.1890 - val_loss: 1967.3298 - val_mse: 34499484.0000 - val_mae: 1967.3298\n",
      "Epoch 22/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1334.0355 - mse: 17970700.0000 - mae: 1334.0355 - val_loss: 1983.8735 - val_mse: 34298940.0000 - val_mae: 1983.8735\n",
      "Epoch 23/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1330.1192 - mse: 17834776.0000 - mae: 1330.1191 - val_loss: 1967.4657 - val_mse: 34113432.0000 - val_mae: 1967.4657\n",
      "Epoch 24/500\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 1325.5357 - mse: 17791124.0000 - mae: 1325.5358 - val_loss: 1981.9281 - val_mse: 33955412.0000 - val_mae: 1981.9281\n",
      "Epoch 25/500\n",
      "4000/4000 [==============================] - 0s 37us/step - loss: 1324.3231 - mse: 17728138.0000 - mae: 1324.3231 - val_loss: 1984.2797 - val_mse: 33806996.0000 - val_mae: 1984.2797\n",
      "Epoch 26/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1318.5815 - mse: 17658000.0000 - mae: 1318.5814 - val_loss: 1986.9255 - val_mse: 33672104.0000 - val_mae: 1986.9255\n",
      "Epoch 27/500\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 1314.5885 - mse: 17592508.0000 - mae: 1314.5885 - val_loss: 1975.7692 - val_mse: 33518228.0000 - val_mae: 1975.7692\n",
      "Epoch 28/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1311.8737 - mse: 17485868.0000 - mae: 1311.8737 - val_loss: 1958.2150 - val_mse: 33381374.0000 - val_mae: 1958.2150\n",
      "Epoch 29/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1310.0900 - mse: 17463020.0000 - mae: 1310.0898 - val_loss: 1979.0974 - val_mse: 33282200.0000 - val_mae: 1979.0974\n",
      "Epoch 30/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1306.9276 - mse: 17391900.0000 - mae: 1306.9275 - val_loss: 1964.9905 - val_mse: 33142512.0000 - val_mae: 1964.9905\n",
      "Epoch 31/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1305.3945 - mse: 17368958.0000 - mae: 1305.3945 - val_loss: 1978.1481 - val_mse: 33058592.0000 - val_mae: 1978.1481\n",
      "Epoch 32/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1301.8391 - mse: 17334858.0000 - mae: 1301.8391 - val_loss: 1957.2152 - val_mse: 32929352.0000 - val_mae: 1957.2152\n",
      "Epoch 33/500\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 1298.2728 - mse: 17276460.0000 - mae: 1298.2727 - val_loss: 1960.0465 - val_mse: 32812748.0000 - val_mae: 1960.0465\n",
      "Epoch 34/500\n",
      "4000/4000 [==============================] - 0s 35us/step - loss: 1302.3148 - mse: 17237316.0000 - mae: 1302.3148 - val_loss: 1949.8439 - val_mse: 32740912.0000 - val_mae: 1949.8439\n",
      "Epoch 35/500\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 1297.9658 - mse: 17250214.0000 - mae: 1297.9657 - val_loss: 1955.0966 - val_mse: 32658260.0000 - val_mae: 1955.0966\n",
      "Epoch 36/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1294.8237 - mse: 17165484.0000 - mae: 1294.8237 - val_loss: 1950.5623 - val_mse: 32560006.0000 - val_mae: 1950.5623\n",
      "Epoch 37/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1294.6591 - mse: 17144012.0000 - mae: 1294.6593 - val_loss: 1937.4166 - val_mse: 32490594.0000 - val_mae: 1937.4166\n",
      "Epoch 38/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1291.1419 - mse: 17130646.0000 - mae: 1291.1418 - val_loss: 1950.2306 - val_mse: 32400100.0000 - val_mae: 1950.2306\n",
      "Epoch 39/500\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 1291.3287 - mse: 17081756.0000 - mae: 1291.3286 - val_loss: 1939.9105 - val_mse: 32319508.0000 - val_mae: 1939.9105\n",
      "Epoch 40/500\n",
      "4000/4000 [==============================] - 0s 37us/step - loss: 1293.0630 - mse: 17123082.0000 - mae: 1293.0629 - val_loss: 1944.4443 - val_mse: 32253570.0000 - val_mae: 1944.4443\n",
      "Epoch 41/500\n",
      "4000/4000 [==============================] - 0s 37us/step - loss: 1290.2246 - mse: 17059528.0000 - mae: 1290.2246 - val_loss: 1944.4637 - val_mse: 32172198.0000 - val_mae: 1944.4637\n",
      "Epoch 42/500\n",
      "4000/4000 [==============================] - 0s 34us/step - loss: 1290.9634 - mse: 17056838.0000 - mae: 1290.9635 - val_loss: 1914.0997 - val_mse: 32094180.0000 - val_mae: 1914.0997\n",
      "Epoch 43/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1287.8295 - mse: 17007480.0000 - mae: 1287.8295 - val_loss: 1912.6810 - val_mse: 32018614.0000 - val_mae: 1912.6810\n",
      "Epoch 44/500\n",
      "4000/4000 [==============================] - 0s 35us/step - loss: 1282.9270 - mse: 16891922.0000 - mae: 1282.9270 - val_loss: 1922.7959 - val_mse: 31920216.0000 - val_mae: 1922.7959\n",
      "Epoch 45/500\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 1284.5638 - mse: 16900782.0000 - mae: 1284.5637 - val_loss: 1903.3706 - val_mse: 31823332.0000 - val_mae: 1903.3706\n",
      "Epoch 46/500\n",
      "4000/4000 [==============================] - 0s 35us/step - loss: 1284.9153 - mse: 16895738.0000 - mae: 1284.9154 - val_loss: 1920.9078 - val_mse: 31773332.0000 - val_mae: 1920.9078\n",
      "Epoch 47/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1279.2611 - mse: 16867628.0000 - mae: 1279.2611 - val_loss: 1914.0592 - val_mse: 31703362.0000 - val_mae: 1914.0592\n",
      "Epoch 48/500\n",
      "4000/4000 [==============================] - 0s 34us/step - loss: 1280.5151 - mse: 16816356.0000 - mae: 1280.5150 - val_loss: 1914.7982 - val_mse: 31634520.0000 - val_mae: 1914.7982\n",
      "Epoch 49/500\n",
      "4000/4000 [==============================] - 0s 37us/step - loss: 1282.3509 - mse: 16850632.0000 - mae: 1282.3510 - val_loss: 1906.4655 - val_mse: 31553836.0000 - val_mae: 1906.4655\n",
      "Epoch 50/500\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 1274.8982 - mse: 16784866.0000 - mae: 1274.8982 - val_loss: 1882.5228 - val_mse: 31470982.0000 - val_mae: 1882.5228\n",
      "Epoch 51/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1278.0713 - mse: 16771713.0000 - mae: 1278.0714 - val_loss: 1880.5934 - val_mse: 31412168.0000 - val_mae: 1880.5934\n",
      "Epoch 52/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1276.3899 - mse: 16774274.0000 - mae: 1276.3899 - val_loss: 1889.9238 - val_mse: 31347994.0000 - val_mae: 1889.9238\n",
      "Epoch 53/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1272.7358 - mse: 16716392.0000 - mae: 1272.7357 - val_loss: 1917.0621 - val_mse: 31284186.0000 - val_mae: 1917.0621\n",
      "Epoch 54/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1273.6568 - mse: 16653079.0000 - mae: 1273.6567 - val_loss: 1873.3679 - val_mse: 31206750.0000 - val_mae: 1873.3679\n",
      "Epoch 55/500\n",
      "4000/4000 [==============================] - 0s 34us/step - loss: 1270.1412 - mse: 16565818.0000 - mae: 1270.1411 - val_loss: 1882.3783 - val_mse: 31124630.0000 - val_mae: 1882.3783\n",
      "Epoch 56/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1270.5247 - mse: 16590898.0000 - mae: 1270.5247 - val_loss: 1862.2925 - val_mse: 31042130.0000 - val_mae: 1862.2925\n",
      "Epoch 57/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1270.3030 - mse: 16658480.0000 - mae: 1270.3031 - val_loss: 1868.9102 - val_mse: 31004132.0000 - val_mae: 1868.9102\n",
      "Epoch 58/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1263.6751 - mse: 16575712.0000 - mae: 1263.6752 - val_loss: 1890.7472 - val_mse: 30938392.0000 - val_mae: 1890.7472\n",
      "Epoch 59/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1264.0616 - mse: 16454653.0000 - mae: 1264.0616 - val_loss: 1856.2816 - val_mse: 30853332.0000 - val_mae: 1856.2816\n",
      "Epoch 60/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1266.7138 - mse: 16460710.0000 - mae: 1266.7137 - val_loss: 1864.9834 - val_mse: 30779282.0000 - val_mae: 1864.9834\n",
      "Epoch 61/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1265.2413 - mse: 16511946.0000 - mae: 1265.2412 - val_loss: 1818.9248 - val_mse: 30686020.0000 - val_mae: 1818.9248\n",
      "Epoch 62/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1261.6569 - mse: 16448311.0000 - mae: 1261.6569 - val_loss: 1834.9171 - val_mse: 30614276.0000 - val_mae: 1834.9171\n",
      "Epoch 63/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1260.9545 - mse: 16417603.0000 - mae: 1260.9546 - val_loss: 1843.3134 - val_mse: 30550376.0000 - val_mae: 1843.3134\n",
      "Epoch 64/500\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 1260.3801 - mse: 16458351.0000 - mae: 1260.3801 - val_loss: 1851.0709 - val_mse: 30500922.0000 - val_mae: 1851.0709\n",
      "Epoch 65/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1258.6731 - mse: 16423723.0000 - mae: 1258.6731 - val_loss: 1821.7424 - val_mse: 30447684.0000 - val_mae: 1821.7424\n",
      "Epoch 66/500\n",
      "4000/4000 [==============================] - 0s 37us/step - loss: 1259.6089 - mse: 16436881.0000 - mae: 1259.6088 - val_loss: 1835.2900 - val_mse: 30371304.0000 - val_mae: 1835.2900\n",
      "Epoch 67/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1258.4477 - mse: 16349653.0000 - mae: 1258.4478 - val_loss: 1817.0693 - val_mse: 30299684.0000 - val_mae: 1817.0693\n",
      "Epoch 68/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1260.0052 - mse: 16413026.0000 - mae: 1260.0052 - val_loss: 1828.2567 - val_mse: 30233314.0000 - val_mae: 1828.2567\n",
      "Epoch 69/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1253.7776 - mse: 16249129.0000 - mae: 1253.7775 - val_loss: 1794.6541 - val_mse: 30153894.0000 - val_mae: 1794.6541\n",
      "Epoch 70/500\n",
      "4000/4000 [==============================] - 0s 35us/step - loss: 1261.3689 - mse: 16322591.0000 - mae: 1261.3689 - val_loss: 1815.7488 - val_mse: 30069466.0000 - val_mae: 1815.7488\n",
      "Epoch 71/500\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 1251.4510 - mse: 16224516.0000 - mae: 1251.4510 - val_loss: 1792.0854 - val_mse: 29992256.0000 - val_mae: 1792.0854\n",
      "Epoch 72/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1251.0746 - mse: 16251107.0000 - mae: 1251.0746 - val_loss: 1804.1062 - val_mse: 29881990.0000 - val_mae: 1804.1062\n",
      "Epoch 73/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1252.3466 - mse: 16192801.0000 - mae: 1252.3467 - val_loss: 1797.0687 - val_mse: 29837090.0000 - val_mae: 1797.0687\n",
      "Epoch 74/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1251.1469 - mse: 16148689.0000 - mae: 1251.1469 - val_loss: 1798.2686 - val_mse: 29768544.0000 - val_mae: 1798.2686\n",
      "Epoch 75/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1249.0211 - mse: 16061521.0000 - mae: 1249.0210 - val_loss: 1777.6761 - val_mse: 29711240.0000 - val_mae: 1777.6761\n",
      "Epoch 76/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1250.0086 - mse: 16118621.0000 - mae: 1250.0087 - val_loss: 1793.9875 - val_mse: 29633990.0000 - val_mae: 1793.9875\n",
      "Epoch 77/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1249.3134 - mse: 16132886.0000 - mae: 1249.3134 - val_loss: 1779.7354 - val_mse: 29563284.0000 - val_mae: 1779.7354\n",
      "Epoch 78/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1250.9519 - mse: 16098654.0000 - mae: 1250.9520 - val_loss: 1797.4646 - val_mse: 29473386.0000 - val_mae: 1797.4646\n",
      "Epoch 79/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1242.6233 - mse: 15940879.0000 - mae: 1242.6233 - val_loss: 1813.6622 - val_mse: 29381878.0000 - val_mae: 1813.6622\n",
      "Epoch 80/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1247.5606 - mse: 16011909.0000 - mae: 1247.5607 - val_loss: 1762.8411 - val_mse: 29352304.0000 - val_mae: 1762.8411\n",
      "Epoch 81/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1244.7215 - mse: 15955200.0000 - mae: 1244.7216 - val_loss: 1760.8593 - val_mse: 29318318.0000 - val_mae: 1760.8593\n",
      "Epoch 82/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1247.8087 - mse: 15946971.0000 - mae: 1247.8087 - val_loss: 1766.9648 - val_mse: 29267962.0000 - val_mae: 1766.9648\n",
      "Epoch 83/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1245.1532 - mse: 15991921.0000 - mae: 1245.1532 - val_loss: 1749.6376 - val_mse: 29194194.0000 - val_mae: 1749.6376\n",
      "Epoch 84/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1240.5072 - mse: 15937935.0000 - mae: 1240.5071 - val_loss: 1773.3362 - val_mse: 29088632.0000 - val_mae: 1773.3362\n",
      "Epoch 85/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1242.9737 - mse: 15847424.0000 - mae: 1242.9736 - val_loss: 1772.1698 - val_mse: 29027144.0000 - val_mae: 1772.1698\n",
      "Epoch 86/500\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 1243.4401 - mse: 15791157.0000 - mae: 1243.4399 - val_loss: 1763.9235 - val_mse: 28971566.0000 - val_mae: 1763.9235\n",
      "Epoch 87/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1243.2585 - mse: 15875864.0000 - mae: 1243.2585 - val_loss: 1771.5900 - val_mse: 28908544.0000 - val_mae: 1771.5900\n",
      "Epoch 88/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1249.9692 - mse: 15958622.0000 - mae: 1249.9692 - val_loss: 1754.1965 - val_mse: 28945864.0000 - val_mae: 1754.1965\n",
      "Epoch 89/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1239.3511 - mse: 15798621.0000 - mae: 1239.3510 - val_loss: 1763.4086 - val_mse: 28901358.0000 - val_mae: 1763.4086\n",
      "Epoch 90/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1240.7887 - mse: 15795102.0000 - mae: 1240.7887 - val_loss: 1749.4608 - val_mse: 28853504.0000 - val_mae: 1749.4608\n",
      "Epoch 91/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1241.0311 - mse: 15834046.0000 - mae: 1241.0311 - val_loss: 1752.8275 - val_mse: 28816660.0000 - val_mae: 1752.8275\n",
      "Epoch 92/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1239.1128 - mse: 15747291.0000 - mae: 1239.1128 - val_loss: 1747.8003 - val_mse: 28788586.0000 - val_mae: 1747.8003\n",
      "Epoch 93/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1242.9818 - mse: 15752113.0000 - mae: 1242.9817 - val_loss: 1755.0248 - val_mse: 28744542.0000 - val_mae: 1755.0248\n",
      "Epoch 94/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1239.9756 - mse: 15778945.0000 - mae: 1239.9756 - val_loss: 1746.4286 - val_mse: 28687798.0000 - val_mae: 1746.4286\n",
      "Epoch 95/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1239.9127 - mse: 15698291.0000 - mae: 1239.9127 - val_loss: 1741.6844 - val_mse: 28647142.0000 - val_mae: 1741.6844\n",
      "Epoch 96/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1241.0086 - mse: 15769602.0000 - mae: 1241.0087 - val_loss: 1720.5013 - val_mse: 28599818.0000 - val_mae: 1720.5013\n",
      "Epoch 97/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1237.5939 - mse: 15658301.0000 - mae: 1237.5939 - val_loss: 1740.9313 - val_mse: 28526626.0000 - val_mae: 1740.9313\n",
      "Epoch 98/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1240.0426 - mse: 15659700.0000 - mae: 1240.0425 - val_loss: 1744.3129 - val_mse: 28500056.0000 - val_mae: 1744.3129\n",
      "Epoch 99/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1232.9688 - mse: 15613739.0000 - mae: 1232.9689 - val_loss: 1743.7983 - val_mse: 28399976.0000 - val_mae: 1743.7983\n",
      "Epoch 100/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1233.1903 - mse: 15535248.0000 - mae: 1233.1903 - val_loss: 1740.0928 - val_mse: 28324944.0000 - val_mae: 1740.0928\n",
      "Epoch 101/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1233.8503 - mse: 15513667.0000 - mae: 1233.8502 - val_loss: 1726.3644 - val_mse: 28319002.0000 - val_mae: 1726.3644\n",
      "Epoch 102/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1232.4546 - mse: 15483625.0000 - mae: 1232.4546 - val_loss: 1733.7723 - val_mse: 28248072.0000 - val_mae: 1733.7723\n",
      "Epoch 103/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1228.0784 - mse: 15497153.0000 - mae: 1228.0782 - val_loss: 1731.8806 - val_mse: 28213574.0000 - val_mae: 1731.8806\n",
      "Epoch 104/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1233.4843 - mse: 15534003.0000 - mae: 1233.4843 - val_loss: 1744.8165 - val_mse: 28218224.0000 - val_mae: 1744.8165\n",
      "Epoch 105/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1233.7493 - mse: 15610200.0000 - mae: 1233.7493 - val_loss: 1721.1317 - val_mse: 28186440.0000 - val_mae: 1721.1317\n",
      "Epoch 106/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1232.2374 - mse: 15443201.0000 - mae: 1232.2374 - val_loss: 1718.1603 - val_mse: 28141236.0000 - val_mae: 1718.1603\n",
      "Epoch 107/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1228.3367 - mse: 15454406.0000 - mae: 1228.3368 - val_loss: 1727.3618 - val_mse: 28042838.0000 - val_mae: 1727.3618\n",
      "Epoch 108/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1232.8550 - mse: 15462627.0000 - mae: 1232.8550 - val_loss: 1720.6688 - val_mse: 28072828.0000 - val_mae: 1720.6688\n",
      "Epoch 109/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1232.9702 - mse: 15537568.0000 - mae: 1232.9701 - val_loss: 1713.1116 - val_mse: 28044014.0000 - val_mae: 1713.1116\n",
      "Epoch 110/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1235.3704 - mse: 15558221.0000 - mae: 1235.3704 - val_loss: 1725.9390 - val_mse: 27966962.0000 - val_mae: 1725.9390\n",
      "Epoch 111/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1227.0264 - mse: 15442385.0000 - mae: 1227.0262 - val_loss: 1703.4374 - val_mse: 27959932.0000 - val_mae: 1703.4374\n",
      "Epoch 112/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1229.9249 - mse: 15413852.0000 - mae: 1229.9249 - val_loss: 1704.7600 - val_mse: 27924202.0000 - val_mae: 1704.7600\n",
      "Epoch 113/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1229.2031 - mse: 15402232.0000 - mae: 1229.2031 - val_loss: 1723.1947 - val_mse: 27848140.0000 - val_mae: 1723.1947\n",
      "Epoch 114/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1231.8024 - mse: 15419055.0000 - mae: 1231.8025 - val_loss: 1712.6593 - val_mse: 27868788.0000 - val_mae: 1712.6593\n",
      "Epoch 115/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1228.6685 - mse: 15388553.0000 - mae: 1228.6685 - val_loss: 1707.7302 - val_mse: 27896414.0000 - val_mae: 1707.7302\n",
      "Epoch 116/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1225.4473 - mse: 15289519.0000 - mae: 1225.4473 - val_loss: 1710.2438 - val_mse: 27817492.0000 - val_mae: 1710.2438\n",
      "Epoch 117/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1228.6670 - mse: 15359075.0000 - mae: 1228.6670 - val_loss: 1702.7533 - val_mse: 27786982.0000 - val_mae: 1702.7533\n",
      "Epoch 118/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1227.0092 - mse: 15232791.0000 - mae: 1227.0093 - val_loss: 1714.2717 - val_mse: 27693544.0000 - val_mae: 1714.2717\n",
      "Epoch 119/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1230.6040 - mse: 15397803.0000 - mae: 1230.6040 - val_loss: 1693.4926 - val_mse: 27731152.0000 - val_mae: 1693.4926\n",
      "Epoch 120/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1228.6845 - mse: 15349760.0000 - mae: 1228.6844 - val_loss: 1699.7137 - val_mse: 27704292.0000 - val_mae: 1699.7137\n",
      "Epoch 121/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1230.7007 - mse: 15398538.0000 - mae: 1230.7007 - val_loss: 1710.0881 - val_mse: 27667614.0000 - val_mae: 1710.0881\n",
      "Epoch 122/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1219.4533 - mse: 15232520.0000 - mae: 1219.4534 - val_loss: 1705.1660 - val_mse: 27636418.0000 - val_mae: 1705.1660\n",
      "Epoch 123/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1224.2506 - mse: 15180653.0000 - mae: 1224.2506 - val_loss: 1695.9863 - val_mse: 27639546.0000 - val_mae: 1695.9863\n",
      "Epoch 124/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1219.3826 - mse: 15148640.0000 - mae: 1219.3826 - val_loss: 1700.2737 - val_mse: 27553036.0000 - val_mae: 1700.2737\n",
      "Epoch 125/500\n",
      "4000/4000 [==============================] - 0s 37us/step - loss: 1222.4284 - mse: 15186873.0000 - mae: 1222.4283 - val_loss: 1693.3196 - val_mse: 27524890.0000 - val_mae: 1693.3196\n",
      "Epoch 126/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1222.8404 - mse: 15117037.0000 - mae: 1222.8403 - val_loss: 1692.4476 - val_mse: 27501114.0000 - val_mae: 1692.4476\n",
      "Epoch 127/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1221.2395 - mse: 15125391.0000 - mae: 1221.2395 - val_loss: 1685.7998 - val_mse: 27425434.0000 - val_mae: 1685.7998\n",
      "Epoch 128/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1221.6371 - mse: 15275112.0000 - mae: 1221.6370 - val_loss: 1692.1910 - val_mse: 27419160.0000 - val_mae: 1692.1910\n",
      "Epoch 129/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1221.2282 - mse: 15191112.0000 - mae: 1221.2281 - val_loss: 1680.4187 - val_mse: 27401168.0000 - val_mae: 1680.4187\n",
      "Epoch 130/500\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 1221.7918 - mse: 15137408.0000 - mae: 1221.7917 - val_loss: 1699.5396 - val_mse: 27394360.0000 - val_mae: 1699.5396\n",
      "Epoch 131/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1217.5392 - mse: 15120986.0000 - mae: 1217.5393 - val_loss: 1692.7665 - val_mse: 27396360.0000 - val_mae: 1692.7665\n",
      "Epoch 132/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1221.4079 - mse: 15252445.0000 - mae: 1221.4078 - val_loss: 1683.7225 - val_mse: 27318328.0000 - val_mae: 1683.7225\n",
      "Epoch 133/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1219.4733 - mse: 15107203.0000 - mae: 1219.4733 - val_loss: 1685.8850 - val_mse: 27275126.0000 - val_mae: 1685.8850\n",
      "Epoch 134/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1220.5293 - mse: 15117698.0000 - mae: 1220.5294 - val_loss: 1682.6198 - val_mse: 27179564.0000 - val_mae: 1682.6198\n",
      "Epoch 135/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1213.3499 - mse: 15063979.0000 - mae: 1213.3499 - val_loss: 1680.0693 - val_mse: 27170290.0000 - val_mae: 1680.0693\n",
      "Epoch 136/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1220.1141 - mse: 15158476.0000 - mae: 1220.1140 - val_loss: 1683.4133 - val_mse: 27108788.0000 - val_mae: 1683.4133\n",
      "Epoch 137/500\n",
      "4000/4000 [==============================] - 0s 37us/step - loss: 1214.5751 - mse: 15025187.0000 - mae: 1214.5750 - val_loss: 1685.1814 - val_mse: 27045220.0000 - val_mae: 1685.1814\n",
      "Epoch 138/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1222.4948 - mse: 15119202.0000 - mae: 1222.4949 - val_loss: 1683.4247 - val_mse: 27036030.0000 - val_mae: 1683.4247\n",
      "Epoch 139/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1214.7341 - mse: 14964765.0000 - mae: 1214.7340 - val_loss: 1677.1238 - val_mse: 27083702.0000 - val_mae: 1677.1238\n",
      "Epoch 140/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1213.1449 - mse: 15078276.0000 - mae: 1213.1450 - val_loss: 1676.6733 - val_mse: 27106780.0000 - val_mae: 1676.6733\n",
      "Epoch 141/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1214.5806 - mse: 15013649.0000 - mae: 1214.5806 - val_loss: 1681.3274 - val_mse: 27033930.0000 - val_mae: 1681.3274\n",
      "Epoch 142/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1210.6269 - mse: 14905426.0000 - mae: 1210.6268 - val_loss: 1678.4984 - val_mse: 26952306.0000 - val_mae: 1678.4984\n",
      "Epoch 143/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1214.3397 - mse: 15020303.0000 - mae: 1214.3396 - val_loss: 1679.2722 - val_mse: 26953356.0000 - val_mae: 1679.2722\n",
      "Epoch 144/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1213.4347 - mse: 14893774.0000 - mae: 1213.4347 - val_loss: 1677.9807 - val_mse: 26885278.0000 - val_mae: 1677.9807\n",
      "Epoch 145/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1218.8503 - mse: 15051220.0000 - mae: 1218.8502 - val_loss: 1686.7781 - val_mse: 26934330.0000 - val_mae: 1686.7781\n",
      "Epoch 146/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1211.4327 - mse: 15021098.0000 - mae: 1211.4326 - val_loss: 1679.7325 - val_mse: 26849846.0000 - val_mae: 1679.7325\n",
      "Epoch 147/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1208.2150 - mse: 14935557.0000 - mae: 1208.2150 - val_loss: 1677.0057 - val_mse: 26817116.0000 - val_mae: 1677.0057\n",
      "Epoch 148/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1207.4685 - mse: 14873738.0000 - mae: 1207.4685 - val_loss: 1685.0570 - val_mse: 26739496.0000 - val_mae: 1685.0570\n",
      "Epoch 149/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1206.1075 - mse: 14773962.0000 - mae: 1206.1075 - val_loss: 1672.7336 - val_mse: 26801056.0000 - val_mae: 1672.7336\n",
      "Epoch 150/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1204.2067 - mse: 14766733.0000 - mae: 1204.2065 - val_loss: 1674.0762 - val_mse: 26681896.0000 - val_mae: 1674.0762\n",
      "Epoch 151/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1209.0929 - mse: 14913452.0000 - mae: 1209.0929 - val_loss: 1668.1833 - val_mse: 26762394.0000 - val_mae: 1668.1833\n",
      "Epoch 152/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1207.9486 - mse: 14798496.0000 - mae: 1207.9486 - val_loss: 1669.2898 - val_mse: 26668818.0000 - val_mae: 1669.2898\n",
      "Epoch 153/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1204.8391 - mse: 14787461.0000 - mae: 1204.8390 - val_loss: 1675.1084 - val_mse: 26604112.0000 - val_mae: 1675.1084\n",
      "Epoch 154/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1203.4920 - mse: 14838079.0000 - mae: 1203.4921 - val_loss: 1677.3817 - val_mse: 26659938.0000 - val_mae: 1677.3817\n",
      "Epoch 155/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1199.5025 - mse: 14671340.0000 - mae: 1199.5024 - val_loss: 1670.7766 - val_mse: 26545224.0000 - val_mae: 1670.7766\n",
      "Epoch 156/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1201.8812 - mse: 14693672.0000 - mae: 1201.8811 - val_loss: 1674.9167 - val_mse: 26500416.0000 - val_mae: 1674.9167\n",
      "Epoch 157/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1199.6212 - mse: 14702180.0000 - mae: 1199.6212 - val_loss: 1665.5299 - val_mse: 26477796.0000 - val_mae: 1665.5299\n",
      "Epoch 158/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1208.8462 - mse: 14761985.0000 - mae: 1208.8462 - val_loss: 1671.3163 - val_mse: 26376676.0000 - val_mae: 1671.3163\n",
      "Epoch 159/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1194.8735 - mse: 14681008.0000 - mae: 1194.8735 - val_loss: 1661.4186 - val_mse: 26266842.0000 - val_mae: 1661.4186\n",
      "Epoch 160/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1205.6699 - mse: 14604462.0000 - mae: 1205.6699 - val_loss: 1660.4495 - val_mse: 26286676.0000 - val_mae: 1660.4495\n",
      "Epoch 161/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1198.1817 - mse: 14637949.0000 - mae: 1198.1816 - val_loss: 1667.2216 - val_mse: 26317352.0000 - val_mae: 1667.2216\n",
      "Epoch 162/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1197.7301 - mse: 14617080.0000 - mae: 1197.7301 - val_loss: 1670.7490 - val_mse: 26238400.0000 - val_mae: 1670.7490\n",
      "Epoch 163/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1194.3642 - mse: 14552828.0000 - mae: 1194.3641 - val_loss: 1673.9788 - val_mse: 26161782.0000 - val_mae: 1673.9788\n",
      "Epoch 164/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1197.4851 - mse: 14636974.0000 - mae: 1197.4850 - val_loss: 1659.5018 - val_mse: 26022192.0000 - val_mae: 1659.5018\n",
      "Epoch 165/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1194.6194 - mse: 14544359.0000 - mae: 1194.6194 - val_loss: 1665.7620 - val_mse: 25977376.0000 - val_mae: 1665.7620\n",
      "Epoch 166/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1187.7982 - mse: 14370433.0000 - mae: 1187.7981 - val_loss: 1653.0139 - val_mse: 25899710.0000 - val_mae: 1653.0139\n",
      "Epoch 167/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1191.6655 - mse: 14361416.0000 - mae: 1191.6655 - val_loss: 1656.8972 - val_mse: 25883204.0000 - val_mae: 1656.8972\n",
      "Epoch 168/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1190.4894 - mse: 14282737.0000 - mae: 1190.4894 - val_loss: 1655.9525 - val_mse: 25763500.0000 - val_mae: 1655.9525\n",
      "Epoch 169/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1186.6853 - mse: 14332749.0000 - mae: 1186.6853 - val_loss: 1654.2245 - val_mse: 25772988.0000 - val_mae: 1654.2245\n",
      "Epoch 170/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1184.5479 - mse: 14301128.0000 - mae: 1184.5479 - val_loss: 1653.9641 - val_mse: 25663256.0000 - val_mae: 1653.9641\n",
      "Epoch 171/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1189.6601 - mse: 14377079.0000 - mae: 1189.6602 - val_loss: 1647.4943 - val_mse: 25514300.0000 - val_mae: 1647.4943\n",
      "Epoch 172/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1185.6334 - mse: 14269844.0000 - mae: 1185.6334 - val_loss: 1645.0092 - val_mse: 25599186.0000 - val_mae: 1645.0092\n",
      "Epoch 173/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1186.7207 - mse: 14245882.0000 - mae: 1186.7206 - val_loss: 1649.4369 - val_mse: 25438660.0000 - val_mae: 1649.4369\n",
      "Epoch 174/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1180.9378 - mse: 14095762.0000 - mae: 1180.9377 - val_loss: 1650.8247 - val_mse: 25387736.0000 - val_mae: 1650.8247\n",
      "Epoch 175/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1184.5643 - mse: 14285756.0000 - mae: 1184.5643 - val_loss: 1649.9530 - val_mse: 25391836.0000 - val_mae: 1649.9530\n",
      "Epoch 176/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1174.7684 - mse: 14071186.0000 - mae: 1174.7684 - val_loss: 1641.1033 - val_mse: 25101476.0000 - val_mae: 1641.1033\n",
      "Epoch 177/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1169.9729 - mse: 13897033.0000 - mae: 1169.9730 - val_loss: 1652.9962 - val_mse: 25010726.0000 - val_mae: 1652.9962\n",
      "Epoch 178/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1183.8927 - mse: 14221472.0000 - mae: 1183.8926 - val_loss: 1644.8806 - val_mse: 24991350.0000 - val_mae: 1644.8806\n",
      "Epoch 179/500\n",
      "4000/4000 [==============================] - 0s 37us/step - loss: 1184.7765 - mse: 14095244.0000 - mae: 1184.7765 - val_loss: 1643.1833 - val_mse: 25072648.0000 - val_mae: 1643.1833\n",
      "Epoch 180/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1173.5333 - mse: 14029714.0000 - mae: 1173.5332 - val_loss: 1659.4353 - val_mse: 24928050.0000 - val_mae: 1659.4353\n",
      "Epoch 181/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1176.9442 - mse: 13936811.0000 - mae: 1176.9441 - val_loss: 1649.7916 - val_mse: 24846788.0000 - val_mae: 1649.7916\n",
      "Epoch 182/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1182.6749 - mse: 13964556.0000 - mae: 1182.6749 - val_loss: 1647.3782 - val_mse: 24778906.0000 - val_mae: 1647.3782\n",
      "Epoch 183/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1173.9229 - mse: 13901203.0000 - mae: 1173.9229 - val_loss: 1647.8207 - val_mse: 24696670.0000 - val_mae: 1647.8207\n",
      "Epoch 184/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1170.8457 - mse: 13815841.0000 - mae: 1170.8456 - val_loss: 1658.5891 - val_mse: 24668108.0000 - val_mae: 1658.5891\n",
      "Epoch 185/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1170.4828 - mse: 14001503.0000 - mae: 1170.4829 - val_loss: 1642.6508 - val_mse: 24534758.0000 - val_mae: 1642.6508\n",
      "Epoch 186/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1168.9953 - mse: 13868752.0000 - mae: 1168.9954 - val_loss: 1633.1881 - val_mse: 24475714.0000 - val_mae: 1633.1881\n",
      "Epoch 187/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1168.9323 - mse: 13767335.0000 - mae: 1168.9323 - val_loss: 1642.4865 - val_mse: 24324702.0000 - val_mae: 1642.4865\n",
      "Epoch 188/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1173.2884 - mse: 13714831.0000 - mae: 1173.2882 - val_loss: 1632.4725 - val_mse: 24344548.0000 - val_mae: 1632.4725\n",
      "Epoch 189/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 1166.4313 - mse: 13732836.0000 - mae: 1166.4313 - val_loss: 1632.9510 - val_mse: 24272960.0000 - val_mae: 1632.9510\n",
      "Epoch 190/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1162.5087 - mse: 13633198.0000 - mae: 1162.5087 - val_loss: 1627.3739 - val_mse: 24132230.0000 - val_mae: 1627.3739\n",
      "Epoch 191/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1170.8411 - mse: 13797413.0000 - mae: 1170.8409 - val_loss: 1634.9387 - val_mse: 24142762.0000 - val_mae: 1634.9387\n",
      "Epoch 192/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1158.4002 - mse: 13558065.0000 - mae: 1158.4003 - val_loss: 1630.8286 - val_mse: 24014936.0000 - val_mae: 1630.8286\n",
      "Epoch 193/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1157.4230 - mse: 13533129.0000 - mae: 1157.4230 - val_loss: 1633.9565 - val_mse: 23835212.0000 - val_mae: 1633.9565\n",
      "Epoch 194/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1157.4382 - mse: 13518647.0000 - mae: 1157.4382 - val_loss: 1626.6472 - val_mse: 23870974.0000 - val_mae: 1626.6472\n",
      "Epoch 195/500\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 1159.3836 - mse: 13601047.0000 - mae: 1159.3837 - val_loss: 1636.5448 - val_mse: 23857928.0000 - val_mae: 1636.5448\n",
      "Epoch 196/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1156.3345 - mse: 13390639.0000 - mae: 1156.3345 - val_loss: 1631.5027 - val_mse: 23911354.0000 - val_mae: 1631.5027\n",
      "Epoch 197/500\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 1154.0053 - mse: 13509212.0000 - mae: 1154.0052 - val_loss: 1637.2524 - val_mse: 23823034.0000 - val_mae: 1637.2524\n",
      "Epoch 198/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1156.8193 - mse: 13624943.0000 - mae: 1156.8192 - val_loss: 1627.0775 - val_mse: 23442658.0000 - val_mae: 1627.0775\n",
      "Epoch 199/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1150.2387 - mse: 13277042.0000 - mae: 1150.2386 - val_loss: 1633.3843 - val_mse: 23645574.0000 - val_mae: 1633.3843\n",
      "Epoch 200/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1151.4106 - mse: 13330281.0000 - mae: 1151.4105 - val_loss: 1625.4224 - val_mse: 23279058.0000 - val_mae: 1625.4224\n",
      "Epoch 201/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1140.0131 - mse: 13168856.0000 - mae: 1140.0129 - val_loss: 1606.7465 - val_mse: 23226378.0000 - val_mae: 1606.7465\n",
      "Epoch 202/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1148.8143 - mse: 13353000.0000 - mae: 1148.8142 - val_loss: 1626.4102 - val_mse: 23150230.0000 - val_mae: 1626.4102\n",
      "Epoch 203/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1138.7703 - mse: 13108987.0000 - mae: 1138.7703 - val_loss: 1600.8525 - val_mse: 22753364.0000 - val_mae: 1600.8525\n",
      "Epoch 204/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1137.1651 - mse: 13007186.0000 - mae: 1137.1650 - val_loss: 1607.6362 - val_mse: 22751772.0000 - val_mae: 1607.6362\n",
      "Epoch 205/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1140.7252 - mse: 13069256.0000 - mae: 1140.7252 - val_loss: 1610.0613 - val_mse: 22567882.0000 - val_mae: 1610.0613\n",
      "Epoch 206/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1144.3304 - mse: 13203806.0000 - mae: 1144.3303 - val_loss: 1608.9229 - val_mse: 22562896.0000 - val_mae: 1608.9229\n",
      "Epoch 207/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1128.4482 - mse: 12803470.0000 - mae: 1128.4482 - val_loss: 1591.4976 - val_mse: 22175020.0000 - val_mae: 1591.4976\n",
      "Epoch 208/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1135.5936 - mse: 12993624.0000 - mae: 1135.5935 - val_loss: 1583.4808 - val_mse: 22210402.0000 - val_mae: 1583.4808\n",
      "Epoch 209/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1122.7294 - mse: 12689012.0000 - mae: 1122.7295 - val_loss: 1614.5339 - val_mse: 22084964.0000 - val_mae: 1614.5339\n",
      "Epoch 210/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 1132.8771 - mse: 12853038.0000 - mae: 1132.8771 - val_loss: 1596.9604 - val_mse: 22077808.0000 - val_mae: 1596.9604\n",
      "Epoch 211/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1135.4816 - mse: 12979812.0000 - mae: 1135.4814 - val_loss: 1620.4410 - val_mse: 21964472.0000 - val_mae: 1620.4410\n",
      "Epoch 212/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1133.6948 - mse: 12929445.0000 - mae: 1133.6948 - val_loss: 1581.1393 - val_mse: 21757030.0000 - val_mae: 1581.1393\n",
      "Epoch 213/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1120.8836 - mse: 12560944.0000 - mae: 1120.8837 - val_loss: 1578.3297 - val_mse: 21597250.0000 - val_mae: 1578.3297\n",
      "Epoch 214/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1117.4868 - mse: 12433425.0000 - mae: 1117.4868 - val_loss: 1561.4644 - val_mse: 21525730.0000 - val_mae: 1561.4644\n",
      "Epoch 215/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1128.3794 - mse: 12696754.0000 - mae: 1128.3794 - val_loss: 1557.4938 - val_mse: 21280008.0000 - val_mae: 1557.4938\n",
      "Epoch 216/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 1121.1050 - mse: 12626826.0000 - mae: 1121.1051 - val_loss: 1568.8975 - val_mse: 21533932.0000 - val_mae: 1568.8975\n",
      "Epoch 217/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1118.6146 - mse: 12504701.0000 - mae: 1118.6146 - val_loss: 1572.3575 - val_mse: 21315702.0000 - val_mae: 1572.3575\n",
      "Epoch 218/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1123.6481 - mse: 12450763.0000 - mae: 1123.6481 - val_loss: 1539.5779 - val_mse: 21110682.0000 - val_mae: 1539.5779\n",
      "Epoch 219/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1119.8581 - mse: 12418995.0000 - mae: 1119.8582 - val_loss: 1549.3058 - val_mse: 20710226.0000 - val_mae: 1549.3058\n",
      "Epoch 220/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1104.4299 - mse: 12138577.0000 - mae: 1104.4299 - val_loss: 1550.4351 - val_mse: 20625392.0000 - val_mae: 1550.4351\n",
      "Epoch 221/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1109.6444 - mse: 12267871.0000 - mae: 1109.6444 - val_loss: 1537.2461 - val_mse: 20547708.0000 - val_mae: 1537.2461\n",
      "Epoch 222/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1103.2135 - mse: 12047957.0000 - mae: 1103.2135 - val_loss: 1533.2896 - val_mse: 20600600.0000 - val_mae: 1533.2896\n",
      "Epoch 223/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1108.1881 - mse: 12226509.0000 - mae: 1108.1881 - val_loss: 1521.6703 - val_mse: 20371548.0000 - val_mae: 1521.6703\n",
      "Epoch 224/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1101.5620 - mse: 11955922.0000 - mae: 1101.5620 - val_loss: 1531.9838 - val_mse: 20287594.0000 - val_mae: 1531.9838\n",
      "Epoch 225/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1110.3742 - mse: 12192229.0000 - mae: 1110.3741 - val_loss: 1526.5421 - val_mse: 20158328.0000 - val_mae: 1526.5421\n",
      "Epoch 226/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1109.0864 - mse: 12268269.0000 - mae: 1109.0864 - val_loss: 1530.6268 - val_mse: 20141252.0000 - val_mae: 1530.6268\n",
      "Epoch 227/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1109.8797 - mse: 12147966.0000 - mae: 1109.8798 - val_loss: 1549.3218 - val_mse: 20111828.0000 - val_mae: 1549.3218\n",
      "Epoch 228/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1100.4403 - mse: 11936395.0000 - mae: 1100.4403 - val_loss: 1526.9734 - val_mse: 19989542.0000 - val_mae: 1526.9734\n",
      "Epoch 229/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1104.2691 - mse: 12201165.0000 - mae: 1104.2692 - val_loss: 1552.8792 - val_mse: 19972808.0000 - val_mae: 1552.8792\n",
      "Epoch 230/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1100.3070 - mse: 12056955.0000 - mae: 1100.3071 - val_loss: 1552.5245 - val_mse: 19948522.0000 - val_mae: 1552.5245\n",
      "Epoch 231/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1099.3086 - mse: 12004301.0000 - mae: 1099.3085 - val_loss: 1501.8485 - val_mse: 19497760.0000 - val_mae: 1501.8485\n",
      "Epoch 232/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1105.1592 - mse: 12040211.0000 - mae: 1105.1592 - val_loss: 1507.6697 - val_mse: 19571604.0000 - val_mae: 1507.6697\n",
      "Epoch 233/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1098.3432 - mse: 11898169.0000 - mae: 1098.3433 - val_loss: 1516.0201 - val_mse: 19574138.0000 - val_mae: 1516.0201\n",
      "Epoch 234/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1100.0943 - mse: 11896082.0000 - mae: 1100.0944 - val_loss: 1505.2921 - val_mse: 19092652.0000 - val_mae: 1505.2921\n",
      "Epoch 235/500\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 1090.4949 - mse: 11708039.0000 - mae: 1090.4949 - val_loss: 1509.0935 - val_mse: 18871628.0000 - val_mae: 1509.0935\n",
      "Epoch 236/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1096.0517 - mse: 11573231.0000 - mae: 1096.0518 - val_loss: 1526.0165 - val_mse: 19056892.0000 - val_mae: 1526.0165\n",
      "Epoch 237/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1096.0395 - mse: 11704819.0000 - mae: 1096.0396 - val_loss: 1492.0320 - val_mse: 19044730.0000 - val_mae: 1492.0320\n",
      "Epoch 238/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1092.4348 - mse: 11718559.0000 - mae: 1092.4347 - val_loss: 1489.9318 - val_mse: 18725876.0000 - val_mae: 1489.9318\n",
      "Epoch 239/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1097.1625 - mse: 11691443.0000 - mae: 1097.1625 - val_loss: 1507.1887 - val_mse: 18518342.0000 - val_mae: 1507.1887\n",
      "Epoch 240/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1077.2474 - mse: 11283936.0000 - mae: 1077.2473 - val_loss: 1508.9641 - val_mse: 18504912.0000 - val_mae: 1508.9641\n",
      "Epoch 241/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1093.8373 - mse: 11443670.0000 - mae: 1093.8373 - val_loss: 1481.9198 - val_mse: 18602072.0000 - val_mae: 1481.9198\n",
      "Epoch 242/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1094.5115 - mse: 11542804.0000 - mae: 1094.5115 - val_loss: 1493.8787 - val_mse: 18607680.0000 - val_mae: 1493.8787\n",
      "Epoch 243/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1083.3344 - mse: 11590915.0000 - mae: 1083.3344 - val_loss: 1494.9594 - val_mse: 18475830.0000 - val_mae: 1494.9594\n",
      "Epoch 244/500\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 1080.9403 - mse: 11365372.0000 - mae: 1080.9403 - val_loss: 1503.8561 - val_mse: 18277462.0000 - val_mae: 1503.8561\n",
      "Epoch 245/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1086.2108 - mse: 11423501.0000 - mae: 1086.2107 - val_loss: 1467.3184 - val_mse: 18054342.0000 - val_mae: 1467.3184\n",
      "Epoch 246/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1076.6261 - mse: 11302512.0000 - mae: 1076.6261 - val_loss: 1484.3473 - val_mse: 18015048.0000 - val_mae: 1484.3473\n",
      "Epoch 247/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1066.5397 - mse: 11040662.0000 - mae: 1066.5398 - val_loss: 1465.7363 - val_mse: 17515746.0000 - val_mae: 1465.7363\n",
      "Epoch 248/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1075.0982 - mse: 11209140.0000 - mae: 1075.0983 - val_loss: 1483.3811 - val_mse: 18271486.0000 - val_mae: 1483.3811\n",
      "Epoch 249/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 1074.9508 - mse: 11326794.0000 - mae: 1074.9508 - val_loss: 1461.3350 - val_mse: 17766512.0000 - val_mae: 1461.3350\n",
      "Epoch 250/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1077.1086 - mse: 11420275.0000 - mae: 1077.1086 - val_loss: 1441.4169 - val_mse: 17601612.0000 - val_mae: 1441.4169\n",
      "Epoch 251/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1079.2723 - mse: 11382311.0000 - mae: 1079.2722 - val_loss: 1444.5116 - val_mse: 17023480.0000 - val_mae: 1444.5116\n",
      "Epoch 252/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1079.6574 - mse: 11185701.0000 - mae: 1079.6573 - val_loss: 1439.4407 - val_mse: 17371636.0000 - val_mae: 1439.4407\n",
      "Epoch 253/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1069.0623 - mse: 11101868.0000 - mae: 1069.0623 - val_loss: 1495.1267 - val_mse: 17662588.0000 - val_mae: 1495.1267\n",
      "Epoch 254/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1068.3365 - mse: 11085045.0000 - mae: 1068.3365 - val_loss: 1484.9497 - val_mse: 17487284.0000 - val_mae: 1484.9497\n",
      "Epoch 255/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1064.8496 - mse: 10884162.0000 - mae: 1064.8496 - val_loss: 1441.8179 - val_mse: 17002478.0000 - val_mae: 1441.8179\n",
      "Epoch 256/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1066.2370 - mse: 10956428.0000 - mae: 1066.2371 - val_loss: 1436.5819 - val_mse: 17154708.0000 - val_mae: 1436.5819\n",
      "Epoch 257/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1072.7019 - mse: 11059292.0000 - mae: 1072.7020 - val_loss: 1460.5417 - val_mse: 17000886.0000 - val_mae: 1460.5417\n",
      "Epoch 258/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1063.0924 - mse: 10888978.0000 - mae: 1063.0924 - val_loss: 1455.5415 - val_mse: 16828176.0000 - val_mae: 1455.5415\n",
      "Epoch 259/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 1049.3406 - mse: 10673893.0000 - mae: 1049.3406 - val_loss: 1460.7853 - val_mse: 16818842.0000 - val_mae: 1460.7853\n",
      "Epoch 260/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1062.1112 - mse: 10744490.0000 - mae: 1062.1112 - val_loss: 1458.3085 - val_mse: 16861848.0000 - val_mae: 1458.3085\n",
      "Epoch 261/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 1057.4610 - mse: 10759322.0000 - mae: 1057.4608 - val_loss: 1447.5674 - val_mse: 16394190.0000 - val_mae: 1447.5674\n",
      "Epoch 262/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 1047.4860 - mse: 10533836.0000 - mae: 1047.4860 - val_loss: 1411.3446 - val_mse: 16300817.0000 - val_mae: 1411.3446\n",
      "Epoch 263/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1069.3252 - mse: 11068347.0000 - mae: 1069.3252 - val_loss: 1460.6198 - val_mse: 16649986.0000 - val_mae: 1460.6198\n",
      "Epoch 264/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1054.9563 - mse: 10641775.0000 - mae: 1054.9563 - val_loss: 1453.8491 - val_mse: 16492039.0000 - val_mae: 1453.8491\n",
      "Epoch 265/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1050.4954 - mse: 10696554.0000 - mae: 1050.4954 - val_loss: 1450.9031 - val_mse: 15961542.0000 - val_mae: 1450.9031\n",
      "Epoch 266/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1058.8614 - mse: 10749209.0000 - mae: 1058.8613 - val_loss: 1461.3789 - val_mse: 15888797.0000 - val_mae: 1461.3789\n",
      "Epoch 267/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1044.9406 - mse: 10658916.0000 - mae: 1044.9407 - val_loss: 1426.7399 - val_mse: 16316361.0000 - val_mae: 1426.7399\n",
      "Epoch 268/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1063.1700 - mse: 10738726.0000 - mae: 1063.1700 - val_loss: 1458.0739 - val_mse: 15673128.0000 - val_mae: 1458.0739\n",
      "Epoch 269/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1064.8508 - mse: 10827711.0000 - mae: 1064.8507 - val_loss: 1450.5564 - val_mse: 15760964.0000 - val_mae: 1450.5564\n",
      "Epoch 270/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1047.7985 - mse: 10525487.0000 - mae: 1047.7985 - val_loss: 1413.0800 - val_mse: 15609527.0000 - val_mae: 1413.0800\n",
      "Epoch 271/500\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 1040.9289 - mse: 10521426.0000 - mae: 1040.9288 - val_loss: 1470.3876 - val_mse: 15589754.0000 - val_mae: 1470.3876\n",
      "Epoch 272/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1033.1298 - mse: 10223258.0000 - mae: 1033.1296 - val_loss: 1473.9198 - val_mse: 15383875.0000 - val_mae: 1473.9198\n",
      "Epoch 273/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 1052.0404 - mse: 10691777.0000 - mae: 1052.0404 - val_loss: 1428.0896 - val_mse: 14987424.0000 - val_mae: 1428.0896\n",
      "Epoch 274/500\n",
      "4000/4000 [==============================] - 0s 52us/step - loss: 1039.4039 - mse: 10304556.0000 - mae: 1039.4039 - val_loss: 1418.1324 - val_mse: 15007082.0000 - val_mae: 1418.1324\n",
      "Epoch 275/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1042.4748 - mse: 10355977.0000 - mae: 1042.4749 - val_loss: 1405.1505 - val_mse: 15258763.0000 - val_mae: 1405.1505\n",
      "Epoch 276/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1036.2957 - mse: 10325794.0000 - mae: 1036.2958 - val_loss: 1409.8241 - val_mse: 14768996.0000 - val_mae: 1409.8241\n",
      "Epoch 277/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1024.5716 - mse: 10049234.0000 - mae: 1024.5717 - val_loss: 1409.5734 - val_mse: 14958416.0000 - val_mae: 1409.5734\n",
      "Epoch 278/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1043.3960 - mse: 10393332.0000 - mae: 1043.3960 - val_loss: 1382.6664 - val_mse: 14516313.0000 - val_mae: 1382.6664\n",
      "Epoch 279/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 1031.6749 - mse: 10250201.0000 - mae: 1031.6750 - val_loss: 1418.0631 - val_mse: 14731462.0000 - val_mae: 1418.0631\n",
      "Epoch 280/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1025.7064 - mse: 10114247.0000 - mae: 1025.7065 - val_loss: 1440.1708 - val_mse: 14831838.0000 - val_mae: 1440.1708\n",
      "Epoch 281/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1036.3330 - mse: 10371681.0000 - mae: 1036.3329 - val_loss: 1403.3273 - val_mse: 14456489.0000 - val_mae: 1403.3273\n",
      "Epoch 282/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1031.6821 - mse: 10178996.0000 - mae: 1031.6821 - val_loss: 1403.0435 - val_mse: 14542688.0000 - val_mae: 1403.0435\n",
      "Epoch 283/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1030.9089 - mse: 10215343.0000 - mae: 1030.9089 - val_loss: 1398.9113 - val_mse: 14035719.0000 - val_mae: 1398.9113\n",
      "Epoch 284/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 1030.7493 - mse: 10157979.0000 - mae: 1030.7494 - val_loss: 1404.2858 - val_mse: 13740219.0000 - val_mae: 1404.2858\n",
      "Epoch 285/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 1005.8615 - mse: 9896387.0000 - mae: 1005.8615 - val_loss: 1447.5955 - val_mse: 14125121.0000 - val_mae: 1447.5955\n",
      "Epoch 286/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1000.4686 - mse: 9620239.0000 - mae: 1000.4685 - val_loss: 1377.8661 - val_mse: 13681067.0000 - val_mae: 1377.8661\n",
      "Epoch 287/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1029.6122 - mse: 10107300.0000 - mae: 1029.6122 - val_loss: 1395.0052 - val_mse: 13432545.0000 - val_mae: 1395.0052\n",
      "Epoch 288/500\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 1012.7995 - mse: 9974802.0000 - mae: 1012.7995 - val_loss: 1393.3824 - val_mse: 14001937.0000 - val_mae: 1393.3824\n",
      "Epoch 289/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1032.7685 - mse: 10169815.0000 - mae: 1032.7686 - val_loss: 1409.5005 - val_mse: 13649393.0000 - val_mae: 1409.5005\n",
      "Epoch 290/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 1009.0293 - mse: 9798174.0000 - mae: 1009.0293 - val_loss: 1371.5697 - val_mse: 13850093.0000 - val_mae: 1371.5697\n",
      "Epoch 291/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1015.6518 - mse: 9949878.0000 - mae: 1015.6519 - val_loss: 1399.8385 - val_mse: 14325946.0000 - val_mae: 1399.8385\n",
      "Epoch 292/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1024.5170 - mse: 10047173.0000 - mae: 1024.5170 - val_loss: 1377.6804 - val_mse: 13946414.0000 - val_mae: 1377.6804\n",
      "Epoch 293/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1024.5217 - mse: 9978319.0000 - mae: 1024.5216 - val_loss: 1372.3549 - val_mse: 13156121.0000 - val_mae: 1372.3549\n",
      "Epoch 294/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 1015.2214 - mse: 9957046.0000 - mae: 1015.2214 - val_loss: 1408.9484 - val_mse: 13372628.0000 - val_mae: 1408.9484\n",
      "Epoch 295/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1013.9883 - mse: 9900299.0000 - mae: 1013.9882 - val_loss: 1381.7802 - val_mse: 13096290.0000 - val_mae: 1381.7802\n",
      "Epoch 296/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 1019.6466 - mse: 10078065.0000 - mae: 1019.6465 - val_loss: 1392.9956 - val_mse: 13745253.0000 - val_mae: 1392.9956\n",
      "Epoch 297/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1013.1140 - mse: 9925057.0000 - mae: 1013.1140 - val_loss: 1391.2594 - val_mse: 13059507.0000 - val_mae: 1391.2594\n",
      "Epoch 298/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1013.7112 - mse: 9941489.0000 - mae: 1013.7112 - val_loss: 1407.7285 - val_mse: 13140970.0000 - val_mae: 1407.7285\n",
      "Epoch 299/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1000.7564 - mse: 9696292.0000 - mae: 1000.7563 - val_loss: 1389.3341 - val_mse: 12846256.0000 - val_mae: 1389.3341\n",
      "Epoch 300/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 1007.0828 - mse: 9881797.0000 - mae: 1007.0828 - val_loss: 1370.9806 - val_mse: 12793629.0000 - val_mae: 1370.9806\n",
      "Epoch 301/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 997.9375 - mse: 9709998.0000 - mae: 997.9375 - val_loss: 1402.3113 - val_mse: 12884191.0000 - val_mae: 1402.3113\n",
      "Epoch 302/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1017.6201 - mse: 10037723.0000 - mae: 1017.6201 - val_loss: 1387.6610 - val_mse: 12994154.0000 - val_mae: 1387.6610\n",
      "Epoch 303/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 992.8739 - mse: 9645612.0000 - mae: 992.8740 - val_loss: 1381.9923 - val_mse: 12682421.0000 - val_mae: 1381.9923\n",
      "Epoch 304/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1005.0765 - mse: 9891246.0000 - mae: 1005.0765 - val_loss: 1376.0751 - val_mse: 12684578.0000 - val_mae: 1376.0751\n",
      "Epoch 305/500\n",
      "4000/4000 [==============================] - 0s 50us/step - loss: 997.0624 - mse: 9642602.0000 - mae: 997.0624 - val_loss: 1398.9708 - val_mse: 12311717.0000 - val_mae: 1398.9708\n",
      "Epoch 306/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 1006.4517 - mse: 9691216.0000 - mae: 1006.4517 - val_loss: 1385.0122 - val_mse: 12642841.0000 - val_mae: 1385.0122\n",
      "Epoch 307/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 997.7907 - mse: 9744058.0000 - mae: 997.7906 - val_loss: 1352.0002 - val_mse: 12613434.0000 - val_mae: 1352.0002\n",
      "Epoch 308/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1015.2646 - mse: 9911179.0000 - mae: 1015.2646 - val_loss: 1362.6711 - val_mse: 12189564.0000 - val_mae: 1362.6711\n",
      "Epoch 309/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 1004.1471 - mse: 9634432.0000 - mae: 1004.1471 - val_loss: 1355.0841 - val_mse: 12107049.0000 - val_mae: 1355.0841\n",
      "Epoch 310/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 995.5317 - mse: 9666692.0000 - mae: 995.5316 - val_loss: 1356.9489 - val_mse: 12415489.0000 - val_mae: 1356.9489\n",
      "Epoch 311/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 996.3189 - mse: 9718599.0000 - mae: 996.3188 - val_loss: 1347.1865 - val_mse: 12123334.0000 - val_mae: 1347.1865\n",
      "Epoch 312/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 1004.6854 - mse: 9757753.0000 - mae: 1004.6854 - val_loss: 1333.2941 - val_mse: 11747210.0000 - val_mae: 1333.2941\n",
      "Epoch 313/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 1011.2874 - mse: 9928587.0000 - mae: 1011.2874 - val_loss: 1368.8745 - val_mse: 11844186.0000 - val_mae: 1368.8745\n",
      "Epoch 314/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 989.8627 - mse: 9565375.0000 - mae: 989.8627 - val_loss: 1351.1095 - val_mse: 11519857.0000 - val_mae: 1351.1095\n",
      "Epoch 315/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 1003.1373 - mse: 9556287.0000 - mae: 1003.1373 - val_loss: 1362.0208 - val_mse: 11868001.0000 - val_mae: 1362.0208\n",
      "Epoch 316/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 990.9538 - mse: 9607594.0000 - mae: 990.9539 - val_loss: 1338.3458 - val_mse: 11392991.0000 - val_mae: 1338.3458\n",
      "Epoch 317/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 1010.7396 - mse: 9744927.0000 - mae: 1010.7396 - val_loss: 1371.4260 - val_mse: 12252734.0000 - val_mae: 1371.4260\n",
      "Epoch 318/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 1003.3403 - mse: 9798711.0000 - mae: 1003.3403 - val_loss: 1364.6479 - val_mse: 12424076.0000 - val_mae: 1364.6479\n",
      "Epoch 319/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 985.5781 - mse: 9572686.0000 - mae: 985.5780 - val_loss: 1360.1570 - val_mse: 11910243.0000 - val_mae: 1360.1570\n",
      "Epoch 320/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 982.6236 - mse: 9717111.0000 - mae: 982.6236 - val_loss: 1344.4474 - val_mse: 11654130.0000 - val_mae: 1344.4474\n",
      "Epoch 321/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 989.9338 - mse: 9653680.0000 - mae: 989.9338 - val_loss: 1354.2784 - val_mse: 11607024.0000 - val_mae: 1354.2784\n",
      "Epoch 322/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 982.3217 - mse: 9410428.0000 - mae: 982.3217 - val_loss: 1350.2953 - val_mse: 11599583.0000 - val_mae: 1350.2953\n",
      "Epoch 323/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 982.7143 - mse: 9511365.0000 - mae: 982.7144 - val_loss: 1388.2933 - val_mse: 11717438.0000 - val_mae: 1388.2933\n",
      "Epoch 324/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 994.9514 - mse: 9628613.0000 - mae: 994.9514 - val_loss: 1341.4470 - val_mse: 11270731.0000 - val_mae: 1341.4470\n",
      "Epoch 325/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 977.0252 - mse: 9382799.0000 - mae: 977.0251 - val_loss: 1361.8173 - val_mse: 11538284.0000 - val_mae: 1361.8173\n",
      "Epoch 326/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 990.7527 - mse: 9650756.0000 - mae: 990.7526 - val_loss: 1324.6761 - val_mse: 11218408.0000 - val_mae: 1324.6761\n",
      "Epoch 327/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 993.8015 - mse: 9690031.0000 - mae: 993.8015 - val_loss: 1348.2893 - val_mse: 11811154.0000 - val_mae: 1348.2893\n",
      "Epoch 328/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 987.5996 - mse: 9581423.0000 - mae: 987.5996 - val_loss: 1320.3080 - val_mse: 11546425.0000 - val_mae: 1320.3080\n",
      "Epoch 329/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 989.7484 - mse: 9647615.0000 - mae: 989.7484 - val_loss: 1344.0819 - val_mse: 11704842.0000 - val_mae: 1344.0819\n",
      "Epoch 330/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 969.0647 - mse: 9316336.0000 - mae: 969.0646 - val_loss: 1329.5792 - val_mse: 11212177.0000 - val_mae: 1329.5792\n",
      "Epoch 331/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 975.4340 - mse: 9538375.0000 - mae: 975.4340 - val_loss: 1327.1378 - val_mse: 11835540.0000 - val_mae: 1327.1378\n",
      "Epoch 332/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 968.7830 - mse: 9329922.0000 - mae: 968.7830 - val_loss: 1324.3065 - val_mse: 11355277.0000 - val_mae: 1324.3065\n",
      "Epoch 333/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 982.6199 - mse: 9569435.0000 - mae: 982.6199 - val_loss: 1299.6610 - val_mse: 11294992.0000 - val_mae: 1299.6610\n",
      "Epoch 334/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 999.7961 - mse: 9894491.0000 - mae: 999.7961 - val_loss: 1349.4208 - val_mse: 11483928.0000 - val_mae: 1349.4208\n",
      "Epoch 335/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 974.7920 - mse: 9398127.0000 - mae: 974.7920 - val_loss: 1322.8627 - val_mse: 11594042.0000 - val_mae: 1322.8627\n",
      "Epoch 336/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 973.6057 - mse: 9364472.0000 - mae: 973.6057 - val_loss: 1291.6897 - val_mse: 11058822.0000 - val_mae: 1291.6897\n",
      "Epoch 337/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 973.7933 - mse: 9283622.0000 - mae: 973.7932 - val_loss: 1306.1605 - val_mse: 11587247.0000 - val_mae: 1306.1605\n",
      "Epoch 338/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 970.7860 - mse: 9327794.0000 - mae: 970.7860 - val_loss: 1359.7843 - val_mse: 11552370.0000 - val_mae: 1359.7843\n",
      "Epoch 339/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 973.2014 - mse: 9399207.0000 - mae: 973.2014 - val_loss: 1328.7517 - val_mse: 11380343.0000 - val_mae: 1328.7517\n",
      "Epoch 340/500\n",
      "4000/4000 [==============================] - 0s 51us/step - loss: 976.4169 - mse: 9492519.0000 - mae: 976.4169 - val_loss: 1319.7299 - val_mse: 11268127.0000 - val_mae: 1319.7299\n",
      "Epoch 341/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 982.6896 - mse: 9523466.0000 - mae: 982.6896 - val_loss: 1327.5776 - val_mse: 11054475.0000 - val_mae: 1327.5776\n",
      "Epoch 342/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 987.0808 - mse: 9582243.0000 - mae: 987.0808 - val_loss: 1301.7776 - val_mse: 10947742.0000 - val_mae: 1301.7776\n",
      "Epoch 343/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 971.5539 - mse: 9412547.0000 - mae: 971.5538 - val_loss: 1323.9828 - val_mse: 11474140.0000 - val_mae: 1323.9828\n",
      "Epoch 344/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 978.5912 - mse: 9512824.0000 - mae: 978.5912 - val_loss: 1341.0325 - val_mse: 11407803.0000 - val_mae: 1341.0325\n",
      "Epoch 345/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 979.4569 - mse: 9427753.0000 - mae: 979.4569 - val_loss: 1341.3448 - val_mse: 11374242.0000 - val_mae: 1341.3448\n",
      "Epoch 346/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 974.5674 - mse: 9458809.0000 - mae: 974.5674 - val_loss: 1318.5341 - val_mse: 11474768.0000 - val_mae: 1318.5341\n",
      "Epoch 347/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 962.2521 - mse: 9331839.0000 - mae: 962.2521 - val_loss: 1350.4585 - val_mse: 11416061.0000 - val_mae: 1350.4585\n",
      "Epoch 348/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 974.9287 - mse: 9397472.0000 - mae: 974.9287 - val_loss: 1363.8533 - val_mse: 11765610.0000 - val_mae: 1363.8533\n",
      "Epoch 349/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 977.5667 - mse: 9419531.0000 - mae: 977.5667 - val_loss: 1360.3015 - val_mse: 12069450.0000 - val_mae: 1360.3015\n",
      "Epoch 350/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 980.3403 - mse: 9448501.0000 - mae: 980.3403 - val_loss: 1350.5458 - val_mse: 11643272.0000 - val_mae: 1350.5458\n",
      "Epoch 351/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 983.2650 - mse: 9539767.0000 - mae: 983.2651 - val_loss: 1343.6677 - val_mse: 11649352.0000 - val_mae: 1343.6677\n",
      "Epoch 352/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 972.6779 - mse: 9296396.0000 - mae: 972.6779 - val_loss: 1308.9011 - val_mse: 11391691.0000 - val_mae: 1308.9011\n",
      "Epoch 353/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 967.1658 - mse: 9280335.0000 - mae: 967.1658 - val_loss: 1336.8059 - val_mse: 11709042.0000 - val_mae: 1336.8059\n",
      "Epoch 354/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 986.9806 - mse: 9621405.0000 - mae: 986.9807 - val_loss: 1282.6656 - val_mse: 11200242.0000 - val_mae: 1282.6656\n",
      "Epoch 355/500\n",
      "4000/4000 [==============================] - 0s 50us/step - loss: 963.8554 - mse: 9331439.0000 - mae: 963.8553 - val_loss: 1329.2623 - val_mse: 11308716.0000 - val_mae: 1329.2623\n",
      "Epoch 356/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 973.0482 - mse: 9452325.0000 - mae: 973.0482 - val_loss: 1329.2413 - val_mse: 11477335.0000 - val_mae: 1329.2413\n",
      "Epoch 357/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 980.5044 - mse: 9586990.0000 - mae: 980.5044 - val_loss: 1311.1912 - val_mse: 10710885.0000 - val_mae: 1311.1912\n",
      "Epoch 358/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 970.3600 - mse: 9490714.0000 - mae: 970.3600 - val_loss: 1318.1031 - val_mse: 11654188.0000 - val_mae: 1318.1031\n",
      "Epoch 359/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 964.8463 - mse: 9305510.0000 - mae: 964.8463 - val_loss: 1341.0690 - val_mse: 11409592.0000 - val_mae: 1341.0690\n",
      "Epoch 360/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 976.7192 - mse: 9443514.0000 - mae: 976.7192 - val_loss: 1273.3270 - val_mse: 10853873.0000 - val_mae: 1273.3270\n",
      "Epoch 361/500\n",
      "4000/4000 [==============================] - 0s 50us/step - loss: 977.1686 - mse: 9420165.0000 - mae: 977.1686 - val_loss: 1346.9471 - val_mse: 12095443.0000 - val_mae: 1346.9471\n",
      "Epoch 362/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 981.3082 - mse: 9533158.0000 - mae: 981.3082 - val_loss: 1348.6493 - val_mse: 11517420.0000 - val_mae: 1348.6493\n",
      "Epoch 363/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 968.3232 - mse: 9299638.0000 - mae: 968.3232 - val_loss: 1359.1523 - val_mse: 11694934.0000 - val_mae: 1359.1523\n",
      "Epoch 364/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 981.6029 - mse: 9709742.0000 - mae: 981.6030 - val_loss: 1368.0164 - val_mse: 11948072.0000 - val_mae: 1368.0164\n",
      "Epoch 365/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 960.8056 - mse: 9338532.0000 - mae: 960.8057 - val_loss: 1311.4570 - val_mse: 11349852.0000 - val_mae: 1311.4570\n",
      "Epoch 366/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 962.3586 - mse: 9431555.0000 - mae: 962.3586 - val_loss: 1342.2212 - val_mse: 11512532.0000 - val_mae: 1342.2212\n",
      "Epoch 367/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 974.4676 - mse: 9355624.0000 - mae: 974.4677 - val_loss: 1368.5526 - val_mse: 11847130.0000 - val_mae: 1368.5526\n",
      "Epoch 368/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 991.8330 - mse: 9724993.0000 - mae: 991.8331 - val_loss: 1355.3749 - val_mse: 12063758.0000 - val_mae: 1355.3749\n",
      "Epoch 369/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 978.5194 - mse: 9411177.0000 - mae: 978.5195 - val_loss: 1367.0728 - val_mse: 11757057.0000 - val_mae: 1367.0728\n",
      "Epoch 370/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 961.1545 - mse: 9287448.0000 - mae: 961.1545 - val_loss: 1359.6580 - val_mse: 12192910.0000 - val_mae: 1359.6580\n",
      "Epoch 371/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 982.2113 - mse: 9533721.0000 - mae: 982.2112 - val_loss: 1345.3944 - val_mse: 11857379.0000 - val_mae: 1345.3944\n",
      "Epoch 372/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 977.6174 - mse: 9569389.0000 - mae: 977.6173 - val_loss: 1351.2577 - val_mse: 12018737.0000 - val_mae: 1351.2577\n",
      "Epoch 373/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 970.6814 - mse: 9483471.0000 - mae: 970.6814 - val_loss: 1291.8252 - val_mse: 11228059.0000 - val_mae: 1291.8252\n",
      "Epoch 374/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 980.1912 - mse: 9602922.0000 - mae: 980.1912 - val_loss: 1316.7120 - val_mse: 11064571.0000 - val_mae: 1316.7120\n",
      "Epoch 375/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 964.4610 - mse: 9407663.0000 - mae: 964.4610 - val_loss: 1340.5894 - val_mse: 11358025.0000 - val_mae: 1340.5894\n",
      "Epoch 376/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 967.3978 - mse: 9329649.0000 - mae: 967.3978 - val_loss: 1314.6725 - val_mse: 11076582.0000 - val_mae: 1314.6725\n",
      "Epoch 377/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 955.1989 - mse: 9179375.0000 - mae: 955.1989 - val_loss: 1321.4375 - val_mse: 10742044.0000 - val_mae: 1321.4375\n",
      "Epoch 378/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 974.8865 - mse: 9415107.0000 - mae: 974.8865 - val_loss: 1330.1915 - val_mse: 11735918.0000 - val_mae: 1330.1915\n",
      "Epoch 379/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 974.6901 - mse: 9329159.0000 - mae: 974.6901 - val_loss: 1367.2025 - val_mse: 12652302.0000 - val_mae: 1367.2025\n",
      "Epoch 380/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 959.8476 - mse: 9241158.0000 - mae: 959.8476 - val_loss: 1357.5166 - val_mse: 11894178.0000 - val_mae: 1357.5166\n",
      "Epoch 381/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 964.1652 - mse: 9227514.0000 - mae: 964.1651 - val_loss: 1324.5504 - val_mse: 11585098.0000 - val_mae: 1324.5504\n",
      "Epoch 382/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 971.3921 - mse: 9341871.0000 - mae: 971.3922 - val_loss: 1354.4432 - val_mse: 11827385.0000 - val_mae: 1354.4432\n",
      "Epoch 383/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 971.6887 - mse: 9563718.0000 - mae: 971.6887 - val_loss: 1337.5828 - val_mse: 11955478.0000 - val_mae: 1337.5828\n",
      "Epoch 384/500\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 982.4949 - mse: 9624766.0000 - mae: 982.4949 - val_loss: 1362.6699 - val_mse: 11772244.0000 - val_mae: 1362.6699\n",
      "Epoch 385/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 954.5185 - mse: 9184555.0000 - mae: 954.5184 - val_loss: 1379.7549 - val_mse: 12215685.0000 - val_mae: 1379.7549\n",
      "Epoch 386/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 961.7054 - mse: 9314135.0000 - mae: 961.7054 - val_loss: 1352.1681 - val_mse: 11488418.0000 - val_mae: 1352.1681\n",
      "Epoch 387/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 963.6673 - mse: 9280365.0000 - mae: 963.6673 - val_loss: 1323.7151 - val_mse: 11756145.0000 - val_mae: 1323.7151\n",
      "Epoch 388/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 973.5948 - mse: 9392311.0000 - mae: 973.5948 - val_loss: 1332.0992 - val_mse: 11493647.0000 - val_mae: 1332.0992\n",
      "Epoch 389/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 962.6929 - mse: 9243007.0000 - mae: 962.6929 - val_loss: 1342.6525 - val_mse: 11572808.0000 - val_mae: 1342.6525\n",
      "Epoch 390/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 959.1902 - mse: 9241946.0000 - mae: 959.1902 - val_loss: 1342.0117 - val_mse: 12188417.0000 - val_mae: 1342.0117\n",
      "Epoch 391/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 965.3699 - mse: 9395011.0000 - mae: 965.3699 - val_loss: 1331.3497 - val_mse: 11543220.0000 - val_mae: 1331.3497\n",
      "Epoch 392/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 958.6565 - mse: 9321138.0000 - mae: 958.6565 - val_loss: 1372.9967 - val_mse: 12240503.0000 - val_mae: 1372.9967\n",
      "Epoch 393/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 969.7033 - mse: 9389996.0000 - mae: 969.7032 - val_loss: 1381.9464 - val_mse: 12491501.0000 - val_mae: 1381.9464\n",
      "Epoch 394/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 952.1981 - mse: 9164249.0000 - mae: 952.1980 - val_loss: 1404.6761 - val_mse: 13234198.0000 - val_mae: 1404.6761\n",
      "Epoch 395/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 968.8690 - mse: 9464089.0000 - mae: 968.8690 - val_loss: 1384.1071 - val_mse: 12345146.0000 - val_mae: 1384.1071\n",
      "Epoch 396/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 969.0945 - mse: 9462970.0000 - mae: 969.0945 - val_loss: 1382.7432 - val_mse: 12514349.0000 - val_mae: 1382.7432\n",
      "Epoch 397/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 946.1466 - mse: 9046382.0000 - mae: 946.1466 - val_loss: 1363.7177 - val_mse: 11813150.0000 - val_mae: 1363.7177\n",
      "Epoch 398/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 967.0164 - mse: 9421076.0000 - mae: 967.0164 - val_loss: 1332.2316 - val_mse: 11753349.0000 - val_mae: 1332.2316\n",
      "Epoch 399/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 942.8863 - mse: 9075444.0000 - mae: 942.8862 - val_loss: 1329.4674 - val_mse: 11211021.0000 - val_mae: 1329.4674\n",
      "Epoch 400/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 959.3942 - mse: 9290737.0000 - mae: 959.3942 - val_loss: 1321.1271 - val_mse: 11607461.0000 - val_mae: 1321.1271\n",
      "Epoch 401/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 976.1776 - mse: 9550060.0000 - mae: 976.1776 - val_loss: 1383.9338 - val_mse: 12326110.0000 - val_mae: 1383.9338\n",
      "Epoch 402/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 962.4614 - mse: 9210308.0000 - mae: 962.4614 - val_loss: 1312.9376 - val_mse: 11104087.0000 - val_mae: 1312.9376\n",
      "Epoch 403/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 972.5639 - mse: 9355616.0000 - mae: 972.5640 - val_loss: 1345.4154 - val_mse: 12017465.0000 - val_mae: 1345.4154\n",
      "Epoch 404/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 960.9194 - mse: 9253531.0000 - mae: 960.9194 - val_loss: 1326.6898 - val_mse: 11459616.0000 - val_mae: 1326.6898\n",
      "Epoch 405/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 973.0104 - mse: 9466906.0000 - mae: 973.0104 - val_loss: 1373.8224 - val_mse: 11647220.0000 - val_mae: 1373.8224\n",
      "Epoch 406/500\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 953.1491 - mse: 9218040.0000 - mae: 953.1490 - val_loss: 1360.2161 - val_mse: 12011488.0000 - val_mae: 1360.2161\n",
      "Epoch 407/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 951.8161 - mse: 9159507.0000 - mae: 951.8160 - val_loss: 1340.1150 - val_mse: 11974082.0000 - val_mae: 1340.1150\n",
      "Epoch 408/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 965.4601 - mse: 9357210.0000 - mae: 965.4601 - val_loss: 1354.4780 - val_mse: 11746568.0000 - val_mae: 1354.4780\n",
      "Epoch 409/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 965.2237 - mse: 9383889.0000 - mae: 965.2237 - val_loss: 1343.9904 - val_mse: 11517364.0000 - val_mae: 1343.9904\n",
      "Epoch 410/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 955.3449 - mse: 9125596.0000 - mae: 955.3448 - val_loss: 1379.7200 - val_mse: 11529550.0000 - val_mae: 1379.7200\n",
      "Epoch 411/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 961.1302 - mse: 9289456.0000 - mae: 961.1302 - val_loss: 1346.4233 - val_mse: 11829447.0000 - val_mae: 1346.4233\n",
      "Epoch 412/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 975.2447 - mse: 9593611.0000 - mae: 975.2447 - val_loss: 1364.3239 - val_mse: 12067821.0000 - val_mae: 1364.3239\n",
      "Epoch 413/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 952.9239 - mse: 9214320.0000 - mae: 952.9240 - val_loss: 1325.1552 - val_mse: 11122339.0000 - val_mae: 1325.1552\n",
      "Epoch 414/500\n",
      "4000/4000 [==============================] - 0s 50us/step - loss: 962.0240 - mse: 9293863.0000 - mae: 962.0241 - val_loss: 1316.7653 - val_mse: 11389133.0000 - val_mae: 1316.7653\n",
      "Epoch 415/500\n",
      "4000/4000 [==============================] - 0s 50us/step - loss: 960.2415 - mse: 9304360.0000 - mae: 960.2415 - val_loss: 1326.3483 - val_mse: 11231913.0000 - val_mae: 1326.3483\n",
      "Epoch 416/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 961.9548 - mse: 9216430.0000 - mae: 961.9548 - val_loss: 1323.8140 - val_mse: 11499365.0000 - val_mae: 1323.8140\n",
      "Epoch 417/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 959.1676 - mse: 9282883.0000 - mae: 959.1675 - val_loss: 1354.0187 - val_mse: 11832914.0000 - val_mae: 1354.0187\n",
      "Epoch 418/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 951.9043 - mse: 9214593.0000 - mae: 951.9043 - val_loss: 1343.3552 - val_mse: 11811449.0000 - val_mae: 1343.3552\n",
      "Epoch 419/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 961.6734 - mse: 9282421.0000 - mae: 961.6734 - val_loss: 1342.6549 - val_mse: 11942940.0000 - val_mae: 1342.6549\n",
      "Epoch 420/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 955.1937 - mse: 9323180.0000 - mae: 955.1937 - val_loss: 1325.6669 - val_mse: 11402359.0000 - val_mae: 1325.6669\n",
      "Epoch 421/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 960.9613 - mse: 9215653.0000 - mae: 960.9613 - val_loss: 1386.7971 - val_mse: 12337070.0000 - val_mae: 1386.7971\n",
      "Epoch 422/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 970.2250 - mse: 9356075.0000 - mae: 970.2250 - val_loss: 1361.0054 - val_mse: 11792084.0000 - val_mae: 1361.0054\n",
      "Epoch 423/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 958.2359 - mse: 9226896.0000 - mae: 958.2360 - val_loss: 1362.9122 - val_mse: 11544749.0000 - val_mae: 1362.9122\n",
      "Epoch 424/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 965.8976 - mse: 9250269.0000 - mae: 965.8976 - val_loss: 1357.6545 - val_mse: 12280871.0000 - val_mae: 1357.6545\n",
      "Epoch 425/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 959.5551 - mse: 9360487.0000 - mae: 959.5552 - val_loss: 1345.9608 - val_mse: 11934534.0000 - val_mae: 1345.9608\n",
      "Epoch 426/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 957.2944 - mse: 9410767.0000 - mae: 957.2944 - val_loss: 1356.9574 - val_mse: 12121926.0000 - val_mae: 1356.9574\n",
      "Epoch 427/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 959.5738 - mse: 9239355.0000 - mae: 959.5738 - val_loss: 1340.4603 - val_mse: 11297417.0000 - val_mae: 1340.4603\n",
      "Epoch 428/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 953.7744 - mse: 9132697.0000 - mae: 953.7744 - val_loss: 1350.5817 - val_mse: 11458587.0000 - val_mae: 1350.5817\n",
      "Epoch 429/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 957.9848 - mse: 9156902.0000 - mae: 957.9847 - val_loss: 1375.0155 - val_mse: 11893364.0000 - val_mae: 1375.0155\n",
      "Epoch 430/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 953.1252 - mse: 9222585.0000 - mae: 953.1252 - val_loss: 1394.7234 - val_mse: 12274885.0000 - val_mae: 1394.7234\n",
      "Epoch 431/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 966.8278 - mse: 9434864.0000 - mae: 966.8278 - val_loss: 1328.9639 - val_mse: 11510531.0000 - val_mae: 1328.9639\n",
      "Epoch 432/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 955.7332 - mse: 9077557.0000 - mae: 955.7333 - val_loss: 1339.4388 - val_mse: 11519891.0000 - val_mae: 1339.4388\n",
      "Epoch 433/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 946.0509 - mse: 9097174.0000 - mae: 946.0509 - val_loss: 1379.4031 - val_mse: 11687651.0000 - val_mae: 1379.4031\n",
      "Epoch 434/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 954.2072 - mse: 9164318.0000 - mae: 954.2073 - val_loss: 1362.5052 - val_mse: 11908961.0000 - val_mae: 1362.5052\n",
      "Epoch 435/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 963.0530 - mse: 9233922.0000 - mae: 963.0529 - val_loss: 1374.9440 - val_mse: 12396188.0000 - val_mae: 1374.9440\n",
      "Epoch 436/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 969.3624 - mse: 9352134.0000 - mae: 969.3624 - val_loss: 1357.0023 - val_mse: 11568870.0000 - val_mae: 1357.0023\n",
      "Epoch 437/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 960.0318 - mse: 9100355.0000 - mae: 960.0317 - val_loss: 1345.4952 - val_mse: 11899455.0000 - val_mae: 1345.4952\n",
      "Epoch 438/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 960.6428 - mse: 9232506.0000 - mae: 960.6428 - val_loss: 1347.6428 - val_mse: 11314301.0000 - val_mae: 1347.6428\n",
      "Epoch 439/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 942.9272 - mse: 8916914.0000 - mae: 942.9271 - val_loss: 1342.6349 - val_mse: 11776749.0000 - val_mae: 1342.6349\n",
      "Epoch 440/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 955.8008 - mse: 9090309.0000 - mae: 955.8007 - val_loss: 1322.8231 - val_mse: 11451573.0000 - val_mae: 1322.8231\n",
      "Epoch 441/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 948.9212 - mse: 8975404.0000 - mae: 948.9212 - val_loss: 1349.1077 - val_mse: 11633315.0000 - val_mae: 1349.1077\n",
      "Epoch 442/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 954.7811 - mse: 9182051.0000 - mae: 954.7811 - val_loss: 1325.2230 - val_mse: 11637002.0000 - val_mae: 1325.2230\n",
      "Epoch 443/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 961.6292 - mse: 9332165.0000 - mae: 961.6293 - val_loss: 1331.3367 - val_mse: 11706824.0000 - val_mae: 1331.3367\n",
      "Epoch 444/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 943.3818 - mse: 9018510.0000 - mae: 943.3819 - val_loss: 1330.7190 - val_mse: 11175226.0000 - val_mae: 1330.7190\n",
      "Epoch 445/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 946.3161 - mse: 9102385.0000 - mae: 946.3161 - val_loss: 1340.6418 - val_mse: 11306285.0000 - val_mae: 1340.6418\n",
      "Epoch 446/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 973.1295 - mse: 9357296.0000 - mae: 973.1295 - val_loss: 1346.1235 - val_mse: 11597201.0000 - val_mae: 1346.1235\n",
      "Epoch 447/500\n",
      "4000/4000 [==============================] - 0s 51us/step - loss: 961.6635 - mse: 9358845.0000 - mae: 961.6635 - val_loss: 1355.9044 - val_mse: 11955622.0000 - val_mae: 1355.9044\n",
      "Epoch 448/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 960.8445 - mse: 9287081.0000 - mae: 960.8444 - val_loss: 1325.1575 - val_mse: 11225863.0000 - val_mae: 1325.1575\n",
      "Epoch 449/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 952.0144 - mse: 9124748.0000 - mae: 952.0145 - val_loss: 1353.8826 - val_mse: 11873567.0000 - val_mae: 1353.8826\n",
      "Epoch 450/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 955.0941 - mse: 9176283.0000 - mae: 955.0941 - val_loss: 1395.0479 - val_mse: 12626020.0000 - val_mae: 1395.0479\n",
      "Epoch 451/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 939.0446 - mse: 8979112.0000 - mae: 939.0446 - val_loss: 1334.6234 - val_mse: 11794202.0000 - val_mae: 1334.6234\n",
      "Epoch 452/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 942.9966 - mse: 9106074.0000 - mae: 942.9966 - val_loss: 1419.9567 - val_mse: 12935272.0000 - val_mae: 1419.9567\n",
      "Epoch 453/500\n",
      "4000/4000 [==============================] - 0s 50us/step - loss: 946.4706 - mse: 9133347.0000 - mae: 946.4705 - val_loss: 1346.5498 - val_mse: 11439043.0000 - val_mae: 1346.5498\n",
      "Epoch 454/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 957.5931 - mse: 9150712.0000 - mae: 957.5931 - val_loss: 1336.6598 - val_mse: 11673061.0000 - val_mae: 1336.6598\n",
      "Epoch 455/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 963.8128 - mse: 9400526.0000 - mae: 963.8128 - val_loss: 1367.4332 - val_mse: 12087870.0000 - val_mae: 1367.4332\n",
      "Epoch 456/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 947.6046 - mse: 9024757.0000 - mae: 947.6046 - val_loss: 1354.1343 - val_mse: 11413548.0000 - val_mae: 1354.1343\n",
      "Epoch 457/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 952.8658 - mse: 9236380.0000 - mae: 952.8657 - val_loss: 1355.5433 - val_mse: 11590646.0000 - val_mae: 1355.5433\n",
      "Epoch 458/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 949.3977 - mse: 9156077.0000 - mae: 949.3977 - val_loss: 1366.4384 - val_mse: 12083096.0000 - val_mae: 1366.4384\n",
      "Epoch 459/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 949.2880 - mse: 9202936.0000 - mae: 949.2880 - val_loss: 1305.0040 - val_mse: 11037920.0000 - val_mae: 1305.0040\n",
      "Epoch 460/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 954.4757 - mse: 9200274.0000 - mae: 954.4757 - val_loss: 1323.6564 - val_mse: 11538615.0000 - val_mae: 1323.6564\n",
      "Epoch 461/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 958.7874 - mse: 9362154.0000 - mae: 958.7875 - val_loss: 1352.9926 - val_mse: 11913608.0000 - val_mae: 1352.9926\n",
      "Epoch 462/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 944.9166 - mse: 9035221.0000 - mae: 944.9166 - val_loss: 1354.5371 - val_mse: 11701851.0000 - val_mae: 1354.5371\n",
      "Epoch 463/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 946.9797 - mse: 9062781.0000 - mae: 946.9797 - val_loss: 1383.2664 - val_mse: 12056741.0000 - val_mae: 1383.2664\n",
      "Epoch 464/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 948.0205 - mse: 9105291.0000 - mae: 948.0204 - val_loss: 1400.5437 - val_mse: 12647800.0000 - val_mae: 1400.5437\n",
      "Epoch 465/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 960.2388 - mse: 9230493.0000 - mae: 960.2388 - val_loss: 1345.3394 - val_mse: 11626424.0000 - val_mae: 1345.3394\n",
      "Epoch 466/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 944.5132 - mse: 9045105.0000 - mae: 944.5132 - val_loss: 1387.5708 - val_mse: 12735611.0000 - val_mae: 1387.5708\n",
      "Epoch 467/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 958.5899 - mse: 9199270.0000 - mae: 958.5899 - val_loss: 1403.4047 - val_mse: 12804973.0000 - val_mae: 1403.4047\n",
      "Epoch 468/500\n",
      "4000/4000 [==============================] - 0s 55us/step - loss: 948.2246 - mse: 9108422.0000 - mae: 948.2245 - val_loss: 1395.0579 - val_mse: 12992399.0000 - val_mae: 1395.0579\n",
      "Epoch 469/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 955.2846 - mse: 9160584.0000 - mae: 955.2845 - val_loss: 1377.3324 - val_mse: 11892043.0000 - val_mae: 1377.3324\n",
      "Epoch 470/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 936.4239 - mse: 9057154.0000 - mae: 936.4240 - val_loss: 1362.2258 - val_mse: 11309800.0000 - val_mae: 1362.2258\n",
      "Epoch 471/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 946.5841 - mse: 9141868.0000 - mae: 946.5841 - val_loss: 1329.2997 - val_mse: 11152546.0000 - val_mae: 1329.2997\n",
      "Epoch 472/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 951.4271 - mse: 9223088.0000 - mae: 951.4270 - val_loss: 1369.4415 - val_mse: 11675467.0000 - val_mae: 1369.4415\n",
      "Epoch 473/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 941.1490 - mse: 8986919.0000 - mae: 941.1490 - val_loss: 1343.6178 - val_mse: 11272929.0000 - val_mae: 1343.6178\n",
      "Epoch 474/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 940.2701 - mse: 9060193.0000 - mae: 940.2701 - val_loss: 1384.0300 - val_mse: 12089766.0000 - val_mae: 1384.0300\n",
      "Epoch 475/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 941.1492 - mse: 9007820.0000 - mae: 941.1492 - val_loss: 1374.2482 - val_mse: 11871091.0000 - val_mae: 1374.2482\n",
      "Epoch 476/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 948.6985 - mse: 9089576.0000 - mae: 948.6984 - val_loss: 1336.2676 - val_mse: 11714782.0000 - val_mae: 1336.2676\n",
      "Epoch 477/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 955.2620 - mse: 9295726.0000 - mae: 955.2620 - val_loss: 1393.6439 - val_mse: 12985569.0000 - val_mae: 1393.6439\n",
      "Epoch 478/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 950.1697 - mse: 9020580.0000 - mae: 950.1697 - val_loss: 1374.6976 - val_mse: 12419786.0000 - val_mae: 1374.6976\n",
      "Epoch 479/500\n",
      "4000/4000 [==============================] - 0s 52us/step - loss: 937.6981 - mse: 8852455.0000 - mae: 937.6981 - val_loss: 1323.0210 - val_mse: 11104268.0000 - val_mae: 1323.0210\n",
      "Epoch 480/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 934.6543 - mse: 8869200.0000 - mae: 934.6543 - val_loss: 1370.5565 - val_mse: 11684276.0000 - val_mae: 1370.5565\n",
      "Epoch 481/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 941.2295 - mse: 9010926.0000 - mae: 941.2295 - val_loss: 1344.0411 - val_mse: 11270344.0000 - val_mae: 1344.0411\n",
      "Epoch 482/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 945.5420 - mse: 9005596.0000 - mae: 945.5421 - val_loss: 1388.6978 - val_mse: 11483084.0000 - val_mae: 1388.6978\n",
      "Epoch 483/500\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 949.3914 - mse: 9074260.0000 - mae: 949.3914 - val_loss: 1354.0225 - val_mse: 11226090.0000 - val_mae: 1354.0225\n",
      "Epoch 484/500\n",
      "4000/4000 [==============================] - 0s 51us/step - loss: 945.0290 - mse: 9063011.0000 - mae: 945.0291 - val_loss: 1342.8192 - val_mse: 11165413.0000 - val_mae: 1342.8192\n",
      "Epoch 485/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 941.2305 - mse: 9116637.0000 - mae: 941.2303 - val_loss: 1326.7863 - val_mse: 10835204.0000 - val_mae: 1326.7863\n",
      "Epoch 486/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 946.3352 - mse: 9013880.0000 - mae: 946.3352 - val_loss: 1348.0688 - val_mse: 11935546.0000 - val_mae: 1348.0688\n",
      "Epoch 487/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 937.2431 - mse: 8903538.0000 - mae: 937.2431 - val_loss: 1335.2765 - val_mse: 11397118.0000 - val_mae: 1335.2765\n",
      "Epoch 488/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 961.8305 - mse: 9336025.0000 - mae: 961.8304 - val_loss: 1355.4595 - val_mse: 11634121.0000 - val_mae: 1355.4595\n",
      "Epoch 489/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 956.8463 - mse: 9204975.0000 - mae: 956.8463 - val_loss: 1339.1105 - val_mse: 11348111.0000 - val_mae: 1339.1105\n",
      "Epoch 490/500\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 943.0452 - mse: 9044913.0000 - mae: 943.0452 - val_loss: 1303.5300 - val_mse: 11516476.0000 - val_mae: 1303.5300\n",
      "Epoch 491/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 945.9244 - mse: 9119142.0000 - mae: 945.9244 - val_loss: 1346.6974 - val_mse: 11505172.0000 - val_mae: 1346.6974\n",
      "Epoch 492/500\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 945.2762 - mse: 9004226.0000 - mae: 945.2762 - val_loss: 1373.6372 - val_mse: 11795471.0000 - val_mae: 1373.6372\n",
      "Epoch 493/500\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 947.3762 - mse: 9161093.0000 - mae: 947.3761 - val_loss: 1351.1085 - val_mse: 11551170.0000 - val_mae: 1351.1085\n",
      "Epoch 494/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 941.2761 - mse: 8965917.0000 - mae: 941.2761 - val_loss: 1366.6703 - val_mse: 12178488.0000 - val_mae: 1366.6703\n",
      "Epoch 495/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 935.9359 - mse: 8882868.0000 - mae: 935.9359 - val_loss: 1368.4331 - val_mse: 12245139.0000 - val_mae: 1368.4331\n",
      "Epoch 496/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 940.8119 - mse: 9055484.0000 - mae: 940.8120 - val_loss: 1348.3903 - val_mse: 11210692.0000 - val_mae: 1348.3903\n",
      "Epoch 497/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 946.5898 - mse: 8944640.0000 - mae: 946.5898 - val_loss: 1330.2008 - val_mse: 10901308.0000 - val_mae: 1330.2008\n",
      "Epoch 498/500\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 947.9952 - mse: 9278693.0000 - mae: 947.9951 - val_loss: 1376.3564 - val_mse: 11280573.0000 - val_mae: 1376.3564\n",
      "Epoch 499/500\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 944.5846 - mse: 9030367.0000 - mae: 944.5845 - val_loss: 1327.9706 - val_mse: 11187291.0000 - val_mae: 1327.9706\n",
      "Epoch 500/500\n",
      "4000/4000 [==============================] - 0s 50us/step - loss: 940.2935 - mse: 8994084.0000 - mae: 940.2936 - val_loss: 1371.9126 - val_mse: 11499396.0000 - val_mae: 1371.9126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5500 samples, validate on 500 samples\n",
      "Epoch 1/500\n",
      "5500/5500 [==============================] - 2s 416us/step - loss: 1734.2897 - mse: 32439262.0000 - mae: 1734.2898 - val_loss: 3480.5786 - val_mse: 143621168.0000 - val_mae: 3480.5786\n",
      "Epoch 2/500\n",
      "5500/5500 [==============================] - 0s 34us/step - loss: 1731.1278 - mse: 32432248.0000 - mae: 1731.1278 - val_loss: 3472.4355 - val_mse: 143609456.0000 - val_mae: 3472.4355\n",
      "Epoch 3/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1722.1346 - mse: 32405640.0000 - mae: 1722.1348 - val_loss: 3463.6208 - val_mse: 143584976.0000 - val_mae: 3463.6208\n",
      "Epoch 4/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1714.7672 - mse: 32368298.0000 - mae: 1714.7672 - val_loss: 3455.8113 - val_mse: 143521152.0000 - val_mae: 3455.8113\n",
      "Epoch 5/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1709.4539 - mse: 32317690.0000 - mae: 1709.4539 - val_loss: 3448.4670 - val_mse: 143422416.0000 - val_mae: 3448.4670\n",
      "Epoch 6/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1704.1295 - mse: 32261090.0000 - mae: 1704.1296 - val_loss: 3441.7036 - val_mse: 143299376.0000 - val_mae: 3441.7036\n",
      "Epoch 7/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1700.1962 - mse: 32198102.0000 - mae: 1700.1964 - val_loss: 3436.2798 - val_mse: 143158240.0000 - val_mae: 3436.2798\n",
      "Epoch 8/500\n",
      "5500/5500 [==============================] - 0s 35us/step - loss: 1696.4701 - mse: 32137422.0000 - mae: 1696.4702 - val_loss: 3431.6365 - val_mse: 143002688.0000 - val_mae: 3431.6365\n",
      "Epoch 9/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1691.8793 - mse: 32080206.0000 - mae: 1691.8792 - val_loss: 3426.6230 - val_mse: 142819776.0000 - val_mae: 3426.6230\n",
      "Epoch 10/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1688.5511 - mse: 31998118.0000 - mae: 1688.5511 - val_loss: 3421.3545 - val_mse: 142619200.0000 - val_mae: 3421.3545\n",
      "Epoch 11/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1684.4552 - mse: 31943360.0000 - mae: 1684.4553 - val_loss: 3415.0779 - val_mse: 142388592.0000 - val_mae: 3415.0779\n",
      "Epoch 12/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 1678.4785 - mse: 31840610.0000 - mae: 1678.4788 - val_loss: 3408.0725 - val_mse: 142121888.0000 - val_mae: 3408.0725\n",
      "Epoch 13/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1674.5739 - mse: 31771626.0000 - mae: 1674.5739 - val_loss: 3401.3545 - val_mse: 141866224.0000 - val_mae: 3401.3545\n",
      "Epoch 14/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1669.8325 - mse: 31675518.0000 - mae: 1669.8325 - val_loss: 3393.8818 - val_mse: 141589552.0000 - val_mae: 3393.8818\n",
      "Epoch 15/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1664.9466 - mse: 31580890.0000 - mae: 1664.9465 - val_loss: 3386.2156 - val_mse: 141309344.0000 - val_mae: 3386.2156\n",
      "Epoch 16/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1661.4203 - mse: 31501624.0000 - mae: 1661.4204 - val_loss: 3378.8474 - val_mse: 141030032.0000 - val_mae: 3378.8474\n",
      "Epoch 17/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 1656.5076 - mse: 31395140.0000 - mae: 1656.5077 - val_loss: 3371.3164 - val_mse: 140737808.0000 - val_mae: 3371.3164\n",
      "Epoch 18/500\n",
      "5500/5500 [==============================] - 0s 35us/step - loss: 1654.0396 - mse: 31322812.0000 - mae: 1654.0397 - val_loss: 3364.3259 - val_mse: 140464048.0000 - val_mae: 3364.3259\n",
      "Epoch 19/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1648.5587 - mse: 31222586.0000 - mae: 1648.5586 - val_loss: 3357.4414 - val_mse: 140195104.0000 - val_mae: 3357.4414\n",
      "Epoch 20/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 1646.8919 - mse: 31190376.0000 - mae: 1646.8918 - val_loss: 3351.3015 - val_mse: 139953744.0000 - val_mae: 3351.3015\n",
      "Epoch 21/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1645.9335 - mse: 31100252.0000 - mae: 1645.9335 - val_loss: 3345.8357 - val_mse: 139731984.0000 - val_mae: 3345.8357\n",
      "Epoch 22/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1641.6470 - mse: 30994004.0000 - mae: 1641.6469 - val_loss: 3340.3806 - val_mse: 139503440.0000 - val_mae: 3340.3806\n",
      "Epoch 23/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1640.1686 - mse: 30947350.0000 - mae: 1640.1687 - val_loss: 3335.0132 - val_mse: 139270464.0000 - val_mae: 3335.0132\n",
      "Epoch 24/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1634.0556 - mse: 30832054.0000 - mae: 1634.0554 - val_loss: 3330.5225 - val_mse: 139063600.0000 - val_mae: 3330.5225\n",
      "Epoch 25/500\n",
      "5500/5500 [==============================] - 0s 35us/step - loss: 1634.4787 - mse: 30763572.0000 - mae: 1634.4785 - val_loss: 3326.2942 - val_mse: 138860464.0000 - val_mae: 3326.2942\n",
      "Epoch 26/500\n",
      "5500/5500 [==============================] - 0s 35us/step - loss: 1630.7545 - mse: 30722934.0000 - mae: 1630.7545 - val_loss: 3321.1477 - val_mse: 138614400.0000 - val_mae: 3321.1477\n",
      "Epoch 27/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1625.3880 - mse: 30609554.0000 - mae: 1625.3879 - val_loss: 3315.7283 - val_mse: 138364720.0000 - val_mae: 3315.7283\n",
      "Epoch 28/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 1624.6289 - mse: 30556046.0000 - mae: 1624.6288 - val_loss: 3310.0447 - val_mse: 138072608.0000 - val_mae: 3310.0447\n",
      "Epoch 29/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1621.4467 - mse: 30504356.0000 - mae: 1621.4468 - val_loss: 3303.9988 - val_mse: 137732288.0000 - val_mae: 3303.9988\n",
      "Epoch 30/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1614.2339 - mse: 30333876.0000 - mae: 1614.2338 - val_loss: 3297.2407 - val_mse: 137376064.0000 - val_mae: 3297.2407\n",
      "Epoch 31/500\n",
      "5500/5500 [==============================] - 0s 34us/step - loss: 1610.4515 - mse: 30206484.0000 - mae: 1610.4517 - val_loss: 3289.4004 - val_mse: 137011184.0000 - val_mae: 3289.4004\n",
      "Epoch 32/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1608.3478 - mse: 30169802.0000 - mae: 1608.3478 - val_loss: 3283.9529 - val_mse: 136723168.0000 - val_mae: 3283.9529\n",
      "Epoch 33/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1601.3308 - mse: 29942544.0000 - mae: 1601.3309 - val_loss: 3276.8496 - val_mse: 136362640.0000 - val_mae: 3276.8496\n",
      "Epoch 34/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1597.4208 - mse: 29860216.0000 - mae: 1597.4209 - val_loss: 3269.2532 - val_mse: 135995792.0000 - val_mae: 3269.2532\n",
      "Epoch 35/500\n",
      "5500/5500 [==============================] - 0s 36us/step - loss: 1595.2094 - mse: 29706858.0000 - mae: 1595.2092 - val_loss: 3262.3669 - val_mse: 135645584.0000 - val_mae: 3262.3669\n",
      "Epoch 36/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1588.8943 - mse: 29613306.0000 - mae: 1588.8944 - val_loss: 3255.1528 - val_mse: 135266176.0000 - val_mae: 3255.1528\n",
      "Epoch 37/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1587.2606 - mse: 29530030.0000 - mae: 1587.2605 - val_loss: 3248.4229 - val_mse: 134906208.0000 - val_mae: 3248.4229\n",
      "Epoch 38/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1585.7398 - mse: 29368922.0000 - mae: 1585.7396 - val_loss: 3243.1943 - val_mse: 134588512.0000 - val_mae: 3243.1943\n",
      "Epoch 39/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1580.8734 - mse: 29213578.0000 - mae: 1580.8733 - val_loss: 3237.3254 - val_mse: 134235136.0000 - val_mae: 3237.3254\n",
      "Epoch 40/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 1579.1914 - mse: 29142540.0000 - mae: 1579.1914 - val_loss: 3232.5820 - val_mse: 133915200.0000 - val_mae: 3232.5820\n",
      "Epoch 41/500\n",
      "5500/5500 [==============================] - 0s 36us/step - loss: 1575.1647 - mse: 29064280.0000 - mae: 1575.1646 - val_loss: 3227.8926 - val_mse: 133566240.0000 - val_mae: 3227.8926\n",
      "Epoch 42/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1572.5103 - mse: 29012140.0000 - mae: 1572.5104 - val_loss: 3224.3296 - val_mse: 133262648.0000 - val_mae: 3224.3296\n",
      "Epoch 43/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 1567.7176 - mse: 28866218.0000 - mae: 1567.7174 - val_loss: 3220.1382 - val_mse: 132929448.0000 - val_mae: 3220.1382\n",
      "Epoch 44/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1565.2394 - mse: 28680436.0000 - mae: 1565.2395 - val_loss: 3218.6948 - val_mse: 132660832.0000 - val_mae: 3218.6948\n",
      "Epoch 45/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1559.9406 - mse: 28593110.0000 - mae: 1559.9407 - val_loss: 3213.0894 - val_mse: 132252216.0000 - val_mae: 3213.0894\n",
      "Epoch 46/500\n",
      "5500/5500 [==============================] - 0s 36us/step - loss: 1559.7959 - mse: 28541342.0000 - mae: 1559.7958 - val_loss: 3212.0059 - val_mse: 131983488.0000 - val_mae: 3212.0059\n",
      "Epoch 47/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1554.4420 - mse: 28372704.0000 - mae: 1554.4420 - val_loss: 3206.0596 - val_mse: 131532168.0000 - val_mae: 3206.0596\n",
      "Epoch 48/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1553.1594 - mse: 28277480.0000 - mae: 1553.1594 - val_loss: 3204.8123 - val_mse: 131271624.0000 - val_mae: 3204.8123\n",
      "Epoch 49/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1550.4169 - mse: 28218866.0000 - mae: 1550.4169 - val_loss: 3203.8286 - val_mse: 130963616.0000 - val_mae: 3203.8286\n",
      "Epoch 50/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1546.5377 - mse: 27952758.0000 - mae: 1546.5378 - val_loss: 3202.9177 - val_mse: 130681048.0000 - val_mae: 3202.9177\n",
      "Epoch 51/500\n",
      "5500/5500 [==============================] - 0s 36us/step - loss: 1544.2955 - mse: 27970690.0000 - mae: 1544.2957 - val_loss: 3199.5471 - val_mse: 130305072.0000 - val_mae: 3199.5471\n",
      "Epoch 52/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1544.2146 - mse: 27885964.0000 - mae: 1544.2147 - val_loss: 3199.9941 - val_mse: 130027120.0000 - val_mae: 3199.9941\n",
      "Epoch 53/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1539.7921 - mse: 27756584.0000 - mae: 1539.7920 - val_loss: 3195.2981 - val_mse: 129556096.0000 - val_mae: 3195.2981\n",
      "Epoch 54/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1536.6301 - mse: 27655844.0000 - mae: 1536.6300 - val_loss: 3191.8208 - val_mse: 129081368.0000 - val_mae: 3191.8208\n",
      "Epoch 55/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1533.4306 - mse: 27522290.0000 - mae: 1533.4305 - val_loss: 3185.6089 - val_mse: 128618144.0000 - val_mae: 3185.6089\n",
      "Epoch 56/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1523.1175 - mse: 27297802.0000 - mae: 1523.1174 - val_loss: 3184.0129 - val_mse: 128288664.0000 - val_mae: 3184.0129\n",
      "Epoch 57/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1525.9413 - mse: 27233258.0000 - mae: 1525.9413 - val_loss: 3182.2869 - val_mse: 127924784.0000 - val_mae: 3182.2869\n",
      "Epoch 58/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1522.4072 - mse: 27157034.0000 - mae: 1522.4072 - val_loss: 3169.8904 - val_mse: 127384720.0000 - val_mae: 3169.8904\n",
      "Epoch 59/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1519.6203 - mse: 27110808.0000 - mae: 1519.6202 - val_loss: 3171.1123 - val_mse: 127101968.0000 - val_mae: 3171.1123\n",
      "Epoch 60/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1513.7425 - mse: 26757098.0000 - mae: 1513.7424 - val_loss: 3168.8379 - val_mse: 126734976.0000 - val_mae: 3168.8379\n",
      "Epoch 61/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1512.1860 - mse: 26701728.0000 - mae: 1512.1860 - val_loss: 3163.6790 - val_mse: 126305056.0000 - val_mae: 3163.6790\n",
      "Epoch 62/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1505.5582 - mse: 26602320.0000 - mae: 1505.5582 - val_loss: 3150.4617 - val_mse: 125750016.0000 - val_mae: 3150.4617\n",
      "Epoch 63/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1503.9626 - mse: 26589806.0000 - mae: 1503.9625 - val_loss: 3148.2439 - val_mse: 125258360.0000 - val_mae: 3148.2439\n",
      "Epoch 64/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1504.0192 - mse: 26386858.0000 - mae: 1504.0192 - val_loss: 3150.8398 - val_mse: 124940488.0000 - val_mae: 3150.8398\n",
      "Epoch 65/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1497.6651 - mse: 26264582.0000 - mae: 1497.6650 - val_loss: 3139.8718 - val_mse: 124346088.0000 - val_mae: 3139.8718\n",
      "Epoch 66/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1496.5542 - mse: 26070622.0000 - mae: 1496.5542 - val_loss: 3133.4045 - val_mse: 123955112.0000 - val_mae: 3133.4045\n",
      "Epoch 67/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1490.3863 - mse: 25981244.0000 - mae: 1490.3864 - val_loss: 3114.8967 - val_mse: 123434504.0000 - val_mae: 3114.8967\n",
      "Epoch 68/500\n",
      "5500/5500 [==============================] - 0s 36us/step - loss: 1474.8891 - mse: 25558456.0000 - mae: 1474.8890 - val_loss: 3099.8308 - val_mse: 122862504.0000 - val_mae: 3099.8308\n",
      "Epoch 69/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1479.9409 - mse: 25679822.0000 - mae: 1479.9408 - val_loss: 3089.2817 - val_mse: 122349352.0000 - val_mae: 3089.2817\n",
      "Epoch 70/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1472.4415 - mse: 25444360.0000 - mae: 1472.4415 - val_loss: 3153.1562 - val_mse: 121936408.0000 - val_mae: 3153.1562\n",
      "Epoch 71/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1462.9887 - mse: 25184860.0000 - mae: 1462.9888 - val_loss: 3142.7102 - val_mse: 121498784.0000 - val_mae: 3142.7102\n",
      "Epoch 72/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1470.6942 - mse: 25313972.0000 - mae: 1470.6942 - val_loss: 3159.5974 - val_mse: 121158136.0000 - val_mae: 3159.5974\n",
      "Epoch 73/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1449.5123 - mse: 24836056.0000 - mae: 1449.5123 - val_loss: 3231.1663 - val_mse: 120616992.0000 - val_mae: 3231.1663\n",
      "Epoch 74/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1459.6608 - mse: 24988194.0000 - mae: 1459.6608 - val_loss: 3095.2532 - val_mse: 120186192.0000 - val_mae: 3095.2532\n",
      "Epoch 75/500\n",
      "5500/5500 [==============================] - 0s 35us/step - loss: 1444.9330 - mse: 24718286.0000 - mae: 1444.9329 - val_loss: 3106.8982 - val_mse: 119623056.0000 - val_mae: 3106.8982\n",
      "Epoch 76/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1458.0625 - mse: 25010126.0000 - mae: 1458.0625 - val_loss: 3168.9041 - val_mse: 119304152.0000 - val_mae: 3168.9041\n",
      "Epoch 77/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1451.8982 - mse: 24812082.0000 - mae: 1451.8981 - val_loss: 3187.3105 - val_mse: 118921840.0000 - val_mae: 3187.3105\n",
      "Epoch 78/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1436.9883 - mse: 24363796.0000 - mae: 1436.9883 - val_loss: 3142.2388 - val_mse: 118308528.0000 - val_mae: 3142.2388\n",
      "Epoch 79/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1434.4390 - mse: 24396406.0000 - mae: 1434.4390 - val_loss: 3247.2466 - val_mse: 117952648.0000 - val_mae: 3247.2466\n",
      "Epoch 80/500\n",
      "5500/5500 [==============================] - 0s 36us/step - loss: 1440.0329 - mse: 24131816.0000 - mae: 1440.0330 - val_loss: 3081.2874 - val_mse: 117287792.0000 - val_mae: 3081.2874\n",
      "Epoch 81/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1422.4160 - mse: 23961004.0000 - mae: 1422.4160 - val_loss: 3071.5037 - val_mse: 116690240.0000 - val_mae: 3071.5037\n",
      "Epoch 82/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1425.4495 - mse: 23907634.0000 - mae: 1425.4496 - val_loss: 3161.2268 - val_mse: 116234904.0000 - val_mae: 3161.2268\n",
      "Epoch 83/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1415.4378 - mse: 23613322.0000 - mae: 1415.4379 - val_loss: 3082.4973 - val_mse: 115601800.0000 - val_mae: 3082.4973\n",
      "Epoch 84/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 1417.1287 - mse: 23680054.0000 - mae: 1417.1287 - val_loss: 3126.1631 - val_mse: 115141640.0000 - val_mae: 3126.1631\n",
      "Epoch 85/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1412.2747 - mse: 23734142.0000 - mae: 1412.2747 - val_loss: 3106.7222 - val_mse: 114580296.0000 - val_mae: 3106.7222\n",
      "Epoch 86/500\n",
      "5500/5500 [==============================] - 0s 35us/step - loss: 1406.4940 - mse: 23217236.0000 - mae: 1406.4940 - val_loss: 3114.0068 - val_mse: 113958272.0000 - val_mae: 3114.0068\n",
      "Epoch 87/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1406.4954 - mse: 23278192.0000 - mae: 1406.4955 - val_loss: 3134.3110 - val_mse: 113556216.0000 - val_mae: 3134.3110\n",
      "Epoch 88/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1406.0085 - mse: 23269322.0000 - mae: 1406.0084 - val_loss: 3152.4419 - val_mse: 113060544.0000 - val_mae: 3152.4419\n",
      "Epoch 89/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1398.4807 - mse: 23054504.0000 - mae: 1398.4807 - val_loss: 3046.1934 - val_mse: 112524312.0000 - val_mae: 3046.1934\n",
      "Epoch 90/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1394.2835 - mse: 23093444.0000 - mae: 1394.2837 - val_loss: 3069.2554 - val_mse: 111955928.0000 - val_mae: 3069.2554\n",
      "Epoch 91/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1389.8586 - mse: 22670942.0000 - mae: 1389.8585 - val_loss: 3035.7954 - val_mse: 111412976.0000 - val_mae: 3035.7954\n",
      "Epoch 92/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1385.5030 - mse: 22633884.0000 - mae: 1385.5031 - val_loss: 3030.8633 - val_mse: 110838776.0000 - val_mae: 3030.8633\n",
      "Epoch 93/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1381.0816 - mse: 22551534.0000 - mae: 1381.0815 - val_loss: 2995.5735 - val_mse: 110199928.0000 - val_mae: 2995.5735\n",
      "Epoch 94/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 1372.1510 - mse: 22167048.0000 - mae: 1372.1510 - val_loss: 2976.5303 - val_mse: 109674896.0000 - val_mae: 2976.5303\n",
      "Epoch 95/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1360.2787 - mse: 22096680.0000 - mae: 1360.2786 - val_loss: 3022.8545 - val_mse: 108976000.0000 - val_mae: 3022.8545\n",
      "Epoch 96/500\n",
      "5500/5500 [==============================] - 0s 36us/step - loss: 1352.7661 - mse: 21782496.0000 - mae: 1352.7662 - val_loss: 3033.3545 - val_mse: 108310920.0000 - val_mae: 3033.3545\n",
      "Epoch 97/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1357.9445 - mse: 21941982.0000 - mae: 1357.9446 - val_loss: 3017.4470 - val_mse: 107588160.0000 - val_mae: 3017.4470\n",
      "Epoch 98/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1359.3206 - mse: 21882600.0000 - mae: 1359.3206 - val_loss: 2968.6099 - val_mse: 107185304.0000 - val_mae: 2968.6099\n",
      "Epoch 99/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1346.9301 - mse: 21625908.0000 - mae: 1346.9303 - val_loss: 3008.7563 - val_mse: 106675376.0000 - val_mae: 3008.7563\n",
      "Epoch 100/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1334.7068 - mse: 21165236.0000 - mae: 1334.7068 - val_loss: 3021.5466 - val_mse: 105808888.0000 - val_mae: 3021.5466\n",
      "Epoch 101/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1332.6982 - mse: 20874298.0000 - mae: 1332.6982 - val_loss: 2952.7170 - val_mse: 105298752.0000 - val_mae: 2952.7170\n",
      "Epoch 102/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1335.6710 - mse: 20922050.0000 - mae: 1335.6709 - val_loss: 3041.6968 - val_mse: 104637536.0000 - val_mae: 3041.6968\n",
      "Epoch 103/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1328.2345 - mse: 20964014.0000 - mae: 1328.2345 - val_loss: 3038.9124 - val_mse: 104093824.0000 - val_mae: 3038.9124\n",
      "Epoch 104/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1328.2071 - mse: 21027824.0000 - mae: 1328.2070 - val_loss: 3008.4551 - val_mse: 103502736.0000 - val_mae: 3008.4551\n",
      "Epoch 105/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1329.4279 - mse: 20901708.0000 - mae: 1329.4279 - val_loss: 2952.2898 - val_mse: 103025728.0000 - val_mae: 2952.2898\n",
      "Epoch 106/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1313.9018 - mse: 20282376.0000 - mae: 1313.9017 - val_loss: 3023.3909 - val_mse: 102225568.0000 - val_mae: 3023.3909\n",
      "Epoch 107/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1303.4236 - mse: 19972266.0000 - mae: 1303.4236 - val_loss: 2967.5144 - val_mse: 101279960.0000 - val_mae: 2967.5144\n",
      "Epoch 108/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1318.6306 - mse: 20573542.0000 - mae: 1318.6305 - val_loss: 2940.1079 - val_mse: 100733992.0000 - val_mae: 2940.1079\n",
      "Epoch 109/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1299.8926 - mse: 19947770.0000 - mae: 1299.8926 - val_loss: 2918.7808 - val_mse: 100130568.0000 - val_mae: 2918.7808\n",
      "Epoch 110/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1302.6740 - mse: 19833404.0000 - mae: 1302.6740 - val_loss: 2956.9983 - val_mse: 99579216.0000 - val_mae: 2956.9983\n",
      "Epoch 111/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1289.2395 - mse: 19266446.0000 - mae: 1289.2395 - val_loss: 2992.6443 - val_mse: 98880712.0000 - val_mae: 2992.6443\n",
      "Epoch 112/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1278.6090 - mse: 19201334.0000 - mae: 1278.6091 - val_loss: 2940.6533 - val_mse: 97985576.0000 - val_mae: 2940.6533\n",
      "Epoch 113/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1275.9232 - mse: 19068056.0000 - mae: 1275.9232 - val_loss: 2907.6863 - val_mse: 97685416.0000 - val_mae: 2907.6863\n",
      "Epoch 114/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1285.5870 - mse: 19353168.0000 - mae: 1285.5870 - val_loss: 2921.5339 - val_mse: 96996736.0000 - val_mae: 2921.5339\n",
      "Epoch 115/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1275.4493 - mse: 18736084.0000 - mae: 1275.4493 - val_loss: 2970.6538 - val_mse: 96393504.0000 - val_mae: 2970.6538\n",
      "Epoch 116/500\n",
      "5500/5500 [==============================] - 0s 36us/step - loss: 1268.2668 - mse: 18443588.0000 - mae: 1268.2667 - val_loss: 2894.6379 - val_mse: 95556048.0000 - val_mae: 2894.6379\n",
      "Epoch 117/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1267.5376 - mse: 18675268.0000 - mae: 1267.5377 - val_loss: 2908.6812 - val_mse: 94777472.0000 - val_mae: 2908.6812\n",
      "Epoch 118/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1253.4951 - mse: 18147872.0000 - mae: 1253.4951 - val_loss: 2867.3118 - val_mse: 94241120.0000 - val_mae: 2867.3118\n",
      "Epoch 119/500\n",
      "5500/5500 [==============================] - 0s 36us/step - loss: 1259.1273 - mse: 18284656.0000 - mae: 1259.1273 - val_loss: 2893.5273 - val_mse: 93679200.0000 - val_mae: 2893.5273\n",
      "Epoch 120/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1265.7364 - mse: 18434220.0000 - mae: 1265.7365 - val_loss: 2891.7852 - val_mse: 92978248.0000 - val_mae: 2891.7852\n",
      "Epoch 121/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1255.9604 - mse: 18018150.0000 - mae: 1255.9604 - val_loss: 2845.5054 - val_mse: 92040632.0000 - val_mae: 2845.5054\n",
      "Epoch 122/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 1245.7986 - mse: 17903182.0000 - mae: 1245.7987 - val_loss: 2822.1313 - val_mse: 91430840.0000 - val_mae: 2822.1313\n",
      "Epoch 123/500\n",
      "5500/5500 [==============================] - 0s 36us/step - loss: 1237.7618 - mse: 17598382.0000 - mae: 1237.7618 - val_loss: 2838.5327 - val_mse: 90471496.0000 - val_mae: 2838.5327\n",
      "Epoch 124/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1230.0829 - mse: 17267070.0000 - mae: 1230.0830 - val_loss: 2819.6226 - val_mse: 89938984.0000 - val_mae: 2819.6226\n",
      "Epoch 125/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1244.6853 - mse: 17515906.0000 - mae: 1244.6853 - val_loss: 2816.5696 - val_mse: 89152464.0000 - val_mae: 2816.5696\n",
      "Epoch 126/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1227.3130 - mse: 17316502.0000 - mae: 1227.3130 - val_loss: 2862.5022 - val_mse: 88678744.0000 - val_mae: 2862.5022\n",
      "Epoch 127/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1226.9066 - mse: 17232406.0000 - mae: 1226.9066 - val_loss: 2814.0281 - val_mse: 87741912.0000 - val_mae: 2814.0281\n",
      "Epoch 128/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1218.1435 - mse: 17006090.0000 - mae: 1218.1436 - val_loss: 2816.4590 - val_mse: 87128880.0000 - val_mae: 2816.4590\n",
      "Epoch 129/500\n",
      "5500/5500 [==============================] - 0s 36us/step - loss: 1207.1291 - mse: 16606064.0000 - mae: 1207.1292 - val_loss: 2773.9695 - val_mse: 86122208.0000 - val_mae: 2773.9695\n",
      "Epoch 130/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1211.0491 - mse: 16541115.0000 - mae: 1211.0492 - val_loss: 2772.4338 - val_mse: 85523472.0000 - val_mae: 2772.4338\n",
      "Epoch 131/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1220.5396 - mse: 17064774.0000 - mae: 1220.5397 - val_loss: 2810.2522 - val_mse: 85408944.0000 - val_mae: 2810.2522\n",
      "Epoch 132/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 1195.0941 - mse: 15627881.0000 - mae: 1195.0942 - val_loss: 2784.0144 - val_mse: 84067392.0000 - val_mae: 2784.0144\n",
      "Epoch 133/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 1184.0587 - mse: 15847914.0000 - mae: 1184.0587 - val_loss: 2764.0830 - val_mse: 83429576.0000 - val_mae: 2764.0830\n",
      "Epoch 134/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1188.0593 - mse: 15844268.0000 - mae: 1188.0593 - val_loss: 2768.1780 - val_mse: 83166176.0000 - val_mae: 2768.1780\n",
      "Epoch 135/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1192.5983 - mse: 15884020.0000 - mae: 1192.5983 - val_loss: 2782.9944 - val_mse: 82471800.0000 - val_mae: 2782.9944\n",
      "Epoch 136/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1191.8616 - mse: 15661250.0000 - mae: 1191.8616 - val_loss: 2747.0728 - val_mse: 81557408.0000 - val_mae: 2747.0728\n",
      "Epoch 137/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1186.8692 - mse: 15838312.0000 - mae: 1186.8691 - val_loss: 2727.4436 - val_mse: 80710304.0000 - val_mae: 2727.4436\n",
      "Epoch 138/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1195.2114 - mse: 15968818.0000 - mae: 1195.2113 - val_loss: 2714.2148 - val_mse: 79811976.0000 - val_mae: 2714.2148\n",
      "Epoch 139/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1161.0522 - mse: 15418841.0000 - mae: 1161.0522 - val_loss: 2699.3962 - val_mse: 79264944.0000 - val_mae: 2699.3962\n",
      "Epoch 140/500\n",
      "5500/5500 [==============================] - 0s 36us/step - loss: 1155.8290 - mse: 15057520.0000 - mae: 1155.8290 - val_loss: 2706.6497 - val_mse: 78306712.0000 - val_mae: 2706.6497\n",
      "Epoch 141/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1167.3335 - mse: 15070390.0000 - mae: 1167.3336 - val_loss: 2677.5767 - val_mse: 77676008.0000 - val_mae: 2677.5767\n",
      "Epoch 142/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1138.5605 - mse: 14270387.0000 - mae: 1138.5605 - val_loss: 2710.1260 - val_mse: 76894504.0000 - val_mae: 2710.1260\n",
      "Epoch 143/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1156.3571 - mse: 15127046.0000 - mae: 1156.3571 - val_loss: 2684.0222 - val_mse: 76194280.0000 - val_mae: 2684.0222\n",
      "Epoch 144/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1160.3730 - mse: 14937586.0000 - mae: 1160.3729 - val_loss: 2693.3271 - val_mse: 76243152.0000 - val_mae: 2693.3271\n",
      "Epoch 145/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1163.9414 - mse: 15028223.0000 - mae: 1163.9414 - val_loss: 2662.2141 - val_mse: 75512160.0000 - val_mae: 2662.2141\n",
      "Epoch 146/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1148.2007 - mse: 14371534.0000 - mae: 1148.2007 - val_loss: 2664.9021 - val_mse: 74687624.0000 - val_mae: 2664.9021\n",
      "Epoch 147/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1136.9764 - mse: 14450350.0000 - mae: 1136.9763 - val_loss: 2663.4475 - val_mse: 73776256.0000 - val_mae: 2663.4475\n",
      "Epoch 148/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1141.3824 - mse: 14114041.0000 - mae: 1141.3824 - val_loss: 2662.2065 - val_mse: 73914528.0000 - val_mae: 2662.2065\n",
      "Epoch 149/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1128.3122 - mse: 13713931.0000 - mae: 1128.3121 - val_loss: 2637.4534 - val_mse: 72748048.0000 - val_mae: 2637.4534\n",
      "Epoch 150/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1136.7015 - mse: 13971468.0000 - mae: 1136.7015 - val_loss: 2608.3308 - val_mse: 71960208.0000 - val_mae: 2608.3308\n",
      "Epoch 151/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1118.2849 - mse: 13763214.0000 - mae: 1118.2848 - val_loss: 2633.6506 - val_mse: 71502760.0000 - val_mae: 2633.6506\n",
      "Epoch 152/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1115.2101 - mse: 13568305.0000 - mae: 1115.2102 - val_loss: 2609.1404 - val_mse: 71631528.0000 - val_mae: 2609.1404\n",
      "Epoch 153/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1114.7281 - mse: 13512208.0000 - mae: 1114.7281 - val_loss: 2616.4707 - val_mse: 70832160.0000 - val_mae: 2616.4707\n",
      "Epoch 154/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1101.7773 - mse: 13324979.0000 - mae: 1101.7772 - val_loss: 2596.9639 - val_mse: 69778728.0000 - val_mae: 2596.9639\n",
      "Epoch 155/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1108.6757 - mse: 13376946.0000 - mae: 1108.6758 - val_loss: 2577.0957 - val_mse: 68889792.0000 - val_mae: 2577.0957\n",
      "Epoch 156/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1120.8331 - mse: 13471570.0000 - mae: 1120.8330 - val_loss: 2568.9890 - val_mse: 68917720.0000 - val_mae: 2568.9890\n",
      "Epoch 157/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1104.7702 - mse: 13074426.0000 - mae: 1104.7701 - val_loss: 2571.4338 - val_mse: 68164960.0000 - val_mae: 2571.4338\n",
      "Epoch 158/500\n",
      "5500/5500 [==============================] - 0s 36us/step - loss: 1098.5460 - mse: 13069265.0000 - mae: 1098.5459 - val_loss: 2581.5012 - val_mse: 67514944.0000 - val_mae: 2581.5012\n",
      "Epoch 159/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 1104.5161 - mse: 13570251.0000 - mae: 1104.5161 - val_loss: 2556.0964 - val_mse: 66856436.0000 - val_mae: 2556.0964\n",
      "Epoch 160/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1084.5955 - mse: 12436435.0000 - mae: 1084.5956 - val_loss: 2573.0681 - val_mse: 66845620.0000 - val_mae: 2573.0681\n",
      "Epoch 161/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1089.5736 - mse: 13150956.0000 - mae: 1089.5735 - val_loss: 2551.8982 - val_mse: 66396888.0000 - val_mae: 2551.8982\n",
      "Epoch 162/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1075.7617 - mse: 12480114.0000 - mae: 1075.7617 - val_loss: 2556.5981 - val_mse: 65755844.0000 - val_mae: 2556.5981\n",
      "Epoch 163/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1070.5708 - mse: 12625171.0000 - mae: 1070.5708 - val_loss: 2535.4124 - val_mse: 65512556.0000 - val_mae: 2535.4124\n",
      "Epoch 164/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1070.7645 - mse: 12367704.0000 - mae: 1070.7646 - val_loss: 2554.5029 - val_mse: 66966208.0000 - val_mae: 2554.5029\n",
      "Epoch 165/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1062.9680 - mse: 12170193.0000 - mae: 1062.9681 - val_loss: 2595.4148 - val_mse: 66644480.0000 - val_mae: 2595.4148\n",
      "Epoch 166/500\n",
      "5500/5500 [==============================] - 0s 37us/step - loss: 1066.5495 - mse: 12224652.0000 - mae: 1066.5496 - val_loss: 2565.4329 - val_mse: 65426728.0000 - val_mae: 2565.4329\n",
      "Epoch 167/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1085.9043 - mse: 12514460.0000 - mae: 1085.9043 - val_loss: 2552.1367 - val_mse: 64654680.0000 - val_mae: 2552.1367\n",
      "Epoch 168/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1071.3272 - mse: 12578618.0000 - mae: 1071.3271 - val_loss: 2517.5381 - val_mse: 63223720.0000 - val_mae: 2517.5381\n",
      "Epoch 169/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1052.1873 - mse: 11978152.0000 - mae: 1052.1873 - val_loss: 2545.0964 - val_mse: 65241384.0000 - val_mae: 2545.0964\n",
      "Epoch 170/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1065.6980 - mse: 12000709.0000 - mae: 1065.6980 - val_loss: 2509.2480 - val_mse: 62622608.0000 - val_mae: 2509.2480\n",
      "Epoch 171/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1057.1031 - mse: 11752148.0000 - mae: 1057.1031 - val_loss: 2504.3425 - val_mse: 62873500.0000 - val_mae: 2504.3425\n",
      "Epoch 172/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1064.5852 - mse: 12248180.0000 - mae: 1064.5851 - val_loss: 2519.6033 - val_mse: 63055332.0000 - val_mae: 2519.6033\n",
      "Epoch 173/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1034.4189 - mse: 11442505.0000 - mae: 1034.4188 - val_loss: 2486.7317 - val_mse: 61761484.0000 - val_mae: 2486.7317\n",
      "Epoch 174/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1059.6601 - mse: 11991368.0000 - mae: 1059.6600 - val_loss: 2512.9495 - val_mse: 62145436.0000 - val_mae: 2512.9495\n",
      "Epoch 175/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1049.9285 - mse: 11849414.0000 - mae: 1049.9283 - val_loss: 2506.4924 - val_mse: 61162244.0000 - val_mae: 2506.4924\n",
      "Epoch 176/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1051.2716 - mse: 11673627.0000 - mae: 1051.2716 - val_loss: 2484.5752 - val_mse: 61303908.0000 - val_mae: 2484.5752\n",
      "Epoch 177/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 1033.1939 - mse: 11512492.0000 - mae: 1033.1940 - val_loss: 2509.7578 - val_mse: 61360732.0000 - val_mae: 2509.7578\n",
      "Epoch 178/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1047.1921 - mse: 12181503.0000 - mae: 1047.1921 - val_loss: 2490.2346 - val_mse: 61061392.0000 - val_mae: 2490.2346\n",
      "Epoch 179/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1020.6729 - mse: 11385172.0000 - mae: 1020.6729 - val_loss: 2549.9971 - val_mse: 62958436.0000 - val_mae: 2549.9971\n",
      "Epoch 180/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1038.5182 - mse: 11238789.0000 - mae: 1038.5182 - val_loss: 2518.9504 - val_mse: 62189344.0000 - val_mae: 2518.9504\n",
      "Epoch 181/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1039.2003 - mse: 11925570.0000 - mae: 1039.2003 - val_loss: 2487.3821 - val_mse: 61422732.0000 - val_mae: 2487.3821\n",
      "Epoch 182/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 1039.4259 - mse: 11814718.0000 - mae: 1039.4260 - val_loss: 2565.4417 - val_mse: 63668712.0000 - val_mae: 2565.4417\n",
      "Epoch 183/500\n",
      "5500/5500 [==============================] - 0s 36us/step - loss: 1038.7875 - mse: 11360438.0000 - mae: 1038.7876 - val_loss: 2557.7083 - val_mse: 63091568.0000 - val_mae: 2557.7083\n",
      "Epoch 184/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1049.2549 - mse: 11931609.0000 - mae: 1049.2549 - val_loss: 2535.9087 - val_mse: 62965880.0000 - val_mae: 2535.9087\n",
      "Epoch 185/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1019.2952 - mse: 10951719.0000 - mae: 1019.2952 - val_loss: 2546.7676 - val_mse: 63848896.0000 - val_mae: 2546.7676\n",
      "Epoch 186/500\n",
      "5500/5500 [==============================] - 0s 36us/step - loss: 1025.3848 - mse: 10754902.0000 - mae: 1025.3849 - val_loss: 2560.6509 - val_mse: 63702748.0000 - val_mae: 2560.6509\n",
      "Epoch 187/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1038.3384 - mse: 11254645.0000 - mae: 1038.3384 - val_loss: 2582.8369 - val_mse: 64949612.0000 - val_mae: 2582.8369\n",
      "Epoch 188/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1019.9063 - mse: 11183550.0000 - mae: 1019.9064 - val_loss: 2569.9673 - val_mse: 63494096.0000 - val_mae: 2569.9673\n",
      "Epoch 189/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1021.5119 - mse: 11147586.0000 - mae: 1021.5119 - val_loss: 2594.8250 - val_mse: 63724152.0000 - val_mae: 2594.8250\n",
      "Epoch 190/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 1017.7926 - mse: 10748827.0000 - mae: 1017.7925 - val_loss: 2551.4868 - val_mse: 62445184.0000 - val_mae: 2551.4868\n",
      "Epoch 191/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 990.7805 - mse: 10789778.0000 - mae: 990.7805 - val_loss: 2545.1296 - val_mse: 62053764.0000 - val_mae: 2545.1296\n",
      "Epoch 192/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1024.1269 - mse: 11221750.0000 - mae: 1024.1270 - val_loss: 2543.6365 - val_mse: 62365400.0000 - val_mae: 2543.6365\n",
      "Epoch 193/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1000.9061 - mse: 10698035.0000 - mae: 1000.9061 - val_loss: 2540.0515 - val_mse: 62274772.0000 - val_mae: 2540.0515\n",
      "Epoch 194/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1009.7446 - mse: 10681392.0000 - mae: 1009.7446 - val_loss: 2541.9656 - val_mse: 61921008.0000 - val_mae: 2541.9656\n",
      "Epoch 195/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1022.3534 - mse: 11053650.0000 - mae: 1022.3533 - val_loss: 2533.7529 - val_mse: 61142616.0000 - val_mae: 2533.7529\n",
      "Epoch 196/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1023.6087 - mse: 11249254.0000 - mae: 1023.6086 - val_loss: 2533.0974 - val_mse: 62243956.0000 - val_mae: 2533.0974\n",
      "Epoch 197/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1012.9423 - mse: 10997641.0000 - mae: 1012.9423 - val_loss: 2537.4736 - val_mse: 60967988.0000 - val_mae: 2537.4736\n",
      "Epoch 198/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1012.8782 - mse: 10905727.0000 - mae: 1012.8782 - val_loss: 2557.4717 - val_mse: 63175440.0000 - val_mae: 2557.4717\n",
      "Epoch 199/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1005.3850 - mse: 10515019.0000 - mae: 1005.3851 - val_loss: 2532.9929 - val_mse: 61209340.0000 - val_mae: 2532.9929\n",
      "Epoch 200/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1000.1837 - mse: 10368773.0000 - mae: 1000.1837 - val_loss: 2542.4702 - val_mse: 61447980.0000 - val_mae: 2542.4702\n",
      "Epoch 201/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1014.6228 - mse: 11077052.0000 - mae: 1014.6228 - val_loss: 2515.6875 - val_mse: 60580224.0000 - val_mae: 2515.6875\n",
      "Epoch 202/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1000.6558 - mse: 10617195.0000 - mae: 1000.6558 - val_loss: 2532.3955 - val_mse: 61531176.0000 - val_mae: 2532.3955\n",
      "Epoch 203/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1035.2595 - mse: 11171636.0000 - mae: 1035.2594 - val_loss: 2501.4968 - val_mse: 60235524.0000 - val_mae: 2501.4968\n",
      "Epoch 204/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1000.1975 - mse: 10509824.0000 - mae: 1000.1974 - val_loss: 2501.1143 - val_mse: 59660312.0000 - val_mae: 2501.1143\n",
      "Epoch 205/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 1017.4687 - mse: 10561649.0000 - mae: 1017.4686 - val_loss: 2519.1521 - val_mse: 61189028.0000 - val_mae: 2519.1521\n",
      "Epoch 206/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 997.6249 - mse: 10348962.0000 - mae: 997.6249 - val_loss: 2513.3179 - val_mse: 60326288.0000 - val_mae: 2513.3179\n",
      "Epoch 207/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 1015.6608 - mse: 10966210.0000 - mae: 1015.6608 - val_loss: 2519.6877 - val_mse: 59981244.0000 - val_mae: 2519.6877\n",
      "Epoch 208/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 1001.5930 - mse: 10610751.0000 - mae: 1001.5929 - val_loss: 2514.6477 - val_mse: 59377480.0000 - val_mae: 2514.6477\n",
      "Epoch 209/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 982.3791 - mse: 9806856.0000 - mae: 982.3791 - val_loss: 2485.9866 - val_mse: 60123560.0000 - val_mae: 2485.9866\n",
      "Epoch 210/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 980.8799 - mse: 10068559.0000 - mae: 980.8799 - val_loss: 2503.5718 - val_mse: 60394844.0000 - val_mae: 2503.5718\n",
      "Epoch 211/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 993.8007 - mse: 9889433.0000 - mae: 993.8008 - val_loss: 2493.0798 - val_mse: 58680732.0000 - val_mae: 2493.0798\n",
      "Epoch 212/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 995.1373 - mse: 10373931.0000 - mae: 995.1375 - val_loss: 2509.6621 - val_mse: 59705196.0000 - val_mae: 2509.6621\n",
      "Epoch 213/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1018.2074 - mse: 10656375.0000 - mae: 1018.2073 - val_loss: 2494.5972 - val_mse: 58537204.0000 - val_mae: 2494.5972\n",
      "Epoch 214/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 1012.3959 - mse: 10685319.0000 - mae: 1012.3960 - val_loss: 2456.3223 - val_mse: 57995792.0000 - val_mae: 2456.3223\n",
      "Epoch 215/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 984.0746 - mse: 10251327.0000 - mae: 984.0746 - val_loss: 2486.5525 - val_mse: 58966072.0000 - val_mae: 2486.5525\n",
      "Epoch 216/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 988.2034 - mse: 10204582.0000 - mae: 988.2034 - val_loss: 2468.2781 - val_mse: 57401596.0000 - val_mae: 2468.2781\n",
      "Epoch 217/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 1003.2951 - mse: 10503619.0000 - mae: 1003.2952 - val_loss: 2466.6462 - val_mse: 57806956.0000 - val_mae: 2466.6462\n",
      "Epoch 218/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 987.3914 - mse: 9970823.0000 - mae: 987.3915 - val_loss: 2485.4526 - val_mse: 57992356.0000 - val_mae: 2485.4526\n",
      "Epoch 219/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 975.5089 - mse: 9833373.0000 - mae: 975.5089 - val_loss: 2462.7395 - val_mse: 57630400.0000 - val_mae: 2462.7395\n",
      "Epoch 220/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 993.9713 - mse: 10353064.0000 - mae: 993.9712 - val_loss: 2457.7395 - val_mse: 57534488.0000 - val_mae: 2457.7395\n",
      "Epoch 221/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 973.6412 - mse: 9738850.0000 - mae: 973.6413 - val_loss: 2468.0708 - val_mse: 57531528.0000 - val_mae: 2468.0708\n",
      "Epoch 222/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 992.8275 - mse: 10232252.0000 - mae: 992.8275 - val_loss: 2487.4062 - val_mse: 57737092.0000 - val_mae: 2487.4062\n",
      "Epoch 223/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 1010.5140 - mse: 10346864.0000 - mae: 1010.5140 - val_loss: 2469.4675 - val_mse: 57023932.0000 - val_mae: 2469.4675\n",
      "Epoch 224/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 969.9653 - mse: 9799000.0000 - mae: 969.9653 - val_loss: 2435.0352 - val_mse: 55733940.0000 - val_mae: 2435.0352\n",
      "Epoch 225/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 990.9786 - mse: 10109615.0000 - mae: 990.9786 - val_loss: 2458.7224 - val_mse: 56849272.0000 - val_mae: 2458.7224\n",
      "Epoch 226/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 990.7197 - mse: 10155561.0000 - mae: 990.7197 - val_loss: 2457.5024 - val_mse: 56614068.0000 - val_mae: 2457.5024\n",
      "Epoch 227/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 1001.7614 - mse: 10330879.0000 - mae: 1001.7615 - val_loss: 2457.3289 - val_mse: 56717384.0000 - val_mae: 2457.3289\n",
      "Epoch 228/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 993.9691 - mse: 10397613.0000 - mae: 993.9692 - val_loss: 2459.6584 - val_mse: 56938900.0000 - val_mae: 2459.6584\n",
      "Epoch 229/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 994.5690 - mse: 10658576.0000 - mae: 994.5691 - val_loss: 2445.1797 - val_mse: 56037400.0000 - val_mae: 2445.1797\n",
      "Epoch 230/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 980.5133 - mse: 9993220.0000 - mae: 980.5132 - val_loss: 2485.9958 - val_mse: 58524728.0000 - val_mae: 2485.9958\n",
      "Epoch 231/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 996.6036 - mse: 10574409.0000 - mae: 996.6035 - val_loss: 2492.9849 - val_mse: 58762104.0000 - val_mae: 2492.9849\n",
      "Epoch 232/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 973.5987 - mse: 9908021.0000 - mae: 973.5988 - val_loss: 2481.4727 - val_mse: 58092204.0000 - val_mae: 2481.4727\n",
      "Epoch 233/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 987.8934 - mse: 9997144.0000 - mae: 987.8934 - val_loss: 2451.7253 - val_mse: 56446560.0000 - val_mae: 2451.7253\n",
      "Epoch 234/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 980.4655 - mse: 9649477.0000 - mae: 980.4655 - val_loss: 2484.4031 - val_mse: 58233000.0000 - val_mae: 2484.4031\n",
      "Epoch 235/500\n",
      "5500/5500 [==============================] - 0s 38us/step - loss: 971.0784 - mse: 10056493.0000 - mae: 971.0784 - val_loss: 2478.2151 - val_mse: 57397916.0000 - val_mae: 2478.2151\n",
      "Epoch 236/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 977.9674 - mse: 9809311.0000 - mae: 977.9673 - val_loss: 2478.0059 - val_mse: 58301964.0000 - val_mae: 2478.0059\n",
      "Epoch 237/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 987.6158 - mse: 9992699.0000 - mae: 987.6157 - val_loss: 2447.2917 - val_mse: 56049504.0000 - val_mae: 2447.2917\n",
      "Epoch 238/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 980.6206 - mse: 9940171.0000 - mae: 980.6205 - val_loss: 2457.2817 - val_mse: 56585220.0000 - val_mae: 2457.2817\n",
      "Epoch 239/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 954.5859 - mse: 9426846.0000 - mae: 954.5859 - val_loss: 2460.0642 - val_mse: 57250408.0000 - val_mae: 2460.0642\n",
      "Epoch 240/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 969.4267 - mse: 9497551.0000 - mae: 969.4267 - val_loss: 2460.3467 - val_mse: 57023504.0000 - val_mae: 2460.3467\n",
      "Epoch 241/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 977.3419 - mse: 9768068.0000 - mae: 977.3420 - val_loss: 2423.2004 - val_mse: 54493872.0000 - val_mae: 2423.2004\n",
      "Epoch 242/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 948.8254 - mse: 9227638.0000 - mae: 948.8254 - val_loss: 2453.4370 - val_mse: 56523476.0000 - val_mae: 2453.4370\n",
      "Epoch 243/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 950.0821 - mse: 9151395.0000 - mae: 950.0820 - val_loss: 2452.0176 - val_mse: 55625936.0000 - val_mae: 2452.0176\n",
      "Epoch 244/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 984.0482 - mse: 10113813.0000 - mae: 984.0482 - val_loss: 2440.3013 - val_mse: 54987604.0000 - val_mae: 2440.3013\n",
      "Epoch 245/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 997.9375 - mse: 10271154.0000 - mae: 997.9374 - val_loss: 2462.3723 - val_mse: 56170532.0000 - val_mae: 2462.3723\n",
      "Epoch 246/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 965.2580 - mse: 9759940.0000 - mae: 965.2581 - val_loss: 2452.2419 - val_mse: 56427744.0000 - val_mae: 2452.2419\n",
      "Epoch 247/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 949.7733 - mse: 9341628.0000 - mae: 949.7733 - val_loss: 2449.1086 - val_mse: 56244464.0000 - val_mae: 2449.1086\n",
      "Epoch 248/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 968.4137 - mse: 9897638.0000 - mae: 968.4136 - val_loss: 2461.4629 - val_mse: 56747884.0000 - val_mae: 2461.4629\n",
      "Epoch 249/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 970.4253 - mse: 9760411.0000 - mae: 970.4253 - val_loss: 2447.8928 - val_mse: 55745152.0000 - val_mae: 2447.8928\n",
      "Epoch 250/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 958.4556 - mse: 9553910.0000 - mae: 958.4556 - val_loss: 2444.6804 - val_mse: 55486668.0000 - val_mae: 2444.6804\n",
      "Epoch 251/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 957.6512 - mse: 9422597.0000 - mae: 957.6512 - val_loss: 2417.4587 - val_mse: 55469468.0000 - val_mae: 2417.4587\n",
      "Epoch 252/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 956.3959 - mse: 9357043.0000 - mae: 956.3959 - val_loss: 2438.2302 - val_mse: 55680500.0000 - val_mae: 2438.2302\n",
      "Epoch 253/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 955.7364 - mse: 9464288.0000 - mae: 955.7364 - val_loss: 2416.6174 - val_mse: 55449880.0000 - val_mae: 2416.6174\n",
      "Epoch 254/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 953.2883 - mse: 9098902.0000 - mae: 953.2884 - val_loss: 2412.6506 - val_mse: 54203744.0000 - val_mae: 2412.6506\n",
      "Epoch 255/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 942.2617 - mse: 9247077.0000 - mae: 942.2617 - val_loss: 2403.7036 - val_mse: 54173660.0000 - val_mae: 2403.7036\n",
      "Epoch 256/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 959.4294 - mse: 9578667.0000 - mae: 959.4294 - val_loss: 2401.0344 - val_mse: 53217248.0000 - val_mae: 2401.0344\n",
      "Epoch 257/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 967.0725 - mse: 9657497.0000 - mae: 967.0724 - val_loss: 2387.1030 - val_mse: 52869256.0000 - val_mae: 2387.1030\n",
      "Epoch 258/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 954.1017 - mse: 9285380.0000 - mae: 954.1017 - val_loss: 2408.2878 - val_mse: 54406756.0000 - val_mae: 2408.2878\n",
      "Epoch 259/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 970.9324 - mse: 9914317.0000 - mae: 970.9324 - val_loss: 2398.1489 - val_mse: 53973860.0000 - val_mae: 2398.1489\n",
      "Epoch 260/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 953.4967 - mse: 9189931.0000 - mae: 953.4967 - val_loss: 2431.4585 - val_mse: 54728748.0000 - val_mae: 2431.4585\n",
      "Epoch 261/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 966.6294 - mse: 9586508.0000 - mae: 966.6295 - val_loss: 2397.6667 - val_mse: 53808664.0000 - val_mae: 2397.6667\n",
      "Epoch 262/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 948.8309 - mse: 9196578.0000 - mae: 948.8309 - val_loss: 2403.6604 - val_mse: 54457112.0000 - val_mae: 2403.6604\n",
      "Epoch 263/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 935.3736 - mse: 8713443.0000 - mae: 935.3735 - val_loss: 2391.7266 - val_mse: 54433656.0000 - val_mae: 2391.7266\n",
      "Epoch 264/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 978.3895 - mse: 9758572.0000 - mae: 978.3895 - val_loss: 2414.6616 - val_mse: 54523156.0000 - val_mae: 2414.6616\n",
      "Epoch 265/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 965.3111 - mse: 9705534.0000 - mae: 965.3111 - val_loss: 2408.6309 - val_mse: 54255060.0000 - val_mae: 2408.6309\n",
      "Epoch 266/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 943.2980 - mse: 8963193.0000 - mae: 943.2981 - val_loss: 2381.3311 - val_mse: 52634804.0000 - val_mae: 2381.3311\n",
      "Epoch 267/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 943.2604 - mse: 9208460.0000 - mae: 943.2604 - val_loss: 2395.0964 - val_mse: 53294772.0000 - val_mae: 2395.0964\n",
      "Epoch 268/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 934.0979 - mse: 9079801.0000 - mae: 934.0980 - val_loss: 2448.2712 - val_mse: 55180968.0000 - val_mae: 2448.2712\n",
      "Epoch 269/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 962.5226 - mse: 9328443.0000 - mae: 962.5225 - val_loss: 2399.5356 - val_mse: 53234596.0000 - val_mae: 2399.5356\n",
      "Epoch 270/500\n",
      "5500/5500 [==============================] - 0s 47us/step - loss: 966.5704 - mse: 9401500.0000 - mae: 966.5704 - val_loss: 2399.8608 - val_mse: 53168484.0000 - val_mae: 2399.8608\n",
      "Epoch 271/500\n",
      "5500/5500 [==============================] - 0s 39us/step - loss: 939.4607 - mse: 8901612.0000 - mae: 939.4606 - val_loss: 2366.3818 - val_mse: 52068560.0000 - val_mae: 2366.3818\n",
      "Epoch 272/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 954.4688 - mse: 9525410.0000 - mae: 954.4688 - val_loss: 2395.9792 - val_mse: 52557608.0000 - val_mae: 2395.9792\n",
      "Epoch 273/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 955.6878 - mse: 9429735.0000 - mae: 955.6878 - val_loss: 2415.6516 - val_mse: 53730392.0000 - val_mae: 2415.6516\n",
      "Epoch 274/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 943.2038 - mse: 9188084.0000 - mae: 943.2038 - val_loss: 2348.9648 - val_mse: 51056996.0000 - val_mae: 2348.9648\n",
      "Epoch 275/500\n",
      "5500/5500 [==============================] - 0s 46us/step - loss: 948.0499 - mse: 9082605.0000 - mae: 948.0498 - val_loss: 2379.3706 - val_mse: 52900140.0000 - val_mae: 2379.3706\n",
      "Epoch 276/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 943.2120 - mse: 9272075.0000 - mae: 943.2120 - val_loss: 2375.7629 - val_mse: 52553920.0000 - val_mae: 2375.7629\n",
      "Epoch 277/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 952.9402 - mse: 9434058.0000 - mae: 952.9402 - val_loss: 2367.1726 - val_mse: 52095560.0000 - val_mae: 2367.1726\n",
      "Epoch 278/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 940.7640 - mse: 9087523.0000 - mae: 940.7641 - val_loss: 2390.1929 - val_mse: 52926692.0000 - val_mae: 2390.1929\n",
      "Epoch 279/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 940.8265 - mse: 9161353.0000 - mae: 940.8265 - val_loss: 2413.8137 - val_mse: 53038408.0000 - val_mae: 2413.8137\n",
      "Epoch 280/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 931.1199 - mse: 8917887.0000 - mae: 931.1200 - val_loss: 2395.1660 - val_mse: 52380132.0000 - val_mae: 2395.1660\n",
      "Epoch 281/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 953.8896 - mse: 9120973.0000 - mae: 953.8896 - val_loss: 2396.2075 - val_mse: 52481692.0000 - val_mae: 2396.2075\n",
      "Epoch 282/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 938.6102 - mse: 8924389.0000 - mae: 938.6102 - val_loss: 2382.9507 - val_mse: 52185792.0000 - val_mae: 2382.9507\n",
      "Epoch 283/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 946.6589 - mse: 9407423.0000 - mae: 946.6589 - val_loss: 2376.3867 - val_mse: 52737144.0000 - val_mae: 2376.3867\n",
      "Epoch 284/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 926.2564 - mse: 8864825.0000 - mae: 926.2565 - val_loss: 2398.4512 - val_mse: 53468060.0000 - val_mae: 2398.4512\n",
      "Epoch 285/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 936.7252 - mse: 9153092.0000 - mae: 936.7252 - val_loss: 2363.9607 - val_mse: 51528024.0000 - val_mae: 2363.9607\n",
      "Epoch 286/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 936.9272 - mse: 8860737.0000 - mae: 936.9272 - val_loss: 2385.9390 - val_mse: 51995700.0000 - val_mae: 2385.9390\n",
      "Epoch 287/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 950.4312 - mse: 9152169.0000 - mae: 950.4312 - val_loss: 2366.2817 - val_mse: 51266168.0000 - val_mae: 2366.2817\n",
      "Epoch 288/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 933.1534 - mse: 8893075.0000 - mae: 933.1533 - val_loss: 2334.1384 - val_mse: 49648360.0000 - val_mae: 2334.1384\n",
      "Epoch 289/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 932.9972 - mse: 8826126.0000 - mae: 932.9972 - val_loss: 2393.8240 - val_mse: 51697988.0000 - val_mae: 2393.8240\n",
      "Epoch 290/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 899.9692 - mse: 8104495.5000 - mae: 899.9693 - val_loss: 2383.5828 - val_mse: 51872320.0000 - val_mae: 2383.5828\n",
      "Epoch 291/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 934.9295 - mse: 9102386.0000 - mae: 934.9294 - val_loss: 2354.7029 - val_mse: 50992828.0000 - val_mae: 2354.7029\n",
      "Epoch 292/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 922.8433 - mse: 8437666.0000 - mae: 922.8433 - val_loss: 2360.2798 - val_mse: 50773392.0000 - val_mae: 2360.2798\n",
      "Epoch 293/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 927.3392 - mse: 8825347.0000 - mae: 927.3391 - val_loss: 2362.0000 - val_mse: 51719644.0000 - val_mae: 2362.0000\n",
      "Epoch 294/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 920.9577 - mse: 8743428.0000 - mae: 920.9577 - val_loss: 2367.2986 - val_mse: 51104636.0000 - val_mae: 2367.2986\n",
      "Epoch 295/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 943.6415 - mse: 9242428.0000 - mae: 943.6415 - val_loss: 2367.3325 - val_mse: 51112448.0000 - val_mae: 2367.3325\n",
      "Epoch 296/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 924.9409 - mse: 8631772.0000 - mae: 924.9409 - val_loss: 2340.5017 - val_mse: 50331916.0000 - val_mae: 2340.5017\n",
      "Epoch 297/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 925.2253 - mse: 8723586.0000 - mae: 925.2252 - val_loss: 2354.5317 - val_mse: 50934536.0000 - val_mae: 2354.5317\n",
      "Epoch 298/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 942.4539 - mse: 9111895.0000 - mae: 942.4538 - val_loss: 2359.2014 - val_mse: 50645100.0000 - val_mae: 2359.2014\n",
      "Epoch 299/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 938.6766 - mse: 8894173.0000 - mae: 938.6766 - val_loss: 2344.2063 - val_mse: 49949128.0000 - val_mae: 2344.2063\n",
      "Epoch 300/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 938.2796 - mse: 9055234.0000 - mae: 938.2795 - val_loss: 2327.1392 - val_mse: 50405868.0000 - val_mae: 2327.1392\n",
      "Epoch 301/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 956.4393 - mse: 9556585.0000 - mae: 956.4393 - val_loss: 2370.1721 - val_mse: 51649472.0000 - val_mae: 2370.1721\n",
      "Epoch 302/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 929.8743 - mse: 8662502.0000 - mae: 929.8743 - val_loss: 2343.8618 - val_mse: 50568372.0000 - val_mae: 2343.8618\n",
      "Epoch 303/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 947.3988 - mse: 8872848.0000 - mae: 947.3988 - val_loss: 2375.7410 - val_mse: 51112168.0000 - val_mae: 2375.7410\n",
      "Epoch 304/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 921.3889 - mse: 8726703.0000 - mae: 921.3889 - val_loss: 2360.3867 - val_mse: 50836016.0000 - val_mae: 2360.3867\n",
      "Epoch 305/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 925.6623 - mse: 8795704.0000 - mae: 925.6624 - val_loss: 2356.5044 - val_mse: 50216900.0000 - val_mae: 2356.5044\n",
      "Epoch 306/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 938.1609 - mse: 8992274.0000 - mae: 938.1608 - val_loss: 2346.9021 - val_mse: 50388780.0000 - val_mae: 2346.9021\n",
      "Epoch 307/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 936.9706 - mse: 8901309.0000 - mae: 936.9705 - val_loss: 2345.6704 - val_mse: 50353824.0000 - val_mae: 2345.6704\n",
      "Epoch 308/500\n",
      "5500/5500 [==============================] - 0s 46us/step - loss: 943.0360 - mse: 9383270.0000 - mae: 943.0361 - val_loss: 2347.2104 - val_mse: 50873488.0000 - val_mae: 2347.2104\n",
      "Epoch 309/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 938.4468 - mse: 8953140.0000 - mae: 938.4468 - val_loss: 2379.2268 - val_mse: 51425400.0000 - val_mae: 2379.2268\n",
      "Epoch 310/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 927.1995 - mse: 8690460.0000 - mae: 927.1995 - val_loss: 2345.8442 - val_mse: 50459336.0000 - val_mae: 2345.8442\n",
      "Epoch 311/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 926.1250 - mse: 8608173.0000 - mae: 926.1251 - val_loss: 2354.2903 - val_mse: 50894392.0000 - val_mae: 2354.2903\n",
      "Epoch 312/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 928.5695 - mse: 8704443.0000 - mae: 928.5695 - val_loss: 2352.4343 - val_mse: 50974564.0000 - val_mae: 2352.4343\n",
      "Epoch 313/500\n",
      "5500/5500 [==============================] - 0s 49us/step - loss: 924.1954 - mse: 8678944.0000 - mae: 924.1953 - val_loss: 2348.3008 - val_mse: 51116736.0000 - val_mae: 2348.3008\n",
      "Epoch 314/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 919.4453 - mse: 8462458.0000 - mae: 919.4453 - val_loss: 2367.7861 - val_mse: 51155396.0000 - val_mae: 2367.7861\n",
      "Epoch 315/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 904.3422 - mse: 8280367.0000 - mae: 904.3422 - val_loss: 2359.0747 - val_mse: 50510024.0000 - val_mae: 2359.0747\n",
      "Epoch 316/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 934.8884 - mse: 8915873.0000 - mae: 934.8884 - val_loss: 2337.8696 - val_mse: 50159068.0000 - val_mae: 2337.8696\n",
      "Epoch 317/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 929.8207 - mse: 8812161.0000 - mae: 929.8207 - val_loss: 2334.0706 - val_mse: 49590544.0000 - val_mae: 2334.0706\n",
      "Epoch 318/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 922.4244 - mse: 8800931.0000 - mae: 922.4244 - val_loss: 2338.3970 - val_mse: 50167444.0000 - val_mae: 2338.3970\n",
      "Epoch 319/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 923.4201 - mse: 8532420.0000 - mae: 923.4201 - val_loss: 2335.4268 - val_mse: 49704636.0000 - val_mae: 2335.4268\n",
      "Epoch 320/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 948.8903 - mse: 9304267.0000 - mae: 948.8904 - val_loss: 2326.5271 - val_mse: 49378940.0000 - val_mae: 2326.5271\n",
      "Epoch 321/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 909.3147 - mse: 8430119.0000 - mae: 909.3146 - val_loss: 2314.5012 - val_mse: 49118224.0000 - val_mae: 2314.5012\n",
      "Epoch 322/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 927.9511 - mse: 8642937.0000 - mae: 927.9511 - val_loss: 2339.0278 - val_mse: 49746164.0000 - val_mae: 2339.0278\n",
      "Epoch 323/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 914.4042 - mse: 8580411.0000 - mae: 914.4042 - val_loss: 2310.2461 - val_mse: 48902044.0000 - val_mae: 2310.2461\n",
      "Epoch 324/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 906.2155 - mse: 8133404.0000 - mae: 906.2155 - val_loss: 2365.0081 - val_mse: 50643604.0000 - val_mae: 2365.0081\n",
      "Epoch 325/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 908.3506 - mse: 8244351.5000 - mae: 908.3506 - val_loss: 2342.7097 - val_mse: 49765604.0000 - val_mae: 2342.7097\n",
      "Epoch 326/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 933.5678 - mse: 8904285.0000 - mae: 933.5679 - val_loss: 2322.0193 - val_mse: 49213888.0000 - val_mae: 2322.0193\n",
      "Epoch 327/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 930.5737 - mse: 8868260.0000 - mae: 930.5737 - val_loss: 2343.6089 - val_mse: 49933848.0000 - val_mae: 2343.6089\n",
      "Epoch 328/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 932.3836 - mse: 8871325.0000 - mae: 932.3836 - val_loss: 2306.7869 - val_mse: 48891124.0000 - val_mae: 2306.7869\n",
      "Epoch 329/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 924.3995 - mse: 8726223.0000 - mae: 924.3997 - val_loss: 2330.4070 - val_mse: 48940696.0000 - val_mae: 2330.4070\n",
      "Epoch 330/500\n",
      "5500/5500 [==============================] - 0s 46us/step - loss: 925.7794 - mse: 8588406.0000 - mae: 925.7795 - val_loss: 2327.4133 - val_mse: 48725096.0000 - val_mae: 2327.4133\n",
      "Epoch 331/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 918.9137 - mse: 8744069.0000 - mae: 918.9136 - val_loss: 2335.8044 - val_mse: 49561892.0000 - val_mae: 2335.8044\n",
      "Epoch 332/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 917.3527 - mse: 8503050.0000 - mae: 917.3527 - val_loss: 2320.9954 - val_mse: 49004408.0000 - val_mae: 2320.9954\n",
      "Epoch 333/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 907.0402 - mse: 8289105.0000 - mae: 907.0402 - val_loss: 2323.2151 - val_mse: 48731316.0000 - val_mae: 2323.2151\n",
      "Epoch 334/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 935.8848 - mse: 8682425.0000 - mae: 935.8848 - val_loss: 2328.4570 - val_mse: 49151936.0000 - val_mae: 2328.4570\n",
      "Epoch 335/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 924.0975 - mse: 8748716.0000 - mae: 924.0975 - val_loss: 2310.0269 - val_mse: 48555900.0000 - val_mae: 2310.0269\n",
      "Epoch 336/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 919.0173 - mse: 8535321.0000 - mae: 919.0173 - val_loss: 2311.8958 - val_mse: 49807768.0000 - val_mae: 2311.8958\n",
      "Epoch 337/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 938.3704 - mse: 9152774.0000 - mae: 938.3704 - val_loss: 2326.1992 - val_mse: 49340412.0000 - val_mae: 2326.1992\n",
      "Epoch 338/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 935.2347 - mse: 8921248.0000 - mae: 935.2347 - val_loss: 2329.0605 - val_mse: 49150776.0000 - val_mae: 2329.0605\n",
      "Epoch 339/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 926.1081 - mse: 9044996.0000 - mae: 926.1081 - val_loss: 2320.8284 - val_mse: 49159604.0000 - val_mae: 2320.8284\n",
      "Epoch 340/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 913.8639 - mse: 8428874.0000 - mae: 913.8640 - val_loss: 2332.0769 - val_mse: 49682932.0000 - val_mae: 2332.0769\n",
      "Epoch 341/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 890.3952 - mse: 8188390.5000 - mae: 890.3953 - val_loss: 2337.4788 - val_mse: 48685704.0000 - val_mae: 2337.4788\n",
      "Epoch 342/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 921.1382 - mse: 8592060.0000 - mae: 921.1382 - val_loss: 2314.4873 - val_mse: 49337188.0000 - val_mae: 2314.4873\n",
      "Epoch 343/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 923.5475 - mse: 8540298.0000 - mae: 923.5474 - val_loss: 2323.3521 - val_mse: 49064456.0000 - val_mae: 2323.3521\n",
      "Epoch 344/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 927.1669 - mse: 8773026.0000 - mae: 927.1668 - val_loss: 2354.4780 - val_mse: 50172320.0000 - val_mae: 2354.4780\n",
      "Epoch 345/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 935.7100 - mse: 8629942.0000 - mae: 935.7100 - val_loss: 2340.0117 - val_mse: 49738292.0000 - val_mae: 2340.0117\n",
      "Epoch 346/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 908.9533 - mse: 8168886.0000 - mae: 908.9532 - val_loss: 2314.2527 - val_mse: 48232492.0000 - val_mae: 2314.2527\n",
      "Epoch 347/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 913.5909 - mse: 8452948.0000 - mae: 913.5908 - val_loss: 2337.1726 - val_mse: 49051700.0000 - val_mae: 2337.1726\n",
      "Epoch 348/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 927.4695 - mse: 8515401.0000 - mae: 927.4695 - val_loss: 2323.2341 - val_mse: 48138960.0000 - val_mae: 2323.2341\n",
      "Epoch 349/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 919.0784 - mse: 8708844.0000 - mae: 919.0786 - val_loss: 2334.7402 - val_mse: 49492172.0000 - val_mae: 2334.7402\n",
      "Epoch 350/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 910.0823 - mse: 8385783.0000 - mae: 910.0823 - val_loss: 2341.3789 - val_mse: 49228244.0000 - val_mae: 2341.3789\n",
      "Epoch 351/500\n",
      "5500/5500 [==============================] - 0s 47us/step - loss: 900.8440 - mse: 8258911.5000 - mae: 900.8440 - val_loss: 2323.3103 - val_mse: 48441404.0000 - val_mae: 2323.3103\n",
      "Epoch 352/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 916.5798 - mse: 8397136.0000 - mae: 916.5798 - val_loss: 2328.1318 - val_mse: 49005412.0000 - val_mae: 2328.1318\n",
      "Epoch 353/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 920.6174 - mse: 8611176.0000 - mae: 920.6174 - val_loss: 2315.9265 - val_mse: 48451428.0000 - val_mae: 2315.9265\n",
      "Epoch 354/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 926.9260 - mse: 8608790.0000 - mae: 926.9259 - val_loss: 2326.2317 - val_mse: 49128860.0000 - val_mae: 2326.2317\n",
      "Epoch 355/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 909.2875 - mse: 8186585.0000 - mae: 909.2875 - val_loss: 2325.9446 - val_mse: 49080660.0000 - val_mae: 2325.9446\n",
      "Epoch 356/500\n",
      "5500/5500 [==============================] - 0s 46us/step - loss: 899.1998 - mse: 8168405.5000 - mae: 899.1998 - val_loss: 2314.8936 - val_mse: 49203424.0000 - val_mae: 2314.8936\n",
      "Epoch 357/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 909.9142 - mse: 8007832.5000 - mae: 909.9142 - val_loss: 2343.5022 - val_mse: 49635572.0000 - val_mae: 2343.5022\n",
      "Epoch 358/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 903.5925 - mse: 8259491.5000 - mae: 903.5925 - val_loss: 2326.4905 - val_mse: 48435212.0000 - val_mae: 2326.4905\n",
      "Epoch 359/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 917.7602 - mse: 8717899.0000 - mae: 917.7602 - val_loss: 2332.9985 - val_mse: 48694120.0000 - val_mae: 2332.9985\n",
      "Epoch 360/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 917.8237 - mse: 8280980.0000 - mae: 917.8236 - val_loss: 2305.3604 - val_mse: 48338012.0000 - val_mae: 2305.3604\n",
      "Epoch 361/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 929.4893 - mse: 8462486.0000 - mae: 929.4893 - val_loss: 2314.3394 - val_mse: 48783012.0000 - val_mae: 2314.3394\n",
      "Epoch 362/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 917.7473 - mse: 8808587.0000 - mae: 917.7472 - val_loss: 2317.2051 - val_mse: 48652684.0000 - val_mae: 2317.2051\n",
      "Epoch 363/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 915.8821 - mse: 8268597.5000 - mae: 915.8821 - val_loss: 2351.3730 - val_mse: 49862124.0000 - val_mae: 2351.3730\n",
      "Epoch 364/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 887.0151 - mse: 7978917.0000 - mae: 887.0151 - val_loss: 2311.4998 - val_mse: 48259664.0000 - val_mae: 2311.4998\n",
      "Epoch 365/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 911.8456 - mse: 8466399.0000 - mae: 911.8456 - val_loss: 2323.8044 - val_mse: 48595536.0000 - val_mae: 2323.8044\n",
      "Epoch 366/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 907.4981 - mse: 8725346.0000 - mae: 907.4981 - val_loss: 2342.6792 - val_mse: 49947864.0000 - val_mae: 2342.6792\n",
      "Epoch 367/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 919.7764 - mse: 8585282.0000 - mae: 919.7764 - val_loss: 2324.0920 - val_mse: 48946668.0000 - val_mae: 2324.0920\n",
      "Epoch 368/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 923.8183 - mse: 8597083.0000 - mae: 923.8184 - val_loss: 2307.6809 - val_mse: 48878216.0000 - val_mae: 2307.6809\n",
      "Epoch 369/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 918.8688 - mse: 8666058.0000 - mae: 918.8688 - val_loss: 2321.8381 - val_mse: 48826352.0000 - val_mae: 2321.8381\n",
      "Epoch 370/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 911.3327 - mse: 8523413.0000 - mae: 911.3327 - val_loss: 2301.3242 - val_mse: 47990172.0000 - val_mae: 2301.3242\n",
      "Epoch 371/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 903.1091 - mse: 8459270.0000 - mae: 903.1092 - val_loss: 2295.3162 - val_mse: 47817176.0000 - val_mae: 2295.3162\n",
      "Epoch 372/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 912.4661 - mse: 8396965.0000 - mae: 912.4660 - val_loss: 2325.5747 - val_mse: 48695636.0000 - val_mae: 2325.5747\n",
      "Epoch 373/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 918.4424 - mse: 8585858.0000 - mae: 918.4424 - val_loss: 2310.4939 - val_mse: 48169884.0000 - val_mae: 2310.4939\n",
      "Epoch 374/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 895.4725 - mse: 7985845.5000 - mae: 895.4725 - val_loss: 2329.0862 - val_mse: 48970232.0000 - val_mae: 2329.0862\n",
      "Epoch 375/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 893.1315 - mse: 8152881.0000 - mae: 893.1315 - val_loss: 2316.8328 - val_mse: 48151188.0000 - val_mae: 2316.8328\n",
      "Epoch 376/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 902.8205 - mse: 8364773.0000 - mae: 902.8204 - val_loss: 2328.4937 - val_mse: 49012260.0000 - val_mae: 2328.4937\n",
      "Epoch 377/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 916.5307 - mse: 8578146.0000 - mae: 916.5307 - val_loss: 2303.9329 - val_mse: 47512812.0000 - val_mae: 2303.9329\n",
      "Epoch 378/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 923.3079 - mse: 8821905.0000 - mae: 923.3078 - val_loss: 2285.0581 - val_mse: 46922888.0000 - val_mae: 2285.0581\n",
      "Epoch 379/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 911.7047 - mse: 8636317.0000 - mae: 911.7047 - val_loss: 2298.5396 - val_mse: 47420052.0000 - val_mae: 2298.5396\n",
      "Epoch 380/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 916.1697 - mse: 8482193.0000 - mae: 916.1696 - val_loss: 2291.1318 - val_mse: 47336952.0000 - val_mae: 2291.1318\n",
      "Epoch 381/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 929.3856 - mse: 8795910.0000 - mae: 929.3856 - val_loss: 2313.8818 - val_mse: 47612956.0000 - val_mae: 2313.8818\n",
      "Epoch 382/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 930.6245 - mse: 8841337.0000 - mae: 930.6246 - val_loss: 2356.9556 - val_mse: 50063676.0000 - val_mae: 2356.9556\n",
      "Epoch 383/500\n",
      "5500/5500 [==============================] - 0s 40us/step - loss: 897.5129 - mse: 8174448.5000 - mae: 897.5129 - val_loss: 2324.3848 - val_mse: 48118176.0000 - val_mae: 2324.3848\n",
      "Epoch 384/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 893.1843 - mse: 7999071.0000 - mae: 893.1843 - val_loss: 2325.8750 - val_mse: 48209420.0000 - val_mae: 2325.8750\n",
      "Epoch 385/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 900.9100 - mse: 8199857.0000 - mae: 900.9100 - val_loss: 2318.8049 - val_mse: 49111612.0000 - val_mae: 2318.8049\n",
      "Epoch 386/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 900.9735 - mse: 8500686.0000 - mae: 900.9736 - val_loss: 2318.8774 - val_mse: 48231468.0000 - val_mae: 2318.8774\n",
      "Epoch 387/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 899.3456 - mse: 8186207.0000 - mae: 899.3456 - val_loss: 2307.0420 - val_mse: 48107508.0000 - val_mae: 2307.0420\n",
      "Epoch 388/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 929.8136 - mse: 8804120.0000 - mae: 929.8137 - val_loss: 2328.0068 - val_mse: 48825984.0000 - val_mae: 2328.0068\n",
      "Epoch 389/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 898.1281 - mse: 8135187.0000 - mae: 898.1281 - val_loss: 2302.4236 - val_mse: 47845080.0000 - val_mae: 2302.4236\n",
      "Epoch 390/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 887.9618 - mse: 7858464.0000 - mae: 887.9618 - val_loss: 2309.9727 - val_mse: 47559728.0000 - val_mae: 2309.9727\n",
      "Epoch 391/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 892.6318 - mse: 8112343.0000 - mae: 892.6318 - val_loss: 2294.5659 - val_mse: 47372616.0000 - val_mae: 2294.5659\n",
      "Epoch 392/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 894.5428 - mse: 8080585.0000 - mae: 894.5428 - val_loss: 2284.5537 - val_mse: 47654848.0000 - val_mae: 2284.5537\n",
      "Epoch 393/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 909.5573 - mse: 8624871.0000 - mae: 909.5573 - val_loss: 2317.9199 - val_mse: 48638196.0000 - val_mae: 2317.9199\n",
      "Epoch 394/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 906.4416 - mse: 8451813.0000 - mae: 906.4417 - val_loss: 2326.8643 - val_mse: 49313944.0000 - val_mae: 2326.8643\n",
      "Epoch 395/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 892.9869 - mse: 7959147.5000 - mae: 892.9869 - val_loss: 2294.5610 - val_mse: 47341052.0000 - val_mae: 2294.5610\n",
      "Epoch 396/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 906.2335 - mse: 8305258.0000 - mae: 906.2335 - val_loss: 2316.1689 - val_mse: 48295876.0000 - val_mae: 2316.1689\n",
      "Epoch 397/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 912.5052 - mse: 8493726.0000 - mae: 912.5052 - val_loss: 2322.8884 - val_mse: 48817124.0000 - val_mae: 2322.8884\n",
      "Epoch 398/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 880.5773 - mse: 7720783.0000 - mae: 880.5773 - val_loss: 2296.8298 - val_mse: 47639332.0000 - val_mae: 2296.8298\n",
      "Epoch 399/500\n",
      "5500/5500 [==============================] - 0s 46us/step - loss: 916.3370 - mse: 8286311.0000 - mae: 916.3370 - val_loss: 2293.3481 - val_mse: 47929348.0000 - val_mae: 2293.3481\n",
      "Epoch 400/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 912.0341 - mse: 8481982.0000 - mae: 912.0341 - val_loss: 2309.4141 - val_mse: 48140828.0000 - val_mae: 2309.4141\n",
      "Epoch 401/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 897.0481 - mse: 8315287.5000 - mae: 897.0480 - val_loss: 2320.0522 - val_mse: 48532868.0000 - val_mae: 2320.0522\n",
      "Epoch 402/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 898.0504 - mse: 8174587.0000 - mae: 898.0505 - val_loss: 2296.2939 - val_mse: 47761364.0000 - val_mae: 2296.2939\n",
      "Epoch 403/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 895.1127 - mse: 8300648.5000 - mae: 895.1127 - val_loss: 2297.0591 - val_mse: 47518156.0000 - val_mae: 2297.0591\n",
      "Epoch 404/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 886.4364 - mse: 7861556.5000 - mae: 886.4363 - val_loss: 2314.1853 - val_mse: 48007312.0000 - val_mae: 2314.1853\n",
      "Epoch 405/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 891.1528 - mse: 7991228.0000 - mae: 891.1527 - val_loss: 2283.7986 - val_mse: 46905112.0000 - val_mae: 2283.7986\n",
      "Epoch 406/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 910.9796 - mse: 8403274.0000 - mae: 910.9796 - val_loss: 2303.6560 - val_mse: 47517316.0000 - val_mae: 2303.6560\n",
      "Epoch 407/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 916.8017 - mse: 8445720.0000 - mae: 916.8017 - val_loss: 2319.3984 - val_mse: 48953704.0000 - val_mae: 2319.3984\n",
      "Epoch 408/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 919.4633 - mse: 8517749.0000 - mae: 919.4633 - val_loss: 2316.3792 - val_mse: 47953928.0000 - val_mae: 2316.3792\n",
      "Epoch 409/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 894.1294 - mse: 7988665.5000 - mae: 894.1293 - val_loss: 2310.9397 - val_mse: 47518740.0000 - val_mae: 2310.9397\n",
      "Epoch 410/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 908.6353 - mse: 8572098.0000 - mae: 908.6353 - val_loss: 2312.2712 - val_mse: 47852052.0000 - val_mae: 2312.2712\n",
      "Epoch 411/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 907.3366 - mse: 8319053.5000 - mae: 907.3366 - val_loss: 2297.9211 - val_mse: 47329952.0000 - val_mae: 2297.9211\n",
      "Epoch 412/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 910.7877 - mse: 8388385.5000 - mae: 910.7877 - val_loss: 2301.8442 - val_mse: 47856744.0000 - val_mae: 2301.8442\n",
      "Epoch 413/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 894.4698 - mse: 8106068.0000 - mae: 894.4697 - val_loss: 2302.2178 - val_mse: 47097664.0000 - val_mae: 2302.2178\n",
      "Epoch 414/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 911.5546 - mse: 8508222.0000 - mae: 911.5546 - val_loss: 2310.9824 - val_mse: 46989464.0000 - val_mae: 2310.9824\n",
      "Epoch 415/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 910.6865 - mse: 8243915.5000 - mae: 910.6865 - val_loss: 2300.4470 - val_mse: 47342620.0000 - val_mae: 2300.4470\n",
      "Epoch 416/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 882.6818 - mse: 7834541.5000 - mae: 882.6817 - val_loss: 2307.4021 - val_mse: 47449984.0000 - val_mae: 2307.4021\n",
      "Epoch 417/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 909.0118 - mse: 8684560.0000 - mae: 909.0118 - val_loss: 2313.8521 - val_mse: 47966920.0000 - val_mae: 2313.8521\n",
      "Epoch 418/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 907.0507 - mse: 8439966.0000 - mae: 907.0507 - val_loss: 2307.1389 - val_mse: 46971000.0000 - val_mae: 2307.1389\n",
      "Epoch 419/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 896.7042 - mse: 8314851.5000 - mae: 896.7042 - val_loss: 2316.4211 - val_mse: 46659172.0000 - val_mae: 2316.4211\n",
      "Epoch 420/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 877.2959 - mse: 8080233.0000 - mae: 877.2959 - val_loss: 2293.9578 - val_mse: 46606392.0000 - val_mae: 2293.9578\n",
      "Epoch 421/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 903.2667 - mse: 8025844.0000 - mae: 903.2667 - val_loss: 2289.2209 - val_mse: 47109988.0000 - val_mae: 2289.2209\n",
      "Epoch 422/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 906.9512 - mse: 8315846.0000 - mae: 906.9512 - val_loss: 2292.9221 - val_mse: 46919164.0000 - val_mae: 2292.9221\n",
      "Epoch 423/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 886.0281 - mse: 7632357.0000 - mae: 886.0281 - val_loss: 2294.0471 - val_mse: 46916640.0000 - val_mae: 2294.0471\n",
      "Epoch 424/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 903.2444 - mse: 8374722.0000 - mae: 903.2443 - val_loss: 2299.9553 - val_mse: 47190176.0000 - val_mae: 2299.9553\n",
      "Epoch 425/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 886.3013 - mse: 7798578.5000 - mae: 886.3013 - val_loss: 2296.9895 - val_mse: 46829992.0000 - val_mae: 2296.9895\n",
      "Epoch 426/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 887.3441 - mse: 7855562.0000 - mae: 887.3442 - val_loss: 2303.5903 - val_mse: 46960168.0000 - val_mae: 2303.5903\n",
      "Epoch 427/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 898.3943 - mse: 8138189.0000 - mae: 898.3943 - val_loss: 2284.4492 - val_mse: 46824496.0000 - val_mae: 2284.4492\n",
      "Epoch 428/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 882.7896 - mse: 8036723.5000 - mae: 882.7896 - val_loss: 2277.3513 - val_mse: 46208548.0000 - val_mae: 2277.3513\n",
      "Epoch 429/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 887.5723 - mse: 8090700.5000 - mae: 887.5723 - val_loss: 2308.1487 - val_mse: 47677312.0000 - val_mae: 2308.1487\n",
      "Epoch 430/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 891.2624 - mse: 7972363.5000 - mae: 891.2624 - val_loss: 2292.5920 - val_mse: 47708084.0000 - val_mae: 2292.5920\n",
      "Epoch 431/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 906.3747 - mse: 8345002.0000 - mae: 906.3746 - val_loss: 2291.3010 - val_mse: 46959040.0000 - val_mae: 2291.3010\n",
      "Epoch 432/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 925.6207 - mse: 8607612.0000 - mae: 925.6207 - val_loss: 2284.0305 - val_mse: 46760220.0000 - val_mae: 2284.0305\n",
      "Epoch 433/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 888.6461 - mse: 8015948.0000 - mae: 888.6462 - val_loss: 2304.3538 - val_mse: 47470292.0000 - val_mae: 2304.3538\n",
      "Epoch 434/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 890.0751 - mse: 8370822.5000 - mae: 890.0750 - val_loss: 2286.6279 - val_mse: 46287896.0000 - val_mae: 2286.6279\n",
      "Epoch 435/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 899.2540 - mse: 8167780.5000 - mae: 899.2540 - val_loss: 2297.0586 - val_mse: 46869340.0000 - val_mae: 2297.0586\n",
      "Epoch 436/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 900.9629 - mse: 8328585.0000 - mae: 900.9629 - val_loss: 2302.7566 - val_mse: 47539340.0000 - val_mae: 2302.7566\n",
      "Epoch 437/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 905.3325 - mse: 8440664.0000 - mae: 905.3325 - val_loss: 2307.2393 - val_mse: 46755116.0000 - val_mae: 2307.2393\n",
      "Epoch 438/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 906.9069 - mse: 8435395.0000 - mae: 906.9069 - val_loss: 2310.3901 - val_mse: 47163452.0000 - val_mae: 2310.3901\n",
      "Epoch 439/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 917.8169 - mse: 8724064.0000 - mae: 917.8169 - val_loss: 2289.8203 - val_mse: 46936572.0000 - val_mae: 2289.8203\n",
      "Epoch 440/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 896.9817 - mse: 7997029.0000 - mae: 896.9818 - val_loss: 2288.2808 - val_mse: 45769380.0000 - val_mae: 2288.2808\n",
      "Epoch 441/500\n",
      "5500/5500 [==============================] - 0s 48us/step - loss: 907.1705 - mse: 8371262.5000 - mae: 907.1705 - val_loss: 2284.7700 - val_mse: 46149704.0000 - val_mae: 2284.7700\n",
      "Epoch 442/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 898.5392 - mse: 8071790.0000 - mae: 898.5391 - val_loss: 2302.8503 - val_mse: 47117076.0000 - val_mae: 2302.8503\n",
      "Epoch 443/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 897.6046 - mse: 8295064.5000 - mae: 897.6047 - val_loss: 2318.7766 - val_mse: 47696892.0000 - val_mae: 2318.7766\n",
      "Epoch 444/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 892.6007 - mse: 8072230.0000 - mae: 892.6007 - val_loss: 2305.8640 - val_mse: 47446128.0000 - val_mae: 2305.8640\n",
      "Epoch 445/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 911.6588 - mse: 8383529.0000 - mae: 911.6588 - val_loss: 2287.5923 - val_mse: 46582044.0000 - val_mae: 2287.5923\n",
      "Epoch 446/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 888.4373 - mse: 7991427.0000 - mae: 888.4373 - val_loss: 2273.8933 - val_mse: 45448612.0000 - val_mae: 2273.8933\n",
      "Epoch 447/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 902.0552 - mse: 8342568.0000 - mae: 902.0552 - val_loss: 2284.6382 - val_mse: 46098856.0000 - val_mae: 2284.6382\n",
      "Epoch 448/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 901.9115 - mse: 8397927.0000 - mae: 901.9116 - val_loss: 2265.5984 - val_mse: 45767908.0000 - val_mae: 2265.5984\n",
      "Epoch 449/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 905.8914 - mse: 8222617.0000 - mae: 905.8915 - val_loss: 2282.6172 - val_mse: 46298116.0000 - val_mae: 2282.6172\n",
      "Epoch 450/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 893.7917 - mse: 8059379.5000 - mae: 893.7916 - val_loss: 2278.6038 - val_mse: 46276220.0000 - val_mae: 2278.6038\n",
      "Epoch 451/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 893.7238 - mse: 8142574.5000 - mae: 893.7238 - val_loss: 2268.5874 - val_mse: 45595996.0000 - val_mae: 2268.5874\n",
      "Epoch 452/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 881.2001 - mse: 7800674.0000 - mae: 881.2000 - val_loss: 2295.4641 - val_mse: 46797100.0000 - val_mae: 2295.4641\n",
      "Epoch 453/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 891.9614 - mse: 8200299.0000 - mae: 891.9614 - val_loss: 2277.7266 - val_mse: 45642188.0000 - val_mae: 2277.7266\n",
      "Epoch 454/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 899.5661 - mse: 8282035.5000 - mae: 899.5660 - val_loss: 2296.3733 - val_mse: 46834244.0000 - val_mae: 2296.3733\n",
      "Epoch 455/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 891.3880 - mse: 8215331.0000 - mae: 891.3879 - val_loss: 2262.1997 - val_mse: 45651128.0000 - val_mae: 2262.1997\n",
      "Epoch 456/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 897.6056 - mse: 8280970.5000 - mae: 897.6055 - val_loss: 2287.5532 - val_mse: 45981340.0000 - val_mae: 2287.5532\n",
      "Epoch 457/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 888.4631 - mse: 8270110.0000 - mae: 888.4632 - val_loss: 2301.3694 - val_mse: 46213188.0000 - val_mae: 2301.3694\n",
      "Epoch 458/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 896.1304 - mse: 8003344.0000 - mae: 896.1304 - val_loss: 2276.8066 - val_mse: 45472940.0000 - val_mae: 2276.8066\n",
      "Epoch 459/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 897.7184 - mse: 8439273.0000 - mae: 897.7184 - val_loss: 2281.8911 - val_mse: 46523396.0000 - val_mae: 2281.8911\n",
      "Epoch 460/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 896.5771 - mse: 8009940.0000 - mae: 896.5771 - val_loss: 2288.3130 - val_mse: 46363312.0000 - val_mae: 2288.3130\n",
      "Epoch 461/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 899.5226 - mse: 8277859.5000 - mae: 899.5226 - val_loss: 2280.7253 - val_mse: 46069812.0000 - val_mae: 2280.7253\n",
      "Epoch 462/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 877.9835 - mse: 7937413.5000 - mae: 877.9835 - val_loss: 2313.6855 - val_mse: 47454368.0000 - val_mae: 2313.6855\n",
      "Epoch 463/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 893.3989 - mse: 8033425.0000 - mae: 893.3989 - val_loss: 2301.2507 - val_mse: 47042252.0000 - val_mae: 2301.2507\n",
      "Epoch 464/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 902.1257 - mse: 8440643.0000 - mae: 902.1258 - val_loss: 2308.9583 - val_mse: 47420972.0000 - val_mae: 2308.9583\n",
      "Epoch 465/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 890.2928 - mse: 7929750.0000 - mae: 890.2928 - val_loss: 2308.5071 - val_mse: 47291348.0000 - val_mae: 2308.5071\n",
      "Epoch 466/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 904.1799 - mse: 8400842.0000 - mae: 904.1798 - val_loss: 2300.6360 - val_mse: 46582396.0000 - val_mae: 2300.6360\n",
      "Epoch 467/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 891.6169 - mse: 8214978.5000 - mae: 891.6168 - val_loss: 2292.3259 - val_mse: 46418396.0000 - val_mae: 2292.3259\n",
      "Epoch 468/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 906.3962 - mse: 8420891.0000 - mae: 906.3962 - val_loss: 2325.7834 - val_mse: 47274316.0000 - val_mae: 2325.7834\n",
      "Epoch 469/500\n",
      "5500/5500 [==============================] - 0s 46us/step - loss: 914.4699 - mse: 8529998.0000 - mae: 914.4699 - val_loss: 2286.2827 - val_mse: 46252024.0000 - val_mae: 2286.2827\n",
      "Epoch 470/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 889.8376 - mse: 7949271.0000 - mae: 889.8376 - val_loss: 2321.0522 - val_mse: 47872088.0000 - val_mae: 2321.0522\n",
      "Epoch 471/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 895.5396 - mse: 8170531.0000 - mae: 895.5396 - val_loss: 2290.2808 - val_mse: 46186948.0000 - val_mae: 2290.2808\n",
      "Epoch 472/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 886.8432 - mse: 7815326.5000 - mae: 886.8432 - val_loss: 2261.5071 - val_mse: 44949536.0000 - val_mae: 2261.5071\n",
      "Epoch 473/500\n",
      "5500/5500 [==============================] - 0s 46us/step - loss: 871.8803 - mse: 7410572.5000 - mae: 871.8802 - val_loss: 2279.7717 - val_mse: 45435296.0000 - val_mae: 2279.7717\n",
      "Epoch 474/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 900.5380 - mse: 8268981.0000 - mae: 900.5380 - val_loss: 2277.0405 - val_mse: 45819784.0000 - val_mae: 2277.0405\n",
      "Epoch 475/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 909.8014 - mse: 8053763.0000 - mae: 909.8016 - val_loss: 2304.8901 - val_mse: 47038872.0000 - val_mae: 2304.8901\n",
      "Epoch 476/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 894.6243 - mse: 8320974.0000 - mae: 894.6243 - val_loss: 2287.1992 - val_mse: 46028168.0000 - val_mae: 2287.1992\n",
      "Epoch 477/500\n",
      "5500/5500 [==============================] - 0s 46us/step - loss: 899.9964 - mse: 8253674.0000 - mae: 899.9963 - val_loss: 2288.7427 - val_mse: 46535948.0000 - val_mae: 2288.7427\n",
      "Epoch 478/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 891.3211 - mse: 7951986.5000 - mae: 891.3212 - val_loss: 2266.7908 - val_mse: 45894596.0000 - val_mae: 2266.7908\n",
      "Epoch 479/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 909.9285 - mse: 8438009.0000 - mae: 909.9285 - val_loss: 2289.4470 - val_mse: 46704204.0000 - val_mae: 2289.4470\n",
      "Epoch 480/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 883.0293 - mse: 8038659.0000 - mae: 883.0295 - val_loss: 2291.9202 - val_mse: 46246908.0000 - val_mae: 2291.9202\n",
      "Epoch 481/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 903.8152 - mse: 8043736.0000 - mae: 903.8152 - val_loss: 2287.8323 - val_mse: 46418560.0000 - val_mae: 2287.8323\n",
      "Epoch 482/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 890.8096 - mse: 7877743.0000 - mae: 890.8096 - val_loss: 2286.0964 - val_mse: 46261048.0000 - val_mae: 2286.0964\n",
      "Epoch 483/500\n",
      "5500/5500 [==============================] - 0s 48us/step - loss: 912.2044 - mse: 8186218.0000 - mae: 912.2043 - val_loss: 2266.9331 - val_mse: 45263764.0000 - val_mae: 2266.9331\n",
      "Epoch 484/500\n",
      "5500/5500 [==============================] - 0s 45us/step - loss: 891.2365 - mse: 7953225.5000 - mae: 891.2366 - val_loss: 2291.5056 - val_mse: 46557812.0000 - val_mae: 2291.5056\n",
      "Epoch 485/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 883.9666 - mse: 7918199.0000 - mae: 883.9664 - val_loss: 2268.3398 - val_mse: 45476504.0000 - val_mae: 2268.3398\n",
      "Epoch 486/500\n",
      "5500/5500 [==============================] - 0s 42us/step - loss: 886.4826 - mse: 7764937.5000 - mae: 886.4827 - val_loss: 2285.5503 - val_mse: 46301968.0000 - val_mae: 2285.5503\n",
      "Epoch 487/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 901.7664 - mse: 8220643.0000 - mae: 901.7664 - val_loss: 2288.3865 - val_mse: 47006964.0000 - val_mae: 2288.3865\n",
      "Epoch 488/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 888.4143 - mse: 7890434.5000 - mae: 888.4144 - val_loss: 2280.6541 - val_mse: 46120480.0000 - val_mae: 2280.6541\n",
      "Epoch 489/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 899.9451 - mse: 8270541.0000 - mae: 899.9450 - val_loss: 2295.5437 - val_mse: 46054688.0000 - val_mae: 2295.5437\n",
      "Epoch 490/500\n",
      "5500/5500 [==============================] - 0s 46us/step - loss: 883.9239 - mse: 7716444.5000 - mae: 883.9238 - val_loss: 2282.5554 - val_mse: 46400528.0000 - val_mae: 2282.5554\n",
      "Epoch 491/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 871.6168 - mse: 7616249.0000 - mae: 871.6168 - val_loss: 2286.9138 - val_mse: 45864248.0000 - val_mae: 2286.9138\n",
      "Epoch 492/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 891.8318 - mse: 7844457.5000 - mae: 891.8318 - val_loss: 2284.9641 - val_mse: 45852896.0000 - val_mae: 2284.9641\n",
      "Epoch 493/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 868.7662 - mse: 7740446.5000 - mae: 868.7663 - val_loss: 2292.3875 - val_mse: 46352016.0000 - val_mae: 2292.3875\n",
      "Epoch 494/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 894.6203 - mse: 8036682.5000 - mae: 894.6203 - val_loss: 2279.9236 - val_mse: 45385068.0000 - val_mae: 2279.9236\n",
      "Epoch 495/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 885.6899 - mse: 8098569.5000 - mae: 885.6898 - val_loss: 2304.8203 - val_mse: 46598244.0000 - val_mae: 2304.8203\n",
      "Epoch 496/500\n",
      "5500/5500 [==============================] - 0s 43us/step - loss: 890.3917 - mse: 8029532.0000 - mae: 890.3917 - val_loss: 2290.8433 - val_mse: 46344332.0000 - val_mae: 2290.8433\n",
      "Epoch 497/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 905.5281 - mse: 8382787.0000 - mae: 905.5281 - val_loss: 2286.3887 - val_mse: 46629576.0000 - val_mae: 2286.3887\n",
      "Epoch 498/500\n",
      "5500/5500 [==============================] - 0s 46us/step - loss: 905.0768 - mse: 8305595.5000 - mae: 905.0768 - val_loss: 2292.9631 - val_mse: 46384224.0000 - val_mae: 2292.9631\n",
      "Epoch 499/500\n",
      "5500/5500 [==============================] - 0s 44us/step - loss: 883.5202 - mse: 8135853.5000 - mae: 883.5202 - val_loss: 2275.0776 - val_mse: 45738896.0000 - val_mae: 2275.0776\n",
      "Epoch 500/500\n",
      "5500/5500 [==============================] - 0s 41us/step - loss: 901.3592 - mse: 8155392.0000 - mae: 901.3593 - val_loss: 2283.4314 - val_mse: 46570288.0000 - val_mae: 2283.4314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 500 samples\n",
      "Epoch 1/500\n",
      "7000/7000 [==============================] - 3s 359us/step - loss: 2171.9446 - mse: 60376652.0000 - mae: 2171.9446 - val_loss: 3914.0647 - val_mse: 164007792.0000 - val_mae: 3914.0647\n",
      "Epoch 2/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2156.8575 - mse: 60327068.0000 - mae: 2156.8577 - val_loss: 3883.1963 - val_mse: 163934144.0000 - val_mae: 3883.1963\n",
      "Epoch 3/500\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2145.2345 - mse: 60261020.0000 - mae: 2145.2344 - val_loss: 3864.1431 - val_mse: 163875360.0000 - val_mae: 3864.1431\n",
      "Epoch 4/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 2141.5334 - mse: 60210696.0000 - mae: 2141.5337 - val_loss: 3856.0630 - val_mse: 163839952.0000 - val_mae: 3856.0630\n",
      "Epoch 5/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2140.7519 - mse: 60187748.0000 - mae: 2140.7520 - val_loss: 3851.6667 - val_mse: 163812432.0000 - val_mae: 3851.6667\n",
      "Epoch 6/500\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2139.6016 - mse: 60161628.0000 - mae: 2139.6016 - val_loss: 3850.7185 - val_mse: 163798048.0000 - val_mae: 3850.7185\n",
      "Epoch 7/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 2138.7478 - mse: 60151500.0000 - mae: 2138.7478 - val_loss: 3849.3032 - val_mse: 163780800.0000 - val_mae: 3849.3032\n",
      "Epoch 8/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2138.5130 - mse: 60141420.0000 - mae: 2138.5132 - val_loss: 3848.4197 - val_mse: 163765824.0000 - val_mae: 3848.4197\n",
      "Epoch 9/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2138.1889 - mse: 60135852.0000 - mae: 2138.1890 - val_loss: 3847.2576 - val_mse: 163749184.0000 - val_mae: 3847.2576\n",
      "Epoch 10/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2137.2041 - mse: 60116404.0000 - mae: 2137.2041 - val_loss: 3846.8274 - val_mse: 163736544.0000 - val_mae: 3846.8274\n",
      "Epoch 11/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 2136.1932 - mse: 60102392.0000 - mae: 2136.1929 - val_loss: 3845.8252 - val_mse: 163720784.0000 - val_mae: 3845.8252\n",
      "Epoch 12/500\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2136.4098 - mse: 60086888.0000 - mae: 2136.4099 - val_loss: 3845.4680 - val_mse: 163708832.0000 - val_mae: 3845.4680\n",
      "Epoch 13/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2135.3976 - mse: 60083024.0000 - mae: 2135.3977 - val_loss: 3844.4429 - val_mse: 163692416.0000 - val_mae: 3844.4429\n",
      "Epoch 14/500\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2134.9393 - mse: 60070240.0000 - mae: 2134.9392 - val_loss: 3843.7271 - val_mse: 163677152.0000 - val_mae: 3843.7271\n",
      "Epoch 15/500\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2134.3694 - mse: 60052528.0000 - mae: 2134.3694 - val_loss: 3843.5967 - val_mse: 163665600.0000 - val_mae: 3843.5967\n",
      "Epoch 16/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 2133.7434 - mse: 60035812.0000 - mae: 2133.7437 - val_loss: 3842.5249 - val_mse: 163648016.0000 - val_mae: 3842.5249\n",
      "Epoch 17/500\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 2132.6819 - mse: 60021048.0000 - mae: 2132.6819 - val_loss: 3841.8545 - val_mse: 163632960.0000 - val_mae: 3841.8545\n",
      "Epoch 18/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2131.4299 - mse: 59999324.0000 - mae: 2131.4297 - val_loss: 3840.5579 - val_mse: 163612720.0000 - val_mae: 3840.5579\n",
      "Epoch 19/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 2130.9654 - mse: 59966460.0000 - mae: 2130.9653 - val_loss: 3840.1309 - val_mse: 163599344.0000 - val_mae: 3840.1309\n",
      "Epoch 20/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 2128.8921 - mse: 59932276.0000 - mae: 2128.8923 - val_loss: 3838.9685 - val_mse: 163577632.0000 - val_mae: 3838.9685\n",
      "Epoch 21/500\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2128.3088 - mse: 59890944.0000 - mae: 2128.3086 - val_loss: 3837.4854 - val_mse: 163519056.0000 - val_mae: 3837.4854\n",
      "Epoch 22/500\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2125.2125 - mse: 59842484.0000 - mae: 2125.2126 - val_loss: 3834.4473 - val_mse: 163368592.0000 - val_mae: 3834.4473\n",
      "Epoch 23/500\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2125.2301 - mse: 59795912.0000 - mae: 2125.2302 - val_loss: 3832.4641 - val_mse: 163215408.0000 - val_mae: 3832.4641\n",
      "Epoch 24/500\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 2123.5981 - mse: 59755456.0000 - mae: 2123.5979 - val_loss: 3829.3162 - val_mse: 163052832.0000 - val_mae: 3829.3162\n",
      "Epoch 25/500\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2122.3988 - mse: 59736520.0000 - mae: 2122.3987 - val_loss: 3826.0413 - val_mse: 162916944.0000 - val_mae: 3826.0413\n",
      "Epoch 26/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2121.0761 - mse: 59667748.0000 - mae: 2121.0762 - val_loss: 3823.4836 - val_mse: 162823200.0000 - val_mae: 3823.4836\n",
      "Epoch 27/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 2120.8805 - mse: 59629756.0000 - mae: 2120.8804 - val_loss: 3821.4124 - val_mse: 162744800.0000 - val_mae: 3821.4124\n",
      "Epoch 28/500\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2118.8742 - mse: 59571236.0000 - mae: 2118.8745 - val_loss: 3819.0730 - val_mse: 162662272.0000 - val_mae: 3819.0730\n",
      "Epoch 29/500\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 2117.9812 - mse: 59543932.0000 - mae: 2117.9812 - val_loss: 3816.6025 - val_mse: 162580128.0000 - val_mae: 3816.6025\n",
      "Epoch 30/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2117.4820 - mse: 59529772.0000 - mae: 2117.4822 - val_loss: 3814.8381 - val_mse: 162520400.0000 - val_mae: 3814.8381\n",
      "Epoch 31/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 2115.8876 - mse: 59498688.0000 - mae: 2115.8875 - val_loss: 3812.6267 - val_mse: 162449344.0000 - val_mae: 3812.6267\n",
      "Epoch 32/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2114.9741 - mse: 59484304.0000 - mae: 2114.9741 - val_loss: 3810.6299 - val_mse: 162389600.0000 - val_mae: 3810.6299\n",
      "Epoch 33/500\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2114.4548 - mse: 59458808.0000 - mae: 2114.4548 - val_loss: 3809.2397 - val_mse: 162350672.0000 - val_mae: 3809.2397\n",
      "Epoch 34/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2113.8555 - mse: 59464736.0000 - mae: 2113.8552 - val_loss: 3807.9453 - val_mse: 162315904.0000 - val_mae: 3807.9453\n",
      "Epoch 35/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 2112.2486 - mse: 59431696.0000 - mae: 2112.2488 - val_loss: 3806.9111 - val_mse: 162288208.0000 - val_mae: 3806.9111\n",
      "Epoch 36/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2113.4223 - mse: 59415772.0000 - mae: 2113.4224 - val_loss: 3806.7065 - val_mse: 162286848.0000 - val_mae: 3806.7065\n",
      "Epoch 37/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 2112.3845 - mse: 59424088.0000 - mae: 2112.3845 - val_loss: 3805.6890 - val_mse: 162256656.0000 - val_mae: 3805.6890\n",
      "Epoch 38/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 2111.7154 - mse: 59407976.0000 - mae: 2111.7153 - val_loss: 3804.3093 - val_mse: 162213936.0000 - val_mae: 3804.3093\n",
      "Epoch 39/500\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 2110.8937 - mse: 59379696.0000 - mae: 2110.8938 - val_loss: 3803.2278 - val_mse: 162180496.0000 - val_mae: 3803.2278\n",
      "Epoch 40/500\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 2111.2419 - mse: 59374512.0000 - mae: 2111.2419 - val_loss: 3803.5220 - val_mse: 162193824.0000 - val_mae: 3803.5220\n",
      "Epoch 41/500\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 2110.9359 - mse: 59386388.0000 - mae: 2110.9360 - val_loss: 3803.4231 - val_mse: 162192576.0000 - val_mae: 3803.4231\n",
      "Epoch 42/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 2109.4613 - mse: 59354524.0000 - mae: 2109.4612 - val_loss: 3803.1096 - val_mse: 162183776.0000 - val_mae: 3803.1096\n",
      "Epoch 43/500\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2110.5735 - mse: 59393676.0000 - mae: 2110.5732 - val_loss: 3802.6155 - val_mse: 162168880.0000 - val_mae: 3802.6155\n",
      "Epoch 44/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2111.0193 - mse: 59376524.0000 - mae: 2111.0193 - val_loss: 3802.3999 - val_mse: 162162960.0000 - val_mae: 3802.3999\n",
      "Epoch 45/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2109.8981 - mse: 59362396.0000 - mae: 2109.8982 - val_loss: 3802.2349 - val_mse: 162158144.0000 - val_mae: 3802.2349\n",
      "Epoch 46/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 2110.3695 - mse: 59376588.0000 - mae: 2110.3694 - val_loss: 3801.7014 - val_mse: 162140752.0000 - val_mae: 3801.7014\n",
      "Epoch 47/500\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 2109.9309 - mse: 59367104.0000 - mae: 2109.9309 - val_loss: 3801.3259 - val_mse: 162127456.0000 - val_mae: 3801.3259\n",
      "Epoch 48/500\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 2109.5137 - mse: 59360972.0000 - mae: 2109.5137 - val_loss: 3801.2930 - val_mse: 162124768.0000 - val_mae: 3801.2930\n",
      "Epoch 49/500\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 2108.6225 - mse: 59338184.0000 - mae: 2108.6223 - val_loss: 3800.4456 - val_mse: 162095056.0000 - val_mae: 3800.4456\n",
      "Epoch 50/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2108.6260 - mse: 59332840.0000 - mae: 2108.6262 - val_loss: 3800.1101 - val_mse: 162082624.0000 - val_mae: 3800.1101\n",
      "Epoch 51/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 2107.0568 - mse: 59328192.0000 - mae: 2107.0569 - val_loss: 3799.5994 - val_mse: 162064464.0000 - val_mae: 3799.5994\n",
      "Epoch 52/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2107.4308 - mse: 59322944.0000 - mae: 2107.4309 - val_loss: 3798.6440 - val_mse: 162030368.0000 - val_mae: 3798.6440\n",
      "Epoch 53/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 2107.7598 - mse: 59306392.0000 - mae: 2107.7598 - val_loss: 3798.9241 - val_mse: 162039008.0000 - val_mae: 3798.9241\n",
      "Epoch 54/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 2106.4408 - mse: 59304808.0000 - mae: 2106.4407 - val_loss: 3798.0579 - val_mse: 162008176.0000 - val_mae: 3798.0579\n",
      "Epoch 55/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2106.4116 - mse: 59300436.0000 - mae: 2106.4114 - val_loss: 3797.4556 - val_mse: 161986576.0000 - val_mae: 3797.4556\n",
      "Epoch 56/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2106.6159 - mse: 59291004.0000 - mae: 2106.6160 - val_loss: 3797.7178 - val_mse: 161994704.0000 - val_mae: 3797.7178\n",
      "Epoch 57/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 2106.3253 - mse: 59281900.0000 - mae: 2106.3252 - val_loss: 3797.3606 - val_mse: 161981168.0000 - val_mae: 3797.3606\n",
      "Epoch 58/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2106.8049 - mse: 59293016.0000 - mae: 2106.8049 - val_loss: 3797.6902 - val_mse: 161991392.0000 - val_mae: 3797.6902\n",
      "Epoch 59/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2106.1646 - mse: 59288664.0000 - mae: 2106.1646 - val_loss: 3797.5842 - val_mse: 161985872.0000 - val_mae: 3797.5842\n",
      "Epoch 60/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2106.1808 - mse: 59292932.0000 - mae: 2106.1809 - val_loss: 3798.1240 - val_mse: 162001088.0000 - val_mae: 3798.1240\n",
      "Epoch 61/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2104.6777 - mse: 59287048.0000 - mae: 2104.6780 - val_loss: 3797.1357 - val_mse: 161964400.0000 - val_mae: 3797.1357\n",
      "Epoch 62/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 2104.7244 - mse: 59287652.0000 - mae: 2104.7244 - val_loss: 3796.3843 - val_mse: 161936144.0000 - val_mae: 3796.3843\n",
      "Epoch 63/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2105.4178 - mse: 59253224.0000 - mae: 2105.4180 - val_loss: 3796.2483 - val_mse: 161929808.0000 - val_mae: 3796.2483\n",
      "Epoch 64/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 2104.5574 - mse: 59266364.0000 - mae: 2104.5576 - val_loss: 3795.9482 - val_mse: 161917024.0000 - val_mae: 3795.9482\n",
      "Epoch 65/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2103.4302 - mse: 59238820.0000 - mae: 2103.4299 - val_loss: 3794.8364 - val_mse: 161875680.0000 - val_mae: 3794.8364\n",
      "Epoch 66/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 2101.6238 - mse: 59213044.0000 - mae: 2101.6238 - val_loss: 3794.0979 - val_mse: 161843120.0000 - val_mae: 3794.0979\n",
      "Epoch 67/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2101.4733 - mse: 59215620.0000 - mae: 2101.4734 - val_loss: 3792.8379 - val_mse: 161791024.0000 - val_mae: 3792.8379\n",
      "Epoch 68/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2099.2115 - mse: 59181920.0000 - mae: 2099.2114 - val_loss: 3790.8225 - val_mse: 161715776.0000 - val_mae: 3790.8225\n",
      "Epoch 69/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2097.6755 - mse: 59167244.0000 - mae: 2097.6758 - val_loss: 3788.6733 - val_mse: 161634352.0000 - val_mae: 3788.6733\n",
      "Epoch 70/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 2096.1273 - mse: 59130708.0000 - mae: 2096.1274 - val_loss: 3786.9919 - val_mse: 161571664.0000 - val_mae: 3786.9919\n",
      "Epoch 71/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 2094.3612 - mse: 59095888.0000 - mae: 2094.3611 - val_loss: 3785.0896 - val_mse: 161496800.0000 - val_mae: 3785.0896\n",
      "Epoch 72/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 2092.4621 - mse: 59045672.0000 - mae: 2092.4622 - val_loss: 3782.3799 - val_mse: 161397776.0000 - val_mae: 3782.3799\n",
      "Epoch 73/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2089.6150 - mse: 59015584.0000 - mae: 2089.6150 - val_loss: 3779.7168 - val_mse: 161302624.0000 - val_mae: 3779.7168\n",
      "Epoch 74/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 2077.6005 - mse: 58967892.0000 - mae: 2077.6008 - val_loss: 3794.5449 - val_mse: 161035984.0000 - val_mae: 3794.5449\n",
      "Epoch 75/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2058.4452 - mse: 58761952.0000 - mae: 2058.4451 - val_loss: 3769.1538 - val_mse: 160457584.0000 - val_mae: 3769.1538\n",
      "Epoch 76/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2046.1684 - mse: 58514020.0000 - mae: 2046.1683 - val_loss: 3748.3877 - val_mse: 159935184.0000 - val_mae: 3748.3877\n",
      "Epoch 77/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 2035.9940 - mse: 58224556.0000 - mae: 2035.9940 - val_loss: 3719.4954 - val_mse: 159393968.0000 - val_mae: 3719.4954\n",
      "Epoch 78/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 2029.6907 - mse: 58050308.0000 - mae: 2029.6907 - val_loss: 3712.7451 - val_mse: 158970432.0000 - val_mae: 3712.7451\n",
      "Epoch 79/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 2022.4572 - mse: 57823324.0000 - mae: 2022.4570 - val_loss: 3717.4539 - val_mse: 158594960.0000 - val_mae: 3717.4539\n",
      "Epoch 80/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2016.2755 - mse: 57621832.0000 - mae: 2016.2755 - val_loss: 3716.7751 - val_mse: 158197536.0000 - val_mae: 3716.7751\n",
      "Epoch 81/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2013.4213 - mse: 57459248.0000 - mae: 2013.4211 - val_loss: 3703.9805 - val_mse: 157891264.0000 - val_mae: 3703.9805\n",
      "Epoch 82/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 2011.1063 - mse: 57387460.0000 - mae: 2011.1063 - val_loss: 3722.0117 - val_mse: 157507056.0000 - val_mae: 3722.0117\n",
      "Epoch 83/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 2005.0191 - mse: 57176912.0000 - mae: 2005.0188 - val_loss: 3697.5784 - val_mse: 157183408.0000 - val_mae: 3697.5784\n",
      "Epoch 84/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2001.4646 - mse: 57011132.0000 - mae: 2001.4646 - val_loss: 3697.1946 - val_mse: 156797600.0000 - val_mae: 3697.1946\n",
      "Epoch 85/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1999.5934 - mse: 56787220.0000 - mae: 1999.5934 - val_loss: 3693.3884 - val_mse: 156457312.0000 - val_mae: 3693.3884\n",
      "Epoch 86/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1998.1621 - mse: 56720304.0000 - mae: 1998.1621 - val_loss: 3695.0056 - val_mse: 156133696.0000 - val_mae: 3695.0056\n",
      "Epoch 87/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1991.0906 - mse: 56610016.0000 - mae: 1991.0906 - val_loss: 3671.1191 - val_mse: 155802112.0000 - val_mae: 3671.1191\n",
      "Epoch 88/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1993.0796 - mse: 56498756.0000 - mae: 1993.0797 - val_loss: 3662.8611 - val_mse: 155566720.0000 - val_mae: 3662.8611\n",
      "Epoch 89/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1985.7755 - mse: 56293488.0000 - mae: 1985.7754 - val_loss: 3653.9578 - val_mse: 155294192.0000 - val_mae: 3653.9578\n",
      "Epoch 90/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1984.9561 - mse: 56170408.0000 - mae: 1984.9562 - val_loss: 3635.3130 - val_mse: 154970576.0000 - val_mae: 3635.3130\n",
      "Epoch 91/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1981.3192 - mse: 56010632.0000 - mae: 1981.3191 - val_loss: 3630.9705 - val_mse: 154649280.0000 - val_mae: 3630.9705\n",
      "Epoch 92/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1980.2788 - mse: 55802560.0000 - mae: 1980.2788 - val_loss: 3638.1052 - val_mse: 154329504.0000 - val_mae: 3638.1052\n",
      "Epoch 93/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1981.2290 - mse: 55808424.0000 - mae: 1981.2290 - val_loss: 3627.3708 - val_mse: 154069232.0000 - val_mae: 3627.3708\n",
      "Epoch 94/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1975.2933 - mse: 55668648.0000 - mae: 1975.2931 - val_loss: 3627.9587 - val_mse: 153711344.0000 - val_mae: 3627.9587\n",
      "Epoch 95/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1972.0596 - mse: 55512844.0000 - mae: 1972.0594 - val_loss: 3611.5544 - val_mse: 153378032.0000 - val_mae: 3611.5544\n",
      "Epoch 96/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1965.5308 - mse: 55326984.0000 - mae: 1965.5309 - val_loss: 3605.7771 - val_mse: 153063216.0000 - val_mae: 3605.7771\n",
      "Epoch 97/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1968.0464 - mse: 55137136.0000 - mae: 1968.0466 - val_loss: 3606.5667 - val_mse: 152713760.0000 - val_mae: 3606.5667\n",
      "Epoch 98/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1961.4219 - mse: 55040896.0000 - mae: 1961.4220 - val_loss: 3598.9556 - val_mse: 152329680.0000 - val_mae: 3598.9556\n",
      "Epoch 99/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1959.8732 - mse: 54787140.0000 - mae: 1959.8730 - val_loss: 3596.4617 - val_mse: 151909072.0000 - val_mae: 3596.4617\n",
      "Epoch 100/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1954.3627 - mse: 54671484.0000 - mae: 1954.3625 - val_loss: 3585.9182 - val_mse: 151455328.0000 - val_mae: 3585.9182\n",
      "Epoch 101/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1952.7558 - mse: 54502772.0000 - mae: 1952.7559 - val_loss: 3579.6934 - val_mse: 151018352.0000 - val_mae: 3579.6934\n",
      "Epoch 102/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1949.6748 - mse: 54205364.0000 - mae: 1949.6747 - val_loss: 3571.6875 - val_mse: 150549600.0000 - val_mae: 3571.6875\n",
      "Epoch 103/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1943.5568 - mse: 54053092.0000 - mae: 1943.5569 - val_loss: 3566.9915 - val_mse: 150064432.0000 - val_mae: 3566.9915\n",
      "Epoch 104/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1939.9371 - mse: 53935424.0000 - mae: 1939.9371 - val_loss: 3568.3433 - val_mse: 149635008.0000 - val_mae: 3568.3433\n",
      "Epoch 105/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1934.0938 - mse: 53626616.0000 - mae: 1934.0938 - val_loss: 3563.3413 - val_mse: 149155616.0000 - val_mae: 3563.3413\n",
      "Epoch 106/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1934.1471 - mse: 53580164.0000 - mae: 1934.1471 - val_loss: 3556.3159 - val_mse: 148688064.0000 - val_mae: 3556.3159\n",
      "Epoch 107/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1926.6617 - mse: 52999784.0000 - mae: 1926.6617 - val_loss: 3545.7563 - val_mse: 148215536.0000 - val_mae: 3545.7563\n",
      "Epoch 108/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1930.5286 - mse: 53314928.0000 - mae: 1930.5283 - val_loss: 3545.1855 - val_mse: 147785568.0000 - val_mae: 3545.1855\n",
      "Epoch 109/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1922.8374 - mse: 52973872.0000 - mae: 1922.8374 - val_loss: 3538.5452 - val_mse: 147269040.0000 - val_mae: 3538.5452\n",
      "Epoch 110/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1920.0289 - mse: 52681696.0000 - mae: 1920.0291 - val_loss: 3529.4871 - val_mse: 146805056.0000 - val_mae: 3529.4871\n",
      "Epoch 111/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1913.4963 - mse: 52351224.0000 - mae: 1913.4963 - val_loss: 3522.6211 - val_mse: 146322912.0000 - val_mae: 3522.6211\n",
      "Epoch 112/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1913.8118 - mse: 52423848.0000 - mae: 1913.8118 - val_loss: 3515.3057 - val_mse: 145828720.0000 - val_mae: 3515.3057\n",
      "Epoch 113/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1905.9246 - mse: 52162980.0000 - mae: 1905.9246 - val_loss: 3513.7278 - val_mse: 145406448.0000 - val_mae: 3513.7278\n",
      "Epoch 114/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1902.2008 - mse: 51890812.0000 - mae: 1902.2010 - val_loss: 3506.3335 - val_mse: 144809504.0000 - val_mae: 3506.3335\n",
      "Epoch 115/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1898.4206 - mse: 51692988.0000 - mae: 1898.4204 - val_loss: 3504.2397 - val_mse: 144399888.0000 - val_mae: 3504.2397\n",
      "Epoch 116/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1894.9613 - mse: 51351312.0000 - mae: 1894.9615 - val_loss: 3494.2993 - val_mse: 143845408.0000 - val_mae: 3494.2993\n",
      "Epoch 117/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1888.2208 - mse: 51375224.0000 - mae: 1888.2207 - val_loss: 3486.2236 - val_mse: 143237392.0000 - val_mae: 3486.2236\n",
      "Epoch 118/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1887.4541 - mse: 51334180.0000 - mae: 1887.4541 - val_loss: 3482.7488 - val_mse: 142738192.0000 - val_mae: 3482.7488\n",
      "Epoch 119/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1883.2615 - mse: 50945820.0000 - mae: 1883.2615 - val_loss: 3479.9343 - val_mse: 142192672.0000 - val_mae: 3479.9343\n",
      "Epoch 120/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1872.5525 - mse: 50529680.0000 - mae: 1872.5526 - val_loss: 3465.4785 - val_mse: 141420656.0000 - val_mae: 3465.4785\n",
      "Epoch 121/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1871.7507 - mse: 50467288.0000 - mae: 1871.7509 - val_loss: 3456.7676 - val_mse: 140755328.0000 - val_mae: 3456.7676\n",
      "Epoch 122/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1863.7778 - mse: 50051068.0000 - mae: 1863.7777 - val_loss: 3473.1909 - val_mse: 140049264.0000 - val_mae: 3473.1909\n",
      "Epoch 123/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1857.9054 - mse: 49922344.0000 - mae: 1857.9055 - val_loss: 3452.9084 - val_mse: 139192768.0000 - val_mae: 3452.9084\n",
      "Epoch 124/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1837.4305 - mse: 49288948.0000 - mae: 1837.4303 - val_loss: 3438.2554 - val_mse: 138197888.0000 - val_mae: 3438.2554\n",
      "Epoch 125/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1837.3285 - mse: 48947108.0000 - mae: 1837.3286 - val_loss: 3427.4719 - val_mse: 137240656.0000 - val_mae: 3427.4719\n",
      "Epoch 126/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1823.8787 - mse: 48536124.0000 - mae: 1823.8787 - val_loss: 3407.2983 - val_mse: 136030752.0000 - val_mae: 3407.2983\n",
      "Epoch 127/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1816.2941 - mse: 48336076.0000 - mae: 1816.2939 - val_loss: 3398.3120 - val_mse: 134845552.0000 - val_mae: 3398.3120\n",
      "Epoch 128/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1801.8280 - mse: 47681252.0000 - mae: 1801.8279 - val_loss: 3378.5815 - val_mse: 133671416.0000 - val_mae: 3378.5815\n",
      "Epoch 129/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1798.9276 - mse: 47497404.0000 - mae: 1798.9276 - val_loss: 3367.6960 - val_mse: 132555824.0000 - val_mae: 3367.6960\n",
      "Epoch 130/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1790.2946 - mse: 46853180.0000 - mae: 1790.2946 - val_loss: 3364.1448 - val_mse: 131551176.0000 - val_mae: 3364.1448\n",
      "Epoch 131/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1785.7041 - mse: 46512736.0000 - mae: 1785.7040 - val_loss: 3338.5378 - val_mse: 130368840.0000 - val_mae: 3338.5378\n",
      "Epoch 132/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1772.6141 - mse: 46214704.0000 - mae: 1772.6143 - val_loss: 3345.4224 - val_mse: 129191312.0000 - val_mae: 3345.4224\n",
      "Epoch 133/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1769.2763 - mse: 45659784.0000 - mae: 1769.2762 - val_loss: 3317.9712 - val_mse: 128039400.0000 - val_mae: 3317.9712\n",
      "Epoch 134/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1761.2590 - mse: 45045656.0000 - mae: 1761.2590 - val_loss: 3308.2434 - val_mse: 126793640.0000 - val_mae: 3308.2434\n",
      "Epoch 135/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1749.4073 - mse: 44668088.0000 - mae: 1749.4072 - val_loss: 3301.6292 - val_mse: 125839448.0000 - val_mae: 3301.6292\n",
      "Epoch 136/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1744.4552 - mse: 44556068.0000 - mae: 1744.4552 - val_loss: 3285.6018 - val_mse: 124684984.0000 - val_mae: 3285.6018\n",
      "Epoch 137/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1736.7800 - mse: 43850336.0000 - mae: 1736.7802 - val_loss: 3272.9375 - val_mse: 123517928.0000 - val_mae: 3272.9375\n",
      "Epoch 138/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1728.5641 - mse: 43458468.0000 - mae: 1728.5643 - val_loss: 3273.4221 - val_mse: 122501304.0000 - val_mae: 3273.4221\n",
      "Epoch 139/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1717.6448 - mse: 42775552.0000 - mae: 1717.6445 - val_loss: 3261.1096 - val_mse: 121227440.0000 - val_mae: 3261.1096\n",
      "Epoch 140/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1714.5210 - mse: 42992232.0000 - mae: 1714.5210 - val_loss: 3242.8264 - val_mse: 120073064.0000 - val_mae: 3242.8264\n",
      "Epoch 141/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1699.9551 - mse: 41886720.0000 - mae: 1699.9552 - val_loss: 3246.9814 - val_mse: 118863552.0000 - val_mae: 3246.9814\n",
      "Epoch 142/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1692.2285 - mse: 41581172.0000 - mae: 1692.2284 - val_loss: 3253.0083 - val_mse: 117731224.0000 - val_mae: 3253.0083\n",
      "Epoch 143/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1689.0210 - mse: 41600428.0000 - mae: 1689.0209 - val_loss: 3215.8945 - val_mse: 116221984.0000 - val_mae: 3215.8945\n",
      "Epoch 144/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1676.0954 - mse: 40553544.0000 - mae: 1676.0955 - val_loss: 3196.8896 - val_mse: 115040112.0000 - val_mae: 3196.8896\n",
      "Epoch 145/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1663.9637 - mse: 39882392.0000 - mae: 1663.9637 - val_loss: 3202.2478 - val_mse: 113748624.0000 - val_mae: 3202.2478\n",
      "Epoch 146/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1662.4231 - mse: 39884108.0000 - mae: 1662.4231 - val_loss: 3183.5522 - val_mse: 112762312.0000 - val_mae: 3183.5522\n",
      "Epoch 147/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1654.4432 - mse: 39433448.0000 - mae: 1654.4431 - val_loss: 3183.2839 - val_mse: 111265832.0000 - val_mae: 3183.2839\n",
      "Epoch 148/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1632.0995 - mse: 37855204.0000 - mae: 1632.0996 - val_loss: 3154.1589 - val_mse: 109843216.0000 - val_mae: 3154.1589\n",
      "Epoch 149/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1633.4745 - mse: 38313056.0000 - mae: 1633.4746 - val_loss: 3158.7932 - val_mse: 108629800.0000 - val_mae: 3158.7932\n",
      "Epoch 150/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1612.7518 - mse: 36899588.0000 - mae: 1612.7517 - val_loss: 3144.1704 - val_mse: 107123776.0000 - val_mae: 3144.1704\n",
      "Epoch 151/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1601.2773 - mse: 36468260.0000 - mae: 1601.2773 - val_loss: 3113.4421 - val_mse: 105212288.0000 - val_mae: 3113.4421\n",
      "Epoch 152/500\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1603.1654 - mse: 36645624.0000 - mae: 1603.1654 - val_loss: 3085.4868 - val_mse: 103620168.0000 - val_mae: 3085.4868\n",
      "Epoch 153/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1599.4447 - mse: 36299388.0000 - mae: 1599.4447 - val_loss: 3101.3896 - val_mse: 102270608.0000 - val_mae: 3101.3896\n",
      "Epoch 154/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1578.6477 - mse: 35655820.0000 - mae: 1578.6477 - val_loss: 3055.1519 - val_mse: 100845680.0000 - val_mae: 3055.1519\n",
      "Epoch 155/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1568.9820 - mse: 34639248.0000 - mae: 1568.9821 - val_loss: 3038.7681 - val_mse: 99220824.0000 - val_mae: 3038.7681\n",
      "Epoch 156/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1557.1347 - mse: 33762068.0000 - mae: 1557.1349 - val_loss: 3017.4299 - val_mse: 97797960.0000 - val_mae: 3017.4299\n",
      "Epoch 157/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1546.1565 - mse: 33425944.0000 - mae: 1546.1564 - val_loss: 2983.2268 - val_mse: 95279368.0000 - val_mae: 2983.2268\n",
      "Epoch 158/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1540.0400 - mse: 33165238.0000 - mae: 1540.0400 - val_loss: 2977.2251 - val_mse: 93967688.0000 - val_mae: 2977.2251\n",
      "Epoch 159/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1527.7065 - mse: 32101260.0000 - mae: 1527.7065 - val_loss: 2931.2703 - val_mse: 91441008.0000 - val_mae: 2931.2703\n",
      "Epoch 160/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1521.0081 - mse: 31835088.0000 - mae: 1521.0082 - val_loss: 2930.4705 - val_mse: 90565824.0000 - val_mae: 2930.4705\n",
      "Epoch 161/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1524.0818 - mse: 32073970.0000 - mae: 1524.0817 - val_loss: 2889.9392 - val_mse: 88108912.0000 - val_mae: 2889.9392\n",
      "Epoch 162/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1496.5185 - mse: 30783840.0000 - mae: 1496.5184 - val_loss: 2876.5337 - val_mse: 86208120.0000 - val_mae: 2876.5337\n",
      "Epoch 163/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1489.0798 - mse: 30135560.0000 - mae: 1489.0797 - val_loss: 2839.7395 - val_mse: 84440912.0000 - val_mae: 2839.7395\n",
      "Epoch 164/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1462.9232 - mse: 28566998.0000 - mae: 1462.9233 - val_loss: 2831.9797 - val_mse: 83162784.0000 - val_mae: 2831.9797\n",
      "Epoch 165/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1453.3072 - mse: 28575184.0000 - mae: 1453.3071 - val_loss: 2783.6611 - val_mse: 80334144.0000 - val_mae: 2783.6611\n",
      "Epoch 166/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1462.0388 - mse: 27831366.0000 - mae: 1462.0387 - val_loss: 2763.2764 - val_mse: 78583800.0000 - val_mae: 2763.2764\n",
      "Epoch 167/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1422.3715 - mse: 26824692.0000 - mae: 1422.3717 - val_loss: 2752.1523 - val_mse: 77739016.0000 - val_mae: 2752.1523\n",
      "Epoch 168/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1424.7983 - mse: 27128512.0000 - mae: 1424.7985 - val_loss: 2706.7737 - val_mse: 74407688.0000 - val_mae: 2706.7737\n",
      "Epoch 169/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1415.3629 - mse: 26327148.0000 - mae: 1415.3630 - val_loss: 2690.4575 - val_mse: 72898744.0000 - val_mae: 2690.4575\n",
      "Epoch 170/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1411.9462 - mse: 26382490.0000 - mae: 1411.9462 - val_loss: 2656.3840 - val_mse: 70813928.0000 - val_mae: 2656.3840\n",
      "Epoch 171/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1396.1260 - mse: 25272252.0000 - mae: 1396.1260 - val_loss: 2607.5090 - val_mse: 68454832.0000 - val_mae: 2607.5090\n",
      "Epoch 172/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1380.8387 - mse: 25268942.0000 - mae: 1380.8387 - val_loss: 2614.2595 - val_mse: 68351936.0000 - val_mae: 2614.2595\n",
      "Epoch 173/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1378.1087 - mse: 24364712.0000 - mae: 1378.1089 - val_loss: 2582.7415 - val_mse: 65659712.0000 - val_mae: 2582.7415\n",
      "Epoch 174/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1350.1987 - mse: 23212570.0000 - mae: 1350.1987 - val_loss: 2558.5032 - val_mse: 64458692.0000 - val_mae: 2558.5032\n",
      "Epoch 175/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1347.9995 - mse: 22855378.0000 - mae: 1347.9994 - val_loss: 2566.7402 - val_mse: 64609824.0000 - val_mae: 2566.7402\n",
      "Epoch 176/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1316.9952 - mse: 22244370.0000 - mae: 1316.9952 - val_loss: 2495.5662 - val_mse: 60807336.0000 - val_mae: 2495.5662\n",
      "Epoch 177/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1320.5035 - mse: 21370786.0000 - mae: 1320.5034 - val_loss: 2466.4116 - val_mse: 58654060.0000 - val_mae: 2466.4116\n",
      "Epoch 178/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1304.0322 - mse: 20968644.0000 - mae: 1304.0322 - val_loss: 2449.6130 - val_mse: 57978968.0000 - val_mae: 2449.6130\n",
      "Epoch 179/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1307.1920 - mse: 21013096.0000 - mae: 1307.1920 - val_loss: 2403.9163 - val_mse: 55276728.0000 - val_mae: 2403.9163\n",
      "Epoch 180/500\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1277.5948 - mse: 20395000.0000 - mae: 1277.5948 - val_loss: 2384.5210 - val_mse: 54312804.0000 - val_mae: 2384.5210\n",
      "Epoch 181/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1274.2543 - mse: 20027532.0000 - mae: 1274.2542 - val_loss: 2327.6040 - val_mse: 51175936.0000 - val_mae: 2327.6040\n",
      "Epoch 182/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1252.0288 - mse: 19042532.0000 - mae: 1252.0287 - val_loss: 2383.4973 - val_mse: 53820184.0000 - val_mae: 2383.4973\n",
      "Epoch 183/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1260.7562 - mse: 19329902.0000 - mae: 1260.7561 - val_loss: 2296.2822 - val_mse: 49437472.0000 - val_mae: 2296.2822\n",
      "Epoch 184/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1258.9209 - mse: 19679642.0000 - mae: 1258.9209 - val_loss: 2268.6333 - val_mse: 47597300.0000 - val_mae: 2268.6333\n",
      "Epoch 185/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1228.8025 - mse: 17962766.0000 - mae: 1228.8024 - val_loss: 2247.7500 - val_mse: 46696312.0000 - val_mae: 2247.7500\n",
      "Epoch 186/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1239.0786 - mse: 18668822.0000 - mae: 1239.0786 - val_loss: 2229.8306 - val_mse: 45982760.0000 - val_mae: 2229.8306\n",
      "Epoch 187/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1225.3748 - mse: 18094082.0000 - mae: 1225.3748 - val_loss: 2162.9841 - val_mse: 42799440.0000 - val_mae: 2162.9841\n",
      "Epoch 188/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1225.8606 - mse: 17791782.0000 - mae: 1225.8607 - val_loss: 2174.7224 - val_mse: 42737716.0000 - val_mae: 2174.7224\n",
      "Epoch 189/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1220.7797 - mse: 17601580.0000 - mae: 1220.7797 - val_loss: 2153.8271 - val_mse: 41808016.0000 - val_mae: 2153.8271\n",
      "Epoch 190/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1211.9031 - mse: 17813556.0000 - mae: 1211.9032 - val_loss: 2122.6616 - val_mse: 40202964.0000 - val_mae: 2122.6616\n",
      "Epoch 191/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1213.7597 - mse: 17022132.0000 - mae: 1213.7598 - val_loss: 2140.5791 - val_mse: 40872300.0000 - val_mae: 2140.5791\n",
      "Epoch 192/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1198.1117 - mse: 16646528.0000 - mae: 1198.1117 - val_loss: 2125.2847 - val_mse: 39840004.0000 - val_mae: 2125.2847\n",
      "Epoch 193/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1214.3781 - mse: 17367836.0000 - mae: 1214.3782 - val_loss: 2090.8210 - val_mse: 38839536.0000 - val_mae: 2090.8210\n",
      "Epoch 194/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1164.5874 - mse: 15648637.0000 - mae: 1164.5874 - val_loss: 2088.8059 - val_mse: 38295032.0000 - val_mae: 2088.8059\n",
      "Epoch 195/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1177.1350 - mse: 16546472.0000 - mae: 1177.1349 - val_loss: 2006.6943 - val_mse: 35469512.0000 - val_mae: 2006.6943\n",
      "Epoch 196/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1186.4312 - mse: 16191755.0000 - mae: 1186.4312 - val_loss: 2050.9155 - val_mse: 36584600.0000 - val_mae: 2050.9155\n",
      "Epoch 197/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1171.3606 - mse: 16102070.0000 - mae: 1171.3606 - val_loss: 1946.5851 - val_mse: 32590514.0000 - val_mae: 1946.5851\n",
      "Epoch 198/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1148.2406 - mse: 15147277.0000 - mae: 1148.2405 - val_loss: 1921.1898 - val_mse: 31831970.0000 - val_mae: 1921.1898\n",
      "Epoch 199/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1158.2787 - mse: 15270743.0000 - mae: 1158.2787 - val_loss: 1956.9122 - val_mse: 32834216.0000 - val_mae: 1956.9122\n",
      "Epoch 200/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1158.8635 - mse: 15287832.0000 - mae: 1158.8635 - val_loss: 1909.7227 - val_mse: 31179668.0000 - val_mae: 1909.7227\n",
      "Epoch 201/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1151.4848 - mse: 15056162.0000 - mae: 1151.4847 - val_loss: 1901.5579 - val_mse: 30712286.0000 - val_mae: 1901.5579\n",
      "Epoch 202/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1132.4965 - mse: 14445561.0000 - mae: 1132.4965 - val_loss: 1852.8397 - val_mse: 29082720.0000 - val_mae: 1852.8397\n",
      "Epoch 203/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1128.4333 - mse: 14179878.0000 - mae: 1128.4332 - val_loss: 1914.0269 - val_mse: 30868846.0000 - val_mae: 1914.0269\n",
      "Epoch 204/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1110.0374 - mse: 13899751.0000 - mae: 1110.0375 - val_loss: 1847.5187 - val_mse: 28564456.0000 - val_mae: 1847.5187\n",
      "Epoch 205/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1129.0995 - mse: 14075389.0000 - mae: 1129.0996 - val_loss: 1798.0316 - val_mse: 26903982.0000 - val_mae: 1798.0316\n",
      "Epoch 206/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1139.4866 - mse: 14294112.0000 - mae: 1139.4866 - val_loss: 1852.5948 - val_mse: 28406246.0000 - val_mae: 1852.5948\n",
      "Epoch 207/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1129.5629 - mse: 14203477.0000 - mae: 1129.5629 - val_loss: 1811.0342 - val_mse: 26955326.0000 - val_mae: 1811.0342\n",
      "Epoch 208/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1110.5520 - mse: 13580984.0000 - mae: 1110.5520 - val_loss: 1797.0753 - val_mse: 26366436.0000 - val_mae: 1797.0753\n",
      "Epoch 209/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1086.5587 - mse: 12948578.0000 - mae: 1086.5588 - val_loss: 1785.1614 - val_mse: 25810834.0000 - val_mae: 1785.1614\n",
      "Epoch 210/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1104.1882 - mse: 13601030.0000 - mae: 1104.1882 - val_loss: 1756.3315 - val_mse: 24871912.0000 - val_mae: 1756.3315\n",
      "Epoch 211/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1102.1180 - mse: 13310710.0000 - mae: 1102.1179 - val_loss: 1730.6971 - val_mse: 24092300.0000 - val_mae: 1730.6971\n",
      "Epoch 212/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1112.9801 - mse: 13250037.0000 - mae: 1112.9802 - val_loss: 1710.2972 - val_mse: 23471602.0000 - val_mae: 1710.2972\n",
      "Epoch 213/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1102.4901 - mse: 13613256.0000 - mae: 1102.4902 - val_loss: 1719.6060 - val_mse: 23684804.0000 - val_mae: 1719.6060\n",
      "Epoch 214/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1113.7995 - mse: 13972605.0000 - mae: 1113.7996 - val_loss: 1683.2317 - val_mse: 22333826.0000 - val_mae: 1683.2317\n",
      "Epoch 215/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1075.1410 - mse: 12586215.0000 - mae: 1075.1411 - val_loss: 1679.4368 - val_mse: 22310446.0000 - val_mae: 1679.4368\n",
      "Epoch 216/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1092.0506 - mse: 13132880.0000 - mae: 1092.0505 - val_loss: 1684.4240 - val_mse: 22496924.0000 - val_mae: 1684.4240\n",
      "Epoch 217/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1086.0271 - mse: 12619692.0000 - mae: 1086.0270 - val_loss: 1658.1844 - val_mse: 21651590.0000 - val_mae: 1658.1844\n",
      "Epoch 218/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1094.9580 - mse: 13194359.0000 - mae: 1094.9580 - val_loss: 1608.5242 - val_mse: 20329842.0000 - val_mae: 1608.5242\n",
      "Epoch 219/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1088.8401 - mse: 12736026.0000 - mae: 1088.8402 - val_loss: 1629.5178 - val_mse: 20827352.0000 - val_mae: 1629.5178\n",
      "Epoch 220/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1077.7075 - mse: 12365733.0000 - mae: 1077.7074 - val_loss: 1622.2362 - val_mse: 20408946.0000 - val_mae: 1622.2362\n",
      "Epoch 221/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1058.7450 - mse: 12066387.0000 - mae: 1058.7449 - val_loss: 1558.9987 - val_mse: 18912038.0000 - val_mae: 1558.9987\n",
      "Epoch 222/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1061.9620 - mse: 11798348.0000 - mae: 1061.9620 - val_loss: 1600.5863 - val_mse: 19621746.0000 - val_mae: 1600.5863\n",
      "Epoch 223/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1054.6288 - mse: 12334133.0000 - mae: 1054.6287 - val_loss: 1622.4330 - val_mse: 20308680.0000 - val_mae: 1622.4330\n",
      "Epoch 224/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1051.8378 - mse: 12034012.0000 - mae: 1051.8378 - val_loss: 1537.5613 - val_mse: 18177266.0000 - val_mae: 1537.5613\n",
      "Epoch 225/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1073.8616 - mse: 12424842.0000 - mae: 1073.8617 - val_loss: 1525.9602 - val_mse: 17900456.0000 - val_mae: 1525.9602\n",
      "Epoch 226/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1052.2035 - mse: 11741456.0000 - mae: 1052.2035 - val_loss: 1528.1089 - val_mse: 18134254.0000 - val_mae: 1528.1089\n",
      "Epoch 227/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1057.0352 - mse: 11505290.0000 - mae: 1057.0353 - val_loss: 1528.3469 - val_mse: 18076144.0000 - val_mae: 1528.3469\n",
      "Epoch 228/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1034.4214 - mse: 10815342.0000 - mae: 1034.4213 - val_loss: 1519.6895 - val_mse: 17666804.0000 - val_mae: 1519.6895\n",
      "Epoch 229/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1028.3115 - mse: 10815763.0000 - mae: 1028.3115 - val_loss: 1527.2269 - val_mse: 17845970.0000 - val_mae: 1527.2269\n",
      "Epoch 230/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1050.0121 - mse: 11418864.0000 - mae: 1050.0121 - val_loss: 1493.9849 - val_mse: 17108820.0000 - val_mae: 1493.9849\n",
      "Epoch 231/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1034.8945 - mse: 10873045.0000 - mae: 1034.8945 - val_loss: 1526.0464 - val_mse: 17907156.0000 - val_mae: 1526.0464\n",
      "Epoch 232/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1060.7525 - mse: 11737478.0000 - mae: 1060.7524 - val_loss: 1469.7179 - val_mse: 16592434.0000 - val_mae: 1469.7179\n",
      "Epoch 233/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1011.3672 - mse: 10221905.0000 - mae: 1011.3673 - val_loss: 1472.4750 - val_mse: 16667969.0000 - val_mae: 1472.4750\n",
      "Epoch 234/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1007.6589 - mse: 10293838.0000 - mae: 1007.6589 - val_loss: 1468.5344 - val_mse: 16542070.0000 - val_mae: 1468.5344\n",
      "Epoch 235/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1021.2810 - mse: 11111690.0000 - mae: 1021.2810 - val_loss: 1477.6003 - val_mse: 16656047.0000 - val_mae: 1477.6003\n",
      "Epoch 236/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1024.5124 - mse: 10457458.0000 - mae: 1024.5123 - val_loss: 1436.6038 - val_mse: 15931942.0000 - val_mae: 1436.6038\n",
      "Epoch 237/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1039.7593 - mse: 11190748.0000 - mae: 1039.7594 - val_loss: 1473.9320 - val_mse: 16683698.0000 - val_mae: 1473.9320\n",
      "Epoch 238/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1042.6574 - mse: 11642825.0000 - mae: 1042.6573 - val_loss: 1476.5077 - val_mse: 16523069.0000 - val_mae: 1476.5077\n",
      "Epoch 239/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1029.2140 - mse: 11161066.0000 - mae: 1029.2140 - val_loss: 1417.8751 - val_mse: 15420824.0000 - val_mae: 1417.8751\n",
      "Epoch 240/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1016.1430 - mse: 10866351.0000 - mae: 1016.1430 - val_loss: 1431.8263 - val_mse: 15651039.0000 - val_mae: 1431.8263\n",
      "Epoch 241/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1029.8915 - mse: 10940775.0000 - mae: 1029.8915 - val_loss: 1383.5248 - val_mse: 14560351.0000 - val_mae: 1383.5248\n",
      "Epoch 242/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1039.3772 - mse: 11216753.0000 - mae: 1039.3772 - val_loss: 1365.1182 - val_mse: 14170748.0000 - val_mae: 1365.1182\n",
      "Epoch 243/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1033.7455 - mse: 10830781.0000 - mae: 1033.7456 - val_loss: 1370.3909 - val_mse: 14192706.0000 - val_mae: 1370.3909\n",
      "Epoch 244/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 1044.5831 - mse: 11047564.0000 - mae: 1044.5831 - val_loss: 1366.0153 - val_mse: 14160803.0000 - val_mae: 1366.0153\n",
      "Epoch 245/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1023.3859 - mse: 10940559.0000 - mae: 1023.3859 - val_loss: 1391.3375 - val_mse: 14608173.0000 - val_mae: 1391.3375\n",
      "Epoch 246/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1016.1680 - mse: 10749926.0000 - mae: 1016.1679 - val_loss: 1390.6144 - val_mse: 14414944.0000 - val_mae: 1390.6144\n",
      "Epoch 247/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1009.5450 - mse: 10227092.0000 - mae: 1009.5449 - val_loss: 1379.6931 - val_mse: 14350664.0000 - val_mae: 1379.6931\n",
      "Epoch 248/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1006.5985 - mse: 10407959.0000 - mae: 1006.5984 - val_loss: 1408.8187 - val_mse: 14921886.0000 - val_mae: 1408.8187\n",
      "Epoch 249/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1025.3849 - mse: 10675654.0000 - mae: 1025.3849 - val_loss: 1342.1005 - val_mse: 13526844.0000 - val_mae: 1342.1005\n",
      "Epoch 250/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1041.3861 - mse: 11170445.0000 - mae: 1041.3861 - val_loss: 1354.3326 - val_mse: 13718673.0000 - val_mae: 1354.3326\n",
      "Epoch 251/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1014.2278 - mse: 10216502.0000 - mae: 1014.2278 - val_loss: 1380.9363 - val_mse: 14431425.0000 - val_mae: 1380.9363\n",
      "Epoch 252/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1003.8333 - mse: 10106685.0000 - mae: 1003.8331 - val_loss: 1343.9652 - val_mse: 13661063.0000 - val_mae: 1343.9652\n",
      "Epoch 253/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 988.3551 - mse: 9591996.0000 - mae: 988.3552 - val_loss: 1314.3199 - val_mse: 13127040.0000 - val_mae: 1314.3199\n",
      "Epoch 254/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1000.4320 - mse: 9794696.0000 - mae: 1000.4321 - val_loss: 1348.8168 - val_mse: 13593117.0000 - val_mae: 1348.8168\n",
      "Epoch 255/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1003.1264 - mse: 10094995.0000 - mae: 1003.1264 - val_loss: 1310.2657 - val_mse: 12932042.0000 - val_mae: 1310.2657\n",
      "Epoch 256/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1026.2819 - mse: 10767468.0000 - mae: 1026.2820 - val_loss: 1306.2151 - val_mse: 12767348.0000 - val_mae: 1306.2151\n",
      "Epoch 257/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 989.7165 - mse: 9540675.0000 - mae: 989.7165 - val_loss: 1317.7633 - val_mse: 12995885.0000 - val_mae: 1317.7633\n",
      "Epoch 258/500\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1011.2906 - mse: 10412905.0000 - mae: 1011.2906 - val_loss: 1306.9806 - val_mse: 12793274.0000 - val_mae: 1306.9806\n",
      "Epoch 259/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1001.7463 - mse: 10098068.0000 - mae: 1001.7463 - val_loss: 1308.8148 - val_mse: 13010498.0000 - val_mae: 1308.8148\n",
      "Epoch 260/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 987.9448 - mse: 9646387.0000 - mae: 987.9449 - val_loss: 1302.7052 - val_mse: 13002056.0000 - val_mae: 1302.7052\n",
      "Epoch 261/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 992.7752 - mse: 9823259.0000 - mae: 992.7752 - val_loss: 1259.7413 - val_mse: 11982649.0000 - val_mae: 1259.7413\n",
      "Epoch 262/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1009.8296 - mse: 10480182.0000 - mae: 1009.8296 - val_loss: 1274.7987 - val_mse: 12251941.0000 - val_mae: 1274.7987\n",
      "Epoch 263/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 984.5898 - mse: 9537646.0000 - mae: 984.5898 - val_loss: 1288.0569 - val_mse: 12315866.0000 - val_mae: 1288.0569\n",
      "Epoch 264/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 985.6635 - mse: 9712152.0000 - mae: 985.6635 - val_loss: 1266.4631 - val_mse: 12002232.0000 - val_mae: 1266.4631\n",
      "Epoch 265/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1012.6599 - mse: 10421886.0000 - mae: 1012.6599 - val_loss: 1263.0986 - val_mse: 11952581.0000 - val_mae: 1263.0986\n",
      "Epoch 266/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 991.8467 - mse: 10248412.0000 - mae: 991.8467 - val_loss: 1271.4868 - val_mse: 11965609.0000 - val_mae: 1271.4868\n",
      "Epoch 267/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 990.4411 - mse: 9882365.0000 - mae: 990.4410 - val_loss: 1314.5212 - val_mse: 12714471.0000 - val_mae: 1314.5212\n",
      "Epoch 268/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1009.6082 - mse: 10142199.0000 - mae: 1009.6083 - val_loss: 1243.6423 - val_mse: 11421620.0000 - val_mae: 1243.6423\n",
      "Epoch 269/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 978.1565 - mse: 9710573.0000 - mae: 978.1565 - val_loss: 1265.7419 - val_mse: 12069928.0000 - val_mae: 1265.7419\n",
      "Epoch 270/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 976.4953 - mse: 9712387.0000 - mae: 976.4954 - val_loss: 1241.4868 - val_mse: 11299413.0000 - val_mae: 1241.4868\n",
      "Epoch 271/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 981.0505 - mse: 9742960.0000 - mae: 981.0507 - val_loss: 1237.9569 - val_mse: 11325853.0000 - val_mae: 1237.9569\n",
      "Epoch 272/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 993.2118 - mse: 10177534.0000 - mae: 993.2119 - val_loss: 1253.4790 - val_mse: 11695853.0000 - val_mae: 1253.4790\n",
      "Epoch 273/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 991.6494 - mse: 9771958.0000 - mae: 991.6494 - val_loss: 1239.6981 - val_mse: 11273560.0000 - val_mae: 1239.6981\n",
      "Epoch 274/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 978.9727 - mse: 9436801.0000 - mae: 978.9728 - val_loss: 1252.8022 - val_mse: 11574048.0000 - val_mae: 1252.8022\n",
      "Epoch 275/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 980.9183 - mse: 9889886.0000 - mae: 980.9183 - val_loss: 1248.6184 - val_mse: 11614534.0000 - val_mae: 1248.6184\n",
      "Epoch 276/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 974.1397 - mse: 9134979.0000 - mae: 974.1396 - val_loss: 1252.4602 - val_mse: 11597664.0000 - val_mae: 1252.4602\n",
      "Epoch 277/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 987.9291 - mse: 9567113.0000 - mae: 987.9291 - val_loss: 1268.6914 - val_mse: 11842350.0000 - val_mae: 1268.6914\n",
      "Epoch 278/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 980.0816 - mse: 9909896.0000 - mae: 980.0815 - val_loss: 1270.7383 - val_mse: 12092468.0000 - val_mae: 1270.7383\n",
      "Epoch 279/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 988.6728 - mse: 10222432.0000 - mae: 988.6727 - val_loss: 1271.5868 - val_mse: 12562570.0000 - val_mae: 1271.5868\n",
      "Epoch 280/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 964.9119 - mse: 9240585.0000 - mae: 964.9119 - val_loss: 1245.1449 - val_mse: 11586787.0000 - val_mae: 1245.1449\n",
      "Epoch 281/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 995.7641 - mse: 10152673.0000 - mae: 995.7640 - val_loss: 1253.8278 - val_mse: 11674812.0000 - val_mae: 1253.8278\n",
      "Epoch 282/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 976.7951 - mse: 9649559.0000 - mae: 976.7952 - val_loss: 1304.1860 - val_mse: 12719686.0000 - val_mae: 1304.1860\n",
      "Epoch 283/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 961.9563 - mse: 9097874.0000 - mae: 961.9564 - val_loss: 1276.3247 - val_mse: 12005851.0000 - val_mae: 1276.3247\n",
      "Epoch 284/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 976.1162 - mse: 9418756.0000 - mae: 976.1161 - val_loss: 1261.6584 - val_mse: 11776733.0000 - val_mae: 1261.6584\n",
      "Epoch 285/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 991.0380 - mse: 10141695.0000 - mae: 991.0378 - val_loss: 1265.5106 - val_mse: 11787055.0000 - val_mae: 1265.5106\n",
      "Epoch 286/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 980.6777 - mse: 9605373.0000 - mae: 980.6779 - val_loss: 1267.9885 - val_mse: 11849543.0000 - val_mae: 1267.9885\n",
      "Epoch 287/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 958.7698 - mse: 9276832.0000 - mae: 958.7698 - val_loss: 1233.9409 - val_mse: 11202750.0000 - val_mae: 1233.9409\n",
      "Epoch 288/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 977.7492 - mse: 9417007.0000 - mae: 977.7492 - val_loss: 1235.8243 - val_mse: 11135004.0000 - val_mae: 1235.8243\n",
      "Epoch 289/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 961.8418 - mse: 9219220.0000 - mae: 961.8419 - val_loss: 1283.5715 - val_mse: 12292125.0000 - val_mae: 1283.5715\n",
      "Epoch 290/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 974.5224 - mse: 9393476.0000 - mae: 974.5224 - val_loss: 1245.7251 - val_mse: 11363268.0000 - val_mae: 1245.7251\n",
      "Epoch 291/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 980.3054 - mse: 9907497.0000 - mae: 980.3054 - val_loss: 1239.7775 - val_mse: 11216515.0000 - val_mae: 1239.7775\n",
      "Epoch 292/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 956.5722 - mse: 8775926.0000 - mae: 956.5721 - val_loss: 1236.4485 - val_mse: 11120773.0000 - val_mae: 1236.4485\n",
      "Epoch 293/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1000.4993 - mse: 10023818.0000 - mae: 1000.4993 - val_loss: 1242.9895 - val_mse: 11219698.0000 - val_mae: 1242.9895\n",
      "Epoch 294/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 984.7949 - mse: 9758248.0000 - mae: 984.7949 - val_loss: 1268.1248 - val_mse: 11762990.0000 - val_mae: 1268.1248\n",
      "Epoch 295/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1024.2365 - mse: 11026293.0000 - mae: 1024.2365 - val_loss: 1266.7780 - val_mse: 11667445.0000 - val_mae: 1266.7780\n",
      "Epoch 296/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 964.1274 - mse: 9272294.0000 - mae: 964.1274 - val_loss: 1236.4235 - val_mse: 11064049.0000 - val_mae: 1236.4235\n",
      "Epoch 297/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 958.1715 - mse: 9237400.0000 - mae: 958.1715 - val_loss: 1244.4410 - val_mse: 11153524.0000 - val_mae: 1244.4410\n",
      "Epoch 298/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 950.2966 - mse: 9106868.0000 - mae: 950.2967 - val_loss: 1237.9247 - val_mse: 11038624.0000 - val_mae: 1237.9247\n",
      "Epoch 299/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 966.5930 - mse: 9402739.0000 - mae: 966.5928 - val_loss: 1234.4956 - val_mse: 10914906.0000 - val_mae: 1234.4956\n",
      "Epoch 300/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 969.7213 - mse: 9619421.0000 - mae: 969.7212 - val_loss: 1240.1388 - val_mse: 11049085.0000 - val_mae: 1240.1388\n",
      "Epoch 301/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 946.5359 - mse: 8714620.0000 - mae: 946.5357 - val_loss: 1283.1909 - val_mse: 11866964.0000 - val_mae: 1283.1909\n",
      "Epoch 302/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 998.1900 - mse: 10196110.0000 - mae: 998.1899 - val_loss: 1276.7972 - val_mse: 11600617.0000 - val_mae: 1276.7972\n",
      "Epoch 303/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 961.2344 - mse: 9381966.0000 - mae: 961.2344 - val_loss: 1258.3773 - val_mse: 11333613.0000 - val_mae: 1258.3773\n",
      "Epoch 304/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 968.7519 - mse: 9194113.0000 - mae: 968.7520 - val_loss: 1245.3396 - val_mse: 11170003.0000 - val_mae: 1245.3396\n",
      "Epoch 305/500\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 965.8306 - mse: 9228958.0000 - mae: 965.8305 - val_loss: 1270.0897 - val_mse: 11645787.0000 - val_mae: 1270.0897\n",
      "Epoch 306/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 960.0538 - mse: 9006601.0000 - mae: 960.0538 - val_loss: 1235.7921 - val_mse: 10902238.0000 - val_mae: 1235.7921\n",
      "Epoch 307/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 966.8698 - mse: 9624899.0000 - mae: 966.8698 - val_loss: 1304.0784 - val_mse: 12376805.0000 - val_mae: 1304.0784\n",
      "Epoch 308/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 945.0167 - mse: 8779815.0000 - mae: 945.0167 - val_loss: 1250.8563 - val_mse: 11184956.0000 - val_mae: 1250.8563\n",
      "Epoch 309/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 984.2019 - mse: 9984926.0000 - mae: 984.2019 - val_loss: 1255.9266 - val_mse: 11339791.0000 - val_mae: 1255.9266\n",
      "Epoch 310/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 919.5784 - mse: 8370259.5000 - mae: 919.5784 - val_loss: 1253.8070 - val_mse: 11118434.0000 - val_mae: 1253.8070\n",
      "Epoch 311/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 961.8172 - mse: 9214498.0000 - mae: 961.8171 - val_loss: 1254.3113 - val_mse: 11151002.0000 - val_mae: 1254.3113\n",
      "Epoch 312/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 994.5146 - mse: 10088224.0000 - mae: 994.5146 - val_loss: 1237.1553 - val_mse: 10777373.0000 - val_mae: 1237.1553\n",
      "Epoch 313/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 947.5150 - mse: 8960528.0000 - mae: 947.5148 - val_loss: 1271.5566 - val_mse: 11533505.0000 - val_mae: 1271.5566\n",
      "Epoch 314/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 988.9604 - mse: 9986078.0000 - mae: 988.9604 - val_loss: 1229.6213 - val_mse: 10324860.0000 - val_mae: 1229.6213\n",
      "Epoch 315/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 955.4691 - mse: 9068979.0000 - mae: 955.4690 - val_loss: 1224.8165 - val_mse: 10065878.0000 - val_mae: 1224.8165\n",
      "Epoch 316/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 962.4974 - mse: 9358713.0000 - mae: 962.4975 - val_loss: 1231.4862 - val_mse: 10241898.0000 - val_mae: 1231.4862\n",
      "Epoch 317/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 947.6179 - mse: 9115142.0000 - mae: 947.6179 - val_loss: 1238.0719 - val_mse: 10370080.0000 - val_mae: 1238.0719\n",
      "Epoch 318/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 945.5729 - mse: 8706446.0000 - mae: 945.5729 - val_loss: 1235.3647 - val_mse: 10458317.0000 - val_mae: 1235.3647\n",
      "Epoch 319/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 947.3221 - mse: 8773693.0000 - mae: 947.3221 - val_loss: 1263.8468 - val_mse: 11069565.0000 - val_mae: 1263.8468\n",
      "Epoch 320/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 949.0024 - mse: 8951073.0000 - mae: 949.0025 - val_loss: 1235.4410 - val_mse: 10347532.0000 - val_mae: 1235.4410\n",
      "Epoch 321/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 961.8662 - mse: 9404675.0000 - mae: 961.8661 - val_loss: 1238.5316 - val_mse: 10368693.0000 - val_mae: 1238.5316\n",
      "Epoch 322/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 959.5857 - mse: 9058153.0000 - mae: 959.5857 - val_loss: 1293.9873 - val_mse: 11492376.0000 - val_mae: 1293.9873\n",
      "Epoch 323/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 948.4624 - mse: 8850030.0000 - mae: 948.4624 - val_loss: 1225.8339 - val_mse: 10100337.0000 - val_mae: 1225.8339\n",
      "Epoch 324/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 963.3444 - mse: 9262278.0000 - mae: 963.3443 - val_loss: 1236.6787 - val_mse: 10213006.0000 - val_mae: 1236.6787\n",
      "Epoch 325/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 945.0909 - mse: 8987812.0000 - mae: 945.0909 - val_loss: 1264.1746 - val_mse: 10804979.0000 - val_mae: 1264.1746\n",
      "Epoch 326/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 958.4204 - mse: 9232153.0000 - mae: 958.4204 - val_loss: 1254.2893 - val_mse: 10650761.0000 - val_mae: 1254.2893\n",
      "Epoch 327/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 940.0569 - mse: 8761214.0000 - mae: 940.0569 - val_loss: 1238.9482 - val_mse: 10338684.0000 - val_mae: 1238.9482\n",
      "Epoch 328/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 928.4602 - mse: 8260991.5000 - mae: 928.4603 - val_loss: 1249.1395 - val_mse: 10405171.0000 - val_mae: 1249.1395\n",
      "Epoch 329/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 969.9358 - mse: 9898889.0000 - mae: 969.9358 - val_loss: 1236.1184 - val_mse: 9936398.0000 - val_mae: 1236.1184\n",
      "Epoch 330/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 946.2910 - mse: 8634747.0000 - mae: 946.2910 - val_loss: 1209.4249 - val_mse: 9461407.0000 - val_mae: 1209.4249\n",
      "Epoch 331/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 948.7502 - mse: 8923741.0000 - mae: 948.7501 - val_loss: 1223.4601 - val_mse: 9840024.0000 - val_mae: 1223.4601\n",
      "Epoch 332/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 958.9190 - mse: 9343528.0000 - mae: 958.9191 - val_loss: 1222.2345 - val_mse: 9747379.0000 - val_mae: 1222.2345\n",
      "Epoch 333/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 933.7312 - mse: 8660829.0000 - mae: 933.7312 - val_loss: 1219.4197 - val_mse: 9734971.0000 - val_mae: 1219.4197\n",
      "Epoch 334/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 949.4531 - mse: 9269666.0000 - mae: 949.4531 - val_loss: 1223.7053 - val_mse: 9779670.0000 - val_mae: 1223.7053\n",
      "Epoch 335/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 952.1484 - mse: 8910863.0000 - mae: 952.1484 - val_loss: 1229.8865 - val_mse: 10056663.0000 - val_mae: 1229.8865\n",
      "Epoch 336/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 953.3886 - mse: 8889789.0000 - mae: 953.3885 - val_loss: 1234.4714 - val_mse: 9950375.0000 - val_mae: 1234.4714\n",
      "Epoch 337/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 958.3190 - mse: 9245454.0000 - mae: 958.3191 - val_loss: 1231.7958 - val_mse: 9815781.0000 - val_mae: 1231.7958\n",
      "Epoch 338/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 940.2266 - mse: 8659208.0000 - mae: 940.2265 - val_loss: 1259.5944 - val_mse: 10288237.0000 - val_mae: 1259.5944\n",
      "Epoch 339/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 949.3588 - mse: 8907395.0000 - mae: 949.3588 - val_loss: 1212.8396 - val_mse: 9400908.0000 - val_mae: 1212.8396\n",
      "Epoch 340/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 951.3678 - mse: 9103400.0000 - mae: 951.3679 - val_loss: 1233.9877 - val_mse: 9927769.0000 - val_mae: 1233.9877\n",
      "Epoch 341/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 949.2279 - mse: 8944060.0000 - mae: 949.2279 - val_loss: 1226.1294 - val_mse: 10026872.0000 - val_mae: 1226.1294\n",
      "Epoch 342/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 931.7899 - mse: 8466339.0000 - mae: 931.7899 - val_loss: 1224.6469 - val_mse: 9866740.0000 - val_mae: 1224.6469\n",
      "Epoch 343/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 957.0417 - mse: 9448282.0000 - mae: 957.0417 - val_loss: 1223.5564 - val_mse: 9803443.0000 - val_mae: 1223.5564\n",
      "Epoch 344/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 939.6024 - mse: 8702176.0000 - mae: 939.6024 - val_loss: 1207.8589 - val_mse: 9144192.0000 - val_mae: 1207.8589\n",
      "Epoch 345/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 940.4653 - mse: 9234921.0000 - mae: 940.4653 - val_loss: 1239.6310 - val_mse: 9918435.0000 - val_mae: 1239.6310\n",
      "Epoch 346/500\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 980.1272 - mse: 9893746.0000 - mae: 980.1273 - val_loss: 1240.8739 - val_mse: 10079763.0000 - val_mae: 1240.8739\n",
      "Epoch 347/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 940.3190 - mse: 8764597.0000 - mae: 940.3190 - val_loss: 1227.0653 - val_mse: 9644010.0000 - val_mae: 1227.0653\n",
      "Epoch 348/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 918.8430 - mse: 8242821.0000 - mae: 918.8431 - val_loss: 1220.6962 - val_mse: 9667530.0000 - val_mae: 1220.6962\n",
      "Epoch 349/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 931.8061 - mse: 8642057.0000 - mae: 931.8062 - val_loss: 1216.2316 - val_mse: 9508410.0000 - val_mae: 1216.2316\n",
      "Epoch 350/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 938.1872 - mse: 8754495.0000 - mae: 938.1873 - val_loss: 1230.7286 - val_mse: 10004577.0000 - val_mae: 1230.7286\n",
      "Epoch 351/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 927.4507 - mse: 8280883.0000 - mae: 927.4506 - val_loss: 1248.0052 - val_mse: 10059881.0000 - val_mae: 1248.0052\n",
      "Epoch 352/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 941.4085 - mse: 8585174.0000 - mae: 941.4086 - val_loss: 1221.3390 - val_mse: 9584629.0000 - val_mae: 1221.3390\n",
      "Epoch 353/500\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 972.1430 - mse: 9906204.0000 - mae: 972.1430 - val_loss: 1220.0657 - val_mse: 9301332.0000 - val_mae: 1220.0657\n",
      "Epoch 354/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 949.3910 - mse: 8959537.0000 - mae: 949.3911 - val_loss: 1241.4448 - val_mse: 9765503.0000 - val_mae: 1241.4448\n",
      "Epoch 355/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 958.8579 - mse: 9491395.0000 - mae: 958.8578 - val_loss: 1216.4380 - val_mse: 9397610.0000 - val_mae: 1216.4380\n",
      "Epoch 356/500\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 944.1803 - mse: 9182738.0000 - mae: 944.1803 - val_loss: 1224.6051 - val_mse: 9506582.0000 - val_mae: 1224.6051\n",
      "Epoch 357/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 940.5752 - mse: 8741720.0000 - mae: 940.5751 - val_loss: 1225.5439 - val_mse: 9535067.0000 - val_mae: 1225.5439\n",
      "Epoch 358/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 929.0026 - mse: 8484804.0000 - mae: 929.0026 - val_loss: 1221.6871 - val_mse: 9463471.0000 - val_mae: 1221.6871\n",
      "Epoch 359/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 940.7386 - mse: 8769486.0000 - mae: 940.7385 - val_loss: 1240.7689 - val_mse: 9965824.0000 - val_mae: 1240.7689\n",
      "Epoch 360/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 955.8775 - mse: 9156470.0000 - mae: 955.8774 - val_loss: 1210.0496 - val_mse: 9200798.0000 - val_mae: 1210.0496\n",
      "Epoch 361/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 957.6391 - mse: 9356327.0000 - mae: 957.6392 - val_loss: 1230.6201 - val_mse: 9423337.0000 - val_mae: 1230.6201\n",
      "Epoch 362/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 952.6269 - mse: 9221918.0000 - mae: 952.6270 - val_loss: 1210.2274 - val_mse: 8968308.0000 - val_mae: 1210.2274\n",
      "Epoch 363/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 927.9546 - mse: 8437216.0000 - mae: 927.9546 - val_loss: 1209.3215 - val_mse: 9029545.0000 - val_mae: 1209.3215\n",
      "Epoch 364/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 939.9741 - mse: 8674622.0000 - mae: 939.9741 - val_loss: 1207.8782 - val_mse: 9004564.0000 - val_mae: 1207.8782\n",
      "Epoch 365/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 938.3867 - mse: 8826895.0000 - mae: 938.3867 - val_loss: 1219.1316 - val_mse: 9296790.0000 - val_mae: 1219.1316\n",
      "Epoch 366/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 921.3040 - mse: 8394754.0000 - mae: 921.3041 - val_loss: 1215.8154 - val_mse: 9189957.0000 - val_mae: 1215.8154\n",
      "Epoch 367/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 918.5971 - mse: 8362899.5000 - mae: 918.5970 - val_loss: 1253.8669 - val_mse: 9790233.0000 - val_mae: 1253.8669\n",
      "Epoch 368/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 929.5634 - mse: 8446606.0000 - mae: 929.5634 - val_loss: 1214.9130 - val_mse: 9105392.0000 - val_mae: 1214.9130\n",
      "Epoch 369/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 920.1856 - mse: 7982379.5000 - mae: 920.1857 - val_loss: 1245.7804 - val_mse: 9832380.0000 - val_mae: 1245.7804\n",
      "Epoch 370/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 936.0871 - mse: 8494339.0000 - mae: 936.0872 - val_loss: 1240.1696 - val_mse: 9830405.0000 - val_mae: 1240.1696\n",
      "Epoch 371/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 939.1630 - mse: 9076804.0000 - mae: 939.1630 - val_loss: 1220.7756 - val_mse: 9283457.0000 - val_mae: 1220.7756\n",
      "Epoch 372/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 926.2222 - mse: 8376697.5000 - mae: 926.2222 - val_loss: 1222.7214 - val_mse: 9195031.0000 - val_mae: 1222.7214\n",
      "Epoch 373/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 941.0844 - mse: 8844600.0000 - mae: 941.0844 - val_loss: 1206.4481 - val_mse: 8824943.0000 - val_mae: 1206.4481\n",
      "Epoch 374/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 929.0686 - mse: 8550351.0000 - mae: 929.0685 - val_loss: 1208.1254 - val_mse: 8878601.0000 - val_mae: 1208.1254\n",
      "Epoch 375/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 933.9679 - mse: 8902737.0000 - mae: 933.9678 - val_loss: 1222.2937 - val_mse: 9081158.0000 - val_mae: 1222.2937\n",
      "Epoch 376/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 958.7241 - mse: 9360894.0000 - mae: 958.7241 - val_loss: 1218.3430 - val_mse: 9140951.0000 - val_mae: 1218.3430\n",
      "Epoch 377/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 922.2459 - mse: 8259931.5000 - mae: 922.2459 - val_loss: 1234.3499 - val_mse: 9438136.0000 - val_mae: 1234.3499\n",
      "Epoch 378/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 934.6533 - mse: 8706730.0000 - mae: 934.6532 - val_loss: 1218.6724 - val_mse: 9421069.0000 - val_mae: 1218.6724\n",
      "Epoch 379/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 928.9431 - mse: 8585440.0000 - mae: 928.9431 - val_loss: 1235.6707 - val_mse: 9410968.0000 - val_mae: 1235.6707\n",
      "Epoch 380/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 922.0426 - mse: 8451267.0000 - mae: 922.0425 - val_loss: 1214.8033 - val_mse: 9083085.0000 - val_mae: 1214.8033\n",
      "Epoch 381/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 940.7715 - mse: 8948972.0000 - mae: 940.7715 - val_loss: 1236.6600 - val_mse: 9429876.0000 - val_mae: 1236.6600\n",
      "Epoch 382/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 937.9931 - mse: 8851540.0000 - mae: 937.9930 - val_loss: 1202.4009 - val_mse: 8552274.0000 - val_mae: 1202.4009\n",
      "Epoch 383/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 934.1366 - mse: 8797247.0000 - mae: 934.1367 - val_loss: 1242.0200 - val_mse: 9392087.0000 - val_mae: 1242.0200\n",
      "Epoch 384/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 930.0596 - mse: 8992047.0000 - mae: 930.0596 - val_loss: 1230.6075 - val_mse: 9399070.0000 - val_mae: 1230.6075\n",
      "Epoch 385/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 939.3494 - mse: 8939280.0000 - mae: 939.3494 - val_loss: 1245.8934 - val_mse: 9769187.0000 - val_mae: 1245.8934\n",
      "Epoch 386/500\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 932.3717 - mse: 8657606.0000 - mae: 932.3717 - val_loss: 1241.2041 - val_mse: 9732393.0000 - val_mae: 1241.2041\n",
      "Epoch 387/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 912.7108 - mse: 8292238.0000 - mae: 912.7108 - val_loss: 1224.1298 - val_mse: 9345913.0000 - val_mae: 1224.1298\n",
      "Epoch 388/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 922.1995 - mse: 8499883.0000 - mae: 922.1994 - val_loss: 1222.5468 - val_mse: 9187418.0000 - val_mae: 1222.5468\n",
      "Epoch 389/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 920.3372 - mse: 8766866.0000 - mae: 920.3372 - val_loss: 1233.0111 - val_mse: 9300153.0000 - val_mae: 1233.0111\n",
      "Epoch 390/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 921.4653 - mse: 8559189.0000 - mae: 921.4653 - val_loss: 1245.0839 - val_mse: 9352145.0000 - val_mae: 1245.0839\n",
      "Epoch 391/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 927.7357 - mse: 8587050.0000 - mae: 927.7357 - val_loss: 1265.9764 - val_mse: 9742034.0000 - val_mae: 1265.9764\n",
      "Epoch 392/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 931.4533 - mse: 8818981.0000 - mae: 931.4533 - val_loss: 1264.7200 - val_mse: 10046521.0000 - val_mae: 1264.7200\n",
      "Epoch 393/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 918.0041 - mse: 8434617.0000 - mae: 918.0042 - val_loss: 1231.5406 - val_mse: 9413825.0000 - val_mae: 1231.5406\n",
      "Epoch 394/500\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 914.4891 - mse: 8080156.0000 - mae: 914.4891 - val_loss: 1224.7498 - val_mse: 9164570.0000 - val_mae: 1224.7498\n",
      "Epoch 395/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 927.0500 - mse: 8724864.0000 - mae: 927.0499 - val_loss: 1249.6376 - val_mse: 9655046.0000 - val_mae: 1249.6376\n",
      "Epoch 396/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 923.5113 - mse: 8629803.0000 - mae: 923.5113 - val_loss: 1240.4436 - val_mse: 9598414.0000 - val_mae: 1240.4436\n",
      "Epoch 397/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 938.5224 - mse: 9697614.0000 - mae: 938.5224 - val_loss: 1225.0824 - val_mse: 9231176.0000 - val_mae: 1225.0824\n",
      "Epoch 398/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 933.1680 - mse: 8878383.0000 - mae: 933.1680 - val_loss: 1216.4707 - val_mse: 9001647.0000 - val_mae: 1216.4707\n",
      "Epoch 399/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 937.1176 - mse: 8813481.0000 - mae: 937.1175 - val_loss: 1214.8525 - val_mse: 8670262.0000 - val_mae: 1214.8525\n",
      "Epoch 400/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 948.0978 - mse: 9534466.0000 - mae: 948.0978 - val_loss: 1229.0402 - val_mse: 9106612.0000 - val_mae: 1229.0402\n",
      "Epoch 401/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 922.5929 - mse: 8552804.0000 - mae: 922.5930 - val_loss: 1256.7316 - val_mse: 9790964.0000 - val_mae: 1256.7316\n",
      "Epoch 402/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 920.5565 - mse: 8320571.5000 - mae: 920.5566 - val_loss: 1251.5177 - val_mse: 9594167.0000 - val_mae: 1251.5177\n",
      "Epoch 403/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 948.8635 - mse: 9528903.0000 - mae: 948.8635 - val_loss: 1248.7789 - val_mse: 9459853.0000 - val_mae: 1248.7789\n",
      "Epoch 404/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 923.4962 - mse: 8371354.5000 - mae: 923.4963 - val_loss: 1240.3074 - val_mse: 9390945.0000 - val_mae: 1240.3074\n",
      "Epoch 405/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 933.7152 - mse: 8649619.0000 - mae: 933.7151 - val_loss: 1217.3201 - val_mse: 9093956.0000 - val_mae: 1217.3201\n",
      "Epoch 406/500\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 937.8095 - mse: 8961453.0000 - mae: 937.8094 - val_loss: 1254.9635 - val_mse: 9639201.0000 - val_mae: 1254.9635\n",
      "Epoch 407/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 917.3531 - mse: 8501643.0000 - mae: 917.3532 - val_loss: 1247.6379 - val_mse: 9457322.0000 - val_mae: 1247.6379\n",
      "Epoch 408/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 932.0704 - mse: 8958068.0000 - mae: 932.0704 - val_loss: 1218.8890 - val_mse: 8862465.0000 - val_mae: 1218.8890\n",
      "Epoch 409/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 930.9779 - mse: 8478410.0000 - mae: 930.9779 - val_loss: 1238.3717 - val_mse: 9040776.0000 - val_mae: 1238.3717\n",
      "Epoch 410/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 922.0561 - mse: 8655792.0000 - mae: 922.0561 - val_loss: 1204.1378 - val_mse: 8622879.0000 - val_mae: 1204.1378\n",
      "Epoch 411/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 915.3652 - mse: 8997505.0000 - mae: 915.3652 - val_loss: 1216.5743 - val_mse: 8962593.0000 - val_mae: 1216.5743\n",
      "Epoch 412/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 912.5133 - mse: 8427810.0000 - mae: 912.5134 - val_loss: 1210.5117 - val_mse: 8649912.0000 - val_mae: 1210.5117\n",
      "Epoch 413/500\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 925.2467 - mse: 8562516.0000 - mae: 925.2467 - val_loss: 1211.3792 - val_mse: 8839123.0000 - val_mae: 1211.3792\n",
      "Epoch 414/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 911.5430 - mse: 8226775.0000 - mae: 911.5430 - val_loss: 1219.7987 - val_mse: 8963180.0000 - val_mae: 1219.7987\n",
      "Epoch 415/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 912.9233 - mse: 8259192.0000 - mae: 912.9233 - val_loss: 1227.1924 - val_mse: 9213502.0000 - val_mae: 1227.1924\n",
      "Epoch 416/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 918.8126 - mse: 8517475.0000 - mae: 918.8127 - val_loss: 1221.5885 - val_mse: 9035973.0000 - val_mae: 1221.5885\n",
      "Epoch 417/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 918.2733 - mse: 8487909.0000 - mae: 918.2734 - val_loss: 1214.1835 - val_mse: 8806907.0000 - val_mae: 1214.1835\n",
      "Epoch 418/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 941.6098 - mse: 9038997.0000 - mae: 941.6098 - val_loss: 1219.4568 - val_mse: 9009164.0000 - val_mae: 1219.4568\n",
      "Epoch 419/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 940.9063 - mse: 9038211.0000 - mae: 940.9062 - val_loss: 1231.9480 - val_mse: 9325347.0000 - val_mae: 1231.9480\n",
      "Epoch 420/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 942.6958 - mse: 9020730.0000 - mae: 942.6958 - val_loss: 1212.3375 - val_mse: 8788040.0000 - val_mae: 1212.3375\n",
      "Epoch 421/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 945.3047 - mse: 9269161.0000 - mae: 945.3046 - val_loss: 1218.0753 - val_mse: 8883611.0000 - val_mae: 1218.0753\n",
      "Epoch 422/500\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 899.4089 - mse: 8406932.0000 - mae: 899.4089 - val_loss: 1207.4736 - val_mse: 8633515.0000 - val_mae: 1207.4736\n",
      "Epoch 423/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 940.4084 - mse: 8954832.0000 - mae: 940.4084 - val_loss: 1216.6479 - val_mse: 9056951.0000 - val_mae: 1216.6479\n",
      "Epoch 424/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 916.1646 - mse: 8581936.0000 - mae: 916.1646 - val_loss: 1207.6687 - val_mse: 8661367.0000 - val_mae: 1207.6687\n",
      "Epoch 425/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 913.9587 - mse: 8341583.5000 - mae: 913.9586 - val_loss: 1215.6594 - val_mse: 8953077.0000 - val_mae: 1215.6594\n",
      "Epoch 426/500\n",
      "7000/7000 [==============================] - 0s 51us/step - loss: 910.3329 - mse: 8405236.0000 - mae: 910.3330 - val_loss: 1217.4165 - val_mse: 9120248.0000 - val_mae: 1217.4165\n",
      "Epoch 427/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 923.4676 - mse: 8547406.0000 - mae: 923.4677 - val_loss: 1229.5228 - val_mse: 9224535.0000 - val_mae: 1229.5228\n",
      "Epoch 428/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 942.6643 - mse: 9202253.0000 - mae: 942.6643 - val_loss: 1229.1167 - val_mse: 9321084.0000 - val_mae: 1229.1167\n",
      "Epoch 429/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 944.3718 - mse: 8972726.0000 - mae: 944.3719 - val_loss: 1215.0010 - val_mse: 8859826.0000 - val_mae: 1215.0010\n",
      "Epoch 430/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 912.7378 - mse: 8281370.0000 - mae: 912.7379 - val_loss: 1218.9310 - val_mse: 8938003.0000 - val_mae: 1218.9310\n",
      "Epoch 431/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 929.4297 - mse: 8670161.0000 - mae: 929.4298 - val_loss: 1211.9203 - val_mse: 8797436.0000 - val_mae: 1211.9203\n",
      "Epoch 432/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 909.5344 - mse: 8283173.0000 - mae: 909.5344 - val_loss: 1218.9515 - val_mse: 8863705.0000 - val_mae: 1218.9515\n",
      "Epoch 433/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 911.4906 - mse: 8388876.0000 - mae: 911.4906 - val_loss: 1216.4803 - val_mse: 8869417.0000 - val_mae: 1216.4803\n",
      "Epoch 434/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 916.7045 - mse: 8284654.0000 - mae: 916.7044 - val_loss: 1238.2684 - val_mse: 9400779.0000 - val_mae: 1238.2684\n",
      "Epoch 435/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 929.1338 - mse: 9117150.0000 - mae: 929.1339 - val_loss: 1236.3195 - val_mse: 9557549.0000 - val_mae: 1236.3195\n",
      "Epoch 436/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 920.3743 - mse: 8517651.0000 - mae: 920.3743 - val_loss: 1250.3462 - val_mse: 9718301.0000 - val_mae: 1250.3462\n",
      "Epoch 437/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 898.0637 - mse: 7884038.0000 - mae: 898.0637 - val_loss: 1239.1654 - val_mse: 9435090.0000 - val_mae: 1239.1654\n",
      "Epoch 438/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 906.3901 - mse: 8274862.0000 - mae: 906.3901 - val_loss: 1227.5033 - val_mse: 9145437.0000 - val_mae: 1227.5033\n",
      "Epoch 439/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 918.5163 - mse: 8638855.0000 - mae: 918.5164 - val_loss: 1210.2465 - val_mse: 8691480.0000 - val_mae: 1210.2465\n",
      "Epoch 440/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 927.4423 - mse: 8682028.0000 - mae: 927.4424 - val_loss: 1220.9052 - val_mse: 8748013.0000 - val_mae: 1220.9052\n",
      "Epoch 441/500\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 919.1054 - mse: 8383413.0000 - mae: 919.1055 - val_loss: 1209.0806 - val_mse: 8593396.0000 - val_mae: 1209.0806\n",
      "Epoch 442/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 924.8784 - mse: 8714523.0000 - mae: 924.8784 - val_loss: 1220.2242 - val_mse: 8859068.0000 - val_mae: 1220.2242\n",
      "Epoch 443/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 919.8323 - mse: 8405064.0000 - mae: 919.8323 - val_loss: 1216.8271 - val_mse: 8720616.0000 - val_mae: 1216.8271\n",
      "Epoch 444/500\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 907.7747 - mse: 8417478.0000 - mae: 907.7746 - val_loss: 1223.7567 - val_mse: 8936944.0000 - val_mae: 1223.7567\n",
      "Epoch 445/500\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 945.2332 - mse: 9211297.0000 - mae: 945.2332 - val_loss: 1232.1208 - val_mse: 9410657.0000 - val_mae: 1232.1208\n",
      "Epoch 446/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 926.8237 - mse: 8769480.0000 - mae: 926.8237 - val_loss: 1220.9435 - val_mse: 9078632.0000 - val_mae: 1220.9435\n",
      "Epoch 447/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 925.9126 - mse: 8472196.0000 - mae: 925.9126 - val_loss: 1234.4069 - val_mse: 9315908.0000 - val_mae: 1234.4069\n",
      "Epoch 448/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 937.9252 - mse: 8954003.0000 - mae: 937.9253 - val_loss: 1238.9218 - val_mse: 9373956.0000 - val_mae: 1238.9218\n",
      "Epoch 449/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 889.1527 - mse: 7843322.5000 - mae: 889.1526 - val_loss: 1231.6757 - val_mse: 9271024.0000 - val_mae: 1231.6757\n",
      "Epoch 450/500\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 906.5233 - mse: 8249378.5000 - mae: 906.5232 - val_loss: 1234.4237 - val_mse: 9355754.0000 - val_mae: 1234.4237\n",
      "Epoch 451/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 911.8192 - mse: 8473592.0000 - mae: 911.8192 - val_loss: 1249.3683 - val_mse: 9573075.0000 - val_mae: 1249.3683\n",
      "Epoch 452/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 920.2103 - mse: 8681796.0000 - mae: 920.2103 - val_loss: 1229.9384 - val_mse: 9218731.0000 - val_mae: 1229.9384\n",
      "Epoch 453/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 904.9490 - mse: 7934890.5000 - mae: 904.9489 - val_loss: 1226.6504 - val_mse: 8923156.0000 - val_mae: 1226.6504\n",
      "Epoch 454/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 924.6263 - mse: 8355047.5000 - mae: 924.6263 - val_loss: 1260.1261 - val_mse: 9549934.0000 - val_mae: 1260.1261\n",
      "Epoch 455/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 917.4573 - mse: 8360853.5000 - mae: 917.4573 - val_loss: 1236.8618 - val_mse: 9157760.0000 - val_mae: 1236.8618\n",
      "Epoch 456/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 907.3525 - mse: 8446843.0000 - mae: 907.3524 - val_loss: 1236.2477 - val_mse: 9049748.0000 - val_mae: 1236.2477\n",
      "Epoch 457/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 895.5394 - mse: 8061076.5000 - mae: 895.5394 - val_loss: 1227.7944 - val_mse: 9026511.0000 - val_mae: 1227.7944\n",
      "Epoch 458/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 900.9877 - mse: 8195588.5000 - mae: 900.9877 - val_loss: 1228.2905 - val_mse: 9290745.0000 - val_mae: 1228.2905\n",
      "Epoch 459/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 903.8700 - mse: 7960242.5000 - mae: 903.8699 - val_loss: 1232.2260 - val_mse: 9239798.0000 - val_mae: 1232.2260\n",
      "Epoch 460/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 909.1182 - mse: 8330249.5000 - mae: 909.1183 - val_loss: 1228.5308 - val_mse: 9112591.0000 - val_mae: 1228.5308\n",
      "Epoch 461/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 927.5145 - mse: 9053057.0000 - mae: 927.5145 - val_loss: 1232.5465 - val_mse: 9041054.0000 - val_mae: 1232.5465\n",
      "Epoch 462/500\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 909.9719 - mse: 8637576.0000 - mae: 909.9719 - val_loss: 1245.3851 - val_mse: 9459686.0000 - val_mae: 1245.3851\n",
      "Epoch 463/500\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 904.8667 - mse: 8449301.0000 - mae: 904.8668 - val_loss: 1239.0659 - val_mse: 9151301.0000 - val_mae: 1239.0659\n",
      "Epoch 464/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 891.7510 - mse: 7832693.5000 - mae: 891.7510 - val_loss: 1248.4501 - val_mse: 9475088.0000 - val_mae: 1248.4501\n",
      "Epoch 465/500\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 910.7965 - mse: 7986991.5000 - mae: 910.7964 - val_loss: 1239.5730 - val_mse: 9218290.0000 - val_mae: 1239.5730\n",
      "Epoch 466/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 901.8459 - mse: 8158690.0000 - mae: 901.8459 - val_loss: 1235.2749 - val_mse: 9155106.0000 - val_mae: 1235.2749\n",
      "Epoch 467/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 901.9415 - mse: 7915818.5000 - mae: 901.9416 - val_loss: 1227.1954 - val_mse: 8898572.0000 - val_mae: 1227.1954\n",
      "Epoch 468/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 916.0881 - mse: 8329178.5000 - mae: 916.0882 - val_loss: 1228.0773 - val_mse: 8834059.0000 - val_mae: 1228.0773\n",
      "Epoch 469/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 910.0869 - mse: 8141185.5000 - mae: 910.0869 - val_loss: 1263.0194 - val_mse: 9374439.0000 - val_mae: 1263.0194\n",
      "Epoch 470/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 897.4265 - mse: 8157039.0000 - mae: 897.4265 - val_loss: 1249.8457 - val_mse: 9157799.0000 - val_mae: 1249.8457\n",
      "Epoch 471/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 912.8337 - mse: 8590338.0000 - mae: 912.8337 - val_loss: 1235.3815 - val_mse: 9234274.0000 - val_mae: 1235.3815\n",
      "Epoch 472/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 917.6304 - mse: 8656543.0000 - mae: 917.6304 - val_loss: 1224.5383 - val_mse: 9047199.0000 - val_mae: 1224.5383\n",
      "Epoch 473/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 913.2481 - mse: 8406714.0000 - mae: 913.2480 - val_loss: 1273.1219 - val_mse: 9980264.0000 - val_mae: 1273.1219\n",
      "Epoch 474/500\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 913.8061 - mse: 8367587.5000 - mae: 913.8062 - val_loss: 1231.6256 - val_mse: 8957544.0000 - val_mae: 1231.6256\n",
      "Epoch 475/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 882.0744 - mse: 7546298.0000 - mae: 882.0745 - val_loss: 1231.5656 - val_mse: 8874250.0000 - val_mae: 1231.5656\n",
      "Epoch 476/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 894.6152 - mse: 7778709.0000 - mae: 894.6152 - val_loss: 1235.4846 - val_mse: 8944705.0000 - val_mae: 1235.4846\n",
      "Epoch 477/500\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 896.1704 - mse: 7906860.0000 - mae: 896.1704 - val_loss: 1229.2771 - val_mse: 9007303.0000 - val_mae: 1229.2771\n",
      "Epoch 478/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 906.1723 - mse: 8144493.0000 - mae: 906.1724 - val_loss: 1246.0364 - val_mse: 9144192.0000 - val_mae: 1246.0364\n",
      "Epoch 479/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 899.2584 - mse: 8090109.5000 - mae: 899.2584 - val_loss: 1222.9430 - val_mse: 8596186.0000 - val_mae: 1222.9430\n",
      "Epoch 480/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 917.1122 - mse: 8417707.0000 - mae: 917.1121 - val_loss: 1234.6913 - val_mse: 9037919.0000 - val_mae: 1234.6913\n",
      "Epoch 481/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 913.0762 - mse: 8534420.0000 - mae: 913.0762 - val_loss: 1230.1116 - val_mse: 8797362.0000 - val_mae: 1230.1116\n",
      "Epoch 482/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 907.8320 - mse: 8670017.0000 - mae: 907.8320 - val_loss: 1232.8290 - val_mse: 8862338.0000 - val_mae: 1232.8290\n",
      "Epoch 483/500\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 909.7998 - mse: 8149781.5000 - mae: 909.7997 - val_loss: 1223.4673 - val_mse: 8486243.0000 - val_mae: 1223.4673\n",
      "Epoch 484/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 890.3655 - mse: 8018065.0000 - mae: 890.3654 - val_loss: 1223.2081 - val_mse: 8523116.0000 - val_mae: 1223.2081\n",
      "Epoch 485/500\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 909.5997 - mse: 8214814.0000 - mae: 909.5998 - val_loss: 1225.6216 - val_mse: 8568025.0000 - val_mae: 1225.6216\n",
      "Epoch 486/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 903.0175 - mse: 8003437.5000 - mae: 903.0175 - val_loss: 1234.3921 - val_mse: 8762760.0000 - val_mae: 1234.3921\n",
      "Epoch 487/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 915.3579 - mse: 8457304.0000 - mae: 915.3578 - val_loss: 1238.7708 - val_mse: 9007167.0000 - val_mae: 1238.7708\n",
      "Epoch 488/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 925.1930 - mse: 8895372.0000 - mae: 925.1931 - val_loss: 1229.4554 - val_mse: 8838764.0000 - val_mae: 1229.4554\n",
      "Epoch 489/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 902.5216 - mse: 7803723.0000 - mae: 902.5217 - val_loss: 1234.8818 - val_mse: 9065789.0000 - val_mae: 1234.8818\n",
      "Epoch 490/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 913.0506 - mse: 8718150.0000 - mae: 913.0507 - val_loss: 1222.9253 - val_mse: 8802314.0000 - val_mae: 1222.9253\n",
      "Epoch 491/500\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 902.3975 - mse: 8600582.0000 - mae: 902.3975 - val_loss: 1222.6836 - val_mse: 8706833.0000 - val_mae: 1222.6836\n",
      "Epoch 492/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 896.5661 - mse: 7935458.5000 - mae: 896.5662 - val_loss: 1234.0692 - val_mse: 8817067.0000 - val_mae: 1234.0692\n",
      "Epoch 493/500\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 919.0819 - mse: 8657245.0000 - mae: 919.0819 - val_loss: 1220.8832 - val_mse: 8611564.0000 - val_mae: 1220.8832\n",
      "Epoch 494/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 893.5219 - mse: 8174512.0000 - mae: 893.5219 - val_loss: 1236.8051 - val_mse: 8740921.0000 - val_mae: 1236.8051\n",
      "Epoch 495/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 900.3754 - mse: 8046984.0000 - mae: 900.3754 - val_loss: 1228.2361 - val_mse: 8780382.0000 - val_mae: 1228.2361\n",
      "Epoch 496/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 904.5227 - mse: 8069243.0000 - mae: 904.5226 - val_loss: 1218.2878 - val_mse: 8433570.0000 - val_mae: 1218.2878\n",
      "Epoch 497/500\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 909.7006 - mse: 8618720.0000 - mae: 909.7006 - val_loss: 1215.2058 - val_mse: 8330356.5000 - val_mae: 1215.2058\n",
      "Epoch 498/500\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 907.3507 - mse: 8416629.0000 - mae: 907.3507 - val_loss: 1238.5802 - val_mse: 8769274.0000 - val_mae: 1238.5802\n",
      "Epoch 499/500\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 895.6009 - mse: 8077458.5000 - mae: 895.6008 - val_loss: 1244.9673 - val_mse: 9054331.0000 - val_mae: 1244.9673\n",
      "Epoch 500/500\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 925.3139 - mse: 8618975.0000 - mae: 925.3138 - val_loss: 1231.9821 - val_mse: 8847813.0000 - val_mae: 1231.9821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8500 samples, validate on 500 samples\n",
      "Epoch 1/500\n",
      "8500/8500 [==============================] - 2s 270us/step - loss: 2503.8166 - mse: 81666464.0000 - mae: 2503.8164 - val_loss: 4502.3169 - val_mse: 220620256.0000 - val_mae: 4502.3169\n",
      "Epoch 2/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 2486.1641 - mse: 81609584.0000 - mae: 2486.1641 - val_loss: 4471.1875 - val_mse: 220497968.0000 - val_mae: 4471.1875\n",
      "Epoch 3/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2473.1070 - mse: 81533048.0000 - mae: 2473.1069 - val_loss: 4446.4766 - val_mse: 220369696.0000 - val_mae: 4446.4766\n",
      "Epoch 4/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2467.0566 - mse: 81460880.0000 - mae: 2467.0566 - val_loss: 4431.0879 - val_mse: 220265200.0000 - val_mae: 4431.0879\n",
      "Epoch 5/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2464.8709 - mse: 81409112.0000 - mae: 2464.8711 - val_loss: 4424.5640 - val_mse: 220207088.0000 - val_mae: 4424.5640\n",
      "Epoch 6/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2463.6198 - mse: 81379024.0000 - mae: 2463.6201 - val_loss: 4420.9287 - val_mse: 220167344.0000 - val_mae: 4420.9287\n",
      "Epoch 7/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2462.6187 - mse: 81354200.0000 - mae: 2462.6184 - val_loss: 4419.3291 - val_mse: 220141632.0000 - val_mae: 4419.3291\n",
      "Epoch 8/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2461.5227 - mse: 81338168.0000 - mae: 2461.5227 - val_loss: 4416.6294 - val_mse: 220106944.0000 - val_mae: 4416.6294\n",
      "Epoch 9/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2461.8346 - mse: 81323032.0000 - mae: 2461.8345 - val_loss: 4416.7383 - val_mse: 220093520.0000 - val_mae: 4416.7383\n",
      "Epoch 10/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2460.5311 - mse: 81298904.0000 - mae: 2460.5312 - val_loss: 4415.3179 - val_mse: 220068032.0000 - val_mae: 4415.3179\n",
      "Epoch 11/500\n",
      "8500/8500 [==============================] - 0s 34us/step - loss: 2458.6666 - mse: 81281408.0000 - mae: 2458.6667 - val_loss: 4413.7461 - val_mse: 220040272.0000 - val_mae: 4413.7461\n",
      "Epoch 12/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2457.7989 - mse: 81251624.0000 - mae: 2457.7991 - val_loss: 4412.2793 - val_mse: 220012448.0000 - val_mae: 4412.2793\n",
      "Epoch 13/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 2456.0208 - mse: 81206504.0000 - mae: 2456.0210 - val_loss: 4411.5259 - val_mse: 219982240.0000 - val_mae: 4411.5259\n",
      "Epoch 14/500\n",
      "8500/8500 [==============================] - 0s 36us/step - loss: 2455.0130 - mse: 81164144.0000 - mae: 2455.0127 - val_loss: 4408.4399 - val_mse: 219721120.0000 - val_mae: 4408.4399\n",
      "Epoch 15/500\n",
      "8500/8500 [==============================] - 0s 35us/step - loss: 2452.0212 - mse: 81050904.0000 - mae: 2452.0210 - val_loss: 4405.2290 - val_mse: 219601904.0000 - val_mae: 4405.2290\n",
      "Epoch 16/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2449.5954 - mse: 80966976.0000 - mae: 2449.5952 - val_loss: 4400.6938 - val_mse: 219349952.0000 - val_mae: 4400.6938\n",
      "Epoch 17/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2448.3466 - mse: 80897784.0000 - mae: 2448.3469 - val_loss: 4397.2168 - val_mse: 219188640.0000 - val_mae: 4397.2168\n",
      "Epoch 18/500\n",
      "8500/8500 [==============================] - 0s 34us/step - loss: 2445.1607 - mse: 80825408.0000 - mae: 2445.1606 - val_loss: 4394.0684 - val_mse: 219052480.0000 - val_mae: 4394.0684\n",
      "Epoch 19/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2442.8682 - mse: 80740000.0000 - mae: 2442.8682 - val_loss: 4390.5645 - val_mse: 218947296.0000 - val_mae: 4390.5645\n",
      "Epoch 20/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2441.4126 - mse: 80688808.0000 - mae: 2441.4124 - val_loss: 4387.5444 - val_mse: 218846848.0000 - val_mae: 4387.5444\n",
      "Epoch 21/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2440.7044 - mse: 80659680.0000 - mae: 2440.7046 - val_loss: 4385.2427 - val_mse: 218752128.0000 - val_mae: 4385.2427\n",
      "Epoch 22/500\n",
      "8500/8500 [==============================] - 0s 36us/step - loss: 2440.2288 - mse: 80614464.0000 - mae: 2440.2290 - val_loss: 4385.0034 - val_mse: 218724176.0000 - val_mae: 4385.0034\n",
      "Epoch 23/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2438.1982 - mse: 80562952.0000 - mae: 2438.1982 - val_loss: 4382.8804 - val_mse: 218646560.0000 - val_mae: 4382.8804\n",
      "Epoch 24/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2437.1447 - mse: 80543888.0000 - mae: 2437.1448 - val_loss: 4382.0386 - val_mse: 218609632.0000 - val_mae: 4382.0386\n",
      "Epoch 25/500\n",
      "8500/8500 [==============================] - 0s 35us/step - loss: 2436.6559 - mse: 80532704.0000 - mae: 2436.6560 - val_loss: 4380.4878 - val_mse: 218554768.0000 - val_mae: 4380.4878\n",
      "Epoch 26/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2436.8785 - mse: 80527352.0000 - mae: 2436.8787 - val_loss: 4379.4731 - val_mse: 218520688.0000 - val_mae: 4379.4731\n",
      "Epoch 27/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2436.2993 - mse: 80508768.0000 - mae: 2436.2988 - val_loss: 4378.6650 - val_mse: 218490768.0000 - val_mae: 4378.6650\n",
      "Epoch 28/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2435.8589 - mse: 80473856.0000 - mae: 2435.8589 - val_loss: 4378.5415 - val_mse: 218482592.0000 - val_mae: 4378.5415\n",
      "Epoch 29/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2435.7976 - mse: 80475640.0000 - mae: 2435.7976 - val_loss: 4377.4365 - val_mse: 218447664.0000 - val_mae: 4377.4365\n",
      "Epoch 30/500\n",
      "8500/8500 [==============================] - 0s 36us/step - loss: 2435.3876 - mse: 80471616.0000 - mae: 2435.3875 - val_loss: 4377.3916 - val_mse: 218445776.0000 - val_mae: 4377.3916\n",
      "Epoch 31/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2435.5080 - mse: 80448160.0000 - mae: 2435.5078 - val_loss: 4376.9106 - val_mse: 218430336.0000 - val_mae: 4376.9106\n",
      "Epoch 32/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2433.4775 - mse: 80433152.0000 - mae: 2433.4773 - val_loss: 4376.7998 - val_mse: 218426160.0000 - val_mae: 4376.7998\n",
      "Epoch 33/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2435.3832 - mse: 80433416.0000 - mae: 2435.3831 - val_loss: 4377.0259 - val_mse: 218432416.0000 - val_mae: 4377.0259\n",
      "Epoch 34/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 2434.5653 - mse: 80448664.0000 - mae: 2434.5654 - val_loss: 4376.8657 - val_mse: 218426624.0000 - val_mae: 4376.8657\n",
      "Epoch 35/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2435.0802 - mse: 80451992.0000 - mae: 2435.0801 - val_loss: 4376.3921 - val_mse: 218410704.0000 - val_mae: 4376.3921\n",
      "Epoch 36/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2435.5645 - mse: 80450880.0000 - mae: 2435.5647 - val_loss: 4376.5981 - val_mse: 218415184.0000 - val_mae: 4376.5981\n",
      "Epoch 37/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2435.4256 - mse: 80466128.0000 - mae: 2435.4258 - val_loss: 4376.8940 - val_mse: 218421440.0000 - val_mae: 4376.8940\n",
      "Epoch 38/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2435.7697 - mse: 80454848.0000 - mae: 2435.7695 - val_loss: 4376.6836 - val_mse: 218412000.0000 - val_mae: 4376.6836\n",
      "Epoch 39/500\n",
      "8500/8500 [==============================] - 0s 35us/step - loss: 2433.8129 - mse: 80456976.0000 - mae: 2433.8130 - val_loss: 4375.9873 - val_mse: 218389200.0000 - val_mae: 4375.9873\n",
      "Epoch 40/500\n",
      "8500/8500 [==============================] - 0s 36us/step - loss: 2434.8483 - mse: 80444224.0000 - mae: 2434.8481 - val_loss: 4376.5571 - val_mse: 218405792.0000 - val_mae: 4376.5571\n",
      "Epoch 41/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2434.7306 - mse: 80450744.0000 - mae: 2434.7305 - val_loss: 4375.6055 - val_mse: 218374544.0000 - val_mae: 4375.6055\n",
      "Epoch 42/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2435.8159 - mse: 80434304.0000 - mae: 2435.8157 - val_loss: 4376.3950 - val_mse: 218397680.0000 - val_mae: 4376.3950\n",
      "Epoch 43/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2434.6621 - mse: 80429808.0000 - mae: 2434.6621 - val_loss: 4376.9082 - val_mse: 218412960.0000 - val_mae: 4376.9082\n",
      "Epoch 44/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2434.5867 - mse: 80463400.0000 - mae: 2434.5872 - val_loss: 4376.1924 - val_mse: 218390416.0000 - val_mae: 4376.1924\n",
      "Epoch 45/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2435.0870 - mse: 80456552.0000 - mae: 2435.0872 - val_loss: 4376.3579 - val_mse: 218395296.0000 - val_mae: 4376.3579\n",
      "Epoch 46/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2433.0519 - mse: 80431120.0000 - mae: 2433.0518 - val_loss: 4375.4741 - val_mse: 218367296.0000 - val_mae: 4375.4741\n",
      "Epoch 47/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2433.7582 - mse: 80415008.0000 - mae: 2433.7581 - val_loss: 4375.5283 - val_mse: 218368480.0000 - val_mae: 4375.5283\n",
      "Epoch 48/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2434.4120 - mse: 80427032.0000 - mae: 2434.4121 - val_loss: 4375.7129 - val_mse: 218373488.0000 - val_mae: 4375.7129\n",
      "Epoch 49/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2433.2094 - mse: 80412752.0000 - mae: 2433.2092 - val_loss: 4375.4458 - val_mse: 218364000.0000 - val_mae: 4375.4458\n",
      "Epoch 50/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2434.5541 - mse: 80418136.0000 - mae: 2434.5542 - val_loss: 4375.4780 - val_mse: 218363872.0000 - val_mae: 4375.4780\n",
      "Epoch 51/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2435.2476 - mse: 80430904.0000 - mae: 2435.2476 - val_loss: 4375.7036 - val_mse: 218369856.0000 - val_mae: 4375.7036\n",
      "Epoch 52/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2434.0153 - mse: 80417672.0000 - mae: 2434.0154 - val_loss: 4375.4512 - val_mse: 218361040.0000 - val_mae: 4375.4512\n",
      "Epoch 53/500\n",
      "8500/8500 [==============================] - 0s 36us/step - loss: 2434.5979 - mse: 80434064.0000 - mae: 2434.5979 - val_loss: 4375.4668 - val_mse: 218360992.0000 - val_mae: 4375.4668\n",
      "Epoch 54/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2435.6917 - mse: 80443904.0000 - mae: 2435.6917 - val_loss: 4376.6558 - val_mse: 218397792.0000 - val_mae: 4376.6558\n",
      "Epoch 55/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2434.1104 - mse: 80420592.0000 - mae: 2434.1106 - val_loss: 4375.9209 - val_mse: 218374320.0000 - val_mae: 4375.9209\n",
      "Epoch 56/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2433.2944 - mse: 80418792.0000 - mae: 2433.2947 - val_loss: 4375.2915 - val_mse: 218353984.0000 - val_mae: 4375.2915\n",
      "Epoch 57/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2434.4107 - mse: 80407968.0000 - mae: 2434.4111 - val_loss: 4375.7168 - val_mse: 218366832.0000 - val_mae: 4375.7168\n",
      "Epoch 58/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 2434.5481 - mse: 80423368.0000 - mae: 2434.5481 - val_loss: 4375.9966 - val_mse: 218374784.0000 - val_mae: 4375.9966\n",
      "Epoch 59/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2434.2221 - mse: 80421464.0000 - mae: 2434.2222 - val_loss: 4375.7144 - val_mse: 218364800.0000 - val_mae: 4375.7144\n",
      "Epoch 60/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 2434.6177 - mse: 80440512.0000 - mae: 2434.6179 - val_loss: 4375.8999 - val_mse: 218368528.0000 - val_mae: 4375.8999\n",
      "Epoch 61/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2429.9250 - mse: 80435768.0000 - mae: 2429.9253 - val_loss: 4402.2793 - val_mse: 218426624.0000 - val_mae: 4402.2793\n",
      "Epoch 62/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2405.7252 - mse: 80304824.0000 - mae: 2405.7251 - val_loss: 4399.3721 - val_mse: 217823872.0000 - val_mae: 4399.3721\n",
      "Epoch 63/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2390.2169 - mse: 79978592.0000 - mae: 2390.2173 - val_loss: 4381.8560 - val_mse: 217150448.0000 - val_mae: 4381.8560\n",
      "Epoch 64/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2375.0537 - mse: 79644088.0000 - mae: 2375.0542 - val_loss: 4370.9399 - val_mse: 216525696.0000 - val_mae: 4370.9399\n",
      "Epoch 65/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2366.8162 - mse: 79348624.0000 - mae: 2366.8162 - val_loss: 4351.2847 - val_mse: 215948832.0000 - val_mae: 4351.2847\n",
      "Epoch 66/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2357.2757 - mse: 79018056.0000 - mae: 2357.2759 - val_loss: 4347.5786 - val_mse: 215473200.0000 - val_mae: 4347.5786\n",
      "Epoch 67/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2352.3458 - mse: 78815416.0000 - mae: 2352.3459 - val_loss: 4319.5688 - val_mse: 214968176.0000 - val_mae: 4319.5688\n",
      "Epoch 68/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2348.5838 - mse: 78594104.0000 - mae: 2348.5837 - val_loss: 4308.8823 - val_mse: 214576144.0000 - val_mae: 4308.8823\n",
      "Epoch 69/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2347.6975 - mse: 78478520.0000 - mae: 2347.6973 - val_loss: 4307.6035 - val_mse: 214287584.0000 - val_mae: 4307.6035\n",
      "Epoch 70/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2348.6994 - mse: 78368856.0000 - mae: 2348.6992 - val_loss: 4304.3032 - val_mse: 214111168.0000 - val_mae: 4304.3032\n",
      "Epoch 71/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 2346.8034 - mse: 78287872.0000 - mae: 2346.8032 - val_loss: 4301.2310 - val_mse: 213928080.0000 - val_mae: 4301.2310\n",
      "Epoch 72/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2341.5571 - mse: 78102296.0000 - mae: 2341.5571 - val_loss: 4292.4585 - val_mse: 213685136.0000 - val_mae: 4292.4585\n",
      "Epoch 73/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2342.4531 - mse: 77986096.0000 - mae: 2342.4529 - val_loss: 4297.2192 - val_mse: 213573584.0000 - val_mae: 4297.2192\n",
      "Epoch 74/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 2339.6427 - mse: 78005904.0000 - mae: 2339.6426 - val_loss: 4288.3564 - val_mse: 213368208.0000 - val_mae: 4288.3564\n",
      "Epoch 75/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 2338.4943 - mse: 77841480.0000 - mae: 2338.4941 - val_loss: 4280.1392 - val_mse: 213155312.0000 - val_mae: 4280.1392\n",
      "Epoch 76/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2337.0456 - mse: 77767776.0000 - mae: 2337.0454 - val_loss: 4277.7969 - val_mse: 212987008.0000 - val_mae: 4277.7969\n",
      "Epoch 77/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2335.7909 - mse: 77711192.0000 - mae: 2335.7905 - val_loss: 4280.7607 - val_mse: 212887840.0000 - val_mae: 4280.7607\n",
      "Epoch 78/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 2334.2990 - mse: 77680976.0000 - mae: 2334.2993 - val_loss: 4279.6250 - val_mse: 212762448.0000 - val_mae: 4279.6250\n",
      "Epoch 79/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2332.1987 - mse: 77556008.0000 - mae: 2332.1987 - val_loss: 4271.8955 - val_mse: 212561760.0000 - val_mae: 4271.8955\n",
      "Epoch 80/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2329.9587 - mse: 77480464.0000 - mae: 2329.9587 - val_loss: 4263.9863 - val_mse: 212361904.0000 - val_mae: 4263.9863\n",
      "Epoch 81/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 2331.5194 - mse: 77444928.0000 - mae: 2331.5195 - val_loss: 4267.4878 - val_mse: 212291120.0000 - val_mae: 4267.4878\n",
      "Epoch 82/500\n",
      "8500/8500 [==============================] - 0s 35us/step - loss: 2327.6614 - mse: 77392024.0000 - mae: 2327.6611 - val_loss: 4256.6216 - val_mse: 212102448.0000 - val_mae: 4256.6216\n",
      "Epoch 83/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2326.6334 - mse: 77263808.0000 - mae: 2326.6331 - val_loss: 4259.9126 - val_mse: 212004608.0000 - val_mae: 4259.9126\n",
      "Epoch 84/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 2322.8417 - mse: 77088760.0000 - mae: 2322.8413 - val_loss: 4251.6001 - val_mse: 211802944.0000 - val_mae: 4251.6001\n",
      "Epoch 85/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 2316.9428 - mse: 77067400.0000 - mae: 2316.9426 - val_loss: 4253.9380 - val_mse: 211620640.0000 - val_mae: 4253.9380\n",
      "Epoch 86/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2315.9564 - mse: 76998736.0000 - mae: 2315.9565 - val_loss: 4244.5605 - val_mse: 211341088.0000 - val_mae: 4244.5605\n",
      "Epoch 87/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2310.5824 - mse: 76824216.0000 - mae: 2310.5823 - val_loss: 4240.7280 - val_mse: 211111840.0000 - val_mae: 4240.7280\n",
      "Epoch 88/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2305.7844 - mse: 76636288.0000 - mae: 2305.7844 - val_loss: 4236.6045 - val_mse: 210855776.0000 - val_mae: 4236.6045\n",
      "Epoch 89/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 2304.1452 - mse: 76587592.0000 - mae: 2304.1455 - val_loss: 4224.8926 - val_mse: 210497984.0000 - val_mae: 4224.8926\n",
      "Epoch 90/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2296.7174 - mse: 76341488.0000 - mae: 2296.7175 - val_loss: 4220.2012 - val_mse: 210089824.0000 - val_mae: 4220.2012\n",
      "Epoch 91/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2296.0378 - mse: 76180864.0000 - mae: 2296.0376 - val_loss: 4212.9316 - val_mse: 209720608.0000 - val_mae: 4212.9316\n",
      "Epoch 92/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2290.0334 - mse: 75954168.0000 - mae: 2290.0334 - val_loss: 4206.9395 - val_mse: 209341488.0000 - val_mae: 4206.9395\n",
      "Epoch 93/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2287.3200 - mse: 75770288.0000 - mae: 2287.3201 - val_loss: 4198.9165 - val_mse: 208903712.0000 - val_mae: 4198.9165\n",
      "Epoch 94/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2285.7916 - mse: 75501696.0000 - mae: 2285.7915 - val_loss: 4187.8081 - val_mse: 208423776.0000 - val_mae: 4187.8081\n",
      "Epoch 95/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 2280.0572 - mse: 75324464.0000 - mae: 2280.0574 - val_loss: 4177.6250 - val_mse: 207902864.0000 - val_mae: 4177.6250\n",
      "Epoch 96/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2276.9856 - mse: 75022280.0000 - mae: 2276.9858 - val_loss: 4172.1006 - val_mse: 207411936.0000 - val_mae: 4172.1006\n",
      "Epoch 97/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2272.7413 - mse: 74895584.0000 - mae: 2272.7415 - val_loss: 4167.2461 - val_mse: 206962560.0000 - val_mae: 4167.2461\n",
      "Epoch 98/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2263.7322 - mse: 74597368.0000 - mae: 2263.7322 - val_loss: 4153.0166 - val_mse: 206320880.0000 - val_mae: 4153.0166\n",
      "Epoch 99/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2261.2119 - mse: 74306224.0000 - mae: 2261.2117 - val_loss: 4141.9448 - val_mse: 205753968.0000 - val_mae: 4141.9448\n",
      "Epoch 100/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2257.0017 - mse: 73963376.0000 - mae: 2257.0020 - val_loss: 4130.4849 - val_mse: 205119808.0000 - val_mae: 4130.4849\n",
      "Epoch 101/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 2250.5654 - mse: 73516040.0000 - mae: 2250.5657 - val_loss: 4127.3530 - val_mse: 204584272.0000 - val_mae: 4127.3530\n",
      "Epoch 102/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2246.6177 - mse: 73461352.0000 - mae: 2246.6179 - val_loss: 4108.5474 - val_mse: 203934288.0000 - val_mae: 4108.5474\n",
      "Epoch 103/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2238.4761 - mse: 73148912.0000 - mae: 2238.4761 - val_loss: 4108.9912 - val_mse: 203387936.0000 - val_mae: 4108.9912\n",
      "Epoch 104/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2238.0182 - mse: 72978528.0000 - mae: 2238.0183 - val_loss: 4091.2341 - val_mse: 202815216.0000 - val_mae: 4091.2341\n",
      "Epoch 105/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2229.3041 - mse: 72364496.0000 - mae: 2229.3040 - val_loss: 4087.2166 - val_mse: 202180592.0000 - val_mae: 4087.2166\n",
      "Epoch 106/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2225.8210 - mse: 72244168.0000 - mae: 2225.8210 - val_loss: 4081.8071 - val_mse: 201564192.0000 - val_mae: 4081.8071\n",
      "Epoch 107/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2219.8092 - mse: 71783992.0000 - mae: 2219.8093 - val_loss: 4064.4060 - val_mse: 200806608.0000 - val_mae: 4064.4060\n",
      "Epoch 108/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2207.2780 - mse: 71289992.0000 - mae: 2207.2781 - val_loss: 4061.0286 - val_mse: 200124960.0000 - val_mae: 4061.0286\n",
      "Epoch 109/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2205.4982 - mse: 71221392.0000 - mae: 2205.4980 - val_loss: 4053.4824 - val_mse: 199506288.0000 - val_mae: 4053.4824\n",
      "Epoch 110/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2202.8185 - mse: 70828024.0000 - mae: 2202.8186 - val_loss: 4038.9866 - val_mse: 198826032.0000 - val_mae: 4038.9866\n",
      "Epoch 111/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2196.7932 - mse: 70500896.0000 - mae: 2196.7932 - val_loss: 4025.9734 - val_mse: 198129568.0000 - val_mae: 4025.9734\n",
      "Epoch 112/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2193.9569 - mse: 70359408.0000 - mae: 2193.9570 - val_loss: 4014.2429 - val_mse: 197451776.0000 - val_mae: 4014.2429\n",
      "Epoch 113/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2183.8227 - mse: 69842888.0000 - mae: 2183.8225 - val_loss: 4002.9482 - val_mse: 196678768.0000 - val_mae: 4002.9482\n",
      "Epoch 114/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2172.8998 - mse: 69325128.0000 - mae: 2172.8999 - val_loss: 3995.0166 - val_mse: 195852992.0000 - val_mae: 3995.0166\n",
      "Epoch 115/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2166.6754 - mse: 68978920.0000 - mae: 2166.6753 - val_loss: 3980.8501 - val_mse: 194987824.0000 - val_mae: 3980.8501\n",
      "Epoch 116/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2162.5067 - mse: 68403880.0000 - mae: 2162.5068 - val_loss: 3970.0603 - val_mse: 194121760.0000 - val_mae: 3970.0603\n",
      "Epoch 117/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2153.9014 - mse: 68357384.0000 - mae: 2153.9014 - val_loss: 3947.9136 - val_mse: 193150688.0000 - val_mae: 3947.9136\n",
      "Epoch 118/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2145.9118 - mse: 67644728.0000 - mae: 2145.9121 - val_loss: 3952.7107 - val_mse: 192370336.0000 - val_mae: 3952.7107\n",
      "Epoch 119/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 2138.9877 - mse: 67389992.0000 - mae: 2138.9878 - val_loss: 3937.1731 - val_mse: 191520240.0000 - val_mae: 3937.1731\n",
      "Epoch 120/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2138.6383 - mse: 67030196.0000 - mae: 2138.6382 - val_loss: 3930.0688 - val_mse: 190661680.0000 - val_mae: 3930.0688\n",
      "Epoch 121/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2119.0796 - mse: 66472308.0000 - mae: 2119.0796 - val_loss: 3908.4402 - val_mse: 189738304.0000 - val_mae: 3908.4402\n",
      "Epoch 122/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2116.6681 - mse: 66151892.0000 - mae: 2116.6682 - val_loss: 3905.8789 - val_mse: 188935824.0000 - val_mae: 3905.8789\n",
      "Epoch 123/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 2113.6275 - mse: 65779108.0000 - mae: 2113.6274 - val_loss: 3902.3704 - val_mse: 188124896.0000 - val_mae: 3902.3704\n",
      "Epoch 124/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2104.6374 - mse: 65245244.0000 - mae: 2104.6375 - val_loss: 3888.6331 - val_mse: 187127856.0000 - val_mae: 3888.6331\n",
      "Epoch 125/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2096.0284 - mse: 64982616.0000 - mae: 2096.0288 - val_loss: 3879.7839 - val_mse: 186193680.0000 - val_mae: 3879.7839\n",
      "Epoch 126/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 2097.6221 - mse: 64738612.0000 - mae: 2097.6221 - val_loss: 3878.2266 - val_mse: 185409936.0000 - val_mae: 3878.2266\n",
      "Epoch 127/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2082.0325 - mse: 64063728.0000 - mae: 2082.0325 - val_loss: 3873.8979 - val_mse: 184589776.0000 - val_mae: 3873.8979\n",
      "Epoch 128/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 2076.3108 - mse: 63713972.0000 - mae: 2076.3110 - val_loss: 3864.0898 - val_mse: 183901232.0000 - val_mae: 3864.0898\n",
      "Epoch 129/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2077.3366 - mse: 63787392.0000 - mae: 2077.3364 - val_loss: 3867.3220 - val_mse: 183128864.0000 - val_mae: 3867.3220\n",
      "Epoch 130/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2072.6750 - mse: 63603648.0000 - mae: 2072.6750 - val_loss: 3855.2727 - val_mse: 182149488.0000 - val_mae: 3855.2727\n",
      "Epoch 131/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2060.5188 - mse: 62852292.0000 - mae: 2060.5190 - val_loss: 3859.7407 - val_mse: 181375472.0000 - val_mae: 3859.7407\n",
      "Epoch 132/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2055.9310 - mse: 62524568.0000 - mae: 2055.9312 - val_loss: 3845.4768 - val_mse: 180457536.0000 - val_mae: 3845.4768\n",
      "Epoch 133/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 2041.7648 - mse: 62049816.0000 - mae: 2041.7649 - val_loss: 3842.1160 - val_mse: 179550384.0000 - val_mae: 3842.1160\n",
      "Epoch 134/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2039.2723 - mse: 61787320.0000 - mae: 2039.2725 - val_loss: 3823.8821 - val_mse: 178636016.0000 - val_mae: 3823.8821\n",
      "Epoch 135/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 2027.4898 - mse: 60888040.0000 - mae: 2027.4899 - val_loss: 3824.0613 - val_mse: 177470400.0000 - val_mae: 3824.0613\n",
      "Epoch 136/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2018.9360 - mse: 60019160.0000 - mae: 2018.9360 - val_loss: 3807.9084 - val_mse: 176328784.0000 - val_mae: 3807.9084\n",
      "Epoch 137/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 2016.8555 - mse: 59764816.0000 - mae: 2016.8555 - val_loss: 3795.3765 - val_mse: 175167184.0000 - val_mae: 3795.3765\n",
      "Epoch 138/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2015.5802 - mse: 59972212.0000 - mae: 2015.5802 - val_loss: 3798.9116 - val_mse: 174274992.0000 - val_mae: 3798.9116\n",
      "Epoch 139/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 2006.3984 - mse: 59507288.0000 - mae: 2006.3983 - val_loss: 3774.2417 - val_mse: 172987920.0000 - val_mae: 3774.2417\n",
      "Epoch 140/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1994.8498 - mse: 58779788.0000 - mae: 1994.8499 - val_loss: 3765.3032 - val_mse: 171809152.0000 - val_mae: 3765.3032\n",
      "Epoch 141/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1992.5446 - mse: 58041640.0000 - mae: 1992.5444 - val_loss: 3770.0437 - val_mse: 170779904.0000 - val_mae: 3770.0437\n",
      "Epoch 142/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1995.4892 - mse: 58478568.0000 - mae: 1995.4894 - val_loss: 3747.8428 - val_mse: 169525264.0000 - val_mae: 3747.8428\n",
      "Epoch 143/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 1989.4235 - mse: 58279344.0000 - mae: 1989.4236 - val_loss: 3731.9390 - val_mse: 168347280.0000 - val_mae: 3731.9390\n",
      "Epoch 144/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1974.5338 - mse: 56836520.0000 - mae: 1974.5339 - val_loss: 3720.4141 - val_mse: 167173760.0000 - val_mae: 3720.4141\n",
      "Epoch 145/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1962.3166 - mse: 56023548.0000 - mae: 1962.3165 - val_loss: 3711.6372 - val_mse: 165823008.0000 - val_mae: 3711.6372\n",
      "Epoch 146/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1958.6144 - mse: 55811636.0000 - mae: 1958.6143 - val_loss: 3684.3591 - val_mse: 164281120.0000 - val_mae: 3684.3591\n",
      "Epoch 147/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1940.2401 - mse: 54672540.0000 - mae: 1940.2400 - val_loss: 3670.7910 - val_mse: 162828480.0000 - val_mae: 3670.7910\n",
      "Epoch 148/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1934.0501 - mse: 53969524.0000 - mae: 1934.0500 - val_loss: 3652.4861 - val_mse: 161344800.0000 - val_mae: 3652.4861\n",
      "Epoch 149/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1929.1546 - mse: 53850556.0000 - mae: 1929.1547 - val_loss: 3631.4229 - val_mse: 159855360.0000 - val_mae: 3631.4229\n",
      "Epoch 150/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 1923.9900 - mse: 53010076.0000 - mae: 1923.9900 - val_loss: 3621.6182 - val_mse: 158431216.0000 - val_mae: 3621.6182\n",
      "Epoch 151/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1914.9551 - mse: 52174744.0000 - mae: 1914.9551 - val_loss: 3604.2751 - val_mse: 156854816.0000 - val_mae: 3604.2751\n",
      "Epoch 152/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1916.3697 - mse: 51988380.0000 - mae: 1916.3696 - val_loss: 3582.0291 - val_mse: 155274672.0000 - val_mae: 3582.0291\n",
      "Epoch 153/500\n",
      "8500/8500 [==============================] - 0s 36us/step - loss: 1895.3540 - mse: 51088404.0000 - mae: 1895.3540 - val_loss: 3558.4436 - val_mse: 153597920.0000 - val_mae: 3558.4436\n",
      "Epoch 154/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1899.7180 - mse: 50774608.0000 - mae: 1899.7181 - val_loss: 3542.1633 - val_mse: 151916080.0000 - val_mae: 3542.1633\n",
      "Epoch 155/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 1893.9129 - mse: 50350312.0000 - mae: 1893.9128 - val_loss: 3518.8152 - val_mse: 150160832.0000 - val_mae: 3518.8152\n",
      "Epoch 156/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1875.0101 - mse: 48775724.0000 - mae: 1875.0099 - val_loss: 3503.1187 - val_mse: 148492688.0000 - val_mae: 3503.1187\n",
      "Epoch 157/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1863.7787 - mse: 48498012.0000 - mae: 1863.7786 - val_loss: 3486.5010 - val_mse: 146715808.0000 - val_mae: 3486.5010\n",
      "Epoch 158/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1860.4209 - mse: 48000132.0000 - mae: 1860.4209 - val_loss: 3456.6677 - val_mse: 144813184.0000 - val_mae: 3456.6677\n",
      "Epoch 159/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1847.4518 - mse: 47196408.0000 - mae: 1847.4520 - val_loss: 3434.5320 - val_mse: 142899376.0000 - val_mae: 3434.5320\n",
      "Epoch 160/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 1834.1347 - mse: 45782432.0000 - mae: 1834.1348 - val_loss: 3422.1165 - val_mse: 140976192.0000 - val_mae: 3422.1165\n",
      "Epoch 161/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1831.7388 - mse: 45535868.0000 - mae: 1831.7386 - val_loss: 3388.6316 - val_mse: 138990688.0000 - val_mae: 3388.6316\n",
      "Epoch 162/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1815.7268 - mse: 44669712.0000 - mae: 1815.7268 - val_loss: 3364.3481 - val_mse: 136863152.0000 - val_mae: 3364.3481\n",
      "Epoch 163/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1821.8061 - mse: 44422244.0000 - mae: 1821.8059 - val_loss: 3332.5317 - val_mse: 134777744.0000 - val_mae: 3332.5317\n",
      "Epoch 164/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1789.7751 - mse: 42869960.0000 - mae: 1789.7750 - val_loss: 3301.9272 - val_mse: 132563728.0000 - val_mae: 3301.9272\n",
      "Epoch 165/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1783.4546 - mse: 41950276.0000 - mae: 1783.4546 - val_loss: 3269.1663 - val_mse: 130262032.0000 - val_mae: 3269.1663\n",
      "Epoch 166/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1781.9925 - mse: 41671492.0000 - mae: 1781.9926 - val_loss: 3243.9202 - val_mse: 128277864.0000 - val_mae: 3243.9202\n",
      "Epoch 167/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1760.7372 - mse: 40882636.0000 - mae: 1760.7372 - val_loss: 3205.9619 - val_mse: 125810288.0000 - val_mae: 3205.9619\n",
      "Epoch 168/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1751.6008 - mse: 40047844.0000 - mae: 1751.6008 - val_loss: 3183.8003 - val_mse: 123600800.0000 - val_mae: 3183.8003\n",
      "Epoch 169/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1741.2673 - mse: 39917996.0000 - mae: 1741.2673 - val_loss: 3153.8833 - val_mse: 121318584.0000 - val_mae: 3153.8833\n",
      "Epoch 170/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1721.7415 - mse: 37952740.0000 - mae: 1721.7415 - val_loss: 3104.7197 - val_mse: 118838568.0000 - val_mae: 3104.7197\n",
      "Epoch 171/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1719.3483 - mse: 37860084.0000 - mae: 1719.3484 - val_loss: 3082.8262 - val_mse: 116557848.0000 - val_mae: 3082.8262\n",
      "Epoch 172/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1717.5120 - mse: 37522944.0000 - mae: 1717.5118 - val_loss: 3048.8950 - val_mse: 114430944.0000 - val_mae: 3048.8950\n",
      "Epoch 173/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1671.9509 - mse: 35447784.0000 - mae: 1671.9510 - val_loss: 3014.8203 - val_mse: 112027248.0000 - val_mae: 3014.8203\n",
      "Epoch 174/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1664.0611 - mse: 35001384.0000 - mae: 1664.0610 - val_loss: 2974.4260 - val_mse: 109636936.0000 - val_mae: 2974.4260\n",
      "Epoch 175/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1654.8961 - mse: 34749296.0000 - mae: 1654.8960 - val_loss: 2941.3484 - val_mse: 107309392.0000 - val_mae: 2941.3484\n",
      "Epoch 176/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1642.7650 - mse: 33726528.0000 - mae: 1642.7649 - val_loss: 2893.0605 - val_mse: 104962320.0000 - val_mae: 2893.0605\n",
      "Epoch 177/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1612.5086 - mse: 31357076.0000 - mae: 1612.5083 - val_loss: 2865.8911 - val_mse: 102749104.0000 - val_mae: 2865.8911\n",
      "Epoch 178/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1598.2956 - mse: 31601550.0000 - mae: 1598.2957 - val_loss: 2834.6367 - val_mse: 100626568.0000 - val_mae: 2834.6367\n",
      "Epoch 179/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1586.7066 - mse: 31251436.0000 - mae: 1586.7065 - val_loss: 2793.8110 - val_mse: 98580864.0000 - val_mae: 2793.8110\n",
      "Epoch 180/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1582.5543 - mse: 30838120.0000 - mae: 1582.5542 - val_loss: 2752.7764 - val_mse: 96352904.0000 - val_mae: 2752.7764\n",
      "Epoch 181/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1571.1938 - mse: 30320744.0000 - mae: 1571.1938 - val_loss: 2721.0852 - val_mse: 94567504.0000 - val_mae: 2721.0852\n",
      "Epoch 182/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1539.4086 - mse: 28934258.0000 - mae: 1539.4084 - val_loss: 2683.2832 - val_mse: 92019648.0000 - val_mae: 2683.2832\n",
      "Epoch 183/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1521.4610 - mse: 28216838.0000 - mae: 1521.4612 - val_loss: 2660.1003 - val_mse: 90054040.0000 - val_mae: 2660.1003\n",
      "Epoch 184/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1492.1253 - mse: 27002266.0000 - mae: 1492.1252 - val_loss: 2617.2407 - val_mse: 87750304.0000 - val_mae: 2617.2407\n",
      "Epoch 185/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1465.8272 - mse: 25426104.0000 - mae: 1465.8271 - val_loss: 2578.9348 - val_mse: 85521640.0000 - val_mae: 2578.9348\n",
      "Epoch 186/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1454.9495 - mse: 25864024.0000 - mae: 1454.9497 - val_loss: 2546.7888 - val_mse: 83189656.0000 - val_mae: 2546.7888\n",
      "Epoch 187/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1430.1322 - mse: 24254400.0000 - mae: 1430.1322 - val_loss: 2535.9783 - val_mse: 81421632.0000 - val_mae: 2535.9783\n",
      "Epoch 188/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1403.8771 - mse: 23633588.0000 - mae: 1403.8771 - val_loss: 2518.3789 - val_mse: 80019608.0000 - val_mae: 2518.3789\n",
      "Epoch 189/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1408.5704 - mse: 23854328.0000 - mae: 1408.5704 - val_loss: 2477.2429 - val_mse: 77700408.0000 - val_mae: 2477.2429\n",
      "Epoch 190/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1381.3966 - mse: 22958048.0000 - mae: 1381.3966 - val_loss: 2450.1692 - val_mse: 75269296.0000 - val_mae: 2450.1692\n",
      "Epoch 191/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1365.9648 - mse: 22400396.0000 - mae: 1365.9648 - val_loss: 2417.8750 - val_mse: 73214288.0000 - val_mae: 2417.8750\n",
      "Epoch 192/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1369.4598 - mse: 21882524.0000 - mae: 1369.4596 - val_loss: 2408.7466 - val_mse: 72021056.0000 - val_mae: 2408.7466\n",
      "Epoch 193/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1321.2684 - mse: 20361886.0000 - mae: 1321.2683 - val_loss: 2388.7539 - val_mse: 70672088.0000 - val_mae: 2388.7539\n",
      "Epoch 194/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1297.0280 - mse: 19296324.0000 - mae: 1297.0280 - val_loss: 2362.3481 - val_mse: 69005912.0000 - val_mae: 2362.3481\n",
      "Epoch 195/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1325.1976 - mse: 20888432.0000 - mae: 1325.1976 - val_loss: 2350.2083 - val_mse: 67492352.0000 - val_mae: 2350.2083\n",
      "Epoch 196/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1294.3414 - mse: 19500014.0000 - mae: 1294.3414 - val_loss: 2338.8994 - val_mse: 66023784.0000 - val_mae: 2338.8994\n",
      "Epoch 197/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1287.5460 - mse: 19358022.0000 - mae: 1287.5460 - val_loss: 2311.5078 - val_mse: 64658904.0000 - val_mae: 2311.5078\n",
      "Epoch 198/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 1261.2167 - mse: 17971492.0000 - mae: 1261.2168 - val_loss: 2294.0190 - val_mse: 63432064.0000 - val_mae: 2294.0190\n",
      "Epoch 199/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1256.6131 - mse: 17945364.0000 - mae: 1256.6132 - val_loss: 2293.4500 - val_mse: 62558796.0000 - val_mae: 2293.4500\n",
      "Epoch 200/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1249.1777 - mse: 18054600.0000 - mae: 1249.1776 - val_loss: 2275.7710 - val_mse: 61487072.0000 - val_mae: 2275.7710\n",
      "Epoch 201/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1259.0103 - mse: 18001958.0000 - mae: 1259.0103 - val_loss: 2266.2795 - val_mse: 60168372.0000 - val_mae: 2266.2795\n",
      "Epoch 202/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1226.0591 - mse: 16949604.0000 - mae: 1226.0592 - val_loss: 2249.2246 - val_mse: 58925172.0000 - val_mae: 2249.2246\n",
      "Epoch 203/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1230.6228 - mse: 17264086.0000 - mae: 1230.6228 - val_loss: 2237.4121 - val_mse: 58238660.0000 - val_mae: 2237.4121\n",
      "Epoch 204/500\n",
      "8500/8500 [==============================] - 0s 37us/step - loss: 1231.4230 - mse: 16822834.0000 - mae: 1231.4231 - val_loss: 2229.7722 - val_mse: 57483120.0000 - val_mae: 2229.7722\n",
      "Epoch 205/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1212.5573 - mse: 17387698.0000 - mae: 1212.5571 - val_loss: 2234.5046 - val_mse: 56115076.0000 - val_mae: 2234.5046\n",
      "Epoch 206/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1178.8905 - mse: 15785801.0000 - mae: 1178.8905 - val_loss: 2218.9888 - val_mse: 55342408.0000 - val_mae: 2218.9888\n",
      "Epoch 207/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1201.4509 - mse: 15619325.0000 - mae: 1201.4509 - val_loss: 2226.0754 - val_mse: 54425248.0000 - val_mae: 2226.0754\n",
      "Epoch 208/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1207.8258 - mse: 16632061.0000 - mae: 1207.8258 - val_loss: 2217.6277 - val_mse: 53927060.0000 - val_mae: 2217.6277\n",
      "Epoch 209/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1176.0797 - mse: 14706147.0000 - mae: 1176.0796 - val_loss: 2229.3464 - val_mse: 52719192.0000 - val_mae: 2229.3464\n",
      "Epoch 210/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1183.4924 - mse: 15861596.0000 - mae: 1183.4924 - val_loss: 2212.4021 - val_mse: 52220520.0000 - val_mae: 2212.4021\n",
      "Epoch 211/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1180.8920 - mse: 15084283.0000 - mae: 1180.8920 - val_loss: 2205.3774 - val_mse: 51985244.0000 - val_mae: 2205.3774\n",
      "Epoch 212/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1153.8039 - mse: 14155293.0000 - mae: 1153.8038 - val_loss: 2200.8577 - val_mse: 51303804.0000 - val_mae: 2200.8577\n",
      "Epoch 213/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1165.5652 - mse: 15245371.0000 - mae: 1165.5652 - val_loss: 2194.0723 - val_mse: 50593836.0000 - val_mae: 2194.0723\n",
      "Epoch 214/500\n",
      "8500/8500 [==============================] - 0s 38us/step - loss: 1164.3431 - mse: 15062882.0000 - mae: 1164.3430 - val_loss: 2191.2979 - val_mse: 50166364.0000 - val_mae: 2191.2979\n",
      "Epoch 215/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1163.1300 - mse: 14585622.0000 - mae: 1163.1300 - val_loss: 2185.2695 - val_mse: 49655620.0000 - val_mae: 2185.2695\n",
      "Epoch 216/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1164.2749 - mse: 14662219.0000 - mae: 1164.2748 - val_loss: 2189.1057 - val_mse: 48912532.0000 - val_mae: 2189.1057\n",
      "Epoch 217/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1144.5996 - mse: 13023747.0000 - mae: 1144.5996 - val_loss: 2165.0894 - val_mse: 48380484.0000 - val_mae: 2165.0894\n",
      "Epoch 218/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1147.6620 - mse: 13980112.0000 - mae: 1147.6620 - val_loss: 2158.2061 - val_mse: 47921660.0000 - val_mae: 2158.2061\n",
      "Epoch 219/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1144.0489 - mse: 13670621.0000 - mae: 1144.0490 - val_loss: 2177.9497 - val_mse: 47587272.0000 - val_mae: 2177.9497\n",
      "Epoch 220/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1131.0041 - mse: 14105627.0000 - mae: 1131.0040 - val_loss: 2171.4973 - val_mse: 47103352.0000 - val_mae: 2171.4973\n",
      "Epoch 221/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1124.4438 - mse: 13414721.0000 - mae: 1124.4437 - val_loss: 2151.0420 - val_mse: 46671644.0000 - val_mae: 2151.0420\n",
      "Epoch 222/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1170.7364 - mse: 14267883.0000 - mae: 1170.7365 - val_loss: 2156.8596 - val_mse: 46388856.0000 - val_mae: 2156.8596\n",
      "Epoch 223/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1145.9395 - mse: 13737546.0000 - mae: 1145.9396 - val_loss: 2177.5554 - val_mse: 46085456.0000 - val_mae: 2177.5554\n",
      "Epoch 224/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1147.0603 - mse: 14122266.0000 - mae: 1147.0604 - val_loss: 2149.5854 - val_mse: 45485096.0000 - val_mae: 2149.5854\n",
      "Epoch 225/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1112.4264 - mse: 12931358.0000 - mae: 1112.4265 - val_loss: 2148.3018 - val_mse: 45063700.0000 - val_mae: 2148.3018\n",
      "Epoch 226/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1121.7021 - mse: 12973928.0000 - mae: 1121.7021 - val_loss: 2134.5410 - val_mse: 44861860.0000 - val_mae: 2134.5410\n",
      "Epoch 227/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1126.7049 - mse: 13725541.0000 - mae: 1126.7050 - val_loss: 2154.1992 - val_mse: 44728516.0000 - val_mae: 2154.1992\n",
      "Epoch 228/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1166.5334 - mse: 14469939.0000 - mae: 1166.5334 - val_loss: 2142.4436 - val_mse: 44470280.0000 - val_mae: 2142.4436\n",
      "Epoch 229/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1145.4281 - mse: 14020892.0000 - mae: 1145.4280 - val_loss: 2127.9578 - val_mse: 44283544.0000 - val_mae: 2127.9578\n",
      "Epoch 230/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1132.9209 - mse: 13253469.0000 - mae: 1132.9209 - val_loss: 2131.6626 - val_mse: 44012796.0000 - val_mae: 2131.6626\n",
      "Epoch 231/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1113.7180 - mse: 13588703.0000 - mae: 1113.7180 - val_loss: 2134.5977 - val_mse: 43942060.0000 - val_mae: 2134.5977\n",
      "Epoch 232/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1123.5284 - mse: 13533058.0000 - mae: 1123.5283 - val_loss: 2154.4238 - val_mse: 43804788.0000 - val_mae: 2154.4238\n",
      "Epoch 233/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1114.3307 - mse: 13161748.0000 - mae: 1114.3307 - val_loss: 2122.6865 - val_mse: 43330736.0000 - val_mae: 2122.6865\n",
      "Epoch 234/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1106.5495 - mse: 13056280.0000 - mae: 1106.5494 - val_loss: 2115.1335 - val_mse: 42880524.0000 - val_mae: 2115.1335\n",
      "Epoch 235/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1096.0846 - mse: 12463902.0000 - mae: 1096.0846 - val_loss: 2104.8713 - val_mse: 42566672.0000 - val_mae: 2104.8713\n",
      "Epoch 236/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1128.2070 - mse: 13278960.0000 - mae: 1128.2070 - val_loss: 2099.0708 - val_mse: 42290896.0000 - val_mae: 2099.0708\n",
      "Epoch 237/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1136.2989 - mse: 13642168.0000 - mae: 1136.2988 - val_loss: 2109.2944 - val_mse: 42165132.0000 - val_mae: 2109.2944\n",
      "Epoch 238/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1105.5317 - mse: 13067944.0000 - mae: 1105.5316 - val_loss: 2141.3918 - val_mse: 42140292.0000 - val_mae: 2141.3918\n",
      "Epoch 239/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1123.6053 - mse: 13958452.0000 - mae: 1123.6055 - val_loss: 2088.8560 - val_mse: 41630604.0000 - val_mae: 2088.8560\n",
      "Epoch 240/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1112.8244 - mse: 13222783.0000 - mae: 1112.8243 - val_loss: 2089.4927 - val_mse: 41762264.0000 - val_mae: 2089.4927\n",
      "Epoch 241/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1112.3071 - mse: 12741091.0000 - mae: 1112.3070 - val_loss: 2093.1340 - val_mse: 41647948.0000 - val_mae: 2093.1340\n",
      "Epoch 242/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1122.8956 - mse: 13677048.0000 - mae: 1122.8956 - val_loss: 2123.9443 - val_mse: 41823724.0000 - val_mae: 2123.9443\n",
      "Epoch 243/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1102.1639 - mse: 12857328.0000 - mae: 1102.1639 - val_loss: 2139.4907 - val_mse: 41759160.0000 - val_mae: 2139.4907\n",
      "Epoch 244/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1122.4190 - mse: 13345226.0000 - mae: 1122.4189 - val_loss: 2127.6011 - val_mse: 41464776.0000 - val_mae: 2127.6011\n",
      "Epoch 245/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1114.6386 - mse: 13192417.0000 - mae: 1114.6387 - val_loss: 2119.5012 - val_mse: 41218388.0000 - val_mae: 2119.5012\n",
      "Epoch 246/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1109.0146 - mse: 12712053.0000 - mae: 1109.0146 - val_loss: 2101.3223 - val_mse: 40798616.0000 - val_mae: 2101.3223\n",
      "Epoch 247/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1086.8816 - mse: 12162361.0000 - mae: 1086.8815 - val_loss: 2107.9006 - val_mse: 40907168.0000 - val_mae: 2107.9006\n",
      "Epoch 248/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1125.7406 - mse: 13798463.0000 - mae: 1125.7406 - val_loss: 2139.5959 - val_mse: 40966136.0000 - val_mae: 2139.5959\n",
      "Epoch 249/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1129.8463 - mse: 13752201.0000 - mae: 1129.8463 - val_loss: 2102.4407 - val_mse: 40797428.0000 - val_mae: 2102.4407\n",
      "Epoch 250/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1128.9525 - mse: 14178615.0000 - mae: 1128.9526 - val_loss: 2073.1174 - val_mse: 40312924.0000 - val_mae: 2073.1174\n",
      "Epoch 251/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1110.1582 - mse: 12778960.0000 - mae: 1110.1582 - val_loss: 2093.0632 - val_mse: 40245928.0000 - val_mae: 2093.0632\n",
      "Epoch 252/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1095.0881 - mse: 12765863.0000 - mae: 1095.0881 - val_loss: 2109.7151 - val_mse: 40266676.0000 - val_mae: 2109.7151\n",
      "Epoch 253/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1098.5164 - mse: 12763597.0000 - mae: 1098.5162 - val_loss: 2125.0859 - val_mse: 40313188.0000 - val_mae: 2125.0859\n",
      "Epoch 254/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1115.4644 - mse: 12994409.0000 - mae: 1115.4644 - val_loss: 2120.9358 - val_mse: 40236684.0000 - val_mae: 2120.9358\n",
      "Epoch 255/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1118.3165 - mse: 13264539.0000 - mae: 1118.3165 - val_loss: 2108.9346 - val_mse: 40022600.0000 - val_mae: 2108.9346\n",
      "Epoch 256/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1112.8921 - mse: 13509324.0000 - mae: 1112.8921 - val_loss: 2101.7598 - val_mse: 39999248.0000 - val_mae: 2101.7598\n",
      "Epoch 257/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1130.7298 - mse: 13867792.0000 - mae: 1130.7297 - val_loss: 2069.6577 - val_mse: 39606044.0000 - val_mae: 2069.6577\n",
      "Epoch 258/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1090.5525 - mse: 12508472.0000 - mae: 1090.5525 - val_loss: 2092.1155 - val_mse: 39644732.0000 - val_mae: 2092.1155\n",
      "Epoch 259/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1104.1528 - mse: 12986356.0000 - mae: 1104.1528 - val_loss: 2112.6101 - val_mse: 39651792.0000 - val_mae: 2112.6101\n",
      "Epoch 260/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1096.1660 - mse: 12058326.0000 - mae: 1096.1659 - val_loss: 2103.0659 - val_mse: 39284260.0000 - val_mae: 2103.0659\n",
      "Epoch 261/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1088.8794 - mse: 12322780.0000 - mae: 1088.8793 - val_loss: 2100.6765 - val_mse: 39284844.0000 - val_mae: 2100.6765\n",
      "Epoch 262/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1107.8915 - mse: 12850881.0000 - mae: 1107.8914 - val_loss: 2095.5122 - val_mse: 39014180.0000 - val_mae: 2095.5122\n",
      "Epoch 263/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1096.9483 - mse: 12504602.0000 - mae: 1096.9482 - val_loss: 2115.2178 - val_mse: 39231944.0000 - val_mae: 2115.2178\n",
      "Epoch 264/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1081.2587 - mse: 12297499.0000 - mae: 1081.2587 - val_loss: 2063.4922 - val_mse: 38544168.0000 - val_mae: 2063.4922\n",
      "Epoch 265/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1094.0237 - mse: 12285225.0000 - mae: 1094.0237 - val_loss: 2087.3323 - val_mse: 38525824.0000 - val_mae: 2087.3323\n",
      "Epoch 266/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1103.9920 - mse: 13260423.0000 - mae: 1103.9919 - val_loss: 2082.6418 - val_mse: 38572172.0000 - val_mae: 2082.6418\n",
      "Epoch 267/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1079.1866 - mse: 12144108.0000 - mae: 1079.1866 - val_loss: 2093.4861 - val_mse: 38670768.0000 - val_mae: 2093.4861\n",
      "Epoch 268/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1090.1823 - mse: 12837732.0000 - mae: 1090.1824 - val_loss: 2082.9592 - val_mse: 38723184.0000 - val_mae: 2082.9592\n",
      "Epoch 269/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1119.1878 - mse: 13614433.0000 - mae: 1119.1879 - val_loss: 2083.5664 - val_mse: 38745632.0000 - val_mae: 2083.5664\n",
      "Epoch 270/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1128.9817 - mse: 13759762.0000 - mae: 1128.9817 - val_loss: 2068.1548 - val_mse: 38522972.0000 - val_mae: 2068.1548\n",
      "Epoch 271/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1065.2163 - mse: 11597068.0000 - mae: 1065.2163 - val_loss: 2065.2629 - val_mse: 38406420.0000 - val_mae: 2065.2629\n",
      "Epoch 272/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1084.3938 - mse: 12395977.0000 - mae: 1084.3938 - val_loss: 2098.0681 - val_mse: 38528188.0000 - val_mae: 2098.0681\n",
      "Epoch 273/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1092.3822 - mse: 12365279.0000 - mae: 1092.3822 - val_loss: 2095.0525 - val_mse: 38472696.0000 - val_mae: 2095.0525\n",
      "Epoch 274/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1075.4633 - mse: 12190708.0000 - mae: 1075.4633 - val_loss: 2115.4001 - val_mse: 38633552.0000 - val_mae: 2115.4001\n",
      "Epoch 275/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1098.8588 - mse: 12820746.0000 - mae: 1098.8588 - val_loss: 2106.9177 - val_mse: 38605216.0000 - val_mae: 2106.9177\n",
      "Epoch 276/500\n",
      "8500/8500 [==============================] - 0s 39us/step - loss: 1103.9282 - mse: 12568762.0000 - mae: 1103.9281 - val_loss: 2087.8416 - val_mse: 38306916.0000 - val_mae: 2087.8416\n",
      "Epoch 277/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1075.9388 - mse: 12106612.0000 - mae: 1075.9388 - val_loss: 2114.8477 - val_mse: 38428672.0000 - val_mae: 2114.8477\n",
      "Epoch 278/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1069.9994 - mse: 11847606.0000 - mae: 1069.9995 - val_loss: 2132.4197 - val_mse: 38613552.0000 - val_mae: 2132.4197\n",
      "Epoch 279/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1087.7320 - mse: 12567400.0000 - mae: 1087.7321 - val_loss: 2132.2261 - val_mse: 38379364.0000 - val_mae: 2132.2261\n",
      "Epoch 280/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1110.8859 - mse: 12846454.0000 - mae: 1110.8860 - val_loss: 2107.3459 - val_mse: 38131768.0000 - val_mae: 2107.3459\n",
      "Epoch 281/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1073.1889 - mse: 12122388.0000 - mae: 1073.1888 - val_loss: 2109.5828 - val_mse: 38342812.0000 - val_mae: 2109.5828\n",
      "Epoch 282/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1066.5955 - mse: 11742320.0000 - mae: 1066.5956 - val_loss: 2106.9497 - val_mse: 38056208.0000 - val_mae: 2106.9497\n",
      "Epoch 283/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1089.2109 - mse: 12258291.0000 - mae: 1089.2109 - val_loss: 2119.3250 - val_mse: 38331984.0000 - val_mae: 2119.3250\n",
      "Epoch 284/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1077.4564 - mse: 12423549.0000 - mae: 1077.4564 - val_loss: 2118.3401 - val_mse: 38397272.0000 - val_mae: 2118.3401\n",
      "Epoch 285/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1083.2297 - mse: 12390282.0000 - mae: 1083.2297 - val_loss: 2114.7859 - val_mse: 38440304.0000 - val_mae: 2114.7859\n",
      "Epoch 286/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1077.7448 - mse: 12490677.0000 - mae: 1077.7449 - val_loss: 2118.0632 - val_mse: 38470100.0000 - val_mae: 2118.0632\n",
      "Epoch 287/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1106.4121 - mse: 13190350.0000 - mae: 1106.4121 - val_loss: 2121.4666 - val_mse: 38463284.0000 - val_mae: 2121.4666\n",
      "Epoch 288/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1081.6533 - mse: 12605649.0000 - mae: 1081.6533 - val_loss: 2129.3228 - val_mse: 38746728.0000 - val_mae: 2129.3228\n",
      "Epoch 289/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1074.6491 - mse: 12127218.0000 - mae: 1074.6490 - val_loss: 2121.9885 - val_mse: 38484456.0000 - val_mae: 2121.9885\n",
      "Epoch 290/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1082.4381 - mse: 12323048.0000 - mae: 1082.4380 - val_loss: 2124.0005 - val_mse: 38468100.0000 - val_mae: 2124.0005\n",
      "Epoch 291/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1076.2450 - mse: 12562629.0000 - mae: 1076.2451 - val_loss: 2106.7561 - val_mse: 38205848.0000 - val_mae: 2106.7561\n",
      "Epoch 292/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1064.8308 - mse: 11810762.0000 - mae: 1064.8308 - val_loss: 2130.3728 - val_mse: 38395576.0000 - val_mae: 2130.3728\n",
      "Epoch 293/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1052.8878 - mse: 11486282.0000 - mae: 1052.8879 - val_loss: 2099.2979 - val_mse: 37814320.0000 - val_mae: 2099.2979\n",
      "Epoch 294/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1062.3597 - mse: 11867005.0000 - mae: 1062.3596 - val_loss: 2133.8982 - val_mse: 38253364.0000 - val_mae: 2133.8982\n",
      "Epoch 295/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1077.3580 - mse: 12243192.0000 - mae: 1077.3579 - val_loss: 2111.2351 - val_mse: 38012864.0000 - val_mae: 2111.2351\n",
      "Epoch 296/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1068.7860 - mse: 11828048.0000 - mae: 1068.7860 - val_loss: 2110.6311 - val_mse: 38149072.0000 - val_mae: 2110.6311\n",
      "Epoch 297/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1076.1513 - mse: 12074244.0000 - mae: 1076.1511 - val_loss: 2095.8906 - val_mse: 37838824.0000 - val_mae: 2095.8906\n",
      "Epoch 298/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1083.0914 - mse: 12184466.0000 - mae: 1083.0914 - val_loss: 2117.4363 - val_mse: 37774076.0000 - val_mae: 2117.4363\n",
      "Epoch 299/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1086.9132 - mse: 12197799.0000 - mae: 1086.9133 - val_loss: 2121.2102 - val_mse: 37864832.0000 - val_mae: 2121.2102\n",
      "Epoch 300/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1070.3687 - mse: 12543873.0000 - mae: 1070.3687 - val_loss: 2137.8022 - val_mse: 37881804.0000 - val_mae: 2137.8022\n",
      "Epoch 301/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1054.2157 - mse: 11158761.0000 - mae: 1054.2157 - val_loss: 2124.4968 - val_mse: 37490188.0000 - val_mae: 2124.4968\n",
      "Epoch 302/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1091.4194 - mse: 12653363.0000 - mae: 1091.4194 - val_loss: 2125.3730 - val_mse: 37673720.0000 - val_mae: 2125.3730\n",
      "Epoch 303/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1064.2435 - mse: 11717507.0000 - mae: 1064.2435 - val_loss: 2130.3579 - val_mse: 37665788.0000 - val_mae: 2130.3579\n",
      "Epoch 304/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1076.2513 - mse: 12589005.0000 - mae: 1076.2513 - val_loss: 2133.1448 - val_mse: 37719252.0000 - val_mae: 2133.1448\n",
      "Epoch 305/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1077.8851 - mse: 12341563.0000 - mae: 1077.8850 - val_loss: 2103.9915 - val_mse: 37344812.0000 - val_mae: 2103.9915\n",
      "Epoch 306/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1086.1849 - mse: 12489675.0000 - mae: 1086.1849 - val_loss: 2107.4492 - val_mse: 37181364.0000 - val_mae: 2107.4492\n",
      "Epoch 307/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1065.8560 - mse: 11706420.0000 - mae: 1065.8560 - val_loss: 2075.9714 - val_mse: 36853580.0000 - val_mae: 2075.9714\n",
      "Epoch 308/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1060.0411 - mse: 11854734.0000 - mae: 1060.0411 - val_loss: 2109.4868 - val_mse: 37544468.0000 - val_mae: 2109.4868\n",
      "Epoch 309/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1062.4436 - mse: 11720929.0000 - mae: 1062.4435 - val_loss: 2127.4172 - val_mse: 37832884.0000 - val_mae: 2127.4172\n",
      "Epoch 310/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1055.6390 - mse: 11692627.0000 - mae: 1055.6388 - val_loss: 2127.7690 - val_mse: 37706284.0000 - val_mae: 2127.7690\n",
      "Epoch 311/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1082.5951 - mse: 12796959.0000 - mae: 1082.5952 - val_loss: 2130.8882 - val_mse: 37847672.0000 - val_mae: 2130.8882\n",
      "Epoch 312/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1096.0911 - mse: 13026958.0000 - mae: 1096.0912 - val_loss: 2115.6851 - val_mse: 37163696.0000 - val_mae: 2115.6851\n",
      "Epoch 313/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1049.4834 - mse: 11241037.0000 - mae: 1049.4834 - val_loss: 2126.2200 - val_mse: 37398872.0000 - val_mae: 2126.2200\n",
      "Epoch 314/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1073.9120 - mse: 11982504.0000 - mae: 1073.9119 - val_loss: 2098.3733 - val_mse: 37110628.0000 - val_mae: 2098.3733\n",
      "Epoch 315/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1066.8767 - mse: 11998001.0000 - mae: 1066.8768 - val_loss: 2130.0293 - val_mse: 37549232.0000 - val_mae: 2130.0293\n",
      "Epoch 316/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1082.7240 - mse: 12296872.0000 - mae: 1082.7240 - val_loss: 2138.8818 - val_mse: 37706560.0000 - val_mae: 2138.8818\n",
      "Epoch 317/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1096.4900 - mse: 12511245.0000 - mae: 1096.4900 - val_loss: 2162.8345 - val_mse: 37713240.0000 - val_mae: 2162.8345\n",
      "Epoch 318/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1073.0062 - mse: 12003670.0000 - mae: 1073.0061 - val_loss: 2138.8870 - val_mse: 37721744.0000 - val_mae: 2138.8870\n",
      "Epoch 319/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1066.8911 - mse: 11431294.0000 - mae: 1066.8912 - val_loss: 2154.4651 - val_mse: 37922580.0000 - val_mae: 2154.4651\n",
      "Epoch 320/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1065.3267 - mse: 11465817.0000 - mae: 1065.3265 - val_loss: 2169.2063 - val_mse: 37978700.0000 - val_mae: 2169.2063\n",
      "Epoch 321/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1070.1969 - mse: 11929100.0000 - mae: 1070.1969 - val_loss: 2145.8696 - val_mse: 37450080.0000 - val_mae: 2145.8696\n",
      "Epoch 322/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1067.5097 - mse: 12045079.0000 - mae: 1067.5096 - val_loss: 2103.0830 - val_mse: 36719684.0000 - val_mae: 2103.0830\n",
      "Epoch 323/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1056.6560 - mse: 11743280.0000 - mae: 1056.6560 - val_loss: 2110.8408 - val_mse: 36824088.0000 - val_mae: 2110.8408\n",
      "Epoch 324/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1073.5057 - mse: 12324895.0000 - mae: 1073.5056 - val_loss: 2110.5334 - val_mse: 36962680.0000 - val_mae: 2110.5334\n",
      "Epoch 325/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1068.7254 - mse: 11937972.0000 - mae: 1068.7255 - val_loss: 2109.5491 - val_mse: 36760564.0000 - val_mae: 2109.5491\n",
      "Epoch 326/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1074.8095 - mse: 12077142.0000 - mae: 1074.8094 - val_loss: 2125.2419 - val_mse: 36883348.0000 - val_mae: 2125.2419\n",
      "Epoch 327/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1065.8776 - mse: 12306962.0000 - mae: 1065.8776 - val_loss: 2115.6743 - val_mse: 36879092.0000 - val_mae: 2115.6743\n",
      "Epoch 328/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1072.1346 - mse: 12360996.0000 - mae: 1072.1346 - val_loss: 2133.6851 - val_mse: 36865440.0000 - val_mae: 2133.6851\n",
      "Epoch 329/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1058.8207 - mse: 11848347.0000 - mae: 1058.8207 - val_loss: 2120.4773 - val_mse: 36604328.0000 - val_mae: 2120.4773\n",
      "Epoch 330/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1035.7379 - mse: 10852942.0000 - mae: 1035.7379 - val_loss: 2140.3875 - val_mse: 37162172.0000 - val_mae: 2140.3875\n",
      "Epoch 331/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1060.3879 - mse: 11864063.0000 - mae: 1060.3879 - val_loss: 2132.1697 - val_mse: 37180064.0000 - val_mae: 2132.1697\n",
      "Epoch 332/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1081.9743 - mse: 12718941.0000 - mae: 1081.9741 - val_loss: 2138.0481 - val_mse: 37230340.0000 - val_mae: 2138.0481\n",
      "Epoch 333/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1055.2612 - mse: 11997180.0000 - mae: 1055.2612 - val_loss: 2131.0322 - val_mse: 37134360.0000 - val_mae: 2131.0322\n",
      "Epoch 334/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1058.1589 - mse: 11890555.0000 - mae: 1058.1589 - val_loss: 2115.2073 - val_mse: 37113700.0000 - val_mae: 2115.2073\n",
      "Epoch 335/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1066.1373 - mse: 11554857.0000 - mae: 1066.1375 - val_loss: 2129.2507 - val_mse: 37379652.0000 - val_mae: 2129.2507\n",
      "Epoch 336/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1066.9600 - mse: 12242975.0000 - mae: 1066.9598 - val_loss: 2115.0791 - val_mse: 37277524.0000 - val_mae: 2115.0791\n",
      "Epoch 337/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1050.7734 - mse: 11953463.0000 - mae: 1050.7734 - val_loss: 2124.1335 - val_mse: 37490120.0000 - val_mae: 2124.1335\n",
      "Epoch 338/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1069.5337 - mse: 12144972.0000 - mae: 1069.5337 - val_loss: 2158.2878 - val_mse: 37721204.0000 - val_mae: 2158.2878\n",
      "Epoch 339/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1050.1064 - mse: 11548631.0000 - mae: 1050.1064 - val_loss: 2143.7776 - val_mse: 37680548.0000 - val_mae: 2143.7776\n",
      "Epoch 340/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1078.2221 - mse: 12350502.0000 - mae: 1078.2222 - val_loss: 2161.5186 - val_mse: 37801748.0000 - val_mae: 2161.5186\n",
      "Epoch 341/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1054.1117 - mse: 11492919.0000 - mae: 1054.1117 - val_loss: 2158.1221 - val_mse: 37969216.0000 - val_mae: 2158.1221\n",
      "Epoch 342/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1043.3253 - mse: 11931381.0000 - mae: 1043.3253 - val_loss: 2147.2004 - val_mse: 37883408.0000 - val_mae: 2147.2004\n",
      "Epoch 343/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1024.8586 - mse: 10919054.0000 - mae: 1024.8588 - val_loss: 2155.3096 - val_mse: 37825324.0000 - val_mae: 2155.3096\n",
      "Epoch 344/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1046.2241 - mse: 11561798.0000 - mae: 1046.2241 - val_loss: 2139.6367 - val_mse: 37331632.0000 - val_mae: 2139.6367\n",
      "Epoch 345/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1038.2874 - mse: 10752508.0000 - mae: 1038.2874 - val_loss: 2151.7319 - val_mse: 37392508.0000 - val_mae: 2151.7319\n",
      "Epoch 346/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1038.2083 - mse: 11400681.0000 - mae: 1038.2083 - val_loss: 2130.4185 - val_mse: 37245400.0000 - val_mae: 2130.4185\n",
      "Epoch 347/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1065.7070 - mse: 12660381.0000 - mae: 1065.7069 - val_loss: 2128.4065 - val_mse: 37215092.0000 - val_mae: 2128.4065\n",
      "Epoch 348/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1033.9836 - mse: 11374007.0000 - mae: 1033.9836 - val_loss: 2138.9255 - val_mse: 37425144.0000 - val_mae: 2138.9255\n",
      "Epoch 349/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1064.4489 - mse: 11603010.0000 - mae: 1064.4489 - val_loss: 2130.2539 - val_mse: 37056640.0000 - val_mae: 2130.2539\n",
      "Epoch 350/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1077.5403 - mse: 11953575.0000 - mae: 1077.5405 - val_loss: 2135.4885 - val_mse: 37033604.0000 - val_mae: 2135.4885\n",
      "Epoch 351/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1060.9201 - mse: 11944487.0000 - mae: 1060.9202 - val_loss: 2131.1294 - val_mse: 37110448.0000 - val_mae: 2131.1294\n",
      "Epoch 352/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1043.6177 - mse: 11655447.0000 - mae: 1043.6177 - val_loss: 2130.9626 - val_mse: 37001176.0000 - val_mae: 2130.9626\n",
      "Epoch 353/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1055.4482 - mse: 11778337.0000 - mae: 1055.4481 - val_loss: 2131.5479 - val_mse: 37045788.0000 - val_mae: 2131.5479\n",
      "Epoch 354/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1061.6002 - mse: 12267452.0000 - mae: 1061.6001 - val_loss: 2139.0242 - val_mse: 37057200.0000 - val_mae: 2139.0242\n",
      "Epoch 355/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1052.5788 - mse: 11584820.0000 - mae: 1052.5789 - val_loss: 2158.2493 - val_mse: 36954096.0000 - val_mae: 2158.2493\n",
      "Epoch 356/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1033.0227 - mse: 11229671.0000 - mae: 1033.0228 - val_loss: 2148.8650 - val_mse: 36825552.0000 - val_mae: 2148.8650\n",
      "Epoch 357/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1064.4376 - mse: 11755037.0000 - mae: 1064.4375 - val_loss: 2138.5979 - val_mse: 36978004.0000 - val_mae: 2138.5979\n",
      "Epoch 358/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1025.5795 - mse: 11263598.0000 - mae: 1025.5795 - val_loss: 2145.6748 - val_mse: 36930576.0000 - val_mae: 2145.6748\n",
      "Epoch 359/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1070.9762 - mse: 11816160.0000 - mae: 1070.9762 - val_loss: 2142.3086 - val_mse: 37113200.0000 - val_mae: 2142.3086\n",
      "Epoch 360/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1042.7277 - mse: 11774487.0000 - mae: 1042.7277 - val_loss: 2146.3848 - val_mse: 37189568.0000 - val_mae: 2146.3848\n",
      "Epoch 361/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1042.9260 - mse: 11684864.0000 - mae: 1042.9260 - val_loss: 2162.5654 - val_mse: 37110644.0000 - val_mae: 2162.5654\n",
      "Epoch 362/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1060.1916 - mse: 12395042.0000 - mae: 1060.1917 - val_loss: 2157.8108 - val_mse: 37176368.0000 - val_mae: 2157.8108\n",
      "Epoch 363/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1042.0365 - mse: 11057410.0000 - mae: 1042.0365 - val_loss: 2152.3210 - val_mse: 36978352.0000 - val_mae: 2152.3210\n",
      "Epoch 364/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1046.9517 - mse: 11687045.0000 - mae: 1046.9518 - val_loss: 2145.6426 - val_mse: 36692656.0000 - val_mae: 2145.6426\n",
      "Epoch 365/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1014.3388 - mse: 10188650.0000 - mae: 1014.3387 - val_loss: 2156.6621 - val_mse: 36591632.0000 - val_mae: 2156.6621\n",
      "Epoch 366/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1061.5825 - mse: 12169326.0000 - mae: 1061.5825 - val_loss: 2139.8640 - val_mse: 36259912.0000 - val_mae: 2139.8640\n",
      "Epoch 367/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1046.4882 - mse: 11679045.0000 - mae: 1046.4883 - val_loss: 2136.4822 - val_mse: 36401848.0000 - val_mae: 2136.4822\n",
      "Epoch 368/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1076.0092 - mse: 12441952.0000 - mae: 1076.0092 - val_loss: 2128.5537 - val_mse: 36245628.0000 - val_mae: 2128.5537\n",
      "Epoch 369/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1034.7695 - mse: 11111521.0000 - mae: 1034.7694 - val_loss: 2128.4753 - val_mse: 36318780.0000 - val_mae: 2128.4753\n",
      "Epoch 370/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1035.6761 - mse: 11331234.0000 - mae: 1035.6760 - val_loss: 2134.8503 - val_mse: 36425396.0000 - val_mae: 2134.8503\n",
      "Epoch 371/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1065.3965 - mse: 12519859.0000 - mae: 1065.3965 - val_loss: 2138.4480 - val_mse: 36234440.0000 - val_mae: 2138.4480\n",
      "Epoch 372/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1072.4313 - mse: 11969097.0000 - mae: 1072.4314 - val_loss: 2128.1543 - val_mse: 36066616.0000 - val_mae: 2128.1543\n",
      "Epoch 373/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1032.4374 - mse: 11355503.0000 - mae: 1032.4375 - val_loss: 2127.7402 - val_mse: 36196108.0000 - val_mae: 2127.7402\n",
      "Epoch 374/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1078.5331 - mse: 12619499.0000 - mae: 1078.5330 - val_loss: 2142.8125 - val_mse: 36316948.0000 - val_mae: 2142.8125\n",
      "Epoch 375/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1058.2024 - mse: 11887928.0000 - mae: 1058.2025 - val_loss: 2134.5430 - val_mse: 36351780.0000 - val_mae: 2134.5430\n",
      "Epoch 376/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1032.1812 - mse: 10777047.0000 - mae: 1032.1813 - val_loss: 2139.4609 - val_mse: 36120812.0000 - val_mae: 2139.4609\n",
      "Epoch 377/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1051.5773 - mse: 11717172.0000 - mae: 1051.5774 - val_loss: 2127.3311 - val_mse: 36260028.0000 - val_mae: 2127.3311\n",
      "Epoch 378/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1041.6917 - mse: 11228976.0000 - mae: 1041.6918 - val_loss: 2131.9441 - val_mse: 36317068.0000 - val_mae: 2131.9441\n",
      "Epoch 379/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1025.8522 - mse: 10658805.0000 - mae: 1025.8522 - val_loss: 2139.1721 - val_mse: 36495340.0000 - val_mae: 2139.1721\n",
      "Epoch 380/500\n",
      "8500/8500 [==============================] - 0s 40us/step - loss: 1020.9259 - mse: 10926543.0000 - mae: 1020.9259 - val_loss: 2126.5986 - val_mse: 36437336.0000 - val_mae: 2126.5986\n",
      "Epoch 381/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1044.3252 - mse: 11578613.0000 - mae: 1044.3252 - val_loss: 2127.3511 - val_mse: 35997640.0000 - val_mae: 2127.3511\n",
      "Epoch 382/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1036.8027 - mse: 11486730.0000 - mae: 1036.8029 - val_loss: 2132.2988 - val_mse: 36214668.0000 - val_mae: 2132.2988\n",
      "Epoch 383/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1039.9125 - mse: 11757239.0000 - mae: 1039.9124 - val_loss: 2138.0630 - val_mse: 36151772.0000 - val_mae: 2138.0630\n",
      "Epoch 384/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1015.8055 - mse: 10467438.0000 - mae: 1015.8055 - val_loss: 2126.2637 - val_mse: 35894320.0000 - val_mae: 2126.2637\n",
      "Epoch 385/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1017.8031 - mse: 10762999.0000 - mae: 1017.8032 - val_loss: 2155.8647 - val_mse: 36328152.0000 - val_mae: 2155.8647\n",
      "Epoch 386/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1038.3965 - mse: 11071717.0000 - mae: 1038.3966 - val_loss: 2143.4080 - val_mse: 36442316.0000 - val_mae: 2143.4080\n",
      "Epoch 387/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1043.6838 - mse: 11673450.0000 - mae: 1043.6838 - val_loss: 2157.3652 - val_mse: 36533644.0000 - val_mae: 2157.3652\n",
      "Epoch 388/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1040.5385 - mse: 11811785.0000 - mae: 1040.5385 - val_loss: 2135.1655 - val_mse: 36008592.0000 - val_mae: 2135.1655\n",
      "Epoch 389/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1033.6235 - mse: 11032597.0000 - mae: 1033.6234 - val_loss: 2138.5405 - val_mse: 36185532.0000 - val_mae: 2138.5405\n",
      "Epoch 390/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1058.7087 - mse: 12119428.0000 - mae: 1058.7087 - val_loss: 2144.9473 - val_mse: 36262224.0000 - val_mae: 2144.9473\n",
      "Epoch 391/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1004.6877 - mse: 10345618.0000 - mae: 1004.6877 - val_loss: 2138.8057 - val_mse: 36330676.0000 - val_mae: 2138.8057\n",
      "Epoch 392/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1054.1729 - mse: 11820023.0000 - mae: 1054.1729 - val_loss: 2131.4004 - val_mse: 36265868.0000 - val_mae: 2131.4004\n",
      "Epoch 393/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1025.1175 - mse: 10982344.0000 - mae: 1025.1176 - val_loss: 2123.6384 - val_mse: 35846668.0000 - val_mae: 2123.6384\n",
      "Epoch 394/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1036.7128 - mse: 11548973.0000 - mae: 1036.7128 - val_loss: 2129.4453 - val_mse: 35896420.0000 - val_mae: 2129.4453\n",
      "Epoch 395/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1035.7633 - mse: 11015800.0000 - mae: 1035.7632 - val_loss: 2128.9763 - val_mse: 35983944.0000 - val_mae: 2128.9763\n",
      "Epoch 396/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1042.5930 - mse: 11224021.0000 - mae: 1042.5929 - val_loss: 2134.6909 - val_mse: 36237264.0000 - val_mae: 2134.6909\n",
      "Epoch 397/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1041.2322 - mse: 11344364.0000 - mae: 1041.2322 - val_loss: 2139.0571 - val_mse: 36491496.0000 - val_mae: 2139.0571\n",
      "Epoch 398/500\n",
      "8500/8500 [==============================] - 0s 46us/step - loss: 1032.9255 - mse: 11322152.0000 - mae: 1032.9254 - val_loss: 2142.4414 - val_mse: 36119988.0000 - val_mae: 2142.4414\n",
      "Epoch 399/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1025.9097 - mse: 11161745.0000 - mae: 1025.9098 - val_loss: 2140.5525 - val_mse: 36184516.0000 - val_mae: 2140.5525\n",
      "Epoch 400/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1044.4125 - mse: 11637296.0000 - mae: 1044.4125 - val_loss: 2135.2588 - val_mse: 36229704.0000 - val_mae: 2135.2588\n",
      "Epoch 401/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1022.9954 - mse: 10796569.0000 - mae: 1022.9955 - val_loss: 2134.8091 - val_mse: 36393668.0000 - val_mae: 2134.8091\n",
      "Epoch 402/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1023.3389 - mse: 10336208.0000 - mae: 1023.3389 - val_loss: 2128.0950 - val_mse: 35868648.0000 - val_mae: 2128.0950\n",
      "Epoch 403/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1034.9917 - mse: 10942668.0000 - mae: 1034.9917 - val_loss: 2135.9612 - val_mse: 36260756.0000 - val_mae: 2135.9612\n",
      "Epoch 404/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1045.0008 - mse: 11743613.0000 - mae: 1045.0009 - val_loss: 2129.0713 - val_mse: 36142188.0000 - val_mae: 2129.0713\n",
      "Epoch 405/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1021.1003 - mse: 10941772.0000 - mae: 1021.1003 - val_loss: 2130.0693 - val_mse: 35938496.0000 - val_mae: 2130.0693\n",
      "Epoch 406/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1022.2798 - mse: 11002319.0000 - mae: 1022.2798 - val_loss: 2125.8586 - val_mse: 35644604.0000 - val_mae: 2125.8586\n",
      "Epoch 407/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1055.4986 - mse: 12259349.0000 - mae: 1055.4985 - val_loss: 2146.9236 - val_mse: 35850128.0000 - val_mae: 2146.9236\n",
      "Epoch 408/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1024.9973 - mse: 10841511.0000 - mae: 1024.9974 - val_loss: 2148.1907 - val_mse: 35988328.0000 - val_mae: 2148.1907\n",
      "Epoch 409/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1020.0679 - mse: 11058923.0000 - mae: 1020.0680 - val_loss: 2126.0635 - val_mse: 35706432.0000 - val_mae: 2126.0635\n",
      "Epoch 410/500\n",
      "8500/8500 [==============================] - 0s 41us/step - loss: 1034.0625 - mse: 11365921.0000 - mae: 1034.0626 - val_loss: 2160.7427 - val_mse: 36211812.0000 - val_mae: 2160.7427\n",
      "Epoch 411/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1042.3538 - mse: 11426289.0000 - mae: 1042.3538 - val_loss: 2128.2175 - val_mse: 35904788.0000 - val_mae: 2128.2175\n",
      "Epoch 412/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1028.7559 - mse: 11103017.0000 - mae: 1028.7559 - val_loss: 2122.4580 - val_mse: 35796352.0000 - val_mae: 2122.4580\n",
      "Epoch 413/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1021.7430 - mse: 11205774.0000 - mae: 1021.7430 - val_loss: 2131.0164 - val_mse: 35786080.0000 - val_mae: 2131.0164\n",
      "Epoch 414/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1031.5162 - mse: 11073399.0000 - mae: 1031.5162 - val_loss: 2140.6277 - val_mse: 36044892.0000 - val_mae: 2140.6277\n",
      "Epoch 415/500\n",
      "8500/8500 [==============================] - 0s 47us/step - loss: 1060.6880 - mse: 12573134.0000 - mae: 1060.6880 - val_loss: 2131.9214 - val_mse: 35877444.0000 - val_mae: 2131.9214\n",
      "Epoch 416/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1020.5617 - mse: 10907693.0000 - mae: 1020.5618 - val_loss: 2128.5383 - val_mse: 35849772.0000 - val_mae: 2128.5383\n",
      "Epoch 417/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1033.1059 - mse: 10863904.0000 - mae: 1033.1058 - val_loss: 2150.7373 - val_mse: 36082492.0000 - val_mae: 2150.7373\n",
      "Epoch 418/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1015.5335 - mse: 10393047.0000 - mae: 1015.5334 - val_loss: 2134.2488 - val_mse: 35588984.0000 - val_mae: 2134.2488\n",
      "Epoch 419/500\n",
      "8500/8500 [==============================] - 0s 48us/step - loss: 1039.8180 - mse: 11359408.0000 - mae: 1039.8181 - val_loss: 2139.3467 - val_mse: 36247804.0000 - val_mae: 2139.3467\n",
      "Epoch 420/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1017.8973 - mse: 10863146.0000 - mae: 1017.8973 - val_loss: 2132.3823 - val_mse: 35745792.0000 - val_mae: 2132.3823\n",
      "Epoch 421/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1010.5377 - mse: 10672010.0000 - mae: 1010.5379 - val_loss: 2125.6177 - val_mse: 35476420.0000 - val_mae: 2125.6177\n",
      "Epoch 422/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1021.5697 - mse: 10616994.0000 - mae: 1021.5696 - val_loss: 2135.7920 - val_mse: 35412808.0000 - val_mae: 2135.7920\n",
      "Epoch 423/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1009.3359 - mse: 10402707.0000 - mae: 1009.3360 - val_loss: 2139.7434 - val_mse: 35087656.0000 - val_mae: 2139.7434\n",
      "Epoch 424/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1019.1523 - mse: 10987236.0000 - mae: 1019.1525 - val_loss: 2141.0889 - val_mse: 35294880.0000 - val_mae: 2141.0889\n",
      "Epoch 425/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1044.2743 - mse: 11302709.0000 - mae: 1044.2743 - val_loss: 2157.7295 - val_mse: 35543812.0000 - val_mae: 2157.7295\n",
      "Epoch 426/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1021.3856 - mse: 11093917.0000 - mae: 1021.3856 - val_loss: 2159.6565 - val_mse: 35896624.0000 - val_mae: 2159.6565\n",
      "Epoch 427/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1020.0945 - mse: 10292231.0000 - mae: 1020.0945 - val_loss: 2143.7000 - val_mse: 35713028.0000 - val_mae: 2143.7000\n",
      "Epoch 428/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1029.3258 - mse: 10603917.0000 - mae: 1029.3259 - val_loss: 2120.6528 - val_mse: 35143032.0000 - val_mae: 2120.6528\n",
      "Epoch 429/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1024.9246 - mse: 11040695.0000 - mae: 1024.9246 - val_loss: 2127.7349 - val_mse: 35570624.0000 - val_mae: 2127.7349\n",
      "Epoch 430/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1035.1181 - mse: 11159894.0000 - mae: 1035.1182 - val_loss: 2141.5088 - val_mse: 35259892.0000 - val_mae: 2141.5088\n",
      "Epoch 431/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1027.7475 - mse: 10780491.0000 - mae: 1027.7477 - val_loss: 2126.6528 - val_mse: 35356600.0000 - val_mae: 2126.6528\n",
      "Epoch 432/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1036.8865 - mse: 10925029.0000 - mae: 1036.8866 - val_loss: 2134.6135 - val_mse: 34959464.0000 - val_mae: 2134.6135\n",
      "Epoch 433/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1044.0769 - mse: 11480450.0000 - mae: 1044.0769 - val_loss: 2149.0256 - val_mse: 35288696.0000 - val_mae: 2149.0256\n",
      "Epoch 434/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1027.2565 - mse: 10804235.0000 - mae: 1027.2565 - val_loss: 2131.1687 - val_mse: 34946364.0000 - val_mae: 2131.1687\n",
      "Epoch 435/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1013.7113 - mse: 10207117.0000 - mae: 1013.7113 - val_loss: 2114.1433 - val_mse: 34761036.0000 - val_mae: 2114.1433\n",
      "Epoch 436/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1055.3029 - mse: 11663665.0000 - mae: 1055.3030 - val_loss: 2136.1116 - val_mse: 34807628.0000 - val_mae: 2136.1116\n",
      "Epoch 437/500\n",
      "8500/8500 [==============================] - 0s 47us/step - loss: 1025.8136 - mse: 11344760.0000 - mae: 1025.8136 - val_loss: 2120.4180 - val_mse: 34379252.0000 - val_mae: 2120.4180\n",
      "Epoch 438/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1027.7342 - mse: 10444659.0000 - mae: 1027.7344 - val_loss: 2130.7930 - val_mse: 34380332.0000 - val_mae: 2130.7930\n",
      "Epoch 439/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 994.8367 - mse: 10032928.0000 - mae: 994.8367 - val_loss: 2140.4849 - val_mse: 34948092.0000 - val_mae: 2140.4849\n",
      "Epoch 440/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1042.2100 - mse: 11948336.0000 - mae: 1042.2100 - val_loss: 2139.8706 - val_mse: 34994740.0000 - val_mae: 2139.8706\n",
      "Epoch 441/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1047.2575 - mse: 11707365.0000 - mae: 1047.2576 - val_loss: 2131.8679 - val_mse: 34513320.0000 - val_mae: 2131.8679\n",
      "Epoch 442/500\n",
      "8500/8500 [==============================] - 0s 48us/step - loss: 1027.6978 - mse: 11290059.0000 - mae: 1027.6976 - val_loss: 2125.4968 - val_mse: 34619580.0000 - val_mae: 2125.4968\n",
      "Epoch 443/500\n",
      "8500/8500 [==============================] - 0s 48us/step - loss: 1006.1104 - mse: 10224249.0000 - mae: 1006.1105 - val_loss: 2144.0332 - val_mse: 34595760.0000 - val_mae: 2144.0332\n",
      "Epoch 444/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1049.4199 - mse: 11964399.0000 - mae: 1049.4199 - val_loss: 2119.1943 - val_mse: 34152252.0000 - val_mae: 2119.1943\n",
      "Epoch 445/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 985.9803 - mse: 9740643.0000 - mae: 985.9802 - val_loss: 2138.9517 - val_mse: 34262320.0000 - val_mae: 2138.9517\n",
      "Epoch 446/500\n",
      "8500/8500 [==============================] - 0s 46us/step - loss: 998.3584 - mse: 9947742.0000 - mae: 998.3583 - val_loss: 2109.9651 - val_mse: 34087248.0000 - val_mae: 2109.9651\n",
      "Epoch 447/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1011.2880 - mse: 10663165.0000 - mae: 1011.2879 - val_loss: 2089.1436 - val_mse: 33854160.0000 - val_mae: 2089.1436\n",
      "Epoch 448/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1029.4079 - mse: 11187709.0000 - mae: 1029.4078 - val_loss: 2122.2200 - val_mse: 33879664.0000 - val_mae: 2122.2200\n",
      "Epoch 449/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1009.7856 - mse: 10503523.0000 - mae: 1009.7854 - val_loss: 2109.4988 - val_mse: 33824304.0000 - val_mae: 2109.4988\n",
      "Epoch 450/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1008.0148 - mse: 10333041.0000 - mae: 1008.0148 - val_loss: 2094.3860 - val_mse: 33805428.0000 - val_mae: 2094.3860\n",
      "Epoch 451/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 997.7300 - mse: 10011512.0000 - mae: 997.7299 - val_loss: 2121.3547 - val_mse: 34288576.0000 - val_mae: 2121.3547\n",
      "Epoch 452/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1049.8139 - mse: 11634632.0000 - mae: 1049.8140 - val_loss: 2129.8770 - val_mse: 34587756.0000 - val_mae: 2129.8770\n",
      "Epoch 453/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1048.8935 - mse: 11299124.0000 - mae: 1048.8934 - val_loss: 2105.6733 - val_mse: 34376564.0000 - val_mae: 2105.6733\n",
      "Epoch 454/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1044.6630 - mse: 11526636.0000 - mae: 1044.6631 - val_loss: 2122.6309 - val_mse: 34647924.0000 - val_mae: 2122.6309\n",
      "Epoch 455/500\n",
      "8500/8500 [==============================] - 0s 42us/step - loss: 1004.6373 - mse: 10180568.0000 - mae: 1004.6373 - val_loss: 2118.2148 - val_mse: 34573360.0000 - val_mae: 2118.2148\n",
      "Epoch 456/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1000.7686 - mse: 10406394.0000 - mae: 1000.7686 - val_loss: 2118.6350 - val_mse: 34451548.0000 - val_mae: 2118.6350\n",
      "Epoch 457/500\n",
      "8500/8500 [==============================] - 0s 46us/step - loss: 1026.5201 - mse: 11065536.0000 - mae: 1026.5200 - val_loss: 2133.2029 - val_mse: 34836120.0000 - val_mae: 2133.2029\n",
      "Epoch 458/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1010.7832 - mse: 10397588.0000 - mae: 1010.7833 - val_loss: 2124.8115 - val_mse: 34596072.0000 - val_mae: 2124.8115\n",
      "Epoch 459/500\n",
      "8500/8500 [==============================] - 0s 47us/step - loss: 1005.7190 - mse: 10056646.0000 - mae: 1005.7189 - val_loss: 2136.8381 - val_mse: 34608104.0000 - val_mae: 2136.8381\n",
      "Epoch 460/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 985.7604 - mse: 9650390.0000 - mae: 985.7604 - val_loss: 2106.8555 - val_mse: 34218568.0000 - val_mae: 2106.8555\n",
      "Epoch 461/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1001.4858 - mse: 10380307.0000 - mae: 1001.4858 - val_loss: 2108.4800 - val_mse: 34388188.0000 - val_mae: 2108.4800\n",
      "Epoch 462/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 992.7817 - mse: 9841728.0000 - mae: 992.7817 - val_loss: 2101.6897 - val_mse: 33797788.0000 - val_mae: 2101.6897\n",
      "Epoch 463/500\n",
      "8500/8500 [==============================] - 0s 46us/step - loss: 1013.7485 - mse: 10571273.0000 - mae: 1013.7485 - val_loss: 2112.9685 - val_mse: 34188524.0000 - val_mae: 2112.9685\n",
      "Epoch 464/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1019.0634 - mse: 11195328.0000 - mae: 1019.0634 - val_loss: 2110.6567 - val_mse: 33666868.0000 - val_mae: 2110.6567\n",
      "Epoch 465/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1046.1718 - mse: 11489008.0000 - mae: 1046.1718 - val_loss: 2102.2817 - val_mse: 33324376.0000 - val_mae: 2102.2817\n",
      "Epoch 466/500\n",
      "8500/8500 [==============================] - 0s 46us/step - loss: 1008.0908 - mse: 10442475.0000 - mae: 1008.0908 - val_loss: 2068.5198 - val_mse: 32563706.0000 - val_mae: 2068.5198\n",
      "Epoch 467/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1008.5946 - mse: 10842420.0000 - mae: 1008.5946 - val_loss: 2075.0005 - val_mse: 32614090.0000 - val_mae: 2075.0005\n",
      "Epoch 468/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1030.4271 - mse: 11191646.0000 - mae: 1030.4270 - val_loss: 2063.1460 - val_mse: 32535060.0000 - val_mae: 2063.1460\n",
      "Epoch 469/500\n",
      "8500/8500 [==============================] - 0s 47us/step - loss: 1021.8952 - mse: 10753453.0000 - mae: 1021.8953 - val_loss: 2095.5420 - val_mse: 33260238.0000 - val_mae: 2095.5420\n",
      "Epoch 470/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1011.4756 - mse: 10545649.0000 - mae: 1011.4755 - val_loss: 2087.7798 - val_mse: 33727352.0000 - val_mae: 2087.7798\n",
      "Epoch 471/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1018.5910 - mse: 10785173.0000 - mae: 1018.5911 - val_loss: 2075.0244 - val_mse: 33221634.0000 - val_mae: 2075.0244\n",
      "Epoch 472/500\n",
      "8500/8500 [==============================] - 0s 46us/step - loss: 1036.0562 - mse: 11357565.0000 - mae: 1036.0563 - val_loss: 2095.0928 - val_mse: 33001348.0000 - val_mae: 2095.0928\n",
      "Epoch 473/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1010.3713 - mse: 10408154.0000 - mae: 1010.3712 - val_loss: 2109.4275 - val_mse: 33080186.0000 - val_mae: 2109.4275\n",
      "Epoch 474/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1005.1117 - mse: 10319080.0000 - mae: 1005.1118 - val_loss: 2125.1675 - val_mse: 33434528.0000 - val_mae: 2125.1675\n",
      "Epoch 475/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 996.4375 - mse: 10045998.0000 - mae: 996.4375 - val_loss: 2097.4585 - val_mse: 33356590.0000 - val_mae: 2097.4585\n",
      "Epoch 476/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 987.6986 - mse: 10020571.0000 - mae: 987.6986 - val_loss: 2102.0205 - val_mse: 33036950.0000 - val_mae: 2102.0205\n",
      "Epoch 477/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1026.8922 - mse: 10775960.0000 - mae: 1026.8921 - val_loss: 2078.8469 - val_mse: 32047346.0000 - val_mae: 2078.8469\n",
      "Epoch 478/500\n",
      "8500/8500 [==============================] - 0s 49us/step - loss: 1011.4358 - mse: 10450824.0000 - mae: 1011.4358 - val_loss: 2098.6057 - val_mse: 32267434.0000 - val_mae: 2098.6057\n",
      "Epoch 479/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1009.6940 - mse: 10577472.0000 - mae: 1009.6940 - val_loss: 2097.0493 - val_mse: 32343852.0000 - val_mae: 2097.0493\n",
      "Epoch 480/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1007.7924 - mse: 10347113.0000 - mae: 1007.7924 - val_loss: 2073.3364 - val_mse: 32012890.0000 - val_mae: 2073.3364\n",
      "Epoch 481/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1043.2257 - mse: 11448441.0000 - mae: 1043.2257 - val_loss: 2121.6299 - val_mse: 33126646.0000 - val_mae: 2121.6299\n",
      "Epoch 482/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1011.2351 - mse: 10372681.0000 - mae: 1011.2350 - val_loss: 2113.3096 - val_mse: 33193980.0000 - val_mae: 2113.3096\n",
      "Epoch 483/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1018.2698 - mse: 10393978.0000 - mae: 1018.2698 - val_loss: 2130.1350 - val_mse: 33540050.0000 - val_mae: 2130.1350\n",
      "Epoch 484/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1018.6735 - mse: 10792203.0000 - mae: 1018.6734 - val_loss: 2103.5698 - val_mse: 32623430.0000 - val_mae: 2103.5698\n",
      "Epoch 485/500\n",
      "8500/8500 [==============================] - 0s 48us/step - loss: 986.0612 - mse: 10149928.0000 - mae: 986.0611 - val_loss: 2090.9094 - val_mse: 32273808.0000 - val_mae: 2090.9094\n",
      "Epoch 486/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1011.2842 - mse: 10452866.0000 - mae: 1011.2842 - val_loss: 2080.1411 - val_mse: 32261598.0000 - val_mae: 2080.1411\n",
      "Epoch 487/500\n",
      "8500/8500 [==============================] - 0s 49us/step - loss: 999.0978 - mse: 10316143.0000 - mae: 999.0979 - val_loss: 2076.0508 - val_mse: 32327310.0000 - val_mae: 2076.0508\n",
      "Epoch 488/500\n",
      "8500/8500 [==============================] - 0s 46us/step - loss: 1023.7147 - mse: 10915498.0000 - mae: 1023.7147 - val_loss: 2067.9514 - val_mse: 32247826.0000 - val_mae: 2067.9514\n",
      "Epoch 489/500\n",
      "8500/8500 [==============================] - 0s 46us/step - loss: 978.2649 - mse: 9707973.0000 - mae: 978.2649 - val_loss: 2096.1721 - val_mse: 33110468.0000 - val_mae: 2096.1721\n",
      "Epoch 490/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 999.8369 - mse: 10062478.0000 - mae: 999.8369 - val_loss: 2091.0415 - val_mse: 33113490.0000 - val_mae: 2091.0415\n",
      "Epoch 491/500\n",
      "8500/8500 [==============================] - 0s 49us/step - loss: 1011.7157 - mse: 10232735.0000 - mae: 1011.7158 - val_loss: 2130.0720 - val_mse: 33663044.0000 - val_mae: 2130.0720\n",
      "Epoch 492/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1034.2837 - mse: 11283705.0000 - mae: 1034.2838 - val_loss: 2128.2146 - val_mse: 33793688.0000 - val_mae: 2128.2146\n",
      "Epoch 493/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 982.3773 - mse: 9825492.0000 - mae: 982.3774 - val_loss: 2100.5376 - val_mse: 32955986.0000 - val_mae: 2100.5376\n",
      "Epoch 494/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1010.8973 - mse: 10861969.0000 - mae: 1010.8973 - val_loss: 2087.9038 - val_mse: 32274252.0000 - val_mae: 2087.9038\n",
      "Epoch 495/500\n",
      "8500/8500 [==============================] - 0s 45us/step - loss: 1010.1766 - mse: 10753911.0000 - mae: 1010.1767 - val_loss: 2098.6780 - val_mse: 32324790.0000 - val_mae: 2098.6780\n",
      "Epoch 496/500\n",
      "8500/8500 [==============================] - 0s 47us/step - loss: 1005.5274 - mse: 10069808.0000 - mae: 1005.5274 - val_loss: 2084.4500 - val_mse: 32292034.0000 - val_mae: 2084.4500\n",
      "Epoch 497/500\n",
      "8500/8500 [==============================] - 0s 46us/step - loss: 991.6039 - mse: 10081706.0000 - mae: 991.6040 - val_loss: 2096.0852 - val_mse: 32558496.0000 - val_mae: 2096.0852\n",
      "Epoch 498/500\n",
      "8500/8500 [==============================] - 0s 43us/step - loss: 1023.0481 - mse: 10817193.0000 - mae: 1023.0480 - val_loss: 2123.3972 - val_mse: 33137646.0000 - val_mae: 2123.3972\n",
      "Epoch 499/500\n",
      "8500/8500 [==============================] - 0s 44us/step - loss: 1010.6149 - mse: 10541628.0000 - mae: 1010.6148 - val_loss: 2127.7339 - val_mse: 32812122.0000 - val_mae: 2127.7339\n",
      "Epoch 500/500\n",
      "8500/8500 [==============================] - 0s 46us/step - loss: 997.2247 - mse: 10025924.0000 - mae: 997.2247 - val_loss: 2143.9373 - val_mse: 33051024.0000 - val_mae: 2143.9373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 500 samples\n",
      "Epoch 1/500\n",
      "10000/10000 [==============================] - 3s 258us/step - loss: 2829.7234 - mse: 99489840.0000 - mae: 2829.7234 - val_loss: 5603.9497 - val_mse: 179987008.0000 - val_mae: 5603.9497\n",
      "Epoch 2/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2805.9635 - mse: 99402408.0000 - mae: 2805.9636 - val_loss: 5564.6309 - val_mse: 179661456.0000 - val_mae: 5564.6309\n",
      "Epoch 3/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2788.8507 - mse: 99262064.0000 - mae: 2788.8508 - val_loss: 5532.0054 - val_mse: 179309264.0000 - val_mae: 5532.0054\n",
      "Epoch 4/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2780.8447 - mse: 99134880.0000 - mae: 2780.8447 - val_loss: 5514.0044 - val_mse: 179059440.0000 - val_mae: 5514.0044\n",
      "Epoch 5/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2778.2781 - mse: 99037584.0000 - mae: 2778.2783 - val_loss: 5506.4302 - val_mse: 178912800.0000 - val_mae: 5506.4302\n",
      "Epoch 6/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2776.7144 - mse: 98962568.0000 - mae: 2776.7146 - val_loss: 5501.3911 - val_mse: 178795920.0000 - val_mae: 5501.3911\n",
      "Epoch 7/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2774.2267 - mse: 98905144.0000 - mae: 2774.2266 - val_loss: 5496.1909 - val_mse: 178673632.0000 - val_mae: 5496.1909\n",
      "Epoch 8/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2772.6349 - mse: 98827456.0000 - mae: 2772.6350 - val_loss: 5490.2080 - val_mse: 178469552.0000 - val_mae: 5490.2080\n",
      "Epoch 9/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2768.9290 - mse: 98691560.0000 - mae: 2768.9287 - val_loss: 5484.5825 - val_mse: 178255616.0000 - val_mae: 5484.5825\n",
      "Epoch 10/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2766.4766 - mse: 98594752.0000 - mae: 2766.4763 - val_loss: 5480.2100 - val_mse: 178102336.0000 - val_mae: 5480.2100\n",
      "Epoch 11/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2765.1280 - mse: 98557968.0000 - mae: 2765.1284 - val_loss: 5477.2847 - val_mse: 177998608.0000 - val_mae: 5477.2847\n",
      "Epoch 12/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2764.4773 - mse: 98493592.0000 - mae: 2764.4773 - val_loss: 5475.6128 - val_mse: 177928112.0000 - val_mae: 5475.6128\n",
      "Epoch 13/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2761.3589 - mse: 98451992.0000 - mae: 2761.3589 - val_loss: 5471.0869 - val_mse: 177807376.0000 - val_mae: 5471.0869\n",
      "Epoch 14/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2760.3363 - mse: 98407472.0000 - mae: 2760.3364 - val_loss: 5467.7290 - val_mse: 177715600.0000 - val_mae: 5467.7290\n",
      "Epoch 15/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 2759.3001 - mse: 98291984.0000 - mae: 2759.3003 - val_loss: 5464.2026 - val_mse: 177523456.0000 - val_mae: 5464.2026\n",
      "Epoch 16/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2758.2259 - mse: 98250840.0000 - mae: 2758.2258 - val_loss: 5462.3027 - val_mse: 177463552.0000 - val_mae: 5462.3027\n",
      "Epoch 17/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2755.7041 - mse: 98206312.0000 - mae: 2755.7041 - val_loss: 5460.0117 - val_mse: 177399408.0000 - val_mae: 5460.0117\n",
      "Epoch 18/500\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 2754.9792 - mse: 98176752.0000 - mae: 2754.9792 - val_loss: 5457.7490 - val_mse: 177337856.0000 - val_mae: 5457.7490\n",
      "Epoch 19/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2755.5394 - mse: 98149688.0000 - mae: 2755.5396 - val_loss: 5457.2905 - val_mse: 177318448.0000 - val_mae: 5457.2905\n",
      "Epoch 20/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2754.4412 - mse: 98144752.0000 - mae: 2754.4414 - val_loss: 5456.5269 - val_mse: 177293520.0000 - val_mae: 5456.5269\n",
      "Epoch 21/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2753.5881 - mse: 98123832.0000 - mae: 2753.5879 - val_loss: 5454.8501 - val_mse: 177248800.0000 - val_mae: 5454.8501\n",
      "Epoch 22/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2753.6990 - mse: 98105744.0000 - mae: 2753.6987 - val_loss: 5453.4224 - val_mse: 177191776.0000 - val_mae: 5453.4224\n",
      "Epoch 23/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2752.9959 - mse: 98056104.0000 - mae: 2752.9961 - val_loss: 5451.7588 - val_mse: 177123488.0000 - val_mae: 5451.7588\n",
      "Epoch 24/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2752.9149 - mse: 98033536.0000 - mae: 2752.9153 - val_loss: 5450.4038 - val_mse: 177070304.0000 - val_mae: 5450.4038\n",
      "Epoch 25/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2753.0909 - mse: 98003808.0000 - mae: 2753.0911 - val_loss: 5450.6548 - val_mse: 177053376.0000 - val_mae: 5450.6548\n",
      "Epoch 26/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2750.9515 - mse: 97997432.0000 - mae: 2750.9514 - val_loss: 5448.2671 - val_mse: 176969920.0000 - val_mae: 5448.2671\n",
      "Epoch 27/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2751.6121 - mse: 97991072.0000 - mae: 2751.6123 - val_loss: 5448.5352 - val_mse: 176968240.0000 - val_mae: 5448.5352\n",
      "Epoch 28/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2751.6655 - mse: 97967696.0000 - mae: 2751.6658 - val_loss: 5448.9805 - val_mse: 176965888.0000 - val_mae: 5448.9805\n",
      "Epoch 29/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2749.8728 - mse: 97968904.0000 - mae: 2749.8728 - val_loss: 5447.5776 - val_mse: 176926448.0000 - val_mae: 5447.5776\n",
      "Epoch 30/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2750.7213 - mse: 97955912.0000 - mae: 2750.7214 - val_loss: 5447.2642 - val_mse: 176916688.0000 - val_mae: 5447.2642\n",
      "Epoch 31/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2751.1789 - mse: 97926720.0000 - mae: 2751.1790 - val_loss: 5447.7158 - val_mse: 176922400.0000 - val_mae: 5447.7158\n",
      "Epoch 32/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2750.5219 - mse: 97934976.0000 - mae: 2750.5220 - val_loss: 5447.4277 - val_mse: 176911632.0000 - val_mae: 5447.4277\n",
      "Epoch 33/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2749.5541 - mse: 97910216.0000 - mae: 2749.5542 - val_loss: 5446.0830 - val_mse: 176877200.0000 - val_mae: 5446.0830\n",
      "Epoch 34/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2751.5187 - mse: 97954592.0000 - mae: 2751.5183 - val_loss: 5447.4707 - val_mse: 176909040.0000 - val_mae: 5447.4707\n",
      "Epoch 35/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2750.3097 - mse: 97916656.0000 - mae: 2750.3096 - val_loss: 5446.6260 - val_mse: 176887376.0000 - val_mae: 5446.6260\n",
      "Epoch 36/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2749.6881 - mse: 97923344.0000 - mae: 2749.6882 - val_loss: 5446.4321 - val_mse: 176880768.0000 - val_mae: 5446.4321\n",
      "Epoch 37/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2749.8256 - mse: 97898456.0000 - mae: 2749.8257 - val_loss: 5446.2271 - val_mse: 176872112.0000 - val_mae: 5446.2271\n",
      "Epoch 38/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2748.7653 - mse: 97900224.0000 - mae: 2748.7656 - val_loss: 5446.5024 - val_mse: 176869424.0000 - val_mae: 5446.5024\n",
      "Epoch 39/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2749.1308 - mse: 97881024.0000 - mae: 2749.1311 - val_loss: 5447.0039 - val_mse: 176858832.0000 - val_mae: 5447.0039\n",
      "Epoch 40/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 2748.2292 - mse: 97890168.0000 - mae: 2748.2292 - val_loss: 5447.5879 - val_mse: 176840464.0000 - val_mae: 5447.5879\n",
      "Epoch 41/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2741.2264 - mse: 97843472.0000 - mae: 2741.2263 - val_loss: 5480.4385 - val_mse: 176982640.0000 - val_mae: 5480.4385\n",
      "Epoch 42/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2708.2223 - mse: 97603936.0000 - mae: 2708.2224 - val_loss: 5476.9438 - val_mse: 175856080.0000 - val_mae: 5476.9438\n",
      "Epoch 43/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2678.4389 - mse: 96943248.0000 - mae: 2678.4390 - val_loss: 5447.1519 - val_mse: 174700352.0000 - val_mae: 5447.1519\n",
      "Epoch 44/500\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 2658.5878 - mse: 96395560.0000 - mae: 2658.5876 - val_loss: 5431.7490 - val_mse: 173734096.0000 - val_mae: 5431.7490\n",
      "Epoch 45/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2639.7050 - mse: 95797872.0000 - mae: 2639.7051 - val_loss: 5395.1514 - val_mse: 172655808.0000 - val_mae: 5395.1514\n",
      "Epoch 46/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2627.3031 - mse: 95249384.0000 - mae: 2627.3027 - val_loss: 5347.7598 - val_mse: 171481952.0000 - val_mae: 5347.7598\n",
      "Epoch 47/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2612.1690 - mse: 94796064.0000 - mae: 2612.1687 - val_loss: 5331.6616 - val_mse: 170604400.0000 - val_mae: 5331.6616\n",
      "Epoch 48/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2604.6306 - mse: 94307632.0000 - mae: 2604.6306 - val_loss: 5305.2998 - val_mse: 169661424.0000 - val_mae: 5305.2998\n",
      "Epoch 49/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2596.0201 - mse: 93869856.0000 - mae: 2596.0200 - val_loss: 5272.1621 - val_mse: 168689072.0000 - val_mae: 5272.1621\n",
      "Epoch 50/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2586.4582 - mse: 93325944.0000 - mae: 2586.4585 - val_loss: 5254.8726 - val_mse: 167838016.0000 - val_mae: 5254.8726\n",
      "Epoch 51/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2576.2958 - mse: 92898648.0000 - mae: 2576.2959 - val_loss: 5246.8691 - val_mse: 167088848.0000 - val_mae: 5246.8691\n",
      "Epoch 52/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2569.5642 - mse: 92458840.0000 - mae: 2569.5647 - val_loss: 5200.3779 - val_mse: 165953648.0000 - val_mae: 5200.3779\n",
      "Epoch 53/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2562.6776 - mse: 92030632.0000 - mae: 2562.6777 - val_loss: 5182.5737 - val_mse: 165082128.0000 - val_mae: 5182.5737\n",
      "Epoch 54/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2550.5032 - mse: 91372888.0000 - mae: 2550.5029 - val_loss: 5166.6499 - val_mse: 164173552.0000 - val_mae: 5166.6499\n",
      "Epoch 55/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2539.9296 - mse: 90784568.0000 - mae: 2539.9299 - val_loss: 5145.9561 - val_mse: 163169824.0000 - val_mae: 5145.9561\n",
      "Epoch 56/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2531.0943 - mse: 90382160.0000 - mae: 2531.0942 - val_loss: 5122.3491 - val_mse: 162182176.0000 - val_mae: 5122.3491\n",
      "Epoch 57/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 2519.0022 - mse: 89782600.0000 - mae: 2519.0020 - val_loss: 5099.7949 - val_mse: 161170704.0000 - val_mae: 5099.7949\n",
      "Epoch 58/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2507.1624 - mse: 89153240.0000 - mae: 2507.1624 - val_loss: 5065.0298 - val_mse: 159907696.0000 - val_mae: 5065.0298\n",
      "Epoch 59/500\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 2496.7348 - mse: 88686096.0000 - mae: 2496.7346 - val_loss: 5058.0132 - val_mse: 158975392.0000 - val_mae: 5058.0132\n",
      "Epoch 60/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2490.2894 - mse: 87872104.0000 - mae: 2490.2893 - val_loss: 5039.0518 - val_mse: 157976096.0000 - val_mae: 5039.0518\n",
      "Epoch 61/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2477.9654 - mse: 87535592.0000 - mae: 2477.9653 - val_loss: 5022.4785 - val_mse: 156923856.0000 - val_mae: 5022.4785\n",
      "Epoch 62/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2472.2263 - mse: 87137920.0000 - mae: 2472.2263 - val_loss: 4996.5815 - val_mse: 155810848.0000 - val_mae: 4996.5815\n",
      "Epoch 63/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2461.6815 - mse: 86515256.0000 - mae: 2461.6816 - val_loss: 4973.9077 - val_mse: 154802816.0000 - val_mae: 4973.9077\n",
      "Epoch 64/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2453.9777 - mse: 85934440.0000 - mae: 2453.9775 - val_loss: 4950.5796 - val_mse: 153614064.0000 - val_mae: 4950.5796\n",
      "Epoch 65/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2442.4491 - mse: 85161336.0000 - mae: 2442.4492 - val_loss: 4931.3628 - val_mse: 152590336.0000 - val_mae: 4931.3628\n",
      "Epoch 66/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2437.8509 - mse: 84842048.0000 - mae: 2437.8508 - val_loss: 4919.2017 - val_mse: 151623440.0000 - val_mae: 4919.2017\n",
      "Epoch 67/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2423.8270 - mse: 84028128.0000 - mae: 2423.8269 - val_loss: 4881.2993 - val_mse: 150364720.0000 - val_mae: 4881.2993\n",
      "Epoch 68/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2419.0542 - mse: 83487640.0000 - mae: 2419.0540 - val_loss: 4856.7847 - val_mse: 149249040.0000 - val_mae: 4856.7847\n",
      "Epoch 69/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2407.6087 - mse: 83014992.0000 - mae: 2407.6089 - val_loss: 4855.9644 - val_mse: 148361712.0000 - val_mae: 4855.9644\n",
      "Epoch 70/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2402.6254 - mse: 82273072.0000 - mae: 2402.6255 - val_loss: 4834.9824 - val_mse: 147292912.0000 - val_mae: 4834.9824\n",
      "Epoch 71/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2392.9530 - mse: 81863760.0000 - mae: 2392.9529 - val_loss: 4827.9028 - val_mse: 146249248.0000 - val_mae: 4827.9028\n",
      "Epoch 72/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2383.6720 - mse: 81398104.0000 - mae: 2383.6721 - val_loss: 4811.3193 - val_mse: 145331872.0000 - val_mae: 4811.3193\n",
      "Epoch 73/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2385.5834 - mse: 81179368.0000 - mae: 2385.5835 - val_loss: 4804.7969 - val_mse: 144393632.0000 - val_mae: 4804.7969\n",
      "Epoch 74/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2375.4670 - mse: 80575408.0000 - mae: 2375.4668 - val_loss: 4785.5732 - val_mse: 143529376.0000 - val_mae: 4785.5732\n",
      "Epoch 75/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2360.9841 - mse: 79874792.0000 - mae: 2360.9841 - val_loss: 4744.9922 - val_mse: 142443504.0000 - val_mae: 4744.9922\n",
      "Epoch 76/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2367.4724 - mse: 79726800.0000 - mae: 2367.4724 - val_loss: 4744.6929 - val_mse: 141890544.0000 - val_mae: 4744.6929\n",
      "Epoch 77/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2352.8786 - mse: 79398368.0000 - mae: 2352.8784 - val_loss: 4743.3203 - val_mse: 141168384.0000 - val_mae: 4743.3203\n",
      "Epoch 78/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2351.2734 - mse: 78789080.0000 - mae: 2351.2737 - val_loss: 4732.2295 - val_mse: 140529296.0000 - val_mae: 4732.2295\n",
      "Epoch 79/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2341.7714 - mse: 78617824.0000 - mae: 2341.7715 - val_loss: 4712.2368 - val_mse: 139684448.0000 - val_mae: 4712.2368\n",
      "Epoch 80/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2341.4839 - mse: 78252272.0000 - mae: 2341.4839 - val_loss: 4700.7949 - val_mse: 138973600.0000 - val_mae: 4700.7949\n",
      "Epoch 81/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2329.7101 - mse: 77549856.0000 - mae: 2329.7102 - val_loss: 4702.1572 - val_mse: 138288928.0000 - val_mae: 4702.1572\n",
      "Epoch 82/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 2337.5804 - mse: 77815752.0000 - mae: 2337.5803 - val_loss: 4667.8721 - val_mse: 137484528.0000 - val_mae: 4667.8721\n",
      "Epoch 83/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 2329.7852 - mse: 77167248.0000 - mae: 2329.7854 - val_loss: 4664.9297 - val_mse: 137077792.0000 - val_mae: 4664.9297\n",
      "Epoch 84/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2318.7314 - mse: 76726416.0000 - mae: 2318.7314 - val_loss: 4653.4092 - val_mse: 136342768.0000 - val_mae: 4653.4092\n",
      "Epoch 85/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2319.1474 - mse: 76159512.0000 - mae: 2319.1472 - val_loss: 4638.7544 - val_mse: 135711808.0000 - val_mae: 4638.7544\n",
      "Epoch 86/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2310.1626 - mse: 75938944.0000 - mae: 2310.1626 - val_loss: 4631.3579 - val_mse: 135090096.0000 - val_mae: 4631.3579\n",
      "Epoch 87/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2299.9082 - mse: 75089392.0000 - mae: 2299.9084 - val_loss: 4626.8511 - val_mse: 134475248.0000 - val_mae: 4626.8511\n",
      "Epoch 88/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2299.3588 - mse: 74987912.0000 - mae: 2299.3589 - val_loss: 4615.0371 - val_mse: 133643176.0000 - val_mae: 4615.0371\n",
      "Epoch 89/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2293.7457 - mse: 74676400.0000 - mae: 2293.7456 - val_loss: 4622.9893 - val_mse: 133191912.0000 - val_mae: 4622.9893\n",
      "Epoch 90/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 2292.7340 - mse: 74338336.0000 - mae: 2292.7339 - val_loss: 4589.3032 - val_mse: 132353960.0000 - val_mae: 4589.3032\n",
      "Epoch 91/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2292.6435 - mse: 74118704.0000 - mae: 2292.6436 - val_loss: 4603.0928 - val_mse: 132048280.0000 - val_mae: 4603.0928\n",
      "Epoch 92/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2277.9976 - mse: 73473176.0000 - mae: 2277.9973 - val_loss: 4580.9111 - val_mse: 131229256.0000 - val_mae: 4580.9111\n",
      "Epoch 93/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2269.1440 - mse: 73043936.0000 - mae: 2269.1440 - val_loss: 4565.9814 - val_mse: 130290360.0000 - val_mae: 4565.9814\n",
      "Epoch 94/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2259.2536 - mse: 72188792.0000 - mae: 2259.2537 - val_loss: 4555.8535 - val_mse: 129414432.0000 - val_mae: 4555.8535\n",
      "Epoch 95/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2258.1025 - mse: 72294176.0000 - mae: 2258.1023 - val_loss: 4558.2305 - val_mse: 128832568.0000 - val_mae: 4558.2305\n",
      "Epoch 96/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2252.1688 - mse: 71561816.0000 - mae: 2252.1689 - val_loss: 4543.6069 - val_mse: 128094120.0000 - val_mae: 4543.6069\n",
      "Epoch 97/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2243.4086 - mse: 71240472.0000 - mae: 2243.4087 - val_loss: 4550.4375 - val_mse: 127405136.0000 - val_mae: 4550.4375\n",
      "Epoch 98/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2231.8973 - mse: 70532664.0000 - mae: 2231.8972 - val_loss: 4543.6401 - val_mse: 126471768.0000 - val_mae: 4543.6401\n",
      "Epoch 99/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2226.2140 - mse: 70229784.0000 - mae: 2226.2144 - val_loss: 4518.3540 - val_mse: 125440688.0000 - val_mae: 4518.3540\n",
      "Epoch 100/500\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2211.2602 - mse: 69446112.0000 - mae: 2211.2605 - val_loss: 4491.7734 - val_mse: 124369272.0000 - val_mae: 4491.7734\n",
      "Epoch 101/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 2208.3587 - mse: 68734280.0000 - mae: 2208.3586 - val_loss: 4502.1689 - val_mse: 123639064.0000 - val_mae: 4502.1689\n",
      "Epoch 102/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2204.8396 - mse: 68730288.0000 - mae: 2204.8398 - val_loss: 4486.8784 - val_mse: 122904720.0000 - val_mae: 4486.8784\n",
      "Epoch 103/500\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 2192.4916 - mse: 68076032.0000 - mae: 2192.4917 - val_loss: 4492.3477 - val_mse: 122367200.0000 - val_mae: 4492.3477\n",
      "Epoch 104/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2174.9819 - mse: 66906364.0000 - mae: 2174.9817 - val_loss: 4476.2212 - val_mse: 121434536.0000 - val_mae: 4476.2212\n",
      "Epoch 105/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 2168.0862 - mse: 66196964.0000 - mae: 2168.0862 - val_loss: 4454.2139 - val_mse: 120499056.0000 - val_mae: 4454.2139\n",
      "Epoch 106/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2162.8601 - mse: 65815656.0000 - mae: 2162.8601 - val_loss: 4457.9160 - val_mse: 119548136.0000 - val_mae: 4457.9160\n",
      "Epoch 107/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 2136.7326 - mse: 64207584.0000 - mae: 2136.7327 - val_loss: 4428.1753 - val_mse: 118254336.0000 - val_mae: 4428.1753\n",
      "Epoch 108/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2135.5613 - mse: 63945612.0000 - mae: 2135.5613 - val_loss: 4414.6895 - val_mse: 117020320.0000 - val_mae: 4414.6895\n",
      "Epoch 109/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2120.9549 - mse: 63215416.0000 - mae: 2120.9546 - val_loss: 4382.7656 - val_mse: 115545488.0000 - val_mae: 4382.7656\n",
      "Epoch 110/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 2111.0398 - mse: 61614764.0000 - mae: 2111.0398 - val_loss: 4366.5400 - val_mse: 114201480.0000 - val_mae: 4366.5400\n",
      "Epoch 111/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2100.5598 - mse: 61708060.0000 - mae: 2100.5598 - val_loss: 4370.3994 - val_mse: 113032040.0000 - val_mae: 4370.3994\n",
      "Epoch 112/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2105.3248 - mse: 61073904.0000 - mae: 2105.3247 - val_loss: 4346.9858 - val_mse: 111721488.0000 - val_mae: 4346.9858\n",
      "Epoch 113/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2082.7624 - mse: 59926236.0000 - mae: 2082.7625 - val_loss: 4295.1792 - val_mse: 109709472.0000 - val_mae: 4295.1792\n",
      "Epoch 114/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2065.1054 - mse: 58781920.0000 - mae: 2065.1055 - val_loss: 4302.5059 - val_mse: 108562584.0000 - val_mae: 4302.5059\n",
      "Epoch 115/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2067.8574 - mse: 59238824.0000 - mae: 2067.8574 - val_loss: 4294.5361 - val_mse: 107155896.0000 - val_mae: 4294.5361\n",
      "Epoch 116/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2041.7671 - mse: 56675724.0000 - mae: 2041.7675 - val_loss: 4285.2139 - val_mse: 105644504.0000 - val_mae: 4285.2139\n",
      "Epoch 117/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2023.7523 - mse: 56398944.0000 - mae: 2023.7524 - val_loss: 4253.2329 - val_mse: 103900656.0000 - val_mae: 4253.2329\n",
      "Epoch 118/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2010.8744 - mse: 54783816.0000 - mae: 2010.8744 - val_loss: 4224.2080 - val_mse: 102012256.0000 - val_mae: 4224.2080\n",
      "Epoch 119/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2004.7490 - mse: 54288584.0000 - mae: 2004.7490 - val_loss: 4195.5229 - val_mse: 100200448.0000 - val_mae: 4195.5229\n",
      "Epoch 120/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1973.5006 - mse: 52957064.0000 - mae: 1973.5004 - val_loss: 4193.4229 - val_mse: 98592440.0000 - val_mae: 4193.4229\n",
      "Epoch 121/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1962.5106 - mse: 52242436.0000 - mae: 1962.5107 - val_loss: 4160.7695 - val_mse: 96692832.0000 - val_mae: 4160.7695\n",
      "Epoch 122/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1943.4091 - mse: 50561360.0000 - mae: 1943.4091 - val_loss: 4132.2935 - val_mse: 94742840.0000 - val_mae: 4132.2935\n",
      "Epoch 123/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1913.3998 - mse: 49107684.0000 - mae: 1913.3997 - val_loss: 4072.0476 - val_mse: 92219264.0000 - val_mae: 4072.0476\n",
      "Epoch 124/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1887.2815 - mse: 47605960.0000 - mae: 1887.2816 - val_loss: 4112.7217 - val_mse: 91514152.0000 - val_mae: 4112.7217\n",
      "Epoch 125/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1882.0305 - mse: 47427284.0000 - mae: 1882.0302 - val_loss: 4035.3596 - val_mse: 88370696.0000 - val_mae: 4035.3596\n",
      "Epoch 126/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1857.2014 - mse: 45933436.0000 - mae: 1857.2012 - val_loss: 4008.9961 - val_mse: 86166632.0000 - val_mae: 4008.9961\n",
      "Epoch 127/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1817.9258 - mse: 43994500.0000 - mae: 1817.9257 - val_loss: 3960.8994 - val_mse: 83826608.0000 - val_mae: 3960.8994\n",
      "Epoch 128/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1796.3300 - mse: 42375600.0000 - mae: 1796.3298 - val_loss: 3966.8337 - val_mse: 82543136.0000 - val_mae: 3966.8337\n",
      "Epoch 129/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1779.1167 - mse: 41863124.0000 - mae: 1779.1166 - val_loss: 3925.3491 - val_mse: 80196784.0000 - val_mae: 3925.3491\n",
      "Epoch 130/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1754.8167 - mse: 39752348.0000 - mae: 1754.8168 - val_loss: 3916.4790 - val_mse: 78532192.0000 - val_mae: 3916.4790\n",
      "Epoch 131/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1735.4685 - mse: 39587496.0000 - mae: 1735.4684 - val_loss: 3873.3806 - val_mse: 76248656.0000 - val_mae: 3873.3806\n",
      "Epoch 132/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1709.1796 - mse: 38120016.0000 - mae: 1709.1794 - val_loss: 3885.1484 - val_mse: 75459544.0000 - val_mae: 3885.1484\n",
      "Epoch 133/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1679.1559 - mse: 37014436.0000 - mae: 1679.1560 - val_loss: 3829.4695 - val_mse: 72602848.0000 - val_mae: 3829.4695\n",
      "Epoch 134/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1666.8049 - mse: 35591688.0000 - mae: 1666.8051 - val_loss: 3837.1428 - val_mse: 71356096.0000 - val_mae: 3837.1428\n",
      "Epoch 135/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1638.5026 - mse: 34984876.0000 - mae: 1638.5027 - val_loss: 3757.5454 - val_mse: 67900360.0000 - val_mae: 3757.5454\n",
      "Epoch 136/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1620.7348 - mse: 33669468.0000 - mae: 1620.7347 - val_loss: 3817.1367 - val_mse: 68524960.0000 - val_mae: 3817.1367\n",
      "Epoch 137/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1595.6116 - mse: 32449702.0000 - mae: 1595.6116 - val_loss: 3729.5625 - val_mse: 64357144.0000 - val_mae: 3729.5625\n",
      "Epoch 138/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1583.2403 - mse: 31811774.0000 - mae: 1583.2404 - val_loss: 3744.8257 - val_mse: 64373128.0000 - val_mae: 3744.8257\n",
      "Epoch 139/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1551.3790 - mse: 31025418.0000 - mae: 1551.3790 - val_loss: 3680.6970 - val_mse: 60529788.0000 - val_mae: 3680.6970\n",
      "Epoch 140/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1517.6466 - mse: 28486252.0000 - mae: 1517.6466 - val_loss: 3688.6362 - val_mse: 60490240.0000 - val_mae: 3688.6362\n",
      "Epoch 141/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1512.8801 - mse: 28216250.0000 - mae: 1512.8800 - val_loss: 3683.4832 - val_mse: 59423724.0000 - val_mae: 3683.4832\n",
      "Epoch 142/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1497.6880 - mse: 28247612.0000 - mae: 1497.6880 - val_loss: 3629.1990 - val_mse: 56740024.0000 - val_mae: 3629.1990\n",
      "Epoch 143/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1481.4096 - mse: 26685614.0000 - mae: 1481.4097 - val_loss: 3612.7349 - val_mse: 55315464.0000 - val_mae: 3612.7349\n",
      "Epoch 144/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1444.4912 - mse: 25908096.0000 - mae: 1444.4911 - val_loss: 3653.4387 - val_mse: 56021180.0000 - val_mae: 3653.4387\n",
      "Epoch 145/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1449.4976 - mse: 25699622.0000 - mae: 1449.4977 - val_loss: 3556.6240 - val_mse: 51781432.0000 - val_mae: 3556.6240\n",
      "Epoch 146/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1432.7113 - mse: 24651712.0000 - mae: 1432.7113 - val_loss: 3590.0117 - val_mse: 52982936.0000 - val_mae: 3590.0117\n",
      "Epoch 147/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1434.8583 - mse: 24328324.0000 - mae: 1434.8584 - val_loss: 3644.0928 - val_mse: 54833988.0000 - val_mae: 3644.0928\n",
      "Epoch 148/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1402.9675 - mse: 23158014.0000 - mae: 1402.9677 - val_loss: 3548.9963 - val_mse: 50777084.0000 - val_mae: 3548.9963\n",
      "Epoch 149/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1418.6959 - mse: 23714518.0000 - mae: 1418.6960 - val_loss: 3541.8555 - val_mse: 50487084.0000 - val_mae: 3541.8555\n",
      "Epoch 150/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1390.5662 - mse: 23126318.0000 - mae: 1390.5660 - val_loss: 3494.1592 - val_mse: 48218900.0000 - val_mae: 3494.1592\n",
      "Epoch 151/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1394.6789 - mse: 22558036.0000 - mae: 1394.6790 - val_loss: 3542.1072 - val_mse: 49780776.0000 - val_mae: 3542.1072\n",
      "Epoch 152/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1368.9841 - mse: 21764962.0000 - mae: 1368.9841 - val_loss: 3516.1328 - val_mse: 48861428.0000 - val_mae: 3516.1328\n",
      "Epoch 153/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1369.7733 - mse: 21347770.0000 - mae: 1369.7733 - val_loss: 3480.2046 - val_mse: 47219804.0000 - val_mae: 3480.2046\n",
      "Epoch 154/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1382.5132 - mse: 22100244.0000 - mae: 1382.5133 - val_loss: 3457.4910 - val_mse: 46371332.0000 - val_mae: 3457.4910\n",
      "Epoch 155/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1337.0373 - mse: 19825946.0000 - mae: 1337.0374 - val_loss: 3366.6140 - val_mse: 42479192.0000 - val_mae: 3366.6140\n",
      "Epoch 156/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1337.4301 - mse: 20202212.0000 - mae: 1337.4301 - val_loss: 3447.7429 - val_mse: 45265656.0000 - val_mae: 3447.7429\n",
      "Epoch 157/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1354.3973 - mse: 21125936.0000 - mae: 1354.3973 - val_loss: 3382.2712 - val_mse: 42936392.0000 - val_mae: 3382.2712\n",
      "Epoch 158/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1327.5014 - mse: 19222876.0000 - mae: 1327.5012 - val_loss: 3389.1431 - val_mse: 43310964.0000 - val_mae: 3389.1431\n",
      "Epoch 159/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1347.1164 - mse: 20132106.0000 - mae: 1347.1163 - val_loss: 3337.5120 - val_mse: 41920568.0000 - val_mae: 3337.5120\n",
      "Epoch 160/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1338.4276 - mse: 21018874.0000 - mae: 1338.4276 - val_loss: 3314.3950 - val_mse: 40549904.0000 - val_mae: 3314.3950\n",
      "Epoch 161/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1312.7339 - mse: 18494400.0000 - mae: 1312.7339 - val_loss: 3388.1497 - val_mse: 43486460.0000 - val_mae: 3388.1497\n",
      "Epoch 162/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1345.1031 - mse: 20416586.0000 - mae: 1345.1029 - val_loss: 3288.6091 - val_mse: 39806796.0000 - val_mae: 3288.6091\n",
      "Epoch 163/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1338.0764 - mse: 19598718.0000 - mae: 1338.0763 - val_loss: 3300.3159 - val_mse: 39986168.0000 - val_mae: 3300.3159\n",
      "Epoch 164/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1299.8727 - mse: 18768600.0000 - mae: 1299.8728 - val_loss: 3263.6147 - val_mse: 38625696.0000 - val_mae: 3263.6147\n",
      "Epoch 165/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1324.7073 - mse: 19070026.0000 - mae: 1324.7074 - val_loss: 3283.6460 - val_mse: 39750820.0000 - val_mae: 3283.6460\n",
      "Epoch 166/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1320.3798 - mse: 19106798.0000 - mae: 1320.3798 - val_loss: 3316.5498 - val_mse: 41476996.0000 - val_mae: 3316.5498\n",
      "Epoch 167/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1296.3276 - mse: 17831316.0000 - mae: 1296.3278 - val_loss: 3301.5786 - val_mse: 40180276.0000 - val_mae: 3301.5786\n",
      "Epoch 168/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1318.8563 - mse: 19358148.0000 - mae: 1318.8561 - val_loss: 3268.0320 - val_mse: 38893876.0000 - val_mae: 3268.0320\n",
      "Epoch 169/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1298.9614 - mse: 18559030.0000 - mae: 1298.9614 - val_loss: 3232.8308 - val_mse: 37754152.0000 - val_mae: 3232.8308\n",
      "Epoch 170/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1289.9938 - mse: 18383686.0000 - mae: 1289.9938 - val_loss: 3257.5122 - val_mse: 39260484.0000 - val_mae: 3257.5122\n",
      "Epoch 171/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1309.4815 - mse: 18645772.0000 - mae: 1309.4814 - val_loss: 3270.9578 - val_mse: 39910980.0000 - val_mae: 3270.9578\n",
      "Epoch 172/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1312.7637 - mse: 18900958.0000 - mae: 1312.7635 - val_loss: 3271.8943 - val_mse: 39864032.0000 - val_mae: 3271.8943\n",
      "Epoch 173/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1303.7842 - mse: 19121352.0000 - mae: 1303.7841 - val_loss: 3250.4995 - val_mse: 38945948.0000 - val_mae: 3250.4995\n",
      "Epoch 174/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1272.7407 - mse: 17635522.0000 - mae: 1272.7406 - val_loss: 3219.0735 - val_mse: 37519172.0000 - val_mae: 3219.0735\n",
      "Epoch 175/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1319.8263 - mse: 19080042.0000 - mae: 1319.8264 - val_loss: 3289.2446 - val_mse: 40855832.0000 - val_mae: 3289.2446\n",
      "Epoch 176/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1274.5431 - mse: 17532716.0000 - mae: 1274.5430 - val_loss: 3253.6245 - val_mse: 39454344.0000 - val_mae: 3253.6245\n",
      "Epoch 177/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1297.6897 - mse: 18697402.0000 - mae: 1297.6897 - val_loss: 3181.5149 - val_mse: 36394064.0000 - val_mae: 3181.5149\n",
      "Epoch 178/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1317.2594 - mse: 19229528.0000 - mae: 1317.2594 - val_loss: 3208.9241 - val_mse: 37688908.0000 - val_mae: 3208.9241\n",
      "Epoch 179/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1285.5434 - mse: 18136460.0000 - mae: 1285.5435 - val_loss: 3179.9062 - val_mse: 36769212.0000 - val_mae: 3179.9062\n",
      "Epoch 180/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1283.9930 - mse: 17871214.0000 - mae: 1283.9929 - val_loss: 3146.5969 - val_mse: 35522828.0000 - val_mae: 3146.5969\n",
      "Epoch 181/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1261.1021 - mse: 16729830.0000 - mae: 1261.1021 - val_loss: 3162.0078 - val_mse: 35900796.0000 - val_mae: 3162.0078\n",
      "Epoch 182/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1281.7630 - mse: 17412328.0000 - mae: 1281.7629 - val_loss: 3149.4077 - val_mse: 35570148.0000 - val_mae: 3149.4077\n",
      "Epoch 183/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1258.1965 - mse: 16482850.0000 - mae: 1258.1964 - val_loss: 3161.3391 - val_mse: 36058252.0000 - val_mae: 3161.3391\n",
      "Epoch 184/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1245.1687 - mse: 16330790.0000 - mae: 1245.1688 - val_loss: 3090.3210 - val_mse: 33830984.0000 - val_mae: 3090.3210\n",
      "Epoch 185/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1250.1154 - mse: 16532252.0000 - mae: 1250.1154 - val_loss: 3096.6252 - val_mse: 33869400.0000 - val_mae: 3096.6252\n",
      "Epoch 186/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1258.0687 - mse: 16614613.0000 - mae: 1258.0688 - val_loss: 3185.9619 - val_mse: 37664900.0000 - val_mae: 3185.9619\n",
      "Epoch 187/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1284.7154 - mse: 17447872.0000 - mae: 1284.7155 - val_loss: 3146.9670 - val_mse: 36418968.0000 - val_mae: 3146.9670\n",
      "Epoch 188/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1282.9149 - mse: 17429952.0000 - mae: 1282.9148 - val_loss: 3161.7947 - val_mse: 36819324.0000 - val_mae: 3161.7947\n",
      "Epoch 189/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1266.6874 - mse: 16875268.0000 - mae: 1266.6873 - val_loss: 3150.8657 - val_mse: 36387148.0000 - val_mae: 3150.8657\n",
      "Epoch 190/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1248.5911 - mse: 15933102.0000 - mae: 1248.5912 - val_loss: 3097.7681 - val_mse: 34322244.0000 - val_mae: 3097.7681\n",
      "Epoch 191/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1258.3848 - mse: 16827324.0000 - mae: 1258.3846 - val_loss: 3097.9097 - val_mse: 34537320.0000 - val_mae: 3097.9097\n",
      "Epoch 192/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1260.3861 - mse: 16564314.0000 - mae: 1260.3859 - val_loss: 3171.0254 - val_mse: 37482732.0000 - val_mae: 3171.0254\n",
      "Epoch 193/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1239.8649 - mse: 15954933.0000 - mae: 1239.8650 - val_loss: 3117.0637 - val_mse: 35361584.0000 - val_mae: 3117.0637\n",
      "Epoch 194/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1229.6424 - mse: 15939663.0000 - mae: 1229.6425 - val_loss: 3151.3621 - val_mse: 36788144.0000 - val_mae: 3151.3621\n",
      "Epoch 195/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1261.6432 - mse: 16057618.0000 - mae: 1261.6432 - val_loss: 3067.7734 - val_mse: 33790040.0000 - val_mae: 3067.7734\n",
      "Epoch 196/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1235.7666 - mse: 16044484.0000 - mae: 1235.7667 - val_loss: 3129.0254 - val_mse: 35937432.0000 - val_mae: 3129.0254\n",
      "Epoch 197/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1238.9744 - mse: 15823598.0000 - mae: 1238.9746 - val_loss: 3047.9570 - val_mse: 33114164.0000 - val_mae: 3047.9570\n",
      "Epoch 198/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1261.9272 - mse: 16921728.0000 - mae: 1261.9272 - val_loss: 3040.1086 - val_mse: 33072812.0000 - val_mae: 3040.1086\n",
      "Epoch 199/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1257.8983 - mse: 15987733.0000 - mae: 1257.8984 - val_loss: 3071.2161 - val_mse: 34254252.0000 - val_mae: 3071.2161\n",
      "Epoch 200/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1236.0223 - mse: 15109212.0000 - mae: 1236.0225 - val_loss: 3085.7815 - val_mse: 34738132.0000 - val_mae: 3085.7815\n",
      "Epoch 201/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1254.5517 - mse: 16199510.0000 - mae: 1254.5518 - val_loss: 3094.4419 - val_mse: 35039576.0000 - val_mae: 3094.4419\n",
      "Epoch 202/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1245.2194 - mse: 16137552.0000 - mae: 1245.2194 - val_loss: 3030.9722 - val_mse: 33211156.0000 - val_mae: 3030.9722\n",
      "Epoch 203/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1232.9969 - mse: 15642169.0000 - mae: 1232.9971 - val_loss: 3068.9919 - val_mse: 34040648.0000 - val_mae: 3068.9919\n",
      "Epoch 204/500\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1220.2811 - mse: 14965989.0000 - mae: 1220.2811 - val_loss: 3005.3774 - val_mse: 32122034.0000 - val_mae: 3005.3774\n",
      "Epoch 205/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1226.1235 - mse: 15467730.0000 - mae: 1226.1234 - val_loss: 3096.4131 - val_mse: 35507220.0000 - val_mae: 3096.4131\n",
      "Epoch 206/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1221.1396 - mse: 15421743.0000 - mae: 1221.1396 - val_loss: 3031.8657 - val_mse: 33473132.0000 - val_mae: 3031.8657\n",
      "Epoch 207/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1262.4603 - mse: 16291791.0000 - mae: 1262.4603 - val_loss: 3055.1738 - val_mse: 33993724.0000 - val_mae: 3055.1738\n",
      "Epoch 208/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1254.4743 - mse: 16453625.0000 - mae: 1254.4746 - val_loss: 3011.8262 - val_mse: 32589360.0000 - val_mae: 3011.8262\n",
      "Epoch 209/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1225.7793 - mse: 15365156.0000 - mae: 1225.7793 - val_loss: 3009.7173 - val_mse: 32834312.0000 - val_mae: 3009.7173\n",
      "Epoch 210/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1238.8756 - mse: 15371346.0000 - mae: 1238.8756 - val_loss: 3008.8787 - val_mse: 32335022.0000 - val_mae: 3008.8787\n",
      "Epoch 211/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1224.7012 - mse: 15346970.0000 - mae: 1224.7012 - val_loss: 3003.2632 - val_mse: 32180762.0000 - val_mae: 3003.2632\n",
      "Epoch 212/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1232.8117 - mse: 15324398.0000 - mae: 1232.8116 - val_loss: 3071.2371 - val_mse: 34481364.0000 - val_mae: 3071.2371\n",
      "Epoch 213/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1251.0052 - mse: 16510317.0000 - mae: 1251.0052 - val_loss: 3064.1941 - val_mse: 34235692.0000 - val_mae: 3064.1941\n",
      "Epoch 214/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1228.9649 - mse: 15139878.0000 - mae: 1228.9648 - val_loss: 3035.2869 - val_mse: 33490566.0000 - val_mae: 3035.2869\n",
      "Epoch 215/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1240.5806 - mse: 15694073.0000 - mae: 1240.5807 - val_loss: 2989.8271 - val_mse: 32082562.0000 - val_mae: 2989.8271\n",
      "Epoch 216/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1202.4531 - mse: 14206676.0000 - mae: 1202.4531 - val_loss: 3012.6948 - val_mse: 32865284.0000 - val_mae: 3012.6948\n",
      "Epoch 217/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1230.7378 - mse: 15305060.0000 - mae: 1230.7377 - val_loss: 3026.6821 - val_mse: 33117658.0000 - val_mae: 3026.6821\n",
      "Epoch 218/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1218.7771 - mse: 15156119.0000 - mae: 1218.7772 - val_loss: 2976.5593 - val_mse: 31910742.0000 - val_mae: 2976.5593\n",
      "Epoch 219/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1202.8237 - mse: 14021198.0000 - mae: 1202.8239 - val_loss: 3089.6746 - val_mse: 35162316.0000 - val_mae: 3089.6746\n",
      "Epoch 220/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1218.8544 - mse: 15151753.0000 - mae: 1218.8542 - val_loss: 2994.6934 - val_mse: 32790580.0000 - val_mae: 2994.6934\n",
      "Epoch 221/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1217.7207 - mse: 15036880.0000 - mae: 1217.7208 - val_loss: 3039.4417 - val_mse: 33654768.0000 - val_mae: 3039.4417\n",
      "Epoch 222/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1203.6990 - mse: 14236264.0000 - mae: 1203.6992 - val_loss: 3082.1521 - val_mse: 35196804.0000 - val_mae: 3082.1521\n",
      "Epoch 223/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1220.4055 - mse: 15533620.0000 - mae: 1220.4055 - val_loss: 2984.3438 - val_mse: 32425158.0000 - val_mae: 2984.3438\n",
      "Epoch 224/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1184.3676 - mse: 14454779.0000 - mae: 1184.3676 - val_loss: 3030.7048 - val_mse: 33352616.0000 - val_mae: 3030.7048\n",
      "Epoch 225/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1246.1717 - mse: 16325563.0000 - mae: 1246.1718 - val_loss: 2982.2693 - val_mse: 32278176.0000 - val_mae: 2982.2693\n",
      "Epoch 226/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1214.2255 - mse: 15082757.0000 - mae: 1214.2255 - val_loss: 3016.3584 - val_mse: 33136790.0000 - val_mae: 3016.3584\n",
      "Epoch 227/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1231.3207 - mse: 15420134.0000 - mae: 1231.3206 - val_loss: 2990.4106 - val_mse: 32064170.0000 - val_mae: 2990.4106\n",
      "Epoch 228/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1219.8898 - mse: 15467667.0000 - mae: 1219.8896 - val_loss: 2961.4934 - val_mse: 31391124.0000 - val_mae: 2961.4934\n",
      "Epoch 229/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1231.6340 - mse: 15763163.0000 - mae: 1231.6340 - val_loss: 2979.3923 - val_mse: 32156034.0000 - val_mae: 2979.3923\n",
      "Epoch 230/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1200.7754 - mse: 14988255.0000 - mae: 1200.7753 - val_loss: 3010.3528 - val_mse: 33079530.0000 - val_mae: 3010.3528\n",
      "Epoch 231/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1225.3296 - mse: 15437360.0000 - mae: 1225.3298 - val_loss: 2991.1643 - val_mse: 32325014.0000 - val_mae: 2991.1643\n",
      "Epoch 232/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1185.9358 - mse: 14325622.0000 - mae: 1185.9359 - val_loss: 2956.8835 - val_mse: 31381448.0000 - val_mae: 2956.8835\n",
      "Epoch 233/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1205.5057 - mse: 14651269.0000 - mae: 1205.5059 - val_loss: 2986.2356 - val_mse: 32597354.0000 - val_mae: 2986.2356\n",
      "Epoch 234/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1216.3094 - mse: 15271677.0000 - mae: 1216.3094 - val_loss: 2962.8562 - val_mse: 31818094.0000 - val_mae: 2962.8562\n",
      "Epoch 235/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1206.0023 - mse: 14436473.0000 - mae: 1206.0023 - val_loss: 2978.9336 - val_mse: 32677664.0000 - val_mae: 2978.9336\n",
      "Epoch 236/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1237.2297 - mse: 16002305.0000 - mae: 1237.2297 - val_loss: 2958.1467 - val_mse: 31665784.0000 - val_mae: 2958.1467\n",
      "Epoch 237/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1175.3879 - mse: 13514437.0000 - mae: 1175.3879 - val_loss: 2982.0083 - val_mse: 32179780.0000 - val_mae: 2982.0083\n",
      "Epoch 238/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1210.1043 - mse: 14471247.0000 - mae: 1210.1044 - val_loss: 2934.7993 - val_mse: 31086422.0000 - val_mae: 2934.7993\n",
      "Epoch 239/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1206.6268 - mse: 15694017.0000 - mae: 1206.6270 - val_loss: 2957.2351 - val_mse: 31590906.0000 - val_mae: 2957.2351\n",
      "Epoch 240/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1223.3202 - mse: 15209393.0000 - mae: 1223.3203 - val_loss: 2972.7812 - val_mse: 32128190.0000 - val_mae: 2972.7812\n",
      "Epoch 241/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1199.4197 - mse: 14309207.0000 - mae: 1199.4196 - val_loss: 2959.6790 - val_mse: 31606622.0000 - val_mae: 2959.6790\n",
      "Epoch 242/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1204.0583 - mse: 14741874.0000 - mae: 1204.0583 - val_loss: 2906.7253 - val_mse: 29966998.0000 - val_mae: 2906.7253\n",
      "Epoch 243/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1200.2354 - mse: 14329977.0000 - mae: 1200.2354 - val_loss: 2963.2876 - val_mse: 31485242.0000 - val_mae: 2963.2876\n",
      "Epoch 244/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1224.4113 - mse: 15687061.0000 - mae: 1224.4114 - val_loss: 2923.5449 - val_mse: 30424180.0000 - val_mae: 2923.5449\n",
      "Epoch 245/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1231.8997 - mse: 14903946.0000 - mae: 1231.8997 - val_loss: 3008.2859 - val_mse: 32962992.0000 - val_mae: 3008.2859\n",
      "Epoch 246/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1216.8355 - mse: 15086502.0000 - mae: 1216.8354 - val_loss: 2981.6558 - val_mse: 32143390.0000 - val_mae: 2981.6558\n",
      "Epoch 247/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1183.1824 - mse: 13870119.0000 - mae: 1183.1823 - val_loss: 2923.9565 - val_mse: 30675950.0000 - val_mae: 2923.9565\n",
      "Epoch 248/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1220.2787 - mse: 14716106.0000 - mae: 1220.2787 - val_loss: 2965.7480 - val_mse: 31502086.0000 - val_mae: 2965.7480\n",
      "Epoch 249/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1221.4921 - mse: 14957345.0000 - mae: 1221.4921 - val_loss: 2932.0991 - val_mse: 30601260.0000 - val_mae: 2932.0991\n",
      "Epoch 250/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1192.2205 - mse: 14581409.0000 - mae: 1192.2205 - val_loss: 2945.0474 - val_mse: 31196030.0000 - val_mae: 2945.0474\n",
      "Epoch 251/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1200.9672 - mse: 14587198.0000 - mae: 1200.9672 - val_loss: 2950.4187 - val_mse: 31274096.0000 - val_mae: 2950.4187\n",
      "Epoch 252/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1212.0242 - mse: 14885801.0000 - mae: 1212.0242 - val_loss: 2952.0122 - val_mse: 31368252.0000 - val_mae: 2952.0122\n",
      "Epoch 253/500\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 1178.5099 - mse: 13588274.0000 - mae: 1178.5099 - val_loss: 2982.3196 - val_mse: 32275396.0000 - val_mae: 2982.3196\n",
      "Epoch 254/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1205.4623 - mse: 14343594.0000 - mae: 1205.4623 - val_loss: 2953.3403 - val_mse: 31531502.0000 - val_mae: 2953.3403\n",
      "Epoch 255/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1213.1389 - mse: 15036803.0000 - mae: 1213.1392 - val_loss: 2915.9919 - val_mse: 30626720.0000 - val_mae: 2915.9919\n",
      "Epoch 256/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1220.5351 - mse: 16015059.0000 - mae: 1220.5349 - val_loss: 2902.9744 - val_mse: 30320596.0000 - val_mae: 2902.9744\n",
      "Epoch 257/500\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 1191.7628 - mse: 14157791.0000 - mae: 1191.7629 - val_loss: 2933.0012 - val_mse: 30977632.0000 - val_mae: 2933.0012\n",
      "Epoch 258/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1188.9248 - mse: 14601388.0000 - mae: 1188.9247 - val_loss: 2930.0471 - val_mse: 31064664.0000 - val_mae: 2930.0471\n",
      "Epoch 259/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1210.1995 - mse: 15598548.0000 - mae: 1210.1996 - val_loss: 2924.7478 - val_mse: 30740838.0000 - val_mae: 2924.7478\n",
      "Epoch 260/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1198.1865 - mse: 14243240.0000 - mae: 1198.1866 - val_loss: 2964.2073 - val_mse: 31865968.0000 - val_mae: 2964.2073\n",
      "Epoch 261/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1224.9439 - mse: 15489614.0000 - mae: 1224.9438 - val_loss: 2907.7678 - val_mse: 30229576.0000 - val_mae: 2907.7678\n",
      "Epoch 262/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1246.2077 - mse: 16614407.0000 - mae: 1246.2078 - val_loss: 2903.7119 - val_mse: 29981212.0000 - val_mae: 2903.7119\n",
      "Epoch 263/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1178.7469 - mse: 13788588.0000 - mae: 1178.7468 - val_loss: 2891.7346 - val_mse: 29752722.0000 - val_mae: 2891.7346\n",
      "Epoch 264/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1240.8808 - mse: 15768946.0000 - mae: 1240.8809 - val_loss: 2934.9712 - val_mse: 30970226.0000 - val_mae: 2934.9712\n",
      "Epoch 265/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1180.4324 - mse: 14615845.0000 - mae: 1180.4324 - val_loss: 2938.0513 - val_mse: 31305790.0000 - val_mae: 2938.0513\n",
      "Epoch 266/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1190.9989 - mse: 14141021.0000 - mae: 1190.9989 - val_loss: 2975.6936 - val_mse: 32343180.0000 - val_mae: 2975.6936\n",
      "Epoch 267/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1173.1091 - mse: 14425543.0000 - mae: 1173.1090 - val_loss: 2919.6133 - val_mse: 30893168.0000 - val_mae: 2919.6133\n",
      "Epoch 268/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1229.3902 - mse: 15367230.0000 - mae: 1229.3903 - val_loss: 2945.4192 - val_mse: 31613972.0000 - val_mae: 2945.4192\n",
      "Epoch 269/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1233.7391 - mse: 15956394.0000 - mae: 1233.7390 - val_loss: 2941.0620 - val_mse: 31127912.0000 - val_mae: 2941.0620\n",
      "Epoch 270/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1191.7682 - mse: 14061501.0000 - mae: 1191.7682 - val_loss: 2920.1589 - val_mse: 30465116.0000 - val_mae: 2920.1589\n",
      "Epoch 271/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1204.4403 - mse: 14603546.0000 - mae: 1204.4404 - val_loss: 2944.9221 - val_mse: 31513134.0000 - val_mae: 2944.9221\n",
      "Epoch 272/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1213.9704 - mse: 15647793.0000 - mae: 1213.9703 - val_loss: 2949.1765 - val_mse: 31212342.0000 - val_mae: 2949.1765\n",
      "Epoch 273/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1201.6941 - mse: 15445348.0000 - mae: 1201.6941 - val_loss: 3015.5959 - val_mse: 33528692.0000 - val_mae: 3015.5959\n",
      "Epoch 274/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1198.5389 - mse: 14709812.0000 - mae: 1198.5389 - val_loss: 2912.3611 - val_mse: 30430626.0000 - val_mae: 2912.3611\n",
      "Epoch 275/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1203.1657 - mse: 15091127.0000 - mae: 1203.1656 - val_loss: 2891.0154 - val_mse: 30143262.0000 - val_mae: 2891.0154\n",
      "Epoch 276/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1201.5010 - mse: 14710986.0000 - mae: 1201.5010 - val_loss: 2883.8066 - val_mse: 29821118.0000 - val_mae: 2883.8066\n",
      "Epoch 277/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1193.2383 - mse: 13948792.0000 - mae: 1193.2384 - val_loss: 3014.0884 - val_mse: 33736116.0000 - val_mae: 3014.0884\n",
      "Epoch 278/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1185.2467 - mse: 14084180.0000 - mae: 1185.2466 - val_loss: 2940.2690 - val_mse: 31849980.0000 - val_mae: 2940.2690\n",
      "Epoch 279/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1190.9240 - mse: 14341048.0000 - mae: 1190.9241 - val_loss: 2952.6213 - val_mse: 32398020.0000 - val_mae: 2952.6213\n",
      "Epoch 280/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1174.2331 - mse: 13806220.0000 - mae: 1174.2332 - val_loss: 2965.0442 - val_mse: 32053690.0000 - val_mae: 2965.0442\n",
      "Epoch 281/500\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 1167.0809 - mse: 13581435.0000 - mae: 1167.0809 - val_loss: 2942.9448 - val_mse: 31943398.0000 - val_mae: 2942.9448\n",
      "Epoch 282/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1184.5913 - mse: 14433885.0000 - mae: 1184.5913 - val_loss: 2918.4563 - val_mse: 31165144.0000 - val_mae: 2918.4563\n",
      "Epoch 283/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1165.6591 - mse: 13721634.0000 - mae: 1165.6592 - val_loss: 2900.7886 - val_mse: 30531294.0000 - val_mae: 2900.7886\n",
      "Epoch 284/500\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 1225.1445 - mse: 15968743.0000 - mae: 1225.1444 - val_loss: 2884.2478 - val_mse: 30012358.0000 - val_mae: 2884.2478\n",
      "Epoch 285/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1192.5852 - mse: 13991441.0000 - mae: 1192.5852 - val_loss: 2941.8635 - val_mse: 31876246.0000 - val_mae: 2941.8635\n",
      "Epoch 286/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1195.2849 - mse: 14313346.0000 - mae: 1195.2850 - val_loss: 2930.8928 - val_mse: 31236752.0000 - val_mae: 2930.8928\n",
      "Epoch 287/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1192.8650 - mse: 14371838.0000 - mae: 1192.8652 - val_loss: 2932.7034 - val_mse: 31147512.0000 - val_mae: 2932.7034\n",
      "Epoch 288/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1209.3946 - mse: 15050760.0000 - mae: 1209.3944 - val_loss: 3021.1572 - val_mse: 34024156.0000 - val_mae: 3021.1572\n",
      "Epoch 289/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1187.1727 - mse: 13716627.0000 - mae: 1187.1727 - val_loss: 2923.4504 - val_mse: 30879028.0000 - val_mae: 2923.4504\n",
      "Epoch 290/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1186.5985 - mse: 14856287.0000 - mae: 1186.5985 - val_loss: 2921.1565 - val_mse: 30795142.0000 - val_mae: 2921.1565\n",
      "Epoch 291/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1174.4767 - mse: 13726083.0000 - mae: 1174.4767 - val_loss: 2866.1260 - val_mse: 29460674.0000 - val_mae: 2866.1260\n",
      "Epoch 292/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1184.8331 - mse: 14298735.0000 - mae: 1184.8330 - val_loss: 2946.8320 - val_mse: 32264340.0000 - val_mae: 2946.8320\n",
      "Epoch 293/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1186.3678 - mse: 14217798.0000 - mae: 1186.3677 - val_loss: 2903.6936 - val_mse: 30700410.0000 - val_mae: 2903.6936\n",
      "Epoch 294/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1217.4200 - mse: 15623814.0000 - mae: 1217.4200 - val_loss: 2947.9290 - val_mse: 31693320.0000 - val_mae: 2947.9290\n",
      "Epoch 295/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1218.9774 - mse: 15845338.0000 - mae: 1218.9774 - val_loss: 2969.2554 - val_mse: 32668076.0000 - val_mae: 2969.2554\n",
      "Epoch 296/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1196.3511 - mse: 14720429.0000 - mae: 1196.3511 - val_loss: 2950.5190 - val_mse: 31935092.0000 - val_mae: 2950.5190\n",
      "Epoch 297/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1180.6391 - mse: 14510693.0000 - mae: 1180.6392 - val_loss: 2897.4788 - val_mse: 30570302.0000 - val_mae: 2897.4788\n",
      "Epoch 298/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1197.0820 - mse: 14241600.0000 - mae: 1197.0819 - val_loss: 2935.4119 - val_mse: 31426684.0000 - val_mae: 2935.4119\n",
      "Epoch 299/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1186.5516 - mse: 14465194.0000 - mae: 1186.5518 - val_loss: 2877.7771 - val_mse: 29830134.0000 - val_mae: 2877.7771\n",
      "Epoch 300/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1186.1149 - mse: 14376691.0000 - mae: 1186.1150 - val_loss: 2911.2207 - val_mse: 30619074.0000 - val_mae: 2911.2207\n",
      "Epoch 301/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1188.4081 - mse: 14315581.0000 - mae: 1188.4081 - val_loss: 2875.4456 - val_mse: 29654536.0000 - val_mae: 2875.4456\n",
      "Epoch 302/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1191.0470 - mse: 14380204.0000 - mae: 1191.0470 - val_loss: 2895.5083 - val_mse: 30111828.0000 - val_mae: 2895.5083\n",
      "Epoch 303/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1200.7916 - mse: 14753913.0000 - mae: 1200.7916 - val_loss: 2933.5024 - val_mse: 31657802.0000 - val_mae: 2933.5024\n",
      "Epoch 304/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1190.8642 - mse: 14637192.0000 - mae: 1190.8643 - val_loss: 2936.5735 - val_mse: 31465730.0000 - val_mae: 2936.5735\n",
      "Epoch 305/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1195.8715 - mse: 14499715.0000 - mae: 1195.8716 - val_loss: 2918.9238 - val_mse: 31203374.0000 - val_mae: 2918.9238\n",
      "Epoch 306/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1161.1338 - mse: 13175066.0000 - mae: 1161.1338 - val_loss: 2924.8350 - val_mse: 31322316.0000 - val_mae: 2924.8350\n",
      "Epoch 307/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1171.9138 - mse: 13695013.0000 - mae: 1171.9136 - val_loss: 2868.2310 - val_mse: 29711618.0000 - val_mae: 2868.2310\n",
      "Epoch 308/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1172.4926 - mse: 13834011.0000 - mae: 1172.4924 - val_loss: 2866.7383 - val_mse: 29648052.0000 - val_mae: 2866.7383\n",
      "Epoch 309/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1200.0282 - mse: 14510242.0000 - mae: 1200.0282 - val_loss: 2909.7275 - val_mse: 30803800.0000 - val_mae: 2909.7275\n",
      "Epoch 310/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1180.4014 - mse: 14590489.0000 - mae: 1180.4012 - val_loss: 2913.5967 - val_mse: 30851170.0000 - val_mae: 2913.5967\n",
      "Epoch 311/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1148.5240 - mse: 13123850.0000 - mae: 1148.5239 - val_loss: 2881.1328 - val_mse: 30067150.0000 - val_mae: 2881.1328\n",
      "Epoch 312/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1180.2629 - mse: 14097978.0000 - mae: 1180.2628 - val_loss: 2915.7375 - val_mse: 31444234.0000 - val_mae: 2915.7375\n",
      "Epoch 313/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1175.9585 - mse: 14460178.0000 - mae: 1175.9586 - val_loss: 2860.2051 - val_mse: 29554906.0000 - val_mae: 2860.2051\n",
      "Epoch 314/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1176.7012 - mse: 13890249.0000 - mae: 1176.7010 - val_loss: 2833.3298 - val_mse: 28634764.0000 - val_mae: 2833.3298\n",
      "Epoch 315/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1198.6970 - mse: 14601830.0000 - mae: 1198.6969 - val_loss: 2869.3005 - val_mse: 29542142.0000 - val_mae: 2869.3005\n",
      "Epoch 316/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1192.5696 - mse: 14324779.0000 - mae: 1192.5696 - val_loss: 2843.3730 - val_mse: 29006376.0000 - val_mae: 2843.3730\n",
      "Epoch 317/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1201.8273 - mse: 14655749.0000 - mae: 1201.8273 - val_loss: 2836.9275 - val_mse: 28804192.0000 - val_mae: 2836.9275\n",
      "Epoch 318/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1176.7888 - mse: 13750198.0000 - mae: 1176.7887 - val_loss: 2868.9229 - val_mse: 29499586.0000 - val_mae: 2868.9229\n",
      "Epoch 319/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1207.0329 - mse: 14804163.0000 - mae: 1207.0328 - val_loss: 2873.3655 - val_mse: 29458954.0000 - val_mae: 2873.3655\n",
      "Epoch 320/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1187.7885 - mse: 14850236.0000 - mae: 1187.7885 - val_loss: 2869.6885 - val_mse: 29840034.0000 - val_mae: 2869.6885\n",
      "Epoch 321/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1190.1998 - mse: 14958241.0000 - mae: 1190.1998 - val_loss: 2839.5149 - val_mse: 28934860.0000 - val_mae: 2839.5149\n",
      "Epoch 322/500\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 1167.4328 - mse: 13944878.0000 - mae: 1167.4327 - val_loss: 2835.1753 - val_mse: 28925154.0000 - val_mae: 2835.1753\n",
      "Epoch 323/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1161.1681 - mse: 13191707.0000 - mae: 1161.1680 - val_loss: 2854.2534 - val_mse: 29239250.0000 - val_mae: 2854.2534\n",
      "Epoch 324/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1172.4489 - mse: 13745009.0000 - mae: 1172.4489 - val_loss: 2876.0925 - val_mse: 30069732.0000 - val_mae: 2876.0925\n",
      "Epoch 325/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1182.4716 - mse: 13946028.0000 - mae: 1182.4717 - val_loss: 2856.1641 - val_mse: 29724492.0000 - val_mae: 2856.1641\n",
      "Epoch 326/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1177.2613 - mse: 14280914.0000 - mae: 1177.2614 - val_loss: 2824.2834 - val_mse: 28671976.0000 - val_mae: 2824.2834\n",
      "Epoch 327/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1156.0667 - mse: 13094743.0000 - mae: 1156.0667 - val_loss: 2868.3638 - val_mse: 29872672.0000 - val_mae: 2868.3638\n",
      "Epoch 328/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1156.3306 - mse: 13299474.0000 - mae: 1156.3306 - val_loss: 2851.4695 - val_mse: 29500666.0000 - val_mae: 2851.4695\n",
      "Epoch 329/500\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 1189.5172 - mse: 14655214.0000 - mae: 1189.5171 - val_loss: 2855.2263 - val_mse: 29464740.0000 - val_mae: 2855.2263\n",
      "Epoch 330/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1169.7143 - mse: 14048313.0000 - mae: 1169.7144 - val_loss: 2806.6667 - val_mse: 28503770.0000 - val_mae: 2806.6667\n",
      "Epoch 331/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1193.6921 - mse: 14869735.0000 - mae: 1193.6923 - val_loss: 2811.0437 - val_mse: 28586426.0000 - val_mae: 2811.0437\n",
      "Epoch 332/500\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 1182.7044 - mse: 13891682.0000 - mae: 1182.7045 - val_loss: 2833.0815 - val_mse: 29049946.0000 - val_mae: 2833.0815\n",
      "Epoch 333/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1171.2443 - mse: 14131977.0000 - mae: 1171.2444 - val_loss: 2827.4680 - val_mse: 28800706.0000 - val_mae: 2827.4680\n",
      "Epoch 334/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1183.2820 - mse: 13918781.0000 - mae: 1183.2820 - val_loss: 2830.0161 - val_mse: 28787902.0000 - val_mae: 2830.0161\n",
      "Epoch 335/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1180.2565 - mse: 13992500.0000 - mae: 1180.2565 - val_loss: 2865.0632 - val_mse: 29846556.0000 - val_mae: 2865.0632\n",
      "Epoch 336/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1192.2292 - mse: 14547812.0000 - mae: 1192.2294 - val_loss: 2946.9895 - val_mse: 32358148.0000 - val_mae: 2946.9895\n",
      "Epoch 337/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1178.9763 - mse: 14445704.0000 - mae: 1178.9763 - val_loss: 2916.3618 - val_mse: 31802126.0000 - val_mae: 2916.3618\n",
      "Epoch 338/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1166.4896 - mse: 13955219.0000 - mae: 1166.4897 - val_loss: 2901.1555 - val_mse: 31567512.0000 - val_mae: 2901.1555\n",
      "Epoch 339/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1199.0776 - mse: 15032410.0000 - mae: 1199.0776 - val_loss: 2824.6580 - val_mse: 28637118.0000 - val_mae: 2824.6580\n",
      "Epoch 340/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1186.0685 - mse: 14270684.0000 - mae: 1186.0686 - val_loss: 2871.9558 - val_mse: 30266000.0000 - val_mae: 2871.9558\n",
      "Epoch 341/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1180.5097 - mse: 14505948.0000 - mae: 1180.5096 - val_loss: 2812.4314 - val_mse: 28559028.0000 - val_mae: 2812.4314\n",
      "Epoch 342/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1184.4858 - mse: 14327407.0000 - mae: 1184.4860 - val_loss: 2859.4023 - val_mse: 29783950.0000 - val_mae: 2859.4023\n",
      "Epoch 343/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1185.8761 - mse: 14143811.0000 - mae: 1185.8762 - val_loss: 2871.3948 - val_mse: 30264550.0000 - val_mae: 2871.3948\n",
      "Epoch 344/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1173.2433 - mse: 14363034.0000 - mae: 1173.2432 - val_loss: 2867.8179 - val_mse: 30048966.0000 - val_mae: 2867.8179\n",
      "Epoch 345/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1167.1500 - mse: 13506737.0000 - mae: 1167.1500 - val_loss: 2899.3516 - val_mse: 30961590.0000 - val_mae: 2899.3516\n",
      "Epoch 346/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1170.2421 - mse: 14171226.0000 - mae: 1170.2419 - val_loss: 2825.4265 - val_mse: 28837286.0000 - val_mae: 2825.4265\n",
      "Epoch 347/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1166.8511 - mse: 13802147.0000 - mae: 1166.8511 - val_loss: 2885.9346 - val_mse: 30434252.0000 - val_mae: 2885.9346\n",
      "Epoch 348/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1178.0965 - mse: 14005895.0000 - mae: 1178.0966 - val_loss: 2893.3198 - val_mse: 30490866.0000 - val_mae: 2893.3198\n",
      "Epoch 349/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1173.7332 - mse: 13858583.0000 - mae: 1173.7332 - val_loss: 2856.3750 - val_mse: 29742008.0000 - val_mae: 2856.3750\n",
      "Epoch 350/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1160.3742 - mse: 13582443.0000 - mae: 1160.3741 - val_loss: 2851.2458 - val_mse: 29729398.0000 - val_mae: 2851.2458\n",
      "Epoch 351/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1191.8267 - mse: 14935210.0000 - mae: 1191.8267 - val_loss: 2882.0366 - val_mse: 30466084.0000 - val_mae: 2882.0366\n",
      "Epoch 352/500\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 1189.4969 - mse: 14217168.0000 - mae: 1189.4969 - val_loss: 2884.2517 - val_mse: 30652452.0000 - val_mae: 2884.2517\n",
      "Epoch 353/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1178.9789 - mse: 13938332.0000 - mae: 1178.9790 - val_loss: 2858.0234 - val_mse: 29888406.0000 - val_mae: 2858.0234\n",
      "Epoch 354/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1150.2245 - mse: 13987178.0000 - mae: 1150.2244 - val_loss: 2839.8425 - val_mse: 29391880.0000 - val_mae: 2839.8425\n",
      "Epoch 355/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1166.6184 - mse: 13959376.0000 - mae: 1166.6183 - val_loss: 2893.0090 - val_mse: 30920410.0000 - val_mae: 2893.0090\n",
      "Epoch 356/500\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 1177.7969 - mse: 14450727.0000 - mae: 1177.7969 - val_loss: 2831.2988 - val_mse: 28815322.0000 - val_mae: 2831.2988\n",
      "Epoch 357/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1139.2231 - mse: 13199036.0000 - mae: 1139.2230 - val_loss: 2863.9250 - val_mse: 29875088.0000 - val_mae: 2863.9250\n",
      "Epoch 358/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1150.7278 - mse: 13215001.0000 - mae: 1150.7277 - val_loss: 2869.7527 - val_mse: 30243672.0000 - val_mae: 2869.7527\n",
      "Epoch 359/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1140.6899 - mse: 12992669.0000 - mae: 1140.6901 - val_loss: 2842.5730 - val_mse: 29209296.0000 - val_mae: 2842.5730\n",
      "Epoch 360/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1172.3267 - mse: 14042797.0000 - mae: 1172.3267 - val_loss: 2861.2817 - val_mse: 29542810.0000 - val_mae: 2861.2817\n",
      "Epoch 361/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1165.8935 - mse: 13822801.0000 - mae: 1165.8936 - val_loss: 2893.8132 - val_mse: 30700314.0000 - val_mae: 2893.8132\n",
      "Epoch 362/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1155.4232 - mse: 13662476.0000 - mae: 1155.4233 - val_loss: 2837.0146 - val_mse: 29257778.0000 - val_mae: 2837.0146\n",
      "Epoch 363/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1163.4646 - mse: 13543196.0000 - mae: 1163.4645 - val_loss: 2888.7815 - val_mse: 30852104.0000 - val_mae: 2888.7815\n",
      "Epoch 364/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1131.6706 - mse: 13015783.0000 - mae: 1131.6705 - val_loss: 2846.4294 - val_mse: 29590320.0000 - val_mae: 2846.4294\n",
      "Epoch 365/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1175.2387 - mse: 14873688.0000 - mae: 1175.2386 - val_loss: 2863.9888 - val_mse: 29772498.0000 - val_mae: 2863.9888\n",
      "Epoch 366/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1179.1296 - mse: 14396265.0000 - mae: 1179.1298 - val_loss: 2866.2473 - val_mse: 30203508.0000 - val_mae: 2866.2473\n",
      "Epoch 367/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1170.8022 - mse: 13903797.0000 - mae: 1170.8024 - val_loss: 2842.2354 - val_mse: 29277150.0000 - val_mae: 2842.2354\n",
      "Epoch 368/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1162.5768 - mse: 13583674.0000 - mae: 1162.5769 - val_loss: 2810.2100 - val_mse: 28321216.0000 - val_mae: 2810.2100\n",
      "Epoch 369/500\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1171.3414 - mse: 13782647.0000 - mae: 1171.3414 - val_loss: 2856.1814 - val_mse: 29556984.0000 - val_mae: 2856.1814\n",
      "Epoch 370/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1177.6857 - mse: 13966400.0000 - mae: 1177.6855 - val_loss: 2855.9268 - val_mse: 29693636.0000 - val_mae: 2855.9268\n",
      "Epoch 371/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1161.1820 - mse: 13686191.0000 - mae: 1161.1823 - val_loss: 2837.3579 - val_mse: 29158944.0000 - val_mae: 2837.3579\n",
      "Epoch 372/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1135.8680 - mse: 12791443.0000 - mae: 1135.8679 - val_loss: 2854.7078 - val_mse: 29887934.0000 - val_mae: 2854.7078\n",
      "Epoch 373/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1161.5301 - mse: 13539234.0000 - mae: 1161.5303 - val_loss: 2839.6343 - val_mse: 29165908.0000 - val_mae: 2839.6343\n",
      "Epoch 374/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1130.6879 - mse: 12577133.0000 - mae: 1130.6877 - val_loss: 2867.3994 - val_mse: 29682884.0000 - val_mae: 2867.3994\n",
      "Epoch 375/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1184.4423 - mse: 14379922.0000 - mae: 1184.4424 - val_loss: 2835.1821 - val_mse: 29143932.0000 - val_mae: 2835.1821\n",
      "Epoch 376/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1160.8249 - mse: 14097090.0000 - mae: 1160.8250 - val_loss: 2859.2273 - val_mse: 29973340.0000 - val_mae: 2859.2273\n",
      "Epoch 377/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1182.8440 - mse: 14562296.0000 - mae: 1182.8441 - val_loss: 2876.3298 - val_mse: 30308548.0000 - val_mae: 2876.3298\n",
      "Epoch 378/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1171.3938 - mse: 14521164.0000 - mae: 1171.3938 - val_loss: 2877.2224 - val_mse: 30482842.0000 - val_mae: 2877.2224\n",
      "Epoch 379/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1158.5913 - mse: 13466364.0000 - mae: 1158.5914 - val_loss: 2838.6814 - val_mse: 29215404.0000 - val_mae: 2838.6814\n",
      "Epoch 380/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1166.1292 - mse: 14364330.0000 - mae: 1166.1292 - val_loss: 2788.5500 - val_mse: 27647898.0000 - val_mae: 2788.5500\n",
      "Epoch 381/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1174.4301 - mse: 14684175.0000 - mae: 1174.4301 - val_loss: 2799.5232 - val_mse: 27944242.0000 - val_mae: 2799.5232\n",
      "Epoch 382/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1171.7528 - mse: 14303512.0000 - mae: 1171.7527 - val_loss: 2806.1870 - val_mse: 28353888.0000 - val_mae: 2806.1870\n",
      "Epoch 383/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1173.9463 - mse: 13721000.0000 - mae: 1173.9463 - val_loss: 2847.6519 - val_mse: 29501126.0000 - val_mae: 2847.6519\n",
      "Epoch 384/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1161.0690 - mse: 13569016.0000 - mae: 1161.0690 - val_loss: 2811.9365 - val_mse: 28592680.0000 - val_mae: 2811.9365\n",
      "Epoch 385/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1171.8947 - mse: 14215851.0000 - mae: 1171.8947 - val_loss: 2884.2563 - val_mse: 30525984.0000 - val_mae: 2884.2563\n",
      "Epoch 386/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1152.2433 - mse: 13428311.0000 - mae: 1152.2433 - val_loss: 2816.4060 - val_mse: 28553314.0000 - val_mae: 2816.4060\n",
      "Epoch 387/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1164.7695 - mse: 14167414.0000 - mae: 1164.7694 - val_loss: 2820.7471 - val_mse: 28672862.0000 - val_mae: 2820.7471\n",
      "Epoch 388/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1151.3910 - mse: 13262516.0000 - mae: 1151.3907 - val_loss: 2834.3071 - val_mse: 29049958.0000 - val_mae: 2834.3071\n",
      "Epoch 389/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1182.8931 - mse: 14080097.0000 - mae: 1182.8929 - val_loss: 2832.4905 - val_mse: 29044100.0000 - val_mae: 2832.4905\n",
      "Epoch 390/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1149.5881 - mse: 13099320.0000 - mae: 1149.5881 - val_loss: 2827.2166 - val_mse: 28682154.0000 - val_mae: 2827.2166\n",
      "Epoch 391/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1131.5209 - mse: 12616664.0000 - mae: 1131.5209 - val_loss: 2817.3198 - val_mse: 28434898.0000 - val_mae: 2817.3198\n",
      "Epoch 392/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1152.7560 - mse: 13519040.0000 - mae: 1152.7559 - val_loss: 2838.0618 - val_mse: 29276578.0000 - val_mae: 2838.0618\n",
      "Epoch 393/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1187.6028 - mse: 15194572.0000 - mae: 1187.6028 - val_loss: 2845.0828 - val_mse: 29635720.0000 - val_mae: 2845.0828\n",
      "Epoch 394/500\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 1149.3707 - mse: 13422079.0000 - mae: 1149.3707 - val_loss: 2852.1150 - val_mse: 29722548.0000 - val_mae: 2852.1150\n",
      "Epoch 395/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1114.1827 - mse: 12432091.0000 - mae: 1114.1827 - val_loss: 2810.7283 - val_mse: 28385518.0000 - val_mae: 2810.7283\n",
      "Epoch 396/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1176.0245 - mse: 14659651.0000 - mae: 1176.0247 - val_loss: 2837.3772 - val_mse: 29032830.0000 - val_mae: 2837.3772\n",
      "Epoch 397/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1137.8171 - mse: 12985459.0000 - mae: 1137.8171 - val_loss: 2834.0361 - val_mse: 28910794.0000 - val_mae: 2834.0361\n",
      "Epoch 398/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1167.3033 - mse: 14193920.0000 - mae: 1167.3033 - val_loss: 2772.5884 - val_mse: 27486898.0000 - val_mae: 2772.5884\n",
      "Epoch 399/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1136.8654 - mse: 12546521.0000 - mae: 1136.8656 - val_loss: 2827.5088 - val_mse: 29075544.0000 - val_mae: 2827.5088\n",
      "Epoch 400/500\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 1170.4470 - mse: 14213222.0000 - mae: 1170.4470 - val_loss: 2813.6304 - val_mse: 28722126.0000 - val_mae: 2813.6304\n",
      "Epoch 401/500\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 1167.3364 - mse: 13611624.0000 - mae: 1167.3364 - val_loss: 2856.4302 - val_mse: 30012608.0000 - val_mae: 2856.4302\n",
      "Epoch 402/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1183.3754 - mse: 14453984.0000 - mae: 1183.3754 - val_loss: 2842.1060 - val_mse: 29256882.0000 - val_mae: 2842.1060\n",
      "Epoch 403/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1176.1028 - mse: 14561155.0000 - mae: 1176.1025 - val_loss: 2868.7881 - val_mse: 30428904.0000 - val_mae: 2868.7881\n",
      "Epoch 404/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1163.2651 - mse: 14148416.0000 - mae: 1163.2653 - val_loss: 2824.7305 - val_mse: 29059132.0000 - val_mae: 2824.7305\n",
      "Epoch 405/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1150.1766 - mse: 13222747.0000 - mae: 1150.1766 - val_loss: 2839.3835 - val_mse: 29396262.0000 - val_mae: 2839.3835\n",
      "Epoch 406/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1123.3256 - mse: 12399086.0000 - mae: 1123.3257 - val_loss: 2785.7017 - val_mse: 27693182.0000 - val_mae: 2785.7017\n",
      "Epoch 407/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1157.0527 - mse: 13542283.0000 - mae: 1157.0529 - val_loss: 2806.6587 - val_mse: 28201368.0000 - val_mae: 2806.6587\n",
      "Epoch 408/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1152.1794 - mse: 13446727.0000 - mae: 1152.1793 - val_loss: 2775.0864 - val_mse: 27299760.0000 - val_mae: 2775.0864\n",
      "Epoch 409/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1168.0694 - mse: 13779540.0000 - mae: 1168.0695 - val_loss: 2798.5005 - val_mse: 28110300.0000 - val_mae: 2798.5005\n",
      "Epoch 410/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1172.0495 - mse: 14092059.0000 - mae: 1172.0496 - val_loss: 2796.5154 - val_mse: 28055036.0000 - val_mae: 2796.5154\n",
      "Epoch 411/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1150.5618 - mse: 12496812.0000 - mae: 1150.5618 - val_loss: 2813.1050 - val_mse: 28531830.0000 - val_mae: 2813.1050\n",
      "Epoch 412/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1164.6833 - mse: 14114862.0000 - mae: 1164.6833 - val_loss: 2751.7053 - val_mse: 26687280.0000 - val_mae: 2751.7053\n",
      "Epoch 413/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1163.5056 - mse: 13707274.0000 - mae: 1163.5056 - val_loss: 2769.3206 - val_mse: 27480310.0000 - val_mae: 2769.3206\n",
      "Epoch 414/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1184.8749 - mse: 14764189.0000 - mae: 1184.8748 - val_loss: 2790.4749 - val_mse: 27824186.0000 - val_mae: 2790.4749\n",
      "Epoch 415/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1176.9101 - mse: 13991900.0000 - mae: 1176.9102 - val_loss: 2787.6975 - val_mse: 27750052.0000 - val_mae: 2787.6975\n",
      "Epoch 416/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1156.7673 - mse: 13956352.0000 - mae: 1156.7673 - val_loss: 2808.8508 - val_mse: 28382196.0000 - val_mae: 2808.8508\n",
      "Epoch 417/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1136.5312 - mse: 13292096.0000 - mae: 1136.5312 - val_loss: 2774.9612 - val_mse: 27558446.0000 - val_mae: 2774.9612\n",
      "Epoch 418/500\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 1166.2843 - mse: 14457386.0000 - mae: 1166.2843 - val_loss: 2805.9070 - val_mse: 28247488.0000 - val_mae: 2805.9070\n",
      "Epoch 419/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1165.2499 - mse: 14426520.0000 - mae: 1165.2499 - val_loss: 2808.2688 - val_mse: 28346618.0000 - val_mae: 2808.2688\n",
      "Epoch 420/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1179.1027 - mse: 14461116.0000 - mae: 1179.1025 - val_loss: 2842.0239 - val_mse: 29492480.0000 - val_mae: 2842.0239\n",
      "Epoch 421/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1177.8436 - mse: 14588214.0000 - mae: 1177.8436 - val_loss: 2825.5591 - val_mse: 29020066.0000 - val_mae: 2825.5591\n",
      "Epoch 422/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1150.5239 - mse: 13916868.0000 - mae: 1150.5239 - val_loss: 2817.7305 - val_mse: 28674328.0000 - val_mae: 2817.7305\n",
      "Epoch 423/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1167.0985 - mse: 14081489.0000 - mae: 1167.0984 - val_loss: 2870.2314 - val_mse: 30102854.0000 - val_mae: 2870.2314\n",
      "Epoch 424/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1127.7623 - mse: 12502565.0000 - mae: 1127.7625 - val_loss: 2840.8506 - val_mse: 29434818.0000 - val_mae: 2840.8506\n",
      "Epoch 425/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1153.4921 - mse: 13480781.0000 - mae: 1153.4921 - val_loss: 2874.4302 - val_mse: 30632760.0000 - val_mae: 2874.4302\n",
      "Epoch 426/500\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 1156.2414 - mse: 14143341.0000 - mae: 1156.2415 - val_loss: 2804.0212 - val_mse: 28324988.0000 - val_mae: 2804.0212\n",
      "Epoch 427/500\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 1150.0610 - mse: 13092742.0000 - mae: 1150.0609 - val_loss: 2778.7485 - val_mse: 27506724.0000 - val_mae: 2778.7485\n",
      "Epoch 428/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1179.9336 - mse: 14309119.0000 - mae: 1179.9336 - val_loss: 2767.2109 - val_mse: 26994770.0000 - val_mae: 2767.2109\n",
      "Epoch 429/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1154.9019 - mse: 13481776.0000 - mae: 1154.9019 - val_loss: 2809.8901 - val_mse: 28444014.0000 - val_mae: 2809.8901\n",
      "Epoch 430/500\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 1131.7756 - mse: 12991806.0000 - mae: 1131.7758 - val_loss: 2784.6580 - val_mse: 27813806.0000 - val_mae: 2784.6580\n",
      "Epoch 431/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1139.4665 - mse: 13216912.0000 - mae: 1139.4666 - val_loss: 2826.4805 - val_mse: 28975132.0000 - val_mae: 2826.4805\n",
      "Epoch 432/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1147.6403 - mse: 13557187.0000 - mae: 1147.6403 - val_loss: 2806.8718 - val_mse: 28399444.0000 - val_mae: 2806.8718\n",
      "Epoch 433/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1148.2872 - mse: 13609473.0000 - mae: 1148.2874 - val_loss: 2826.7786 - val_mse: 28665470.0000 - val_mae: 2826.7786\n",
      "Epoch 434/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1157.4846 - mse: 13460383.0000 - mae: 1157.4847 - val_loss: 2822.5876 - val_mse: 28669010.0000 - val_mae: 2822.5876\n",
      "Epoch 435/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1149.6740 - mse: 13287999.0000 - mae: 1149.6740 - val_loss: 2816.2000 - val_mse: 28555526.0000 - val_mae: 2816.2000\n",
      "Epoch 436/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1166.5312 - mse: 14046221.0000 - mae: 1166.5312 - val_loss: 2871.0396 - val_mse: 30126804.0000 - val_mae: 2871.0396\n",
      "Epoch 437/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1195.6153 - mse: 15671692.0000 - mae: 1195.6154 - val_loss: 2825.6682 - val_mse: 28763934.0000 - val_mae: 2825.6682\n",
      "Epoch 438/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1154.9265 - mse: 13199120.0000 - mae: 1154.9265 - val_loss: 2873.3684 - val_mse: 30615434.0000 - val_mae: 2873.3684\n",
      "Epoch 439/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1123.7473 - mse: 12595941.0000 - mae: 1123.7472 - val_loss: 2837.8335 - val_mse: 29183982.0000 - val_mae: 2837.8335\n",
      "Epoch 440/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1146.9436 - mse: 12930436.0000 - mae: 1146.9436 - val_loss: 2805.7075 - val_mse: 28209958.0000 - val_mae: 2805.7075\n",
      "Epoch 441/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1145.5656 - mse: 12893218.0000 - mae: 1145.5656 - val_loss: 2841.3247 - val_mse: 29062938.0000 - val_mae: 2841.3247\n",
      "Epoch 442/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1119.9103 - mse: 12664285.0000 - mae: 1119.9104 - val_loss: 2836.0393 - val_mse: 28905862.0000 - val_mae: 2836.0393\n",
      "Epoch 443/500\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 1153.0120 - mse: 13525044.0000 - mae: 1153.0120 - val_loss: 2801.3369 - val_mse: 27686772.0000 - val_mae: 2801.3369\n",
      "Epoch 444/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1161.4170 - mse: 14049681.0000 - mae: 1161.4171 - val_loss: 2752.0552 - val_mse: 26734772.0000 - val_mae: 2752.0552\n",
      "Epoch 445/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1150.2992 - mse: 13407527.0000 - mae: 1150.2994 - val_loss: 2824.0688 - val_mse: 28556176.0000 - val_mae: 2824.0688\n",
      "Epoch 446/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1163.0628 - mse: 14130802.0000 - mae: 1163.0627 - val_loss: 2838.9153 - val_mse: 28952138.0000 - val_mae: 2838.9153\n",
      "Epoch 447/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1144.4977 - mse: 13200030.0000 - mae: 1144.4978 - val_loss: 2818.3000 - val_mse: 28298076.0000 - val_mae: 2818.3000\n",
      "Epoch 448/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1141.1937 - mse: 12948741.0000 - mae: 1141.1936 - val_loss: 2803.3604 - val_mse: 28166490.0000 - val_mae: 2803.3604\n",
      "Epoch 449/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1136.9986 - mse: 13569733.0000 - mae: 1136.9985 - val_loss: 2849.8931 - val_mse: 29650846.0000 - val_mae: 2849.8931\n",
      "Epoch 450/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1148.1091 - mse: 13096724.0000 - mae: 1148.1093 - val_loss: 2840.0540 - val_mse: 29592076.0000 - val_mae: 2840.0540\n",
      "Epoch 451/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1134.8428 - mse: 13371996.0000 - mae: 1134.8428 - val_loss: 2807.8438 - val_mse: 28414520.0000 - val_mae: 2807.8438\n",
      "Epoch 452/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1145.5714 - mse: 13413448.0000 - mae: 1145.5714 - val_loss: 2791.2000 - val_mse: 28147836.0000 - val_mae: 2791.2000\n",
      "Epoch 453/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1145.7242 - mse: 14151983.0000 - mae: 1145.7241 - val_loss: 2756.3232 - val_mse: 26955846.0000 - val_mae: 2756.3232\n",
      "Epoch 454/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1152.0870 - mse: 13753310.0000 - mae: 1152.0870 - val_loss: 2776.7993 - val_mse: 27494302.0000 - val_mae: 2776.7993\n",
      "Epoch 455/500\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 1172.3447 - mse: 13981811.0000 - mae: 1172.3447 - val_loss: 2821.7324 - val_mse: 28720484.0000 - val_mae: 2821.7324\n",
      "Epoch 456/500\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1139.2475 - mse: 13069227.0000 - mae: 1139.2476 - val_loss: 2866.2249 - val_mse: 30321800.0000 - val_mae: 2866.2249\n",
      "Epoch 457/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1138.2103 - mse: 13413976.0000 - mae: 1138.2104 - val_loss: 2830.8584 - val_mse: 29104910.0000 - val_mae: 2830.8584\n",
      "Epoch 458/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1137.2287 - mse: 13007628.0000 - mae: 1137.2288 - val_loss: 2864.5911 - val_mse: 30116828.0000 - val_mae: 2864.5911\n",
      "Epoch 459/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1148.3615 - mse: 13948574.0000 - mae: 1148.3615 - val_loss: 2814.7085 - val_mse: 28400438.0000 - val_mae: 2814.7085\n",
      "Epoch 460/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1111.3931 - mse: 12334096.0000 - mae: 1111.3931 - val_loss: 2829.7361 - val_mse: 29243564.0000 - val_mae: 2829.7361\n",
      "Epoch 461/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1166.9676 - mse: 14063991.0000 - mae: 1166.9677 - val_loss: 2812.6562 - val_mse: 28331606.0000 - val_mae: 2812.6562\n",
      "Epoch 462/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1139.7854 - mse: 13218508.0000 - mae: 1139.7855 - val_loss: 2762.7476 - val_mse: 26918630.0000 - val_mae: 2762.7476\n",
      "Epoch 463/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1147.0854 - mse: 14061442.0000 - mae: 1147.0854 - val_loss: 2777.6965 - val_mse: 27303102.0000 - val_mae: 2777.6965\n",
      "Epoch 464/500\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 1162.1755 - mse: 14394778.0000 - mae: 1162.1757 - val_loss: 2800.1235 - val_mse: 28059496.0000 - val_mae: 2800.1235\n",
      "Epoch 465/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1147.8074 - mse: 13933070.0000 - mae: 1147.8074 - val_loss: 2785.1331 - val_mse: 27558014.0000 - val_mae: 2785.1331\n",
      "Epoch 466/500\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 1162.0063 - mse: 13945456.0000 - mae: 1162.0063 - val_loss: 2821.7319 - val_mse: 29030278.0000 - val_mae: 2821.7319\n",
      "Epoch 467/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1159.2398 - mse: 13910897.0000 - mae: 1159.2397 - val_loss: 2842.4937 - val_mse: 29620782.0000 - val_mae: 2842.4937\n",
      "Epoch 468/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1153.3885 - mse: 14182144.0000 - mae: 1153.3885 - val_loss: 2839.7886 - val_mse: 29649810.0000 - val_mae: 2839.7886\n",
      "Epoch 469/500\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 1133.3867 - mse: 12971836.0000 - mae: 1133.3867 - val_loss: 2777.1423 - val_mse: 27389674.0000 - val_mae: 2777.1423\n",
      "Epoch 470/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1161.4229 - mse: 13831935.0000 - mae: 1161.4229 - val_loss: 2797.0825 - val_mse: 27765384.0000 - val_mae: 2797.0825\n",
      "Epoch 471/500\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 1135.8615 - mse: 12897029.0000 - mae: 1135.8616 - val_loss: 2760.8635 - val_mse: 26663450.0000 - val_mae: 2760.8635\n",
      "Epoch 472/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1150.2987 - mse: 13559174.0000 - mae: 1150.2987 - val_loss: 2819.4119 - val_mse: 28424520.0000 - val_mae: 2819.4119\n",
      "Epoch 473/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1186.9169 - mse: 14481036.0000 - mae: 1186.9169 - val_loss: 2853.7053 - val_mse: 30124674.0000 - val_mae: 2853.7053\n",
      "Epoch 474/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1149.8527 - mse: 13325733.0000 - mae: 1149.8527 - val_loss: 2852.5693 - val_mse: 29677038.0000 - val_mae: 2852.5693\n",
      "Epoch 475/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1156.5006 - mse: 14106981.0000 - mae: 1156.5007 - val_loss: 2853.1885 - val_mse: 29840868.0000 - val_mae: 2853.1885\n",
      "Epoch 476/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1182.0517 - mse: 14575115.0000 - mae: 1182.0518 - val_loss: 2829.4102 - val_mse: 29005810.0000 - val_mae: 2829.4102\n",
      "Epoch 477/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1150.6011 - mse: 13491513.0000 - mae: 1150.6010 - val_loss: 2818.3342 - val_mse: 28455022.0000 - val_mae: 2818.3342\n",
      "Epoch 478/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1124.2676 - mse: 12597758.0000 - mae: 1124.2678 - val_loss: 2788.3086 - val_mse: 27783332.0000 - val_mae: 2788.3086\n",
      "Epoch 479/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1160.7763 - mse: 13579436.0000 - mae: 1160.7762 - val_loss: 2793.7903 - val_mse: 27688858.0000 - val_mae: 2793.7903\n",
      "Epoch 480/500\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 1131.5259 - mse: 12534721.0000 - mae: 1131.5259 - val_loss: 2766.4558 - val_mse: 26921640.0000 - val_mae: 2766.4558\n",
      "Epoch 481/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1140.7329 - mse: 12837759.0000 - mae: 1140.7330 - val_loss: 2757.4812 - val_mse: 26716916.0000 - val_mae: 2757.4812\n",
      "Epoch 482/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1119.7738 - mse: 12463896.0000 - mae: 1119.7736 - val_loss: 2811.8848 - val_mse: 28375380.0000 - val_mae: 2811.8848\n",
      "Epoch 483/500\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 1140.5334 - mse: 13026585.0000 - mae: 1140.5334 - val_loss: 2848.2803 - val_mse: 29456074.0000 - val_mae: 2848.2803\n",
      "Epoch 484/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1155.2510 - mse: 13823948.0000 - mae: 1155.2509 - val_loss: 2813.5352 - val_mse: 28499976.0000 - val_mae: 2813.5352\n",
      "Epoch 485/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1132.0112 - mse: 13048993.0000 - mae: 1132.0112 - val_loss: 2837.6475 - val_mse: 29229912.0000 - val_mae: 2837.6475\n",
      "Epoch 486/500\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 1151.6609 - mse: 13564294.0000 - mae: 1151.6609 - val_loss: 2862.1292 - val_mse: 29888024.0000 - val_mae: 2862.1292\n",
      "Epoch 487/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1131.8631 - mse: 13349433.0000 - mae: 1131.8632 - val_loss: 2830.4485 - val_mse: 28985032.0000 - val_mae: 2830.4485\n",
      "Epoch 488/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1157.1797 - mse: 13354331.0000 - mae: 1157.1797 - val_loss: 2821.7336 - val_mse: 28641808.0000 - val_mae: 2821.7336\n",
      "Epoch 489/500\n",
      "10000/10000 [==============================] - 1s 50us/step - loss: 1147.0836 - mse: 13638744.0000 - mae: 1147.0837 - val_loss: 2806.1316 - val_mse: 28085066.0000 - val_mae: 2806.1316\n",
      "Epoch 490/500\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 1149.3689 - mse: 13492234.0000 - mae: 1149.3689 - val_loss: 2850.5425 - val_mse: 29556510.0000 - val_mae: 2850.5425\n",
      "Epoch 491/500\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 1146.9393 - mse: 13766105.0000 - mae: 1146.9395 - val_loss: 2785.3528 - val_mse: 27558454.0000 - val_mae: 2785.3528\n",
      "Epoch 492/500\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 1128.3466 - mse: 12541464.0000 - mae: 1128.3466 - val_loss: 2814.2607 - val_mse: 28165406.0000 - val_mae: 2814.2607\n",
      "Epoch 493/500\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 1125.8104 - mse: 13368639.0000 - mae: 1125.8105 - val_loss: 2769.5986 - val_mse: 26910572.0000 - val_mae: 2769.5986\n",
      "Epoch 494/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1107.3553 - mse: 12781642.0000 - mae: 1107.3552 - val_loss: 2799.3564 - val_mse: 27882664.0000 - val_mae: 2799.3564\n",
      "Epoch 495/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1104.6685 - mse: 12155425.0000 - mae: 1104.6685 - val_loss: 2791.3772 - val_mse: 27567018.0000 - val_mae: 2791.3772\n",
      "Epoch 496/500\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1145.3776 - mse: 13109679.0000 - mae: 1145.3776 - val_loss: 2815.7473 - val_mse: 28241252.0000 - val_mae: 2815.7473\n",
      "Epoch 497/500\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 1159.1121 - mse: 14026637.0000 - mae: 1159.1121 - val_loss: 2835.8694 - val_mse: 28832874.0000 - val_mae: 2835.8694\n",
      "Epoch 498/500\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1145.5618 - mse: 14107723.0000 - mae: 1145.5619 - val_loss: 2806.3696 - val_mse: 28097904.0000 - val_mae: 2806.3696\n",
      "Epoch 499/500\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 1144.4421 - mse: 13954345.0000 - mae: 1144.4421 - val_loss: 2799.0015 - val_mse: 28076372.0000 - val_mae: 2799.0015\n",
      "Epoch 500/500\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 1156.3357 - mse: 14116146.0000 - mae: 1156.3358 - val_loss: 2803.2896 - val_mse: 28165034.0000 - val_mae: 2803.2896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11500 samples, validate on 500 samples\n",
      "Epoch 1/500\n",
      "11500/11500 [==============================] - 2s 216us/step - loss: 3372.5542 - mse: 120432808.0000 - mae: 3372.5544 - val_loss: 9166.7617 - val_mse: 612596736.0000 - val_mae: 9166.7617\n",
      "Epoch 2/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 3345.2165 - mse: 120306136.0000 - mae: 3345.2168 - val_loss: 9123.3721 - val_mse: 612121536.0000 - val_mae: 9123.3721\n",
      "Epoch 3/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 3325.1733 - mse: 120136912.0000 - mae: 3325.1736 - val_loss: 9081.7686 - val_mse: 611603968.0000 - val_mae: 9081.7686\n",
      "Epoch 4/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 3317.9344 - mse: 119999936.0000 - mae: 3317.9343 - val_loss: 9060.5967 - val_mse: 611254592.0000 - val_mae: 9060.5967\n",
      "Epoch 5/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 3315.3233 - mse: 119905952.0000 - mae: 3315.3232 - val_loss: 9050.1660 - val_mse: 611031296.0000 - val_mae: 9050.1660\n",
      "Epoch 6/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 3313.7801 - mse: 119847504.0000 - mae: 3313.7803 - val_loss: 9045.0088 - val_mse: 610893120.0000 - val_mae: 9045.0088\n",
      "Epoch 7/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 3313.0641 - mse: 119813704.0000 - mae: 3313.0645 - val_loss: 9040.8281 - val_mse: 610774528.0000 - val_mae: 9040.8281\n",
      "Epoch 8/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 3311.3946 - mse: 119762552.0000 - mae: 3311.3945 - val_loss: 9038.8818 - val_mse: 610693312.0000 - val_mae: 9038.8818\n",
      "Epoch 9/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 3310.0765 - mse: 119721920.0000 - mae: 3310.0764 - val_loss: 9034.9736 - val_mse: 610564736.0000 - val_mae: 9034.9736\n",
      "Epoch 10/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 3308.0949 - mse: 119660720.0000 - mae: 3308.0950 - val_loss: 9030.9219 - val_mse: 610411392.0000 - val_mae: 9030.9219\n",
      "Epoch 11/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 3307.5480 - mse: 119616976.0000 - mae: 3307.5479 - val_loss: 9025.9541 - val_mse: 610202624.0000 - val_mae: 9025.9541\n",
      "Epoch 12/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 3305.1257 - mse: 119515288.0000 - mae: 3305.1260 - val_loss: 9021.5049 - val_mse: 609909056.0000 - val_mae: 9021.5049\n",
      "Epoch 13/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 3300.7177 - mse: 119332880.0000 - mae: 3300.7180 - val_loss: 9012.8330 - val_mse: 609430464.0000 - val_mae: 9012.8330\n",
      "Epoch 14/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 3296.6287 - mse: 119127640.0000 - mae: 3296.6284 - val_loss: 9003.7021 - val_mse: 608693056.0000 - val_mae: 9003.7021\n",
      "Epoch 15/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 3293.6698 - mse: 118912864.0000 - mae: 3293.6692 - val_loss: 8994.6729 - val_mse: 608237376.0000 - val_mae: 8994.6729\n",
      "Epoch 16/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 3290.7374 - mse: 118733336.0000 - mae: 3290.7373 - val_loss: 8986.0195 - val_mse: 607629824.0000 - val_mae: 8986.0195\n",
      "Epoch 17/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 3287.1795 - mse: 118552688.0000 - mae: 3287.1794 - val_loss: 8979.9727 - val_mse: 607273088.0000 - val_mae: 8979.9727\n",
      "Epoch 18/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 3285.2847 - mse: 118467400.0000 - mae: 3285.2849 - val_loss: 8975.2627 - val_mse: 607014400.0000 - val_mae: 8975.2627\n",
      "Epoch 19/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 3283.1897 - mse: 118362848.0000 - mae: 3283.1895 - val_loss: 8971.6377 - val_mse: 606817024.0000 - val_mae: 8971.6377\n",
      "Epoch 20/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 3283.4328 - mse: 118318488.0000 - mae: 3283.4326 - val_loss: 8967.8613 - val_mse: 606629568.0000 - val_mae: 8967.8613\n",
      "Epoch 21/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 3282.2277 - mse: 118256576.0000 - mae: 3282.2283 - val_loss: 8966.7871 - val_mse: 606575872.0000 - val_mae: 8966.7871\n",
      "Epoch 22/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 3282.3569 - mse: 118225792.0000 - mae: 3282.3564 - val_loss: 8966.0771 - val_mse: 606534080.0000 - val_mae: 8966.0771\n",
      "Epoch 23/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 3281.4921 - mse: 118204048.0000 - mae: 3281.4922 - val_loss: 8966.2432 - val_mse: 606528192.0000 - val_mae: 8966.2432\n",
      "Epoch 24/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 3282.4748 - mse: 118236816.0000 - mae: 3282.4751 - val_loss: 8966.4150 - val_mse: 606524480.0000 - val_mae: 8966.4150\n",
      "Epoch 25/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 3279.8740 - mse: 118220432.0000 - mae: 3279.8738 - val_loss: 8965.2998 - val_mse: 606465664.0000 - val_mae: 8965.2998\n",
      "Epoch 26/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 3278.6658 - mse: 118205712.0000 - mae: 3278.6658 - val_loss: 8968.8330 - val_mse: 606469248.0000 - val_mae: 8968.8330\n",
      "Epoch 27/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 3241.2923 - mse: 117973504.0000 - mae: 3241.2922 - val_loss: 9024.6475 - val_mse: 605120704.0000 - val_mae: 9024.6475\n",
      "Epoch 28/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 3198.7117 - mse: 117023680.0000 - mae: 3198.7117 - val_loss: 8955.2266 - val_mse: 602244096.0000 - val_mae: 8955.2266\n",
      "Epoch 29/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 3167.0136 - mse: 116131104.0000 - mae: 3167.0139 - val_loss: 8907.9521 - val_mse: 599874560.0000 - val_mae: 8907.9521\n",
      "Epoch 30/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 3147.0016 - mse: 115329920.0000 - mae: 3147.0015 - val_loss: 8863.8555 - val_mse: 597689920.0000 - val_mae: 8863.8555\n",
      "Epoch 31/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 3124.2573 - mse: 114440464.0000 - mae: 3124.2573 - val_loss: 8819.7969 - val_mse: 595399872.0000 - val_mae: 8819.7969\n",
      "Epoch 32/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 3103.6762 - mse: 113697264.0000 - mae: 3103.6763 - val_loss: 8771.3877 - val_mse: 592967232.0000 - val_mae: 8771.3877\n",
      "Epoch 33/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 3086.9490 - mse: 112971808.0000 - mae: 3086.9495 - val_loss: 8725.0723 - val_mse: 590512896.0000 - val_mae: 8725.0723\n",
      "Epoch 34/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 3070.8902 - mse: 112180208.0000 - mae: 3070.8896 - val_loss: 8687.1172 - val_mse: 588237440.0000 - val_mae: 8687.1172\n",
      "Epoch 35/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 3052.8369 - mse: 111213576.0000 - mae: 3052.8369 - val_loss: 8644.5771 - val_mse: 585751040.0000 - val_mae: 8644.5771\n",
      "Epoch 36/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 3039.1989 - mse: 110489048.0000 - mae: 3039.1990 - val_loss: 8601.0117 - val_mse: 583267136.0000 - val_mae: 8601.0117\n",
      "Epoch 37/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 3021.9554 - mse: 109343936.0000 - mae: 3021.9551 - val_loss: 8553.3262 - val_mse: 580676992.0000 - val_mae: 8553.3262\n",
      "Epoch 38/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 3004.4405 - mse: 108637376.0000 - mae: 3004.4407 - val_loss: 8522.4551 - val_mse: 578408832.0000 - val_mae: 8522.4551\n",
      "Epoch 39/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2990.7476 - mse: 107746904.0000 - mae: 2990.7478 - val_loss: 8473.9443 - val_mse: 575741248.0000 - val_mae: 8473.9443\n",
      "Epoch 40/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2976.2567 - mse: 106931640.0000 - mae: 2976.2563 - val_loss: 8433.2568 - val_mse: 573235776.0000 - val_mae: 8433.2568\n",
      "Epoch 41/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2961.7120 - mse: 106252128.0000 - mae: 2961.7119 - val_loss: 8396.3613 - val_mse: 570764160.0000 - val_mae: 8396.3613\n",
      "Epoch 42/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2943.5675 - mse: 104962960.0000 - mae: 2943.5676 - val_loss: 8355.6152 - val_mse: 568137536.0000 - val_mae: 8355.6152\n",
      "Epoch 43/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2929.8430 - mse: 104185440.0000 - mae: 2929.8428 - val_loss: 8315.0176 - val_mse: 565484480.0000 - val_mae: 8315.0176\n",
      "Epoch 44/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2911.8493 - mse: 103129696.0000 - mae: 2911.8494 - val_loss: 8284.5195 - val_mse: 563059456.0000 - val_mae: 8284.5195\n",
      "Epoch 45/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 2905.7673 - mse: 102620624.0000 - mae: 2905.7676 - val_loss: 8247.0146 - val_mse: 560506752.0000 - val_mae: 8247.0146\n",
      "Epoch 46/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2890.6163 - mse: 101562072.0000 - mae: 2890.6162 - val_loss: 8212.5547 - val_mse: 557985024.0000 - val_mae: 8212.5547\n",
      "Epoch 47/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 2889.3444 - mse: 100810144.0000 - mae: 2889.3445 - val_loss: 8192.9736 - val_mse: 556009792.0000 - val_mae: 8192.9736\n",
      "Epoch 48/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 2865.4023 - mse: 99909312.0000 - mae: 2865.4021 - val_loss: 8165.9360 - val_mse: 553673024.0000 - val_mae: 8165.9360\n",
      "Epoch 49/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 2860.2480 - mse: 99608872.0000 - mae: 2860.2480 - val_loss: 8137.4888 - val_mse: 551618112.0000 - val_mae: 8137.4888\n",
      "Epoch 50/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 2854.0326 - mse: 98659760.0000 - mae: 2854.0322 - val_loss: 8118.7061 - val_mse: 549648064.0000 - val_mae: 8118.7061\n",
      "Epoch 51/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2843.6366 - mse: 97833712.0000 - mae: 2843.6367 - val_loss: 8093.6445 - val_mse: 547582336.0000 - val_mae: 8093.6445\n",
      "Epoch 52/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2829.8631 - mse: 97385504.0000 - mae: 2829.8630 - val_loss: 8070.5107 - val_mse: 545705728.0000 - val_mae: 8070.5107\n",
      "Epoch 53/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2828.7919 - mse: 96723384.0000 - mae: 2828.7917 - val_loss: 8042.6948 - val_mse: 543736832.0000 - val_mae: 8042.6948\n",
      "Epoch 54/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2818.4117 - mse: 96187640.0000 - mae: 2818.4121 - val_loss: 8022.1934 - val_mse: 541977472.0000 - val_mae: 8022.1934\n",
      "Epoch 55/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2807.7536 - mse: 95376904.0000 - mae: 2807.7537 - val_loss: 8005.0024 - val_mse: 540253440.0000 - val_mae: 8005.0024\n",
      "Epoch 56/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2803.1235 - mse: 95224360.0000 - mae: 2803.1235 - val_loss: 7983.6182 - val_mse: 538474112.0000 - val_mae: 7983.6182\n",
      "Epoch 57/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2801.1870 - mse: 94440024.0000 - mae: 2801.1870 - val_loss: 7958.0054 - val_mse: 536798048.0000 - val_mae: 7958.0054\n",
      "Epoch 58/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 2799.0258 - mse: 94232648.0000 - mae: 2799.0256 - val_loss: 7952.3491 - val_mse: 535454272.0000 - val_mae: 7952.3491\n",
      "Epoch 59/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 2787.6449 - mse: 93379032.0000 - mae: 2787.6448 - val_loss: 7922.0791 - val_mse: 533598592.0000 - val_mae: 7922.0791\n",
      "Epoch 60/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2775.4220 - mse: 92424352.0000 - mae: 2775.4221 - val_loss: 7899.6504 - val_mse: 531749600.0000 - val_mae: 7899.6504\n",
      "Epoch 61/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2782.3534 - mse: 92383936.0000 - mae: 2782.3533 - val_loss: 7878.4985 - val_mse: 530197248.0000 - val_mae: 7878.4985\n",
      "Epoch 62/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2771.9806 - mse: 91781120.0000 - mae: 2771.9810 - val_loss: 7862.3716 - val_mse: 528658912.0000 - val_mae: 7862.3716\n",
      "Epoch 63/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2763.9863 - mse: 91018760.0000 - mae: 2763.9863 - val_loss: 7847.6245 - val_mse: 527245952.0000 - val_mae: 7847.6245\n",
      "Epoch 64/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2760.1586 - mse: 90649112.0000 - mae: 2760.1587 - val_loss: 7830.9258 - val_mse: 525820448.0000 - val_mae: 7830.9258\n",
      "Epoch 65/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 2764.3096 - mse: 90987016.0000 - mae: 2764.3096 - val_loss: 7804.5850 - val_mse: 524070880.0000 - val_mae: 7804.5850\n",
      "Epoch 66/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2747.4129 - mse: 89883312.0000 - mae: 2747.4128 - val_loss: 7784.7017 - val_mse: 522335808.0000 - val_mae: 7784.7017\n",
      "Epoch 67/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2742.3905 - mse: 89523904.0000 - mae: 2742.3909 - val_loss: 7763.2393 - val_mse: 520677920.0000 - val_mae: 7763.2393\n",
      "Epoch 68/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2734.0827 - mse: 89092920.0000 - mae: 2734.0825 - val_loss: 7748.2515 - val_mse: 519267680.0000 - val_mae: 7748.2515\n",
      "Epoch 69/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 2725.2234 - mse: 88084920.0000 - mae: 2725.2234 - val_loss: 7712.0220 - val_mse: 517339296.0000 - val_mae: 7712.0220\n",
      "Epoch 70/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 2717.4897 - mse: 87579584.0000 - mae: 2717.4900 - val_loss: 7693.0317 - val_mse: 515768256.0000 - val_mae: 7693.0317\n",
      "Epoch 71/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2721.0441 - mse: 87372480.0000 - mae: 2721.0439 - val_loss: 7683.3828 - val_mse: 514520448.0000 - val_mae: 7683.3828\n",
      "Epoch 72/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2714.0433 - mse: 86995488.0000 - mae: 2714.0432 - val_loss: 7660.3389 - val_mse: 512930272.0000 - val_mae: 7660.3389\n",
      "Epoch 73/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2706.4453 - mse: 86788880.0000 - mae: 2706.4453 - val_loss: 7636.4395 - val_mse: 511169120.0000 - val_mae: 7636.4395\n",
      "Epoch 74/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2693.6862 - mse: 85665312.0000 - mae: 2693.6863 - val_loss: 7612.4023 - val_mse: 509504832.0000 - val_mae: 7612.4023\n",
      "Epoch 75/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 2678.0365 - mse: 85198856.0000 - mae: 2678.0366 - val_loss: 7585.5454 - val_mse: 507445568.0000 - val_mae: 7585.5454\n",
      "Epoch 76/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2671.9465 - mse: 84385736.0000 - mae: 2671.9465 - val_loss: 7562.7993 - val_mse: 505480096.0000 - val_mae: 7562.7993\n",
      "Epoch 77/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2663.6903 - mse: 83701904.0000 - mae: 2663.6902 - val_loss: 7530.4917 - val_mse: 503273600.0000 - val_mae: 7530.4917\n",
      "Epoch 78/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2655.6846 - mse: 83146072.0000 - mae: 2655.6848 - val_loss: 7502.7778 - val_mse: 501044096.0000 - val_mae: 7502.7778\n",
      "Epoch 79/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 2644.0153 - mse: 82579032.0000 - mae: 2644.0151 - val_loss: 7474.5781 - val_mse: 498863328.0000 - val_mae: 7474.5781\n",
      "Epoch 80/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 2633.2239 - mse: 82253800.0000 - mae: 2633.2236 - val_loss: 7438.2007 - val_mse: 496187808.0000 - val_mae: 7438.2007\n",
      "Epoch 81/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 2602.9040 - mse: 80595824.0000 - mae: 2602.9041 - val_loss: 7410.9893 - val_mse: 493816160.0000 - val_mae: 7410.9893\n",
      "Epoch 82/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2605.0037 - mse: 80265984.0000 - mae: 2605.0039 - val_loss: 7385.8823 - val_mse: 491337536.0000 - val_mae: 7385.8823\n",
      "Epoch 83/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2582.5125 - mse: 79213760.0000 - mae: 2582.5125 - val_loss: 7347.1577 - val_mse: 488452320.0000 - val_mae: 7347.1577\n",
      "Epoch 84/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2579.5853 - mse: 78583120.0000 - mae: 2579.5854 - val_loss: 7314.4072 - val_mse: 485568960.0000 - val_mae: 7314.4072\n",
      "Epoch 85/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2552.8936 - mse: 77474112.0000 - mae: 2552.8938 - val_loss: 7281.0117 - val_mse: 482616128.0000 - val_mae: 7281.0117\n",
      "Epoch 86/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2519.3467 - mse: 75911560.0000 - mae: 2519.3467 - val_loss: 7241.0811 - val_mse: 478964768.0000 - val_mae: 7241.0811\n",
      "Epoch 87/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2521.3394 - mse: 75995704.0000 - mae: 2521.3391 - val_loss: 7207.9106 - val_mse: 475537888.0000 - val_mae: 7207.9106\n",
      "Epoch 88/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2489.7116 - mse: 74036176.0000 - mae: 2489.7117 - val_loss: 7158.4644 - val_mse: 471486112.0000 - val_mae: 7158.4644\n",
      "Epoch 89/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 2479.4690 - mse: 73159464.0000 - mae: 2479.4688 - val_loss: 7114.6577 - val_mse: 467515296.0000 - val_mae: 7114.6577\n",
      "Epoch 90/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 2443.8903 - mse: 71095376.0000 - mae: 2443.8904 - val_loss: 7057.5122 - val_mse: 462994432.0000 - val_mae: 7057.5122\n",
      "Epoch 91/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 2425.2893 - mse: 70153120.0000 - mae: 2425.2893 - val_loss: 7014.0005 - val_mse: 458789856.0000 - val_mae: 7014.0005\n",
      "Epoch 92/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 2395.0150 - mse: 67920344.0000 - mae: 2395.0149 - val_loss: 6949.8545 - val_mse: 453665344.0000 - val_mae: 6949.8545\n",
      "Epoch 93/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2345.6246 - mse: 65586468.0000 - mae: 2345.6243 - val_loss: 6864.7964 - val_mse: 447875136.0000 - val_mae: 6864.7964\n",
      "Epoch 94/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2344.6856 - mse: 64831812.0000 - mae: 2344.6855 - val_loss: 6822.7031 - val_mse: 443325184.0000 - val_mae: 6822.7031\n",
      "Epoch 95/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2332.7509 - mse: 63524040.0000 - mae: 2332.7505 - val_loss: 6766.7041 - val_mse: 438469760.0000 - val_mae: 6766.7041\n",
      "Epoch 96/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2306.5671 - mse: 62274280.0000 - mae: 2306.5669 - val_loss: 6694.3569 - val_mse: 433056384.0000 - val_mae: 6694.3569\n",
      "Epoch 97/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2260.9199 - mse: 60135980.0000 - mae: 2260.9199 - val_loss: 6635.0386 - val_mse: 427802880.0000 - val_mae: 6635.0386\n",
      "Epoch 98/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2244.4321 - mse: 58828768.0000 - mae: 2244.4321 - val_loss: 6588.6699 - val_mse: 423054432.0000 - val_mae: 6588.6699\n",
      "Epoch 99/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2221.8994 - mse: 58537700.0000 - mae: 2221.8992 - val_loss: 6514.3931 - val_mse: 417618048.0000 - val_mae: 6514.3931\n",
      "Epoch 100/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2186.4235 - mse: 56384440.0000 - mae: 2186.4236 - val_loss: 6461.9849 - val_mse: 412563424.0000 - val_mae: 6461.9849\n",
      "Epoch 101/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2154.7346 - mse: 53654676.0000 - mae: 2154.7349 - val_loss: 6416.5410 - val_mse: 407849824.0000 - val_mae: 6416.5410\n",
      "Epoch 102/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2125.9706 - mse: 52499860.0000 - mae: 2125.9707 - val_loss: 6372.5967 - val_mse: 403069760.0000 - val_mae: 6372.5967\n",
      "Epoch 103/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2101.0330 - mse: 50744792.0000 - mae: 2101.0330 - val_loss: 6291.8574 - val_mse: 397426048.0000 - val_mae: 6291.8574\n",
      "Epoch 104/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 2094.2482 - mse: 49747512.0000 - mae: 2094.2483 - val_loss: 6257.5654 - val_mse: 392983232.0000 - val_mae: 6257.5654\n",
      "Epoch 105/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 2038.3240 - mse: 47769616.0000 - mae: 2038.3240 - val_loss: 6208.6621 - val_mse: 388327712.0000 - val_mae: 6208.6621\n",
      "Epoch 106/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 2037.2370 - mse: 47334192.0000 - mae: 2037.2368 - val_loss: 6130.1362 - val_mse: 383114656.0000 - val_mae: 6130.1362\n",
      "Epoch 107/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 2013.0691 - mse: 45530128.0000 - mae: 2013.0691 - val_loss: 6090.6582 - val_mse: 378952256.0000 - val_mae: 6090.6582\n",
      "Epoch 108/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 1986.8881 - mse: 44235744.0000 - mae: 1986.8879 - val_loss: 6039.1577 - val_mse: 374070464.0000 - val_mae: 6039.1577\n",
      "Epoch 109/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1993.9576 - mse: 44562792.0000 - mae: 1993.9574 - val_loss: 6010.0005 - val_mse: 370070432.0000 - val_mae: 6010.0005\n",
      "Epoch 110/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1945.6412 - mse: 41789152.0000 - mae: 1945.6412 - val_loss: 5947.0698 - val_mse: 365085760.0000 - val_mae: 5947.0698\n",
      "Epoch 111/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1922.6282 - mse: 40251312.0000 - mae: 1922.6285 - val_loss: 5912.6865 - val_mse: 360958752.0000 - val_mae: 5912.6865\n",
      "Epoch 112/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1897.1057 - mse: 38853748.0000 - mae: 1897.1057 - val_loss: 5844.0503 - val_mse: 356117696.0000 - val_mae: 5844.0503\n",
      "Epoch 113/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 1888.6717 - mse: 38790332.0000 - mae: 1888.6715 - val_loss: 5830.2847 - val_mse: 352724832.0000 - val_mae: 5830.2847\n",
      "Epoch 114/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 1866.2621 - mse: 38363716.0000 - mae: 1866.2621 - val_loss: 5798.1226 - val_mse: 349035072.0000 - val_mae: 5798.1226\n",
      "Epoch 115/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 1847.7793 - mse: 36732884.0000 - mae: 1847.7793 - val_loss: 5729.8066 - val_mse: 344121984.0000 - val_mae: 5729.8066\n",
      "Epoch 116/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1838.6298 - mse: 36593876.0000 - mae: 1838.6295 - val_loss: 5711.7383 - val_mse: 340967872.0000 - val_mae: 5711.7383\n",
      "Epoch 117/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1831.5132 - mse: 35358712.0000 - mae: 1831.5132 - val_loss: 5737.9399 - val_mse: 339181824.0000 - val_mae: 5737.9399\n",
      "Epoch 118/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1818.0482 - mse: 34800464.0000 - mae: 1818.0485 - val_loss: 5727.4624 - val_mse: 336052960.0000 - val_mae: 5727.4624\n",
      "Epoch 119/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 1815.2377 - mse: 34350776.0000 - mae: 1815.2375 - val_loss: 5691.6343 - val_mse: 332280224.0000 - val_mae: 5691.6343\n",
      "Epoch 120/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1793.5194 - mse: 33779704.0000 - mae: 1793.5195 - val_loss: 5703.5391 - val_mse: 329957952.0000 - val_mae: 5703.5391\n",
      "Epoch 121/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1769.3515 - mse: 31881448.0000 - mae: 1769.3517 - val_loss: 5694.2891 - val_mse: 327444832.0000 - val_mae: 5694.2891\n",
      "Epoch 122/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1765.1066 - mse: 32099168.0000 - mae: 1765.1069 - val_loss: 5676.1812 - val_mse: 324288064.0000 - val_mae: 5676.1812\n",
      "Epoch 123/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1734.0335 - mse: 30627218.0000 - mae: 1734.0334 - val_loss: 5653.3037 - val_mse: 321505056.0000 - val_mae: 5653.3037\n",
      "Epoch 124/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 1728.9003 - mse: 30118020.0000 - mae: 1728.9001 - val_loss: 5633.3232 - val_mse: 318113504.0000 - val_mae: 5633.3232\n",
      "Epoch 125/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1728.0639 - mse: 30599182.0000 - mae: 1728.0638 - val_loss: 5643.4780 - val_mse: 316974528.0000 - val_mae: 5643.4780\n",
      "Epoch 126/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1735.8447 - mse: 30395844.0000 - mae: 1735.8447 - val_loss: 5631.5190 - val_mse: 314538880.0000 - val_mae: 5631.5190\n",
      "Epoch 127/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1714.5446 - mse: 29093994.0000 - mae: 1714.5446 - val_loss: 5608.5840 - val_mse: 311595008.0000 - val_mae: 5608.5840\n",
      "Epoch 128/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1720.1695 - mse: 29691446.0000 - mae: 1720.1696 - val_loss: 5613.7456 - val_mse: 310188736.0000 - val_mae: 5613.7456\n",
      "Epoch 129/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1664.1011 - mse: 27542710.0000 - mae: 1664.1011 - val_loss: 5597.8945 - val_mse: 308077792.0000 - val_mae: 5597.8945\n",
      "Epoch 130/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1675.6217 - mse: 27307792.0000 - mae: 1675.6216 - val_loss: 5577.8789 - val_mse: 305420224.0000 - val_mae: 5577.8789\n",
      "Epoch 131/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1707.7753 - mse: 28979474.0000 - mae: 1707.7751 - val_loss: 5568.5845 - val_mse: 303821664.0000 - val_mae: 5568.5845\n",
      "Epoch 132/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1676.4051 - mse: 27726438.0000 - mae: 1676.4053 - val_loss: 5564.4829 - val_mse: 301542880.0000 - val_mae: 5564.4829\n",
      "Epoch 133/500\n",
      "11500/11500 [==============================] - 0s 38us/step - loss: 1674.6340 - mse: 26615156.0000 - mae: 1674.6340 - val_loss: 5591.1367 - val_mse: 301023232.0000 - val_mae: 5591.1367\n",
      "Epoch 134/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1693.1986 - mse: 27716408.0000 - mae: 1693.1987 - val_loss: 5562.8232 - val_mse: 298477856.0000 - val_mae: 5562.8232\n",
      "Epoch 135/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1648.3425 - mse: 26445306.0000 - mae: 1648.3424 - val_loss: 5532.3154 - val_mse: 295806752.0000 - val_mae: 5532.3154\n",
      "Epoch 136/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1646.8546 - mse: 26584154.0000 - mae: 1646.8546 - val_loss: 5563.9634 - val_mse: 295926944.0000 - val_mae: 5563.9634\n",
      "Epoch 137/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1644.5287 - mse: 26114440.0000 - mae: 1644.5287 - val_loss: 5592.7827 - val_mse: 294929760.0000 - val_mae: 5592.7827\n",
      "Epoch 138/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1631.1284 - mse: 25104420.0000 - mae: 1631.1283 - val_loss: 5597.6382 - val_mse: 293154112.0000 - val_mae: 5597.6382\n",
      "Epoch 139/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1636.0471 - mse: 25571010.0000 - mae: 1636.0471 - val_loss: 5573.4780 - val_mse: 291445792.0000 - val_mae: 5573.4780\n",
      "Epoch 140/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1631.6231 - mse: 25531822.0000 - mae: 1631.6229 - val_loss: 5566.7383 - val_mse: 290555040.0000 - val_mae: 5566.7383\n",
      "Epoch 141/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 1641.9697 - mse: 25742852.0000 - mae: 1641.9697 - val_loss: 5572.3350 - val_mse: 289260896.0000 - val_mae: 5572.3350\n",
      "Epoch 142/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1645.7692 - mse: 26410726.0000 - mae: 1645.7692 - val_loss: 5561.4087 - val_mse: 287776832.0000 - val_mae: 5561.4087\n",
      "Epoch 143/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 1628.4188 - mse: 24863044.0000 - mae: 1628.4188 - val_loss: 5566.1108 - val_mse: 285879616.0000 - val_mae: 5566.1108\n",
      "Epoch 144/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 1622.1700 - mse: 24944866.0000 - mae: 1622.1700 - val_loss: 5566.1357 - val_mse: 285510272.0000 - val_mae: 5566.1357\n",
      "Epoch 145/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1618.7496 - mse: 24174758.0000 - mae: 1618.7498 - val_loss: 5556.1328 - val_mse: 284581632.0000 - val_mae: 5556.1328\n",
      "Epoch 146/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1619.2172 - mse: 24763684.0000 - mae: 1619.2172 - val_loss: 5542.7710 - val_mse: 283478496.0000 - val_mae: 5542.7710\n",
      "Epoch 147/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1602.1674 - mse: 24710292.0000 - mae: 1602.1675 - val_loss: 5534.3398 - val_mse: 282355488.0000 - val_mae: 5534.3398\n",
      "Epoch 148/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1586.2214 - mse: 24068908.0000 - mae: 1586.2212 - val_loss: 5563.2920 - val_mse: 280952832.0000 - val_mae: 5563.2920\n",
      "Epoch 149/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1606.5165 - mse: 24650760.0000 - mae: 1606.5167 - val_loss: 5550.1216 - val_mse: 279699584.0000 - val_mae: 5550.1216\n",
      "Epoch 150/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1594.5513 - mse: 24066540.0000 - mae: 1594.5515 - val_loss: 5550.9712 - val_mse: 278421120.0000 - val_mae: 5550.9712\n",
      "Epoch 151/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1607.7232 - mse: 24664058.0000 - mae: 1607.7231 - val_loss: 5554.9976 - val_mse: 277049216.0000 - val_mae: 5554.9976\n",
      "Epoch 152/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1557.9359 - mse: 22472776.0000 - mae: 1557.9357 - val_loss: 5517.3584 - val_mse: 276087968.0000 - val_mae: 5517.3584\n",
      "Epoch 153/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1586.2080 - mse: 23637860.0000 - mae: 1586.2080 - val_loss: 5535.5459 - val_mse: 274767616.0000 - val_mae: 5535.5459\n",
      "Epoch 154/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1586.0229 - mse: 23673310.0000 - mae: 1586.0228 - val_loss: 5547.4380 - val_mse: 274692512.0000 - val_mae: 5547.4380\n",
      "Epoch 155/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1577.6479 - mse: 22673096.0000 - mae: 1577.6478 - val_loss: 5546.1260 - val_mse: 274365504.0000 - val_mae: 5546.1260\n",
      "Epoch 156/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1588.8542 - mse: 23231588.0000 - mae: 1588.8545 - val_loss: 5559.2544 - val_mse: 272664384.0000 - val_mae: 5559.2544\n",
      "Epoch 157/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1565.0011 - mse: 22718118.0000 - mae: 1565.0011 - val_loss: 5560.9814 - val_mse: 271720928.0000 - val_mae: 5560.9814\n",
      "Epoch 158/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 1575.7478 - mse: 22999436.0000 - mae: 1575.7478 - val_loss: 5559.3413 - val_mse: 271107808.0000 - val_mae: 5559.3413\n",
      "Epoch 159/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1556.6905 - mse: 22491756.0000 - mae: 1556.6903 - val_loss: 5556.0264 - val_mse: 269630784.0000 - val_mae: 5556.0264\n",
      "Epoch 160/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1579.8254 - mse: 23363540.0000 - mae: 1579.8256 - val_loss: 5548.6826 - val_mse: 269513984.0000 - val_mae: 5548.6826\n",
      "Epoch 161/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1527.6202 - mse: 21329370.0000 - mae: 1527.6201 - val_loss: 5524.2354 - val_mse: 267990400.0000 - val_mae: 5524.2354\n",
      "Epoch 162/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1556.0906 - mse: 22022714.0000 - mae: 1556.0906 - val_loss: 5549.1548 - val_mse: 266869472.0000 - val_mae: 5549.1548\n",
      "Epoch 163/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 1551.1315 - mse: 21953324.0000 - mae: 1551.1315 - val_loss: 5540.8271 - val_mse: 266563888.0000 - val_mae: 5540.8271\n",
      "Epoch 164/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1541.9211 - mse: 21416884.0000 - mae: 1541.9210 - val_loss: 5537.8721 - val_mse: 264972656.0000 - val_mae: 5537.8721\n",
      "Epoch 165/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 1559.0078 - mse: 22187792.0000 - mae: 1559.0078 - val_loss: 5530.5464 - val_mse: 264901904.0000 - val_mae: 5530.5464\n",
      "Epoch 166/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1556.0292 - mse: 21942958.0000 - mae: 1556.0292 - val_loss: 5541.8247 - val_mse: 263654656.0000 - val_mae: 5541.8247\n",
      "Epoch 167/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1584.3664 - mse: 23758506.0000 - mae: 1584.3662 - val_loss: 5524.7480 - val_mse: 263917648.0000 - val_mae: 5524.7480\n",
      "Epoch 168/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1537.8172 - mse: 22437604.0000 - mae: 1537.8173 - val_loss: 5519.5591 - val_mse: 263629520.0000 - val_mae: 5519.5591\n",
      "Epoch 169/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1509.5410 - mse: 20398820.0000 - mae: 1509.5410 - val_loss: 5542.2534 - val_mse: 261558384.0000 - val_mae: 5542.2534\n",
      "Epoch 170/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 1537.0943 - mse: 21327228.0000 - mae: 1537.0942 - val_loss: 5551.6572 - val_mse: 262193392.0000 - val_mae: 5551.6572\n",
      "Epoch 171/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1555.1601 - mse: 22912650.0000 - mae: 1555.1600 - val_loss: 5520.4873 - val_mse: 260906128.0000 - val_mae: 5520.4873\n",
      "Epoch 172/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1514.7533 - mse: 21167374.0000 - mae: 1514.7534 - val_loss: 5507.9380 - val_mse: 259799040.0000 - val_mae: 5507.9380\n",
      "Epoch 173/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1530.4617 - mse: 21507806.0000 - mae: 1530.4619 - val_loss: 5480.5947 - val_mse: 258416800.0000 - val_mae: 5480.5947\n",
      "Epoch 174/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1569.0603 - mse: 22495346.0000 - mae: 1569.0602 - val_loss: 5514.9912 - val_mse: 259522640.0000 - val_mae: 5514.9912\n",
      "Epoch 175/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1506.3824 - mse: 20161422.0000 - mae: 1506.3824 - val_loss: 5510.7222 - val_mse: 259180288.0000 - val_mae: 5510.7222\n",
      "Epoch 176/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1543.5425 - mse: 21242068.0000 - mae: 1543.5425 - val_loss: 5503.3740 - val_mse: 258601200.0000 - val_mae: 5503.3740\n",
      "Epoch 177/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1509.9921 - mse: 20302406.0000 - mae: 1509.9922 - val_loss: 5513.2593 - val_mse: 258812672.0000 - val_mae: 5513.2593\n",
      "Epoch 178/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1506.2823 - mse: 19905258.0000 - mae: 1506.2825 - val_loss: 5490.0073 - val_mse: 258142624.0000 - val_mae: 5490.0073\n",
      "Epoch 179/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1487.3603 - mse: 19316130.0000 - mae: 1487.3604 - val_loss: 5486.2231 - val_mse: 257544496.0000 - val_mae: 5486.2231\n",
      "Epoch 180/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1510.1386 - mse: 20526712.0000 - mae: 1510.1384 - val_loss: 5503.0562 - val_mse: 257543072.0000 - val_mae: 5503.0562\n",
      "Epoch 181/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1520.4971 - mse: 20362040.0000 - mae: 1520.4971 - val_loss: 5522.5737 - val_mse: 255594544.0000 - val_mae: 5522.5737\n",
      "Epoch 182/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1529.8676 - mse: 21433652.0000 - mae: 1529.8674 - val_loss: 5464.8511 - val_mse: 255378272.0000 - val_mae: 5464.8511\n",
      "Epoch 183/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1527.2932 - mse: 21686872.0000 - mae: 1527.2931 - val_loss: 5489.3906 - val_mse: 255141792.0000 - val_mae: 5489.3906\n",
      "Epoch 184/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1498.0565 - mse: 20047512.0000 - mae: 1498.0565 - val_loss: 5506.5986 - val_mse: 255149600.0000 - val_mae: 5506.5986\n",
      "Epoch 185/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1524.4613 - mse: 20940166.0000 - mae: 1524.4612 - val_loss: 5485.3662 - val_mse: 254286048.0000 - val_mae: 5485.3662\n",
      "Epoch 186/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1527.4253 - mse: 20713722.0000 - mae: 1527.4252 - val_loss: 5485.0107 - val_mse: 254052880.0000 - val_mae: 5485.0107\n",
      "Epoch 187/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1514.9819 - mse: 20309932.0000 - mae: 1514.9817 - val_loss: 5466.4409 - val_mse: 253179664.0000 - val_mae: 5466.4409\n",
      "Epoch 188/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1508.7463 - mse: 20562012.0000 - mae: 1508.7462 - val_loss: 5449.5889 - val_mse: 252950688.0000 - val_mae: 5449.5889\n",
      "Epoch 189/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1523.3614 - mse: 20925438.0000 - mae: 1523.3616 - val_loss: 5475.6812 - val_mse: 251923344.0000 - val_mae: 5475.6812\n",
      "Epoch 190/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1498.5291 - mse: 20312914.0000 - mae: 1498.5291 - val_loss: 5494.9658 - val_mse: 252230240.0000 - val_mae: 5494.9658\n",
      "Epoch 191/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1492.6107 - mse: 20220982.0000 - mae: 1492.6108 - val_loss: 5458.8979 - val_mse: 251500304.0000 - val_mae: 5458.8979\n",
      "Epoch 192/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1494.7993 - mse: 19395478.0000 - mae: 1494.7994 - val_loss: 5486.9307 - val_mse: 251912096.0000 - val_mae: 5486.9307\n",
      "Epoch 193/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1507.8230 - mse: 20423730.0000 - mae: 1507.8231 - val_loss: 5490.0225 - val_mse: 251655392.0000 - val_mae: 5490.0225\n",
      "Epoch 194/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1495.5943 - mse: 19846980.0000 - mae: 1495.5945 - val_loss: 5459.5957 - val_mse: 251398400.0000 - val_mae: 5459.5957\n",
      "Epoch 195/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1497.4636 - mse: 20735440.0000 - mae: 1497.4636 - val_loss: 5520.1538 - val_mse: 250584544.0000 - val_mae: 5520.1538\n",
      "Epoch 196/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1506.7189 - mse: 20135652.0000 - mae: 1506.7190 - val_loss: 5475.0044 - val_mse: 249816208.0000 - val_mae: 5475.0044\n",
      "Epoch 197/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1508.6143 - mse: 19861282.0000 - mae: 1508.6143 - val_loss: 5502.5054 - val_mse: 250251168.0000 - val_mae: 5502.5054\n",
      "Epoch 198/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1508.2951 - mse: 20564598.0000 - mae: 1508.2949 - val_loss: 5484.1938 - val_mse: 249383872.0000 - val_mae: 5484.1938\n",
      "Epoch 199/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1450.1831 - mse: 18377806.0000 - mae: 1450.1833 - val_loss: 5454.8652 - val_mse: 248522000.0000 - val_mae: 5454.8652\n",
      "Epoch 200/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1507.0722 - mse: 20552700.0000 - mae: 1507.0724 - val_loss: 5481.2246 - val_mse: 248743680.0000 - val_mae: 5481.2246\n",
      "Epoch 201/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1500.1276 - mse: 19824408.0000 - mae: 1500.1274 - val_loss: 5511.1909 - val_mse: 248459840.0000 - val_mae: 5511.1909\n",
      "Epoch 202/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1500.5166 - mse: 19871698.0000 - mae: 1500.5167 - val_loss: 5485.6519 - val_mse: 247565696.0000 - val_mae: 5485.6519\n",
      "Epoch 203/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1497.4502 - mse: 19825862.0000 - mae: 1497.4501 - val_loss: 5463.3013 - val_mse: 247028352.0000 - val_mae: 5463.3013\n",
      "Epoch 204/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1501.5973 - mse: 20364474.0000 - mae: 1501.5974 - val_loss: 5475.9712 - val_mse: 247355424.0000 - val_mae: 5475.9712\n",
      "Epoch 205/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1505.9057 - mse: 19776624.0000 - mae: 1505.9058 - val_loss: 5461.2788 - val_mse: 247570064.0000 - val_mae: 5461.2788\n",
      "Epoch 206/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1507.1642 - mse: 19768798.0000 - mae: 1507.1642 - val_loss: 5462.4023 - val_mse: 247140528.0000 - val_mae: 5462.4023\n",
      "Epoch 207/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1494.2845 - mse: 20267000.0000 - mae: 1494.2845 - val_loss: 5468.9238 - val_mse: 246607616.0000 - val_mae: 5468.9238\n",
      "Epoch 208/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1448.5304 - mse: 18110710.0000 - mae: 1448.5304 - val_loss: 5413.4224 - val_mse: 245555648.0000 - val_mae: 5413.4224\n",
      "Epoch 209/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1505.7539 - mse: 20307930.0000 - mae: 1505.7539 - val_loss: 5450.4468 - val_mse: 244900336.0000 - val_mae: 5450.4468\n",
      "Epoch 210/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1492.0767 - mse: 19324848.0000 - mae: 1492.0767 - val_loss: 5405.5459 - val_mse: 244907408.0000 - val_mae: 5405.5459\n",
      "Epoch 211/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1476.0472 - mse: 19636318.0000 - mae: 1476.0471 - val_loss: 5446.0190 - val_mse: 244278464.0000 - val_mae: 5446.0190\n",
      "Epoch 212/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1481.5609 - mse: 19895904.0000 - mae: 1481.5609 - val_loss: 5460.7192 - val_mse: 243713808.0000 - val_mae: 5460.7192\n",
      "Epoch 213/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1461.4388 - mse: 18688472.0000 - mae: 1461.4388 - val_loss: 5498.9453 - val_mse: 244134896.0000 - val_mae: 5498.9453\n",
      "Epoch 214/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1501.3012 - mse: 20382024.0000 - mae: 1501.3013 - val_loss: 5470.2544 - val_mse: 244081632.0000 - val_mae: 5470.2544\n",
      "Epoch 215/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1479.4522 - mse: 19217062.0000 - mae: 1479.4524 - val_loss: 5469.1299 - val_mse: 243495536.0000 - val_mae: 5469.1299\n",
      "Epoch 216/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1495.6807 - mse: 19957120.0000 - mae: 1495.6807 - val_loss: 5458.9746 - val_mse: 243106928.0000 - val_mae: 5458.9746\n",
      "Epoch 217/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1488.2012 - mse: 19817640.0000 - mae: 1488.2012 - val_loss: 5459.4468 - val_mse: 243069584.0000 - val_mae: 5459.4468\n",
      "Epoch 218/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1479.3495 - mse: 19335406.0000 - mae: 1479.3494 - val_loss: 5429.7104 - val_mse: 242789120.0000 - val_mae: 5429.7104\n",
      "Epoch 219/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1503.9870 - mse: 20931888.0000 - mae: 1503.9872 - val_loss: 5403.5142 - val_mse: 242769152.0000 - val_mae: 5403.5142\n",
      "Epoch 220/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1467.3708 - mse: 19051194.0000 - mae: 1467.3707 - val_loss: 5411.6323 - val_mse: 241995776.0000 - val_mae: 5411.6323\n",
      "Epoch 221/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1465.5849 - mse: 19088044.0000 - mae: 1465.5847 - val_loss: 5430.6074 - val_mse: 240909408.0000 - val_mae: 5430.6074\n",
      "Epoch 222/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1459.8403 - mse: 18286600.0000 - mae: 1459.8403 - val_loss: 5418.6108 - val_mse: 240809280.0000 - val_mae: 5418.6108\n",
      "Epoch 223/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1473.7687 - mse: 19094124.0000 - mae: 1473.7687 - val_loss: 5387.6567 - val_mse: 240820464.0000 - val_mae: 5387.6567\n",
      "Epoch 224/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1465.4024 - mse: 18670044.0000 - mae: 1465.4025 - val_loss: 5416.5786 - val_mse: 241635264.0000 - val_mae: 5416.5786\n",
      "Epoch 225/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1462.2753 - mse: 19516498.0000 - mae: 1462.2751 - val_loss: 5387.0010 - val_mse: 240799072.0000 - val_mae: 5387.0010\n",
      "Epoch 226/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1460.8912 - mse: 18874316.0000 - mae: 1460.8911 - val_loss: 5397.1230 - val_mse: 240538688.0000 - val_mae: 5397.1230\n",
      "Epoch 227/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1470.6231 - mse: 19434910.0000 - mae: 1470.6233 - val_loss: 5397.3198 - val_mse: 239714016.0000 - val_mae: 5397.3198\n",
      "Epoch 228/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1469.6508 - mse: 19346888.0000 - mae: 1469.6510 - val_loss: 5395.6084 - val_mse: 240555920.0000 - val_mae: 5395.6084\n",
      "Epoch 229/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1453.5243 - mse: 18406646.0000 - mae: 1453.5242 - val_loss: 5418.0586 - val_mse: 239439376.0000 - val_mae: 5418.0586\n",
      "Epoch 230/500\n",
      "11500/11500 [==============================] - 0s 39us/step - loss: 1472.6331 - mse: 19142894.0000 - mae: 1472.6332 - val_loss: 5404.5977 - val_mse: 239852688.0000 - val_mae: 5404.5977\n",
      "Epoch 231/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1457.7452 - mse: 18190374.0000 - mae: 1457.7450 - val_loss: 5400.3809 - val_mse: 238947696.0000 - val_mae: 5400.3809\n",
      "Epoch 232/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1472.1186 - mse: 19140820.0000 - mae: 1472.1187 - val_loss: 5407.2373 - val_mse: 240226928.0000 - val_mae: 5407.2373\n",
      "Epoch 233/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1485.3926 - mse: 19359932.0000 - mae: 1485.3923 - val_loss: 5432.4219 - val_mse: 239993104.0000 - val_mae: 5432.4219\n",
      "Epoch 234/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1464.9701 - mse: 18237420.0000 - mae: 1464.9701 - val_loss: 5414.9429 - val_mse: 240087136.0000 - val_mae: 5414.9429\n",
      "Epoch 235/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1465.3213 - mse: 18454560.0000 - mae: 1465.3212 - val_loss: 5456.3164 - val_mse: 238926528.0000 - val_mae: 5456.3164\n",
      "Epoch 236/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1469.7702 - mse: 19006168.0000 - mae: 1469.7703 - val_loss: 5394.8789 - val_mse: 240086560.0000 - val_mae: 5394.8789\n",
      "Epoch 237/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1485.3515 - mse: 19268944.0000 - mae: 1485.3514 - val_loss: 5380.0034 - val_mse: 239925424.0000 - val_mae: 5380.0034\n",
      "Epoch 238/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1471.6121 - mse: 19255264.0000 - mae: 1471.6121 - val_loss: 5399.0088 - val_mse: 239461248.0000 - val_mae: 5399.0088\n",
      "Epoch 239/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1461.0853 - mse: 18826200.0000 - mae: 1461.0852 - val_loss: 5423.5610 - val_mse: 238157232.0000 - val_mae: 5423.5610\n",
      "Epoch 240/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1467.3409 - mse: 18960454.0000 - mae: 1467.3408 - val_loss: 5377.8716 - val_mse: 238114112.0000 - val_mae: 5377.8716\n",
      "Epoch 241/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1475.2216 - mse: 19674272.0000 - mae: 1475.2216 - val_loss: 5381.1748 - val_mse: 238314592.0000 - val_mae: 5381.1748\n",
      "Epoch 242/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1459.7324 - mse: 18507436.0000 - mae: 1459.7323 - val_loss: 5398.7803 - val_mse: 238541248.0000 - val_mae: 5398.7803\n",
      "Epoch 243/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1473.7753 - mse: 18894932.0000 - mae: 1473.7753 - val_loss: 5400.7329 - val_mse: 237973216.0000 - val_mae: 5400.7329\n",
      "Epoch 244/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1471.7624 - mse: 18850170.0000 - mae: 1471.7625 - val_loss: 5392.2925 - val_mse: 236933056.0000 - val_mae: 5392.2925\n",
      "Epoch 245/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1446.7359 - mse: 18258374.0000 - mae: 1446.7358 - val_loss: 5413.3179 - val_mse: 236305664.0000 - val_mae: 5413.3179\n",
      "Epoch 246/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1441.8003 - mse: 17835108.0000 - mae: 1441.8004 - val_loss: 5375.2793 - val_mse: 236660112.0000 - val_mae: 5375.2793\n",
      "Epoch 247/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1438.9922 - mse: 18174368.0000 - mae: 1438.9921 - val_loss: 5421.6064 - val_mse: 235707792.0000 - val_mae: 5421.6064\n",
      "Epoch 248/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1486.4314 - mse: 20366360.0000 - mae: 1486.4312 - val_loss: 5406.7192 - val_mse: 236587856.0000 - val_mae: 5406.7192\n",
      "Epoch 249/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1446.5913 - mse: 18387046.0000 - mae: 1446.5912 - val_loss: 5405.3096 - val_mse: 236785648.0000 - val_mae: 5405.3096\n",
      "Epoch 250/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1466.3679 - mse: 18913034.0000 - mae: 1466.3678 - val_loss: 5428.8745 - val_mse: 237692080.0000 - val_mae: 5428.8745\n",
      "Epoch 251/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1458.2676 - mse: 17994544.0000 - mae: 1458.2677 - val_loss: 5375.1836 - val_mse: 236586432.0000 - val_mae: 5375.1836\n",
      "Epoch 252/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1454.6568 - mse: 18529640.0000 - mae: 1454.6567 - val_loss: 5387.9663 - val_mse: 235854208.0000 - val_mae: 5387.9663\n",
      "Epoch 253/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1440.0603 - mse: 17525984.0000 - mae: 1440.0603 - val_loss: 5389.2183 - val_mse: 235534624.0000 - val_mae: 5389.2183\n",
      "Epoch 254/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1427.5094 - mse: 17494366.0000 - mae: 1427.5093 - val_loss: 5407.8149 - val_mse: 235418656.0000 - val_mae: 5407.8149\n",
      "Epoch 255/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1465.9015 - mse: 18811004.0000 - mae: 1465.9016 - val_loss: 5385.5645 - val_mse: 235544768.0000 - val_mae: 5385.5645\n",
      "Epoch 256/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1458.4654 - mse: 19034736.0000 - mae: 1458.4655 - val_loss: 5373.1001 - val_mse: 236443392.0000 - val_mae: 5373.1001\n",
      "Epoch 257/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1458.8794 - mse: 18514912.0000 - mae: 1458.8795 - val_loss: 5351.1553 - val_mse: 236806128.0000 - val_mae: 5351.1553\n",
      "Epoch 258/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1447.0476 - mse: 17903296.0000 - mae: 1447.0476 - val_loss: 5395.6748 - val_mse: 235918784.0000 - val_mae: 5395.6748\n",
      "Epoch 259/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1455.0570 - mse: 18103520.0000 - mae: 1455.0570 - val_loss: 5380.3809 - val_mse: 236083648.0000 - val_mae: 5380.3809\n",
      "Epoch 260/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1472.5603 - mse: 18955824.0000 - mae: 1472.5603 - val_loss: 5357.2197 - val_mse: 235715008.0000 - val_mae: 5357.2197\n",
      "Epoch 261/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1455.2309 - mse: 18717942.0000 - mae: 1455.2308 - val_loss: 5379.4097 - val_mse: 234401472.0000 - val_mae: 5379.4097\n",
      "Epoch 262/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1434.2564 - mse: 18454690.0000 - mae: 1434.2565 - val_loss: 5387.3765 - val_mse: 233568768.0000 - val_mae: 5387.3765\n",
      "Epoch 263/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1432.3156 - mse: 17401704.0000 - mae: 1432.3158 - val_loss: 5366.8379 - val_mse: 233655472.0000 - val_mae: 5366.8379\n",
      "Epoch 264/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1435.7118 - mse: 18152698.0000 - mae: 1435.7118 - val_loss: 5377.2041 - val_mse: 232713248.0000 - val_mae: 5377.2041\n",
      "Epoch 265/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1447.6825 - mse: 18179320.0000 - mae: 1447.6825 - val_loss: 5349.2959 - val_mse: 234204800.0000 - val_mae: 5349.2959\n",
      "Epoch 266/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1420.2208 - mse: 16943190.0000 - mae: 1420.2208 - val_loss: 5352.4121 - val_mse: 233842864.0000 - val_mae: 5352.4121\n",
      "Epoch 267/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1447.0152 - mse: 17858480.0000 - mae: 1447.0153 - val_loss: 5363.6406 - val_mse: 233939296.0000 - val_mae: 5363.6406\n",
      "Epoch 268/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1446.6709 - mse: 18933522.0000 - mae: 1446.6709 - val_loss: 5352.6909 - val_mse: 233318096.0000 - val_mae: 5352.6909\n",
      "Epoch 269/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1439.6609 - mse: 18359428.0000 - mae: 1439.6610 - val_loss: 5342.0347 - val_mse: 234376272.0000 - val_mae: 5342.0347\n",
      "Epoch 270/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1458.6254 - mse: 19252418.0000 - mae: 1458.6252 - val_loss: 5367.4854 - val_mse: 234323680.0000 - val_mae: 5367.4854\n",
      "Epoch 271/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1423.5531 - mse: 17185372.0000 - mae: 1423.5531 - val_loss: 5363.6528 - val_mse: 233477568.0000 - val_mae: 5363.6528\n",
      "Epoch 272/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1437.6874 - mse: 17694088.0000 - mae: 1437.6875 - val_loss: 5374.5293 - val_mse: 233310000.0000 - val_mae: 5374.5293\n",
      "Epoch 273/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1428.2473 - mse: 17671046.0000 - mae: 1428.2474 - val_loss: 5390.4771 - val_mse: 233501808.0000 - val_mae: 5390.4771\n",
      "Epoch 274/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1472.4883 - mse: 19677564.0000 - mae: 1472.4884 - val_loss: 5345.0317 - val_mse: 234265056.0000 - val_mae: 5345.0317\n",
      "Epoch 275/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1443.4915 - mse: 18967574.0000 - mae: 1443.4917 - val_loss: 5324.7168 - val_mse: 234516928.0000 - val_mae: 5324.7168\n",
      "Epoch 276/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1444.2052 - mse: 18667498.0000 - mae: 1444.2052 - val_loss: 5384.6572 - val_mse: 234109248.0000 - val_mae: 5384.6572\n",
      "Epoch 277/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1431.3736 - mse: 17745470.0000 - mae: 1431.3738 - val_loss: 5341.3613 - val_mse: 233775584.0000 - val_mae: 5341.3613\n",
      "Epoch 278/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1446.9202 - mse: 18105536.0000 - mae: 1446.9202 - val_loss: 5341.5356 - val_mse: 234544880.0000 - val_mae: 5341.5356\n",
      "Epoch 279/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1459.5319 - mse: 18638920.0000 - mae: 1459.5319 - val_loss: 5383.2080 - val_mse: 234390496.0000 - val_mae: 5383.2080\n",
      "Epoch 280/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1415.8408 - mse: 17534600.0000 - mae: 1415.8409 - val_loss: 5370.9497 - val_mse: 234165664.0000 - val_mae: 5370.9497\n",
      "Epoch 281/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1448.2221 - mse: 18115152.0000 - mae: 1448.2222 - val_loss: 5384.0347 - val_mse: 233211952.0000 - val_mae: 5384.0347\n",
      "Epoch 282/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1457.2677 - mse: 18279080.0000 - mae: 1457.2677 - val_loss: 5353.6270 - val_mse: 233098016.0000 - val_mae: 5353.6270\n",
      "Epoch 283/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1463.0301 - mse: 18961414.0000 - mae: 1463.0300 - val_loss: 5379.6426 - val_mse: 232560704.0000 - val_mae: 5379.6426\n",
      "Epoch 284/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1465.6666 - mse: 19476814.0000 - mae: 1465.6666 - val_loss: 5352.3447 - val_mse: 232553120.0000 - val_mae: 5352.3447\n",
      "Epoch 285/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1422.6859 - mse: 17228040.0000 - mae: 1422.6859 - val_loss: 5368.7212 - val_mse: 233463584.0000 - val_mae: 5368.7212\n",
      "Epoch 286/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1429.3582 - mse: 17580064.0000 - mae: 1429.3583 - val_loss: 5346.6865 - val_mse: 232158048.0000 - val_mae: 5346.6865\n",
      "Epoch 287/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1435.4275 - mse: 18523386.0000 - mae: 1435.4276 - val_loss: 5377.7129 - val_mse: 232256272.0000 - val_mae: 5377.7129\n",
      "Epoch 288/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1435.3167 - mse: 18194250.0000 - mae: 1435.3169 - val_loss: 5360.8213 - val_mse: 231965664.0000 - val_mae: 5360.8213\n",
      "Epoch 289/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1438.0016 - mse: 18932266.0000 - mae: 1438.0016 - val_loss: 5338.1553 - val_mse: 232145952.0000 - val_mae: 5338.1553\n",
      "Epoch 290/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1478.4702 - mse: 19961824.0000 - mae: 1478.4702 - val_loss: 5375.6699 - val_mse: 231825312.0000 - val_mae: 5375.6699\n",
      "Epoch 291/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1422.1474 - mse: 17818954.0000 - mae: 1422.1475 - val_loss: 5360.3262 - val_mse: 231440368.0000 - val_mae: 5360.3262\n",
      "Epoch 292/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1416.0805 - mse: 17736696.0000 - mae: 1416.0804 - val_loss: 5372.8799 - val_mse: 231531824.0000 - val_mae: 5372.8799\n",
      "Epoch 293/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1451.9220 - mse: 18570492.0000 - mae: 1451.9219 - val_loss: 5340.7988 - val_mse: 232064320.0000 - val_mae: 5340.7988\n",
      "Epoch 294/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1448.4214 - mse: 18502858.0000 - mae: 1448.4214 - val_loss: 5372.4126 - val_mse: 232240800.0000 - val_mae: 5372.4126\n",
      "Epoch 295/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1429.4931 - mse: 19138868.0000 - mae: 1429.4932 - val_loss: 5362.5381 - val_mse: 232010880.0000 - val_mae: 5362.5381\n",
      "Epoch 296/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1419.5374 - mse: 17971840.0000 - mae: 1419.5376 - val_loss: 5374.2241 - val_mse: 231731280.0000 - val_mae: 5374.2241\n",
      "Epoch 297/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1455.0320 - mse: 19423332.0000 - mae: 1455.0320 - val_loss: 5373.7993 - val_mse: 231205696.0000 - val_mae: 5373.7993\n",
      "Epoch 298/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1428.9003 - mse: 18045844.0000 - mae: 1428.9004 - val_loss: 5364.6841 - val_mse: 231458992.0000 - val_mae: 5364.6841\n",
      "Epoch 299/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1455.9114 - mse: 18272204.0000 - mae: 1455.9115 - val_loss: 5316.9780 - val_mse: 230370096.0000 - val_mae: 5316.9780\n",
      "Epoch 300/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1432.0416 - mse: 17359990.0000 - mae: 1432.0417 - val_loss: 5368.2866 - val_mse: 230955936.0000 - val_mae: 5368.2866\n",
      "Epoch 301/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1428.9654 - mse: 17527750.0000 - mae: 1428.9653 - val_loss: 5376.1499 - val_mse: 231452576.0000 - val_mae: 5376.1499\n",
      "Epoch 302/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1412.5957 - mse: 16934828.0000 - mae: 1412.5958 - val_loss: 5364.9624 - val_mse: 232387712.0000 - val_mae: 5364.9624\n",
      "Epoch 303/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1436.4596 - mse: 18724026.0000 - mae: 1436.4595 - val_loss: 5359.1582 - val_mse: 231320784.0000 - val_mae: 5359.1582\n",
      "Epoch 304/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1434.5380 - mse: 18179322.0000 - mae: 1434.5380 - val_loss: 5384.6265 - val_mse: 231747680.0000 - val_mae: 5384.6265\n",
      "Epoch 305/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1443.6594 - mse: 18103456.0000 - mae: 1443.6594 - val_loss: 5339.3794 - val_mse: 231066256.0000 - val_mae: 5339.3794\n",
      "Epoch 306/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1448.4498 - mse: 18998078.0000 - mae: 1448.4497 - val_loss: 5369.3555 - val_mse: 230988672.0000 - val_mae: 5369.3555\n",
      "Epoch 307/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1432.7098 - mse: 18079636.0000 - mae: 1432.7100 - val_loss: 5345.4258 - val_mse: 230419888.0000 - val_mae: 5345.4258\n",
      "Epoch 308/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1407.2936 - mse: 17328364.0000 - mae: 1407.2936 - val_loss: 5350.1523 - val_mse: 230403152.0000 - val_mae: 5350.1523\n",
      "Epoch 309/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1451.7193 - mse: 18927772.0000 - mae: 1451.7192 - val_loss: 5331.3525 - val_mse: 230455648.0000 - val_mae: 5331.3525\n",
      "Epoch 310/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1427.7793 - mse: 17546900.0000 - mae: 1427.7794 - val_loss: 5297.0693 - val_mse: 230245856.0000 - val_mae: 5297.0693\n",
      "Epoch 311/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1443.0464 - mse: 18379270.0000 - mae: 1443.0465 - val_loss: 5352.8086 - val_mse: 229508112.0000 - val_mae: 5352.8086\n",
      "Epoch 312/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1442.2613 - mse: 17536526.0000 - mae: 1442.2614 - val_loss: 5356.5078 - val_mse: 229984000.0000 - val_mae: 5356.5078\n",
      "Epoch 313/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1429.5501 - mse: 17427012.0000 - mae: 1429.5503 - val_loss: 5345.5195 - val_mse: 229122352.0000 - val_mae: 5345.5195\n",
      "Epoch 314/500\n",
      "11500/11500 [==============================] - 0s 40us/step - loss: 1420.3502 - mse: 17875362.0000 - mae: 1420.3502 - val_loss: 5316.4976 - val_mse: 229003040.0000 - val_mae: 5316.4976\n",
      "Epoch 315/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1449.0701 - mse: 18936410.0000 - mae: 1449.0699 - val_loss: 5331.2856 - val_mse: 228688240.0000 - val_mae: 5331.2856\n",
      "Epoch 316/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1422.0694 - mse: 17835552.0000 - mae: 1422.0693 - val_loss: 5326.9790 - val_mse: 229565312.0000 - val_mae: 5326.9790\n",
      "Epoch 317/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1436.5819 - mse: 18118134.0000 - mae: 1436.5819 - val_loss: 5318.8667 - val_mse: 229496848.0000 - val_mae: 5318.8667\n",
      "Epoch 318/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1450.0086 - mse: 18779650.0000 - mae: 1450.0085 - val_loss: 5319.0234 - val_mse: 229988944.0000 - val_mae: 5319.0234\n",
      "Epoch 319/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1425.3551 - mse: 18259644.0000 - mae: 1425.3551 - val_loss: 5321.5059 - val_mse: 228702624.0000 - val_mae: 5321.5059\n",
      "Epoch 320/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1424.1764 - mse: 17789666.0000 - mae: 1424.1765 - val_loss: 5337.1411 - val_mse: 228358592.0000 - val_mae: 5337.1411\n",
      "Epoch 321/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1413.9678 - mse: 17236116.0000 - mae: 1413.9678 - val_loss: 5336.2827 - val_mse: 229025312.0000 - val_mae: 5336.2827\n",
      "Epoch 322/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1445.7075 - mse: 18372248.0000 - mae: 1445.7074 - val_loss: 5338.2266 - val_mse: 228225664.0000 - val_mae: 5338.2266\n",
      "Epoch 323/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1446.9487 - mse: 18712292.0000 - mae: 1446.9487 - val_loss: 5369.5986 - val_mse: 228973072.0000 - val_mae: 5369.5986\n",
      "Epoch 324/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1423.3962 - mse: 17455762.0000 - mae: 1423.3962 - val_loss: 5370.5649 - val_mse: 228127600.0000 - val_mae: 5370.5649\n",
      "Epoch 325/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1423.9065 - mse: 17164080.0000 - mae: 1423.9065 - val_loss: 5313.8145 - val_mse: 227186544.0000 - val_mae: 5313.8145\n",
      "Epoch 326/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1430.4395 - mse: 17622138.0000 - mae: 1430.4396 - val_loss: 5301.8745 - val_mse: 228089328.0000 - val_mae: 5301.8745\n",
      "Epoch 327/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1451.6770 - mse: 18874486.0000 - mae: 1451.6770 - val_loss: 5319.2510 - val_mse: 228006752.0000 - val_mae: 5319.2510\n",
      "Epoch 328/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1419.1697 - mse: 17363950.0000 - mae: 1419.1698 - val_loss: 5299.4761 - val_mse: 229417392.0000 - val_mae: 5299.4761\n",
      "Epoch 329/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1407.8980 - mse: 17552408.0000 - mae: 1407.8979 - val_loss: 5283.1787 - val_mse: 230271360.0000 - val_mae: 5283.1787\n",
      "Epoch 330/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1432.1892 - mse: 17918090.0000 - mae: 1432.1892 - val_loss: 5266.9600 - val_mse: 228721888.0000 - val_mae: 5266.9600\n",
      "Epoch 331/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1437.6827 - mse: 17289432.0000 - mae: 1437.6826 - val_loss: 5307.5708 - val_mse: 228029632.0000 - val_mae: 5307.5708\n",
      "Epoch 332/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1394.5365 - mse: 16622325.0000 - mae: 1394.5365 - val_loss: 5290.7510 - val_mse: 228089440.0000 - val_mae: 5290.7510\n",
      "Epoch 333/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1421.2577 - mse: 17724588.0000 - mae: 1421.2577 - val_loss: 5275.7310 - val_mse: 229091536.0000 - val_mae: 5275.7310\n",
      "Epoch 334/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1417.3552 - mse: 17532746.0000 - mae: 1417.3551 - val_loss: 5295.3779 - val_mse: 228735424.0000 - val_mae: 5295.3779\n",
      "Epoch 335/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1431.1486 - mse: 18042940.0000 - mae: 1431.1487 - val_loss: 5310.9893 - val_mse: 228421312.0000 - val_mae: 5310.9893\n",
      "Epoch 336/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1414.0873 - mse: 17309186.0000 - mae: 1414.0872 - val_loss: 5318.1416 - val_mse: 227595280.0000 - val_mae: 5318.1416\n",
      "Epoch 337/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1428.7276 - mse: 17902908.0000 - mae: 1428.7275 - val_loss: 5336.6606 - val_mse: 227357040.0000 - val_mae: 5336.6606\n",
      "Epoch 338/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1424.8323 - mse: 17132302.0000 - mae: 1424.8323 - val_loss: 5301.1479 - val_mse: 227891744.0000 - val_mae: 5301.1479\n",
      "Epoch 339/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1419.2111 - mse: 17767476.0000 - mae: 1419.2111 - val_loss: 5285.9717 - val_mse: 226678640.0000 - val_mae: 5285.9717\n",
      "Epoch 340/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1416.6234 - mse: 17507570.0000 - mae: 1416.6234 - val_loss: 5253.2319 - val_mse: 227325952.0000 - val_mae: 5253.2319\n",
      "Epoch 341/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1430.7216 - mse: 17718898.0000 - mae: 1430.7217 - val_loss: 5254.6387 - val_mse: 226228672.0000 - val_mae: 5254.6387\n",
      "Epoch 342/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1419.2625 - mse: 17005784.0000 - mae: 1419.2625 - val_loss: 5250.7480 - val_mse: 225888672.0000 - val_mae: 5250.7480\n",
      "Epoch 343/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1439.0966 - mse: 17948838.0000 - mae: 1439.0967 - val_loss: 5270.8018 - val_mse: 225468816.0000 - val_mae: 5270.8018\n",
      "Epoch 344/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1420.0178 - mse: 17289192.0000 - mae: 1420.0178 - val_loss: 5304.8960 - val_mse: 226500960.0000 - val_mae: 5304.8960\n",
      "Epoch 345/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1403.2296 - mse: 16954508.0000 - mae: 1403.2296 - val_loss: 5315.0527 - val_mse: 227057408.0000 - val_mae: 5315.0527\n",
      "Epoch 346/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1400.3335 - mse: 16928380.0000 - mae: 1400.3335 - val_loss: 5317.5962 - val_mse: 227519296.0000 - val_mae: 5317.5962\n",
      "Epoch 347/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1403.8129 - mse: 17129376.0000 - mae: 1403.8127 - val_loss: 5261.4800 - val_mse: 227421920.0000 - val_mae: 5261.4800\n",
      "Epoch 348/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1435.3611 - mse: 17822050.0000 - mae: 1435.3608 - val_loss: 5299.5693 - val_mse: 227050160.0000 - val_mae: 5299.5693\n",
      "Epoch 349/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1402.5995 - mse: 16579455.0000 - mae: 1402.5994 - val_loss: 5287.7612 - val_mse: 226365680.0000 - val_mae: 5287.7612\n",
      "Epoch 350/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1426.1980 - mse: 17924368.0000 - mae: 1426.1981 - val_loss: 5259.5293 - val_mse: 226871088.0000 - val_mae: 5259.5293\n",
      "Epoch 351/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1441.7105 - mse: 18562656.0000 - mae: 1441.7106 - val_loss: 5258.7900 - val_mse: 226611392.0000 - val_mae: 5258.7900\n",
      "Epoch 352/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1409.1513 - mse: 17930478.0000 - mae: 1409.1514 - val_loss: 5274.5693 - val_mse: 226626112.0000 - val_mae: 5274.5693\n",
      "Epoch 353/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1424.5999 - mse: 18169510.0000 - mae: 1424.5997 - val_loss: 5296.6050 - val_mse: 227638832.0000 - val_mae: 5296.6050\n",
      "Epoch 354/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1432.1194 - mse: 18187564.0000 - mae: 1432.1193 - val_loss: 5274.7959 - val_mse: 227591472.0000 - val_mae: 5274.7959\n",
      "Epoch 355/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1402.1685 - mse: 17049496.0000 - mae: 1402.1686 - val_loss: 5303.6958 - val_mse: 226876144.0000 - val_mae: 5303.6958\n",
      "Epoch 356/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1408.2435 - mse: 17198196.0000 - mae: 1408.2435 - val_loss: 5300.6509 - val_mse: 226512336.0000 - val_mae: 5300.6509\n",
      "Epoch 357/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1433.4137 - mse: 17572348.0000 - mae: 1433.4137 - val_loss: 5284.1279 - val_mse: 225759680.0000 - val_mae: 5284.1279\n",
      "Epoch 358/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1421.4207 - mse: 17953024.0000 - mae: 1421.4207 - val_loss: 5316.9375 - val_mse: 226002544.0000 - val_mae: 5316.9375\n",
      "Epoch 359/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1407.4835 - mse: 17157170.0000 - mae: 1407.4835 - val_loss: 5307.2373 - val_mse: 228030640.0000 - val_mae: 5307.2373\n",
      "Epoch 360/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1433.1512 - mse: 18362390.0000 - mae: 1433.1511 - val_loss: 5330.4258 - val_mse: 226623728.0000 - val_mae: 5330.4258\n",
      "Epoch 361/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1404.0967 - mse: 17086862.0000 - mae: 1404.0967 - val_loss: 5307.7583 - val_mse: 227038320.0000 - val_mae: 5307.7583\n",
      "Epoch 362/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1397.4893 - mse: 16928702.0000 - mae: 1397.4893 - val_loss: 5280.4492 - val_mse: 226746896.0000 - val_mae: 5280.4492\n",
      "Epoch 363/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1423.2409 - mse: 17632864.0000 - mae: 1423.2407 - val_loss: 5296.0786 - val_mse: 225549760.0000 - val_mae: 5296.0786\n",
      "Epoch 364/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1428.7526 - mse: 18168660.0000 - mae: 1428.7524 - val_loss: 5298.7549 - val_mse: 225948352.0000 - val_mae: 5298.7549\n",
      "Epoch 365/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1427.2345 - mse: 18105668.0000 - mae: 1427.2345 - val_loss: 5273.8154 - val_mse: 226997872.0000 - val_mae: 5273.8154\n",
      "Epoch 366/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1430.4650 - mse: 18526492.0000 - mae: 1430.4652 - val_loss: 5276.3330 - val_mse: 226809792.0000 - val_mae: 5276.3330\n",
      "Epoch 367/500\n",
      "11500/11500 [==============================] - 0s 41us/step - loss: 1418.2923 - mse: 17576016.0000 - mae: 1418.2924 - val_loss: 5285.2852 - val_mse: 226355600.0000 - val_mae: 5285.2852\n",
      "Epoch 368/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1389.3084 - mse: 17106930.0000 - mae: 1389.3083 - val_loss: 5305.5972 - val_mse: 226512784.0000 - val_mae: 5305.5972\n",
      "Epoch 369/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1404.3525 - mse: 16752780.0000 - mae: 1404.3524 - val_loss: 5319.2104 - val_mse: 227302096.0000 - val_mae: 5319.2104\n",
      "Epoch 370/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1411.3399 - mse: 17526188.0000 - mae: 1411.3400 - val_loss: 5284.6787 - val_mse: 226064560.0000 - val_mae: 5284.6787\n",
      "Epoch 371/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1394.8347 - mse: 16515954.0000 - mae: 1394.8347 - val_loss: 5292.6729 - val_mse: 226410960.0000 - val_mae: 5292.6729\n",
      "Epoch 372/500\n",
      "11500/11500 [==============================] - 1s 43us/step - loss: 1441.3518 - mse: 18586744.0000 - mae: 1441.3518 - val_loss: 5270.0811 - val_mse: 225556992.0000 - val_mae: 5270.0811\n",
      "Epoch 373/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1408.0007 - mse: 17283300.0000 - mae: 1408.0007 - val_loss: 5274.7627 - val_mse: 225519568.0000 - val_mae: 5274.7627\n",
      "Epoch 374/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1409.4054 - mse: 17044130.0000 - mae: 1409.4054 - val_loss: 5261.3755 - val_mse: 225263296.0000 - val_mae: 5261.3755\n",
      "Epoch 375/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1422.7954 - mse: 17613254.0000 - mae: 1422.7955 - val_loss: 5280.0552 - val_mse: 224827968.0000 - val_mae: 5280.0552\n",
      "Epoch 376/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1419.1476 - mse: 17972650.0000 - mae: 1419.1477 - val_loss: 5265.3452 - val_mse: 224884224.0000 - val_mae: 5265.3452\n",
      "Epoch 377/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1420.3477 - mse: 17806330.0000 - mae: 1420.3477 - val_loss: 5312.6782 - val_mse: 225592704.0000 - val_mae: 5312.6782\n",
      "Epoch 378/500\n",
      "11500/11500 [==============================] - 1s 48us/step - loss: 1439.4984 - mse: 18578180.0000 - mae: 1439.4984 - val_loss: 5253.9976 - val_mse: 225553616.0000 - val_mae: 5253.9976\n",
      "Epoch 379/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1394.1704 - mse: 16777166.0000 - mae: 1394.1704 - val_loss: 5309.6621 - val_mse: 225265488.0000 - val_mae: 5309.6621\n",
      "Epoch 380/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1395.6572 - mse: 16918858.0000 - mae: 1395.6572 - val_loss: 5280.6919 - val_mse: 224389504.0000 - val_mae: 5280.6919\n",
      "Epoch 381/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1425.2367 - mse: 17109124.0000 - mae: 1425.2366 - val_loss: 5299.1948 - val_mse: 224999552.0000 - val_mae: 5299.1948\n",
      "Epoch 382/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1396.6240 - mse: 17258986.0000 - mae: 1396.6241 - val_loss: 5294.8037 - val_mse: 224256096.0000 - val_mae: 5294.8037\n",
      "Epoch 383/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1402.0211 - mse: 16654436.0000 - mae: 1402.0212 - val_loss: 5281.4468 - val_mse: 225204016.0000 - val_mae: 5281.4468\n",
      "Epoch 384/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1403.7430 - mse: 16538655.0000 - mae: 1403.7430 - val_loss: 5298.8765 - val_mse: 223685232.0000 - val_mae: 5298.8765\n",
      "Epoch 385/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1425.8353 - mse: 17773166.0000 - mae: 1425.8354 - val_loss: 5309.7451 - val_mse: 224633296.0000 - val_mae: 5309.7451\n",
      "Epoch 386/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1383.5224 - mse: 15963544.0000 - mae: 1383.5222 - val_loss: 5295.5059 - val_mse: 224374208.0000 - val_mae: 5295.5059\n",
      "Epoch 387/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1394.4396 - mse: 17154974.0000 - mae: 1394.4397 - val_loss: 5295.5371 - val_mse: 224344640.0000 - val_mae: 5295.5371\n",
      "Epoch 388/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1393.6634 - mse: 16723810.0000 - mae: 1393.6633 - val_loss: 5266.4976 - val_mse: 224160096.0000 - val_mae: 5266.4976\n",
      "Epoch 389/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1401.5475 - mse: 16806930.0000 - mae: 1401.5476 - val_loss: 5257.2378 - val_mse: 224080160.0000 - val_mae: 5257.2378\n",
      "Epoch 390/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1417.5512 - mse: 17696406.0000 - mae: 1417.5511 - val_loss: 5267.8149 - val_mse: 224016928.0000 - val_mae: 5267.8149\n",
      "Epoch 391/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1381.9333 - mse: 16624409.0000 - mae: 1381.9332 - val_loss: 5222.5474 - val_mse: 224096416.0000 - val_mae: 5222.5474\n",
      "Epoch 392/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1414.4791 - mse: 17229552.0000 - mae: 1414.4790 - val_loss: 5244.6592 - val_mse: 223674656.0000 - val_mae: 5244.6592\n",
      "Epoch 393/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1417.0435 - mse: 17570484.0000 - mae: 1417.0437 - val_loss: 5257.4761 - val_mse: 222346208.0000 - val_mae: 5257.4761\n",
      "Epoch 394/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1393.0702 - mse: 17067588.0000 - mae: 1393.0703 - val_loss: 5274.9502 - val_mse: 222098224.0000 - val_mae: 5274.9502\n",
      "Epoch 395/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1424.4025 - mse: 17723810.0000 - mae: 1424.4025 - val_loss: 5297.1167 - val_mse: 223274688.0000 - val_mae: 5297.1167\n",
      "Epoch 396/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1409.1638 - mse: 17066732.0000 - mae: 1409.1636 - val_loss: 5227.6621 - val_mse: 223502736.0000 - val_mae: 5227.6621\n",
      "Epoch 397/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1400.5452 - mse: 17492326.0000 - mae: 1400.5450 - val_loss: 5251.3896 - val_mse: 223088928.0000 - val_mae: 5251.3896\n",
      "Epoch 398/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1388.7412 - mse: 16850154.0000 - mae: 1388.7411 - val_loss: 5266.1284 - val_mse: 223354288.0000 - val_mae: 5266.1284\n",
      "Epoch 399/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1408.4179 - mse: 17323774.0000 - mae: 1408.4180 - val_loss: 5266.5854 - val_mse: 222629536.0000 - val_mae: 5266.5854\n",
      "Epoch 400/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1396.4386 - mse: 16469499.0000 - mae: 1396.4385 - val_loss: 5219.4678 - val_mse: 222190608.0000 - val_mae: 5219.4678\n",
      "Epoch 401/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1382.2461 - mse: 16263117.0000 - mae: 1382.2461 - val_loss: 5225.7056 - val_mse: 220798544.0000 - val_mae: 5225.7056\n",
      "Epoch 402/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1399.9871 - mse: 16746676.0000 - mae: 1399.9869 - val_loss: 5250.4526 - val_mse: 221239728.0000 - val_mae: 5250.4526\n",
      "Epoch 403/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1385.2269 - mse: 15784877.0000 - mae: 1385.2267 - val_loss: 5233.6533 - val_mse: 221883728.0000 - val_mae: 5233.6533\n",
      "Epoch 404/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1410.5940 - mse: 17708416.0000 - mae: 1410.5941 - val_loss: 5227.6450 - val_mse: 221702224.0000 - val_mae: 5227.6450\n",
      "Epoch 405/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1408.6779 - mse: 17703188.0000 - mae: 1408.6779 - val_loss: 5294.6338 - val_mse: 221006240.0000 - val_mae: 5294.6338\n",
      "Epoch 406/500\n",
      "11500/11500 [==============================] - 0s 42us/step - loss: 1421.5067 - mse: 17820434.0000 - mae: 1421.5065 - val_loss: 5240.4990 - val_mse: 222242592.0000 - val_mae: 5240.4990\n",
      "Epoch 407/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1388.8952 - mse: 16831758.0000 - mae: 1388.8953 - val_loss: 5296.7637 - val_mse: 221660304.0000 - val_mae: 5296.7637\n",
      "Epoch 408/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1382.2565 - mse: 16283915.0000 - mae: 1382.2566 - val_loss: 5316.7515 - val_mse: 221306224.0000 - val_mae: 5316.7515\n",
      "Epoch 409/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1400.7146 - mse: 16698333.0000 - mae: 1400.7146 - val_loss: 5257.5557 - val_mse: 222115888.0000 - val_mae: 5257.5557\n",
      "Epoch 410/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1388.9240 - mse: 16359448.0000 - mae: 1388.9240 - val_loss: 5264.4160 - val_mse: 221647072.0000 - val_mae: 5264.4160\n",
      "Epoch 411/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1389.0988 - mse: 15888747.0000 - mae: 1389.0988 - val_loss: 5275.9863 - val_mse: 221499328.0000 - val_mae: 5275.9863\n",
      "Epoch 412/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1377.8512 - mse: 15709350.0000 - mae: 1377.8512 - val_loss: 5296.1851 - val_mse: 221482784.0000 - val_mae: 5296.1851\n",
      "Epoch 413/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1377.5081 - mse: 15336591.0000 - mae: 1377.5082 - val_loss: 5285.4956 - val_mse: 220984352.0000 - val_mae: 5285.4956\n",
      "Epoch 414/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1414.5075 - mse: 17223716.0000 - mae: 1414.5076 - val_loss: 5253.6245 - val_mse: 221753456.0000 - val_mae: 5253.6245\n",
      "Epoch 415/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1410.0010 - mse: 17782732.0000 - mae: 1410.0010 - val_loss: 5281.2231 - val_mse: 221380832.0000 - val_mae: 5281.2231\n",
      "Epoch 416/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1384.4400 - mse: 16283126.0000 - mae: 1384.4399 - val_loss: 5234.1240 - val_mse: 222449664.0000 - val_mae: 5234.1240\n",
      "Epoch 417/500\n",
      "11500/11500 [==============================] - 1s 48us/step - loss: 1402.0419 - mse: 17766782.0000 - mae: 1402.0419 - val_loss: 5228.0991 - val_mse: 222062496.0000 - val_mae: 5228.0991\n",
      "Epoch 418/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1410.1930 - mse: 17215422.0000 - mae: 1410.1930 - val_loss: 5205.4536 - val_mse: 222361088.0000 - val_mae: 5205.4536\n",
      "Epoch 419/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1403.5496 - mse: 17201678.0000 - mae: 1403.5497 - val_loss: 5232.0977 - val_mse: 221355872.0000 - val_mae: 5232.0977\n",
      "Epoch 420/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1414.5793 - mse: 17233364.0000 - mae: 1414.5793 - val_loss: 5185.1484 - val_mse: 221672624.0000 - val_mae: 5185.1484\n",
      "Epoch 421/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1404.5209 - mse: 17832950.0000 - mae: 1404.5210 - val_loss: 5238.8540 - val_mse: 221150144.0000 - val_mae: 5238.8540\n",
      "Epoch 422/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1415.2994 - mse: 17945578.0000 - mae: 1415.2994 - val_loss: 5249.6538 - val_mse: 222002944.0000 - val_mae: 5249.6538\n",
      "Epoch 423/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1405.6770 - mse: 17352454.0000 - mae: 1405.6769 - val_loss: 5247.5479 - val_mse: 221093744.0000 - val_mae: 5247.5479\n",
      "Epoch 424/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1423.0955 - mse: 17326052.0000 - mae: 1423.0953 - val_loss: 5261.3301 - val_mse: 220941936.0000 - val_mae: 5261.3301\n",
      "Epoch 425/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1391.5268 - mse: 16587697.0000 - mae: 1391.5266 - val_loss: 5235.7622 - val_mse: 221084608.0000 - val_mae: 5235.7622\n",
      "Epoch 426/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1415.8650 - mse: 17300556.0000 - mae: 1415.8650 - val_loss: 5232.2285 - val_mse: 221101456.0000 - val_mae: 5232.2285\n",
      "Epoch 427/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1406.0160 - mse: 17643364.0000 - mae: 1406.0159 - val_loss: 5262.9917 - val_mse: 221932768.0000 - val_mae: 5262.9917\n",
      "Epoch 428/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1389.2406 - mse: 16337880.0000 - mae: 1389.2407 - val_loss: 5283.6890 - val_mse: 222236976.0000 - val_mae: 5283.6890\n",
      "Epoch 429/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1396.8177 - mse: 16213302.0000 - mae: 1396.8176 - val_loss: 5286.7505 - val_mse: 222465296.0000 - val_mae: 5286.7505\n",
      "Epoch 430/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1457.4250 - mse: 19096910.0000 - mae: 1457.4250 - val_loss: 5262.6948 - val_mse: 222419952.0000 - val_mae: 5262.6948\n",
      "Epoch 431/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1405.5353 - mse: 16837870.0000 - mae: 1405.5352 - val_loss: 5242.6299 - val_mse: 221811216.0000 - val_mae: 5242.6299\n",
      "Epoch 432/500\n",
      "11500/11500 [==============================] - 1s 48us/step - loss: 1388.0538 - mse: 16624259.0000 - mae: 1388.0537 - val_loss: 5228.3979 - val_mse: 221903264.0000 - val_mae: 5228.3979\n",
      "Epoch 433/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1410.6978 - mse: 17661890.0000 - mae: 1410.6979 - val_loss: 5250.3276 - val_mse: 221227712.0000 - val_mae: 5250.3276\n",
      "Epoch 434/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1396.7603 - mse: 16688673.0000 - mae: 1396.7603 - val_loss: 5242.7642 - val_mse: 219954816.0000 - val_mae: 5242.7642\n",
      "Epoch 435/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1415.0980 - mse: 17492780.0000 - mae: 1415.0980 - val_loss: 5243.3062 - val_mse: 220690864.0000 - val_mae: 5243.3062\n",
      "Epoch 436/500\n",
      "11500/11500 [==============================] - 1s 48us/step - loss: 1431.5536 - mse: 17882750.0000 - mae: 1431.5536 - val_loss: 5233.5186 - val_mse: 220077136.0000 - val_mae: 5233.5186\n",
      "Epoch 437/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1401.0250 - mse: 17307786.0000 - mae: 1401.0249 - val_loss: 5215.9976 - val_mse: 220859440.0000 - val_mae: 5215.9976\n",
      "Epoch 438/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1375.9597 - mse: 16493079.0000 - mae: 1375.9597 - val_loss: 5229.0083 - val_mse: 221742960.0000 - val_mae: 5229.0083\n",
      "Epoch 439/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1380.1634 - mse: 15918065.0000 - mae: 1380.1635 - val_loss: 5209.7534 - val_mse: 220696688.0000 - val_mae: 5209.7534\n",
      "Epoch 440/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1394.4988 - mse: 16766403.0000 - mae: 1394.4988 - val_loss: 5247.5850 - val_mse: 220437184.0000 - val_mae: 5247.5850\n",
      "Epoch 441/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1417.2263 - mse: 17828372.0000 - mae: 1417.2263 - val_loss: 5222.7090 - val_mse: 221348096.0000 - val_mae: 5222.7090\n",
      "Epoch 442/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1386.6141 - mse: 16291070.0000 - mae: 1386.6141 - val_loss: 5236.8003 - val_mse: 221990944.0000 - val_mae: 5236.8003\n",
      "Epoch 443/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1425.6605 - mse: 18182944.0000 - mae: 1425.6604 - val_loss: 5214.0034 - val_mse: 221670768.0000 - val_mae: 5214.0034\n",
      "Epoch 444/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1392.3988 - mse: 16926750.0000 - mae: 1392.3989 - val_loss: 5204.4287 - val_mse: 220844032.0000 - val_mae: 5204.4287\n",
      "Epoch 445/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1389.5243 - mse: 16711082.0000 - mae: 1389.5242 - val_loss: 5226.0933 - val_mse: 220565952.0000 - val_mae: 5226.0933\n",
      "Epoch 446/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1417.7148 - mse: 18053054.0000 - mae: 1417.7147 - val_loss: 5235.9785 - val_mse: 220904128.0000 - val_mae: 5235.9785\n",
      "Epoch 447/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1402.6155 - mse: 17988396.0000 - mae: 1402.6155 - val_loss: 5213.2002 - val_mse: 221853248.0000 - val_mae: 5213.2002\n",
      "Epoch 448/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1393.0324 - mse: 17214240.0000 - mae: 1393.0322 - val_loss: 5219.0947 - val_mse: 221439296.0000 - val_mae: 5219.0947\n",
      "Epoch 449/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1407.7879 - mse: 17487412.0000 - mae: 1407.7881 - val_loss: 5223.1812 - val_mse: 220158256.0000 - val_mae: 5223.1812\n",
      "Epoch 450/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1379.1384 - mse: 16462590.0000 - mae: 1379.1384 - val_loss: 5210.1265 - val_mse: 220690784.0000 - val_mae: 5210.1265\n",
      "Epoch 451/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1377.8266 - mse: 16547517.0000 - mae: 1377.8267 - val_loss: 5224.2383 - val_mse: 221351472.0000 - val_mae: 5224.2383\n",
      "Epoch 452/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1374.7375 - mse: 16401356.0000 - mae: 1374.7374 - val_loss: 5205.4810 - val_mse: 220531632.0000 - val_mae: 5205.4810\n",
      "Epoch 453/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1406.5074 - mse: 17874468.0000 - mae: 1406.5074 - val_loss: 5231.8125 - val_mse: 219880080.0000 - val_mae: 5231.8125\n",
      "Epoch 454/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1412.5980 - mse: 17498570.0000 - mae: 1412.5981 - val_loss: 5231.3623 - val_mse: 220000160.0000 - val_mae: 5231.3623\n",
      "Epoch 455/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1395.6946 - mse: 17349804.0000 - mae: 1395.6946 - val_loss: 5216.7896 - val_mse: 219872752.0000 - val_mae: 5216.7896\n",
      "Epoch 456/500\n",
      "11500/11500 [==============================] - 1s 48us/step - loss: 1403.8984 - mse: 17432866.0000 - mae: 1403.8984 - val_loss: 5168.7827 - val_mse: 220421824.0000 - val_mae: 5168.7827\n",
      "Epoch 457/500\n",
      "11500/11500 [==============================] - 1s 49us/step - loss: 1409.5585 - mse: 18017176.0000 - mae: 1409.5585 - val_loss: 5218.2100 - val_mse: 220185760.0000 - val_mae: 5218.2100\n",
      "Epoch 458/500\n",
      "11500/11500 [==============================] - 1s 48us/step - loss: 1413.2551 - mse: 17421136.0000 - mae: 1413.2551 - val_loss: 5205.9111 - val_mse: 219748672.0000 - val_mae: 5205.9111\n",
      "Epoch 459/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1387.6729 - mse: 16428235.0000 - mae: 1387.6729 - val_loss: 5223.5820 - val_mse: 220125456.0000 - val_mae: 5223.5820\n",
      "Epoch 460/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1401.1306 - mse: 16839356.0000 - mae: 1401.1307 - val_loss: 5222.5654 - val_mse: 220714992.0000 - val_mae: 5222.5654\n",
      "Epoch 461/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1414.6360 - mse: 18637520.0000 - mae: 1414.6359 - val_loss: 5235.9834 - val_mse: 221186896.0000 - val_mae: 5235.9834\n",
      "Epoch 462/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1374.9024 - mse: 16391629.0000 - mae: 1374.9023 - val_loss: 5242.6074 - val_mse: 220074208.0000 - val_mae: 5242.6074\n",
      "Epoch 463/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1398.2232 - mse: 17437358.0000 - mae: 1398.2233 - val_loss: 5267.7598 - val_mse: 220312816.0000 - val_mae: 5267.7598\n",
      "Epoch 464/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1384.9073 - mse: 16418099.0000 - mae: 1384.9073 - val_loss: 5238.6689 - val_mse: 220277008.0000 - val_mae: 5238.6689\n",
      "Epoch 465/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1398.6663 - mse: 16817208.0000 - mae: 1398.6661 - val_loss: 5226.8159 - val_mse: 219830512.0000 - val_mae: 5226.8159\n",
      "Epoch 466/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1394.3644 - mse: 16881782.0000 - mae: 1394.3644 - val_loss: 5246.1504 - val_mse: 218727056.0000 - val_mae: 5246.1504\n",
      "Epoch 467/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1381.9189 - mse: 16705490.0000 - mae: 1381.9188 - val_loss: 5227.5845 - val_mse: 219246192.0000 - val_mae: 5227.5845\n",
      "Epoch 468/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1386.7760 - mse: 16916532.0000 - mae: 1386.7761 - val_loss: 5198.3945 - val_mse: 219366352.0000 - val_mae: 5198.3945\n",
      "Epoch 469/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1394.6909 - mse: 17885684.0000 - mae: 1394.6912 - val_loss: 5228.8149 - val_mse: 219487536.0000 - val_mae: 5228.8149\n",
      "Epoch 470/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1381.2384 - mse: 16370974.0000 - mae: 1381.2386 - val_loss: 5198.4370 - val_mse: 218871504.0000 - val_mae: 5198.4370\n",
      "Epoch 471/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1392.6061 - mse: 16671729.0000 - mae: 1392.6061 - val_loss: 5220.5073 - val_mse: 219815408.0000 - val_mae: 5220.5073\n",
      "Epoch 472/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1407.1547 - mse: 17261732.0000 - mae: 1407.1547 - val_loss: 5245.8623 - val_mse: 220075680.0000 - val_mae: 5245.8623\n",
      "Epoch 473/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1391.9640 - mse: 17187754.0000 - mae: 1391.9639 - val_loss: 5254.6250 - val_mse: 221132816.0000 - val_mae: 5254.6250\n",
      "Epoch 474/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1372.9477 - mse: 16449064.0000 - mae: 1372.9476 - val_loss: 5257.2500 - val_mse: 219842368.0000 - val_mae: 5257.2500\n",
      "Epoch 475/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1368.0628 - mse: 15516152.0000 - mae: 1368.0629 - val_loss: 5263.4175 - val_mse: 219265872.0000 - val_mae: 5263.4175\n",
      "Epoch 476/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1408.0475 - mse: 17757364.0000 - mae: 1408.0476 - val_loss: 5266.1533 - val_mse: 219802976.0000 - val_mae: 5266.1533\n",
      "Epoch 477/500\n",
      "11500/11500 [==============================] - 1s 48us/step - loss: 1369.9739 - mse: 16002512.0000 - mae: 1369.9739 - val_loss: 5248.6768 - val_mse: 219046736.0000 - val_mae: 5248.6768\n",
      "Epoch 478/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1382.5316 - mse: 16865416.0000 - mae: 1382.5316 - val_loss: 5272.5996 - val_mse: 219191600.0000 - val_mae: 5272.5996\n",
      "Epoch 479/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1399.0182 - mse: 16712911.0000 - mae: 1399.0182 - val_loss: 5238.0195 - val_mse: 219342768.0000 - val_mae: 5238.0195\n",
      "Epoch 480/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1387.4907 - mse: 16708781.0000 - mae: 1387.4908 - val_loss: 5224.3203 - val_mse: 219505040.0000 - val_mae: 5224.3203\n",
      "Epoch 481/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1366.9944 - mse: 15939848.0000 - mae: 1366.9944 - val_loss: 5197.4033 - val_mse: 219314576.0000 - val_mae: 5197.4033\n",
      "Epoch 482/500\n",
      "11500/11500 [==============================] - 1s 44us/step - loss: 1413.1488 - mse: 17477922.0000 - mae: 1413.1489 - val_loss: 5197.5654 - val_mse: 219430976.0000 - val_mae: 5197.5654\n",
      "Epoch 483/500\n",
      "11500/11500 [==============================] - 1s 49us/step - loss: 1408.0139 - mse: 17918886.0000 - mae: 1408.0138 - val_loss: 5198.4956 - val_mse: 219076256.0000 - val_mae: 5198.4956\n",
      "Epoch 484/500\n",
      "11500/11500 [==============================] - 1s 48us/step - loss: 1378.0755 - mse: 16707054.0000 - mae: 1378.0754 - val_loss: 5175.8740 - val_mse: 219628688.0000 - val_mae: 5175.8740\n",
      "Epoch 485/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1399.3135 - mse: 17469168.0000 - mae: 1399.3136 - val_loss: 5189.5957 - val_mse: 219534000.0000 - val_mae: 5189.5957\n",
      "Epoch 486/500\n",
      "11500/11500 [==============================] - 1s 49us/step - loss: 1403.3320 - mse: 17675170.0000 - mae: 1403.3318 - val_loss: 5193.1206 - val_mse: 218886640.0000 - val_mae: 5193.1206\n",
      "Epoch 487/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1403.1110 - mse: 17311630.0000 - mae: 1403.1111 - val_loss: 5210.0718 - val_mse: 219413456.0000 - val_mae: 5210.0718\n",
      "Epoch 488/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1378.9859 - mse: 16314308.0000 - mae: 1378.9857 - val_loss: 5193.7598 - val_mse: 218677872.0000 - val_mae: 5193.7598\n",
      "Epoch 489/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1391.0689 - mse: 16975952.0000 - mae: 1391.0691 - val_loss: 5228.1611 - val_mse: 218535360.0000 - val_mae: 5228.1611\n",
      "Epoch 490/500\n",
      "11500/11500 [==============================] - 1s 48us/step - loss: 1389.7215 - mse: 16644838.0000 - mae: 1389.7216 - val_loss: 5233.2720 - val_mse: 218162432.0000 - val_mae: 5233.2720\n",
      "Epoch 491/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1386.3132 - mse: 16642934.0000 - mae: 1386.3134 - val_loss: 5210.8340 - val_mse: 219492544.0000 - val_mae: 5210.8340\n",
      "Epoch 492/500\n",
      "11500/11500 [==============================] - 1s 45us/step - loss: 1374.9367 - mse: 16070191.0000 - mae: 1374.9366 - val_loss: 5220.0137 - val_mse: 219623920.0000 - val_mae: 5220.0137\n",
      "Epoch 493/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1357.3106 - mse: 14966003.0000 - mae: 1357.3105 - val_loss: 5252.8657 - val_mse: 218479136.0000 - val_mae: 5252.8657\n",
      "Epoch 494/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1401.9954 - mse: 17125596.0000 - mae: 1401.9952 - val_loss: 5244.3320 - val_mse: 219775136.0000 - val_mae: 5244.3320\n",
      "Epoch 495/500\n",
      "11500/11500 [==============================] - 1s 48us/step - loss: 1385.8855 - mse: 16974678.0000 - mae: 1385.8856 - val_loss: 5217.2920 - val_mse: 219629808.0000 - val_mae: 5217.2920\n",
      "Epoch 496/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1388.4554 - mse: 17243856.0000 - mae: 1388.4553 - val_loss: 5201.3242 - val_mse: 219414816.0000 - val_mae: 5201.3242\n",
      "Epoch 497/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1354.5241 - mse: 15808759.0000 - mae: 1354.5242 - val_loss: 5224.2070 - val_mse: 218174256.0000 - val_mae: 5224.2070\n",
      "Epoch 498/500\n",
      "11500/11500 [==============================] - 0s 43us/step - loss: 1380.4953 - mse: 16625457.0000 - mae: 1380.4954 - val_loss: 5228.3784 - val_mse: 218990240.0000 - val_mae: 5228.3784\n",
      "Epoch 499/500\n",
      "11500/11500 [==============================] - 1s 46us/step - loss: 1386.4995 - mse: 16704917.0000 - mae: 1386.4995 - val_loss: 5260.7993 - val_mse: 219389008.0000 - val_mae: 5260.7993\n",
      "Epoch 500/500\n",
      "11500/11500 [==============================] - 1s 47us/step - loss: 1373.9788 - mse: 16022360.0000 - mae: 1373.9788 - val_loss: 5233.5112 - val_mse: 219196944.0000 - val_mae: 5233.5112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "result_sklearn, running_time_static, united_df = scikit_learn(result, pretrain_days)  # Updated Now\n",
    "save_united_df(united_df, exp2_static_united_df_path)  # Added Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TZSPkuft7YMp",
    "outputId": "9e0a6310-3266-4294-81dd-75c6b6642ddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Score\n",
      "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge     LSTM\n",
      "0                      MAE        30.000      2836.333          3704.997   2106.660      6754.469       1658.318 1342.395\n",
      "1                      MAE        60.000      1637.215          1339.464   3838.418      1710.792       1709.346 1725.405\n",
      "2                      MAE        90.000      1996.061          1862.493   4605.222      2039.129       2104.286 2631.681\n",
      "3                      MAE       120.000      1816.388          1462.434   1981.307      1848.753       2161.108 2223.153\n",
      "4                      MAE       150.000      2114.614          1640.785   4247.639      1945.395       2626.347 2017.279\n",
      "5                      MAE       180.000      2622.349          2561.337   3722.595      3019.785       2522.026 2413.682\n",
      "6                      MAE       210.000      5028.751          4825.993   6875.685      5480.489       4901.760 5236.348\n",
      "7                      MAE       240.000      7522.040          7018.604   9240.287      6502.661       5860.868 7124.834\n",
      "mean                   NaN       135.000      3196.719          3052.013   4577.227      3662.684       2943.008 3089.347\n",
      "-----------------------------------------------------------------------------------\n",
      "RMSE Score\n",
      "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge      LSTM\n",
      "0                     RMSE        30.000      4121.558          6141.487   7027.743     12935.371       2950.055  4121.003\n",
      "1                     RMSE        60.000      3322.405          3672.510  11049.203      4650.778       4050.313  4963.323\n",
      "2                     RMSE        90.000      5537.608          5385.145  12258.316      6155.246       6464.182  8160.310\n",
      "3                     RMSE       120.000      5034.694          3971.043   6880.370      5023.203       6056.116  6882.036\n",
      "4                     RMSE       150.000      5847.950          6071.270  14917.217      6266.338       7399.755  5627.260\n",
      "5                     RMSE       180.000      5924.396          6511.669   9753.894      7496.712       6555.802  4998.806\n",
      "6                     RMSE       210.000     12338.209         13450.031  19052.722     14504.625      11960.619 12231.953\n",
      "7                     RMSE       240.000     19785.833         14794.089  21652.350     13082.492      15932.994 19187.972\n",
      "mean                   NaN       135.000      7739.082          7499.656  12823.977      8764.345       7671.230  8271.583\n",
      "-----------------------------------------------------------------------------------\n",
      "MAPE Score\n",
      "     EvaluationMeasurement  PretrainDays  RandomForest  GradientBoosting  LinearSVR  DecisionTree  BayesianRidge    LSTM\n",
      "0                     MAPE        30.000       414.317          1061.428    115.384      1236.531        273.794  47.220\n",
      "1                     MAPE        60.000       266.878           234.445     29.205       392.686        264.803  42.402\n",
      "2                     MAPE        90.000       196.556           260.122     31.565       223.949        182.037 309.238\n",
      "3                     MAPE       120.000        39.718            30.906      3.574        31.790         16.729  21.055\n",
      "4                     MAPE       150.000       111.077            88.605      7.550        85.370         52.650   5.518\n",
      "5                     MAPE       180.000        43.708            28.134      3.340        47.499         28.703   2.119\n",
      "6                     MAPE       210.000        18.951            10.969      2.207        11.531         16.357   1.253\n",
      "7                     MAPE       240.000        20.173            11.151      1.955        10.401         22.211   1.117\n",
      "mean                   NaN       135.000       138.922           215.720     24.348       254.970        107.160  53.740\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sklearn = calc_save_err_metric_combined(error_metrics, result_sklearn, max_of_pretrain_days, max_selected_countries, path=exp2_path, static_learner=True, alternate_batch=False, transpose=True)\n",
    "display_scores(df_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "U44wYr8WAqq8",
    "outputId": "9e6e3a69-b7e8-4ed4-b119-a4d9b917c5fa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PretrainDays</th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>GradientBoosting</th>\n",
       "      <th>LinearSVR</th>\n",
       "      <th>DecisionTree</th>\n",
       "      <th>BayesianRidge</th>\n",
       "      <th>LSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>33.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>0.927</td>\n",
       "      <td>1.808</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.026</td>\n",
       "      <td>60.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>1.875</td>\n",
       "      <td>3.909</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.020</td>\n",
       "      <td>93.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>2.702</td>\n",
       "      <td>5.936</td>\n",
       "      <td>1.360</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.033</td>\n",
       "      <td>120.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150</td>\n",
       "      <td>3.488</td>\n",
       "      <td>7.824</td>\n",
       "      <td>1.875</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.031</td>\n",
       "      <td>152.735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180</td>\n",
       "      <td>4.314</td>\n",
       "      <td>9.680</td>\n",
       "      <td>2.708</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.036</td>\n",
       "      <td>182.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>210</td>\n",
       "      <td>5.186</td>\n",
       "      <td>11.639</td>\n",
       "      <td>3.441</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.047</td>\n",
       "      <td>217.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>240</td>\n",
       "      <td>6.011</td>\n",
       "      <td>13.594</td>\n",
       "      <td>4.581</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.055</td>\n",
       "      <td>249.336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PretrainDays  RandomForest  ...  BayesianRidge    LSTM\n",
       "0            30         0.346  ...          0.042  33.485\n",
       "1            60         0.927  ...          0.026  60.096\n",
       "2            90         1.875  ...          0.020  93.677\n",
       "3           120         2.702  ...          0.033 120.079\n",
       "4           150         3.488  ...          0.031 152.735\n",
       "5           180         4.314  ...          0.036 182.047\n",
       "6           210         5.186  ...          0.047 217.081\n",
       "7           240         6.011  ...          0.055 249.336\n",
       "\n",
       "[8 rows x 7 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_runtime(running_time_static, path=exp2_runtime_path, static_learner=True)\n",
    "running_time_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "-7NiYw19VchA",
    "outputId": "8d1376f7-fbd7-4534-89d0-50a94e3e07b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>GradientBoosting</th>\n",
       "      <th>LinearSVR</th>\n",
       "      <th>DecisionTree</th>\n",
       "      <th>BayesianRidge</th>\n",
       "      <th>LSTM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>3196.719</td>\n",
       "      <td>3052.013</td>\n",
       "      <td>4577.227</td>\n",
       "      <td>3662.684</td>\n",
       "      <td>2943.008</td>\n",
       "      <td>3089.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE</th>\n",
       "      <td>138.922</td>\n",
       "      <td>215.720</td>\n",
       "      <td>24.348</td>\n",
       "      <td>254.970</td>\n",
       "      <td>107.160</td>\n",
       "      <td>53.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE</th>\n",
       "      <td>7739.082</td>\n",
       "      <td>7499.656</td>\n",
       "      <td>12823.977</td>\n",
       "      <td>8764.345</td>\n",
       "      <td>7671.230</td>\n",
       "      <td>8271.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time(sec)</th>\n",
       "      <td>3.106</td>\n",
       "      <td>6.858</td>\n",
       "      <td>1.907</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.036</td>\n",
       "      <td>138.567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           RandomForest  GradientBoosting  ...  BayesianRidge     LSTM\n",
       "Metric                                     ...                        \n",
       "MAE            3196.719          3052.013  ...       2943.008 3089.347\n",
       "MAPE            138.922           215.720  ...        107.160   53.740\n",
       "RMSE           7739.082          7499.656  ...       7671.230 8271.583\n",
       "Time(sec)         3.106             6.858  ...          0.036  138.567\n",
       "\n",
       "[4 rows x 6 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_table_static = get_summary_table(df_sklearn, running_time_static, error_metrics, static_learner=True)\n",
    "save_summary_table(summary_table_static, exp2_summary_path,static_learner=True,alternate_batch=False, transpose=True)\n",
    "summary_table_static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lj09CNlNQYYL"
   },
   "source": [
    "# Experiment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAdncNlrKXD8"
   },
   "source": [
    "## Incremental Learner: Interleaved Batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGGSn84iKraj"
   },
   "outputs": [],
   "source": [
    "def scikit_multiflow_alternate_batch(df, pretrain_days):\n",
    "\n",
    "    model, model_names = instantiate_regressors()\n",
    "\n",
    "    len_countries = len(df['country'].unique())\n",
    "\n",
    "    # Selecting only required countries\n",
    "    df = df[df['country'].isin(df['country'].unique()[0:len_countries])]  # Added Now\n",
    "\n",
    "    frames, running_time_frames = [], []\n",
    "\n",
    "    united_dataframe = []  # Added Now\n",
    "\n",
    "    # Setup the evaluator\n",
    "    for day in pretrain_days:\n",
    "\n",
    "        df_subset = create_alternate_batch_subset(df, day, batch_size=10)\n",
    "\n",
    "        # Creating a stream from dataframe\n",
    "        stream = DataStream(np.array(df_subset.iloc[:, 4:-1]), y=np.array(df_subset.iloc[:, -1])) \n",
    "\n",
    "        pretrain_size = (day//2) * len_countries\n",
    "        max_samples = len(df_subset)  # (day//2 + 30) * len_countries  # Testing on set one month ahead only\n",
    "\n",
    "        evaluator = EvaluatePrequential(show_plot=False,\n",
    "                                    pretrain_size=pretrain_size,\n",
    "                                    metrics = ['mean_square_error', 'mean_absolute_error', 'mean_absolute_percentage_error'],\n",
    "                                    max_samples=max_samples)\n",
    "        # Run evaluation\n",
    "        evaluator.evaluate(stream=stream, model=model, model_names=model_names)\n",
    "\n",
    "        date_idx = list(df_subset.columns).index('date')  # Added Now\n",
    "        country_idx = list(df_subset.columns).index('country')  # Added Now\n",
    "\n",
    "        target_dates = df_subset.iloc[pretrain_size: max_samples, date_idx]  # Added Now\n",
    "        subset_countries_names = df_subset.iloc[pretrain_size:max_samples, country_idx]  # Added Now\n",
    "\n",
    "        united_dataframe.append(unit_incremental_df(subset_countries_names, evaluator, target_dates, day))  # Added now\n",
    "\n",
    "        # Dictionary to store each iteration error scores\n",
    "        mdl_evaluation_scores = {}\n",
    "\n",
    "        # Adding Evaluation Measurements and pretraining days\n",
    "        mdl_evaluation_scores['EvaluationMeasurement'] = ['RMSE','MAE','MAPE'] #,'MSE']\n",
    "        mdl_evaluation_scores['PretrainDays'] = [day//2] * len(mdl_evaluation_scores['EvaluationMeasurement'])\n",
    "\n",
    "        mdl_evaluation_df = get_error_scores_per_model(evaluator, mdl_evaluation_scores, inc_alt_batches=True)\n",
    "\n",
    "        # Errors of each model on a specific pre-train days\n",
    "        frames.append(mdl_evaluation_df)\n",
    "\n",
    "        # Run time for each algorithm\n",
    "        running_time_frames.append(get_running_time_per_model_incremental_learner(evaluator, day))\n",
    "\n",
    "    # Final Run Time DataFrame\n",
    "    running_time_df = pd.concat(running_time_frames,ignore_index=True)\n",
    "\n",
    "    united_df = pd.concat(united_dataframe, ignore_index=True)\n",
    "\n",
    "    # Final Evaluation Score Dataframe\n",
    "    evaluation_scores_df = pd.concat(frames, ignore_index=True)\n",
    "    return evaluation_scores_df, running_time_df, united_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Dv6KT2eyheiH",
    "outputId": "af86443b-f364-4778-9c44-0113bf646f4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 750 sample(s).\n",
      "Evaluating...\n",
      " #################### [100%] [35.71s]\n",
      "Processed samples: 1750\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 133269939.4735\n",
      "HT_Reg - MAPE          : 115.0086\n",
      "HT_Reg - MAE          : 2943.953795\n",
      "HAT_Reg - MSE          : 133160875.0133\n",
      "HAT_Reg - MAPE          : 114.6996\n",
      "HAT_Reg - MAE          : 2940.425192\n",
      "ARF_Reg - MSE          : 164624240.0096\n",
      "ARF_Reg - MAPE          : 55.6659\n",
      "ARF_Reg - MAE          : 2864.440551\n",
      "PA_Reg - MSE          : 2474980012.2394\n",
      "PA_Reg - MAPE          : 97.2752\n",
      "PA_Reg - MAE          : 9302.184090\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 1500 sample(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n",
      "/content/src/measure_collection.py:1286: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      " #################### [100%] [54.55s]\n",
      "Processed samples: 2500\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 154869117.4124\n",
      "HT_Reg - MAPE          : 24.2989\n",
      "HT_Reg - MAE          : 2584.916897\n",
      "HAT_Reg - MSE          : 154868383.4411\n",
      "HAT_Reg - MAPE          : 24.3941\n",
      "HAT_Reg - MAE          : 2584.865898\n",
      "ARF_Reg - MSE          : 122565656.3086\n",
      "ARF_Reg - MAPE          : 7.6230\n",
      "ARF_Reg - MAE          : 2408.730175\n",
      "PA_Reg - MSE          : 9761173248.9137\n",
      "PA_Reg - MAPE          : 74.6468\n",
      "PA_Reg - MAE          : 22699.021744\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 2250 sample(s).\n",
      "Evaluating...\n",
      " ###################- [95%] [66.56s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n",
      "/content/src/measure_collection.py:1286: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #################### [100%] [70.70s]\n",
      "Processed samples: 3250\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 90235808.0461\n",
      "HT_Reg - MAPE          : 19.3415\n",
      "HT_Reg - MAE          : 1880.684885\n",
      "HAT_Reg - MSE          : 90233399.6362\n",
      "HAT_Reg - MAPE          : 19.3807\n",
      "HAT_Reg - MAE          : 1879.667433\n",
      "ARF_Reg - MSE          : 78979275.1664\n",
      "ARF_Reg - MAPE          : 3.7696\n",
      "ARF_Reg - MAE          : 1815.878744\n",
      "PA_Reg - MSE          : 2354975192.8982\n",
      "PA_Reg - MAPE          : 65.1502\n",
      "PA_Reg - MAE          : 13543.329802\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 3000 sample(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n",
      "/content/src/measure_collection.py:1286: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      " #################### [100%] [89.68s]\n",
      "Processed samples: 4000\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 170646198.3557\n",
      "HT_Reg - MAPE          : 5.0166\n",
      "HT_Reg - MAE          : 2743.021077\n",
      "HAT_Reg - MSE          : 170645740.2614\n",
      "HAT_Reg - MAPE          : 4.9222\n",
      "HAT_Reg - MAE          : 2742.535011\n",
      "ARF_Reg - MSE          : 174642257.2820\n",
      "ARF_Reg - MAPE          : 1.2228\n",
      "ARF_Reg - MAE          : 2941.147540\n",
      "PA_Reg - MSE          : 9111594426.8601\n",
      "PA_Reg - MAPE          : 12.5088\n",
      "PA_Reg - MAE          : 19729.551554\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 3750 sample(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n",
      "/content/src/measure_collection.py:1286: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      " #################### [100%] [111.63s]\n",
      "Processed samples: 4750\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 415808716.5285\n",
      "HT_Reg - MAPE          : 24.2528\n",
      "HT_Reg - MAE          : 4831.156488\n",
      "HAT_Reg - MSE          : 415803410.6487\n",
      "HAT_Reg - MAPE          : 24.2210\n",
      "HAT_Reg - MAE          : 4831.332009\n",
      "ARF_Reg - MSE          : 276602901.5802\n",
      "ARF_Reg - MAPE          : 5.1679\n",
      "ARF_Reg - MAE          : 3774.980606\n",
      "PA_Reg - MSE          : 8810914268.9992\n",
      "PA_Reg - MAPE          : 33.2258\n",
      "PA_Reg - MAE          : 20876.934922\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 4500 sample(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/src/measure_collection.py:1163: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n",
      "/content/src/measure_collection.py:1286: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (np.fabs(actual - pred) / np.fabs(actual))[mask].mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      " #################### [100%] [135.65s]\n",
      "Processed samples: 5500\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 549896256.7415\n",
      "HT_Reg - MAPE          : 17.8139\n",
      "HT_Reg - MAE          : 6169.495066\n",
      "HAT_Reg - MSE          : 549831337.5919\n",
      "HAT_Reg - MAPE          : 17.7794\n",
      "HAT_Reg - MAE          : 6166.204170\n",
      "ARF_Reg - MSE          : 476951492.6260\n",
      "ARF_Reg - MAPE          : 4.7532\n",
      "ARF_Reg - MAE          : 5369.572057\n",
      "PA_Reg - MSE          : 8476825536.9238\n",
      "PA_Reg - MAPE          : 25.4907\n",
      "PA_Reg - MAE          : 25109.963244\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 5250 sample(s).\n",
      "Evaluating...\n",
      " #################### [100%] [157.69s]\n",
      "Processed samples: 6250\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 615481075.0552\n",
      "HT_Reg - MAPE          : 8.5560\n",
      "HT_Reg - MAE          : 7853.351387\n",
      "HAT_Reg - MSE          : 615469253.4774\n",
      "HAT_Reg - MAPE          : 8.5402\n",
      "HAT_Reg - MAE          : 7852.458490\n",
      "ARF_Reg - MSE          : 431701335.9073\n",
      "ARF_Reg - MAPE          : 1.5550\n",
      "ARF_Reg - MAE          : 6495.218908\n",
      "PA_Reg - MSE          : 5079908566.2218\n",
      "PA_Reg - MAPE          : 7.4327\n",
      "PA_Reg - MAE          : 20876.990648\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Pre-training on 6000 sample(s).\n",
      "Evaluating...\n",
      " #################### [100%] [159.58s]\n",
      "Processed samples: 6270\n",
      "Mean performance:\n",
      "HT_Reg - MSE          : 864286956.3361\n",
      "HT_Reg - MAPE          : 3.9826\n",
      "HT_Reg - MAE          : 10131.987225\n",
      "HAT_Reg - MSE          : 864272632.5362\n",
      "HAT_Reg - MAPE          : 3.9784\n",
      "HAT_Reg - MAE          : 10131.076172\n",
      "ARF_Reg - MSE          : 672272049.1636\n",
      "ARF_Reg - MAPE          : 1.5215\n",
      "ARF_Reg - MAE          : 8393.690162\n",
      "PA_Reg - MSE          : 14291306918.1492\n",
      "PA_Reg - MAPE          : 11.1812\n",
      "PA_Reg - MAE          : 45690.610433\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-b899e9b2e5a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresult_skmlflow_alternate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_time_incremental_alternate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munited_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscikit_multiflow_alternate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrain_days\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msave_united_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munited_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp3_inc_alt_united_df_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Added Now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-802df940bcdb>\u001b[0m in \u001b[0;36msave_united_df\u001b[0;34m(df, path, country)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/{country}.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/united_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[1;32m   3168\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3169\u001b[0m         )\n\u001b[0;32m-> 3170\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             )\n\u001b[1;32m    192\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Result/exp2/united_dataframe/incremental_alternate/united_df.csv'"
     ]
    }
   ],
   "source": [
    "result_skmlflow_alternate_batch, running_time_incremental_alternate_batch, united_df = scikit_multiflow_alternate_batch(result, pretrain_days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZX44ojem_F9"
   },
   "outputs": [],
   "source": [
    "save_united_df(united_df, exp3_inc_alt_united_df_path)  # Added Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bfluozdk7zod"
   },
   "outputs": [],
   "source": [
    "df_alternate_batch = calc_save_err_metric_combined(error_metrics, result_skmlflow_alternate_batch, max_of_pretrain_days, max_selected_countries, path=exp3_path, static_learner=False, alternate_batch=True, transpose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dL_NOS6va8bo",
    "outputId": "932f29ea-8e26-4916-8530-3ee2e97b9e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Score\n",
      "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg  ARF_Reg    PA_Reg\n",
      "0                      MAE        15.000  2943.954  2940.425 2864.441  9302.184\n",
      "1                      MAE        30.000  2584.917  2584.866 2408.730 22699.022\n",
      "2                      MAE        45.000  1880.685  1879.667 1815.879 13543.330\n",
      "3                      MAE        60.000  2743.021  2742.535 2941.148 19729.552\n",
      "4                      MAE        75.000  4831.156  4831.332 3774.981 20876.935\n",
      "5                      MAE        90.000  6169.495  6166.204 5369.572 25109.963\n",
      "6                      MAE       105.000  7853.351  7852.458 6495.219 20876.991\n",
      "7                      MAE       120.000 10131.987 10131.076 8393.690 45690.610\n",
      "mean                   NaN        67.500  4892.321  4891.071 4257.957 22228.573\n",
      "-----------------------------------------------------------------------------------\n",
      "RMSE Score\n",
      "     EvaluationMeasurement  PretrainDays    HT_Reg   HAT_Reg   ARF_Reg     PA_Reg\n",
      "0                     RMSE        15.000 11544.260 11539.535 12830.598  49749.171\n",
      "1                     RMSE        30.000 12444.642 12444.613 11070.937  98798.650\n",
      "2                     RMSE        45.000  9499.253  9499.126  8887.028  48528.087\n",
      "3                     RMSE        60.000 13063.162 13063.144 13215.228  95454.672\n",
      "4                     RMSE        75.000 20391.388 20391.258 16631.383  93866.470\n",
      "5                     RMSE        90.000 23449.867 23448.483 21839.219  92069.678\n",
      "6                     RMSE       105.000 24808.891 24808.653 20777.424  71273.477\n",
      "7                     RMSE       120.000 29398.758 29398.514 25928.210 119546.254\n",
      "mean                   NaN        67.500 18075.028 18074.166 16397.503  83660.807\n",
      "-----------------------------------------------------------------------------------\n",
      "MAPE Score\n",
      "     EvaluationMeasurement  PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
      "0                     MAPE        15.000 115.009  114.700   55.666  97.275\n",
      "1                     MAPE        30.000  24.299   24.394    7.623  74.647\n",
      "2                     MAPE        45.000  19.342   19.381    3.770  65.150\n",
      "3                     MAPE        60.000   5.017    4.922    1.223  12.509\n",
      "4                     MAPE        75.000  24.253   24.221    5.168  33.226\n",
      "5                     MAPE        90.000  17.814   17.779    4.753  25.491\n",
      "6                     MAPE       105.000   8.556    8.540    1.555   7.433\n",
      "7                     MAPE       120.000   3.983    3.978    1.521  11.181\n",
      "mean                   NaN        67.500  27.284   27.239   10.160  40.864\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_scores(df_alternate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "zNBEQbIeA-7H",
    "outputId": "e9335f5c-ce71-4ee3-8e53-d6b3697821c8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PretrainDays</th>\n",
       "      <th>HT_Reg</th>\n",
       "      <th>HAT_Reg</th>\n",
       "      <th>ARF_Reg</th>\n",
       "      <th>PA_Reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>1.525</td>\n",
       "      <td>2.463</td>\n",
       "      <td>31.035</td>\n",
       "      <td>0.561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>2.758</td>\n",
       "      <td>4.232</td>\n",
       "      <td>46.853</td>\n",
       "      <td>0.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>3.115</td>\n",
       "      <td>6.307</td>\n",
       "      <td>60.609</td>\n",
       "      <td>0.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>3.753</td>\n",
       "      <td>9.648</td>\n",
       "      <td>75.609</td>\n",
       "      <td>0.549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150</td>\n",
       "      <td>4.914</td>\n",
       "      <td>15.892</td>\n",
       "      <td>90.163</td>\n",
       "      <td>0.543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180</td>\n",
       "      <td>5.987</td>\n",
       "      <td>21.069</td>\n",
       "      <td>107.937</td>\n",
       "      <td>0.543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>210</td>\n",
       "      <td>7.901</td>\n",
       "      <td>26.088</td>\n",
       "      <td>123.051</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>240</td>\n",
       "      <td>6.757</td>\n",
       "      <td>23.454</td>\n",
       "      <td>129.213</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PretrainDays  HT_Reg  HAT_Reg  ARF_Reg  PA_Reg\n",
       "0            30   1.525    2.463   31.035   0.561\n",
       "1            60   2.758    4.232   46.853   0.580\n",
       "2            90   3.115    6.307   60.609   0.546\n",
       "3           120   3.753    9.648   75.609   0.549\n",
       "4           150   4.914   15.892   90.163   0.543\n",
       "5           180   5.987   21.069  107.937   0.543\n",
       "6           210   7.901   26.088  123.051   0.542\n",
       "7           240   6.757   23.454  129.213   0.150"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_runtime(running_time_incremental_alternate_batch, path=exp3_runtime_path, static_learner=False, alternate_batch=True)\n",
    "running_time_incremental_alternate_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "1RuBt5n7Vqzf",
    "outputId": "19834cb0-07e4-40ca-fa37-9197f95ea1a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HT_Reg</th>\n",
       "      <th>HAT_Reg</th>\n",
       "      <th>ARF_Reg</th>\n",
       "      <th>PA_Reg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>4892.321</td>\n",
       "      <td>4891.071</td>\n",
       "      <td>4257.957</td>\n",
       "      <td>22228.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE</th>\n",
       "      <td>27.284</td>\n",
       "      <td>27.239</td>\n",
       "      <td>10.160</td>\n",
       "      <td>40.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE</th>\n",
       "      <td>18075.028</td>\n",
       "      <td>18074.166</td>\n",
       "      <td>16397.503</td>\n",
       "      <td>83660.807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time(sec)</th>\n",
       "      <td>4.589</td>\n",
       "      <td>13.644</td>\n",
       "      <td>83.059</td>\n",
       "      <td>0.502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             HT_Reg   HAT_Reg   ARF_Reg    PA_Reg\n",
       "Metric                                           \n",
       "MAE        4892.321  4891.071  4257.957 22228.573\n",
       "MAPE         27.284    27.239    10.160    40.864\n",
       "RMSE      18075.028 18074.166 16397.503 83660.807\n",
       "Time(sec)     4.589    13.644    83.059     0.502"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_table_incremental_alternate = get_summary_table(df_alternate_batch, running_time_incremental_alternate_batch, error_metrics, static_learner=False)\n",
    "save_summary_table(summary_table_incremental_alternate, exp3_summary_path, static_learner=False, alternate_batch=True, transpose=True)\n",
    "summary_table_incremental_alternate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "NMA7zx5cBEmj",
    "outputId": "b542afed-4386-483e-c448-b44ae2229e29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/Result/exp3/ (stored 0%)\n",
      "  adding: content/Result/exp3/combined_country_MAE_incremental_alternate_batch.csv (deflated 42%)\n",
      "  adding: content/Result/exp3/combined_country_MAE_incremental_alternate_batch.tex (deflated 58%)\n",
      "  adding: content/Result/exp3/united_dataframe/ (stored 0%)\n",
      "  adding: content/Result/exp3/united_dataframe/incremental_alternate/ (stored 0%)\n",
      "  adding: content/Result/exp3/united_dataframe/incremental_alternate/united_df.csv (deflated 62%)\n",
      "  adding: content/Result/exp3/summary/ (stored 0%)\n",
      "  adding: content/Result/exp3/summary/combined_country_summary_table_incremental_alternate_batch.tex (deflated 38%)\n",
      "  adding: content/Result/exp3/summary/combined_country_summary_table_incremental_alternate_batch.csv (deflated 27%)\n",
      "  adding: content/Result/exp3/combined_country_RMSE_incremental_alternate_batch.csv (deflated 43%)\n",
      "  adding: content/Result/exp3/combined_country_RMSE_incremental_alternate_batch.tex (deflated 58%)\n",
      "  adding: content/Result/exp3/combined_country_MAPE_incremental_alternate_batch.tex (deflated 56%)\n",
      "  adding: content/Result/exp3/combined_country_MAPE_incremental_alternate_batch.csv (deflated 41%)\n",
      "  adding: content/Result/exp3/runtime/ (stored 0%)\n",
      "  adding: content/Result/exp3/runtime/combined_country_runtime_incremental_alternate_batch.tex (deflated 56%)\n",
      "  adding: content/Result/exp3/runtime/combined_country_runtime_incremental_alternate_batch.csv (deflated 37%)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_0db47176-79e6-4a4a-b8af-a41d7ce2ac00\", \"exp3.zip\", 306340)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving experiment 3\n",
    "!zip -r /content/Result/exp3.zip /content/Result/exp3\n",
    "from google.colab import files\n",
    "files.download(\"/content/Result/exp3.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQYy8fGQ_EJ5"
   },
   "source": [
    "## Significance tests for Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yH-IbaPBk2D"
   },
   "outputs": [],
   "source": [
    "## EXP2\n",
    "# Significance results for Experiment 2\n",
    "err_metric_for_significance = 'MAPE'\n",
    "significance_thresh = 0.05\n",
    "plot_pop = False\n",
    "\n",
    "# Concatenating a population of all results (as in boxplot) for experiment 2\n",
    "# This is done for runs per batch for experiment 2. But for experiment 1 it's done for runs per country (their final averages, like the result sent to the boxplots).\n",
    "static = df_sklearn[df_sklearn['EvaluationMeasurement'] == err_metric_for_significance].drop(columns=['EvaluationMeasurement'], axis=1).transpose()\n",
    "incremental = df_alternate_batch[df_alternate_batch['EvaluationMeasurement'] == err_metric_for_significance].drop(columns=['EvaluationMeasurement', 'PretrainDays'], axis=1).transpose()\n",
    "concated_df = pd.concat([static, incremental]).transpose()\n",
    "concated_df.set_index('PretrainDays', inplace=True, drop=True)\n",
    "concated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbQZ10ONBwJZ"
   },
   "outputs": [],
   "source": [
    "# Selecting the best algorithm for statistical comparisons\n",
    "# We want to know if the best is statistically significantly better compared to the rest.\n",
    "best_algo = concated_df.mean().sort_values(ascending=True).index[0]\n",
    "best_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8i0TvT9BNHwS"
   },
   "outputs": [],
   "source": [
    "print('AVG results across countries')\n",
    "concated_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3v6za-agNIrF"
   },
   "outputs": [],
   "source": [
    "print('STEDEV across countries')\n",
    "concated_df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cG_TSWRXBwtM"
   },
   "outputs": [],
   "source": [
    "# Iterate through all the other algorithms to see if the difference in results is significant\n",
    "competitors = list(concated_df.columns)\n",
    "competitors.remove(best_algo)\n",
    "for significance_thresh in [0.01,0.05]:\n",
    "  print(f'Running significane at: {significance_thresh}')\n",
    "  for competitor in competitors:\n",
    "    # print(competitor)\n",
    "    pval, significant = check_significance(concated_df[best_algo], concated_df[competitor], significance_at=significance_thresh)\n",
    "    print(f'Comparison of {best_algo} to {competitor} pvalue: {pval}   /  significant?: {significant}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsx2eNV0WqxC"
   },
   "outputs": [],
   "source": [
    "# Iterate through all the other algorithms to see if the difference in results is significant\n",
    "best_algo2 = concated_df.mean().sort_values(ascending=True).index[1]\n",
    "competitors = list(concated_df.columns)\n",
    "competitors.remove(best_algo2)\n",
    "for significance_thresh in [0.01,0.05]:\n",
    "  print(f'Running significance at: {significance_thresh}')\n",
    "  for competitor in competitors:\n",
    "    # print(competitor)\n",
    "    pval, significant = check_significance(concated_df[best_algo2], concated_df[competitor], significance_at=significance_thresh)\n",
    "    print(f'Comparison of {best_algo2} to {competitor} pvalue: {pval}   /  significant?: {significant}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPwMeUDwn5Tk"
   },
   "source": [
    "# Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "-fVUOfpj78ES",
    "outputId": "61fd6ce8-deaf-4c47-c5fc-b879d2a07dc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/Result/ (stored 0%)\n",
      "  adding: content/Result/exp3/ (stored 0%)\n",
      "  adding: content/Result/exp3/summary/ (stored 0%)\n",
      "  adding: content/Result/exp3/united_dataframe/ (stored 0%)\n",
      "  adding: content/Result/exp3/united_dataframe/incremental_alternate/ (stored 0%)\n",
      "  adding: content/Result/exp3/runtime/ (stored 0%)\n",
      "  adding: content/Result/exp1/ (stored 0%)\n",
      "  adding: content/Result/exp1/summary/ (stored 0%)\n",
      "  adding: content/Result/exp1/united_dataframe/ (stored 0%)\n",
      "  adding: content/Result/exp1/united_dataframe/incremental/ (stored 0%)\n",
      "  adding: content/Result/exp1/united_dataframe/static/ (stored 0%)\n",
      "  adding: content/Result/exp1/runtime/ (stored 0%)\n",
      "  adding: content/Result/exp2/ (stored 0%)\n",
      "  adding: content/Result/exp2/combined_country_RMSE_static.csv (deflated 40%)\n",
      "  adding: content/Result/exp2/combined_country_MAE_static.csv (deflated 39%)\n",
      "  adding: content/Result/exp2/summary/ (stored 0%)\n",
      "  adding: content/Result/exp2/summary/combined_country_summary_table_static.tex (deflated 39%)\n",
      "  adding: content/Result/exp2/summary/combined_country_summary_table_static.csv (deflated 25%)\n",
      "  adding: content/Result/exp2/united_dataframe/ (stored 0%)\n",
      "  adding: content/Result/exp2/united_dataframe/incremental/ (stored 0%)\n",
      "  adding: content/Result/exp2/united_dataframe/static/ (stored 0%)\n",
      "  adding: content/Result/exp2/united_dataframe/static/united_df.csv (deflated 62%)\n",
      "  adding: content/Result/exp2/combined_country_MAPE_static.tex (deflated 54%)\n",
      "  adding: content/Result/exp2/runtime/ (stored 0%)\n",
      "  adding: content/Result/exp2/runtime/combined_country_runtime_static.tex (deflated 63%)\n",
      "  adding: content/Result/exp2/runtime/combined_country_runtime_static.csv (deflated 37%)\n",
      "  adding: content/Result/exp2/combined_country_RMSE_static.tex (deflated 53%)\n",
      "  adding: content/Result/exp2/combined_country_MAE_static.tex (deflated 53%)\n",
      "  adding: content/Result/exp2/combined_country_MAPE_static.csv (deflated 40%)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_735530fe-c06e-448c-8321-46fc225c63a8\", \"Result.zip\", 542866)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!zip -r /content/Result.zip /content/Result\n",
    "from google.colab import files\n",
    "files.download(\"/content/Result.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDICI28viZYg"
   },
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DR7Z02IGVCk"
   },
   "source": [
    "## Bar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmQXDjmrXAiS"
   },
   "outputs": [],
   "source": [
    "# Download csv files\n",
    "!zip -r /content/csv_files.zip /content/csv_files\n",
    "from google.colab import files\n",
    "files.download(\"/content/csv_files.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQRkdX0oGaec"
   },
   "source": [
    "## Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oL2TxDdHRwM"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df, metric_type, learner_type, col_mapper):\n",
    "  # Only pretrain days records are required not the mean row\n",
    "  df.drop(['mean'], axis=1, inplace=True)\n",
    "\n",
    "  # Renaming the Algorithm Columns\n",
    "  df.rename(columns={'Unnamed: 0': 'Algorithms'}, inplace=True)  \n",
    "\n",
    "  # Dropping first two rows: \"EvaluationMeasurement\" & \"PretrainDays\"\n",
    "  df.drop([0,1], axis=0, inplace=True)  \n",
    "\n",
    "  # Renaming columns based on mapper\n",
    "  df['Algorithms'].replace(col_mapper, inplace=True)  \n",
    "\n",
    "  # Melting the dataframe based on 'Algorithms'\n",
    "  df_melt = df.melt(id_vars=['Algorithms'])  \n",
    "\n",
    "  # Dropping unwanted varibale column(created bcoz of index)\n",
    "  df_melt.drop('variable', axis=1, inplace=True)  \n",
    "\n",
    "  # Renaming the value column by metric type\n",
    "  df_melt.rename(columns={'value':metric_type}, inplace=True)  \n",
    "\n",
    "  # Converting to float value bcoz by default the values are of type object\n",
    "  df_melt[metric_type] = df_melt[metric_type].astype('float64')  \n",
    "\n",
    "  df_melt['Learner Type']= learner_type  # Adding the learner type\n",
    "\n",
    "  return df_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKugGWSVBPH6"
   },
   "outputs": [],
   "source": [
    "def order_by_median(df,reverse = False):\n",
    "    grouped_df = df.groupby('Algorithms')\n",
    "    algo_medians = {}\n",
    "    for cur_group in grouped_df.groups.keys():\n",
    "        df_cur_grp = grouped_df.get_group(cur_group)\n",
    "        algo_medians[cur_group] = df_cur_grp['MAPE'].median()\n",
    "    sorted_algo_medians = dict(sorted(algo_medians.items(), key=lambda kv: kv[1], reverse=reverse))\n",
    "    return list(sorted_algo_medians.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-GmwJSgmvw6"
   },
   "outputs": [],
   "source": [
    "# If value is less than zero return float value otherwise an integer value\n",
    "def format_values(y_val,pos):\n",
    "    if y_val < 1:\n",
    "        return format(float(y_val))\n",
    "    else:\n",
    "        return format(int(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1676SLhHSe_"
   },
   "outputs": [],
   "source": [
    "def draw_save_boxplot(df, hue_order_learner, save_filename, prequential=False):\n",
    "    if not prequential:\n",
    "        colors = ['#1f77b4', '#2ca02c', '#ff7f0e', '#ca0020']\n",
    "    else:\n",
    "        colors = ['#ca0020', '#2ca02c', '#ff7f0e']\n",
    "    \n",
    "    # Setting custom color palette\n",
    "    sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "    plt.figure(figsize=(10, 6), dpi=90)\n",
    "    ordered_algo_list = order_by_median(df, reverse=False)\n",
    "\n",
    "    ax = sns.boxplot(x=\"Algorithms\", y=metric_type, hue='Learner Type', data=df, order=ordered_algo_list, dodge=False, width=0.5, hue_order=hue_order_learner)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    ax.set(yscale='log')\n",
    "    ax.set_ylim(top=1000)\n",
    "    ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(format_values))  # lambda x, p: format(int(x), ',')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax.set_ylabel(metric_type,fontsize=18)\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{box_plot_path}/{save_filename}.pdf')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKWFUip8HZd3"
   },
   "outputs": [],
   "source": [
    "def read_preprocess_plot_graph(filenames, col_mapper, save_filename, metric_type='MAPE'):\n",
    "    metric_type = metric_type\n",
    "    frames = []\n",
    "    for filename in filenames:\n",
    "        if 'static' in filename:\n",
    "            learner_type = 'Static'\n",
    "        elif 'alternate' in filename:\n",
    "            learner_type = 'Incremental(prequential)'\n",
    "        else:\n",
    "            learner_type = 'Incremental'\n",
    "\n",
    "        df = pd.read_csv(filename)\n",
    "        df_melt = preprocess_data(df, metric_type, learner_type, col_mapper)\n",
    "        frames.append(df_melt)\n",
    "\n",
    "    final_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # Updating LSTM learner type as Sequential\n",
    "    final_df.loc[final_df['Algorithms'] == 'LSTM', 'Learner Type'] = 'Sequential'\n",
    "\n",
    "    # Sorting final dataframe\n",
    "    final_df = final_df.sort_values(by=['MAPE'])\n",
    "\n",
    "    hue_order_learner = sorted(final_df['Learner Type'].unique())\n",
    "\n",
    "    prequential_flag = 'Incremental(prequential)' in hue_order_learner\n",
    "\n",
    "    draw_save_boxplot(final_df, hue_order_learner, save_filename, prequential=prequential_flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeltxQL_JnUH"
   },
   "outputs": [],
   "source": [
    "col_mapper = {'HT_Reg':'Hoeffding Trees',\n",
    "              'HAT_Reg':'Hoeffding Adapt Tr',\n",
    "              'ARF_Reg':'Adaptive RF',\n",
    "              'PA_Reg':'Pass Agg Regr',\n",
    "              'RandomForest':'Random Forest',\n",
    "              'GradientBoosting': 'Gradient Boosting',\n",
    "              'DecisionTree': 'Decision Trees',\n",
    "              'LinearSVR': 'Linear SVR',\n",
    "              'BayesianRidge':'Bayesian Ridge'\n",
    "              }\n",
    "\n",
    "metric_type = 'MAPE'\n",
    "exp1_path = 'content/Result/exp1'\n",
    "exp2_path = 'content/Result/exp2'\n",
    "exp3_path = 'content/Result/exp3'\n",
    "exp1_filenames = glob.glob(f'{exp1_path}/*{metric_type}*.csv')\n",
    "exp2_filenames = glob.glob(f'{exp2_path}/*{metric_type}*.csv')\n",
    "exp3_filenames = glob.glob(f'{exp3_path}/*{metric_type}*.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uXMLcXK_ZkQ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "col_mapper = {'HT_Reg':'HT',\n",
    "              'HAT_Reg':'HAT',\n",
    "              'ARF_Reg':'ARF',\n",
    "              'PA_Reg':'PA',\n",
    "              'RandomForest':'Random Forest',\n",
    "              'GradientBoosting': 'Gradient Boosting',\n",
    "              'DecisionTree': 'Decision Tree',\n",
    "              'LinearSVR': 'Linear SVR',\n",
    "              'BayesianRidge':'Bayesian Ridge',\n",
    "              'LSTM': 'LSTM'\n",
    "              }\n",
    "\n",
    "metric_type = 'MAPE'\n",
    "exp1_filenames = glob.glob(f'{exp1_path}/*{metric_type}*.csv')\n",
    "exp2_filenames = []\n",
    "\n",
    "for filename in glob.glob(f'{exp2_path}/*{metric_type}*.csv'):\n",
    "  if 'alternate' in filename or 'static' in filename:\n",
    "    exp2_filenames.append(filename)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFw4A4RoXGhs"
   },
   "outputs": [],
   "source": [
    "save_filename = 'fig1'\n",
    "read_preprocess_plot_graph(exp1_filenames, col_mapper, save_filename, metric_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9MYJNUYjvfZ"
   },
   "outputs": [],
   "source": [
    "save_filename = 'fig2'\n",
    "read_preprocess_plot_graph(exp2_filenames, col_mapper, save_filename,  metric_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZzRDnXZdLBf"
   },
   "outputs": [],
   "source": [
    "exp3_filenames.append(exp2_filenames[1])\n",
    "save_filename = 'fig3'\n",
    "read_preprocess_plot_graph(exp3_filenames, col_mapper, save_filename,  metric_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eA9pc2xtzDUL"
   },
   "outputs": [],
   "source": [
    "# Downloading plots\n",
    "!zip -r /content/Plots.zip /content/Plots\n",
    "from google.colab import files\n",
    "files.download(\"/content/Plots.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCfQEvF1oEzE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Incremental_learning_main.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
